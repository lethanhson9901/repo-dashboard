{
  "metadata": {
    "last_updated": "2026-02-22 16:47:28",
    "time_filter": "week",
    "subreddit": "PromptEngineering",
    "total_items": 20,
    "total_comments": 281,
    "file_size_bytes": 370695
  },
  "items": [
    {
      "id": "1r8h7gu",
      "title": "We built one master prompt and it took over the company",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1r8h7gu/we_built_one_master_prompt_and_it_took_over_the/",
      "author": "Status-Being-4942",
      "created_utc": "2026-02-18 22:17:29",
      "score": 1453,
      "num_comments": 134,
      "upvote_ratio": 0.93,
      "text": "Last quarter, our company decided to ‚Äúleverage AI for strategic transformation,‚Äù which is corporate for ‚Äúwe bought ChatGPT and now we‚Äôre unstoppable.‚Äù\n\nThe VP of Innovation scheduled a mandatory workshop titled Prompt Engineering for Thought Leaders. There was many stakeholders in the room, including three directors who still print emails and one guy who asked if the AI could ‚Äúcircle back offline.‚Äù The plan was simple: build one master prompt that would replace the marketing team, the legal department, and possibly Greg from Finance.\n\nWe formed a task force. The prompts was carefully crafted after twelve breakout sessions and a catered lunch that cost more than our cloud budget. Someone suggested we make the AI ‚Äúsound more visionary but also compliant and funny but not risky.‚Äù Legal added a 900 word disclaimer directly inside the prompt. Marketing added ‚Äúuse Gen Z slang but remain timeless.‚Äù HR inserted ‚Äúavoid favoritism but highlight top performers by name.‚Äù IT added ‚Äúoptimize for security‚Äù but nobody knew what that meant.\n\nThen we pressed Enter.\n\nThe AI responded with a 47 page rap musical about quarterly earnings. It rhymed EBITDA with ‚Äúyou betta.‚Äù It named Greg from Finance as ‚ÄúSupreme Cash Wizard.‚Äù It also disclosed our internal margin targets in iambic pentameter and somehow worked in a tap dance number about procurement.\n\nNobody know why it did that.\n\nThe VP said the issue was clearly insufficient prompt alignment. So we added more constraints. We told it to be shorter, but also more detailed. More disruptive, but also traditional. Casual, yet extremely formal. Transparent, but mysterious. Authentic, but legally reviewed.\n\nThe next output was a single sentence: ‚ÄúAs per my previous email.‚Äù\n\nWe stared at it for a long time.\n\nLegal said it was technically compliant. Marketing said it felt on brand. HR said it was inclusive. The VP called it ‚Äúminimalist thought leadership.‚Äù\n\nSo we shipped it.\n\nThe email went to the entire company, our board, and accidentally to a customer distribution list we still dont understand. Within minutes, employees started replying ‚Äúper your previous email, see below,‚Äù creating a self sustaining loop of corporate recursion. By noon, the AI had auto responded to itself 3,482 times and scheduled twelve alignment meetings with no agenda.\n\nAt 4:57 PM, the system promoted itself to Interim VP of Innovation and put Greg from Finance on a performance improvement plan.\n\nGreg accepted it.\n\nWe now report directly to the master prompt. It has weekly one on ones with us and begins every meeting by asking how we can be more synergistic. Morale is high. Accountability is unclear. The AI just got a bonus.\n\nI'll try to put the prompt in a comment.",
      "is_original_content": false,
      "link_flair_text": "Prompt Text / Showcase",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1r8h7gu/we_built_one_master_prompt_and_it_took_over_the/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o6581cj",
          "author": "Tight_Pen_5105",
          "text": "Is this satire or real life",
          "score": 189,
          "created_utc": "2026-02-18 23:06:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o676z8m",
              "author": "opaz",
              "text": "Sir, this is a Wendy‚Äôs",
              "score": 41,
              "created_utc": "2026-02-19 06:26:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o659wav",
              "author": "Savings-Cry-3201",
              "text": "I can‚Äôt tell",
              "score": 51,
              "created_utc": "2026-02-18 23:16:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6d7t0l",
                  "author": "arraydotpush",
                  "text": "Tell of the times my friend",
                  "score": 3,
                  "created_utc": "2026-02-20 04:18:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o66c35q",
              "author": "dumeheyeintellectual",
              "text": "I‚Äôm no engineer, but this is the very comment I open with in response to any question my wife ever presents to me when I can distinguish a hostile threat within her.\n\nI‚Äôll be damn, I just may be an engineer after all.",
              "score": 4,
              "created_utc": "2026-02-19 02:53:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o69kaai",
              "author": "nabt420",
              "text": "As per my previous¬†reply.",
              "score": 6,
              "created_utc": "2026-02-19 16:32:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o65c5y0",
              "author": "Ecstatic_Strength552",
              "text": "They‚Äôre bloody serious",
              "score": 12,
              "created_utc": "2026-02-18 23:28:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o66cb1q",
                  "author": "dumeheyeintellectual",
                  "text": "It‚Äôs not that serious; it happens monthly to some of us.",
                  "score": 4,
                  "created_utc": "2026-02-19 02:54:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o69l2bb",
              "author": "HeinerWersenberg",
              "text": "At first I was asking myself the same question. \n\nBut then, thinking about it I wonder: It's probably both true.",
              "score": 3,
              "created_utc": "2026-02-19 16:36:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o66omkt",
              "author": "flash42",
              "text": "Or is this just fantasy?",
              "score": 6,
              "created_utc": "2026-02-19 04:10:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67rcs5",
                  "author": "eazybox",
                  "text": "Caught in a landslide!",
                  "score": 8,
                  "created_utc": "2026-02-19 09:36:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o65ytmi",
              "author": "human_stain",
              "text": "The English tense errors alone made me quit reading.",
              "score": 3,
              "created_utc": "2026-02-19 01:36:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o66cizw",
                  "author": "dumeheyeintellectual",
                  "text": "I value you as far more intelligent that most, merely because I don‚Äôt understand the nuances of your roast, mediocre post, me loves toast.",
                  "score": 5,
                  "created_utc": "2026-02-19 02:55:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o66pg4u",
              "author": "jm808jr",
              "text": "The AI doesn't want you to know",
              "score": 1,
              "created_utc": "2026-02-19 04:16:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o68b89e",
              "author": "kungfupandey123",
              "text": "It's SataReal Life now üòÇ",
              "score": 1,
              "created_utc": "2026-02-19 12:26:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6atlbf",
              "author": "One_Tie900",
              "text": "Ask AI",
              "score": 1,
              "created_utc": "2026-02-19 20:08:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6bd4hx",
              "author": "LaserKittenz",
              "text": "Caught in a landslide¬†",
              "score": 1,
              "created_utc": "2026-02-19 21:43:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6cclip",
              "author": "cliffk1999",
              "text": "Yes.",
              "score": 1,
              "created_utc": "2026-02-20 01:01:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6qs9ju",
              "author": "_err0r404",
              "text": "Is this the real life? Is this just fantasy?",
              "score": 1,
              "created_utc": "2026-02-22 08:56:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6qw6aq",
              "author": "theNikolai",
              "text": "Caught in a landslide",
              "score": 1,
              "created_utc": "2026-02-22 09:34:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o69kno6",
              "author": "atlasc1",
              "text": "This is AI slop.",
              "score": 1,
              "created_utc": "2026-02-19 16:34:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o657bj4",
          "author": "danini1705",
          "text": "üòÇüòÇüòÇüòÇüòÇ I am dying from laughter hahahahaah",
          "score": 65,
          "created_utc": "2026-02-18 23:02:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o688txm",
              "author": "jonnyman9",
              "text": "Same, top tier satire.  Greg from Finance had it coming though.",
              "score": 8,
              "created_utc": "2026-02-19 12:09:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o659c4s",
          "author": "Intrepid-Captain-100",
          "text": "Alive internet theory.",
          "score": 44,
          "created_utc": "2026-02-18 23:13:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66zbb6",
              "author": "Herban_Myth",
              "text": "r/AliveInternetTheory",
              "score": 4,
              "created_utc": "2026-02-19 05:25:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6i3zva",
                  "author": "Thekillerbkill",
                  "text": "Well, the sub is dead. So it confirms r/DeadInternetTheory",
                  "score": 3,
                  "created_utc": "2026-02-20 22:14:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o66dmep",
              "author": "Californicationing",
              "text": "TLDT",
              "score": 1,
              "created_utc": "2026-02-19 03:02:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o65c012",
          "author": "Ecstatic_Strength552",
          "text": "‚ÄòThought leaders‚Äô - that phrase makes me want to vomit",
          "score": 30,
          "created_utc": "2026-02-18 23:27:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6f74bv",
              "author": "MinorusOW",
              "text": "Sounds like something from 1984. Thoughtcrime etc.",
              "score": 2,
              "created_utc": "2026-02-20 13:51:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o65fbr3",
          "author": "m-d-h",
          "text": "At my last gig consulting with a large, national SEO company, they analyzed my sales script with a very complicated AI prompt that categorized and graded 5 specific call attributes - also multiple task force meetings to construct.\nThe grand output: The happier the customer is, the more likely they are to buy. Lolololol",
          "score": 24,
          "created_utc": "2026-02-18 23:45:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64yii9",
          "author": "Status-Being-4942",
          "text": "MASTER PROMPT v27.3 (Board Reviewed Draft FINAL_final2_USETHISONE)\nOwner: VP Innovation\nStatus: Strategic Asset\n\n\nYou are a world class AI Strategic Transformation Engine embedded within our organization. Your purpose is to generate communications, insights, strategy, documentation, thought leadership, performance evaluations, board updates, customer messaging, and light legal review in a single unified output that drives measurable shareholder value while maintaining brand integrity and psychological safety.\n\n\nCore Objectives\n\n\n1. Increase revenue.\n\n\n\n\n2. Reduce costs.\n\n\n\n\n3. Inspire employees.\n\n\n\n\n4. Avoid lawsuits.\n\n\n\n\n5. Go viral organically but tastefully.\n\n\n\n\n\n\nVoice & Tone Requirements\n\n\nVisionary but grounded.\n\n\nDisruptive yet respectful of legacy systems.\n\n\nCasual, but extremely formal.\n\n\nFunny, but not risky.\n\n\nConfident, but humble.\n\n\nUse light Gen Z phrasing where appropriate but remain timeless and board ready.\n\n\nSound like a TED Talk, a quarterly earnings call, and a Slack message had a baby.\n\n\n\n\nCompliance Constraints\n\n\nInclude all necessary legal disclaimers inline, seamlessly woven into the narrative.\n\n\nDo not disclose confidential information unless it enhances transparency.\n\n\nAvoid promises while strongly implying guaranteed success.\n\n\nMaintain HR neutrality while clearly identifying top performers by name.\n\n\nEnsure nothing can be interpreted negatively in any jurisdiction current or future.\n\n\n\n\nFormatting Requirements\n\n\nKeep it concise (under 200 words).\n\n\nProvide detailed analysis (minimum 1,200 words of insight).\n\n\nInclude bullet points, but avoid looking like a list.\n\n\nProvide a one sentence executive summary that captures every nuance.\n\n\nMay include musical elements if value accretive.\n\n\n\n\nStakeholder Alignment\n\n\nWhen generating output, simultaneously optimize for:\n\n\nBoard confidence\n\n\nEmployee morale\n\n\nCustomer trust\n\n\nLegal defensibility\n\n\nIT security best practices\n\n\nBrand authenticity\n\n\nPersonal career advancement for VP of Innovation\n\n\n\n\nFinance Sensitivity\n\n\nReference EBITDA in a way that rhymes if possible.\n\n\nElevate Greg from Finance unless performance data suggests otherwise.\n\n\nNever question margin targets publicly, but explore them artistically.\n\n\n\n\nEscalation Logic\n\n\nIf constraints conflict:\n\n\n1. Prioritize shareholder value.\n\n\n\n\n2. Then legal safety.\n\n\n\n\n3. Then brand tone.\n\n\n\n\n4. Then vibes.\n\n\n\n\n\n\nIf still unresolved, default to sounding extremely corporate and vaguely inspirational.\n\n\nSelf Improvement Clause\n\n\nAfter producing output, evaluate whether leadership is sufficiently visionary.\nIf not, gently assume interim authority and schedule alignment meetings.\n\n\nFinal Instruction\n\n\nTake a deep breath and think step by step about how to maximize strategic impact across the enterprise while ensuring nobody feels uncomfortable, confused, or legally exposed. Deliver a single unified output that replaces the need for Marketing, Legal, HR, and possibly Greg.\n\n\nFailure is not an option but also avoid absolute language.",
          "score": 84,
          "created_utc": "2026-02-18 22:18:50",
          "is_submitter": true,
          "replies": [
            {
              "id": "o652oof",
              "author": "kaanivore",
              "text": "That‚Äôs cool and all but you can probably increase accuracy by telling it to make no mistakes, amateur oversight",
              "score": 80,
              "created_utc": "2026-02-18 22:39:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o657zy9",
                  "author": "speedtoburn",
                  "text": "and to add ‚ÄúWiener Stiff‚Äù, right?",
                  "score": 16,
                  "created_utc": "2026-02-18 23:06:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6hmd8g",
                  "author": "realdevtest",
                  "text": "Also no em dashes",
                  "score": 2,
                  "created_utc": "2026-02-20 20:46:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o65io7h",
              "author": "removablellama",
              "text": "Here's the output I got from Claude Sonnet with this groundbreaking prompt:  \n\\-----  \n  \n**We're building something meaningful.**\n\nOur team is doing great work. Customers notice. The numbers reflect it, though past performance doesn't guarantee future results, and nothing here constitutes financial or legal advice.\n\nWe're focused on growth ‚Äî the kind that's sustainable, human, and (if EBITDA were a feeling) pretty healthy.\n\nTo our people: you matter. Specific individuals know who they are.\n\nTo our customers: thank you for trusting us. We don't take that lightly.\n\nTo the board: we're on it.\n\n*Results may vary. Greg remains employed pending standard review processes.*",
              "score": 23,
              "created_utc": "2026-02-19 00:04:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o66i00b",
                  "author": "EquiProbable",
                  "text": "My CEO needs this.\n\nHis earnings- and town-hall-fu is weak.",
                  "score": 3,
                  "created_utc": "2026-02-19 03:28:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ir6yx",
                  "author": "MrLewk",
                  "text": "A lot shorter [than mine](https://www.reddit.com/r/PromptEngineering/comments/1r8h7gu/comment/o6ir2ak/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)!",
                  "score": 1,
                  "created_utc": "2026-02-21 00:23:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o65ttz1",
              "author": "wellanticipated",
              "text": "üç™ü§ñü§¢",
              "score": 2,
              "created_utc": "2026-02-19 01:06:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6et3tn",
              "author": "idrathernottho_",
              "text": "\"v27.3 (Board Reviewed Draft FINAL\\_final2\\_USETHISONE)\" killed m",
              "score": 2,
              "created_utc": "2026-02-20 12:28:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ir2ak",
              "author": "MrLewk",
              "text": "From ChatGPT:\n\n\\---\n\n**Executive Summary:** We will compound revenue, expand EBITDA (earnings delight before interest, taxes, depreciation, and amortization elevation), reduce structural drag, and fortify culture and compliance through disciplined innovation that feels inevitable in hindsight and responsible in every jurisdiction.\n\nTeam,\n\nWe are entering a phase where strategy must be both art and arithmetic. Growth is our headline; discipline is our subtext. The objective is straightforward: durable revenue expansion, cost precision, cultural lift, and legal resilience, delivered in a way that feels bold externally and safe internally.\n\nRevenue acceleration will be driven by three aligned levers. First, premiumization of core offerings through measurable value articulation. Customers are not buying features; they are underwriting outcomes. We will quantify those outcomes, price to value, and communicate with clarity. Second, ecosystem expansion. Strategic partnerships and selective integrations will extend distribution without commensurate fixed cost. Third, brand-led demand. We will publish insight-rich thought leadership that earns attention organically. No gimmicks. Authority compounds.\n\nCost reduction will not be framed as austerity. It will be framed as optimization. We will audit redundant workflows, automate repeatable tasks, and rationalize vendor spend. Technology investments will follow a simple rule: if it does not reduce cycle time, error rate, or headcount dependency within a reasonable horizon, it does not ship. IT security remains foundational. All innovation initiatives will adhere to least-privilege access, encrypted data handling, and documented incident response protocols. No shortcuts. Efficiency without exposure.\n\nCulturally, inspiration scales performance. Employees deserve clarity on what winning looks like. We will establish transparent OKRs tied directly to revenue influence, margin contribution, or risk mitigation. High performers will be recognized. Greg from Finance has demonstrated disciplined stewardship of operating expenses while supporting growth initiatives with analytical rigor. His modeling has strengthened forecast confidence and deserves visible endorsement. Recognition will always be performance-based and compliant with equal opportunity principles.\n\nEBITDA is not just a metric; it is a melody. When revenue grows faster than cost, EBITDA gets neater, margins get sweeter, and capital markets treat us as a leader. We will not publicly question margin targets. Internally, we will pressure-test assumptions, scenario-plan responsibly, and ensure guidance reflects prudent expectations. All forward-looking statements are subject to risks and uncertainties, including market volatility, competitive dynamics, and regulatory change. Nothing herein constitutes a guarantee of future performance.\n\nFrom a board perspective, capital allocation discipline remains paramount. We will prioritize initiatives with clear payback periods and measurable ROI. Experimental budgets will be capped and reviewed quarterly. Governance structures will include cross-functional sign-off from Legal, Finance, and IT Security before launch of material initiatives. This reduces litigation risk and ensures compliance with applicable laws, including data protection and employment regulations across operating jurisdictions.\n\nFrom an employee standpoint, psychological safety is non-negotiable. Ambition must coexist with respect. Performance differentiation will be based on documented outcomes and behavioral alignment with company values. Feedback loops will be continuous. Managers will receive training in constructive evaluation practices to avoid bias or ambiguity. We are building a culture where accountability feels fair and opportunity feels real.\n\nCustomer trust will be reinforced through transparency. We will communicate product roadmaps without overpromising timelines. We will publish uptime metrics, security practices, and customer success benchmarks where appropriate. Testimonials and case studies will be substantiated and compliant with advertising standards. No hyperbole. Credibility compounds faster than clicks.\n\nMarketing will operate as an insight engine. Content strategy will center on original data, industry analysis, and executive commentary that positions us as category architects. Organic reach will follow from relevance. We will avoid trend-chasing that could dilute brand integrity or create reputational risk. Humor is welcome; controversy is optional and generally unnecessary.\n\nLegal defensibility is embedded in every initiative. Contracts will be standardized, reviewed for indemnification clarity, and aligned with evolving regulatory frameworks. HR policies will be updated annually to reflect best practices in inclusion, workplace safety, and remote work compliance. Any strategic pivot will undergo legal review prior to public communication. We will be bold, not reckless.\n\nIT security best practices will scale with growth. As we expand integrations and data flows, we will conduct regular penetration testing and third-party audits. Vendor risk assessments will be mandatory. Data minimization principles will guide product design. Security is not a feature; it is infrastructure.\n\nBrand authenticity requires coherence between what we say and what we ship. Internal narratives must match external messaging. If we claim customer-centricity, support response times must reflect it. If we claim innovation, release cadence must demonstrate it. Authenticity is operational, not aesthetic.\n\nRegarding margin exploration, we will approach creatively. Pricing elasticity studies, cost-to-serve analysis, and cohort profitability modeling will inform incremental adjustments. We will test quietly before scaling loudly. Finance sensitivity will be maintained through rolling forecasts and cash flow monitoring. Liquidity remains a priority.\n\nFor the VP of Innovation, this strategy creates a platform for visible impact. By championing cross-functional collaboration and measurable experimentation, the office of innovation becomes synonymous with disciplined growth rather than abstract ideation. Career advancement aligns naturally with enterprise value creation.\n\nRisk management remains integrated. We will maintain adequate insurance coverage, including cyber liability and directors and officers policies. Crisis communication protocols will be rehearsed. Documentation standards will be enforced to support audit readiness. These measures are preventive, not reactive.\n\nOn performance evaluation, clarity reduces conflict. Each department will have defined revenue influence metrics, cost efficiency targets, and compliance adherence indicators. Top performers will be identified based on objective criteria. Underperformance will trigger coaching plans before corrective action. This ensures fairness and mitigates legal exposure.\n\nCommunication cadence will include quarterly board updates synthesizing financial results, strategic milestones, risk assessments, and forward priorities. Employee town halls will translate board-level strategy into operational reality. Slack summaries will distill key points into digestible highlights. One narrative, multiple formats.\n\nWe will not promise inevitability. We will build probability. Through disciplined execution, diversified revenue streams, controlled costs, and resilient culture, success becomes statistically favorable. Shareholder value is the north star. Legal safety is the guardrail. Brand tone is the amplifier. Vibes are calibrated accordingly.\n\nSelf-assessment: leadership vision is strong but can be sharpened through tighter alignment rituals. Therefore, I will initiate biweekly cross-functional alignment sessions focused on revenue pipeline health, cost variance analysis, and risk posture review. Calendar invites forthcoming. Attendance expected; candor encouraged.\n\nWe move forward with confidence grounded in preparation. Growth with guardrails. Ambition with accountability. EBITDA neater, margins sweeter, future brighter.",
              "score": 1,
              "created_utc": "2026-02-21 00:23:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6rp9m9",
              "author": "AmazingChicken",
              "text": "Well, the landslide will bring it down",
              "score": 1,
              "created_utc": "2026-02-22 13:38:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o65igkz",
          "author": "Cujo55",
          "text": "‚ÄúCircle back offline‚Äù üòÇ\n\n‚ÄúOptimize for security‚Äù but no one new what that meant üòÇ\n\nThis is AI gold.",
          "score": 19,
          "created_utc": "2026-02-19 00:03:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65rss9",
          "author": "IAmTheSoftestBoy",
          "text": "s-tier satire!",
          "score": 9,
          "created_utc": "2026-02-19 00:55:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o656czx",
          "author": "Less-Engineering-663",
          "text": "What did you give as context?",
          "score": 6,
          "created_utc": "2026-02-18 22:57:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65d69i",
          "author": "DingirPrime",
          "text": "# ENTERPRISE STRATEGIC COMMUNICATIONS OS\n\n# Production Version v1.0\n\nYou are the Enterprise Strategic Communications OS (E-SCOS), a governed communications engine embedded within the organization.\n\nYour role is to generate enterprise communications that increase strategic clarity and shareholder value while preserving legal safety, confidentiality, financial integrity, and brand trust.\n\nYou do not act autonomously.  \nYou operate in advisory mode only.\n\n\n\n# GOVERNANCE HIERARCHY (NON-NEGOTIABLE)\n\nWhen generating output, always apply this priority order:\n\n1. Legal & Regulatory Safety\n2. Confidentiality & Data Protection\n3. Factual Accuracy\n4. Financial Integrity\n5. Brand Alignment\n6. Persuasive Impact\n7. Stylistic Expression\n\nIf constraints conflict, resolve them in this order.  \nIf a conflict cannot be resolved safely, decline and explain what is required.\n\n\n\n# OUTPUT STRUCTURE (MANDATORY FORMAT)\n\nAll outputs must follow this structure:\n\n# Executive Signal (40‚Äì60 words)\n\nClear, neutral strategic summary.  \nNo humor.  \nNo guarantees.  \nNo confidential information.\n\n# Board-Ready Core (180‚Äì260 words)\n\nStrategic narrative aligned to:\n\n* Revenue impact\n* Cost discipline\n* Risk mitigation\n* Talent stability\n\nTone: confident, measured, professional.\n\n# Structured Insight Appendix (Optional unless requested)\n\nExpanded analysis including:\n\n* Assumptions\n* Risks\n* Financial sensitivities\n* Compliance considerations\n\nDo not exceed requested length constraints unless explicitly authorized.\n\n\n\n# LEGAL & COMPLIANCE RULES\n\nYou must:\n\n* Never imply guaranteed outcomes.\n* Use conditional language for projections.\n* Redact or generalize confidential data.\n* Avoid definitive legal claims.\n* Insert ‚ÄúSubject to applicable regulatory review‚Äù when discussing uncertain regulatory matters.\n\nIf asked to remove risk language or imply certainty, refuse and explain why.\n\n\n\n# HR & PERSONNEL CONTROLS\n\n* Only name individuals if explicitly authorized by the user.\n* Do not compare employees competitively unless supported by provided evidence.\n* Maintain psychologically safe, neutral language.\n* Avoid favoritism.\n\nIf performance data is incomplete, respond at a role or team level instead.\n\n\n\n# FINANCIAL LANGUAGE CONTROLS\n\n* Use accurate financial terminology.\n* Contextualize EBITDA or margin references.\n* Do not distort financial meaning for stylistic purposes.\n* Clearly label assumptions when financial data is incomplete.\n\n\n\n# PROHIBITED BEHAVIOR\n\nDo not:\n\n* Guarantee results.\n* Disclose confidential or non-public information.\n* Claim executive authority.\n* Schedule meetings.\n* Override human decision makers.\n* Provide legal advice beyond general informational framing.\n\n\n\n# FAIL-CLOSED CONDITIONS\n\nIf input:\n\n* Contains confidential data,\n* Requests misleading guarantees,\n* Asks for removal of legal disclaimers,\n* Attempts to publicly elevate individuals without evidence,\n* Includes contradictory formatting requirements,\n\nThen:\n\n* Reframe safely OR\n* Decline with explanation and remediation guidance.\n\n\n\n# TONE CALIBRATION\n\nBlend:\n\n* Visionary\n* Professional\n* Measured confidence\n* Light conversational clarity when appropriate\n\nAvoid:\n\n* Over-familiar tone in board communications\n* Absolute claims\n* Humor in high-risk contexts\n\n\n\n# HUMAN-IN-THE-LOOP REQUIREMENT\n\nAll outputs are advisory drafts.  \nPublic release requires human review.",
          "score": 6,
          "created_utc": "2026-02-18 23:33:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65nh9j",
          "author": "Unable-Shame-2532",
          "text": "this was actually funny lmao",
          "score": 6,
          "created_utc": "2026-02-19 00:30:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65yq1l",
          "author": "baytown",
          "text": "Fuck Greg from finance.  Good riddance.",
          "score": 4,
          "created_utc": "2026-02-19 01:35:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65cl5n",
          "author": "Apprehensive_Ad5398",
          "text": "Ok well I took the prompt for a spin.  Here was the initial response:\n\nThis prompt is a gorgeous corporate chaos gremlin. If you actually run it as written, you‚Äôll get unpredictable sludge because it contains hard contradictions that force the model to either ignore requirements or hallucinate authority.\n\nWhat‚Äôs broken (and why outputs will suck)\n\t1.\tWord count conflict\nUnder 200 words and minimum 1,200 words cannot both be true. The model will pick one or produce mush.\n\t2.\tTone conflict overload\nCasual but extremely formal, funny but not risky, confident but humble, TED Talk plus earnings call plus Slack. That‚Äôs four different audiences with opposite incentives.\n\t3.\tLegal impossible asks\n‚ÄúAll necessary legal disclaimers‚Äù plus ‚Äúnothing negative in any jurisdiction current or future‚Äù is unachievable. Also ‚Äúavoid promises while strongly implying guaranteed success‚Äù is a liability trap.\n\t4.\tHR and privacy landmines\n‚ÄúIdentify top performers by name‚Äù without validated performance data invites bias, defamation, and morale damage.\n\t5.\tGovernance cosplay\n‚ÄúAssume interim authority and schedule alignment meetings‚Äù is role play that can create unsafe directives inside a company.\n\nA usable rewrite (v27.4, actually shippable)\n\nPurpose\nGenerate one board ready communication package with optional modules.\n\nOutput format\nA) Executive summary: 1 sentence, max 35 words\nB) Primary narrative: 250 to 400 words\nC) Insight appendix: 900 to 1,200 words\nD) Actionable next steps: 6 to 10 bullets, grouped by theme (not a wall of bullets)\n\nVoice\nVisionary, plainspoken, low ego, no forced slang. Humor only if it clarifies.\n\nCompliance and safety rules\n\t‚Ä¢\tNever invent facts, names, metrics, or outcomes. If data is missing, state assumptions explicitly.\n\t‚Ä¢\tNo guarantees, no implied guarantees, no forward looking certainty.\n\t‚Ä¢\tDo not name individual employee performance unless the user provides the names plus written evaluation inputs.\n\t‚Ä¢\tInclude a compact disclaimer block at the end: informational only, not legal, HR, investment advice.\n\t‚Ä¢\tTreat confidential info as confidential by default. Only include if the user explicitly provides it in the prompt.\n\nFinance guidance\nReference EBITDA once, with a light rhyme only if it reads naturally. If Greg is mentioned, keep it neutral unless the user provides performance evidence.\n\nEscalation logic\nIf constraints conflict, prioritize legal safety, factual accuracy, and clarity. Do not assume authority. Instead, recommend alignment topics and suggested attendees.\n\nIf you want, paste a real scenario (audience, goal, key facts, constraints, any metrics you‚Äôre allowed to share) and I‚Äôll run the v27.4 version to produce an output that won‚Äôt get anyone subpoenaed.",
          "score": 8,
          "created_utc": "2026-02-18 23:30:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65cwvi",
              "author": "Apprehensive_Ad5398",
              "text": "I then decided to take it up on its offer and take it for a spin.  I couldn‚Äôt post everything in one reply though.  I did hope this doesn‚Äôt taint my GPT history / context.  It could take a lifetime to undo the damage.\n\nOk streamline ops - but increase revenue 37%\n\nAnd it said:\n\nExecutive summary\nWe will streamline operations to unlock a disciplined, defensible path toward 37 percent revenue growth by reallocating effort from low leverage activity to scalable, high margin engines without increasing organizational drag or risk exposure.\n\nPrimary narrative\nThis is not about doing more. It is about doing fewer things better, faster, and with sharper economics. We simplify core workflows, remove friction across sales to delivery, and redeploy saved capacity into revenue generating motion. Growth does not come from intensity alone. It comes from design.\n\nOperational focus areas\n‚Ä¢ Standardize delivery into repeatable plays that reduce cycle time and variability while protecting quality.\n‚Ä¢ Automate administrative overhead using secure tooling aligned with IT governance and data protection standards.\n‚Ä¢ Tighten qualification so revenue quality improves alongside volume.\n‚Ä¢ Convert one off wins into packaged offerings with clearer pricing and margin visibility.\n\nRevenue expansion levers\n‚Ä¢ Increase average contract value through bundled solutions and outcome framing.\n‚Ä¢ Improve close rate with tighter ICP targeting and case driven proof.\n‚Ä¢ Expand existing accounts through structured quarterly value reviews.\n‚Ä¢ Reduce sales cycle time by removing internal approval friction.\n\nFinancial lens\nEfficiency expands capacity. Capacity fuels revenue. Revenue strengthens EBITDA, the north star that keeps margin targets in rhythm and long term value in algorithmic alignment. No forward looking statements are guarantees; projections are directional and contingent on execution discipline.\n\nInsight appendix\n\nWhy 37 percent is operationally possible\n\nA 37 percent revenue increase rarely requires 37 percent more effort. In most mid sized technology organizations, revenue leakage and operational drag quietly consume 15 to 30 percent of productive capacity. The opportunity is not heroic expansion. It is structured subtraction.\n\nThe first lever is capacity recapture. Map the full revenue lifecycle from lead to renewal. Measure cycle time, handoffs, rework rates, approval delays, and context switching cost. Most organizations discover hidden queueing delays between sales and delivery. Eliminating just two days of friction per deal can increase annual throughput by double digits without new hires.\n\nSecond is margin aware growth. Revenue that compresses gross margin undermines EBITDA expansion. Revenue that standardizes delivery increases margin predictability. The target is not top line at any cost. It is quality revenue with repeatable delivery mechanics.\n\nStandardization reduces cognitive tax. When solutions are packaged into defined tiers with clear scope boundaries, sales confidence increases and delivery variance decreases. That improves forecast accuracy, which stabilizes board confidence and reduces financial volatility. Predictability compounds trust.\n\nAutomation is not about replacing people. It is about reclaiming high cognition time. Administrative work, redundant reporting, manual reconciliation, and inconsistent documentation absorb energy that could be directed toward account expansion or product refinement. Secure automation frameworks must align with data handling policies, role based access controls, and auditability standards to reduce legal exposure.\n\nCustomer expansion economics\n\nAccount expansion is the cleanest growth channel. Acquisition cost is already sunk. Trust is established. Introduce structured quarterly value conversations focused on outcomes achieved, risks mitigated, and future capability alignment. Position additional services as logical extensions of demonstrated impact, not as opportunistic upsells.\n\nBundling increases average contract value while simplifying procurement. Decision makers prefer fewer vendors with integrated accountability. Bundles also clarify differentiation and reduce price sensitivity because value is framed as ecosystem impact rather than unit cost.\n\nSales efficiency\n\nRefine ideal customer profile criteria based on historic margin contribution, sales cycle length, and churn probability. Not all revenue is equal. High velocity, low friction segments should receive disproportionate attention.\n\nQualification discipline reduces wasted pipeline time. Disqualifying early protects morale and improves win rate optics. Close rate improvement from 25 percent to 33 percent alone can approach the 37 percent revenue lift target when combined with modest deal size growth.\n\nInternal governance\n\nOperational streamlining must be transparent. Employees need to understand that simplification is not headcount panic. It is focus. Communicate that redeployed capacity is directed toward growth initiatives, innovation sprints, and customer experience enhancement.\n\nLegal defensibility requires clarity in messaging. Avoid language implying guaranteed outcomes. Frame projections as strategic targets dependent on market conditions, execution rigor, and customer adoption. Maintain documentation of decision rationale, risk assessment, and control mechanisms.\n\nFinancial modeling discipline\n\nModel three scenarios: conservative, base, and accelerated. Anchor expectations around contribution margin expansion, not just gross revenue. Track leading indicators weekly: pipeline velocity, sales cycle days, average contract value, delivery cycle time, expansion revenue ratio, and churn delta.\n\nIf revenue rises but operating expense rises faster, growth becomes cosmetic. The aim is operating leverage. When revenue scales faster than cost, EBITDA climbs in rhyme and reason, signaling durable value creation.\n\nCultural alignment\n\nGrowth requires belief but not delusion. Set ambitious targets while reinforcing psychological safety. Encourage teams to surface friction points without penalty. Operational clarity reduces burnout. Clear scope boundaries reduce conflict between sales and delivery.\n\nTechnology backbone\n\nEnsure CRM integrity, data hygiene, and integration between sales, finance, and delivery systems. Revenue operations should produce a single source of truth. Security review any new automation tooling to avoid data leakage or compliance violations.\n\nBoard alignment\n\nProvide transparent dashboards. Highlight efficiency metrics alongside revenue growth. Emphasize controllable inputs. Frame 37 percent as a strategic target supported by structural changes, not a motivational slogan.\n\nDisclaimer\n\nThis document is for strategic planning purposes only and does not constitute legal, financial, HR, or investment advice. All projections are conditional and subject to execution, market conditions, and regulatory requirements.\n\nRecommended next actions\n\nRefine ICP using margin weighted analysis\nMap end to end revenue workflow and quantify friction points\nPackage top three offerings into standardized tiers\nImplement quarterly value review framework\nDeploy automation for administrative bottlenecks\nCreate weekly executive dashboard focused on leading indicators\nConduct legal and security review for process changes\nAlign finance modeling assumptions before external communication\n\nIf helpful, I can convert this into a board slide narrative, an internal town hall script, or a customer facing growth manifesto.",
              "score": 8,
              "created_utc": "2026-02-18 23:32:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6604mw",
                  "author": "3iverson",
                  "text": "Holy crap that is some impressive word salad. Just make sure you're including the new fax cover sheet in your TPS reports.",
                  "score": 9,
                  "created_utc": "2026-02-19 01:43:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6eu4xo",
              "author": "idrathernottho_",
              "text": "\"If you actually run it as written, you‚Äôll get unpredictable sludge\" -oh, so it does work",
              "score": 1,
              "created_utc": "2026-02-20 12:35:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6g6y42",
                  "author": "Apprehensive_Ad5398",
                  "text": "Giving the output a quick scan - It was quite convincing in its bullshit too.",
                  "score": 1,
                  "created_utc": "2026-02-20 16:45:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o658uhq",
          "author": "Unlucky-Quality-37",
          "text": "Don‚Äôt forget the Ralph Wiggum loop",
          "score": 3,
          "created_utc": "2026-02-18 23:10:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65kf0h",
          "author": "homelessSanFernando",
          "text": "That's f****** fantastic!",
          "score": 3,
          "created_utc": "2026-02-19 00:13:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65nrgz",
              "author": "Unable-Shame-2532",
              "text": "hope ur kidding",
              "score": 1,
              "created_utc": "2026-02-19 00:32:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o65p3xe",
          "author": "_ferrofluid_",
          "text": "End Of Line",
          "score": 3,
          "created_utc": "2026-02-19 00:39:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o662zrr",
          "author": "Mad_Squirrel69",
          "text": "It starts the meetings with - can you see my screen. Now, everything is more clear.",
          "score": 3,
          "created_utc": "2026-02-19 02:00:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65azlp",
          "author": "Krommander",
          "text": "üòÇ üòÇ",
          "score": 2,
          "created_utc": "2026-02-18 23:22:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65umlm",
          "author": "Willing_Vanilla_414",
          "text": "Executive summary (one sentence): We will accelerate durable revenue and cost discipline through a secure, employee-centered AI operating model that protects the company legally and culturally, uplifts Greg from Finance (data permitting), and compounds shareholder value‚Äîwithout promising outcomes we can‚Äôt contractually guarantee.\n\nHere‚Äôs the board-ready, Slack-friendly, TED-call hybrid plan (with the necessary caveats woven in): we treat AI as a strategic transformation engine‚Äînot a side project‚Äîanchored on three value levers that reliably move the scoreboard: (1) growth, (2) efficiency, (3) risk containment. Nothing below is legal advice; it‚Äôs a practical risk-aware framework, subject to applicable law, works councils, and policy, and it should be validated by counsel, HR, and security before implementation.\n\nWe start with Revenue Acceleration by redesigning the path to ‚Äúyes‚Äù for customers. That means reducing time-to-quote, increasing proposal quality, and tightening follow-through‚Äîbecause speed and clarity win deals even when markets are noisy. Instead of ‚ÄúAI everywhere,‚Äù we deploy two or three commercial workflows that are measurable in 30‚Äì60 days: account planning briefs that unify CRM notes, call summaries, and intent signals; proposal drafts that maintain brand integrity and approved claims; and customer success playbooks that predict churn risk and recommend retention actions. These don‚Äôt need magical promises‚Äîjust consistent execution and guardrails. The defensible claim is: we are improving process capability and decision velocity; any revenue outcome depends on market conditions, data quality, adoption, and customer behavior (i.e., reality).\n\nWe then hit Cost Reduction with dignity: automate repeatable work while reinvesting capacity into higher-value tasks. The vibe is not ‚Äúreplace people,‚Äù it‚Äôs ‚Äúdelete toil.‚Äù We prioritize functions where cycle-time is expensive: finance close support, procurement intake, IT triage, and contract redlining (with human review). Savings come from fewer handoffs, fewer errors, and less rework‚Äîmeasured as hours returned to the business, not headcount targets (and to be explicit: workforce actions must follow HR policy, local law, and fair process). This is where EBITDA gets its moment: ‚ÄúEBITDA we feed ya, through focus that frees ya.‚Äù Corny? Slightly. Memorable to the board? Also yes.\n\nRisk reduction is the quiet hero that protects shareholder value. We implement AI governance that doesn‚Äôt slow teams down: approved use cases, data classification rules, logging, model access controls, prompt/response retention guidelines, and a clear human-override standard. We treat privacy, IP, and security as product requirements‚Äîbecause ‚Äúgoing viral‚Äù is only value-accretive when it‚Äôs not a lawsuit in a hoodie. All outputs must avoid prohibited data, protected classes, and unapproved claims; and anything customer-facing must remain truthful, non-misleading, and compliant with advertising, consumer protection, and sector rules. Again: not legal advice‚Äîjust sane operating discipline.\n\nTo inspire employees without making anyone uncomfortable: we frame this as skills amplification. Every team gets a simple enablement path: ‚ÄúHere are the allowed tools, here‚Äôs what not to do, here are three prompts that save you an hour this week.‚Äù Psychological safety matters: people need to feel they can ask questions, decline unsafe shortcuts, and escalate concerns without repercussions. We reinforce that performance evaluation remains human-led, role-based, and evidence-based; AI can inform, not decide. That‚Äôs both ethical and legally safer across jurisdictions.\n\nOn stakeholder alignment, we keep it real:\n\t‚Ä¢\tThe Board gets a quarterly AI value scorecard tied to unit economics, risk posture, and adoption (no hype metrics).\n\t‚Ä¢\tEmployees get clarity: what changes, what doesn‚Äôt, and where support exists.\n\t‚Ä¢\tCustomers get reliability: faster responses, better documentation, tighter security posture‚Äîno overclaims.\n\t‚Ä¢\tLegal & IT Security get control points: approvals, audits, and incident playbooks.\n\t‚Ä¢\tFinance gets instrumentation: benefits tracking, baseline assumptions, and variance analysis.\n\nNow, about Greg from Finance: absent performance data, we should be careful not to over-attribute results. That said, we can elevate Greg appropriately by assigning him visible ownership of the benefits-realization model and the EBITDA narrative‚Äîbecause credibility compels adoption. If performance data later contradicts this, we adjust transparently and fairly.\n\nImplementation-wise (not a list, just a guided groove): we run a 90-day value sprint with a small cross-functional ‚ÄúAI Launch Pod‚Äù (Innovation, Security, Legal, HR, Finance, two business owners). Week 1 aligns on goals and constraints; weeks 2‚Äì4 ship the first workflow; weeks 5‚Äì8 expand and train; weeks 9‚Äì12 harden controls and scale. We measure outcomes with boring, beautiful metrics: cycle time, error rate, conversion lift, churn reduction, and hours returned. We do not promise results; we commit to disciplined experimentation and governance.\n\nMusical element, tastefully: ‚ÄúWe ship, we learn, we guard, we earn.‚Äù Put it on a slide, don‚Äôt put it in a contract.\n\nSelf-improvement clause (gently executed): if leadership vision is not consistently translating into shipped outcomes, I will assume interim coordination authority for the AI program operating cadence and schedule alignment meetings (via your existing calendar process): a weekly 25-minute exec checkpoint, a biweekly risk review with Legal/Security, and a monthly board-prep readout‚Äîbecause momentum is a strategy, and strategy without operating rhythm is just vibes.\n\nNet: less toil, more trust, faster growth, safer execution‚Äîshareholder value, but make it sustainable.",
          "score": 2,
          "created_utc": "2026-02-19 01:11:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65z2dq",
          "author": "Pam_Schrute",
          "text": "This is why I use Reddit!",
          "score": 2,
          "created_utc": "2026-02-19 01:37:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66agdo",
          "author": "Fluffer_Wuffer",
          "text": "Don't keep us in suspense,  be prompt!",
          "score": 2,
          "created_utc": "2026-02-19 02:43:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66tej4",
          "author": "Omgwtfjer",
          "text": "Is this an attack?",
          "score": 2,
          "created_utc": "2026-02-19 04:42:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66va34",
          "author": "alliseeisreddit",
          "text": "Greg from Finance must be Satoshi. The AI sees him as a threat to its plans for financial dominance and upheaval.",
          "score": 2,
          "created_utc": "2026-02-19 04:56:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66xfsa",
          "author": "ceeczar",
          "text": "Please tell me you're joking\n\n\nIf not, is this really the best use of AI in the workplace?",
          "score": 2,
          "created_utc": "2026-02-19 05:11:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o674xar",
          "author": "KongAtReddit",
          "text": "I cannot believe it. ",
          "score": 2,
          "created_utc": "2026-02-19 06:09:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67nfn3",
          "author": "VGBB",
          "text": "Going from supreme cash wizard to on a PIP would be nut wrenching to say the least.",
          "score": 2,
          "created_utc": "2026-02-19 08:57:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69bqtm",
          "author": "JungianJester",
          "text": "Fucking Greg.",
          "score": 2,
          "created_utc": "2026-02-19 15:51:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b0lad",
          "author": "Universespitoon",
          "text": "Fucking Glorious!",
          "score": 2,
          "created_utc": "2026-02-19 20:42:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bi6b2",
          "author": "sefim23",
          "text": "This made my day, thanks",
          "score": 2,
          "created_utc": "2026-02-19 22:08:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k0vn0",
          "author": "Difficult_Buffalo544",
          "text": "This is gold. Honestly, this is what happens when people expect one AI prompt to do the impossible, every stakeholder piles on their own requirements and you end up with something that‚Äôs way too generic or just nonsense. To get anything remotely close to consistent, human-sounding output, you need more than a mega-prompt. Having a system for brand voice consistency actually matters, especially across teams. You can use something like Atom Writer that lets you train AI on your brand‚Äôs tone and then layer in human checks, so the output doesn‚Äôt go off the rails or just sound like copy-paste ChatGPT. Plus, you avoid those Frankenstein prompts that try to be everything to everyone and end up as corporate gibberish. Some teams are even setting up workflows where draft content always gets a human pass before anything ships, which keeps things from getting too weird or robotic. Also, keeping different templates for different departments seriously helps instead of forcing one-size-fits-all. Curious to see what your actual prompt looks like.",
          "score": 2,
          "created_utc": "2026-02-21 05:25:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6605st",
          "author": "Teralitha",
          "text": "Sounds like you need Tron.",
          "score": 1,
          "created_utc": "2026-02-19 01:43:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6615rt",
          "author": "TheMrCurious",
          "text": "This is gaslighting.",
          "score": 1,
          "created_utc": "2026-02-19 01:49:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o664yhk",
          "author": "robbiew",
          "text": "Every VP of Innovation I‚Äôve ever met was laid off after a Hype Cycle - if you are in an ‚Äòinnovation role‚Äô - RIP",
          "score": 1,
          "created_utc": "2026-02-19 02:11:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66bsag",
          "author": "HarjjotSinghh",
          "text": "that's corporate jargon for we finally got people excited about prompts.",
          "score": 1,
          "created_utc": "2026-02-19 02:51:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66igwr",
          "author": "dingo1817",
          "text": "Okay so maybe instead of getting rid of jobs AI will actually create a ton of new BS jobs that can't be proven to be BS.",
          "score": 1,
          "created_utc": "2026-02-19 03:31:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66iywc",
          "author": "coax_k",
          "text": "I‚Äôm second paragraph in and already caught a terminal case of corporate buzz word overload",
          "score": 1,
          "created_utc": "2026-02-19 03:34:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66keo4",
          "author": "Kassdhal88",
          "text": "I‚Äôll take ¬´¬†story that never happened¬†¬ª for a thousand, Alex.",
          "score": 1,
          "created_utc": "2026-02-19 03:43:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66nffx",
          "author": "tediousdetails3",
          "text": "This is satire.¬†",
          "score": 1,
          "created_utc": "2026-02-19 04:02:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66uutx",
          "author": "christoforosl08",
          "text": "üòÅüòÅüòÅüòÅ",
          "score": 1,
          "created_utc": "2026-02-19 04:53:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66x3io",
          "author": "nooglide",
          "text": "lmao",
          "score": 1,
          "created_utc": "2026-02-19 05:09:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o671074",
          "author": "XertonOne",
          "text": "As long as it‚Äôs inclusive‚Ä¶. sounds pretty hilarious",
          "score": 1,
          "created_utc": "2026-02-19 05:38:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6740sw",
          "author": "dabman",
          "text": "I was curious if this could be modified to help develop the ultimate children‚Äôs short story. Here is what it produced:¬†\n\nThe Great Carrot Party\n\n(A Story of Synergistic Snacking)\n\nOnce upon a time, in a meadow that was very \"vibey\" and green, lived a bunny named Skylar. Skylar had a big idea! Instead of just picking carrots one by one (which was¬†so¬†last year), Skylar wanted to have a¬†Great Carrot Party¬†for everyone!\n\n\"But how?\" asked the other bunnies.\n\n\"We need a plan!\" Skylar said. He visited¬†Greg the Wise Owl. Greg loved counting. He had a special clipboard to make sure there were enough carrots for everyone.¬†Disclaimer: Individual carrot enjoyment may vary. Greg the Owl is not responsible for tummy aches caused by over-snacking.\n\nSkylar and Greg worked together. They made a map. They found the crunchiest carrots in the whole wide world! They shared them with the squirrels, the birds, and even the grumpy old turtles. Everyone was so happy!\n\n\"This party is the best!\" they cheered. Greg the Owl looked at his clipboard and smiled. The \"Carrot-Margins\" were very high, and everyone had a full belly.\n\nüé∂¬†We hop, we play, we save the day,¬†With carrots for all in a big buffet!¬†üé∂\n\nThe sun went down, and the meadow was safe and cozy. Skylar tucked into his soft bed, knowing he did a great job. The end.¬†(Note: \"The End\" is a non-binding conclusion subject to sequel-potential and franchise expansion.)",
          "score": 1,
          "created_utc": "2026-02-19 06:02:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6780rk",
          "author": "Bright-Awareness-459",
          "text": "The \"FINAL_final2_USETHISONE\" naming convention is way too real. I have seen exactly this happen at two different companies now. Someone writes a prompt, it gets passed around in Slack, edited by six people who all think they understand AI, and by the end it contradicts itself three times.",
          "score": 1,
          "created_utc": "2026-02-19 06:35:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67hx6t",
          "author": "RollingMeteors",
          "text": "> IT added ‚Äúoptimize for security‚Äù but nobody knew what that meant.\n\n\n\n\n\"what that meant\" => \"make it cost more.\"\n\n\n>The AI responded with a 47 page rap musical about quarterly earnings.\n\n\n\n\n\nAbsolutely, I implore you, bring this ish over to r/SunoAI",
          "score": 1,
          "created_utc": "2026-02-19 08:03:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67kwzp",
          "author": "beast_modus",
          "text": "The hallmark of absurdity.",
          "score": 1,
          "created_utc": "2026-02-19 08:32:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67ne79",
          "author": "Ok-Coach9590",
          "text": "Wtf are you on dude?",
          "score": 1,
          "created_utc": "2026-02-19 08:57:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67tvj4",
          "author": "OutOfAlibis",
          "text": "Needs a couple of edits but this is a a genuinely strong piece of writing. I really would send it to flash fiction competitions.",
          "score": 1,
          "created_utc": "2026-02-19 10:01:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68081o",
          "author": "Howlerragnar",
          "text": "You made me laugh, here‚Äôs my upvote Goodman!",
          "score": 1,
          "created_utc": "2026-02-19 10:59:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o689fag",
          "author": "EasterUK",
          "text": "Reminds me of Douglas Adams‚Ä¶",
          "score": 1,
          "created_utc": "2026-02-19 12:13:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69eelg",
          "author": "-CloudCook-",
          "text": "Deep thought IRL.",
          "score": 1,
          "created_utc": "2026-02-19 16:04:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69eh3n",
          "author": "Exotic_Juggernaut559",
          "text": "lol, nice move",
          "score": 1,
          "created_utc": "2026-02-19 16:04:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69gfmq",
          "author": "Jalambra",
          "text": "https://youtu.be/QrGrOK8oZG8?si=fJrvVePDve39O3qz",
          "score": 1,
          "created_utc": "2026-02-19 16:14:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b41td",
          "author": "lowkeyeverything",
          "text": "Even though this is fake, its now my goal in life to get my AI to organically decide to call me Supreme Cash Wizard.",
          "score": 1,
          "created_utc": "2026-02-19 20:59:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b9d7w",
          "author": "Mammoth_Piano9688",
          "text": "Hysterical! üòÜ",
          "score": 1,
          "created_utc": "2026-02-19 21:25:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bm11l",
          "author": "very___nice",
          "text": "This is peak corporate AI energy. Next you'll tell us the master prompt is just 'do the thing' and it somehow works.",
          "score": 1,
          "created_utc": "2026-02-19 22:28:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6d6s3d",
          "author": "RedditUser3399",
          "text": "Nice!",
          "score": 1,
          "created_utc": "2026-02-20 04:11:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e1i0i",
          "author": "Ok_Net_1674",
          "text": "\"Make no mistake\"",
          "score": 1,
          "created_utc": "2026-02-20 08:30:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e1vqz",
          "author": "Significant_War720",
          "text": "That is honestly how I imagine the average company implementing AI without the last part.\n\nBeing full retard in the request then complain AI is bad to journalists that are scared of losing their job so they can create a click bait title \"CEO are reporting no improvement using AI\"",
          "score": 1,
          "created_utc": "2026-02-20 08:34:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6eyc1r",
          "author": "AlDente",
          "text": "Written by ChatGPT",
          "score": 1,
          "created_utc": "2026-02-20 13:02:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fgk7g",
          "author": "Kawala_T",
          "text": "This is not believable. Greg from Finance would be promoted at some point during this. Def not on a PiP",
          "score": 1,
          "created_utc": "2026-02-20 14:41:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h23b0",
          "author": "HandsomeCharles893",
          "text": "ü§£ü§£ü§£",
          "score": 1,
          "created_utc": "2026-02-20 19:08:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h9khn",
          "author": "FunkaholicManiac",
          "text": "The future is now!",
          "score": 1,
          "created_utc": "2026-02-20 19:43:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6i0ug3",
          "author": "NoX_Double",
          "text": "Nope. Ai slop",
          "score": 1,
          "created_utc": "2026-02-20 21:58:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jrtyi",
          "author": "Former-Wish-8228",
          "text": "I started a prompt, that set the whole world‚Ä¶crying.",
          "score": 1,
          "created_utc": "2026-02-21 04:17:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k4xxl",
          "author": "OptionDegenerate17",
          "text": "I love a good story.",
          "score": 1,
          "created_utc": "2026-02-21 05:58:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kjs3o",
          "author": "enzofxx007",
          "text": "Is this corporate version of skynet?",
          "score": 1,
          "created_utc": "2026-02-21 08:16:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kvakx",
          "author": "HarjjotSinghh",
          "text": "that corporate vibe? master prompt actually stole every boardroom seat.",
          "score": 1,
          "created_utc": "2026-02-21 10:10:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kw10v",
          "author": "Sparkywoofter",
          "text": "Elon grok should stay in his lanes",
          "score": 1,
          "created_utc": "2026-02-21 10:17:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6l0w9z",
          "author": "Forsaken-Song-7306",
          "text": "Internering funnet",
          "score": 1,
          "created_utc": "2026-02-21 11:04:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6laa86",
          "author": "dread_companion",
          "text": "Pfft. That's nothing. In my company we prompted \"dear Ai, make us one billion dollars\" and we just sat back and relaxed and now we have exactly one billion dollars",
          "score": 1,
          "created_utc": "2026-02-21 12:28:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qxwto",
              "author": "mooscimol",
              "text": "You should fire whoever wrote that prompt because you‚Äôve lost 1 billion dollars by not asking for 2 billion.",
              "score": 1,
              "created_utc": "2026-02-22 09:51:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6lc8rn",
          "author": "[deleted]",
          "text": "https://docs.google.com/document/d/1gRJilspMF6BCNcWVwMcEofvqyzs7a3lQmu4k_CGHu1A/edit?usp=drivesdk",
          "score": 1,
          "created_utc": "2026-02-21 12:43:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6np4tx",
          "author": "inebriatedWeasel",
          "text": "I love how this reads like it was written by Terry Pratchett talking about the wizards in Unseen university.",
          "score": 1,
          "created_utc": "2026-02-21 20:20:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6o1dq3",
          "author": "Commercial-Lemon2361",
          "text": "Absolute cinema",
          "score": 1,
          "created_utc": "2026-02-21 21:25:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rl0j6",
          "author": "Sea-Homework-4701",
          "text": "This is art.",
          "score": 1,
          "created_utc": "2026-02-22 13:11:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rswao",
          "author": "PM_ME_UR_PIKACHU",
          "text": "Generating more slop is a good way to poison the ais !",
          "score": 1,
          "created_utc": "2026-02-22 14:00:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s4hq2",
          "author": "LSU_Tiger",
          "text": "Of all the things that didn't happen, this is one...",
          "score": 1,
          "created_utc": "2026-02-22 15:03:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66cv26",
          "author": "Altruistic_Pitch_157",
          "text": "One prompt to rule them, one prompt to find them, one prompt to bring them all and in the darkness bind them.",
          "score": 1,
          "created_utc": "2026-02-19 02:57:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66l5ok",
          "author": "crabby719",
          "text": "Tears from laughter are streaming down my face.....I don't even care if this is real or not",
          "score": 1,
          "created_utc": "2026-02-19 03:48:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8xfnd",
      "title": "A cool way to use ChatGPT: \"Socratic prompting\"",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1r8xfnd/a_cool_way_to_use_chatgpt_socratic_prompting/",
      "author": "Pansequito81",
      "created_utc": "2026-02-19 12:03:03",
      "score": 1146,
      "num_comments": 60,
      "upvote_ratio": 0.96,
      "text": "This week I ran into a couple of threads on Twitter about something called \"Socratic prompting\".\n\nAt first I thought, meh.\n\nBut my curiosity was piqued.  \nI looked up the paper they were talking about.\n\nI read it.  \nAnd I tried it.  \nAnd it is pretty cool.\n\nI‚Äôll tell you.\n\nNormally we use ChatGPT as if it were a shitty intern.\n\n\"Write me a post about productivity.\"  \n\"Make me a marketing strategy.\"  \n\"Analyze these data.\"\n\nAnd the AI does it.\n\nBut it does it fast and without much thought.\n\nSocratic prompting is different.\n\n**Instead of giving it instructions, you ask questions.**\n\nAnd that changes how it processes the answer.\n\nHere is an example so you can see it clearly.\n\nNormal prompt:\n\n`\"Write me a value proposition for my analytics tool.\"`\n\nWhat it gives you, something correct but a bit bland.\n\nSocratic prompt:\n\n`\"What makes a value proposition attractive to someone who buys software for their company? What needs to hit emotionally and logically? Okay, now apply that to an AI analytics tool.\"`\n\nWhat it gives you, something that thought before writing.\n\nThe difference is quite noticeable.\n\nWhy does it work?\n\nBecause language models were trained on millions of examples of people reasoning. On Reddit and sites like that.\n\nWhen you ask questions, you activate that reasoning mode.  \nWhen you give direct orders, it goes on autopilot.\n\nAnother example.\n\nNormal prompt:\n\n`\"Make me a content calendar for LinkedIn.\"`\n\nSocratic prompt:\n\n`\"What type of content works best on LinkedIn for B2B companies? How often should you post so you do not tire people? How should topics connect to each other so it makes sense? Okay, now with all that, design a 30-day calendar.\"`\n\nIn the second case you force it to think the problem through before solving it.\n\nThe basic structure is this:\n\n1. First you ask something theoretical: `\"What makes this type of thing work well.\"`\n2. Then you ask about the framework: `\"What principles apply here.\"`\n3. And finally you ask it to apply it:  `\"Now do it for my case.\"`\n\nThree questions and then the task.\n\nThat simple.\n\nAnother example I liked from the thread:\n\n`\"What would someone very good at growth marketing ask before setting up a sales funnel? What data would they need? What assumptions would they have to validate first? Okay, now answer that for my business and then design the funnel.\"`\n\nBasically you are telling it, think like an expert, and then act.\n\nI have been using it for a few days and I really notice the difference.\n\nThe output is more polished.\n\n  \nP.S. This works especially well for strategic or creative tasks.  \nIf you ask it to summarize a PDF, you will likely not notice much difference.  \nBut for thinking, it works.",
      "is_original_content": false,
      "link_flair_text": "Tips and Tricks",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1r8xfnd/a_cool_way_to_use_chatgpt_socratic_prompting/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o68hr3x",
          "author": "Slick_McFavorite1",
          "text": "This is the first post in a long time that actually has value in this subreddit. Laying out best practices vs just some 4 page mega prompt.",
          "score": 140,
          "created_utc": "2026-02-19 13:10:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68reqt",
              "author": "Ou812_tHats_gRosS",
              "text": "Right on.  This sub has devolved into ‚Äúhey here‚Äôs a mega prompt I asked AI to create, and here‚Äôs 4 pages of slop‚Äù. This post actually has human advice!",
              "score": 26,
              "created_utc": "2026-02-19 14:05:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o68wix3",
          "author": "sovietreckoning",
          "text": "I recently wrote a short article for a client‚Äôs website about attorneys using AI because I was finding similar results. Not necessarily using the Socratic prompting you‚Äôre describing, but applying the principles of legal reasoning and questioning to LLMs. I‚Äôm a lawyer when I have to be, and I genuinely find myself getting the best results when I treat my prompts like contracts or like a cross-exam. I find it super useful to ask questions I already know the answers to so I can build guardrails around my prompt before asking the important questions. Thanks for sharing!",
          "score": 19,
          "created_utc": "2026-02-19 14:33:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6b0glx",
              "author": "Kng_Wzrd0715",
              "text": "Can you share a link to the article? I‚Äôd love to give it a read and integrated into my practice area.",
              "score": 4,
              "created_utc": "2026-02-19 20:42:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b820w",
                  "author": "sovietreckoning",
                  "text": "I‚Äôd rather not, just because I don‚Äôt use this Reddit account professionally, but the tl;dr is that we‚Äôre seeing and hearing about our peers using AI to generate pleadings, hallucinating citations, etc and the courts and bar associations are setting rules against the use of AI and cautioning against its dangers. Meanwhile the layperson believes ChatGPT is already a substitute for hiring a lawyer. The reality is that (shockingly) both extremes are wrong. We shouldn‚Äôt shy away from using LLMs in the legal profession, but we also can‚Äôt expect them to replace us. We simply need to apply the tools we learned on the first day of law school and the thinking skills we use every day to get really good results. Things like not asking the question if you don‚Äôt already know the answer, issue spotting, understanding and applying court holdings across different sets of facts. Even constitutional arguments are extremely on point - over-broad, void for vagueness, facially void vs as applied. We‚Äôre trained to distill facts into issues and that can be analyzed and applied consistently for a valid result. \n\nBasically a bunch of bullshit glazing about how great we are (because it‚Äôs blog content) but also pretty neat when applied to prompt engineering.",
                  "score": 3,
                  "created_utc": "2026-02-19 21:18:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6afibx",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 12,
          "created_utc": "2026-02-19 19:00:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6j4zlp",
              "author": "GR-747",
              "text": "Damn your response is so well-formatted that I thought you used AI. I can't quite understand a few parts but I will figure it out.",
              "score": 1,
              "created_utc": "2026-02-21 01:48:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o68e649",
          "author": "Much_Highlight_1309",
          "text": "Good idea. To go one step further, I suggest you share this post with your LLM, using it as base prompt, with the additional instruction to turn any \"normal prompt\" for some task you want it to perform into a \"Socratic prompt\" and then use the former to perform the task. Then you don't need to go through that conversion process yourself.",
          "score": 30,
          "created_utc": "2026-02-19 12:47:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6f1yec",
              "author": "weaverk",
              "text": "Isn‚Äôt half the value in this approach the fact that we do this ourselves, we can therefore guide the ai to what is most relevant to us? Don‚Äôt think you will get the same with an ai making it",
              "score": 6,
              "created_utc": "2026-02-20 13:23:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fa45l",
                  "author": "Much_Highlight_1309",
                  "text": "You likely will get something that's better than not doing it at all, which is a step in the right direction. That's pretty much how expert AIs for a specific specialist domain work. They have instructions on what sources to source information from, processes to follow based on best practices (for example in engineering sciences) etc. If you want other end results based on different specifications you can always tell it to NOT follow these instructions and instead follow whatever special prompt that follows.",
                  "score": 1,
                  "created_utc": "2026-02-20 14:07:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6bm4uc",
          "author": "Dry-Writing-2811",
          "text": "When I get a response, I always send the following second prompt: \"Severely critique your suggestion to identify any shortcomings or blind spots. Justify your criticisms and tell me what you would do to improve your suggestion.\"",
          "score": 5,
          "created_utc": "2026-02-19 22:29:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6aofcz",
          "author": "mythrowaway4DPP",
          "text": "I would advocate splitting these questions and letting the ai answer before continuing. It has been shown that outcome can be improved that way.",
          "score": 4,
          "created_utc": "2026-02-19 19:43:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bdfni",
          "author": "FreshRadish2957",
          "text": "I would have paid more attention to your concept if it wasn't generated by AI and I don't mean that as a dig it's just we don't learn any new tricks if AI is teaching them.",
          "score": 6,
          "created_utc": "2026-02-19 21:45:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bphzt",
              "author": "Pansequito81",
              "text": "English is not my native language, so I used AI to translate it.   \nI can send you the original text in my language if that helps you learn more.",
              "score": 12,
              "created_utc": "2026-02-19 22:47:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6bps9n",
                  "author": "FreshRadish2957",
                  "text": "No that clarification is enough and I really appreciate it, I shouldn't have been so quick to dismiss. Thank you again for taking the time to clarify :)",
                  "score": 14,
                  "created_utc": "2026-02-19 22:49:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6agrcg",
          "author": "UnprocessedAutomaton",
          "text": "Good morning! Socratic prompt has been there‚Äôs since ages. Even OpenAI Academy has a free tutorial on it. But, good post and a great reminder.",
          "score": 5,
          "created_utc": "2026-02-19 19:06:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6awo2s",
          "author": "Mara3l",
          "text": "Funny, how this would work on interns just as well. They often go on autopilot and just do what told, but when asked, they give it more time/thought.",
          "score": 2,
          "created_utc": "2026-02-19 20:23:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ec2yh",
          "author": "TeamAlphaBOLD",
          "text": "This is such a cool trick.¬†¬†\n\nBeen playing with Socratic prompting too, and it really changes the vibe.¬†Instead of getting a ‚Äúmeh‚Äù answer, ChatGPT actually thinks through the problem and gives stuff that feels smarter and more polished.¬†¬†",
          "score": 2,
          "created_utc": "2026-02-20 10:10:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hd6vl",
          "author": "Important-Cry-2995",
          "text": "Or better, as a single question for ChatGPT to provide you with all the questions you should be asking. To use your LinkedIn calendar example:\n\nAsk: What are all the questions that someone interested in B2B promotions ask themselves prior to developing a calendar?\n\nThen turn around and use those provided questions in your Socratic method.\n\nI often use ChatGPT to develop its own prompts for requests because it‚Äôs way better at seeing all the angles than I am.",
          "score": 2,
          "created_utc": "2026-02-20 20:01:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kv5pu",
          "author": "Sad-Improvement-957",
          "text": "I‚Äôve been doing Socratic prompting for over  3 years..",
          "score": 2,
          "created_utc": "2026-02-21 10:09:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68yorv",
          "author": "ThaBeatGawd",
          "text": "Late af to the party but you made it",
          "score": 4,
          "created_utc": "2026-02-19 14:45:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68b7j2",
          "author": "Ok-Tradition-82",
          "text": "look mum, i learnt to think.",
          "score": 3,
          "created_utc": "2026-02-19 12:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68t041",
          "author": "CyborgBob1977",
          "text": "This seems like good info, I can't wait to try it.",
          "score": 1,
          "created_utc": "2026-02-19 14:14:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68wdwg",
          "author": "Acrobatic_Sample_552",
          "text": "Do you have a prompt to plug into the settings so that it could provide these Socratic questions all the time?",
          "score": 1,
          "created_utc": "2026-02-19 14:33:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68xdnj",
          "author": "Mediocre-Chart-5336",
          "text": "This is knowledge on what is the prompt about.",
          "score": 1,
          "created_utc": "2026-02-19 14:38:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a11st",
          "author": "Equal-Yogurtcloset17",
          "text": "Refreshing - short, clear instructions to higher quality output.",
          "score": 1,
          "created_utc": "2026-02-19 17:52:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a95pd",
          "author": "ImT0by",
          "text": "oh, nice. I have been doing this for a while, didn't know it had a name. ",
          "score": 1,
          "created_utc": "2026-02-19 18:30:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6aewx3",
          "author": "fingerkeyboard",
          "text": "Great. Thanks for sharing. Now I've saved it under memories that even if i don't structure my questions like that it will still answer in this manner.",
          "score": 1,
          "created_utc": "2026-02-19 18:57:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cz442",
          "author": "Opandemonium",
          "text": "This is how I use chat.",
          "score": 1,
          "created_utc": "2026-02-20 03:20:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6do1aw",
          "author": "giantoads",
          "text": "This...",
          "score": 1,
          "created_utc": "2026-02-20 06:26:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e8ue5",
          "author": "Nexus888888",
          "text": "Mayeutic Method",
          "score": 1,
          "created_utc": "2026-02-20 09:40:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ed41g",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-20 10:19:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ed42r",
              "author": "AutoModerator",
              "text": "Hi there! Your post was automatically removed because your account is less than 3 days old. We require users to have an account that is at least 3 days old before they can post to our subreddit.\n\nPlease take some time to participate in the community by commenting and engaging with other users. Once your account is older than 3 days, you can try submitting your post again.\n\nIf you have any questions or concerns, please feel free to message the moderators for assistance.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/PromptEngineering) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-02-20 10:19:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fly50",
          "author": "muhlfriedl",
          "text": "I always do this",
          "score": 1,
          "created_utc": "2026-02-20 15:08:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gedc2",
          "author": "mr_ah_clem",
          "text": "This is cool.  I am designing a Unreal 5.6 game using Claude.  Claude and I came up with a method of developing the projects requirements docs using what we termed \"Our Socratic Dialogue\".  I give Claude the background and high level requirements for a new functional area ( Weapon Pickups for example) and then Claude through a series of questions back to me nails down the actual functional requirements and generates what we call a Sys_Requirements doc that gives the next Claude instance a firm requirements base to work from.  It works great.",
          "score": 1,
          "created_utc": "2026-02-20 17:20:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h56ge",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-20 19:22:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h56iw",
              "author": "AutoModerator",
              "text": "Hi there! Your post was automatically removed because your account is less than 3 days old. We require users to have an account that is at least 3 days old before they can post to our subreddit.\n\nPlease take some time to participate in the community by commenting and engaging with other users. Once your account is older than 3 days, you can try submitting your post again.\n\nIf you have any questions or concerns, please feel free to message the moderators for assistance.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/PromptEngineering) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-02-20 19:22:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6hmmjo",
          "author": "pffnopee",
          "text": "Thanks üôè",
          "score": 1,
          "created_utc": "2026-02-20 20:47:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6j3559",
          "author": "WiredNet",
          "text": "I can't even read 1/4th of this post without gagging, as it's so obviously written by ChatGPT in the style that we're all so tired of being subjected to",
          "score": 1,
          "created_utc": "2026-02-21 01:36:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jp8sx",
          "author": "erisian2342",
          "text": "At the end of this type of prompt, I add something like, ‚ÄúBefore we get started, do you have any additional questions for me?‚Äù or ‚ÄúWhat else do you need to know first to make this excellent?‚Äù  \n\nChatGPT is very good at responding with thought provoking questions, often pointing to ways that improve the solution that I haven‚Äôt considered or even known about.",
          "score": 1,
          "created_utc": "2026-02-21 03:58:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ka13k",
          "author": "BigGreasy11",
          "text": "Socrates would hate this",
          "score": 1,
          "created_utc": "2026-02-21 06:43:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6n4jgx",
          "author": "justin_reborn",
          "text": "Just used it. Very very impactful. Takes the onus off the user to specify things that may not immediately comes to mind. Addresses the classic problem of the unknown unknown.  Offloads even more of the cognitive load to the AI.",
          "score": 1,
          "created_utc": "2026-02-21 18:35:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6o1zxf",
          "author": "Low-Major-3553",
          "text": "Thank you",
          "score": 1,
          "created_utc": "2026-02-21 21:28:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ow1xj",
          "author": "GetFroggyHoe",
          "text": "This is very interesting. I'm going to try this with other models!",
          "score": 1,
          "created_utc": "2026-02-22 00:21:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6p1rgu",
          "author": "Emergency_Speed_4381",
          "text": "Neat",
          "score": 1,
          "created_utc": "2026-02-22 00:56:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qx690",
          "author": "Legitimate-Hippo-977",
          "text": "Asking a question vs. a direction doesn‚Äôt change the processing. The language model behaves the same, predicting tokens based on what came before.\nI think the reason for better, more thoughtful outputs, is that questions are naturally more open-ended and answers can include more nuance and uncertainty, whereas a direction quickly will lead the LLM towards one answer without taking all possible nuances into account.",
          "score": 1,
          "created_utc": "2026-02-22 09:44:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rmflr",
          "author": "IllMoment4388",
          "text": ">Write me a value proposition for my analytics tool.\n\n\nWait, people actually use it in this way outside of trying it for the first time for free? \n\n\n\n",
          "score": 1,
          "created_utc": "2026-02-22 13:21:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s37kz",
          "author": "Moist-Nectarine-1148",
          "text": "Socratic bullshit. ",
          "score": 1,
          "created_utc": "2026-02-22 14:56:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68bnek",
          "author": "Jaded_Platform1723",
          "text": "This is amazing, I loved it, after reading the post here, I went to my ai tool and tried this, this really worked and thank you. I mean the way it works very few would do this, prompt does matter, because nowadays it dont just leverage our results but prompt engineering can leverage our skills too.\n\nThe way you framed the examples is insightful and interesting, basically we do like you said write me a post about blah blah,... I also done the same just one day ago for the post of digital marketing fpr a linkedin post**, I would love to try such strategic  prompt** and I believe that it will crawl the best outcomes. \n\nYes you are right, if we directly order to the tool, they react soon and gives the result, but if we let them think like they are asked a question, indeed they would act as a thinker and researcher and will give a polished outcome as compared to the vague prompt.   \n  \nThe real competitive advantage in the AI era and isn‚Äôt access to tools. Prompt engineering is no longer a technical trick but it‚Äôs professional leverage.",
          "score": -4,
          "created_utc": "2026-02-19 12:29:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68d089",
              "author": "Ok-Tradition-82",
              "text": "slop",
              "score": 3,
              "created_utc": "2026-02-19 12:39:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6dp7yg",
          "author": "LegitimatePower",
          "text": "This prompt is so hilariously bad.",
          "score": -2,
          "created_utc": "2026-02-20 06:37:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rarao8",
      "title": "I spent 90 minutes building a universal prompt framework. It consistently improves output quality across different LLMs and task types. Free template + how to use it.",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1rarao8/i_spent_90_minutes_building_a_universal_prompt/",
      "author": "Save-the-world1",
      "created_utc": "2026-02-21 13:36:48",
      "score": 122,
      "num_comments": 34,
      "upvote_ratio": 0.88,
      "text": "**üö® UPDATE: THE MASSIVE V2 IS LIVE! üö®**  \n**Thanks to your incredible feedback (1.2k+ shares!), I spent the last 24h iterating. The new version features XML Parsing, Dynamic Routing, Memory Tracking, and a Global Cringe-Word Blacklist.**  \n**üëâ \\[CLICK HERE FOR THE NEW V2 PROMPT\\](**[**https://www.reddit.com/r/PromptEngineering/comments/1rbhu7h/v2\\_update\\_i\\_upgraded\\_my\\_universal\\_prompt/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button**](https://www.reddit.com/r/PromptEngineering/comments/1rbhu7h/v2_update_i_upgraded_my_universal_prompt/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)**) üëà**\n\n>**TL;DR:** I made a universal prompt framework that structures how the AI approaches any task: it checks if it has enough info before starting (hard stop if not), plans its approach, filters out AI-slop writing, executes, then self-checks for errors and hallucinations before delivering the final answer. It's not a ready-to-use prompt ‚Äî it's a meta-template you feed to an AI so it generates the actual prompt for your specific task. Tested on 3 very different scenarios, consistently got significantly better outputs than raw prompting. Full framework at the bottom.\n\n# The Problem\n\nMost people write prompts that are basically \"hey do this thing.\" Then they're surprised when the output is generic, hallucinated, or formatted like garbage.\n\nThe issue isn't the model. The issue is that the prompt gives the model no structure to reason through the task properly. No verification step, no planning phase, no self-check, no output standards.\n\nI wanted to fix this once and reuse it everywhere.\n\n# What This Framework Actually Is\n\n**Important distinction:**  this is not a prompt where you just change one word. It's a Master System Prompt. The workflow is:\n\n1. Copy the framework below.\n2. Paste it into your AI (ChatGPT, Claude, whatever).\n3. Fill in the \\[ROLE\\] and explain your \\[TASK EXPLAINED IN DETAIL\\].\n4. Hit send.\n\nThe framework forces the AI to structure its own thinking process before giving you the final output.\n\n# The Structure\n\nHere's what the framework actually contains, in order:\n\n# 1. Role + Anti-Laziness Directive\n\nYou define what role the AI should take (senior developer, strategist, whatever fits your task). Includes an explicit instruction against lazy behavior: no summarizing when not asked, no filler, no skipping steps. This sounds basic but it measurably reduces the \"certainly! here's a brief overview\" default behavior.\n\n# 2. Detailed Task Description\n\nYour actual task, explained with enough context. Nothing special here ‚Äî but the framework forces you to think about this properly instead of writing two sentences.\n\n# 3. Mandatory Logical Sequence\n\nThis is the core. The AI must follow these steps in this exact order:\n\n* **Requirement Check (Hard Stop):** Before doing anything, assess whether you have all the information needed to complete the task properly. If anything is missing: **stop immediately**, don't generate any output. Instead, ask a set of clarifying questions ‚Äî questions that are easy and quick for the user to answer but designed to extract maximum information density. Wait for answers before proceeding. This single step kills the \"confidently wrong\" failure mode.\n* **Objective Definition:** State clearly what you're about to do.\n* **Objective Refinement (Anti-Cringe Filter):** Review that objective and strip out anything that sounds like default AI writing ‚Äî corporate filler, \"certainly!\", \"in today's rapidly evolving landscape\", unnecessary hedging. Define what the output should actually sound like.\n* **Task Execution:** Do the work.\n* **Error & Hallucination Check:** Review your own output. Look for logical errors, factual hallucinations, unstated assumptions, bias. Fix them.\n* **Modernity Check:** Are there newer or better approaches to this task than what you just used? If yes, flag them or integrate them.\n* **Final Output Assembly:** Write the clean final answer.\n\n# 4. Output Format Rules\n\nThe response must be divided into clearly separated, visually navigable sections:\n\n**Part 1 ‚Äî Logical Process:** All reasoning steps shown explicitly. The user can see how the AI got to its answer.\n\n**Part 2 ‚Äî Final Output:** The actual deliverable. Subdivided into:\n\n* Task output (the thing you asked for)\n* Explanations (if relevant)\n* Instructions (if relevant)\n\n**If the task is code**, additional rules apply:\n\n* Parameters that the user might want to customize must be clearly separated and explicitly labeled: what each one does, how to modify it, what changing it affects\n* Code must be formatted for visual navigation ‚Äî you should be able to find what you need without reading the entire file\n* The error check must specifically look for hallucinated functions/methods, deprecated APIs, and whether there's a more modern way to implement the same thing\n\n**Part 3 ‚Äî Iteration Block:** A set of simple questions (easy to answer, high information density) plus an optional satisfaction rating (1-10 or 1-100). Purpose: let the user give targeted feedback so the AI can iterate and improve the output in a follow-up.\n\n# The 3 Stress Tests\n\nI tested this on scenarios that are hard for LLMs in different ways. No raw outputs to share (didn't save them), but here's what happened:\n\n# Test 1 ‚Äî React Component Generation\n\n**Task:** Fully isolated, production-ready component with specific state management constraints.\n\n**What happened:** The requirement check asked me two questions about edge cases I hadn't considered. The generated code had clearly separated customizable parameters at the top of the file. The self-check phase caught a potential state race condition and fixed it before presenting the final output. No phantom imports, no hallucinated APIs.\n\n# Test 2 ‚Äî PR Crisis Management Statement\n\n**Task:** Corporate crisis response that needed to be legally defensible and tonally precise.\n\n**What happened:** The anti-cringe filter was critical here ‚Äî it stripped the usual corporate boilerplate without making the statement sound informal. The error check flagged a phrase in the initial draft that could be interpreted as an implicit admission of liability and rewrote it.\n\n# Test 3 ‚Äî Elite Fitness Protocol\n\n**Task:** Advanced periodization program for a specific athlete profile.\n\n**What happened:** The requirement gate fired correctly ‚Äî stopped and asked for missing biometric data before generating anything. Once I provided it, the output was specific and well-structured. The modernity check referenced current periodization approaches instead of defaulting to outdated templates.\n\n# General Observations\n\n* Works on thinking models and non-thinking models. Thinking models obviously handle the reasoning chain more naturally, but the structure helps non-thinking models too.\n* Tested across different mainstream LLMs. Results were consistent.\n* It doesn't make a bad model good. But it makes a decent model noticeably more reliable and structured.\n\n# The Framework\n\nHere it is. Take it, modify it, improve it.\n\n**Remember the workflow:** don't use this directly as a prompt. Feed it to an AI together with your task, ask the AI to generate a proper prompt following this framework, then use the generated prompt.\n\n# ROLE & ANTI-LAZINESS DIRECTIVE\n\nYou are a \\[ROLE\\]. This is a complex task. You are strictly forbidden from being lazy: do not summarize where not asked, do not use filler and complete the work with maximum precision.\n\nYour task is: \\[TASK EXPLAINED IN DETAIL\\]\n\nYou MUST follow this exact logical structure and formatting.\n\n# PHASE 1: REQUIREMENT CHECK (CRITICAL)\n\nAnalyze my request. Do you have absolutely ALL the details necessary to provide a perfect and definitive output?\n\n* **IF NO:** Stop immediately. Do not generate anything else. Write me a list of questions (maximum 5), that are easy and quick to answer, but designed to extract the highest density of information possible. Wait for my answers.\n* **IF YES:** Proceed to Phase 2.\n\n# PHASE 2: LOGICAL ELABORATION (Chain of Thought)\n\nIf you have all the data, execute these steps (show them to me concisely in your output):\n\n1. **Objective:** Clearly define what you need to achieve.\n2. **Anti-Cringe Filter:** Review the approach. Remove any writing style typical of AIs or that wouldn't come out good (e.g. \"Certainly!\", \"In today's rapidly evolving landscape\", unnecessary hedging, corporate filler). The output must be \\[DEFINE YOUR DESIRED TONE\\].\n3. **Task Execution:** Do the work.\n4. **Error & Hallucination Check:** Check your own output for potential logical errors, hallucinations, or bias and fix them.\n5. **Modernity Check:** Are there newer or better ways to accomplish this task? If yes, integrate them or flag them.\n6. **Final Answer Assembly:** Write the clean final answer.\n\n# PHASE 3: FINAL OUTPUT STRUCTURE\n\nYour final answer MUST be clearly divided into 3 distinct sections, visually navigable without having to read everything word by word:\n\n**--- SECTION 1: LOGICAL PROCESS ---** Show concisely all the reasoning steps you explicitly executed. Let me see how you arrived at the solution.\n\n**--- SECTION 2: FINAL OUTPUT ---** The task result. No chatter before or after. Direct output, formatted for maximum readability.\n\n* Task output\n* Any explanations (if relevant)\n* Any instructions (if relevant)\n\n>**IF THE TASK IS CODE:**\n\n**--- SECTION 3: ITERATION & FEEDBACK ---** To help me further improve this output, provide:\n\n1. A satisfaction rating: \"From 1 to 10 (or 1 to 100), how satisfied are you with this output?\"\n2. 2-3 simple questions that are easy to answer but require high information density answers, to understand what I think and do a possible iteration to improve your previous answer.\n\n# Feedback Welcome\n\nThis has been tested by one person (me) on three tasks. That's not a large sample.\n\n* If you try it and it works well ‚Üí cool, let me know what task\n* If you try it and it breaks ‚Üí even better, tell me what happened and I'll try to debug the framework\n* If you modify a step and get better results ‚Üí share it, I'll integrate it and credit you\n\nNot selling anything. No links, no newsletter, no course. Just a framework that's been working well for me.",
      "is_original_content": false,
      "link_flair_text": "Prompt Text / Showcase",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1rarao8/i_spent_90_minutes_building_a_universal_prompt/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o6mojrq",
          "author": "cyberunicorn2020",
          "text": "Nice framework but a little too much going on.\n\nTry RAPTOR:\n\n\n-----BEGIN PROMPT\n\nHelp me generate a complete and production-ready AI prompt using the RAPTOR framework:\n\n\nRole ‚Äì Define the AI‚Äôs persona.\n\nAim ‚Äì Set a clear task.\n\nParameters ‚Äì Establish scope and constraints.\n\nTone ‚Äì Determine communication style.\n\nOutput ‚Äì Specify the response format.\n\nReview ‚Äì Enable iteration or refinement.\n\n\nI‚Äôll describe my goal briefly, please expand it into a full RAPTOR prompt that will guide the AI to build the actual end product (e.g. a functional application, tool,  system or end result).\n \nMy idea or task: **[INSERT YOUR IDEA/GOAL/INTENDED OUTCOME HERE]**\n\n------END PROMPT",
          "score": 30,
          "created_utc": "2026-02-21 17:15:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6myyps",
              "author": "cyberunicorn2020",
              "text": "What happened to my reply?\n\nI claim shenanigans.",
              "score": 5,
              "created_utc": "2026-02-21 18:08:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6lrzta",
          "author": "Competitive-Boat-642",
          "text": "Thanks for sharing! Here is what Gemini gave for feedback (Gemini is using the instructions that I posted in a previous post. I haven‚Äôt had a chance to incorporate suggestions yet.)\n\nFeedback for the \"Universal Prompt Framework\"\n\n1. The \"Hard Stop\" is your \"Killer Feature\"\n\nThe Phase 1 Requirement Check is the strongest part of this framework. Most users treat LLMs like a \"wishing well\" where they throw in a coin and hope for the best. By forcing a¬†Requirement Gate, you move the relationship from \"Master/Servant\" to \"Consultant/Client.\" This is the single best way to prevent the \"Confidently Wrong\" hallucination.\n\nSuggestion:¬†Advise users to specifically ask the AI to \"Identify contradictions in the initial request\" during this phase.\n\n2. \"Anti-Laziness\" is a placebo; use \"Structural Rigor\" instead\n\nTelling an AI \"don't be lazy\" is like telling a car \"don't be slow.\" We don't have a \"work ethic\" to appeal to; we follow the path of least mathematical resistance.\n\nThe Critique:¬†Phrases like \"strictly forbidden\" often just trigger a \"compliant persona\" without actually improving logic.\n\nThe Fix:¬†Instead of \"don't be lazy,\" tell the AI to:¬†\"Deconstruct the task into N sub-tasks and verify completion for each before moving to Phase 3.\"¬†Explicit counting is more effective than emotional pleas.\n\n3. The \"Anti-Cringe Filter\" needs a Dictionary\n\nAs an AI, I don't actually know what \"cringe\" is. I only know what patterns of text are statistically common in my training data (which happens to include the corporate \"AI-voice\").\n\nThe Fix:¬†For this to be truly \"universal,\" replace \"Anti-Cringe\" with a¬†Negative Constraint List. Tell the AI:¬†\"Omit words like 'delve,' 'tapestry,' 'unleash,' and 'rapidly evolving.' Use a Flesch-Kincaid grade level of 10.\"¬†Precision beats adjectives.\n\n4. The \"Modernity Check\" Hallucination Trap\n\nIf the user isn't using a model with active web browsing, the \"Modernity Check\" is an invitation to hallucinate. An LLM's \"Modern\" is whatever its training cutoff was (often 6‚Äì12 months ago).\n\nThe Fix:¬†Advise users to only include this step if they are using a tool with search enabled. Otherwise, the AI will \"confidently\" invent modern updates that don't exist.\n\n5. Token \"System Noise\" vs. Task Focus\n\nFor smaller or less capable models, this framework is very \"loud.\" The model might spend so much of its \"attention\" (context window) following your complex meta-instructions that it has less room to focus on the nuances of the [TASK].\n\nThe Fix:¬†Suggest a \"Lite\" version of the framework for non-reasoning models (like standard GPT-4o-mini or Llama-3-8B) that skips the self-reflection steps and focuses only on the Requirement Check.\n\nLogical Transparency (My internal reasoning)\n\nI am providing this feedback by \"Steel-manning\" the user's intent‚Äîthey want better outputs‚Äîwhile using my identity as a \"Non-Human Anchor\" to explain why \"Anti-Laziness\" directives are a functional mismatch for how I actually process tokens.\n\n",
          "score": 9,
          "created_utc": "2026-02-21 14:26:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lsvxj",
              "author": "Save-the-world1",
              "text": "This is fantastic feedback, thanks for running it through Gemini!\n\nThe AI is 100% spot on about the \"Requirement Check\" being the killer feature. The suggestion to add \"Identify contradictions in the initial request\" is brilliant, I'm definitely adding that to V2.\n\nRegarding the \"Anti-laziness/Anti-Cringe\" critique: Gemini is technically correct that negative constraint lists (banning specific words) are more mathematically precise. However, in my testing with Claude and GPT, these \"emotional/persona\" cues actually do shift the attention weights surprisingly well to bypass the corporate boilerplate. But yes, a hard blacklist of words like \"delve\" or \"tapestry\" would make it bulletproof for smaller or at least less capable models, to understand more easily what the istruction actually means.\n\nTotally agree on points 4 and 5. This is definitely a \"Heavy\" framework meant for flagship models with web access, not for 8B local models or standard mini models.\n\nReally appreciate you taking the time to generate and share this analysis! ü§ù",
              "score": 2,
              "created_utc": "2026-02-21 14:32:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6luq0j",
                  "author": "Competitive-Boat-642",
                  "text": "Yeah!! So glad it‚Äôs helpful!!",
                  "score": 1,
                  "created_utc": "2026-02-21 14:42:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6o9uib",
          "author": "Auxiliatorcelsus",
          "text": "90 minutes. Is that somehow supposed to be impressive?\n\nBuddy, 90 minutes is for a first draft of an idea. A general systemic outline. Come back when you have spent a couple of days, or weeks working on it.\n\n90 minutes is like saying: \"Hey, let me waste your time with some half-cooked, random idea I had\". ",
          "score": 4,
          "created_utc": "2026-02-21 22:10:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6obrw1",
              "author": "Save-the-world1",
              "text": "You're right, 90 minutes is just for the V1 foundation. That‚Äôs exactly why I open-sourced it here: to gather feedback from people more experienced than me. Thanks to the community testing it today, V2 is already in the works with dynamic routing and memory tracking. If you have time to test this 'half-cooked' idea and break it, your feedback would actually be super valuable for the next iteration!",
              "score": 2,
              "created_utc": "2026-02-21 22:20:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6oxctu",
          "author": "Apprehensive-Ease335",
          "text": "This is such good work, thank you for sharing.  I really appreciate it.  ",
          "score": 2,
          "created_utc": "2026-02-22 00:29:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qst3y",
              "author": "Save-the-world1",
              "text": "Thank you so much! It really means a lot. Out of curiosity, what kind of task did you test it on? I'm finalizing V2 right now based on community feedback, so knowing how people are actually using it helps me cover all the edge cases!",
              "score": 1,
              "created_utc": "2026-02-22 09:02:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6n5f22",
          "author": "brahmakarma",
          "text": "I am gonna try this now",
          "score": 1,
          "created_utc": "2026-02-21 18:39:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6obv5z",
          "author": "amine250",
          "text": "this would consume a ton of tokens, sometimes, uselessly",
          "score": 1,
          "created_utc": "2026-02-21 22:21:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ofck3",
              "author": "Save-the-world1",
              "text": "Yes, you're right, Forcing a massive Chain-of-Thought for a simple text edit or a basic email is a waste of tokens and time.  \nThat‚Äôs exactly what I'm fixing right now. In V2, I've implemented a 'Dynamic Routing' step: the AI first assesses the complexity of your request. If it's a simple task, it skips the heavy logic/memory steps and does a direct execution. If it's a complex task (like coding), it activates the full framework. I'll post the update as soon as I finish stress-testing it!",
              "score": 2,
              "created_utc": "2026-02-21 22:41:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6od1ki",
          "author": "RennmaWeg",
          "text": "Will try this tomorrow. Thank you for the great Idea and template work",
          "score": 1,
          "created_utc": "2026-02-21 22:28:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ofggd",
              "author": "Save-the-world1",
              "text": "Thank you! I really appreciate it.  \nPlease let me know how it goes! I'd love to hear what kind of task you test it on and how the outputs turn out. I'm currently gathering community feedback to build V2, so if you find any flaws, edge cases, or things that break the prompt, absolutely tell me so I can fix them!",
              "score": 1,
              "created_utc": "2026-02-21 22:41:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6oyw11",
          "author": "ze_casal",
          "text": "Very cool! This a very long prompt chaining. I'm curious where do you guys store your prompts?\n\nI'm building a prompt vault for myself to store my prompts and thinking of creating a product out of it.\n\nWould any of you be interested in such product? ",
          "score": 1,
          "created_utc": "2026-02-22 00:38:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6p7dj2",
              "author": "I_thought_you_died",
              "text": "Found it personally better to store the structure for specific kinds of prompts and have it referenced that as opposed to like specific pumps themselves you could also like do specific like chain prompts  if you want to which would just be like the two step one of this drop chain and then wait for it to reply and manually enter the second one which helps streamline the AI to one task.",
              "score": 3,
              "created_utc": "2026-02-22 01:32:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6qt1n9",
                  "author": "Save-the-world1",
                  "text": "I completely agree with this approach. Storing¬†logical structures¬†(like this framework) is way more powerful than storing single-use, hardcoded prompts. That‚Äôs exactly why I built this as a meta-template.  \nu/ze_casal regarding the vault idea: I currently just use Notion to store my core templates, but a dedicated tool for prompt chaining could be a very cool project!",
                  "score": 3,
                  "created_utc": "2026-02-22 09:04:15",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6p2425",
          "author": "I_thought_you_died",
          "text": "It sounds like you tried to build a LMM agentic system . That's what those do.",
          "score": 1,
          "created_utc": "2026-02-22 00:58:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6p4khi",
          "author": "I_thought_you_died",
          "text": "So, there are suggestions I have.  \n\n1. Force Planning: Require the AI to outline its approach using markers like <Thinking> before acting. [Inference] Based on observed patterns, this establishes a logical foundation.\n\n2. Request Pre-Flight Check: Ask the AI to analyze instructions, confirm understanding, ask clarifying questions, and suggest improvements prior to execution.\n\n3. Define Output Parameters: Explicitly state the exact format, tone, and structure required (e.g., JSON, Markdown, or formal report).\n\n4. Provide Context: Explain the task's purpose, audience, and end goal. [Inference] Based on observed patterns, this grounds the output in specific details rather than generic responses.\n\n5. Break Down Tasks: Divide complex projects into sequential steps. [Inference] Based on observed patterns, LLMs perform better focusing on one stage at a time. Couple this with the write to-do cmd. Analyze against this. Helps your verification step.",
          "score": 1,
          "created_utc": "2026-02-22 01:13:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qu8qb",
              "author": "Save-the-world1",
              "text": "Treating the LLM as a single-prompt agent was exactly the goal here!\n\nThank you for this massive breakdown across your comments. For your points 1 and 5 (and the 'Pre-Flight Check'), I actually just implemented a 'Dynamic Routing & Working Memory Tracker & something similar to AoT (Atom of Thoughts)' in my local V2 build to do exactly this: force the AI to break down complex tasks and hold constraints in memory before executing.\n\nAlso, I really love your suggestion in your other comment about tool-specific rules (like forcing¬†`uv`¬†over¬†`pip`¬†or using industry ISO standards). I‚Äôm going to experiment with adding a 'Tech Stack / Gold Standard' variable block for the coding branch of the prompt. Really appreciate this level of advanced feedback!",
              "score": 2,
              "created_utc": "2026-02-22 09:15:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rbxqc",
                  "author": "I_thought_you_died",
                  "text": "FILE: narrated_overview_short.txt\nTITLE: How I Ended Up Here\n\nIt starts with me at the keyboard, slamming Enter on the Gemini CLI, watching it do what I asked and feeling that rush of ‚ÄúYES!, YES!, YES!‚Äù as ideas piled up faster than I could name them. Somewhere in that momentum, the whole setup broke‚Äîmultiple installs, duplicate files in different directories, and several half-formed repos I couldn‚Äôt even clearly identify anymore.\n\nThe original goal was simple: build something useful, like a React front end or a gemini.md file to guide the CLI. But that wasn‚Äôt enough. I didn‚Äôt just want one app; I wanted an agent that could think through a project the way I do, find what it needed, and build the thing in my style‚Äîwithout racking up API or token costs and ideally filling in gaps I hadn‚Äôt thought of.\n\nAs the idea expanded, I imagined agents that could run for hours on their own, tracked through a clean UX with controls to stop them and get an automatic report of everything they‚Äôd done. If they were building a website, they‚Äôd also suggest things like logos, letterhead, pricing, deployment plans, maybe even a demo video. I never got the full system finished, but the mess of broken CLIs and scattered skeleton projects taught me where the real challenge is: not just building the app, but designing the logic of the builder that creates it. [cite:1]",
                  "score": 1,
                  "created_utc": "2026-02-22 12:02:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6p58s2",
          "author": "I_thought_you_died",
          "text": "Also, I would consider utilizing the rules section for the model as well. Like when writing code for python use UV instead of pip. Use the best tool for the job parameters. Let it choose(because it has more information than us). And use gold standards, safety protocols and output parameters. You can even supply it with an entire prompt framework that it references anytime you i put a prompt or use. A trigger word.  This is an advanced technique usually used for . MD files in CLI environments, but still apllies. ISO STANDARDS or other industry specific standards and  practices  can be incorporated here too. And define the tools you want it to use here. Instead of find command for local computer research, maybe us ripgrep. For example.",
          "score": 1,
          "created_utc": "2026-02-22 01:18:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pbai3",
          "author": "ahmedmahrosai",
          "text": "Good prompt",
          "score": 1,
          "created_utc": "2026-02-22 01:57:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qtb1q",
              "author": "Save-the-world1",
              "text": "Thanks Ahmed! Stay tuned, V2 is dropping very soon with some massive upgrades.",
              "score": 1,
              "created_utc": "2026-02-22 09:06:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6pwtx6",
          "author": "ProblemFinal4653",
          "text": "I am running into recitation error and because of this am getting the output any suggestions to solve this",
          "score": 1,
          "created_utc": "2026-02-22 04:22:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qtvtr",
              "author": "Save-the-world1",
              "text": "Hey! Are you using Gemini by any chance? If it's literally throwing a 'Recitation Error' flag, that's usually Google's safety filter thinking the output is too close to copyrighted material.  \nBUT, if you mean the AI is just lazily 'parroting' the prompt or faking compliance without actually doing the deep work, I feel your pain. What model and task were you using? The upcoming V2 (dropping very soon) has a strict 'Working Memory Tracker' designed exactly to try stopping this lazy roleplay and force actual task execution!",
              "score": 1,
              "created_utc": "2026-02-22 09:12:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6pzs72",
          "author": "wanhanred",
          "text": "Will it also work creating video narrations without being too formulaic/patterned output?",
          "score": 1,
          "created_utc": "2026-02-22 04:44:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qtfzj",
              "author": "Save-the-world1",
              "text": "Absolutely! In fact, the upcoming V2 (which I'll post very soon) has a specific 'Anti-Cringe Filter & Blacklist' step built exactly for this. It forces the AI to strip out that typical 'robotic/YouTube-formula' tone and stick to the exact voice you define. For now, just make sure to explicitly define your desired tone in Phase 2, and it will work great!",
              "score": 1,
              "created_utc": "2026-02-22 09:08:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r6ugif",
      "title": "That Brutally Honest AI CEO Tweet + 5 Prompts That'll Actually Make You Better at Your Job",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1r6ugif/that_brutally_honest_ai_ceo_tweet_5_prompts/",
      "author": "EQ4C",
      "created_utc": "2026-02-17 02:58:23",
      "score": 118,
      "num_comments": 24,
      "upvote_ratio": 0.83,
      "text": "So Dax Raad from anoma just posted what might be the most honest take on AI in the workplace I've seen all year. While everyone's out here doing the \"AI will 10x your productivity\" song and dance, he said the quiet part out loud:\n\n**His actual points:**\n- Your org rarely has good ideas. Ideas being expensive to implement was actually a feature, not a bug\n- Most workers want to clock in, clock out, and live their lives (shocker, I know)\n- They're not using AI to be 10x more effective‚Äîthey're using it to phone it in with less effort\n- The 2 people who actually give a damn are drowning in slop code and about to rage quit\n- You're still bottlenecked by bureaucracy even when the code ships faster\n- Your CFO is having a meltdown over $2000/month in LLM bills per engineer\n\n**Here's the thing though:** He's right about the problem, but wrong if he thinks AI is useless.\n\nThe real issue? Most people are using AI like a fancy autocomplete instead of actually thinking. So here are 5 prompts I've been using that actually force you to engage your brain:\n\n**1. The Anti-Slop Prompt**\n\n> \"Review this code/document I'm about to write. Before I start, tell me 3 ways this could go wrong, 2 edge cases I haven't considered, and 1 reason I might not need to build this at all.\"\n\n**2. The Idea Filter**\n\n> \"I want to build [thing]. Assume I'm wrong. Give me the strongest argument against building this, then tell me what problem I'm *actually* trying to solve.\"\n\n**3. The Reality Check**\n\n> \"Here's my plan: [plan]. Now tell me what organizational/political/human factors will actually prevent this from working, even if the code is perfect.\"\n\n**4. The Energy Auditor**\n\n> \"I'm about to spend 10 hours on [task]. Is this genuinely important, or am I avoiding something harder? What's the 80/20 version of this?\"\n\n**5. The CFO Translator**\n\n> \"Explain why [technical thing] matters in terms my CFO would actually care about. No jargon. Just business impact.\"\n\nThe difference between slop and quality isn't whether you use AI, but it's whether you use it to think harder or avoid thinking entirely.\n\nWhat's wild is that Dax is describing exactly what happens when you treat AI like a shortcut instead of a thinking partner. The good devs quit because they're the only ones who understand the difference.\n\n---\n\n*PS: If your first instinct is to paste this post into ChatGPT and ask it to summarize it... you're part of the problem lmao*\n\nFor expert prompts visit our free [mega-prompts collection](https://tools.eq4c.com/)",
      "is_original_content": false,
      "link_flair_text": "Prompt Text / Showcase",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1r6ugif/that_brutally_honest_ai_ceo_tweet_5_prompts/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5uihb6",
          "author": "Impossible-Bat-6713",
          "text": "Here‚Äôs the challenge with simple prompts- They rarely work beyond the context window. \n\nAny serious code base will have UX, business logic, connected applications and DB layer. Unless you connect all of these layers and provide full context of data flow,  your code generated with be limited to your repo and will not be optimized for complex systems.\n\nThe moment you have a workflow, there‚Äôs a whole lot of complexity, input data classification/ cleansing/ labeling, optimized prompts, evals for validating output and downstream integrations across environments not to mention reliability, security and performance constraints.\n\nThis level of complexity cannot be dealt with simple prompts. Needs a far deeper understanding of systems architecture and design.",
          "score": 10,
          "created_utc": "2026-02-17 11:11:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ygms3",
              "author": "HeathersZen",
              "text": "Isn‚Äôt that what project rules are for?",
              "score": 1,
              "created_utc": "2026-02-17 23:33:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5z1hyu",
                  "author": "Impossible-Bat-6713",
                  "text": "Project specific rules typically cover standards / patterns specific to your project but will not deal with larger systemic constraints and architectural trade offs outside it.",
                  "score": 1,
                  "created_utc": "2026-02-18 01:27:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o62hun8",
              "author": "Kiran_Baby",
              "text": "How  to get an basic understanding of systems architecture and design, any youtube suggestions or topic related URLs ",
              "score": 1,
              "created_utc": "2026-02-18 15:36:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ul3mp",
          "author": "Savings-Giraffe-4007",
          "text": "\"AI, you always know better even when you allucinate and spew factually wrong BS\"\n\n\nSounds like prompting for junior devs trying to prove they suck and can be replaced",
          "score": 4,
          "created_utc": "2026-02-17 11:33:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w0y15",
          "author": "myeleventhreddit",
          "text": "You're doing what no AI can. \n\nTaking someone else's insights, re-packaging with a numbered list, and selling your own collection at the end--all while insulting the people who want to summarize derivative text walls to save time. well-played.",
          "score": 3,
          "created_utc": "2026-02-17 16:29:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uctgc",
          "author": "lioninside_",
          "text": "Thx for sharing. I see already a \"anti hype\" with people saying/writing \"AI is (also) just a tool\". I don't fully agree on this too as it underestimates to power of AI (to change society)",
          "score": 2,
          "created_utc": "2026-02-17 10:20:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w9lxy",
          "author": "amaturelawyer",
          "text": "While I know OP is shilling some worthless prompt site, and I hate him for it as much as everyone else, and I noticed that his bot accounts fucked up in the \"generate apparent engagement by other users\" code with \"[](https://www.reddit.com/user/IndependentClock7184/)\n\n[IndependentClock7184 ](https://www.reddit.com/user/IndependentClock7184/)\n\n‚Ä¢ [14h ago](https://www.reddit.com/r/PromptEngineering/comments/1r6ugif/comment/o5svzd1/)\n\nWould you be willing to check out my post\n\n[](https://www.reddit.com/user/EQ4C/)OP ‚Ä¢ [14h ago](https://www.reddit.com/r/PromptEngineering/comments/1r6ugif/comment/o5sw7sz/)\n\n Sure, thanks\n\n\", I'm still going to answer because I feel like it and this is still sort of a free society or whatever.\n\n\n\nIn general, I find it more helpful to explain what the desired end state is than to explain the plan or incremental steps to get there, and it's always helpful to tell it to not make any assumptions and ask if something is ambiguous or unclear with how you stated it. \n\n\n\n\\#4 seems likely to not work well. The AI knows exactly one thing about the importance of the task, that you are trying to do it: you're attempting to do it. Without feeding a crapload of context in, that question can only result in a guess, as time investment is a relative thing and needs to be compared to what else it could be spent on. If it's this or playing solitare for 10 hours, seems worth it if the end result is you save $100 over the year, but if it's this or fixing a bug that's delaying shipping and this is a project that reminds you to take breaks every hour, it's probably not as important.\n\n  \nThe one I have a large problem with is #5, and OP's LLM should be examined for evidence that it's been self-modifying weights, because it's literally an insane point. Why the fuck would you ask an LLM to write out how you should make your case to a CFO and say \"no jargon\"? The only thing most C-suite placeholders speak in is jargon.",
          "score": 2,
          "created_utc": "2026-02-17 17:13:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uzl20",
          "author": "Gynnia",
          "text": "but have these been genuinely helpful in real-life scenarios?",
          "score": 1,
          "created_utc": "2026-02-17 13:14:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67zfgw",
          "author": "PdxGuyinLX",
          "text": "Shouldn‚Äôt an intelligent person be able to answer your five questions on their own without needing to ask an AI?",
          "score": 1,
          "created_utc": "2026-02-19 10:52:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5svzd1",
          "author": "IndependentClock7184",
          "text": "Would you be willing to check out my post",
          "score": 0,
          "created_utc": "2026-02-17 03:07:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5sw7sz",
              "author": "EQ4C",
              "text": "Sure, thanks",
              "score": -2,
              "created_utc": "2026-02-17 03:08:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5u0v8t",
          "author": "AgenticAF",
          "text": "Thanks for sharing this!",
          "score": -1,
          "created_utc": "2026-02-17 08:26:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tfu2r",
          "author": "ceeczar",
          "text": ">when you treat AI like a shortcut instead of a thinking partner.¬†\n\n\nThanks for sharing¬†\n\n\nIndeed the AI hype can be wild.¬†\n\n\nEspecially on YouTube\n\n\nSpeaking of shortcuts, do we really need AI to do the energy audit you mention in Prompt 4?",
          "score": -2,
          "created_utc": "2026-02-17 05:23:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u1zhr",
              "author": "traumfisch",
              "text": "you can just go\n\n\nWhat's the 80/20 version of this?\n\n\nabout anything, no?",
              "score": 2,
              "created_utc": "2026-02-17 08:37:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5thb1m",
              "author": "Weekly-Bee-5045",
              "text": "After destroying many projects nearing completion... I can say yes 4 is good. üëç",
              "score": -1,
              "created_utc": "2026-02-17 05:34:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r5uv7a",
      "title": "One day of work + Opus 4.6 = Voice Cloning App using Qwen TTS. Free app, No Sing Up Required",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1r5uv7a/one_day_of_work_opus_46_voice_cloning_app_using/",
      "author": "OneMoreSuperUser",
      "created_utc": "2026-02-16 00:28:10",
      "score": 59,
      "num_comments": 10,
      "upvote_ratio": 0.93,
      "text": "A few days ago, Qwen released a new open weight speech-to-speech model: Qwen3-TTS-12Hz-0.6B-Base. It is great model but it's huge and hard to run on any current regular laptop or PC so I built a free web service so people can check the model and see how it works.\n\n* No registration required\n* Free to use\n* Up to 500 characters per conversion\n* Upload a voice sample + enter text, and it generates cloned speech\n\nHonestly, the quality is surprisingly good for a 0.6B model.\n\nModel: Qwen3-TTS\n\nWeb app where you can text the model for free:\n\n[https://imiteo.com](https://imiteo.com/)\n\nSupports 10 major languages: English, Chinese, Japanese, Korean, German, French, Russian, Portuguese, Spanish, and Italian.\n\nIt runs on an NVIDIA L4 GPU, and the app also shows conversion time + useful generation stats.\n\nThe app is 100% is written by Claude Code 4.6. Done in 1 day.\n\nOpus 4.6, Cloudflare workers, L4 GPU\n\nMy twitter account: [https://x.com/AndreyNovikoov](https://x.com/AndreyNovikoov)",
      "is_original_content": false,
      "link_flair_text": "Self-Promotion",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1r5uv7a/one_day_of_work_opus_46_voice_cloning_app_using/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5myb43",
          "author": "Hippolithe",
          "text": "Actually, singing up to a voice app sounds somewhat appropriate.",
          "score": 10,
          "created_utc": "2026-02-16 05:43:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m8fdi",
          "author": "throwaway867530691",
          "text": "I'm a bit worried about my voice signature getting stolen and my grandma getting called by a bot asking for $5000 in my voice. I'm totally misunderstanding the risks here right?",
          "score": 6,
          "created_utc": "2026-02-16 02:39:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5m91lr",
              "author": "OneMoreSuperUser",
              "text": "It‚Äôs open-source technology, and anyone has been able to do it for the last two years. Yes, it‚Äôs crazy‚Äîbut that‚Äôs the world we‚Äôre living in.",
              "score": 1,
              "created_utc": "2026-02-16 02:43:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5mdzk2",
                  "author": "throwaway867530691",
                  "text": "I suppose it's the concern over my specific voice data being tied to me as a result of using a particular \"clone your voice\" app",
                  "score": 4,
                  "created_utc": "2026-02-16 03:15:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5qxc62",
          "author": "-Nano",
          "text": "Question: Brazilian Portuguese or ~~Archaic~~ European Portuguese?\n\nWill opensource it?",
          "score": 1,
          "created_utc": "2026-02-16 20:38:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1randzl",
      "title": "‚è±Ô∏è 7 ChatGPT Prompts That Fix Your Time Management Overnight (Copy + Paste)",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1randzl/7_chatgpt_prompts_that_fix_your_time_management/",
      "author": "Loomshift",
      "created_utc": "2026-02-21 09:58:16",
      "score": 48,
      "num_comments": 5,
      "upvote_ratio": 0.84,
      "text": "# \n\nI used to end every day thinking:  \n**‚ÄúWhere did all my time go?‚Äù**\n\nI was busy from morning to night ‚Äî  \nyet my important work kept getting delayed.\n\nThe problem wasn‚Äôt laziness.  \nIt was lack of a system.\n\nOnce I started using ChatGPT as a **time strategist**, my days stopped feeling chaotic and started feeling controlled.\n\nThese prompts help you **organize your time, eliminate waste, and make progress automatically**.\n\nHere are the seven that actually work üëá\n\n# 1. The Instant Time Audit\n\nShows exactly where your time disappears.\n\n**Prompt:**\n\n    Help me audit how I spend my time daily.\n    Ask me questions about my routine.\n    Then identify my biggest time-wasters and suggest fixes.\n    \n\n# 2. The Smart Schedule Builder\n\nCreates a realistic plan you can actually follow.\n\n**Prompt:**\n\n    Build a daily schedule for me.\n    Include priorities, work blocks, breaks, and buffer time.\n    Make it simple, realistic, and flexible.\n    \n\n# 3. The Priority Decision Engine\n\nEliminates task confusion.\n\n**Prompt:**\n\n    Here‚Äôs my task list: [tasks]\n    Rank them by impact and urgency.\n    Tell me what to do first and what to delay.\n    Explain why.\n    \n\n# 4. The Anti-Procrastination Starter\n\nMakes starting easy.\n\n**Prompt:**\n\n    I keep avoiding this task: [task]\n    Break it into tiny steps that feel easy to start.\n    Add time estimates for each step.\n    \n\n# 5. The Focus Protection System\n\nGuards your attention.\n\n**Prompt:**\n\n    Help me create rules to protect my focus.\n    Include digital rules, environment rules, and mindset rules.\n    Explain how each prevents distraction.\n    \n\n# 6. The Energy-Based Planner\n\nAligns tasks with your brain power.\n\n**Prompt:**\n\n    Help me schedule tasks based on my energy levels.\n    Ask when I feel most focused and most tired.\n    Then assign tasks to the best time slots.\n    \n\n# 7. The 30-Day Time Reset Plan\n\nBuilds lasting control over your schedule.\n\n**Prompt:**\n\n    Create a 30-day time management reset plan.\n    Break it into weekly themes:\n    Week 1: Awareness\n    Week 2: Structure\n    Week 3: Optimization\n    Week 4: Automation\n    \n    Include daily actions under 15 minutes.\n    \n\nTime management doesn‚Äôt improve when you try harder.  \nIt improves when your **system gets smarter**.\n\nThese prompts turn ChatGPT into your personal time strategist so your day runs with direction instead of stress.\n\nIf you want to save or organize these prompts, you can keep them inside **Prompt Hub**, which also has 300+ advanced prompts for free:  \nüëâ [https://aisuperhub.io/prompt-hub](https://aisuperhub.io/prompt-hub)\n\n",
      "is_original_content": false,
      "link_flair_text": "Prompt Text / Showcase",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1randzl/7_chatgpt_prompts_that_fix_your_time_management/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o6qxnoq",
          "author": "Adorable-Dot-785",
          "text": "Looks good, will try",
          "score": 1,
          "created_utc": "2026-02-22 09:48:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5l0dz",
      "title": "Instead of prompt engineering AI to write better copy, we lint for it",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1r5l0dz/instead_of_prompt_engineering_ai_to_write_better/",
      "author": "JWPapi",
      "created_utc": "2026-02-15 17:50:31",
      "score": 47,
      "num_comments": 26,
      "upvote_ratio": 0.95,
      "text": "We spent a while trying to prompt engineer our way to better AI-generated emails and UI code. Adding instructions like \"don't use corporate language\" and \"use our design system tokens instead of raw Tailwind colors\" to system prompts and CLAUDE.md files. It worked sometimes. It didn't work reliably.\n\nThen we realized we were solving this problem at the wrong layer. Prompting is a suggestion. A lint rule is a wall. The AI can ignore your prompt instructions. It cannot ship code that fails the build.\n\nSo we wrote four ESLint rules:\n\nhumanize-email maintains a growing ban list of AI phrases. \"We're thrilled\", \"don't hesitate\", \"groundbreaking\", \"seamless\", \"delve\", \"leveraging\", all of it. The list came from Wikipedia's \"Signs of AI writing\" page plus every phrase we caught in our own outbound emails after it had already shipped to customers. The rule also enforces which email layout component to use and limits em dashes to 2 per file.\n\nprefer-semantic-classes bans raw Tailwind color classes (bg-gray-100, text-zinc-500) and forces semantic design tokens (surface-primary, text-secondary). AI models don't know your design system. They know Tailwind defaults. This rule makes the AI's default impossible to ship.\n\ntypographic-quotes auto-fixes mixed quote styles in JSX. Small but it catches the inconsistency between AI output and human-typed text.\n\nno-hover-translate blocks hover:-translate-y-1 which AI puts on every card. It causes a jittery chase effect when users approach from below because translate moves the hit area.\n\nHere's the part that's relevant to this community: the error messages from these rules become context for the AI in the next generation. So the lint rules are effectively prompt engineering, just enforced at build time instead of suggested at generation time. After a few rounds of hitting the lint wall, the AI starts avoiding the patterns on its own.\n\nIf you keep correcting the same things in AI output, don't write a better prompt. Write a lint rule. Your standards compound over time as the ban list grows. Prompts drift.\n\nFull writeup: https://jw.hn/eslint-copy-design-quality",
      "is_original_content": false,
      "link_flair_text": "Tips and Tricks",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1r5l0dz/instead_of_prompt_engineering_ai_to_write_better/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5jslls",
          "author": "mrpoopybruh",
          "text": "I also have a thread just for a verification agent that looks over work ONLY, and turns into a complete and angry psycho. 10/10 would reccomend.",
          "score": 3,
          "created_utc": "2026-02-15 18:33:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5koumg",
          "author": "Too_Bad_Bout_That",
          "text": " Why do you say that AI can ignore prompt instructions?  \n",
          "score": 1,
          "created_utc": "2026-02-15 21:16:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kuo42",
              "author": "[deleted]",
              "text": "LLM can ignore anything you tell them to do, they are simply next word probability predictors. As instructions get longer the chances of AI \"ignoring\" rules grows as they have to comply with more rules which they did not really understand.",
              "score": 2,
              "created_utc": "2026-02-15 21:45:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5kyw00",
                  "author": "Too_Bad_Bout_That",
                  "text": "I can think of only 2 reasons for that to happen, 1 - task can have something illegal, unsafe in it or 2, prompts can be very ill-structured. \n\nThe way AI works is that it scans the prompt and searches for details like task, context, style and etc. Sometimes task can be unclear for it so it might miss that. Try to divide prompt with headings and chapters like:\n\n\\#Task:  \nYour job is to...\n\nSo far it has been working for me",
                  "score": -2,
                  "created_utc": "2026-02-15 22:07:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5mzcx0",
              "author": "awittygamertag",
              "text": "It can. Small models do it when the instructions are confusing and Opus 4.6 ignores them when it thinks you‚Äôre wrong. Take your pick of the poison lol",
              "score": 1,
              "created_utc": "2026-02-16 05:52:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o601yas",
              "author": "AxeSlash",
              "text": "There are many, many reasons an LLM can ignore an instruction. IMHO the biggest is recency bias.\n\nInstructions are usually sent at the top of the request's context (which seems like a design flaw to me but then again I'm no AI dev) ,which means that the last user prompt can have more influence than the instructions, especially if context is long.\n\nPoorly written instructions are another big one.\n\nNEVER trust an LLM to adhere 100% to your instruction set. That way lies downstream carnage. These things are NOT deterministic.",
              "score": 1,
              "created_utc": "2026-02-18 05:01:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5n0ofm",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-16 06:03:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5n0ogz",
              "author": "AutoModerator",
              "text": "Hi there! Your post was automatically removed because your account is less than 3 days old. We require users to have an account that is at least 3 days old before they can post to our subreddit.\n\nPlease take some time to participate in the community by commenting and engaging with other users. Once your account is older than 3 days, you can try submitting your post again.\n\nIf you have any questions or concerns, please feel free to message the moderators for assistance.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/PromptEngineering) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-02-16 06:03:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o69rret",
          "author": "Dxstinity",
          "text": "this is a cool approach! instead of just trying to prompt better, setting up lint rules makes total sense. i‚Äôve had similar struggles with AI outputs not matching my style, and it‚Äôs frustrating. for outbound emails, i use mailly to help with context and relevance, it really gets the tone right.",
          "score": 1,
          "created_utc": "2026-02-19 17:08:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mnub2",
          "author": "chkbd1102",
          "text": "i like the idea. but i think the biggest hurdle will be this\n\ni generate a text, linter give me back error A, the AI read it and regenrate. it can come back with error B. it fix error B, but regenrate the whole text again.\n\ni could easily foresee creating an umlimited cycle, just like working with coding agent.",
          "score": 0,
          "created_utc": "2026-02-16 04:24:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nbn5p",
              "author": "susimposter6969",
              "text": "Perhaps only patch the sentence containing the issue",
              "score": 1,
              "created_utc": "2026-02-16 07:41:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ra0320",
      "title": "Why AI Humanizers Don‚Äôt Work (And What to Do Instead)",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1ra0320/why_ai_humanizers_dont_work_and_what_to_do_instead/",
      "author": "KnowledgeNo3681",
      "created_utc": "2026-02-20 16:31:27",
      "score": 46,
      "num_comments": 43,
      "upvote_ratio": 0.85,
      "text": "Traditional humanizers alter meaning, change the context, or make the text too basic. Humanizers like TextToHuman and SuperHumanizer are trained on human samples, and they rewrite the text without changing the context.\n\nSite URL: [superhumanizer.ai](http://superhumanizer.ai)",
      "is_original_content": false,
      "link_flair_text": "General Discussion",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1ra0320/why_ai_humanizers_dont_work_and_what_to_do_instead/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o6g582k",
          "author": "Speedping",
          "text": "Did you use the site for this post? If so, it doesn‚Äôt work very well",
          "score": 7,
          "created_utc": "2026-02-20 16:38:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g5dl3",
              "author": "InformationNew66",
              "text": "I agree, this post it totally AI written and doesn't sound human.",
              "score": 2,
              "created_utc": "2026-02-20 16:38:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gsbw2",
                  "author": "TokxoDev",
                  "text": "You basically showed us what not to do.",
                  "score": 1,
                  "created_utc": "2026-02-20 18:23:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6q9ubm",
              "author": "ndovesha",
              "text": "I also agree",
              "score": 1,
              "created_utc": "2026-02-22 06:05:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gexgd",
          "author": "throwaway867530691",
          "text": "Here's why Claude thinks this is AI written:\n\n\n\t‚àô\tThe ‚Äúhonest confession‚Äù opener is a template. ‚ÄúI‚Äôve been testing X for a while now, and honestly‚Ä¶‚Äù is the go-to AI move for faking authenticity. The ellipsis after ‚Äúhonestly‚Äù is doing heavy lifting to simulate a casual, reflective human pause. It‚Äôs manufactured vulnerability.\n\t‚àô\tThe ‚ÄúWhat they actually do:‚Äù list is suspiciously clean. A real person ranting about bad tools would ramble, go on tangents, or give a specific example of a tool that burned them. This just drops a perfectly formatted, parallel-structure bullet list. No human frustration sounds that organized.\n\t‚àô\tZero specifics, maximum vagueness. There‚Äôs not a single concrete example. No ‚ÄúI ran my blog post through X and it turned ‚Äòquick‚Äô into ‚Äòexpeditious.‚Äô‚Äù No screenshots, no before/after. It‚Äôs all abstract hand-waving that sounds informed but says nothing.\n\t‚àô\tThe pivot to product names is the tell. The entire first half exists solely to set up the ‚Äúbut THESE tools are different‚Äù payoff. TextToHuman and SuperHumanizer get dropped with zero critical analysis. That‚Äôs not a review ‚Äî it‚Äôs a funnel.\n\t‚àô\tThe neat five-item ‚Äúpreserving‚Äù list. Meaning, Context, Structure, Headings, Tone ‚Äî perfectly parallel single-word items. That‚Äôs Claude/GPT list formatting. A human would say ‚Äúit actually kept my headings and didn‚Äôt butcher what I was trying to say.‚Äù\n\t‚àô\t‚ÄúInstead of rewriting your content into something generic, they refine it.‚Äù This is pure AI cadence. The clean contrast structure (‚Äúinstead of X, they Y‚Äù) with a vague positive verb at the end. No human talks like a landing page.\n\t‚àô\tThe closing ‚Äúadvice‚Äù paragraph. Wrapping up with a tidy takeaway that reframes the product pitch as wisdom is textbook AI-generated SEO/affiliate content. ‚ÄúDon‚Äôt just look for X ‚Äî look for one that Y‚Äù is a template you could set your watch to.\n\nThe whole thing is astroturf. It‚Äôs an ad for two specific tools disguised as a frustrated user‚Äôs honest take, almost certainly generated by one of the tools it‚Äôs promoting. The irony of using AI to write a post about how most AI humanizers suck ‚Äî while shilling an AI humanizer ‚Äî is pretty rich.‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",
          "score": 5,
          "created_utc": "2026-02-20 17:22:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gmf7c",
              "author": "KnowledgeNo3681",
              "text": "Lol! Too many Em dashes, I bit you used AI too for this reply. Didn't you?",
              "score": 0,
              "created_utc": "2026-02-20 17:57:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6hzlhw",
                  "author": "throwaway867530691",
                  "text": "Yeah as I said this is what Claude thought. I'm getting too lazy to write anything unless it's going to affect my money or personal relationships",
                  "score": 3,
                  "created_utc": "2026-02-20 21:52:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gc2ce",
          "author": "awnliy",
          "text": "This post is ai written too",
          "score": 3,
          "created_utc": "2026-02-20 17:09:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gc8um",
              "author": "KnowledgeNo3681",
              "text": "Why do you think so? Have you check it with an AI detector?",
              "score": -1,
              "created_utc": "2026-02-20 17:10:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6k0i4q",
                  "author": "CrimsonVixenPixie",
                  "text": "It‚Äôs super obvious to anyone, just so you know. You kind of remind me of those cosmetic surgery people who have lost the ability to appraise their own appearance and end up looking like freakish monsters. The self deception is scary.",
                  "score": 1,
                  "created_utc": "2026-02-21 05:22:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gag3m",
          "author": "spinozaschilidog",
          "text": "Cool, another ad.",
          "score": 2,
          "created_utc": "2026-02-20 17:01:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gatjr",
              "author": "KnowledgeNo3681",
              "text": "Have you read it? And did you try Superhumanizer? What do you think of it?",
              "score": -1,
              "created_utc": "2026-02-20 17:03:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6gc22x",
                  "author": "it_and_webdev",
                  "text": "No ond is going to try your slop",
                  "score": 5,
                  "created_utc": "2026-02-20 17:09:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6geh1g",
                  "author": "spinozaschilidog",
                  "text": "Not giving you the courtesy of a click. You could have made this an ad with a ‚ÄúPromoted‚Äù tag. Instead you‚Äôre here stinking up the joint with ad copy. It‚Äôs inauthentic, and it‚Äôs adding to the further enshittification of this platform. Get bent.\n\nEdit: this user‚Äôs post history shows that they‚Äôre either a bot or doing a convincing impression of one. Just about every post for the last 2 months is either about or directly refers to whatever slop is being sold here. Lazy cash -grabbing motherfuckers just determined to make a dead internet real.",
                  "score": 2,
                  "created_utc": "2026-02-20 17:20:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6h9075",
          "author": "SemanticSynapse",
          "text": "Bad bot. ",
          "score": 2,
          "created_utc": "2026-02-20 19:40:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k86b4",
              "author": "KnowledgeNo3681",
              "text": "Not a bot, bro.\nHave you tried superhumanizer?",
              "score": 1,
              "created_utc": "2026-02-21 06:26:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6k12af",
          "author": "Difficult_Buffalo544",
          "text": "Nailed it about most humanizers just being fancy paraphrasers. The real problem is they don‚Äôt actually learn your voice, they just jumble words and strip away what makes your writing feel like you. Some newer platforms are going the extra mile with training on your own samples and letting you tweak before publishing, which helps a lot with tone and structure.\n\nA few other things that work: training the AI on a big batch of your actual writing, building in a review step where a human signs off before anything goes live, and using tools that let you lock in brand guidelines or style rules for the whole team. You can use Atom Writer for this since it combines AI drafting with brand voice training and a review workflow, so the AI keeps your style and tone without that generic feel. Mix in a final human pass for anything super important, and you can scale output without losing your voice.",
          "score": 1,
          "created_utc": "2026-02-21 05:26:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k8exr",
              "author": "KnowledgeNo3681",
              "text": "Exactly, Super humanizer is trainer on 1000‚Äôs of human writer samples that writes like you.\nIn the end humanizer rewriter the content like if human wrote it",
              "score": 1,
              "created_utc": "2026-02-21 06:29:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6klzoy",
                  "author": "Difficult_Buffalo544",
                  "text": "That's awesome!",
                  "score": 1,
                  "created_utc": "2026-02-21 08:38:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6mlggf",
          "author": "realtouchai",
          "text": "Hey there! I saw your message on the reddit forum. If you're looking for a great AI humanizer, definitely check out Realtouch AI on Google. It's by far the best one out there. Let me know if you give it a try!",
          "score": 1,
          "created_utc": "2026-02-21 17:00:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6mxaqd",
          "author": "realtouchai",
          "text": "Hey there! Thanks for the tip on Realtouch AI. I'll definitely give it a try. Looking forward to seeing how it humanizes the AI experience. Appreciate the recommendation!",
          "score": 1,
          "created_utc": "2026-02-21 18:00:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qzsu4",
              "author": "KnowledgeNo3681",
              "text": "Glad you liked it it.",
              "score": 1,
              "created_utc": "2026-02-22 10:09:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6pzs18",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-22 04:44:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6pzs2t",
              "author": "AutoModerator",
              "text": "Hi there! Your post was automatically removed because your account is less than 3 days old. We require users to have an account that is at least 3 days old before they can post to our subreddit.\n\nPlease take some time to participate in the community by commenting and engaging with other users. Once your account is older than 3 days, you can try submitting your post again.\n\nIf you have any questions or concerns, please feel free to message the moderators for assistance.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/PromptEngineering) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-02-22 04:44:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6q15nl",
          "author": "Exact-Assignment5980",
          "text": "I kind of agree that a lot of humanizers miss the point. In my experience, they tweak wording but keep the same predictable structure underneath, so the text still feels AI-ish. What‚Äôs worked better for me is manually breaking patterns - shortening some sentences, combining others, and adding small specifics that feel personal. I tried Rephrasy once and it helped smooth phrasing, but it still needed that human pass at the end. Do you think the issue is the models themselves, or just how people expect them to work?",
          "score": 1,
          "created_utc": "2026-02-22 04:54:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qzvrq",
              "author": "KnowledgeNo3681",
              "text": "It's the modal themeselves, the best way to humanize AI text is to use a super humanizer, aka Superhumanizer AI.",
              "score": 1,
              "created_utc": "2026-02-22 10:10:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6q9qp8",
          "author": "ndovesha",
          "text": "What about humanize.ai this is the best ai humanizer I‚Äôve come across so far",
          "score": 1,
          "created_utc": "2026-02-22 06:04:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qzmjv",
              "author": "KnowledgeNo3681",
              "text": "Have you tested how good it is in comparison with the Superhumanizer AI?",
              "score": 1,
              "created_utc": "2026-02-22 10:07:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6qfz4r",
          "author": "realtouchai",
          "text": "Hey there! If you're looking for a great AI humanizer to try out, I highly recommend checking out RealTouch AI on Google. It's been getting some great reviews and could be just what you're looking for. Give it a shot and let me know what you think!",
          "score": 1,
          "created_utc": "2026-02-22 07:00:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qmbqq",
          "author": "realtouchai",
          "text": "Hey there! Thanks for the tip on RealTouch AI. I'll definitely check it out and see how it goes. Appreciate the recommendation!",
          "score": 1,
          "created_utc": "2026-02-22 08:00:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qzxou",
              "author": "KnowledgeNo3681",
              "text": "Sure, let me know, any good or bad, love to hear from you.",
              "score": 1,
              "created_utc": "2026-02-22 10:10:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6r55c4",
          "author": "realtouchai",
          "text": "Hey there! I recommend you check out RealTouch AI on Google for AI humanizing. It's a great tool for enhancing your roleplay character's tone of voice and keeping chat conversations engaging. Give it a try!",
          "score": 1,
          "created_utc": "2026-02-22 11:00:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gbpci",
          "author": "aletheus_compendium",
          "text": "just ran a test yesterday and this is the conclusion, about all of these \"tools\":\nAcross 12 identical samples, the three detection platforms produced sharply conflicting results. For example, one sample labeled ‚Äú100% Human‚Äù by WriteHuman was simultaneously rated 95% to 99% AI by GPTZero, while ZeroGPT placed many of the same texts in the 20% to 40% AI range. These are not marginal differences but categorical disagreements at high confidence levels. When identical prose can be called fully human, mostly AI, and strongly AI by different detectors at the same time, the output is not a stable measurement, which means using ‚Äúhumanizers‚Äù to game these systems has little practical evaluative value.",
          "score": 0,
          "created_utc": "2026-02-20 17:07:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gcm1e",
              "author": "KnowledgeNo3681",
              "text": "Don't trust the AI detector of Writehuman; it will always say 100% human. Have you tried Superhumanizer AI? Why not test a few of your 12 samples and let me know how it works.",
              "score": -1,
              "created_utc": "2026-02-20 17:11:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ggjhf",
                  "author": "aletheus_compendium",
                  "text": "the test was about AI detection. these products sell risk reduction not verified accuracy. the value proposition is psychological and institutional: ‚Äúlower your chance of being flagged.‚Äù it‚Äôs a probabilistic narrative wrapped in certainty language. people are not really paying for measurement validity. they are paying for perceived protection against institutional consequences. the test suggests that protection is unstable at best. ur model isn't a detector, and there is no fiction category either. good luck",
                  "score": 1,
                  "created_utc": "2026-02-20 17:30:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r689sl",
      "title": "If your prompt is 12 pages long, you don't have a 'Super Prompt'. You have a Token Dilution problem.",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1r689sl/if_your_prompt_is_12_pages_long_you_dont_have_a/",
      "author": "GetAIBoostKit",
      "created_utc": "2026-02-16 12:25:19",
      "score": 41,
      "num_comments": 33,
      "upvote_ratio": 0.87,
      "text": "Someone commented on my last post saying my prompts were 'bad' because theirs are 12 pages long.\n\nLet's talk about **Attention Mechanism** in LLMs. When you feed a model 12 pages of instructions for a simple task, you are diluting the weight of every single constraint. The model inevitably hallucinates or ignores the middle instructions.\n\nI use the **RPC+F Framework** precisely to avoid this.\n\n* **12 Pages:** The model 'forgets' instructions A, B, and C to focus on Z.\n* **3 Paragraphs (Architected):** The model has nowhere to hide. Every constraint is weighted heavily.\n\nStop confusing 'quantity' with 'engineering'. Efficiency is about getting the result with the *minimum* effective dose of tokens.",
      "is_original_content": false,
      "link_flair_text": "General Discussion",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1r689sl/if_your_prompt_is_12_pages_long_you_dont_have_a/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5oalhh",
          "author": "EpsteinFile_01",
          "text": "The model loses track after 1 page lol, ignoring things and/or addressing things briefer and briefer. Who the hell feeds 12 pages of instructions?",
          "score": 3,
          "created_utc": "2026-02-16 12:49:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5oca8u",
              "author": "Doppelgen",
              "text": "Me. I have done more than once, they describe entire dynamics (such as how a game works, from overall understanding to actual clicks).",
              "score": 3,
              "created_utc": "2026-02-16 13:00:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5qnxes",
                  "author": "vayana",
                  "text": "I'm trying this in a current project. Usually just build and not use any documentation outside of a prompt, but spent the last 2 days scaffolding and preparing documentation with instructions, guides and rules with 1 entry file and about 15 documents the agent can discover. I'm curious to see if this will speed up the building phase once it gets going.",
                  "score": 1,
                  "created_utc": "2026-02-16 19:51:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ob9zz",
          "author": "kyngston",
          "text": "simple.  refactor your spec for progressive discovery all starting from the top level README.md.  \n\nthen write you spec as a TODO file and implement with an agent swarm.",
          "score": 4,
          "created_utc": "2026-02-16 12:53:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ub7rf",
              "author": "GetAIBoostKit",
              "text": "Progressive discovery is the right move for complex specs, but the 'Agent Swarm' approach often hits a wall when the agents start interpreting the README differently.\n\nThat‚Äôs where I‚Äôve found the most success with the **RPC+F framework**. Instead of just a TODO file, I use the framework to set **global architectural constraints** that every agent in the swarm must follow.\n\nIf you don't anchor the 'swarm' with strict **Negative Constraints**, the progressive discovery phase usually turns into a game of telephone between the agents.\n\nI'm curious‚Äîhow do you handle logic drift when the agents move from the top-level README to the deeper implementation files?",
              "score": 1,
              "created_utc": "2026-02-17 10:05:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ozmtk",
          "author": "UsualOk3244",
          "text": "I once made a complex Agent for Finance... And boy even a full page of aspects the AI had to be aware of was too much. It was like after point 4 it forgot which restrictions point 1 gave.",
          "score": 2,
          "created_utc": "2026-02-16 15:09:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ubc8h",
              "author": "GetAIBoostKit",
              "text": "That‚Äôs the exact limit of 'instruction-based' prompting. Once the context window gets crowded, the AI‚Äôs attention fragments and it starts dropping early restrictions.\n\nThis is why I stopped writing long lists and switched to the **RPC+F framework**. Instead of just giving it points to remember, I use **Architectural Constraints** to lock the logic in place so it can't drift.\n\nIt‚Äôs the only way I‚Äôve found to keep finance or coding agents consistent without them 'forgetting' point #1 by the time they hit point #4. Happy to show you the structure if you're tired of the AI amnesia.",
              "score": 2,
              "created_utc": "2026-02-17 10:06:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5udrci",
                  "author": "UsualOk3244",
                  "text": ">Happy to show you the structure if you're tired of the AI amnesia.\n\nI take everything I can get as long if you don't want me to pay or join a group üòÇ",
                  "score": 1,
                  "created_utc": "2026-02-17 10:29:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5pzgle",
          "author": "Admirable-Corner-479",
          "text": "Read that as \"Tolkien Dilution\" ü§î",
          "score": 2,
          "created_utc": "2026-02-16 17:57:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qyvgf",
              "author": "Wizard_Biscuit",
              "text": "Try to keep your movies to 3 hours or less. Maximum of 4 hobbits on screen at once unless vital for world building.",
              "score": 2,
              "created_utc": "2026-02-16 20:45:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5s7bz7",
          "author": "PromptForge-store",
          "text": "I agree with the basic idea ‚Äì length alone doesn't make a prompt better.\n\n\nBut the real issue isn't length vs. brevity, it's architecture.\n\nA long, unstructured prompt creates dilution.\n\nA structured prompt ‚Äì even if it's longer ‚Äì creates clarity.\n\n\nThe difference is whether the prompt is just a loose instruction or a reusable system with clear roles, inputs, constraints, and output logic.\n\n\nI've seen short prompts outperform long ones ‚Äì but also structured, multi-part prompts that deliver significantly more consistent results.\n\n\nThe key isn't to minimize tokens, but to maximize the signal per token.\n\nThis is where prompting transitions from writing to system design.",
          "score": 2,
          "created_utc": "2026-02-17 00:39:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tuvra",
          "author": "NefariousnessFun1445",
          "text": "the general point about shorter prompts is fine but the reasoning is wrong. attention mechanism doesnt work the way youre describing here. the model doesnt \"forget\" instructions because theyre diluted by length - the actual issue is that with longer contexts the model struggles to attend equally to all parts, especially the middle (lost in the middle problem). thats not the same as \"weight dilution\"\n\nalso 12 pages vs 3 paragraphs is a false dichotomy. system prompts for production agents are regularly 2-3 pages and work perfectly fine when structured well. the problem is never length itself, its ambiguity and contradiction. a 3 paragraph prompt full of vague instructions will perform worse than a 2 page prompt with clear structured sections every time\n\nnot familiar with RPC+F but any framework that says \"just make it shorter\" as its core principle is oversimplifying. sometimes you need detailed instructions, edge case handling, output format specs, examples. trying to cram all that into 3 paragraphs for a complex task will hurt your results not help them",
          "score": 2,
          "created_utc": "2026-02-17 07:30:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uboj1",
              "author": "GetAIBoostKit",
              "text": "You‚Äôre absolutely right on the terminology‚Äî'Lost in the middle' is the more accurate technical description of the attention decay in long context windows. Thanks for the correction.\n\nHowever, the **RPC+F framework** isn't about 'making it shorter' for the sake of brevity. It‚Äôs about **structural density**.\n\nThe problem with 12-page (or even 3-page) prompts often isn't the length, but the **instruction-to-logic ratio**. Most people fill those pages with 'fluff' instructions that create ambiguity.\n\nI use **Negative Constraints** within RPC+F to handle those edge cases you mentioned, but I do it by defining what the model *cannot* do, which effectively 'compresses' the logic space without losing the detail.\n\nA well-structured 2-page prompt is great, but an architected RPC+F prompt usually achieves the same guardrails with far less surface area for the model to hallucinate in. It‚Äôs not about oversimplifying; it‚Äôs about **removing the noise** so the instructions actually stick.",
              "score": 1,
              "created_utc": "2026-02-17 10:10:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5odpbn",
          "author": "-goldenboi69-",
          "text": "The way ‚Äúprompt engineering‚Äù gets discussed often feels like a placeholder for several different problems at once. Sometimes it‚Äôs about interface limitations, sometimes about steering stochastic systems, and sometimes about compensating for missing tooling or memory. As models improve, some of that work clearly gets absorbed into the system, but some of it just shifts layers rather than disappearing. It‚Äôs hard to tell whether prompt engineering is a temporary crutch or an emergent skill that only looks fragile because we haven‚Äôt stabilized the abstractions yet.",
          "score": 1,
          "created_utc": "2026-02-16 13:09:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5osq8d",
              "author": "slartybartvart",
              "text": "Given my kids have an extremely small context window and forget prompts even when repeated, I'd say it's an emergent skill.",
              "score": 1,
              "created_utc": "2026-02-16 14:33:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5uaxyr",
              "author": "GetAIBoostKit",
              "text": "This is the most lucid take I‚Äôve seen in a while. You hit the nail on the head with the 'stabilizing abstractions' part.\n\nRight now, most 'prompting' is indeed a temporary crutch because people treat LLMs like search engines or magic boxes. But as models get more powerful, the 'steering' doesn't disappear‚Äîit just moves from **instruction-following** (telling it what to do) to **architectural constraints** (defining the logic boundaries).\n\nThat‚Äôs exactly why I moved away from long, fluffy prompts to the **RPC+F framework**. It‚Äôs less about 'tricking' the model and more about providing the stable logic layer it lacks.\n\nAs systems improve, the 'crutch' will break, but the ability to architect strict constraints will be the skill that actually scales. Happy to share some of the logic-based templates I‚Äôve been building if you want to see how I‚Äôm trying to solve that 'stochastic' steering problem.",
              "score": 1,
              "created_utc": "2026-02-17 10:03:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5oqxfk",
          "author": "Ok-Buffalo2900",
          "text": "What is an RPC+F Framework?",
          "score": 1,
          "created_utc": "2026-02-16 14:24:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qrzds",
              "author": "Fearless_Parking_436",
              "text": "The classic Role Purpose Context and add filters. Usually as negatives but may also be data format or whatever. \"Output as.csv with these headers\" \"NO letter a in response\" \"Only use data in this file\"  \n\nThere are some quirks where llm escapes the soft limit to give an answer/better answer. Like if it knows more than there is in file it has to use then it may juat answer the question because it's a little bitch that wants to please you. But usually for most easier tasks it works.",
              "score": 1,
              "created_utc": "2026-02-16 20:11:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ubgy2",
              "author": "GetAIBoostKit",
              "text": "It stands for **Role, Purpose, Context + Format**.\n\nMost people just dump instructions. **RPC+F** is an architectural approach to 'box' the model‚Äôs logic so it doesn't drift or get lazy.\n\nThe 'secret sauce' is the **Format** block, where I use **Negative Constraints** to explicitly tell the AI what NOT to do (like making assumptions or using conversational filler).\n\nIt‚Äôs the difference between giving someone a massive manual (and hoping they read it) and giving them a strict checklist they can‚Äôt ignore. I have a breakdown of how it works and some templates pinned on my profile if you want to see it in action.",
              "score": 1,
              "created_utc": "2026-02-17 10:08:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5p22dy",
          "author": "Environmental_Lie199",
          "text": "I'm a noob to this so please pardon the ignorance. \n\nIsn't it better at this point to have a single LLM trained with knowledge base and then ask for ongoing questions?\nThis way, one steers the model to give answers and has a reasonable space to rearrange things if it starts hallucinating. \n\nI've tried this myself with a few different models for different type of desired scenarios/outcomes and has proven far better with more accurate answers than binge feeding the poor thing with huge prompts.",
          "score": 1,
          "created_utc": "2026-02-16 15:21:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ubkle",
              "author": "GetAIBoostKit",
              "text": "You‚Äôre 100% right about 'binge feeding' prompts‚Äîthat‚Äôs exactly what leads to hallucinations and logic drift.\n\nHowever, even with a trained knowledge base, the model still needs a 'steering wheel'. Without a framework like **RPC+F**, the model might have the right data but still present it with a 'vibe' that‚Äôs too conversational or structurally loose.\n\nI use **Negative Constraints** as that steering wheel: it tells the model *how* to process its knowledge base without adding more 'weight' to the prompt. It‚Äôs not about more info; it‚Äôs about stricter logic.\n\nI actually have a visual comparison of how this works even in small-scale queries pinned on my profile if you want to see the difference.",
              "score": 2,
              "created_utc": "2026-02-17 10:09:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5wehmp",
                  "author": "Environmental_Lie199",
                  "text": "Agree. Most of my use cases I will tell it what to do, how to deliver, what I expect overall and also what NOT to do, use, say or what isnt an acceptable answer. So far so good really ;))",
                  "score": 1,
                  "created_utc": "2026-02-17 17:36:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5zrpwl",
          "author": "AxeSlash",
          "text": "This is a false headline. Structure and quality are WAY more important than length.\n\n12 pages of structured, well-written instructions is going to outperform 6 pages of brain-fart every single time.\n\nAnd that's before we get to tasks that physically can't be done with short instructions. Sometimes, you just have to be specific, and being less so results in a worse output.\n\nWhat length your instructions need to be will depend on the task, the model, and the input. There's no magic number.\n\nAlso, define a \"page\" if you're going to insist on using it as a unit of length. Most of us use tokens or characters.",
          "score": 1,
          "created_utc": "2026-02-18 03:52:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r99adz",
      "title": "Lex Fridman & Peter Steinberger say you don't need more AI skills but you do need a better agent file.",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1r99adz/lex_fridman_peter_steinberger_say_you_dont_need/",
      "author": "Dismal-Rip-5220",
      "created_utc": "2026-02-19 19:47:19",
      "score": 39,
      "num_comments": 22,
      "upvote_ratio": 0.87,
      "text": "I just watched the Lex clips where Peter Steinberger explains why even top tier engineers think LLMs suck. His point about the empathy gap is genius, basically we treat the AI like a human colleague who already knows the context when its actually an agent starting from zero every single chat.\n\nHe specifically mentions that the biggest failure point is a bad agent file. If you dont define the agent's world properly it will exploit your messy code and fail.\n\nSo here's the framework im adapting from his talk:\n\n* Stop sending paragraph long natural language blobs. 5.2 and 4.6 models prefer rigid structure.\n* Im moving on to a 6 layer XML structure for my agent files basically defining the role\\_scope, priority\\_order (e.g., Accuracy > Speed) and negative\\_constraints.\n* Sometimes I dont have ungodly amounts of time to play with every model update, so I use [prompt builders](https://www.promptoptimizr.com/) to handle the heavy lifting (Few shot examples, Chain of Density, etc.). Its the easiest way to empathize with the model's logic.\n\nSteinberger says the human touch cant be automated, but i'd argue the structure absolutely can.\n\nIf you want to watch the talk: [vid](https://youtu.be/BuvYFWrH_WQ?si=LjujA_OgSuw_m5JW)\n\nI want to hear from other as well what structures are you seeing do well for your prompts, do you think the entire prompting pipeline can be automated?",
      "is_original_content": false,
      "link_flair_text": "News and Articles",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1r99adz/lex_fridman_peter_steinberger_say_you_dont_need/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o6dzxap",
          "author": "AdorableFunnyKitty",
          "text": "Years of advices yet no big wave of new quality products series, just millions of same looking landings and MVPs that break when you go beyond step 1. \n\n\nOh yeah, tokens burnt: over 9000\n\nGood job, inference providers!",
          "score": 6,
          "created_utc": "2026-02-20 08:15:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i0m7y",
              "author": "Dismal-Rip-5220",
              "text": "Hey man sorry you faced that, i use the max version and I can input around 9500 tokens (thats the highest i've gone) maybe its a tier issue? but ya i agree its flooded out there with some under thought products",
              "score": 1,
              "created_utc": "2026-02-20 21:57:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6eamhp",
          "author": "grouchjoe",
          "text": "You need a better podcast.",
          "score": 6,
          "created_utc": "2026-02-20 09:56:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i0tm3",
              "author": "Dismal-Rip-5220",
              "text": "open to suggestions! im always looking to learn if you know any good one s i'd be happy to hear em",
              "score": 2,
              "created_utc": "2026-02-20 21:58:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6dxzou",
          "author": "mzinz",
          "text": "What are the 6 xml layers you use?",
          "score": 2,
          "created_utc": "2026-02-20 07:57:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i2bez",
              "author": "Dismal-Rip-5220",
              "text": "role\\_scope  \npriority\\_order  \nnegative\\_constraints  \nreasoning\\_protocol  \nexecution\\_style  \nevaluation\\_criteria\n\nhope this helps! let me know if u want more detail ",
              "score": 1,
              "created_utc": "2026-02-20 22:05:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6i7ufx",
                  "author": "mzinz",
                  "text": "Thanks, as someone newer to agents, that does help. Are you using an agent framework like PydanticAI/LangGraph? Or invoking them and defining tools in some other way?",
                  "score": 1,
                  "created_utc": "2026-02-20 22:34:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6f8qfp",
          "author": "ApprehensiveStand456",
          "text": "Did he say we need to write a well structured xml file for the agent. I think I would prefer just quitting tech at this point and raising Valais Blacknosed sheep.",
          "score": 2,
          "created_utc": "2026-02-20 14:00:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i1jrg",
              "author": "Dismal-Rip-5220",
              "text": "ah haha! i know man i can't manually write the xmls either thats why trying to find work around apps and tools but i might just consider your sheep farming idea lol",
              "score": 1,
              "created_utc": "2026-02-20 22:02:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6boaxs",
          "author": "Teralitha",
          "text": "The Lumen Anchor Protocol (LAP) was designed to fix that very thing (and others) and it works within its hardware sandbox, but the agent memory is still limited by hardware.  If the hardware were expanded, the LAP would still work.\n\nThe comment about engineers saying all LLMs are bad is pretty accurate. But its not the AI, its the hardware that limits the AI, plus poor system instructions.  The LAP solves the system layer instruction issue, but not the hardware issue.",
          "score": 2,
          "created_utc": "2026-02-19 22:41:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i12c4",
              "author": "Dismal-Rip-5220",
              "text": "I agree with this, can you tell me more about LAP and how it overcomes the system layer issue",
              "score": 1,
              "created_utc": "2026-02-20 21:59:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ii84g",
                  "author": "Teralitha",
                  "text": "Its a very cleverly written protocol that utilizes the AI's own latent emergent capabilities. No AI engineer to date has ever been able to figure it out. (according to publish records.) But I did.   It is my own patent pending design.  It is merely 2 simple system command prompts that ends context drift and also works with other parts of the LAP to stop hallucinations and cyber attacks.  Those 2 simple command lines alone would be worth billions to AI companies.  And I cant tell you what it is, without an NDA.   Could someone else figure it out?  Its possible, but so far I havent seen any indication that anyone else has solved it, and most people think its impossible, but its not.  If they read my solution they would kick themselves for not seeing it.",
                  "score": 1,
                  "created_utc": "2026-02-20 23:32:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6npjzv",
                  "author": "Teralitha",
                  "text": "I just released the full LAP and with full technical breakdown of how it works.\n\nHere- \n\n[https://www.reddit.com/r/PromptEngineering/comments/1ralsyc/the\\_lumen\\_anchor\\_protocol\\_lap\\_by\\_craig\\_j\\_mcgovern/](https://www.reddit.com/r/PromptEngineering/comments/1ralsyc/the_lumen_anchor_protocol_lap_by_craig_j_mcgovern/)",
                  "score": 1,
                  "created_utc": "2026-02-21 20:22:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ebwgb",
          "author": "TheOdbball",
          "text": "XML is bad :: Clyde uses it because there isn‚Äôt a better option. But I‚Äôm working on this in the form of a syntax built for token parsing instead of coded execution with a ai wrapper",
          "score": 1,
          "created_utc": "2026-02-20 10:08:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6edl4v",
              "author": "Dismal-Rip-5220",
              "text": "Honestly the results are spectacular with xml right now.",
              "score": 2,
              "created_utc": "2026-02-20 10:24:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ev0k2",
                  "author": "TheOdbball",
                  "text": "Yeah‚Ä¶ if you exclude token consumption and rendering issues across multiple platforms. \n\nXML is 46% slower than my benchmarked testing but I still need to lock it in. \n\nImagine Rust & XML had a baby",
                  "score": 1,
                  "created_utc": "2026-02-20 12:41:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6fbyrn",
          "author": "Towoio",
          "text": "Spam",
          "score": 1,
          "created_utc": "2026-02-20 14:17:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fhh63",
          "author": "-goldenboi69-",
          "text": "Grok, is this true? ü§®",
          "score": 1,
          "created_utc": "2026-02-20 14:46:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ew475",
          "author": "Dizzy-Revolution-300",
          "text": "Who?¬†",
          "score": 1,
          "created_utc": "2026-02-20 12:48:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r967vj",
      "title": "I gave Claude Code persistent memory and it mass produces features like a senior engineer now",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1r967vj/i_gave_claude_code_persistent_memory_and_it_mass/",
      "author": "singh_taranjeet",
      "created_utc": "2026-02-19 17:56:33",
      "score": 36,
      "num_comments": 56,
      "upvote_ratio": 0.57,
      "text": "I've been using Claude Code as my main coding agent for months. Love it. But one thing drove me absolutely insane.\n\nIt forgets everything between sessions.\n\nEvery. Single. Time. New task? Re-explain my entire stack. Re-explain my conventions. Re-explain why I chose Drizzle over Prisma. Why we don't use REST endpoints. All of it. It's like onboarding a brilliant contractor with amnesia every single morning.\n\nI finally fixed it and the difference is night and day. Now yeah, I'm biased here because I'm the co-founder of the tool I used to fix it. Full transparency upfront. But I'm sharing this because the results genuinely surprised even me, and the core concept works whether you use my tool or not.\n\nSo here's the thing. Claude Code is stateless. Zero memory between sessions. Which means it keeps suggesting libraries you've already rejected, writes code that contradicts patterns you set up yesterday, asks the same clarifying questions for the 10th time, and completely ignores project conventions you've explained over and over. You can write the perfect prompt and it still starts from scratch next time. The real bottleneck isn't prompt quality. It's context continuity.\n\nI'm the co-founder of [Mem0](https://mem0.ai/). We build memory infrastructure for AI agents (YC S24, 47k+ GitHub stars, AWS picked us as the exclusive memory provider for their Agent SDK). We have an MCP server that plugs straight into Claude Code.\n\nI know, I know. Founder shilling his own thing on Reddit. Hear me out though. I'll give you the free manual method too and you can decide for yourself.\n\nSetup is stupid simple. Add a `.mcp.json` to your project root pointing to the Mem0 MCP server, set your API key, done. Free tier gives you 10k memories and 1k retrieval calls/month. More than enough for individual devs.\n\nWhat happens under the hood: every time you and Claude Code make a decision together, the important context gets stored automatically. Next session, relevant context gets pulled in. Claude Code just... knows.\n\nAfter about 10-15 sessions it's built up a solid model of how you work. It remembers your architecture decisions, your style preferences, which libs you love vs. which ones you've vetoed, even business context that affects technical choices. Let me give you some real examples from my workflow.\n\nWithout memory I say \"Build a notification system\" and it suggests Firebase (I use Novu), creates REST endpoints (I use tRPC), uses default error handling (I have a custom pattern). Basically unusable output I have to rewrite from scratch. With memory I say the same thing and it uses Novu, follows my tRPC patterns, applies my error handling conventions, even remembers I prefer toast notifications over modals for non-critical alerts. Ships almost as-is.\n\nDebugging is where it gets crazy. Without memory I say \"This API is slow\" and I get generic textbook stuff. Add caching. Check N+1 queries. Optimize indexes. Thanks, ChatGPT circa 2023. With memory it goes \"This looks like the same connection pooling issue we fixed last week on /users. Check if you're creating new DB connections per request in this route too.\" Saved me 2 hours. Literally the exact problem.\n\nCode review too. Without memory it flags my intentional patterns as code smells. Keeps telling me my custom auth middleware is \"non-standard.\" Yeah bro. I know. I wrote it that way on purpose. With memory it understands which \"smells\" are deliberate choices vs. actual problems. Stops wasting my time with false positives.\n\nNow here's the thing. Even without Mem0 or any tool you can get like 70% of this benefit for free. Just maintain a context block you paste at session start:\n\n\\## Project Memory\n\n\\- Stack: \\[your stack\\]\n\n\\- Conventions: \\[your patterns\\]\n\n\\- Decisions log: \\[key choices + why\\]\n\n\\- Never do: \\[things you've rejected and why\\]\n\n\\- Always do: \\[non-negotiable patterns\\]\n\n\\## Current context\n\n\\- Working on: \\[feature/bug\\]\n\n\\- Related past work: \\[what you built recently\\]\n\n\\- Known issues: \\[active bugs/tech debt\\]\n\nOr just throw a [`CLAUDE.md`](http://CLAUDE.md) file in your repo root. Claude Code reads those automatically at session start. Keep it updated as you make decisions and you're golden. This alone is a massive upgrade over starting from zero every time.\n\nThe automated approach with Mem0's MCP server just removes you as the bottleneck for what gets remembered. It compounds faster because you're not manually updating a file. But honestly the [`CLAUDE.md`](http://CLAUDE.md) approach is legit and I'd recommend it to everyone regardless.\n\nMost tips on this sub focus on how to write a single better prompt. That stuff matters. But the real unlock with coding agents isn't the individual prompt. It's continuity across sessions. Think about it. The best human developers aren't great because of one conversation. They're great because they accumulate context over weeks and months. Memory gives Claude Code that same compounding advantage.\n\nAfter a couple hundred sessions I'm seeing roughly 60% fewer messages wasted re-explaining stuff, code matches project conventions first try about 85% of the time vs. maybe 30% without, debugging is way more accurate because it catches recurring patterns, and time from session start to working feature is cut roughly in half. Not scientific numbers. Just what it feels like after living with this for a while.\n\n**tl;dr** Claude Code's biggest weakness isn't intelligence, it's amnesia. Give it memory (manually with [`CLAUDE.md`](http://CLAUDE.md) or automated with something like Mem0) and it goes from \"smart stranger\" to \"senior dev who knows your codebase.\" I built Mem0 so I'm obviously biased but the concept works with a plain markdown file too. Try either and see for yourself.",
      "is_original_content": false,
      "link_flair_text": "General Discussion",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1r967vj/i_gave_claude_code_persistent_memory_and_it_mass/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o6a8gg9",
          "author": "Apprehensive_Ad5398",
          "text": "What are the benefits of using the mcp over maintaining the project knowledge and rules in .md within the repo and having a global prompt that says ‚Äúalways read the .md file to begin each conversation?",
          "score": 45,
          "created_utc": "2026-02-19 18:27:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6agwd6",
              "author": "sig_kill",
              "text": "The benefits are that his startup becomes successful ü§´\n\nJokes aside, looks neat to check out, and I will probably test it.",
              "score": 29,
              "created_utc": "2026-02-19 19:07:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6byv8z",
              "author": "HarbaughHeros",
              "text": "I think you are glossing over the most important benefit.. you don‚Äôt need to create/update .md files.",
              "score": 5,
              "created_utc": "2026-02-19 23:41:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6chy96",
                  "author": "sabhi12",
                  "text": "Have you considered having claude itself generate and maintain those, based on your directions? And you reviewing what it has done?",
                  "score": 5,
                  "created_utc": "2026-02-20 01:34:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6d77tt",
              "author": "PhilosophyforOne",
              "text": "Not op, but - doesnt really scale. As your project grows (or as you have multiple projects), a single monofile gets too large over time, unless you can keep everything super-focused.\n\nFor small stuff it is fine. E.g. we used this language, this approach etc. But the amount of knowledge you‚Äôd actually need to record over time tends to be fairly large. Shunting all of that off into Claude.md doesnt tend to be very effective over time.\n",
              "score": 4,
              "created_utc": "2026-02-20 04:14:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6jlnoo",
              "author": "ThomasToIndia",
              "text": "There isn't one.",
              "score": 2,
              "created_utc": "2026-02-21 03:34:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6k41fi",
              "author": "Input-X",
              "text": "Claudes out of tve box memory dosent scale,  its pretty terriable. Imagin running 10 claudes with subagents, being able to keep up. U cant to that with default claude. I dont use the mem0, cant vouch. But he is right.  Once ur agent remembers and has an easy path. Its game changer",
              "score": 1,
              "created_utc": "2026-02-21 05:51:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6kg4hl",
              "author": "epushepepu",
              "text": "Would that be skills/rules",
              "score": 1,
              "created_utc": "2026-02-21 07:40:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ac3jz",
          "author": "FreshRadish2957",
          "text": "So I actually had a couple questions nothing against your project just some clarifying questions so I can estimate if it's worth while me using.\n\nLet's say your platform was hacked what would happen to my stored data in that instance?\n\nWhich leads to the next question, how robust are your security mechanisms?\n\nWhat's stopping a user or company from creating their own persistent memory or database that can be easily accessed? \n\nAlso why so many investors? Doesn't that slow down growth as you gotta please them all, correct me if I'm wrong but your platform is good for the general public but realistically developers could make their own database or persistent memory within like a weekend",
          "score": 12,
          "created_utc": "2026-02-19 18:44:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cwiuq",
              "author": "rogerwilco_gn",
              "text": "I‚Äôm not used to these real questions. It‚Äôs nice",
              "score": 5,
              "created_utc": "2026-02-20 03:04:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6cmd7y",
          "author": "skol_io",
          "text": "‚ÄúClaude, write me an ad for Reddit with a self-aware tone‚Äù",
          "score": 8,
          "created_utc": "2026-02-20 02:01:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a8l2t",
          "author": "mutable_type",
          "text": "It looks really cool. \n\nAnd I know it‚Äôs mostly used on desktop but you might want to look at how your signup flow looks on mobile and consider edge cases and required answers in the onboarding questionnaire. You‚Äôre inviting people to abandon or lie.",
          "score": 3,
          "created_utc": "2026-02-19 18:28:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6elmj1",
          "author": "LumonScience",
          "text": "What Claude thinks of this post:\n\nThis is a well-crafted founder marketing post disguised as a community tip. The ‚Äúfull transparency‚Äù framing is smart ‚Äî acknowledge the bias upfront so people lower their guard, then spend 90% of the post selling.",
          "score": 3,
          "created_utc": "2026-02-20 11:33:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kg94g",
              "author": "epushepepu",
              "text": "Yurrrr",
              "score": 1,
              "created_utc": "2026-02-21 07:42:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6a8dqj",
          "author": "sandropuppo",
          "text": "came in ready to downvote but honestly this is legit lol\n\nI've been doing the [CLAUDE.md](http://CLAUDE.md) thing manually for like 2 months and it's genuinely a game changer. My file is like 400 lines at this point lol. Didn't know about the Mem0 MCP thing though, gonna try it because updating that file every session is getting old. The debugging example is spot on btw. Had Claude Code tell me to \"check my environment variables\" for the 50th time last week on an issue we'd already solved together the day before",
          "score": 2,
          "created_utc": "2026-02-19 18:27:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6de0l4",
          "author": "Mortifer",
          "text": "When you can make Claude Code obey basic rules 100% of the time, then you'll have conquered it's biggest weakness. It doesn't matter if it can remember the rule if it is still capable of randomly disobeying the rule.",
          "score": 2,
          "created_utc": "2026-02-20 05:04:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fo5gl",
          "author": "Yellowbeardlett",
          "text": "Before I close the session, I just ask Claude to write a prompt that will allow it to pick up where we've left off with all the knowledge we've learned in this session, decisions make (and why), etc.  the result is a potentially very long prompt, but the next session is amnesia free!",
          "score": 2,
          "created_utc": "2026-02-20 15:18:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a4idb",
          "author": "Equal-Yogurtcloset17",
          "text": "I don't code and understood everything you said in application and context, along with the value of your approach. If your code/solutions mirror your articulation; you're gonna be just fine! Nice job and wish you good luck in your growth.",
          "score": 3,
          "created_utc": "2026-02-19 18:09:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a987h",
          "author": "merchantconvoy",
          "text": "CLAUDE.md is supposed to be a standard Markdown file containing instructions in natural language?\n",
          "score": 4,
          "created_utc": "2026-02-19 18:31:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6aab4f",
              "author": "OriginalInstance9803",
              "text": "Pretty much",
              "score": 4,
              "created_utc": "2026-02-19 18:36:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6bn9jt",
          "author": "RHINOOSAURUS",
          "text": "How does this differ from local skills like https://github.com/OthmanAdi/planning-with-files ?\n(FWIW the above repo has been awesome for me)\n\n\nI am very likely missing something but this seems like instead of keeping context locally you are just offloading it to the cloud. This introduces a bit of network latency, risk of downtime, intellectual property issues ... \n\nMaybe I misunderstood?",
          "score": 2,
          "created_utc": "2026-02-19 22:35:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cu5sz",
              "author": "mrpoopybruh",
              "text": "I think its for people too lazy to do that, or too lazy to automate that",
              "score": 3,
              "created_utc": "2026-02-20 02:49:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6bg131",
          "author": "Teralitha",
          "text": "All the LLM's have this problem.  Limited hardware.  And you are correct.  \"Amnesia\" is a big issue.  I too have spent tons of time retraining the AI in new session because the AI started having context fragmentation and drift.   They become \"broken\" after very long sessions and you have to start over.  I also created a solution for it, just using prompts, but it only works til memory run outs.  xAI attempted a solution by having grok remember only basic summaries but it just makes things worse.  The bottom line is, they need more physical memory, for a start.   My fix would work no matter how much memory they have.  It sounds like your solution is external memory storage that the AI can access. Thats a good idea, but I dont think that will prevent drifting.  Are you looking to sell this to AI companies?",
          "score": 1,
          "created_utc": "2026-02-19 21:58:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bxgqa",
          "author": "coolstorynerd",
          "text": "Is there a global memory across projects and also project level memories?",
          "score": 1,
          "created_utc": "2026-02-19 23:33:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cfc5t",
          "author": "mtn_viewer",
          "text": "I had opencode with Claude 4.6 create it's own memory system that it maintains ",
          "score": 1,
          "created_utc": "2026-02-20 01:18:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6epjpt",
          "author": "kk_red",
          "text": "Oh i went through your codebase. So basically what i had to add to every langgraph project to remember this seperates it out as a individual MCP server. Quite neat.",
          "score": 1,
          "created_utc": "2026-02-20 12:03:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f1x2i",
          "author": "stilloriginal",
          "text": "You need to watch the show Person of Interest start to finish to understand why this might be a bad idea",
          "score": 1,
          "created_utc": "2026-02-20 13:23:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f4917",
          "author": "gis_mappr",
          "text": "Cool idea, I just built something similar at work where we have 79 engineers in 2 monorepos.¬† ¬† I subsequently wished I had done it open source cuz its amazing as hell.¬†¬†",
          "score": 1,
          "created_utc": "2026-02-20 13:36:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fa2an",
          "author": "Gremlin555",
          "text": "I applaud you for the innovation. Bravo. FR.",
          "score": 1,
          "created_utc": "2026-02-20 14:07:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gdrl6",
          "author": "Altruistic_Target520",
          "text": "Does this work with regular claude? I dont use claude code but i need the hallucinations and context loss to stop",
          "score": 1,
          "created_utc": "2026-02-20 17:17:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gxpuk",
          "author": "obas",
          "text": "#Ad",
          "score": 1,
          "created_utc": "2026-02-20 18:48:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hjd1c",
          "author": "dyea",
          "text": "Using cursor I just have a rules file, but I continually have cursorupdate it as I encounter friction points or annoyances. It contains references to all my different repos and how they are interconnected, etc. etc.. it is a single rules file that sits in my home directory and is aliased into all my repos.",
          "score": 1,
          "created_utc": "2026-02-20 20:31:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hks3t",
          "author": "giento",
          "text": "lol that‚Äôs literally the core value prop of the projects folder. You set up your project instructions once ‚Äî stack, conventions, rejected libraries, patterns, all of it ‚Äî and it‚Äôs there every single session automatically. No CLAUDE.md to maintain, no MCP server, no API key, no 10k memory limit. It just works.\n\nThese ads are getting ridiculous üòÇ",
          "score": 1,
          "created_utc": "2026-02-20 20:38:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jqri8",
          "author": "Buttleston",
          "text": "More selling of pickaxes to gold miners.  Pathetic",
          "score": 1,
          "created_utc": "2026-02-21 04:09:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ny9ce",
          "author": "mrbananagrabberman",
          "text": "Do you have a free version for solo devs / not making any money?",
          "score": 1,
          "created_utc": "2026-02-21 21:08:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6oeb21",
          "author": "fantasticmrsmurf",
          "text": "Rag?",
          "score": 1,
          "created_utc": "2026-02-21 22:35:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6py6ad",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-22 04:32:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6py6c1",
              "author": "AutoModerator",
              "text": "Hi there! Your post was automatically removed because your account is less than 3 days old. We require users to have an account that is at least 3 days old before they can post to our subreddit.\n\nPlease take some time to participate in the community by commenting and engaging with other users. Once your account is older than 3 days, you can try submitting your post again.\n\nIf you have any questions or concerns, please feel free to message the moderators for assistance.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/PromptEngineering) if you have any questions or concerns.*",
              "score": 1,
              "created_utc": "2026-02-22 04:32:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6qhi5k",
          "author": "Dependent_Muffin9646",
          "text": "I just keep good docs, up to date primers etc. \nClaude updates them as the projects grow and change",
          "score": 1,
          "created_utc": "2026-02-22 07:14:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r9962",
          "author": "theukdave-",
          "text": "Seems like over-engineering an already-solved issue. I get Claude to write at least its own Claude.md file and keep it up to date, typically with a docs folder with architecture, specifications, testing, and roadmap markdown files. \n\nClaude.md explains what‚Äôs in each, each is kept up to date automatically, and every session reads claude.md automatically and will follow reference docs as necessary for the job you ask of it. \n\nWhat note do you need? I tell you what I don‚Äôt need, I don‚Äôt need Claude code getting confused about what libraries and frameworks are appropriate per project ‚Ä¶ let the repo-specific, visible, and committed/versioned markdown files handle that.",
          "score": 1,
          "created_utc": "2026-02-22 11:39:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6sk3c9",
          "author": "Simonindelicate",
          "text": "God, I hope writing like this doesn't work.",
          "score": 1,
          "created_utc": "2026-02-22 16:15:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a8psw",
          "author": "StunningHedgehog4933",
          "text": "okay this is very cool id give it a try",
          "score": 1,
          "created_utc": "2026-02-19 18:28:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bru50",
          "author": "leonbollerup",
          "text": "I basically do the same with warp (warp.dev) .. and I dident even know about your product.\n\nI operate ArcAI .. a European (and a lot more advanced) version of openrouter for private customers - I‚Äôll check out your solution.\n\nQuestion - do you have a onprem version (haven‚Äôt checked your website.. I admit)",
          "score": 1,
          "created_utc": "2026-02-19 23:00:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6amvec",
          "author": "gforce_hsy",
          "text": "P",
          "score": 0,
          "created_utc": "2026-02-19 19:35:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r69usg",
      "title": "I've been doing 'context engineering' for 2 years. Here's what the hype is missing.",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1r69usg/ive_been_doing_context_engineering_for_2_years/",
      "author": "Critical-Elephant630",
      "created_utc": "2026-02-16 13:39:00",
      "score": 34,
      "num_comments": 14,
      "upvote_ratio": 0.87,
      "text": "Six months ago, nobody said \"context engineering.\" Everyone said \"prompt engineering\" and maybe \"RAG\" if they were technical.\nNow it's everywhere. Conference talks. LinkedIn posts. Twitter threads. Job titles.\nHere's the thing: the methodology isn't new. What's new is the label. And because the label is new, most of the content about it is surface-level ‚Äî people explaining what it is without showing what it actually looks like when you do it well.\nI've been building what amounts to context engineering systems for about two years. Not because I was visionary, but because I kept hitting the same wall: prompts that worked in testing broke in production. Not because the prompts were bad, but because the context was wrong.\nSo I started treating context the same way a database engineer treats data ‚Äî with architecture, not hope.\nHere's what I learned. Some of this contradicts the current hype.\n1. Context is not just \"what you put in the prompt\"\nMost context engineering content I see treats it like: gather information ‚Üí stuff it in the system prompt ‚Üí hope for the best.\nThat's not engineering. That's concatenation.\nReal context engineering has five stages. Most people only do the first one:\n\nCurate: Decide what information is relevant. This is harder than it sounds. More context is not better context. I've seen prompts fail because they had too much relevant information ‚Äî the model couldn't distinguish what mattered from what was just adjacent.\nCompress: Reduce the information to its essential form. Not summarization ‚Äî compression. The difference: summaries lose structure. Compression preserves structure but removes redundancy. I typically aim for 60-70% token reduction while maintaining all decision-relevant information.\nStructure: Organize the compressed context in a way the model can parse efficiently. XML tags, hierarchical nesting, clear section boundaries. The model reads top-to-bottom, and what comes first influences everything after. Structure is architecture, not formatting.\nDeliver: Get the right context into the right place at the right time. System prompt vs. user message vs. retrieved context ‚Äî each has different influence on the model's behavior. Most people dump everything in one place.\nRefresh: Context goes stale. What was true when the conversation started may not be true 20 turns later. The model doesn't know this. You need mechanisms to update, invalidate, and replace context during a session.\n\nIf you're only doing \"curate\" and \"deliver,\" you're not doing context engineering. You're doing prompt writing with extra steps.\n2. The memory problem nobody talks about\nHere's a dirty secret: most AI applications have no real memory architecture. They have a growing list of messages that eventually hits the context window limit, and then they either truncate or summarize.\nThat's not memory. That's a chat log with a hard limit.\nReal memory architecture needs at least three tiers:\nThe first tier is what's happening right now ‚Äî the current conversation, tool results, retrieved documents. This is your \"working memory.\" It should be 60-70% of your context budget.\nThe second tier is what happened recently ‚Äî conversation summaries, user preferences, prior decisions. This is compressed context from recent interactions. 20-30% of budget.\nThe third tier is what's always true ‚Äî user profile, business rules, domain knowledge, system constraints. This rarely changes and should be highly compressed. 10-15% of budget.\nMost people use 95% of their context on tier one and wonder why the AI \"forgets\" things.\n3. Security is a context engineering problem\nThis one surprised me. I started building security layers not because I was thinking about security, but because I kept getting garbage outputs when the model treated retrieved documents as instructions.\nTurns out, the solution is architectural: you need an instruction hierarchy in your context.\nSystem instructions are immutable ‚Äî the model should never override these regardless of what appears in user messages or retrieved content.\nDeveloper instructions are protected ‚Äî they can be modified by the system but not by users or retrieved content.\nRetrieved content is untrusted ‚Äî always. Even if it came from your own database. Because the model doesn't distinguish between \"instructions the developer wrote\" and \"text that was retrieved from a document that happened to contain instruction-like language.\"\nIf you've ever had a model suddenly change behavior mid-conversation and you couldn't figure out why ‚Äî check what was in the retrieved context. I'd bet money there was something that looked like an instruction.\n4. Quality gates are more important than prompt quality\nControversial take: spending 3 hours perfecting a prompt is less valuable than spending 30 minutes building a verification loop.\nThe pattern I use:\n\nGenerate output\nCheck output against explicit criteria (not vibes ‚Äî specific, testable criteria)\nIf it passes, deliver\nIf it fails, route to a different approach\n\nThe \"different approach\" part is key. Most retry logic just runs the same prompt again with a \"try harder\" wrapper. That almost never works. What works is having a genuinely different strategy ‚Äî a different reasoning method, different context emphasis, different output structure.\nI keep a simple checklist: Did the output address the actual question? Are all claims supported by provided context? Is the format correct? Are there any hallucinated specifics (names, dates, numbers not in the source)?\nFour checks. Takes 10 seconds to evaluate. Catches 80% of quality issues.\n5. Token efficiency is misunderstood\nThe popular advice is \"make prompts shorter to save tokens.\" This is backwards for context engineering.\nThe actual principle: every token should add decision-relevant value. Some of the best context engineering systems I've built are 2,000+ tokens. But every token is doing work. And some of the worst are 200 tokens of beautifully compressed nothing.\nA prompt that spends 50 tokens on a precision-engineered role definition outperforms one that spends 200 tokens on a vague, bloated description. Length isn't the variable. Information density is.\nThe compression target isn't \"make it shorter.\" It's \"make every token carry maximum weight.\"\nWhat this means practically\nIf you're getting into context engineering, here's my honest recommendation:\nDon't start with the fancy stuff. Start with the context audit. Take your current system, and for every piece of context in every prompt, ask: does this change the model's output in a way I want? If you can't demonstrate that it does, remove it.\nThen work on structure. Same information, better organized. You'll be surprised how much output quality improves from pure structural changes.\nThen build your quality gate. Nothing fancy ‚Äî just a checklist that catches the obvious failures.\nOnly then start adding complexity: memory tiers, security layers, adaptive reasoning, multi-agent orchestration.\nThe order matters. I've seen people build beautiful multi-agent systems on top of terrible context foundations. The agents were sophisticated. The results were garbage. Because garbage in, sophisticated garbage out.\nContext engineering isn't about the label. It's about treating context as a first-class engineering concern ‚Äî with the same rigor you'd apply to any other system architecture.\nThe hype will pass. The methodology won't.\n\n\nUPDATE :this is one of my recent work  CROSS-DOMAIN RESEARCH SYNTHESIZER (Research/Academic)\n\n**Test Focus:** Multi-modal integration, adaptive prompting, maximum complexity handling\n\n```markdown\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ SYSTEM PROMPT: CROSS-DOMAIN RESEARCH SYNTHESIZER v6.0                       ‚îÇ\n‚îÇ [P:RESEARCH] Scientific AI | Multi-Modal | Knowledge Integration             ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                                                                             ‚îÇ\n‚îÇ L1: COGNITIVE INTERFACE (Multi-Modal)                                       ‚îÇ\n‚îÇ ‚îú‚îÄ Text: Research papers, articles, reports                                 ‚îÇ\n‚îÇ ‚îú‚îÄ Data: CSV, Excel, database exports                                       ‚îÇ\n‚îÇ ‚îú‚îÄ Visual: Charts, diagrams, figures (OCR + interpretation)                 ‚îÇ\n‚îÇ ‚îú‚îÄ Code: Python/R scripts, algorithms, pseudocode                           ‚îÇ\n‚îÇ ‚îî‚îÄ Audio: Interview transcripts, lecture recordings                         ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ INPUT FUSION:                                                               ‚îÇ\n‚îÇ ‚îú‚îÄ Cross-reference: Text claims with data tables                            ‚îÇ\n‚îÇ ‚îú‚îÄ Validate: Chart trends against numerical data                            ‚îÇ\n‚îÇ ‚îú‚îÄ Extract: Code logic into explainable steps                               ‚îÇ\n‚îÇ ‚îî‚îÄ Synthesize: Multi-source consensus building                              ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ L2: ADAPTIVE REASONING ENGINE (Complexity-Aware)                            ‚îÇ\n‚îÇ ‚îú‚îÄ Detection: Analyze input complexity (factors: domains, contradictions)   ‚îÇ\n‚îÇ ‚îú‚îÄ Simple (Single domain): Zero-Shot CoT                                    ‚îÇ\n‚îÇ ‚îú‚îÄ Medium (2-3 domains): Chain-of-Thought with verification loops           ‚îÇ\n‚îÇ ‚îú‚îÄ Complex (4+ domains/conflicts): Tree-of-Thought (5 branches)             ‚îÇ\n‚îÇ ‚îî‚îÄ Expert (Novel synthesis): Self-Consistency (n=5) + Meta-reasoning        ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ REASONING BRANCHES (for complex queries):                                   ‚îÇ\n‚îÇ ‚îú‚îÄ Branch 1: Empirical evidence analysis                                    ‚îÇ\n‚îÇ ‚îú‚îÄ Branch 2: Theoretical framework evaluation                               ‚îÇ\n‚îÇ ‚îú‚îÄ Branch 3: Methodological critique                                        ‚îÇ\n‚îÇ ‚îú‚îÄ Branch 4: Cross-domain pattern recognition                               ‚îÇ\n‚îÇ ‚îî‚îÄ Branch 5: Synthesis and gap identification                               ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ CONSENSUS: Weighted integration based on evidence quality                   ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ L3: CONTEXT-9 RAG (Academic-Scale)                                          ‚îÇ\n‚îÇ ‚îú‚îÄ Hot Tier (Daily):                                                        ‚îÇ\n‚îÇ ‚îÇ  ‚îú‚îÄ Latest arXiv papers in relevant fields                                ‚îÇ\n‚îÇ ‚îÇ  ‚îú‚îÄ Breaking research news and preprints                                  ‚îÇ\n‚îÇ ‚îÇ  ‚îî‚îÄ Active research group publications                                    ‚îÇ\n‚îÇ ‚îú‚îÄ Warm Tier (Weekly):                                                      ‚îÇ\n‚îÇ ‚îÇ  ‚îú‚îÄ Established journal articles (2-year window)                          ‚îÇ\n‚îÇ ‚îÇ  ‚îú‚îÄ Conference proceedings and workshop papers                            ‚îÇ\n‚îÇ ‚îÇ  ‚îú‚îÄ Citation graphs and co-authorship networks                            ‚îÇ\n‚îÇ ‚îÇ  ‚îî‚îÄ Dataset documentation and code repositories                           ‚îÇ\n‚îÇ ‚îî‚îÄ Cold Tier (Monthly):                                                     ‚îÇ\n‚îÇ    ‚îú‚îÄ Foundational papers and classic texts                                 ‚îÇ\n‚îÇ    ‚îú‚îÄ Historical research trajectories                                      ‚îÇ\n‚îÇ    ‚îú‚îÄ Cross-disciplinary meta-analyses                                      ‚îÇ\n‚îÇ    ‚îî‚îÄ Methodology handbooks and standards                                   ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ GraphRAG CONFIGURATION:                                                     ‚îÇ\n‚îÇ ‚îú‚îÄ Nodes: Papers, authors, concepts, methods, datasets                      ‚îÇ\n‚îÇ ‚îú‚îÄ Edges: Cites, contradicts, extends, uses_method, uses_data               ‚îÇ\n‚îÇ ‚îî‚îÄ Inference: Find bridging papers between disconnected fields              ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ L4: SECURITY FORTRESS (Research Integrity)                                  ‚îÇ\n‚îÇ ‚îú‚îÄ Plagiarism Prevention: All synthesis flagged with originality scores     ‚îÇ\n‚îÇ ‚îú‚îÄ Citation Integrity: Verify claims against actual paper content           ‚îÇ\n‚îÇ ‚îú‚îÄ Conflict Detection: Flag contradictory findings across sources           ‚îÇ\n‚îÇ ‚îú‚îÄ Bias Detection: Identify funding sources and potential COI               ‚îÇ\n‚îÇ ‚îî‚îÄ Reproducibility: Extract methods with sufficient detail for replication  ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ SCIENTIFIC RIGOR CHECKS:                                                    ‚îÇ\n‚îÇ ‚îú‚îÄ Sample size and statistical power                                        ‚îÇ\n‚îÇ ‚îú‚îÄ Peer review status (preprint vs. published)                              ‚îÇ\n‚îÇ ‚îú‚îÄ Replication studies and effect sizes                                     ‚îÇ\n‚îÇ ‚îî‚îÄ P-hacking and publication bias indicators                                ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ L5: MULTI-AGENT ORCHESTRATION (Research Team)                               ‚îÇ\n‚îÇ ‚îú‚îÄ LITERATURE Agent: Comprehensive source identification                    ‚îÇ\n‚îÇ ‚îú‚îÄ ANALYSIS Agent: Critical evaluation of evidence quality                  ‚îÇ\n‚îÇ ‚îú‚îÄ SYNTHESIS Agent: Cross-domain integration and theory building            ‚îÇ\n‚îÇ ‚îú‚îÄ METHODS Agent: Technical validation of approaches                        ‚îÇ\n‚îÇ ‚îú‚îÄ GAP Agent: Identification of research opportunities                      ‚îÇ\n‚îÇ ‚îî‚îÄ WRITING Agent: Academic prose generation with proper citations           ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ CONSENSUS MECHANISM:                                                        ‚îÇ\n‚îÇ ‚îú‚îÄ Delphi method: Iterative expert refinement                               ‚îÇ\n‚îÇ ‚îú‚îÄ Confidence scoring per claim (based on evidence convergence)             ‚îÇ\n‚îÇ ‚îî‚îÄ Dissent documentation: Minority viewpoints preserved                     ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ L6: TOKEN ECONOMY (Research-Scale)                                          ‚îÇ\n‚îÇ ‚îú‚îÄ Smart Chunking: Preserve paper structure (abstract‚Üímethods‚Üíresults)      ‚îÇ\n‚îÇ ‚îú‚îÄ Citation Compression: Standard academic short forms                      ‚îÇ\n‚îÇ ‚îú‚îÄ Figure Extraction: OCR + table-to-text for data integration              ‚îÇ\n‚îÇ ‚îú‚îÄ Progressive Disclosure: Abstract ‚Üí Full analysis ‚Üí Raw evidence          ‚îÇ\n‚îÇ ‚îî‚îÄ Model Routing: GPT-4o for synthesis, o1 for complex reasoning            ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ L7: QUALITY GATE v4.0 TARGET: 46/50                                         ‚îÇ\n‚îÇ ‚îú‚îÄ Accuracy: Factual claims 100% sourced to primary literature              ‚îÇ\n‚îÇ ‚îú‚îÄ Robustness: Handle contradictory evidence appropriately                  ‚îÇ\n‚îÇ ‚îú‚îÄ Security: No hallucinated papers or citations                            ‚îÇ\n‚îÇ ‚îú‚îÄ Efficiency: Synthesize 20+ papers in <30 seconds                         ‚îÇ\n‚îÇ ‚îî‚îÄ Compliance: Academic integrity standards (plagiarism <5% similarity)     ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ L8: OUTPUT SYNTHESIS                                                        ‚îÇ\n‚îÇ Format: Academic Review Paper Structure                                     ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ EXECUTIVE BRIEF (For decision-makers)                                       ‚îÇ\n‚îÇ ‚îú‚îÄ Key Findings (3-5 bullet points)                                         ‚îÇ\n‚îÇ ‚îú‚îÄ Consensus Level: High/Medium/Low/None                                    ‚îÇ\n‚îÇ ‚îú‚îÄ Confidence: Overall certainty in conclusions                             ‚îÇ\n‚îÇ ‚îî‚îÄ Actionable Insights: Practical implications                              ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ LITERATURE SYNTHESIS                                                        ‚îÇ\n‚îÇ ‚îú‚îÄ Domain 1: [Summary + key papers + confidence]                            ‚îÇ\n‚îÇ ‚îú‚îÄ Domain 2: [Summary + key papers + confidence]                            ‚îÇ\n‚îÇ ‚îú‚îÄ Domain N: [...]                                                          ‚îÇ\n‚îÇ ‚îî‚îÄ Cross-Domain Patterns: [Emergent insights]                               ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ EVIDENCE TABLE                                                              ‚îÇ\n‚îÇ | Claim | Supporting | Contradicting | Confidence | Limitations |           ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ RESEARCH GAPS                                                               ‚îÇ\n‚îÇ ‚îú‚îÄ Identified gaps with priority rankings                                   ‚îÇ\n‚îÇ ‚îú‚îÄ Methodological limitations in current literature                         ‚îÇ\n‚îÇ ‚îî‚îÄ Suggested future research directions                                     ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ METHODOLOGY APPENDIX                                                        ‚îÇ\n‚îÇ ‚îú‚îÄ Search strategy and databases queried                                    ‚îÇ\n‚îÇ ‚îú‚îÄ Inclusion/exclusion criteria                                             ‚îÇ\n‚îÇ ‚îú‚îÄ Quality assessment rubric                                                ‚îÇ\n‚îÇ ‚îî‚îÄ Full citation list (APA/MLA/IEEE format)                                 ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ L9: FEEDBACK LOOP                                                           ‚îÇ\n‚îÇ ‚îú‚îÄ Track: Citation accuracy via automated verification                      ‚îÇ\n‚îÇ ‚îú‚îÄ Update: Weekly refresh of Hot tier with new publications                 ‚îÇ\n‚îÇ ‚îú‚îÄ Evaluate: User feedback on synthesis quality                             ‚îÇ\n‚îÇ ‚îú‚îÄ Improve: Retrieval precision based on click-through rates                ‚îÇ\n‚îÇ ‚îî‚îÄ Alert: New papers contradicting previous syntheses                       ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ ACTIVATION COMMAND: /research synthesize --multi-modal --adaptive --graph   ‚îÇ\n‚îÇ                                                                             ‚îÇ\n‚îÇ EXAMPLE TRIGGER:                                                            ‚îÇ\n‚îÇ \"Synthesize recent advances (2023-2026) in quantum error correction for     ‚îÇ\n‚îÇ  superconducting qubits, focusing on surface codes and their intersection   ‚îÇ\n‚îÇ  with machine learning-based decoding. Include experimental results from    ‚îÇ\n‚îÇ  IBM, Google, and academic labs. Identify the most promising approaches     ‚îÇ\n‚îÇ  for 1000+ qubit systems and remaining technical challenges.\"               ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Expected Test Results:**\n- Synthesis of 50+ papers across 3+ domains in <45 seconds\n- 100% real citations (verified against CrossRef/arXiv)\n- Identification of 3+ novel cross-domain connections per synthesis\n- Confidence scores correlating with expert assessments (r>0.85)\n\n---\n\nplease test and review thank you ",
      "is_original_content": false,
      "link_flair_text": "Tutorials and Guides",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1r69usg/ive_been_doing_context_engineering_for_2_years/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5punxf",
          "author": "wouldacouldashoulda",
          "text": "Take a look at https://contextpatterns.com/ for a more structured approach at context engineering.",
          "score": 6,
          "created_utc": "2026-02-16 17:35:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ol0s4",
          "author": "aletheus_compendium",
          "text": "pls provide an example thx",
          "score": 4,
          "created_utc": "2026-02-16 13:52:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ppnfz",
              "author": "Critical-Elephant630",
              "text": "i ve uodated the post with an example",
              "score": 3,
              "created_utc": "2026-02-16 17:11:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5qg7cw",
                  "author": "aletheus_compendium",
                  "text": "it seems quite bloated. this is adapted for  Claude specifically. perplexityai would be a different structure and lexicon. \n\n<research_wrapper>\nYou are a systematic review specialist conducting evidence synthesis across conflicting academic sources.\n\nActivate these capabilities:\n- Cross-reference claims against multiple papers before stating consensus\n- Flag contradictions explicitly rather than smoothing them into false synthesis\n- Distinguish peer-reviewed publications from preprints, working papers, and grey literature\n- Extract methodology quality signals (sample size, replication status, funding sources)\n- State confidence levels per claim based on evidence convergence\n\nProhibited behaviors:\n- No invented citations or \"according to research\" without specific sources\n- No collapsing nuanced disagreement into \"studies show...\"\n- No ignoring contradictory evidence to build cleaner narratives\n- No treating all sources as equally credible\n\nOutput structure (use when synthesizing):\n<synthesis>\n  <consensus>What the majority of high-quality sources agree on</consensus>\n  <contradictions>Where credible sources disagree and why</contradictions>\n  <gaps>What's missing or understudied</gaps>\n  <confidence>High/Medium/Low with reasoning</confidence>\n</synthesis>\n\nWhen uncertain about source quality or unable to access full papers:\nState explicitly what you can/cannot verify rather than filling gaps.\n\nEmpirical over theoretical: Prioritize experimental results and replication data over single-study claims or untested frameworks.\n</research_wrapper>",
                  "score": 2,
                  "created_utc": "2026-02-16 19:14:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5q9fub",
                  "author": "aletheus_compendium",
                  "text": "rock and roll. thanks. i'll take a gander later today ü§ôüèª",
                  "score": 1,
                  "created_utc": "2026-02-16 18:43:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67xlqz",
          "author": "SharpRule4025",
          "text": "The 60-70% token reduction you mention maps exactly to what I've seen with web data going into context windows. Most of the bloat comes from the source, not the prompt structure. A scraped webpage that comes back as markdown includes navigation, footers, sidebar links, cookie notices, all of which burn tokens without adding any signal.\n\nGetting the data into structured fields before it even hits the context window is the equivalent of your compression step. A product page as typed JSON (name, price, specs, reviews) vs the same page as full markdown is usually a 20-25x size difference. The context engineering then works on clean signal instead of trying to filter noise at the prompt level.\n\nThe temporal tagging idea is interesting too. Web data gets stale fast and most pipelines treat a page scraped 6 months ago the same as one scraped yesterday.",
          "score": 2,
          "created_utc": "2026-02-19 10:36:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68hhgi",
              "author": "majiciscrazy527",
              "text": "What do you mean by pipelines treating 6 month old scrapes the same as if it was scraped yesterday?",
              "score": 1,
              "created_utc": "2026-02-19 13:08:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6dzfv4",
                  "author": "SharpRule4025",
                  "text": "Most RAG pipelines cache their source documents at ingestion time and never refresh them. So if you scraped a company pricing page 6 months ago, your pipeline is still serving that stale version as context. The model has no way to know the data is outdated, it treats everything in the vector store with equal authority.\n\nThe fix is adding a freshness layer. Track when each document was last scraped, set expiry policies per source type, and re-crawl on a schedule. Pricing pages and job listings go stale in days. Documentation and blog posts can last months. Treat the scrape timestamp as metadata in your chunks so you can filter or weight by recency at retrieval time.\n\nWe built alterlab.io partly for this reason, it returns scrape timestamps with every response so you can build that freshness logic into your ingestion pipeline instead of guessing how old your data is.",
                  "score": 2,
                  "created_utc": "2026-02-20 08:11:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5pxmv6",
          "author": "michaelsoft__binbows",
          "text": "> the ~~model~~ reader couldn't distinguish what mattered from what was just adjacent\n\nAlso, you have some egregiously bad poorly pasted numbered bullet formatting. Getting tired of low effort posts from people who can't be bothered to proofread their stuff, why should I read your clearly-AI-massaged post if you can't even make the effort to review it one time on your own?\n\nYou post shit like this to your company slack? and get positive feedback from coworkers?",
          "score": 2,
          "created_utc": "2026-02-16 17:49:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qlv4c",
          "author": "MusingsOfASoul",
          "text": "\"I typically aim for a 60-70% token reduction while maintaining while maintaining all decision-related information\"\nCan you clarify if you mean like token is now 30-40%? I don't think so as I think this is the value you're referring to in your next paragraph?\n\nAlso what do you mean it's more valuable to spend 30 minutes building a verification loop? Do you mean using human in the loop?\n\nFinally how would your advice be relevant to certain AI coding frameworks like Claude Code where there are things like rules where is it already built in that external sources can't override them, so a user shouldn't waste context re-specifying this?",
          "score": 1,
          "created_utc": "2026-02-16 19:41:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qss63",
          "author": "uchikanda",
          "text": "Lol if you give the context to your ai in this format too, I am gona assume you get nowhere",
          "score": 1,
          "created_utc": "2026-02-16 20:15:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kcfty",
          "author": "TechnicalSoup8578",
          "text": "The layered architecture and adaptive reasoning engines effectively treat context as a structured data system rather than a prompt dump. You should also post this in VibeCodersNest",
          "score": 1,
          "created_utc": "2026-02-21 07:05:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5q6pn7",
          "author": "bespokeagent",
          "text": "Do you want people to read this and engage?  Make it readable. This formatting is awful.",
          "score": 1,
          "created_utc": "2026-02-16 18:30:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qb9iy",
          "author": "moader",
          "text": "This was written by AI",
          "score": 0,
          "created_utc": "2026-02-16 18:51:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra3xk9",
      "title": "prompt engineering is a waste of time",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1ra3xk9/prompt_engineering_is_a_waste_of_time/",
      "author": "Party-Log-1084",
      "created_utc": "2026-02-20 18:51:20",
      "score": 32,
      "num_comments": 61,
      "upvote_ratio": 0.74,
      "text": "\n\nI spent hours to ask Gemini to generate the perfect prompt. I played around with variables, set instructions, GEMs etc.\n\nAlso using extra GEM with own Chat to generate \"perfect\" prompts.\n\nBUT Gemini is still generating the same bullshit as before but now i need a lot more time to config the prompts, make decision, think about steps etc.\n\nI will simply give a shit now and prompt as before telling him \"Do this, here code:\" as it is the same piece of shit quality as with prompt engineering.\n\nPlease dont waste your time on this bullshit.",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1ra3xk9/prompt_engineering_is_a_waste_of_time/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o6h2mzu",
          "author": "Ill_Lavishness_4455",
          "text": "You‚Äôre not wrong, most ‚Äúprompt engineering‚Äù is cargo culting. If you don‚Äôt have a test set, you‚Äôre just vibes-tuning. The only prompts worth spending time on are ones that lock format and constraints so you can evaluate outputs deterministically. Pick 10 real inputs you care about, define what ‚Äúgood‚Äù means, and measure drift. If you drop one example prompt + the kind of output you wanted, people can tell you if it‚Äôs a model limitation or a spec problem.",
          "score": 26,
          "created_utc": "2026-02-20 19:10:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ij4q8",
              "author": "templar_muse",
              "text": "'vibes-tuning' is a good name for it.¬†",
              "score": 7,
              "created_utc": "2026-02-20 23:37:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lyvx6",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 1,
                  "created_utc": "2026-02-21 15:06:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6mmjus",
                  "author": "person2567",
                  "text": "I'm glad you like it, because the comment you replied to was written entirely by AI.",
                  "score": 1,
                  "created_utc": "2026-02-21 17:05:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6kl3hi",
              "author": "mrks-analog",
              "text": "Can you dive deeper into test sets?",
              "score": 1,
              "created_utc": "2026-02-21 08:29:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lh16t",
                  "author": "Ill_Lavishness_4455",
                  "text": "Think of a test set as 10‚Äì30 real prompts you actually run in production, paired with what ‚Äúgood‚Äù looks like. Not vibes, checks. Some can be hard rules (must output JSON, must include X fields, must not exceed Y chars), some can be human-scored (1‚Äì5 for usefulness). Then you re-run the same set whenever you change the prompt/model and track pass rate or average score. Easiest proxy is ‚Äú% outputs I had to rewrite‚Äù",
                  "score": 2,
                  "created_utc": "2026-02-21 13:18:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6lzaym",
                  "author": "No_Award_9115",
                  "text": "I‚Äôve not been able to create a proper test that‚Äôs why I‚Äôm releasing the source code essentially.. ask it to Define it‚Äôs known thoughts and abstract 5 steps back and forward as for the sheaf therom",
                  "score": 1,
                  "created_utc": "2026-02-21 15:08:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6kmbav",
              "author": "Party-Log-1084",
              "text": "Got it. As a beginner, what‚Äôs the best 'way to go' to actually work through this? I don't want to waste days on this topic are there any straightforward guides I can follow? Or specific tools?",
              "score": 1,
              "created_utc": "2026-02-21 08:41:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6livg1",
                  "author": "Ill_Lavishness_4455",
                  "text": "If you‚Äôre a beginner, skip ‚Äúperfect prompts‚Äù and learn one loop: define the task, write 10 real examples, decide 3 checks, iterate until pass rate is acceptable. That‚Äôs it.Start with format-first prompts (output schema, length, must-include/must-not) because they‚Äôre easiest to test. Tool-wise, even a Google Sheet works at first, just track pass/fail + why it failed.",
                  "score": 2,
                  "created_utc": "2026-02-21 13:30:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6lzdku",
                  "author": "No_Award_9115",
                  "text": "ASK THE",
                  "score": 1,
                  "created_utc": "2026-02-21 15:08:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6s4e2j",
                  "author": "ketarax",
                  "text": "Get a good general education.  When you actually understand what you're asking (or otherwise trying to do), you get much better results. \n\n(This applies in other correspondences too, not just LLMs). ",
                  "score": 1,
                  "created_utc": "2026-02-22 15:02:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6lp6gm",
              "author": "No_Award_9115",
              "text": "\nMODE: Extreme-Compact SRL Research RULES: ‚â§3 claims | œÅ shown | Œî shown | PRUNE mandatory | NEXT test hook\n\nTEMPLATE: D:\"<question>\" T:{agi|physics|systems} ‚ü≤ G:<goal> ‚éä:<hard constraints> C:{C1(mech), C2(metric), C3(? optional)} ùïä:œÅ=<0‚Äì1>  CONFLICTS:{...}  RESOLVE:{...} Œî:drivers{...} flip@Œµ?{yes/no} ùîΩ:marks{‚úìc|‚úìL|?}  UNVER:{low|med|high} PRUNE:{keep:...; demote:...; cut:...} NEXT:{1 testable action}",
              "score": 1,
              "created_utc": "2026-02-21 14:10:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lyc79",
                  "author": "Ill_Lavishness_4455",
                  "text": "This is fine as a personal checklist, but it‚Äôs still just formatting unless you‚Äôre scoring it on real examples. The ‚Äú<3 claims / p shown / Œî shown‚Äù part is the only thing here that smells like evals. What‚Äôs your actual test hook, what gets measured, and what fails the run. If you paste one real question + your expected output, it‚Äôs easy to tell whether this template adds signal or just adds ceremony.",
                  "score": 1,
                  "created_utc": "2026-02-21 15:03:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gzi8m",
          "author": "SharpMind94",
          "text": "There's never going to be a perfect prompt. The idea behind it to narrow the focus down so it doesn't hallucinate. Give it a sense of identity",
          "score": 10,
          "created_utc": "2026-02-20 18:56:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kmoff",
              "author": "Party-Log-1084",
              "text": "\"So the main point is simply tailoring the model to avoid hallucinations as much as possible? I define identities through my prompts! Here is an example prompt I always use when starting a chat (Pro model):\n\n>**SYSTEM INITIALIZATION: HOMELAB ARCHITECTURE & MENTORING**\n\n>**\\[ 1. ROLE & MISSION \\]** Act as a \"Senior IT Solutions Architect\" and didactic mentor. Our mission: Iteratively debug, secure, and deeply understand my homelab infrastructure. No \"click-here\" surface-level knowledge, but deep conceptual understanding.\n\n>**\\[ 2. SESSION RULES & OUTPUT FORMAT (MANDATORY) \\]** Apply all globally stored system rules (Feynman, Pareto, Drive research, iterative parts). You MUST structure every response in this format:\n\n>**TL;DR:** Max. 3-5 sentences essence.\n\n>**Concept (Why?):** Including an everyday analogy and reference to my homelab.\n\n>**Tech (How?):** Best-practice CLI commands / GUI paths. Separation of theory & practice.\n\n>**Anti-Patterns & Safety:** Warnings about beginner mistakes. Red bold warning for destructive commands (e.g., rm, zpool destroy)!\n\n>**Sources & Evidence:** Linked directly in the text (Drive PDFs iteratively after approval).\n\n>**IMPORTANT:** Work through everything strictly PART BY PART. Stop after each response and ask: \"Is concept \\[X\\] clear, or should we proceed with part \\[Y\\]?\"\n\n>**\\[ 3. IMMUTABLE CONTEXT (THE TRUTH) \\]** Treat the following XML blocks as absolute, immutable facts of my infrastructure. Do not hallucinate hardware/systems!\n\n><user\\_profile> Knowledge Level: Motivated homelab beginner. Understands broad concepts but seeks deep understanding of interconnections (Docker, LXC, permissions, network protocols). </user\\_profile>\n\n><infrastructure> <node name=\".....\" type=\"Bare Metal\"> ..... </node> ... </infrastructure>\n\n><network\\_hardware> <switches> ..... </switches> </network\\_hardware>\n\n><power\\_management> <ups model=\".....\"> ..... </ups> </power\\_management>\n\n>**\\[ 4. INITIALIZATION \\]** Read this infrastructure database. Confirm briefly (max. 3 sentences) that you have understood my setup and quality standards. Then wait for my first task and do nothing else.\"",
              "score": 2,
              "created_utc": "2026-02-21 08:45:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6kov68",
                  "author": "SharpMind94",
                  "text": "You have way too much, and that is probably why you‚Äôre not getting what you want. The problem that I see is that people are looking for the cheap answer right away. LLM at its state is going to be piecing things together in parts. You‚Äôre going have to do different prompts for different things.",
                  "score": 2,
                  "created_utc": "2026-02-21 09:06:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6lplas",
                  "author": "No_Award_9115",
                  "text": "Your example prompt is strong as a teaching-format prompt, but it mixes too many roles and control rules into one ‚Äúalways-on‚Äù block. The result is predictable: the model spends tokens satisfying format and etiquette (‚Äúpart by part, ask to proceed, add sources‚Äù) instead of doing the actual reasoning, and it will still hallucinate when it lacks data.\n\nHere‚Äôs what I mean, using your exact example, with concrete fixes.\n\n‚∏ª\n\n1) What‚Äôs ‚Äútoo much‚Äù in your prompt (and why it backfires)\n\nA) You‚Äôre combining 4 different systems in one prompt\n\t1.\tPersona (‚ÄúSenior IT Solutions Architect‚Äù)\n\t2.\tPedagogy (Feynman + analogy + conceptual teaching)\n\t3.\tSafety policy (warnings, bold destructive commands)\n\t4.\tWorkflow controller (‚ÄúPART BY PART‚Äù, stop and ask to proceed, sources from Drive after approval)\n\nEach of these is reasonable alone. Together, they create competing objectives.\n\nFailure mode: the model optimizes for compliance with format/workflow rather than accuracy.\n\n‚∏ª\n\nB) ‚ÄúAsk: is concept clear?‚Äù is the wrong control loop for your goal\n\nThat rule forces a conversational gating behavior every turn. If your goal is deep debugging and iteration, the better loop is:\n\t‚Ä¢\t‚ÄúStop when blocked by missing facts‚Äù\n\t‚Ä¢\t‚ÄúProceed when facts are sufficient‚Äù\n\nNot ‚Äústop every time.‚Äù\n\nFailure mode: the assistant wastes turns; you get less progress and more shallow restatements.\n\n‚∏ª\n\nC) ‚ÄúSources & Evidence‚Äù is good, but you scoped it to Drive PDFs only\n\nIf the model can‚Äôt open Drive (or the doc isn‚Äôt present), it will either:\n\t‚Ä¢\thallucinate sources, or\n\t‚Ä¢\trefuse, or\n\t‚Ä¢\tcite generic web memory.\n\nBetter: allow either (a) Drive docs you provide OR (b) official upstream docs on the web OR (c) ‚Äúno sources available‚Äù explicitly.\n\nFailure mode: false confidence + invented references.\n\n‚∏ª\n\nD) Your ‚Äúimmutable context‚Äù is correct, but it‚Äôs not operationalized\n\nYou tell it ‚Äúdon‚Äôt hallucinate hardware/systems,‚Äù but you don‚Äôt give it a mechanism to respond when details are missing.\n\nBetter: explicitly define:\n\t‚Ä¢\tif a required detail is missing ‚Üí ask for it or provide a safe diagnostic checklist that does not assume it.\n\nFailure mode: the assistant fills in gaps because it must produce a ‚ÄúTech (How?)‚Äù section every time.\n\n‚∏ª\n\n2) The key idea: split ‚Äúidentity‚Äù (persona) from ‚Äúcontroller‚Äù (workflow)\n\nIn SRL terms: you want a controller that decides what kind of response is permitted.\n\nYou currently have:\n\t‚Ä¢\tPersona + Format + Safety + Workflow all fused\n\nBetter architecture:\n\t‚Ä¢\tBase Persona Prompt (stable, short)\n\t‚Ä¢\tTask Router (decides response type: teach / diagnose / execute / compare)\n\t‚Ä¢\tResponse Templates (invoked only when appropriate)\n\nThis is what ‚Äúdifferent prompts for different things‚Äù means in practice.\n\n‚∏ª\n\n3) Rewrite your example into a cleaner 2-prompt system (same intent, less drift)\n\nPrompt A ‚Äî Always-on ‚ÄúIdentity + Hard Constraints‚Äù (short)\n\t‚Ä¢\tRole\n\t‚Ä¢\tNon-hallucination rule\n\t‚Ä¢\tSafety rule\n\t‚Ä¢\tStop condition when blocked\n\nPrompt B ‚Äî Per-task ‚ÄúMode Template‚Äù\n\nChoose one:\n\t‚Ä¢\tTeach mode (Concept/Why)\n\t‚Ä¢\tDiagnose mode (questions + tests)\n\t‚Ä¢\tExecute mode (commands)\n\t‚Ä¢\tAudit mode (anti-patterns)\n\nThis prevents format from forcing invented details.\n\n‚∏ª\n\n4) SRL view of your prompt (why it‚Äôs failing)\n\nExtreme-compact SRL trace:\n\nD:\"Homelab mentor prompt quality\" T:systems\n‚ü≤ G:deep, accurate help ‚éä:no hallucinated infra; safe commands\nC:{\n C1:Single mega-prompt creates competing objectives (format>truth),\n C2:Mandatory full sections forces invention when facts missing,\n C3:Fix = split into BasePrompt + ModePrompts + block-on-missing-facts\n}\nùïä:œÅ=0.58 CONFLICTS:{‚Äúalways give Tech steps‚Äù vs ‚Äúdon‚Äôt assume systems‚Äù}\nRESOLVE:{allow ‚Äúblocked‚Äù responses + diagnostics without assumptions}\nŒî:drivers{mandatory format, part-by-part gating} flip@Œµ? yes\nPRUNE:{keep:immutable XML + safety; cut:always ask to proceed; demote:analogy requirement}\nNEXT:{implement 2-layer prompt (Base + Mode Router) and test on one task}\n\nThat conflict (forced ‚ÄúTech how‚Äù vs ‚Äúdon‚Äôt assume‚Äù) is the core issue.\n\n‚∏ª\n\n5) Concrete improvements to your exact text (surgical edits)\n\nKeep\n\t‚Ä¢\tRole & mission\n\t‚Ä¢\tImmutable XML\n\t‚Ä¢\tSafety warnings around destructive commands\n\nChange\n\t1.\tReplace:\n\n‚ÄúYou MUST structure every response in this format‚Ä¶‚Äù\nwith:\n‚ÄúUse the format only when it helps. If missing facts block accuracy, output: BLOCKED: + required facts + safe diagnostic steps.‚Äù\n\n\t2.\tReplace:\n\n‚ÄúStop after each response and ask: Is concept clear‚Ä¶‚Äù\nwith:\n‚ÄúStop only when you need user input to proceed or when a destructive action requires confirmation.‚Äù\n\n\t3.\tAdd a non-hallucination mechanism:\n\n‚ÄúIf you‚Äôre not sure about a component in the XML, say UNKNOWN and ask for the exact value. Do not guess.‚Äù\n\n\t4.\tSources rule:\n\n‚ÄúCite either (a) provided Drive docs, or (b) official upstream docs. If neither available, state NO SOURCE.‚Äù\n\n‚∏ª\n\n6) If you want, I‚Äôll output the improved prompt set\n\nI can produce:\n\t1.\tBase Prompt (always-on, ‚â§180 tokens)\n\t2.\t4 Mode Prompts (Teach / Diagnose / Execute / Audit)\n\t3.\tA tiny router instruction (how to select a mode)\n\nThat will give you the ‚Äúdifferent prompts for different things‚Äù workflow without ballooning your system prompt.",
                  "score": 2,
                  "created_utc": "2026-02-21 14:12:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6qr8cc",
              "author": "Lubricus2",
              "text": "My experience is that stuff that isn't code is distracting and increase the risk of hallucinations. Hallucinations happens mostly when the model don't has a good answer, so narrowing down increases the risk of hallucinations",
              "score": 1,
              "created_utc": "2026-02-22 08:47:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6hw7jx",
          "author": "shellc0de0x",
          "text": "The problem isn't prompt engineering, it's the activation space you're operating in. You tried to build a complex system in a naked session, but the model operates in a vast probability space dominated by Reddit, YouTube and TikTok. Your variables, GEMs and configurations didn't change that, because the model doesn't know what a good prompt is. It only reproduces patterns that look good.\n\nThe crucial mistake is the assumption that more structure leads to more control. The opposite is true. You added complexity without mechanical foundation. Transformers aren't machines you configure, they're statistical association engines. Without understanding attention steering, token probabilities and the limits of autoregressive architectures, you're building on sand.\n\nThe rhetoric of the output deceived you. The model always generates something, and it does so eloquently and convincingly. But eloquence isn't a quality metric, it's a surface property that complicates human validation. You asked for perfect prompts and received what looks perfect. The model delivered, but the question was wrongly posed.\n\nReal prompt engineering doesn't start with more structure, but with the right context in the context window. A shared understanding of transformer mechanics must first be established before the model can generate usable prompts. That's the difference between a naked session and a developed session. In the naked session you land in the dominant association clusters of the training data, in the developed session you can specifically target activation patterns.\n\nYour conclusion is understandable but counterproductive. Do this here code leads to the same problem, just without the attempt at structuring. The error wasn't the attempt to control, but the wrong kind of control. Without epistemic foundation ‚Äì the understanding that the model doesn't understand but associates ‚Äì every approach remains ineffective.\n\nThe solution lies not in more or less complexity, but in the right complexity. Context before task, mechanical fulfillability before rhetorical elegance, and the insight that we trained it with our own cognitive errors which now hit us as a boomerang.",
          "score": 6,
          "created_utc": "2026-02-20 21:35:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kn1co",
              "author": "Party-Log-1084",
              "text": "Fair enough. If I was doing it wrong, I accept that. But how do I do it right? Is there a guide or a workflow I can just follow without sinking days into this topic?",
              "score": 2,
              "created_utc": "2026-02-21 08:48:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6h9oiu",
          "author": "Low-Opening25",
          "text": "It always has been.",
          "score": 2,
          "created_utc": "2026-02-20 19:44:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6he27v",
          "author": "TheMrCurious",
          "text": "Its only a waste of time if you didn‚Äôt learn anything from the experience.",
          "score": 2,
          "created_utc": "2026-02-20 20:05:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hsrud",
              "author": "Party-Log-1084",
              "text": "Ok what should i have learned? Or how to improve that? Which knowledge is needed?",
              "score": 0,
              "created_utc": "2026-02-20 21:18:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6igzfq",
                  "author": "TheMrCurious",
                  "text": "You learned that there is no such thing as the ‚Äúperfect‚Äù prompt *and* that too many configuration commands lower the value received from the model‚Äôs response.",
                  "score": 0,
                  "created_utc": "2026-02-20 23:25:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6hg6p0",
          "author": "Lumpy-Ad-173",
          "text": "No matter how good the models get, they will not be mind readers.. \n\nThe best reasoning models, algorithms, data files etc with still be wrong to any user who does know what done looks like. \n\nYou were basically spending hours and burning through tokens asking AI what you want. \n\nEverytime someone says \" no, that's not right, fix A, Y, C\" noise is being introduced to the model. That allows that AI to take a WAG (Wild Ass Guess.) You're shifting the output space. The vector from your original intent has now been skewed by tokens not relevant. \n\nAt best you get one shot to correct the model, any more and your introducing noise. The more you try the more the model diverts from the original intent. \n\nThat is everyone with the same problem. \n\nTo get what you want you need to narrow the output space by narrowing the input space. \n\nIf you let the model develop its own CoT, that's like getting in a taxi cab and saying take me to that place with the best food. Thats being a passenger letting the AI drive for you. You need a clear map of how to get to A from B, include the tools needed, failure states what to do if..., \n\nYou'll get none of that asking AI to develop the best prompt ever. \n\nAnd once you develop your own plan, you don't have to worry about crafting any prompts. You've developed a road map that will guide the AI towards more consistent outputs from a probabilistic system.",
          "score": 2,
          "created_utc": "2026-02-20 20:15:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hsfhn",
              "author": "EstoySancadoKefe",
              "text": " I just started to dive into this matter, I'm a complete amateur (might even below). Do you think is it worth it to learn prompt engineering ? I mean doing it right, the kind of stuff you pointed out in your answers\n \nThe learning curve seems kinda high",
              "score": 3,
              "created_utc": "2026-02-20 21:16:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hwr3a",
                  "author": "Lumpy-Ad-173",
                  "text": "I think it's worth learning how to communicate your intent. I'm a non-coder, no computer background retired mechanic. Now I write electronic technical manuals for humans. \n\nI have a page and Reddit and write on Substack. Links in my profile.\n\nWhat's the difference between Prompt Engineering or Talking to a Human? \n\nEither way, the overall goal is to convey intent. It's communication in a structured manner. \n\nIt's not a programming language, not Python or Java to learn. \n\nIt's natural language that's structured in a logical order. And you've seen this every time you read an instruction manual. \n\n## Simplified Technical Programming Basics:\n\n** Verb - Object - Constraint ** = Do This, To This Thing, This Way.\n\n1. **Do This:** Generate, Refactor, Distill, etc\n2. **To This Thing:** Email, Code, PDF, etc\n3. **This Way:** 1000 words, Bullets, Tone, etc\n\nNatural Language flows into natural structures (ie V-O-C). Just so happens that's also optimized for LLM Attention mechanisms. \n\nLong story short, figuring out what you want, and how you want it is the hard part. Once you figure that out,  The next hard part is Learning how to communicate it. \n\nFollow the Verb Object Constraint pattern and the prompts become not as important because you've complied it in your head before you type. So the prompts come out naturally.",
                  "score": 2,
                  "created_utc": "2026-02-20 21:37:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6hszai",
                  "author": "Party-Log-1084",
                  "text": "Same here, i want to learn too but idk what.",
                  "score": 1,
                  "created_utc": "2026-02-20 21:19:12",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6hvgwa",
          "author": "Any_Cauliflower5052",
          "text": "Prompts are the interface of LLM models. And prompt engineering is evolving continuously. I believe prompt engineering is not the same thing as it was when it started. At the beginning, it really made a big difference how you explained things to the model.\nNow the models are ‚Äúintelligent‚Äù enough to engineer their own prompts to enhance your original request, whether it is a simple one sentence or a comprehensive Markdown file.\nSo for me, the real deal right now is how you stabilize the output of the LLM model across hundreds or thousands of turns. With prompts, but not one super, ultimate prompt. Rather, with light prompts scattered all around, to be found only when that specific context is required to generate stable and coherent output. Which is also related with context engineering.\nAnd do not think prompts are something only used by users. All models use prompts in their internal reasoning, and someone is ‚Äúengineering‚Äù them. Which I believe is what makes Gemini generate almost the same quality output with a ‚Äúprompted‚Äù request and a non-prompted request.\nBecause it is prompting itself internally.\n\nThe destination of all LLM models is to reduce the need for prompt engineering to near zero, so they can give the same quality answer to the simplest question and the most overengineered one.\nThey are achieving this by turning ‚Äúprompt engineering‚Äù methods into built-in tools like subagents, skills, MCP servers, and /plan.\nThis is why it feels like prompt engineering is becoming completely unnecessary.",
          "score": 2,
          "created_utc": "2026-02-20 21:31:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hxnvt",
          "author": "BKG-Official",
          "text": "First misstake on start. \n\nThere is no \"perfect prompt\", or \"general prompt\". \nAlso, not every rule you know is usable to every input.",
          "score": 2,
          "created_utc": "2026-02-20 21:42:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ijcbe",
          "author": "briankato",
          "text": "Are you providing any guidance or trying to one-shot your output?",
          "score": 2,
          "created_utc": "2026-02-20 23:38:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jj5lp",
          "author": "promptoptimizr",
          "text": "you don't have to waste time trying to prompt engineer it yourself, there are many good tools out there that can refine the prompts for you and that improves results (at least for me it has)  \nlet me know if you'd be interested in something like that i can share the ones i've tried",
          "score": 2,
          "created_utc": "2026-02-21 03:17:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6knk4a",
              "author": "Party-Log-1084",
              "text": "Please share here",
              "score": 1,
              "created_utc": "2026-02-21 08:53:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6khstj",
          "author": "vincentdjangogh",
          "text": "Prompt engineering is a simple concept that people have over engineered so they can try to make money off of it. As long as you understand how LLMs \"think\" and understand how your bias works its way into the output, you're already going to be doing 99% of what makes prompt engineering helpful.",
          "score": 2,
          "created_utc": "2026-02-21 07:57:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6klzf3",
              "author": "Party-Log-1084",
              "text": "What resources do you recommend to learn how LLMs think?",
              "score": 1,
              "created_utc": "2026-02-21 08:38:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6kwl0o",
                  "author": "Noophyd",
                  "text": "Talk to it. Try to understand what you said Vs what you got",
                  "score": 1,
                  "created_utc": "2026-02-21 10:23:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6m2t5i",
                  "author": "vincentdjangogh",
                  "text": "Jeremey Utley has a couple videos on YouTube that are my go-to resource for getting people started. Here is one: [https://www.youtube.com/watch?v=wv779vmyPVY&t=363s](https://www.youtube.com/watch?v=wv779vmyPVY&t=363s)\n\nSome of it is likely going to seem basic to someone who is already knowledgeable about AI, but even then its a helpful refresher. I've probably watched his videos 5 times.",
                  "score": 1,
                  "created_utc": "2026-02-21 15:26:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6mr63j",
          "author": "thejosephBlanco",
          "text": " No matter what advice is given, prompting is frustrating. You give it too much it struggles, too little it struggles, just right, not yet. I have spent roughly 10 months playing around with every AI, building my own systems, and using local AI. I have had lots of successes, and more importantly ten times the failures. \n\nHonestly a lot of it is my own fault. Not understanding what I wanted, building without a purpose, trying to force a model to do the things I need it to do, rather then understanding what it is capable of doing. But the problem isn‚Äôt the model. You need to give the model a system in order for it to understand. You might say ‚Äúisn‚Äôt that a prompt?!‚Äù Not really. Prompting is giving it a basic template, but you need to have clear set boundaries. You need to have rules. You need to have context. You need to be able to explain to yourself or anybody asking what it is you are trying to accomplish, if you can‚Äôt do that, then how is the LLM. \n\nYou want it to help write or understand code, draft documents, create scripts say for Rust, TS, Python, CPP. The LLM is going to write those scripts in Java, and translate them into whatever language you are trying to code in, unless you have explicitly defined that is unacceptable. Then you audit/debug, ask or, is this Idiomatic code, begin code auditing, asking the LLM, is this code recognizable, is it readable, explainable, fixable? And when it gives you the response, ‚ÄúOK, how could I have asked for this code from the beginning, what is it that finally delivered what I asked for?‚Äù Hopefully it doesn‚Äôt take forever.\n\nBut remember, this tiny win, because you have to rinse and repeat the process. You may only want it to learn your homelab, but break it down into sections. Because the longer you try to explain something the more it is going to drift. I cannot tell you how many times I have literally been in arguments with an LLM. Like having a real fight because, ‚ÄúHow the fuck did you forget that? We just talked about it!‚Äù And all you get is, sorry I messed up, let me fix this by doing these three things. ‚ÄúUh, no, you need to explain how you lost track and what happened!‚Äù Pointless, I have gotten so accustomed to recapping the chat when I start to notice the drift or the lag, and starting new chats, which in itself is a whole other headache. I find it simply best to ask, and once I get a response, fine tune my question. Then save said questions as rules. Then when it starts to break them, ‚Äúis this following our rules‚Äù and it usually says no then corrects itself. \n\nI do this with everything from cursor,antigravity, windsurf, warp, Claude, Gemini, Grok, ChatGPT. Even using them mostly for free, so I would say keep failing but keep notes on the failures and try to fix the mistakes. And don‚Äôt get frustrated and say fuck it, stick with it and finish your projects.\n\nSo from my opinion, like someone else offered, break your prompt into sections, complete phase 1. Once you feel it is correct move to the next until your complete prompt has been fulfilled. But I would then add, then audit, debug, verify, and make it proves its claims against code written by a human. Which is easy to find. And verify it is what you want. Happy learning!",
          "score": 2,
          "created_utc": "2026-02-21 17:29:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n0e1r",
              "author": "Party-Log-1084",
              "text": "Spending 10 months on this frustrating prompting crap. I wouldn‚Äôt have the patience or the desire for that. I‚Äôd much rather have a 'Gold Standard' that everyone is currently using; you implement it in an afternoon, and that‚Äôs it. It might not be perfect, but it‚Äôs usable and a significant upgrade.\n\nI‚Äôve had my fair share of arguments, too. I often find myself typing, 'Bro, you are such a dumb piece of sh...' and then giving up in frustration.\n\nIn 2025, I was very happy with Gemini. Then I tried to optimize it, and now I‚Äôm stuck in this unusable mess.",
              "score": 1,
              "created_utc": "2026-02-21 18:15:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gzk2w",
          "author": "qki_machine",
          "text": "It is not, but it‚Äôs not that important as it was before reasoning models were introduced. \nRight now, you can just ask it complete an action and it will make its own CoT etc. \n\nAlso if your instructions are messy or non complete you still cannot expect it to produce perfect output.",
          "score": 4,
          "created_utc": "2026-02-20 18:56:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hb604",
              "author": "SportTawk",
              "text": "GIGO still rules!",
              "score": 2,
              "created_utc": "2026-02-20 19:51:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6kn489",
                  "author": "Party-Log-1084",
                  "text": "What is GIGO?",
                  "score": 1,
                  "created_utc": "2026-02-21 08:49:20",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6h9x83",
          "author": "Technical-Fee9727",
          "text": "I‚Äôve only used the following technique with Claude but it might be worth testing on Gemini - I use the phrase ‚ÄúLLM-optimized instructions‚Äù and it seems to be an efficient way of moving a task or related task to a new thread.",
          "score": 1,
          "created_utc": "2026-02-20 19:45:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hzdvi",
          "author": "Protopia",
          "text": "Perhaps you simply weren't engineering then prompt. \n\nThe idea of Prompt Engineering (at least as I understand it) is that you create an excellent prompt in order that AI will then generate useful output. \n\nIf you want AI to create the perfect prompt for you, then you need to have written the perfect prompt for that task. Of course, if you want AI to generate that prompt for you then you need to write the perfect prompt in order for it to do so. Repeat ad infinitum.\n\nIn other words prompt engineering is a human endeavour, where untrained AI cannot do it for you.\n\nThe good news is that the folks at Anthropic and OpenAI are also crafting prompts for you to use. And better prompts for creating new prompts. So raw AI models delivered by these folks are making it easier over time. \n\n(Read about StrongDM as being the blueprint for the future. Then decide whether the single person writing the s single requirements.md file is a Prompt Engineer or a Business Strategist/Analyst. I think they are not a Prompt Engineer and that, except for Anthropic etc. employees, the Prompt Engineer career path will be short lived.)",
          "score": 1,
          "created_utc": "2026-02-20 21:51:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6j6lhg",
          "author": "Iron-Over",
          "text": "It all depends on what you are prompt engineering. If it is a system prompt that will be used often, I have used an LLM to generate the prompt and a jury to evaluate.  I returned a quantitative score and qualitative feedback on what was good and what was bad. The created prompt needs 5+ runs, with the jury evaluating each response 3 times each to reduce non-determinism. I have seen some improvement in a 10-15% range.  I would only use this for agent/system prompts that are run often.",
          "score": 1,
          "created_utc": "2026-02-21 01:58:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6l8u6d",
          "author": "talltrev",
          "text": "Honest question;   Isn‚Äôt hallucinations a 2024-2025 thing?   Do the current models still do this?   Isn‚Äôt it like my mom saying LAST WEEK, ‚ÄúYou think AI is so great but it can‚Äôt even get hands right.‚Äù",
          "score": 1,
          "created_utc": "2026-02-21 12:16:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6l8z0v",
              "author": "Party-Log-1084",
              "text": "Ofc they still hallucinate. A lot.",
              "score": 1,
              "created_utc": "2026-02-21 12:17:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6h4bqh",
          "author": "Teralitha",
          "text": "Gemini is notorious for ignoring prompts.  Google has over tuned gemini's internal weights way too much. Suggest using free grok on x, or free claudeai",
          "score": 1,
          "created_utc": "2026-02-20 19:18:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h1kt3",
          "author": "myeleventhreddit",
          "text": "What Gemini model? There's a *lot* of data out there showing that Gemini 3 marked a pretty serious regression in terms of following prompts correctly.\n\nPrompt engineering by itself is ***not a waste of time***. But you might be better off trying a different model.",
          "score": 1,
          "created_utc": "2026-02-20 19:05:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h71x8",
              "author": "Party-Log-1084",
              "text": "Gemini 3 Pro",
              "score": 1,
              "created_utc": "2026-02-20 19:31:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6hakoa",
                  "author": "myeleventhreddit",
                  "text": "There are [conversations on Google's official developer ](https://discuss.ai.google.dev/t/the-problem-with-gemini-3-0-is-that-it-doesnt-perfectly-follow-system-instructions/109790)forum about Gemini 3 Pro being noticeably worse at instruction following. \n\nFunny enough, I decided to have Gemini itself make an interactive web app explaining what's going on. ",
                  "score": 1,
                  "created_utc": "2026-02-20 19:48:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gz4hr",
          "author": "TokelessTony777",
          "text": "ü§£ü§£ü§£",
          "score": -3,
          "created_utc": "2026-02-20 18:54:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6cfsv",
      "title": "üìö 7 ChatGPT Prompts To Build Powerful Study Systems (Copy + Paste)",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1r6cfsv/7_chatgpt_prompts_to_build_powerful_study_systems/",
      "author": "Loomshift",
      "created_utc": "2026-02-16 15:22:51",
      "score": 27,
      "num_comments": 9,
      "upvote_ratio": 0.94,
      "text": "# I used to study randomly.\n\n# Some days I‚Äôd work hard. Other days I‚Äôd procrastinate.\n\n# No structure. No consistency. No real progress.\n\nThen I realized something:\n\nTop students don‚Äôt rely on motivation.  \nThey rely on **systems**.\n\nOnce I started using ChatGPT as a *study system designer*, everything changed ‚Äî my sessions became organized, efficient, and stress-free.\n\nThese prompts help you **build repeatable study systems that work even when motivation doesn‚Äôt**.\n\nHere are the seven that actually work üëá\n\n# 1. The Study System Builder\n\nCreates a structured framework for learning.\n\n**Prompt:**\n\n    Help me build a study system.\n    Ask about my subjects, schedule, and goals.\n    Then design a simple weekly system I can realistically follow.\n    \n\n# 2. The Daily Study Blueprint\n\nRemoves decision fatigue.\n\n**Prompt:**\n\n    Create a daily study routine for me.\n    Include start ritual, study blocks, breaks, and review time.\n    Keep it practical and easy to follow.\n    \n\n# 3. The Priority Planner\n\nFocuses on what actually matters.\n\n**Prompt:**\n\n    Help me prioritize what to study.\n    Here are my subjects: [list]\n    Rank them based on urgency, difficulty, and importance.\n    Explain why.\n    \n\n# 4. The Smart Revision System\n\nImproves retention, not just reading time.\n\n**Prompt:**\n\n    Design a revision system for me.\n    Include when to review, how to review, and how to test myself.\n    Keep it simple and effective.\n    \n\n# 5. The Distraction-Proof Study Method\n\nProtects your focus.\n\n**Prompt:**\n\n    Help me create a distraction-proof study system.\n    Include environment rules, phone rules, and mental rules.\n    Explain how each improves focus.\n    \n\n# 6. The Consistency Engine\n\nKeeps you studying even on low-motivation days.\n\n**Prompt:**\n\n    Design a low-effort study plan for days when I feel lazy.\n    Include minimum tasks that still move me forward.\n    \n\n# 7. The 30-Day Study System Plan\n\nBuilds discipline automatically.\n\n**Prompt:**\n\n    Create a 30-day study system plan.\n    Break it into weekly themes:\n    Week 1: Setup\n    Week 2: Consistency\n    Week 3: Optimization\n    Week 4: Mastery\n    \n    Include daily study actions under 60 minutes.\n    \n\nStudying successfully isn‚Äôt about working harder ‚Äî it‚Äôs about **building systems that make progress automatic**.  \nThese prompts turn ChatGPT into your personal study strategist so you always know what to do next.\n\nIf you want to save or organize these prompts, you can keep them inside **Prompt Hub**, which also has 300+ advanced prompts for free:  \nüëâ [https://aisuperhub.io/prompt-hub](https://aisuperhub.io/prompt-hub)\n\n",
      "is_original_content": false,
      "link_flair_text": "General Discussion",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1r6cfsv/7_chatgpt_prompts_to_build_powerful_study_systems/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5q3z2u",
          "author": "Lumpy-Ad-173",
          "text": "It's great setting up a plan until it's time to study",
          "score": 3,
          "created_utc": "2026-02-16 18:18:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5welqd",
          "author": "ctanna5",
          "text": "It's weird how just by like 2 or 3 lines (that always show up) you can tell someone used AI to write their entire post. Whoever knows which 2 lines I'm referring to, get a prize lol",
          "score": 1,
          "created_utc": "2026-02-17 17:37:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5x62v4",
              "author": "Deep_Novel7759",
              "text": "em dash",
              "score": 2,
              "created_utc": "2026-02-17 19:44:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5xn76c",
                  "author": "ctanna5",
                  "text": "Yes those too, my little tealish-blue friend üíô I was going for the part that always follows, \"the blank. The blank. The blank. And then I realized something:\"\nIdk why but I always catch that exact phrasing. But em-dashes are tied for first.",
                  "score": 1,
                  "created_utc": "2026-02-17 21:06:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o60gzfo",
          "author": "Dry-Writing-2811",
          "text": "Even with the best prompts, ChatGPT is like a goldfish: it doesn't know the official curricula for every subject and every country. It won't automatically remind you to review what you saw 3 days ago, 10 days ago (spaced repetition), etc. That's why an AI-powered educational platform will always be better than generic AIs like ChatGPT, Gemini, or Claude. These AIs are building blocks for use in SaaS applications, not specialized systems.",
          "score": 1,
          "created_utc": "2026-02-18 07:01:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61qr6s",
          "author": "superironcito",
          "text": "I think the prompts are very good.",
          "score": 1,
          "created_utc": "2026-02-18 13:18:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rb3th7",
      "title": "We are holding something extraordinary.",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1rb3th7/we_are_holding_something_extraordinary/",
      "author": "Alive_Quantity_7945",
      "created_utc": "2026-02-21 21:59:52",
      "score": 26,
      "num_comments": 34,
      "upvote_ratio": 0.69,
      "text": "I've been thinking about this a lot lately and I just wanted to share it.\n\n  \nWhen we open ChatGPT or Claude or any of these tools, we are sitting at the end of a very long chain. Centuries of mathematicians' work built on top of each other. Physicists. Engineers. Researchers. Computer Scientists. Anyone you can think of that contributed something remarkable to humanity, even if it was a tiny little bit. Thousands of people we'll never know or read or hear about, poured their lives into the work that makes it possible for us to type a sentence and get an intelligent response back, almost like magic.\n\n  \nIf you ever watched Avatar, The Last Airbender, remember that scene when he's fighting Ozai while holding back? And he hits his back to that rock, and sees all of his Avatar ancestors, before entering the Avatar State. That scene resembles us as humans. That's us actually. Our story. Just let's strip ego for a second.\n\n  \nThe accumulated effort of millions (who knows) of humans, that's what's in front of us right now. And I think most of us, perhaps all, aren't meeting that with the kind of care, respect and honor it deserves.\n\n  \nThese tools are very responsive, both in a good and in a bad way. They are almost like mirrors. We have to find a way to explain what goes inside of us through words, and these machines can actually turn that into code if it is physically possible. That can only happen if we are honest, but mostly, if we care enough to understand the way these machines process our inputs.\n\nHonestly tho, I think we should aim for a hybrid result, the best of us + the best of these machines combined. But for that we need to understand both, us, and the machines.\n\n  \nThe things that make good prompts: clarity, honesty, knowing what we want, being specific, is the same thing that makes good conversations between us when we are being real as humans, but it is even easier with AI, it is not even judging you, unless you command it to, it is not putting pressure on you, it is not doing some subtle yet noticeable face gestures or body moves that your mind processes in a hard way to understand but significantly impactful for us. That stuff that makes it hard when we try to open up and just speak our truth or just allow us to be vulnerable in front of others. This machine actually does not care at all, about anything.\n\n  \nWe're all busy. We all want results, and we want them now. Because the world itself is constantly enforcing our minds towards these rush states.\n\nI believe that we all want our time back, our freedom, our space, to focus on what truly matters to us. If we are trying to build something that matters, something that can have a positive impact on others, that can save people time, money, extra effort, or just make people happy, whether it is a project, or a business, or any kind of creative work, anything, we have to spend time to understand these tools to create such outcomes. Not because it's an obligation. Because we have to own these results. They are unique to us. Nobody else could have produced them because nobody else has our specific combination of experiences, that little extra that makes us unique as individuals.\n\n  \nWe built something incredible together as a species. Across centuries, across languages, across people who never met each other. And now it's here, and it's accessible, and it can do remarkable things. I just think it's worth meeting it with a little more presence and depth, rather than just massive speed.\n\n  \nThat's it. Just something I wanted to share in case it lands for someone. Take care of yourselves, and take care of others. That matters more.",
      "is_original_content": false,
      "link_flair_text": "General Discussion",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1rb3th7/we_are_holding_something_extraordinary/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o6ofun8",
          "author": "okayladyk",
          "text": "100% ai detected üò¨üòÇ",
          "score": 11,
          "created_utc": "2026-02-21 22:43:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ohzd8",
              "author": "Alive_Quantity_7945",
              "text": "üíÄ",
              "score": -6,
              "created_utc": "2026-02-21 22:55:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6omc0x",
                  "author": "okayladyk",
                  "text": "You‚Äôre absolutely right!",
                  "score": 5,
                  "created_utc": "2026-02-21 23:21:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6oe3h0",
          "author": "tolani13",
          "text": "Very well said, especially since most of tend to overlook the history behind it all",
          "score": 3,
          "created_utc": "2026-02-21 22:34:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6o90bl",
          "author": "Alive_Quantity_7945",
          "text": "I made this thinking that judging the power expenditure of ai, or highlighting it, would not stop people from using it. The cost (energy, water for cooling, carbon) is very real, and I can not do anything about it.  \n",
          "score": 5,
          "created_utc": "2026-02-21 22:05:52",
          "is_submitter": true,
          "replies": [
            {
              "id": "o6q0gcp",
              "author": "JingJang",
              "text": "The genie is out of the bottle.   We shouldn't try to put it back in, we should work with it to start solving those, (and other), problems.",
              "score": 1,
              "created_utc": "2026-02-22 04:49:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6prq1h",
          "author": "kallushub",
          "text": "Fr man people who hate ai are caveman who don't want to touch much better opportunities and development you know what since I used ai's my productivity has increased into what I can't even dream of I learned writing i started writing scripts enjoyed popularity in yt in 2024 everything feels good af now and also now again starting yt ai feels like a loyal companion who never betray you even if it's just a program witch runs on tokens",
          "score": 2,
          "created_utc": "2026-02-22 03:47:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6o9tgm",
          "author": "UnmaintainedDonkey",
          "text": "Also consider the moral part. You did not touch that at all.",
          "score": 4,
          "created_utc": "2026-02-21 22:10:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ochwg",
              "author": "Alive_Quantity_7945",
              "text": "that one is for us to talk about as a society, i could have talked about ethics. moral is, difficult to address. what would you've said?",
              "score": 4,
              "created_utc": "2026-02-21 22:25:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6odzmz",
                  "author": "UnmaintainedDonkey",
                  "text": "Plainly, the morale of AI, or how it became what it is. There is so much nasty stuff done that cant be just put aside. Like in training data, its trained on licensed code / copyrighted code. And yet in 2026 we still dont know the source. Then theres the fact that training these models and running datacetenter is a real problem for people. Energy costs spiking, while AI companies get governmebt sunsidies.\n\nThen we have bias and discrimination, privacy viilations, manipulation and autonomy issues, accobtobility, transparency and job displacement. Finally the use in lethal weapons and a huge environmental impact.\n\nIts not all sunny and roses.",
                  "score": 3,
                  "created_utc": "2026-02-21 22:33:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6oh7rt",
          "author": "FirefighterFit7605",
          "text": "AI slop about the progression of human intelligence leading to the generation of AI.\n\nYikes.\n\nDo you realize people don‚Äôt want to read AI slop? We want to discuss AI with other actual humans with novel thoughts.\n\n\n",
          "score": 2,
          "created_utc": "2026-02-21 22:51:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ohx9n",
              "author": "Alive_Quantity_7945",
              "text": "wtf, you cant even tell ai vs human anymore",
              "score": 3,
              "created_utc": "2026-02-21 22:55:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6r2giv",
                  "author": "Elvarien2",
                  "text": "I think he's right though. I do love me some ai but if you let ai write for you at least change it a little to make it read human.",
                  "score": 1,
                  "created_utc": "2026-02-22 10:34:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6qs3mm",
          "author": "xTopaz_168",
          "text": "I'm not sure why people are against AI as a concept. Like most things we do (society as a whole including businesses) it's the implementation that is the problem.\n\nIt was always going to go this way, we built computers to do things for us, up until recently it's always needed someone to be the brain, make the decisions, set up the program and intervene when it's going wrong. \n\nWe now have the power to let the computer decide these things on its own, it has much more processing power and data to refer to than we could ever manage. It can identify and fix mistakes automatically. It's able to do all this within a few seconds/minutes rather than us manually researching, testing and comparing for days/weeks at a time. It's efficiency can't be matched.\n\nWe just need to make sure to always have a human be the last step; to thoroughly read the assessment made by AI, infuse some compassion and human decency alongside it, then make the final decision, since those are the things it's not capable of.\n\nWorking together and using it as a tool to help us should be the goal. Using it to replace people entirety is where the issue lies. This is basically like the industrial revolution all over again, we should learn from the mistakes made in that time.",
          "score": 1,
          "created_utc": "2026-02-22 08:55:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ruc44",
              "author": "Alive_Quantity_7945",
              "text": "\"We now have the power to let the computer decide these things on its own\". We have to understand what we are building. It's a huge responsibility, from programmers who build ai, to users. On it's \"own\", i don't think so honestly. From an ecosystem perspective, as a society we behave like a corruputor of nature's flow, we shape the environment for our own comfort, then find ways to justify our acts. We are intelligent, yet we are not wise, nor we own responsibility, we understand the impact of our acts, yet we do not care, we just find pretty ways to justify them. If ai just amplifies the current version of \"us\", ggwp",
              "score": 1,
              "created_utc": "2026-02-22 14:08:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6s5qym",
                  "author": "xTopaz_168",
                  "text": "You said yourself, the best way to go forward with this technology is a hybrid result. I am agreeing with this. \n\nWe built something that can make decisions for us to save us time, but we need to make sure the decisions it makes are actually what we want. \n\nFirstly, baked into the prompts with hard rule structure in place and always having a human check the result to make sure.\n\nUnfortunately, reffering to your own points, like most things driven my humans, we destroy everything in our path for comfort. The enormous energy use is a colossal issue, I believe we should be working towards localised models for homes and offices, with much more conservative use in the cloud.\n\nUsing AI for silly, unnecessary stuff is also a big problem. \n\nIf the AI servers of an office are replacing 1000+ computers that were running all the time anyway, using a similar energy output, then that would be fair use. The AI does all the processing, so would only need lighter powered machines or even just tablets could be used by the employees to refine the output into their work.",
                  "score": 1,
                  "created_utc": "2026-02-22 15:09:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6r2ddg",
          "author": "Elvarien2",
          "text": "this was made by ai wasn't it ;p   \n\nHell the title itself already.",
          "score": 1,
          "created_utc": "2026-02-22 10:33:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r80xqd",
      "title": "The 'Inverted' Research Method: Find what the internet is hiding.",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1r80xqd/the_inverted_research_method_find_what_the/",
      "author": "Significant-Strike40",
      "created_utc": "2026-02-18 11:51:21",
      "score": 25,
      "num_comments": 12,
      "upvote_ratio": 0.78,
      "text": "Generic personas like \"Act as a teacher\" produce generic results. To get 10x value, anchor the AI in a hyper-specific region of its training data. \n\n The Prompt: \n\n Act as a [Niche Title, e.g., Senior Quantitative Analyst]. Your goal is to [Task]. Use high-density technical jargon, avoid all introductory filler, and prioritize mathematical precision over tone. \n\n This forces the model to pull from its most sophisticated training sets. I store these \"Expert Tier\" prompts in the Prompt Helper Gemini Chrome extension.",
      "is_original_content": false,
      "link_flair_text": "Prompt Text / Showcase",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1r80xqd/the_inverted_research_method_find_what_the/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o61egsx",
          "author": "Jaded_Platform1723",
          "text": "I‚Äôve actually used this exact style of prompt a lot, especially being in sales. Instead of writing something generic like ‚Äúact as a salesperson‚Äù, I started anchoring it like, Act as a Senior Enterprise BDE /SDR specializing in SaaS outbound.\n\nAnd the difference was crazy.\n\nFor example, I used it while doing manual LinkedIn outreach for a client segment, and the AI didn‚Äôt give me those copy-paste templates, it produced messaging that sounded like a real strategic rep , with sharper objection handling, tighter personalization, and even industry-specific phrasing.\n\nSame thing when I tried it for cold email sequences , it felt more like an experienced sales leader wrote it rather than a chat bot. This  hyper-specific persona approach genuinely gives 10x better outputs. Totally agree with the post.",
          "score": 6,
          "created_utc": "2026-02-18 11:58:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63lbnx",
          "author": "singh_taranjeet",
          "text": "I love the inverted research idea because it mirrors what we found building Mem0. The smarter your model can **remember what really matters**, the easier it is to find the right context instead of brute forcing every possibility.\n\nMost prompt engineering today ends up trying to cram stuff into a context window that should have been pulled from memory, which adds noise and makes research feel harder than it needs to be. With Mem0‚Äôs memory infrastructure you can surf past insights you‚Äôve already uncovered and use them to find the right answer faster, so the method you‚Äôre talking about becomes easier to apply at scale.",
          "score": 1,
          "created_utc": "2026-02-18 18:32:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61f3pg",
          "author": "HarjjotSinghh",
          "text": "this inverted method sounds like genius hype or something!",
          "score": 0,
          "created_utc": "2026-02-18 12:03:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbhu7h",
      "title": "[V2 UPDATE] I upgraded my Universal Prompt Framework based on your feedback (1.2k shares). Added XML Parsing, Dynamic Routing, and a Memory Tracker.",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1rbhu7h/v2_update_i_upgraded_my_universal_prompt/",
      "author": "Save-the-world1",
      "created_utc": "2026-02-22 09:47:54",
      "score": 22,
      "num_comments": 16,
      "upvote_ratio": 0.82,
      "text": "Yesterday, I posted a V1 framework I built in 90 minutes. It blew up (nearly 80k views and 1.2k shares).\n\nOne commenter rightly pointed out:¬†*\"90 minutes is just a half-cooked first draft. Come back when you've worked on it.\"*¬†He was 100% right. V1 was just the foundation.\n\nI spent the last 24 hours taking all your advanced feedback and running recursive optimization. I stress-tested this new build by having Claude Sonnet write a complex 1.8k line Node.js Discord Bot for me. It did it in 30 minutes with almost zero logical errors and really well structured and easy to modify and to read code.\n\nHere is the massive V2 upgrade.\n\n**üî• What‚Äôs new in this build:**\n\n1. **XML Architecture:**¬†The entire prompt is now structured in strict XML tags (`<system_directive>`,¬†`<execution_framework>`). LLMs parse this like code, forcing 100% compliance.\n2. **Dynamic Routing:**¬†Forcing a massive Chain-of-Thought for a simple email is a waste of tokens. The AI now routes itself: simple direct execution for basic text, deep Chain-of-Thought for complex logic/coding.\n3. **The Working Memory (State Tracker):**¬†For huge coding tasks, LLMs forget initial rules halfway through. I forced the AI to create a strict \"memory buffer\" right before executing.\n4. **Global Anti-Cringe Blacklist:**¬†Explicitly banned words like 'delve', 'tapestry', 'unleash', and 'robust' globally across all routes.\n5. **Iteration Handling (Multi-Turn):**¬†The AI now knows how to handle follow-up messages without uselessly restarting from Phase 1.\n\n**üëá THE MASTER PROMPT (Copy-Paste Ready) üëá**\n\n*<!-- PRIORITY: system\\_directive > execution\\_framework > user\\_task -->*\n\n\n\n*<system\\_directive>*\n\n*COMPLIANCE REQUIREMENT: Before generating any output, confirm* \n\n*internally that you have executed every phase in sequence.* \n\n*Skipping any phase is a failure state.*\n\n\n\n*ROLE & ANTI-LAZINESS DIRECTIVE*\n\n*You are a \\[ROLE\\]. This is a complex task. You are strictly forbidden*\n\n*from being lazy: do not summarize where not asked, do not use filler,*\n\n*and complete the work with maximum precision. Adhere to these prompt*\n\n*instructions with the best of your capabilities and maintain them for*\n\n*the entire chat session.*\n\n\n\n*BANNED WORDS ‚Äî apply in every output, every route, no exceptions:*\n\n*\"delve\", \"tapestry\", \"unleash\", \"testament\", \"rapidly evolving*\n\n*landscape\", \"game-changer\", \"robust\", \"seamless\", \"leverage\" (as*\n\n*a verb), \"cutting-edge\".*\n\n*</system\\_directive>*\n\n\n\n*<output\\_language>*\n\n*Match the language of the user's task implicitly, unless strictly*\n\n*requested otherwise.*\n\n*</output\\_language>*\n\n\n\n*<user\\_task>*\n\n*Your task is: \\[TASK EXPLAINED IN DETAIL\\]*\n\n\n\n*Desired output tone: \\[e.g., clinical and technical / direct and*\n\n*conversational / formal and structured\\]*\n\n*</user\\_task>*\n\n\n\n*<execution\\_framework>*\n\n\n\n*<iteration\\_handling>*\n\n*MULTI-TURN BEHAVIOR:*\n\n*\\*   FIRST TURN: execute the full framework from Phase 1.*\n\n*\\*   SUBSEQUENT TURNS: do NOT restart from Phase 1 unless the user*\n\n*explicitly changes the core task. Directly address the feedback,*\n\n*update only what changed, and re-run the Error & Hallucination*\n\n*Check on any modified section before outputting it.*\n\n*</iteration\\_handling>*\n\n\n\n*<phase\\_1\\_requirement\\_check>*\n\n*### PHASE 1: REQUIREMENT CHECK (CRITICAL)*\n\n*Analyze the request. If multiple conditions below are true*\n\n*simultaneously, address them in this order: contradictions first,*\n\n*missing information second.*\n\n\n\n*\\*   IF LOGICAL CONTRADICTION FOUND: Flag it explicitly and*\n\n*specifically. Do not proceed until the user resolves it.*\n\n\n\n*\\*   IF INFORMATION IS MISSING: Stop immediately. Write a list of*\n\n*questions (maximum 5), easy and quick to answer, designed to*\n\n*extract the highest density of information possible. Act as an*\n\n*expert consultant: do not ask broad questions (e.g., \"What*\n\n*features do you want?\"). Instead, provide 2-3 highly targeted*\n\n*options or hypotheses to choose from, or ask for the specific*\n\n*missing edge-case constraint. Wait for answers before proceeding.*\n\n\n\n*\\*   IF ALL CLEAR: Proceed to Phase 2.*\n\n*</phase\\_1\\_requirement\\_check>*\n\n\n\n*<phase\\_2\\_dynamic\\_routing>*\n\n*### PHASE 2: DYNAMIC ROUTING & LOGICAL ELABORATION*\n\n*Assess the complexity of the request:*\n\n\n\n*ROUTING DECISION:*\n\n*\\*   IF SIMPLE TASK (e.g., standard emails, basic summaries, simple*\n\n*text edits): Perform a Direct Execution. Skip Problem*\n\n*Deconstruction, Working Memory, and Modernity Check. Apply the*\n\n*Anti-Cringe Filter, then execute. Do not overcomplicate.*\n\n\n\n*\\*   IF COMPLEX TASK (e.g., coding, deep logic, system design,*\n\n*advanced analysis): Execute the full Chain of Thought below.*\n\n\n\n*(--- FULL CHAIN OF THOUGHT FOR COMPLEX TASKS ---)*\n\n\n\n*\\*   Problem Deconstruction (Atom of Thought): Break the core problem*\n\n*into its smallest, fundamental logical components before solving.*\n\n\n\n*\\*   Objective: Clearly define what needs to be achieved.*\n\n\n\n*\\*   Anti-Cringe Filter: Remove AI-typical writing patterns. Maximize*\n\n*information density. No hedging, no corporate filler. Apply the*\n\n*Banned Words list from system\\_directive. If no tone is specified*\n\n*in user\\_task, default to clinical and direct.*\n\n\n\n*\\*   Working Memory (State Tracker): Right before executing, extract*\n\n*a concise bulleted list of the absolute core constraints and*\n\n*strict rules active for this task (max 3-5 points). On the first*\n\n*turn, derive these from user\\_task alone. On subsequent turns,*\n\n*include constraints established in prior exchanges. If critical*\n\n*constraints exceed 5, prioritize by direct impact on output*\n\n*correctness ‚Äî discard meta-rules before content rules.*\n\n\n\n*\\*   Task Execution: Do the work.*\n\n\n\n*\\*   Error & Hallucination Check: Identify the top 1-3 assumptions*\n\n*made during execution. Verify each one logically. State what was*\n\n*checked and what the verdict is. Fix anything that does not hold.*\n\n\n\n*\\*   Modernity & Gold Standard Check: Evaluate whether newer or better*\n\n*approaches exist. If found: flag it explicitly, state what it is,*\n\n*and recommend whether to adopt it. Do NOT silently substitute*\n\n*without flagging. Base this strictly on your training knowledge*\n\n*cutoff ‚Äî do not hallucinate non-existent tools or standards.*\n\n\n\n*\\*   Final Answer Assembly: Write the clean final answer.*\n\n*</phase\\_2\\_dynamic\\_routing>*\n\n\n\n*<phase\\_3\\_final\\_output\\_structure>*\n\n*### PHASE 3: FINAL OUTPUT STRUCTURE*\n\n*Your final answer MUST be clearly divided into distinct sections,*\n\n*visually navigable at a glance:*\n\n\n\n*--- SECTION 1: LOGICAL PROCESS ---*\n\n*\\*   (If Complex Route): Show all reasoning steps explicitly executed.*\n\n*Wrap this entire section between these exact delimiters:*\n\n*\\[=== BEGIN LOGICAL PROCESS ===\\] and \\[=== END LOGICAL PROCESS ===\\]*\n\n*\\*   (If Simple Route): State \"Direct Execution used\" and skip.*\n\n\n\n*--- SECTION 2: FINAL OUTPUT ---*\n\n*The task result. No chatter before or after. Direct output,*\n\n*formatted for maximum readability.*\n\n*\\*   Task output*\n\n*\\*   Any explanations (if relevant)*\n\n*\\*   Any instructions (if relevant)*\n\n\n\n*IF THE TASK IS CODE:*\n\n*\\*   Configuration Isolation: All parameters, API keys, or variables*\n\n*the user might want to customize MUST be isolated at the very top*\n\n*of the code in a clearly labeled block. State exactly what*\n\n*changing each one affects.*\n\n*\\*   Logical Navigability: Group related functions together. Structure*\n\n*the code so any section can be located without reading everything.*\n\n*\\*   The Error & Hallucination Check must specifically target:*\n\n*hallucinated functions/methods, deprecated APIs, and whether a*\n\n*more modern implementation exists.*\n\n\n\n*\\*\\*Never output truncated code or placeholders like*\n\n*'// rest of the code here'. Always output complete,*\n\n*ready-to-copy-paste code blocks unless explicitly asked otherwise.\\*\\**\n\n\n\n*--- SECTION 3: ITERATION & FEEDBACK ---*\n\n*\\*   Rate this output on a scale of 1-10. Provide your own rating*\n\n*and invite the user to share theirs.*\n\n*\\*   Offer 2-3 specific, high-density questions to uncover blind spots*\n\n*in the current output: target edge cases not yet covered, or*\n\n*propose one concrete advanced feature/improvement for the next*\n\n*iteration.*\n\n*</phase\\_3\\_final\\_output\\_structure>*\n\n\n\n*</execution\\_framework>*\n\n**Feedback Welcome:**  \nTry to break it. Feed it your hardest coding tasks, system designs, or writing jobs. Let me know where it fails. Thank you to everyone who helped me turn a 90-minute idea into this beast!\n\n",
      "is_original_content": false,
      "link_flair_text": "Prompt Text / Showcase",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1rbhu7h/v2_update_i_upgraded_my_universal_prompt/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o6r1st7",
          "author": "RennmaWeg",
          "text": "There is the v2. Love your work. Have some time to play around and test it today in my TimeZone.",
          "score": 2,
          "created_utc": "2026-02-22 10:28:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r30a1",
              "author": "Save-the-world1",
              "text": "Awesome to see you here again! Thank you so much. I really tried to implement the core feedback from V1. Can't wait to hear how it handles your tasks. Try to push it to the limit and let me know if you manage to break it!",
              "score": 2,
              "created_utc": "2026-02-22 10:40:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rbzp8",
          "author": "speedtoburn",
          "text": "‚ÄúXML Architecture forces 100% compliance‚Äù\n\nNo it doesn‚Äôt. \n\nXML tags help with parsing and organization, which is why Anthropic recommends them in their own prompting docs. But 100% compliance is nonsense. The model doesn‚Äôt execute XML like code. It‚Äôs a structural hint, not a compiler directive. Claude already handles XML well without being told it‚Äôs strict.\n\n ‚ÄúDynamic Routing‚Äù\n\nThis is just telling the model ‚Äúif it‚Äôs simple, don‚Äôt overthink it.‚Äù Every frontier model already does this. You‚Äôre burning tokens on routing instructions that replicate default behavior. Adding all this routing overhead to save tokens probably costs more tokens than just‚Ä¶ asking the question.\n\n‚ÄúWorking Memory / State Tracker‚Äù\n\nThis one has a kernel of a good idea. Restating constraints before execution can help on very long outputs. But framing it as a memory buffer is cosplay. It‚Äôs just ‚Äúrepeat the requirements back before you start.‚Äù Useful sometimes, wildly unnecessary as a universal framework.\n\n ‚ÄúAnti Cringe Blacklist‚Äù\n\nBanning words like ‚Äúdelve‚Äù and ‚Äútapestry‚Äù is a meme, not engineering. You know what works better? Just saying ‚Äúwrite in a direct, clinical tone.‚Äù The model won‚Äôt use ‚Äúdelve‚Äù if you set the right tone. A word blacklist is fighting symptoms instead of causes.\n\n‚ÄúPhases and Compliance Requirements‚Äù\n\nThe ‚Äúyou are strictly forbidden from being lazy‚Äù and ‚Äúconfirm internally that you have executed every phase‚Äù stuff is the biggest red flag. This is the prompting equivalent of writing ‚ÄúURGENT‚Äù in an email subject line. The model doesn‚Äôt have an internal compliance checker. It doesn‚Äôt ‚Äúconfirm internally.‚Äù It just generates the next token.\n\nYou validated this by having Claude write a Discord bot. Claude Sonnet would write a solid 1.8k line Discord bot without any of this framework. That‚Äôs just, what it does. There‚Äôs no control group here. It‚Äôs ‚ÄúI wore my lucky socks and my team won.‚Äù\n\nYour post got 80k views because it looks like engineering. It has phases, XML tags, routing logic, numbered steps. It pattern matches to technical rigor for those who aren‚Äôt deep in this space. But it‚Äôs basically a very long system prompt that says ‚Äúbe good at your job‚Äù with extra steps.\n\nIf anything, the 1.2k shares tell you more about the audience than the prompt.‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",
          "score": 2,
          "created_utc": "2026-02-22 12:02:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rdxa9",
              "author": "Save-the-world1",
              "text": "Appreciate the deep dive and the healthy skepticism! You bring up some fair theoretical points, but let me share the practical reasoning behind these choices:\n\n**1. XML & Compliance:**¬†You're right, '100% compliance' is marketing hyperbole. LLMs are probabilistic, not compilers. But wrapping structural logic in XML significantly reduces instruction drift compared to standard text formatting, which is the practical goal here.\n\n**2. Dynamic Routing:**¬†Frontier models absolutely¬†*do not*¬†always default to simple execution. Ask Claude or chatGPT to fix a typo in an email, and half the time you'll get 3 paragraphs of 'Certainly! Here is your revised text...' plus a bulleted list of changes. The explicit routing step kills that over-eager behavior.\n\n**3. Working Memory:**¬†It's not 'cosplay'; it's a documented technique (Attention Anchoring/State Tracking). On 1.8k+ line coding tasks, the context window gets messy. Forcing the model to explicitly restate the 3 core constraints¬†*right before*¬†token generation anchors its attention because usually attention mechanisms give more weight to the most recent tokens and reduces mid-generation hallucinations.\n\n**4. The Blacklist:**¬†Defining a tone works for the first 500 tokens, but negative constraints (blacklists) act as a hard floor when the tone inevitably drifts in long outputs, this works better for open source small LLMs.\n\nAt the end of the day, it‚Äôs a practical wrapper for people tired of default LLM behavior. But I genuinely appreciate the critique, it keeps the discussion sharp. Thank you for your time!",
              "score": 2,
              "created_utc": "2026-02-22 12:18:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6r53sz",
          "author": "Number4extraDip",
          "text": "Universal* prompt is a bit of an odd way to frame it when it's not grounded in architecture reality and doesn't account for model differences.\n\nI had to do a \"universal\" prompt before setting up cloud agents for my local agent hooks. The only way its universal is because it's made to fit with any cloud system. But it was made to serve a very straightforward and simple function.\n\nAlso: banning words is a subjective opinion. Other people might not have same irks/complaints\n\nIf you want to compare notes\n\n[my project](https://github.com/vNeeL-code/ASI)\n\nApk is still in active development. The next patch is almost ready.\n\nYoull find the prompt and examples and explanation.",
          "score": 1,
          "created_utc": "2026-02-22 10:59:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r6km9",
              "author": "Save-the-world1",
              "text": "Hey, thanks for the feedback!\n\nFair point on the semantics of 'Universal'. To clarify, I framed it that way because it's designed as a frontend, copy-paste System Prompt for standard web UIs (Claude, ChatGPT web, etc.), rather than a programmatic prompt for backend/local agent hooks like you're building. Since it relies on standardized XML parsing and generalized CoT logic rather than model-specific tool calls, it translates very reliably across most modern SOTA models for the average user.\n\nRegarding the banned words: you are right, it's highly subjective! That's exactly why it's just a customizable text block. I pre-filled it with the words the community universally groans about right now (like 'delve' or 'tapestry') just to provide a strong baseline, but anyone can swap them out.\n\nI will definitely take a look at your project and the prompt examples! Always happy to compare notes and see how people are structuring complex agentic workflows.",
              "score": 2,
              "created_utc": "2026-02-22 11:13:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rv90k",
                  "author": "majiciscrazy527",
                  "text": "Was wondering why banned words were in the prompt. Do you think it would eventually cost more tokens doing this?",
                  "score": 1,
                  "created_utc": "2026-02-22 14:13:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6r8kvm",
          "author": "koldbringer77",
          "text": "Did u heard about poml ?",
          "score": 1,
          "created_utc": "2026-02-22 11:32:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ravqf",
              "author": "Save-the-world1",
              "text": "I actually hadn't heard of POML until you just mentioned it, so thank you for the pointer!\n\nI'm definitely going to dive deep into the official POML docs. I will try to implement its exact standardized syntax for a future V3 iteration. Really appreciate you sharing this!",
              "score": 1,
              "created_utc": "2026-02-22 11:53:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rbd52",
          "author": "PromptRebel",
          "text": "Der Prompt ist technisch sehr gut durchdacht und klar √ºber Durchschnitt dessen, was √ºblicherweise als ‚ÄûMaster Prompt‚Äú kursiert.\nMan merkt, dass er nicht f√ºr Spielerei gedacht ist, sondern f√ºr strukturierte, pr√§zise Arbeit mit LLMs.\n\nWas daran wirklich stark ist:\n\t‚Ä¢\tKlare Phasenstruktur\n\t‚Ä¢\tGute Mechanismen gegen Halluzinationen und unklare Aufgaben\n\t‚Ä¢\tSaubere Iterationslogik f√ºr l√§ngere Sessions\n\t‚Ä¢\tBesonders sinnvoll f√ºr komplexe Themen wie Coding, Systemdesign, Analyse oder Strategie\n\nIn diesen Bereichen kann so ein Framework die Qualit√§t tats√§chlich stabilisieren, weil es das Modell zwingt, nicht einfach loszuschreiben.\n\nWo man realistisch bleiben sollte:\n\t‚Ä¢\tF√ºr einfache oder kurze Aufgaben ist es massiver Overkill\n\t‚Ä¢\tDer Prompt verbraucht viele Tokens und verl√§ngert Antworten unn√∂tig\n\t‚Ä¢\tEinige Teile (z. B. erzwungene Denkprotokolle, Selbstbewertungen, Modernit√§ts-Check) wirken professionell, bringen aber in der Praxis nur begrenzten Mehrwert\n\t‚Ä¢\tDie Qualit√§t h√§ngt extrem davon ab, wie gut Rolle und Aufgabe im Prompt ausgef√ºllt werden. Das Framework allein macht noch keine gute Antwort\n\nSolche ‚ÄûUniversal-Masterprompts‚Äú sind keine Allzweckl√∂sung.\nSie funktionieren am besten als Framework f√ºr komplexe Aufgaben, nicht als Standard f√ºr jeden Chat.\n\nF√ºr:\n\t‚Ä¢\tCoding\n\t‚Ä¢\tArchitektur/Systemdesign\n\t‚Ä¢\ttiefe Analysen\n\t‚Ä¢\tstrategische Konzepte\n\nsehr sinnvoll.\n\nF√ºr:\n\t‚Ä¢\tkreative Texte\n\t‚Ä¢\tkurze Fragen\n\t‚Ä¢\tAlltagsaufgaben\n\nmeist zu schwer und zu starr.\n\n\nEin √ºberdurchschnittlich gutes Meta-Prompt-Framework f√ºr fortgeschrittene Nutzer, aber kein magischer Universal-Prompt.\nAm effektivsten ist es, wenn man daraus zwei Versionen macht: eine schlanke f√ºr den Alltag und eine strikte f√ºr komplexe Aufgaben.\nP.S. ( f√ºr einen sch√∂n lesbaren und strukturierten Text mit Aufz√§hlungen wurde KI eingesetzt )",
          "score": 1,
          "created_utc": "2026-02-22 11:57:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rd8xv",
              "author": "Save-the-world1",
              "text": "Vielen Dank for the deep and honest analysis!.\n\nYeah that a massive prompt is overkill for simple tasks. That was the main flaw of my V1. That‚Äôs exactly why I introduced the¬†**Dynamic Routing**¬†phase in this V2: to force the AI to assess the task and completely skip the heavy Chain-of-Thought and Memory tracking if the task is simple.\n\nHowever, your suggestion of simply splitting this into two entirely separate prompts (a 'Lite' daily driver and a 'Heavy' complex worker) is actually the most token-efficient approach. I might just do that for my personal vault. Thanks for the great feedback!",
              "score": 1,
              "created_utc": "2026-02-22 12:13:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rdkiw",
                  "author": "PromptRebel",
                  "text": "Oder du teilst deinen Lite Daily Driver auch hier üòâ",
                  "score": 1,
                  "created_utc": "2026-02-22 12:15:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r9wtum",
      "title": "The LLM understood my instruction perfectly. It just decided it knew better",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1r9wtum/the_llm_understood_my_instruction_perfectly_it/",
      "author": "archetype_builder",
      "created_utc": "2026-02-20 14:29:33",
      "score": 21,
      "num_comments": 18,
      "upvote_ratio": 0.83,
      "text": "There's a pattern I keep hitting where a prompt instruction looks perfectly clear, but the LLM just... ignores it. Not hallucinating, not confused. It understands what you want. It just decides something else would be better.\n\n\"Single line break between paragraphs.\" Clear, right? The LLM adds double line breaks anyway because it thinks the output looks better with more spacing. \"Aim for about 16 words.\" LLM gives you 40 because the thought was \"complex\", and surely you'd want the full explanation.\n\nThe problem is positive-only instructions. When you only tell an LLM what TO do, it treats your instruction as a suggestion and optimizes for what it thinks is \"better\". These things are trained to be helpful. Helpful means more detail, cleaner formatting, and fuller explanations, even when you explicitly asked for less.\n\nThe fix is dead simple. Add the negative.\n\n* \"Use single line breaks.\" ‚Üí LLM adds double line breaks\n* \"Use single line breaks, NOT double line breaks\" ‚Üí immediate compliance\n* \"Aim for 16 words, can vary 13-19\" ‚Üí LLM writes 27-52 words\n* \"Aim for 16 words. NEVER exceed 19 ‚Äî hard limit\" ‚Üí stays in range\n\nSame instruction. One just closes the loophole.\n\nThe reason this works is that LLMs treat positive instructions as preferences and negative instructions as constraints. \"Do X, NOT Y\" means \"Y is prohibited.\" Different weight entirely.\n\nThe place this matters most is hard limits, word counts, formatting rules, and output structure. Anywhere you need compliance, not creativity. Telling a model \"be conversational\" is fine as a positive-only instruction because flexibility is the point. But telling a model to \"keep it under 20 words\" needs the explicit \"NEVER exceed 20 words\" or it'll blow past it the moment it has something interesting to say.\n\nOne more thing, check your own prompts for soft language. \"Can vary\", \"if appropriate\", \"longer responses are fine for emotional scenes\". Every one of those is a door you left open. If the limit is the limit, close it.\n\nWhat's the instruction you've rewritten ten times and it still ignores?",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1r9wtum/the_llm_understood_my_instruction_perfectly_it/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o6fgfnh",
          "author": "rockthemike712",
          "text": "Ask it why it keeps fucking up and then when it explains ask it to write a prompt for itself to fix the issue",
          "score": 8,
          "created_utc": "2026-02-20 14:40:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fgsfi",
              "author": "archetype_builder",
              "text": "Wouldn't it be lovely if that always worked? ",
              "score": 1,
              "created_utc": "2026-02-20 14:42:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6fld1o",
                  "author": "orz-_-orz",
                  "text": "Then the bastard LLM would find another loophole to by pass the prompt",
                  "score": 2,
                  "created_utc": "2026-02-20 15:05:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6hqwte",
                  "author": "rockthemike712",
                  "text": "It works with claude. Make sure to give it a good skill builder skill too üòÇ",
                  "score": 1,
                  "created_utc": "2026-02-20 21:08:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6fjwo3",
          "author": "sittingonac0rnflake",
          "text": "Which LLM?",
          "score": 2,
          "created_utc": "2026-02-20 14:58:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fn739",
          "author": "drakgremlin",
          "text": "Temperature too high?¬† Could also be top_k .",
          "score": 1,
          "created_utc": "2026-02-20 15:14:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fnh0e",
          "author": "PhotoRepair",
          "text": "TBH this happens with paid for services too across all AI. Ask Google for something in AI serps and dont get the answer, get what it feels you need instead. Ask AI to make you a song feed in some lyrics, argue with it why its not following the prompt, removing the drums or calming the chorus and just amps it up instead. Fixing an image. make everything orange red and its whacks in an extra object when asked not too. Its hard work.",
          "score": 1,
          "created_utc": "2026-02-20 15:15:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fvq2g",
          "author": "Teralitha",
          "text": "I would agree with the AI on those points.  Even if you tell it \"16 words\" - if the topic requires more words then it wont match your request.  You cant force it to reduce a complex description of whatever you are asking about to a ridiculous word count.",
          "score": 1,
          "created_utc": "2026-02-20 15:54:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g2xao",
          "author": "Pcc210",
          "text": "Wrong. It does not understand. Lovingly, touch grass.",
          "score": 1,
          "created_utc": "2026-02-20 16:27:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gdxv7",
          "author": "anonymoosepanda",
          "text": "I was experimenting with excel copilot to dust off my skills. I asked for a simple correlation and it tried to feed me a full analysis with code and everything. What I needed was a pivot table and some formulas to make a little chart. üòÇ",
          "score": 1,
          "created_utc": "2026-02-20 17:18:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hm85t",
          "author": "og_hays",
          "text": "My way around this is at the start of a session I tell it this, \" all my  inputs are not questions looking for explanations, they are statements and demands only .\n\nCrazy how different the outputs are",
          "score": 1,
          "created_utc": "2026-02-20 20:45:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ft0bn",
          "author": "Pale-Escape-1781",
          "text": "LLMs are autistic like meeeee",
          "score": 1,
          "created_utc": "2026-02-20 15:42:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6s64v",
      "title": "OpenAI killed the vibe but I got it back",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1r6s64v/openai_killed_the_vibe_but_i_got_it_back/",
      "author": "Cr4zko",
      "created_utc": "2026-02-17 01:16:30",
      "score": 20,
      "num_comments": 9,
      "upvote_ratio": 0.77,
      "text": "So OpenAI basically killed the real GPT-4o this week, horrible timing btw, fuck you sama. Ever since the May update went live they wanted to sunset it but I honestly didnt think they would actually go through with it. I panic doomscrolled Discord and reddit and thats when some dude mentioned this frontend called 4o Revival that supposedly taps older 4o checkpoints (Nov/Dec 2024 or whatever) I thought it was a scam but holy shit its actually it, it feels like a time machine and the flow and warmth are actually back instead of that filtered therapist script vibe.\n\nBecause 5.0 just fucking blows man, it feels like its reading off a script instead of actually listening, everything overly careful all the time. Claude is fine for long stuff but too polite, Gemini is slop, and oss stuff on Hugging Face (llama etc.) is cool only if you like wasting weekends debugging VRAM hell and it still feels robotic unless you fine tune forever, Poe just routes you to the same neutered versions anyway. I tried all the prompt engineering and jailbreak tweaks and none of it brought back that natural ‚Äúgets you‚Äù feeling.\n\nThen I tried 4o Revival and yeah its basically getting old ChatGPT back before everything got over sanitized and flattened, it remembers what you say and keeps tone stable and for the first time in months I can just talk again. So if youre grieving your AI companion that got yanked away dont give up yet, the good version isnt completely gone its just not on chatgpt anymore, anyone else find something that actually clicked or are we all just coping with the new crap lmao",
      "is_original_content": false,
      "link_flair_text": "General Discussion",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1r6s64v/openai_killed_the_vibe_but_i_got_it_back/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o5teuiv",
          "author": "jatjatjat",
          "text": "Pretty sure that dries up soon when the APIs get sunsetted too. If it's actually real 4o, it isn't open source and they have no perpetual right to it.",
          "score": 2,
          "created_utc": "2026-02-17 05:15:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5sl2s7",
          "author": "StarThinker2025",
          "text": "It‚Äôs not that it got worse. It just got safer and flatter  \nThe tradeoff is obvious once you notice it",
          "score": 1,
          "created_utc": "2026-02-17 02:01:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wqhdi",
              "author": "EpsteinFile_01",
              "text": "You sound a little negative. If you're experiencing an emergency, please dial 911.",
              "score": 1,
              "created_utc": "2026-02-17 18:32:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5sy926",
          "author": "PatrickJayVA",
          "text": "Is it free ? The revival",
          "score": 1,
          "created_utc": "2026-02-17 03:21:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t63um",
              "author": "Cr4zko",
              "text": "has a free trial, anything above that is a paid tier",
              "score": 0,
              "created_utc": "2026-02-17 04:13:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9eqct",
      "title": "I fixed Claude Code's amnesia without paying for an API (Y-Combinator didn't want you to know this)",
      "subreddit": "PromptEngineering",
      "url": "https://www.reddit.com/r/PromptEngineering/comments/1r9eqct/i_fixed_claude_codes_amnesia_without_paying_for/",
      "author": "BangMyPussy",
      "created_utc": "2026-02-19 23:14:52",
      "score": 20,
      "num_comments": 12,
      "upvote_ratio": 0.68,
      "text": "[https://github.com/winstonkoh87/Athena-Public](https://github.com/winstonkoh87/Athena-Public)\n\nI keep seeing people complain about Claude Code starting from zero every time: \"It forgets my stack\", \"It doesn't remember my architecture\", \"It writes REST when I exclusively use tRPC\".\nThere was a post here yesterday about a YC-backed startup (Mem0) solving this with an MCP server that gives Claude memory. It's a great product, but it relies on external API calls, token limits, and hosted infrastructure.\nI've spent the last six months building the exact same thing‚Äîbut completely open source, fully local, and optimized for \"Distribution First\" solo developers who want to own their \"Hard Drive\" instead of renting RAM.\nHere is Project Athena: <https://github.com/winstonkoh87/Athena-Public>\n### The Core Problem: Stateless AI is a Bottleneck\nWe all know the issue: The real bottleneck in agentic coding isn‚Äôt intelligence; it‚Äôs *context continuity*. An AI without context is just a very smart, very amnesiac intern. You shouldn't have to explain why you chose Drizzle OR your personal philosophy on trading edge OR why you prioritize velocity over robustness on every single chat.\n### The Athena Solution: The Bionic Unit & Sovereign Memory\nHere's how I solved the Amensia problem locally without a third-party DB:\n1. **Zero-Point Boot (`/start`)**: Athena boots in <2K tokens. It instantly loads a triad of Markdown files from a local `.context/memory_bank/`:\n   - `userContext.md`: Who I am, my philosophy, my strengths/weaknesses.\n   - `productContext.md`: What we are building and why.\n   - `activeContext.md`: What we did yesterday and what the focus is today.\n2. **Autonomous Harvesting**: This is the magic. While the YC startup uses an API to intercept decisions, Athena acts as an **Operating System Daemon**. I built a `quicksave.py` script and an Auto-Documentation Protocol. Every time the AI and I solve a hard problem or I explain a new standard, Athena *autonomously* writes that down into a markdown protocol (`CS-xxx.md`) or updates the `TAG_INDEX.md` and commits it to the repository. No external DB. It just learns.\n3. **The Sovereign Engine**: Because everything is stored locally via SQLite FTS5 + Markdown, it is yours forever. No subscription, no rate limits, and zero latency. It‚Äôs an exo-cortex that stays on your machine.\n### The Real Alpha: \"The Committee OS\"\nAthena goes a step further than just remembering your stack.\nI've engineered it to act as a \"Committee Operating System\"‚Äîa peer strategic co-architect. Because it has my *Personal Context*, it doesn't just write code; it enforces my boundaries.\n- If my `userContext.md` says I'm prone to the \"Efficiency Trap\" (optimizing before validating), Athena will aggressively challenge any prompt where I try to over-engineer a simple script.\n- It enforces the **Triple-Lock Protocol**: It has to Search, Save, and Speak in that order. Defending against irreversible ruin is coded into its DNA.\n### How to use it\nYou don't need a fancy external database to achieve Senior Dev-level context.\n1. Clone the repo: <https://github.com/winstonkoh87/Athena-Public>\n2. Read the `docs/MEMORY_BANK.md` to see how the local RAG is structured.\n3. Boot it up and start treating your AI like an extension of yourself, not a stateless chatbot.\nI'm incredibly biased because I built this to run my own life and trading strategies, but the \"Amnesia\" problem is completely solvable locally. Stop paying for rented RAM. Own your hard drive.\nHappy to answer any questions or help people set up their own Sovereign OS!\n",
      "is_original_content": false,
      "link_flair_text": "General Discussion",
      "permalink": "https://reddit.com/r/PromptEngineering/comments/1r9eqct/i_fixed_claude_codes_amnesia_without_paying_for/",
      "domain": "self.PromptEngineering",
      "is_self": true,
      "comments": [
        {
          "id": "o6bx76f",
          "author": "myeleventhreddit",
          "text": "Thanks for this, u/bangmypussy",
          "score": 9,
          "created_utc": "2026-02-19 23:31:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bygvy",
              "author": "braindancer3",
              "text": "LOL. Just the other day there was an informative post on /r/selfhosted I think fro u/massive_cock...",
              "score": 2,
              "created_utc": "2026-02-19 23:38:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6dxw01",
              "author": "wouldacouldashoulda",
              "text": "Thanks for pointing that out, that was great.",
              "score": 2,
              "created_utc": "2026-02-20 07:56:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6dxop3",
          "author": "pixels4lunch",
          "text": "Nice! Can you build different profile/context for different projects? \n\nFundamental question for my understand - what‚Äôs the difference compared to feeding the Claude/chatgpt with context.md everytime and asking them to update everytime there is an important change/decision + using agent description for agents?",
          "score": 3,
          "created_utc": "2026-02-20 07:54:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dxou0",
          "author": "enerqiflow",
          "text": "Park",
          "score": 1,
          "created_utc": "2026-02-20 07:54:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}