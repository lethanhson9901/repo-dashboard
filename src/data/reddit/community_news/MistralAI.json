{
  "metadata": {
    "last_updated": "2026-01-29 02:49:02",
    "time_filter": "week",
    "subreddit": "MistralAI",
    "total_items": 20,
    "total_comments": 101,
    "file_size_bytes": 114190
  },
  "items": [
    {
      "id": "1qoig0q",
      "title": "Vibe 2.0 - Terminally online Mistral Vibe.",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qoig0q/vibe_20_terminally_online_mistral_vibe/",
      "author": "Clement_at_Mistral",
      "created_utc": "2026-01-27 16:22:48",
      "score": 176,
      "num_comments": 29,
      "upvote_ratio": 0.99,
      "text": "Today, we're releasing Mistral Vibe 2.0 - a major upgrade to our terminal-native coding agent, powered by the state-of-the-art Devstral 2 model family. Build custom subagents, clarify before you execute, load skills with slash commands, and configure your own workflows to match how you work.\n\nMistral Vibe is¬†**now available on the Le Chat Pro and Team plans**¬†\\- with pay-as-you-go credits for power use, or bring your own API key.  \nDo you already have a Le Chat Pro/Teams plan? Get your Vibe key¬†[here](https://console.mistral.ai/codestral/cli).\n\n*Learn more about how to use Vibe*¬†[*here*](https://docs.mistral.ai/mistral-vibe/introduction)\n\n# Whats New\n\n* Mistral Vibe 2.0: Custom¬†**subagents**,¬†**multi-choice clarifications**,¬†**slash-command skills**,¬†**unified agent modes**, and¬†**automatic updates**.\n* Available today on¬†**Le Chat Pro and Team plans**¬†with PAYG for extra usage, or BYOK.\n* Devstral 2 moves to¬†**paid API access**:¬†**Free on the Experiment plan**¬†in Mistral Studio.\n* Enterprise services:¬†**fine-tuning**,¬†**reinforcement learning**, and¬†**code modernization**.\n\n*Learn more about*¬†[*Vibe 2.0*](https://github.com/mistralai/mistral-vibe)¬†*in our*¬†[*blog post*](https://mistral.ai/news/mistral-vibe-2-0)¬†*and*¬†[*product page*](https://mistral.ai/products/vibe)\n\nhttps://reddit.com/link/1qoig0q/video/bx60g52v3xfg1/player\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qoig0q/vibe_20_terminally_online_mistral_vibe/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o21pzju",
          "author": "Gen5nake",
          "text": "I hope the quotas will at least match those of other providers, but this is exactly what I was waiting for to cancel my Caude subscription :)\n\nOne thing I might have missed, though is where we can set an usage limit? How can we tell when the quota has been exceeded and the system switches to API usage? There's a global settings for API's but can't set to 0.",
          "score": 23,
          "created_utc": "2026-01-27 17:04:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21w0sn",
          "author": "sndrtj",
          "text": "Please tell me it keeps the quirky status messages. Loved \"petting le chat\" etc.",
          "score": 15,
          "created_utc": "2026-01-27 17:30:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26zuez",
              "author": "pandora_s_reddit",
              "text": "Yes :)",
              "score": 2,
              "created_utc": "2026-01-28 11:02:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o270lo3",
                  "author": "sndrtj",
                  "text": "Purrrrfect.",
                  "score": 1,
                  "created_utc": "2026-01-28 11:08:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21teez",
          "author": "deegwaren",
          "text": "Is Le Chat Pro vibe-CLI usage in third-party harnesses like OpenCode allowed and supported? Because disallowing use of subscription in other apps was the reason for me to cancel my Claude subscription.\n\nSince GitHub Copilot and GPT Plus/Pro officially support OpenCode, I really hope mistral does the same.",
          "score": 10,
          "created_utc": "2026-01-27 17:19:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22qzjf",
              "author": "DueKaleidoscope1884",
              "text": "this would be an important factor for me too\n\n  \nDoes Mistral typically reply to the questions in this sub?",
              "score": 1,
              "created_utc": "2026-01-27 19:43:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2776tq",
              "author": "vienna_city_skater",
              "text": "You can just use the API, no need to go through the CLI.",
              "score": 0,
              "created_utc": "2026-01-28 11:59:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27c299",
                  "author": "deegwaren",
                  "text": "But is API usage included in the Pro subscription?",
                  "score": 1,
                  "created_utc": "2026-01-28 12:33:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21mzy0",
          "author": "cosimoiaia",
          "text": "Unfortunately that broke completely the local functionality of it. \n\nI was using devstral-24b with local endpoints and after the update it's unable to do anything at all. üò¢\n\nDoes this mean that we can't use local models anymore?",
          "score": 9,
          "created_utc": "2026-01-27 16:51:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21nxqx",
              "author": "SourceCodeplz",
              "text": "Probably a bug, in Mistral Vibe they said from the start it should support local deployments of LLMs.",
              "score": 6,
              "created_utc": "2026-01-27 16:55:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o22ikam",
                  "author": "cosimoiaia",
                  "text": "Yeah, I agree. I will debug it more to see what's going on better but switching back to the old version worked as before.",
                  "score": 1,
                  "created_utc": "2026-01-27 19:06:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o21qfj1",
                  "author": "Downtown-Elevator369",
                  "text": "Yeah, the local option is still there under /model for me",
                  "score": 0,
                  "created_utc": "2026-01-27 17:06:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26zzgr",
              "author": "pandora_s_reddit",
              "text": "Hi there - It should be allowed, the team is taking a look but feel free to open an issue if you did not yet. You are free to use local models or any provider.",
              "score": 2,
              "created_utc": "2026-01-28 11:03:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27ftqy",
                  "author": "cosimoiaia",
                  "text": "Thank you for answering!\n\nStrangely enough I did a fresh reinstall and it started working also with the v2.0.0. \n\nMaybe something causes some conflict when just updating and the local model doesn't use any tool call.\n\nIn any case, thank you for the great work, I really love vibe!",
                  "score": 1,
                  "created_utc": "2026-01-28 12:57:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o22hqum",
              "author": "cosimoiaia",
              "text": "I switched back to v1.3.5 and everything started working again with the exact same setup, so it's definitely something with the new release.",
              "score": 1,
              "created_utc": "2026-01-27 19:02:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21i3f5",
          "author": "fxdev1",
          "text": "Can someone already say something about the usage quota compared to codex, gemini cli, claude code, antigravity?",
          "score": 4,
          "created_utc": "2026-01-27 16:30:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26mlb0",
              "author": "Gen5nake",
              "text": "Since I started testing yesterday, the quota seems much larger than with others. Yesterday, I did hit some API rate limits, but I could continue after a few minutes. Today, I‚Äôve already been coding non-stop for 1.5 hours and haven‚Äôt hit any limits yet. Also the context seems to fill up more slowly compared to Claude.  \nSo far it's great!",
              "score": 1,
              "created_utc": "2026-01-28 09:02:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27jder",
          "author": "NiceTryAmanda",
          "text": "so the pro plan comes with some usage built in? I generated an api key and /terminal shows that I'm spending money, though I have a pro plan",
          "score": 2,
          "created_utc": "2026-01-28 13:18:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2abdqv",
              "author": "Salt-Willingness-513",
              "text": "username checks out",
              "score": 1,
              "created_utc": "2026-01-28 20:52:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21khjy",
          "author": "Downtown-Elevator369",
          "text": "I just paid for a Pro plan this morning. This is great! Edit: I ran this to upgrade my existing Vibe setup on Mac: uv pip install --upgrade vibe",
          "score": 3,
          "created_utc": "2026-01-27 16:40:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21h6ut",
          "author": "EzioO14",
          "text": "Amazing news ! Can‚Äôt wait to test it out",
          "score": 2,
          "created_utc": "2026-01-27 16:26:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21lstg",
          "author": "Hofi_CZ",
          "text": "Is possible to use devstral via Kilo code as part of the Pro plan?",
          "score": 1,
          "created_utc": "2026-01-27 16:46:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21syb6",
              "author": "nordenstrom",
              "text": "Yes, I've been doing that for a while.",
              "score": 1,
              "created_utc": "2026-01-27 17:17:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o239d95",
          "author": "909876b4-cf8c",
          "text": "Is the user's input and data used for training, when using this through Le Chat Pro subscription?",
          "score": 1,
          "created_utc": "2026-01-27 21:06:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o261j2r",
          "author": "Old-Glove9438",
          "text": "Hope it‚Äôs better than Codex with GPT-5.2 high",
          "score": 1,
          "created_utc": "2026-01-28 06:03:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26xvkw",
          "author": "ProdbyTwoFace",
          "text": "Love your models for local inference so definitely gonna try that.",
          "score": 1,
          "created_utc": "2026-01-28 10:45:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a8wou",
          "author": "Sarttek",
          "text": "I'm new to this so sorry for asking but how exactly pricing works? I'm Pro Le Chat buyer, I downloaded the program and noticed that on the right lower corner there is a tokens counter. I assume that because of my pro sub my base token pool is higher? Or do I have to pay for it separately? What will happen if I use all of that? When will it renew?\n\nI tired looking for FAQs but nothing in there really answers my questions",
          "score": 1,
          "created_utc": "2026-01-28 20:41:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2b9flp",
              "author": "Gen5nake",
              "text": "The token counter you see is the context size usage for your current chat session, not your API usage or token balance. It indicates how much of the conversation history is being kept active in this chat. ;)",
              "score": 1,
              "created_utc": "2026-01-28 23:29:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qo7xoz",
      "title": "üòÇ",
      "subreddit": "MistralAI",
      "url": "https://i.redd.it/bsesmuvrbrfg1.jpeg",
      "author": "mobileJay77",
      "created_utc": "2026-01-27 08:10:53",
      "score": 146,
      "num_comments": 3,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qo7xoz/_/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1zru53",
          "author": "Nefhis",
          "text": "Meanwhile, Mistral... ü§£\n\nhttps://preview.redd.it/h4d8kw56evfg1.png?width=484&format=png&auto=webp&s=52cb4b2667463298c51071ff332a8d2d9fe55473",
          "score": 18,
          "created_utc": "2026-01-27 10:36:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zl238",
          "author": "Wickywire",
          "text": "Claude even has the puckering animation üòÖ",
          "score": 9,
          "created_utc": "2026-01-27 09:34:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zt5ba",
          "author": "whoisyurii",
          "text": "Lech√° üì¢",
          "score": 2,
          "created_utc": "2026-01-27 10:47:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmg70d",
      "title": "Move to Mistral",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qmg70d/move_to_mistral/",
      "author": "InternalBroad2522",
      "created_utc": "2026-01-25 10:57:42",
      "score": 141,
      "num_comments": 18,
      "upvote_ratio": 0.98,
      "text": "Currently I am using ChatGpt pro, Codex and GitHub Copilot, however I would like to switch to European provider or open source projects due to the critical situation with US. In your opinion, which are the best services I should use to do the switch I want?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qmg70d/move_to_mistral/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1mg6dg",
          "author": "thedisturbedflask",
          "text": "I'd suggest also using Mistral and other services to help refine a custom 'system prompt' for the Mistral's Le Chat to be more in line with what you need.\n\n\nI was initially a bit worried that i just wasn't getting the value out of chat compared to other services but creating my own instructions helped a lot with development especially but also day to day usage.\n\n\nDevstral2 vibe is also good but if your used to having it in an ide then the cline.bot vscode extension seems to work well",
          "score": 17,
          "created_utc": "2026-01-25 14:35:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sj0le",
              "author": "GreenStorm_01",
              "text": "Any hints on proper system prompting? It probably isn't helpful to just copy my customisation from chatgpt over, right?",
              "score": 1,
              "created_utc": "2026-01-26 09:48:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1vvh9g",
                  "author": "thedisturbedflask",
                  "text": "It's a good starting point.\n\n\nIf you're happy with chatgpt's responses you can ask it to help define a starting system prompt with the characteristics you need and set it as the instructions or agent prompt in Mistral.\n\n\nThen in Mistral you can ask a question that fits your use case repeatedly and you can then tweak the system prompt as you go.",
                  "score": 2,
                  "created_utc": "2026-01-26 20:18:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1llo8g",
          "author": "whoisyurii",
          "text": "mistral, mistral vibe or codestral + ollama\n\n**edit: devstral",
          "score": 10,
          "created_utc": "2026-01-25 11:07:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1lpvjl",
              "author": "cosimoiaia",
              "text": "Yes, except not ollama (too shady, buggy, bad software), better lmstudio or Jan. \n\nAlso the latest is called Devstral üôÇ",
              "score": 9,
              "created_utc": "2026-01-25 11:43:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1mwf31",
                  "author": "guyfromwhitechicks",
                  "text": "What's buggy about ollama?",
                  "score": 0,
                  "created_utc": "2026-01-25 15:53:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1sj6uz",
                  "author": "razziath",
                  "text": "Compared to ollama, lmstudio is very slow.  \nOllama is good. Ollama with Anythingllm is a very good option is you want a UI and add connectors/mcp/agents... to your llm.",
                  "score": 0,
                  "created_utc": "2026-01-26 09:49:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1mxh75",
          "author": "guyfromwhitechicks",
          "text": "This comment section being the equivalent of tumbleweeds really shows how big this problem is. You can look into /r/BuyFromEU and https://european-alternatives.eu/, they are ran by people who keep trying to solve \"what do I replace my american products with?\". The options are slim (especially for software) but it is getting better.",
          "score": 2,
          "created_utc": "2026-01-25 15:57:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1oahbu",
          "author": "UpstairsCheetah235",
          "text": "You could check out Proton‚Äôs lumo. It uses a variety of models and there‚Äôs a free tier to try out. Might be a good solution, especially for those switching email and cloud storage over to them.¬†",
          "score": 1,
          "created_utc": "2026-01-25 19:25:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wps7q",
              "author": "cosimoiaia",
              "text": "Proton has indeed excellent offerings, however, they don't have their own model but rather run other open weights models. I haven't checked in a minute but afaik they don't disclose which one and that for me is a security issue. But I'm sure it will become rock solid given where it comes from.",
              "score": 1,
              "created_utc": "2026-01-26 22:34:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1v27f8",
          "author": "empireofadhd",
          "text": "I also switched, also switching to mailo, but have not figured out how to use their office suite.",
          "score": 1,
          "created_utc": "2026-01-26 18:13:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22dost",
          "author": "New-era-begins",
          "text": "if too many switches to european they will run out of inference resources. Thats why switch,  but only with sensitive tasks, and the BS chat put to US AI so they loose money on electricity. Dont ever pay anything for US",
          "score": 1,
          "created_utc": "2026-01-27 18:45:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o274u0j",
          "author": "officialexaking",
          "text": "Or use a european open source AI like https://www.xprivo.com where you can also run a small Mistral model locally on your computer fully offline",
          "score": 1,
          "created_utc": "2026-01-28 11:42:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uzuve",
          "author": "GreenGreasyGreasels",
          "text": "if it is open source you could use the best Chinese open weight model hosted by yourself or by a trusted vendor in EU. the usual suspects DS, K2, GLM-4.7, M2.1 etc. Devstral 2 and Mistral Large 3 remain top notch choices. If you consider Russia European you can look at Gigachat :P",
          "score": 0,
          "created_utc": "2026-01-26 18:03:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ymr69",
              "author": "dsvost",
              "text": "I would add also add then SourceCraft stuff in that case..",
              "score": 1,
              "created_utc": "2026-01-27 04:50:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qnnieh",
      "title": "Devstrale 2 > other Chinese AIs like DeepSeek etc",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qnnieh/devstrale_2_other_chinese_ais_like_deepseek_etc/",
      "author": "nycigo",
      "created_utc": "2026-01-26 18:00:38",
      "score": 55,
      "num_comments": 23,
      "upvote_ratio": 0.97,
      "text": "Why is nobody talking about Devstrale 2 in the same way as GLM 4.7 Deepseek and Minimax when the AI ‚Äã‚Äãis in the top 6 on OpenRouter in the best programming AI category, ahead of all the other Chinese models and with a damn free API?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qnnieh/devstrale_2_other_chinese_ais_like_deepseek_etc/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1v1ilu",
          "author": "SourceCodeplz",
          "text": "Because the Chinese have an online army on reddit and they promote it heavily.  \nBut GLM, Deepseek and Minimax are really good actually, not like from Anthropic, but fine.",
          "score": 23,
          "created_utc": "2026-01-26 18:10:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1v1l3g",
          "author": "cosimoiaia",
          "text": "Hate.\n\nDevstral is a superb model, even the small-24b running locally is better than all the other open weights.\n\nBut if they start to admit that the privacy/consumer first policies actually don't block progress completely and that the EU can, and did, produce SOTA models for their size, their delusions will break and they'll have a panic attack.\n\nAt the WEF they openly admitted that they wanted only the US to be players in the AI field and that they should do anything to block progress for everyone else.",
          "score": 22,
          "created_utc": "2026-01-26 18:11:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22wo60",
              "author": "robogame_dev",
              "text": "    even the small-24b running locally is better than all the other open weights.\n\nFor devstral-2-small to beat all open weights it would have to beat devstral-2... \n\nI've got 60 million tokens through Devstral 2, it's a great model - is it better than every other open weight model at everything, even the ones that are 10x is param count? no, definitely not.",
              "score": 1,
              "created_utc": "2026-01-27 20:08:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o23105z",
                  "author": "cosimoiaia",
                  "text": "Well yes, I meant to compare size to size of course, I could have phrased It better. \n\nAlthough I have to say the ones with 10x the params are not mind-blowing better for me, most of the times they still have the same pitfalls, they just go a bit further, so the convenience and efficiency of Devstral remain unbeatable for me.",
                  "score": 1,
                  "created_utc": "2026-01-27 20:28:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1vxfe9",
          "author": "tisDDM",
          "text": "I did a few tests with devstral 2 (small) and both are performing very good in agentic coding. I did a post here [https://www.reddit.com/r/opencodeCLI/comments/1qlqj0q/benchmarking\\_with\\_opencode\\_opuscodexgemini\\_flash/](https://www.reddit.com/r/opencodeCLI/comments/1qlqj0q/benchmarking_with_opencode_opuscodexgemini_flash/) about benchmarking and said a few things about Devstral as well (but did not include the Devstral 2 results) because it is part of my subagent harness project (not published yet) - where I tried to use Devstral 2 as intelligent worker nodes. \n\nI found my results impressing. Both Devstral 2 Models were fully able to run the test suite. Deepseek 3.2 and Kimi K2  and Grok Fast showed a lot of issues with following agentic tasks.\n\nBut in case you ask me why I am not using devstral 2 for coding? It is far behind Opus and Codex. Not in quality of code. Behind in understanding and following a humans complex task. which both of the big two easily can manage. But this might be an issue of Reasonig.",
          "score": 4,
          "created_utc": "2026-01-26 20:27:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xm32o",
              "author": "Historical_Roll_2974",
              "text": "How did you get Devstral 2 Small to work on OpenCode? When I try to use it with OpenCode I get an error where the filepath is null and the message is null with the tools API? Thanks",
              "score": 1,
              "created_utc": "2026-01-27 01:19:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1z3bx1",
                  "author": "tisDDM",
                  "text": "I got Devstral 2 Small working as a subagent for my benchmark, that was very cool - but still a little bit flaky depending on prompting. I used Devstral 2 ( the big one) in Zed but I think it shall work in Opencode as well. I will give it a try again later.\n\nhttps://preview.redd.it/xel4553hrvfg1.png?width=1404&format=png&auto=webp&s=63299ff43b4cafade0e9c046ac8bf38c132d1a6d\n\nI gave it a try. It works OOB with the Mistral-API and an API-Key. It's free.",
                  "score": 2,
                  "created_utc": "2026-01-27 06:54:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1v1sd5",
          "author": "Front_Eagle739",
          "text": "Well, its slow to run locally and while decent at coding its less flexible, mostly from the lack of ability to turn on high thinking. It's a nice model and it's got it's niche but glm 4.7 is usually better and more flexible even quantised to a similar size as devstral. It is a useful thing and I'm trying to use it more if only to support a european company but I think it kind of misses the benefit of being the only big dense model that will fit on a local 128GB machine (i.e. being smarter than anything comparable in size) due to said lack of thinking. For a really really smart model I could run locally I would be willing to wait but because it doesn't think it is not actually smarter than a q2 quantised glm 4.7 that also fits on my machine and it's slower. \n\nThats my take at the moment, I'm trying to explore it more, see if I can get more out of it.",
          "score": 8,
          "created_utc": "2026-01-26 18:12:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xjluk",
              "author": "Holiday_Purpose_3166",
              "text": "https://preview.redd.it/tqly3ygqksfg1.jpeg?width=4096&format=pjpg&auto=webp&s=8f21f93e021bfb6b05fab7706f02b541c4bf56af\n\nHaving spent billions of tokens last year in coding between different models under the sun, Devstral models are indeed excellent for what they are.\n\nl take the benchmarks as a guide and test them in my prod, and I find ironic how \"behind\" they visually look on the charts. Anecdotally, Devstral Small 2 repeatedly beats GPT-OSS-120B on my codebases despite being so far apart on benchmarks.\n\nGLM 4.7 Flash on the other hand is also excellent and kicks the 120B on the same stuff I'm working (ML, financial algos, Rust, NextJS, etc), but Devstral Small 2 has more eye to detail where GLM left some issues I had \"as is\".\n\nHowever, GLM 4.7 Flash is broadly smarter and more update to date on greater schemes compared to Devstral but has less knack on repo work. Like them both, but I lean towards Devstral over the fact it has more enterprise grade efficiency. Could be bias, but I like minimalistic response, and it can be extensively detailed when require (docs, blueprints >1k lines).\n\nAnother important note that resonates with my experience, the GPT-OSS models have large response variance where Devstral is more deterministic and is consistent on every round, and this reflected on SWE-Rebench charts from Ibragim.\n\nI find this variance inconvenient for coding despite tighter sampling as repeated, personal tests yielded different results.\n\nI haven't used GLM 4.7 Flash long enough to detect that but generally the apple doesn't fall far from tree if it follows the grand 4.7.",
              "score": 3,
              "created_utc": "2026-01-27 01:05:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1v95xy",
              "author": "SourceCodeplz",
              "text": "Yeah. Thinking adds a LOT o value, even though sometimes it can go in circles.",
              "score": 3,
              "created_utc": "2026-01-26 18:42:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1vafxw",
                  "author": "Front_Eagle739",
                  "text": "Yeah it really does. I get the value of a fast nonthinking model for code completion, i get the value of a slow reasoning model thats very coherent and intelligent for its size. I struggle to get the purpose of a slow non thinking model that maxes out my memory. the 24B devstral I do understand, I have more memory than would force me to go down to that size but if I was on 24GB/32GB That one I could see myself using. The 123B really wants reasoning to be the best possible answer I can get for 128GB memory option.",
                  "score": 1,
                  "created_utc": "2026-01-26 18:48:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1wlad8",
              "author": "cosimoiaia",
              "text": "Have you tried it with vibe? I've been using it quite a lot, for fairly complex tasks and, so far, it never failed, even with navigating in decent sized projects and it's FAST. Btw you need a lot less than 128GB to run it at full context, 62GB will leave you room to spare.\n\nAlso, I am very doubtful that a Q2 model of any size can do well coding tasks since they're very very quantization sensitive, doesn't matter the amount of thinking.\n\nEdit: this seems like an example of classic casting shadows disguised as comment. I hate worthless propaganda.",
              "score": 1,
              "created_utc": "2026-01-26 22:13:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1wmuiu",
                  "author": "Front_Eagle739",
                  "text": "Unsloth dynamic quants are a wonderful thing. I will admit the mlx 2 bit quants are fairly useless but the unsloth iq2m glm 4.7? That one works great. The minimax ones are worse though, not quite sure why but different models seem to take better or worse to dynamic quants and glm is the best ive seen. Anyway.\n\n\nNo i havent tried vibe with it. Tried it in claude code but I'll give vibe a go. Makes sense it would be optimised for its own harness.\n\n\nWont be fast though at least not the big one unless i go api, i get about 5 token/second with devstral 123 vs 60 with gpt oss 120 or 20 ish with glm. As i said, worth it if its good enough but it has to be consistent enough to not want to retry things a lot. Will give it a go in vibe.\n\n\nEdit. Thanks for declaring my opinions are worthless propoganda. Appreciate it. Really makes a man feel like he's in a good useful discussion. Honestly.",
                  "score": 1,
                  "created_utc": "2026-01-26 22:20:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ylw16",
          "author": "Waste-Intention-2806",
          "text": "Because it's a dense model. While minimax and reaped glm 4.7 r MOE models and most of us can run these at least 4-11 tokens per second. While devstral 2 was running at .7 to 1 token per second for me on i9 and 16gb rtx 4070 ti with 128 gb ram",
          "score": 1,
          "created_utc": "2026-01-27 04:44:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1v7wxo",
          "author": "Marciplan",
          "text": "It really isn't on par with GLM 4.7 though.",
          "score": 0,
          "created_utc": "2026-01-26 18:37:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vgpmv",
              "author": "kiwibonga",
              "text": "For agentic coding though, Devstral Small 2 scores 10 pts higher than GLM-4.7-Flash on SWEBench. The GLM team completely omitted it from their benchmarks, maybe because it's not MoE.",
              "score": 7,
              "created_utc": "2026-01-26 19:14:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vieoc",
          "author": "pinmux",
          "text": "I think Mistral offering their API for devstral-2 and devstral-small-2 for free is actually hurting adoption by inference providers and hence users don't know about it.\n\nIn my brief experience trying devstral-small-2, it's quite good.  I don't have beefy enough hardware locally to run it at a reasonable speed and last I checked the only cloud inference providers offering the devstral-2 models will train on your data (Mistral included for their consumer offerings).  On OpenRouter you get Mistral or Chutes, that's it.\n\nI'm hoping some cloud inference providers will pick up devstral-2 (and devstral-small-2) after tomorrow once Mistral starts charging for the API access.  That'll make it easier for people to find and use it.",
          "score": 1,
          "created_utc": "2026-01-26 19:21:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vt5xc",
          "author": "Large-Example-1275",
          "text": "It runs slowly on my DGX Spark compared to MoE alternatives.",
          "score": 0,
          "created_utc": "2026-01-26 20:08:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkv00t",
      "title": "Quick note",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qkv00t/quick_note/",
      "author": "Clement_at_Mistral",
      "created_utc": "2026-01-23 16:12:43",
      "score": 51,
      "num_comments": 40,
      "upvote_ratio": 0.96,
      "text": "Devstral 2 will move to paid API access starting January 27. You‚Äôll still get free usage under the¬†[Mistral Studio](https://console.mistral.ai/home)¬†Experiment plan.\n\nPS: something's coming next week!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qkv00t/quick_note/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1dyfe6",
          "author": "vienna_city_skater",
          "text": "Please keep it free for Le Chat Pro users or give us a subscription option for all your models that we can use via API. Honestly, it‚Äôs hard to justify paying 20‚Ç¨ per month for just chat usage.",
          "score": 11,
          "created_utc": "2026-01-24 07:04:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19fheh",
          "author": "SourceCodeplz",
          "text": "Maybe keep Devstral 2 Small free in Vibe at least?",
          "score": 5,
          "created_utc": "2026-01-23 16:20:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a6yyk",
              "author": "Bob5k",
              "text": "as long as you'll be on experiment plan you'll have devstral 2 free in vibe.",
              "score": 5,
              "created_utc": "2026-01-23 18:25:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o19k8i8",
          "author": "Egoz3ntrum",
          "text": "Thinking version?",
          "score": 3,
          "created_utc": "2026-01-23 16:41:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dyqku",
              "author": "vienna_city_skater",
              "text": "A Devstral model with inference time reasoning would amazing indeed.",
              "score": 3,
              "created_utc": "2026-01-24 07:06:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1gczsi",
              "author": "txgsync",
              "text": "I‚Äôve been using the ‚Äúsequential thinking‚Äù MCP.  It slows things down but improves accuracy, particularly in uncertain situations.",
              "score": 1,
              "created_utc": "2026-01-24 16:58:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1a0gpm",
          "author": "cosimoiaia",
          "text": "I suppose that will include vibe usage as well? üò¢",
          "score": 2,
          "created_utc": "2026-01-23 17:56:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19hpvl",
          "author": "Hopeful-Kale-5143",
          "text": "Excited to see what's coming! There is not much of a bump needed for codestral in order to make it really viable!",
          "score": 1,
          "created_utc": "2026-01-23 16:30:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19hzt1",
              "author": "EzioO14",
              "text": "Vibable you mean? üòÇ",
              "score": 7,
              "created_utc": "2026-01-23 16:31:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1bufvv",
          "author": "Mystical_Whoosing",
          "text": "do we know the 1m token prices?",
          "score": 1,
          "created_utc": "2026-01-23 23:06:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yztl1",
              "author": "nycigo",
              "text": "Really cheap, less than Deepseek v3.2 on OpenRouter, ‚Ç¨0.20 exit fee I think and ‚Ç¨0.05 entry fee per million tokens.",
              "score": 1,
              "created_utc": "2026-01-27 06:26:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ldof4",
          "author": "DueKaleidoscope1884",
          "text": "Of course I do not know the long term plans of Mistral but I do want them to succeed being the only European (seemingly) viable alternative to US and Chinese models.\n\nThe truth currently is, the models are not at the same level as the Codex or Anthropic. I have tried Devstral from Opencode but at some point I needed to get work done. I think it can handle a lot of the implementation but the competition is just easier to work with it seems.\n\nAt the same time I am happy to see Mistral Vibe BUT the documentation is either very limited or very hard to find. I came to the conclusion it is incomplete. I may be wrong. For example, I know Vibe supports skills because I saw the release notes but try finding the documentation on it.\n\nIn general I  do not think it reasonable to expect people to beta test a product (Vibe) that is lagging behind on the competition so much.\n\nWhat would keep me trying Vibe and Devstral is being able to use it free, daily or weekly limits are reasonable to impose, until it has caught up more to the competition.\n\nSo please consider making the Devstral models (limited) free on Mistral Vibe.  \n(I do not know what is technically possible given the trouble Claude Code is having limiting the subscription model to Claude Code only but a clear TOS from the help may help.)\n\nThis way I could keep on using (and testing) both.",
          "score": 1,
          "created_utc": "2026-01-25 09:56:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19ipf1",
          "author": "EzioO14",
          "text": "Thanks for the heads up. I hope they don‚Äôt make the api key paid because it‚Äôs super useful to test features on my projects",
          "score": 0,
          "created_utc": "2026-01-23 16:35:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19qtsl",
          "author": "Impossible_Comment49",
          "text": "Oh no! But at the same time, is anyone actually using it? I achieve significantly better results with OpenCode‚Äôs free models, such as Big Pickle or others. I was delighted that Mistral was free to test out occasionally, but I would never use it if it wasn‚Äôt free.\n\nOn the other hand, I‚Äôm disappointed. I was hoping for Mistral‚Äôs adoption and the widespread use of ‚Äòvibe‚Äô. This will likely not be beneficial for Mistral. ‚Äòqwen‚Äô remains free.",
          "score": -3,
          "created_utc": "2026-01-23 17:11:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a189i",
              "author": "cosimoiaia",
              "text": "That says that you never used Devstral at all.",
              "score": 3,
              "created_utc": "2026-01-23 17:59:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1a49an",
                  "author": "Cyberblob42",
                  "text": "Iam using devstral-2 via CLI. Its usable m, but Claude is better Unfortunately",
                  "score": 2,
                  "created_utc": "2026-01-23 18:13:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1aqdtq",
                  "author": "Impossible_Comment49",
                  "text": "I have, but it‚Äôs nowhere near Codex, Opus, or even GLM4.7.",
                  "score": 1,
                  "created_utc": "2026-01-23 19:54:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1elqnb",
              "author": "Ok-Elderberry-2923",
              "text": "Tried Vibe CLI with Devstral 2. It took 3 hours for it to rename a field in a sql entity + usage (20-30 files affected) in a small to medium sized KMP project. This includes function names, local variable names etc. It also stopped like 10 times and tried persuading me that I shouldn't continue as it's a lot of work :D I mean it's free but I could have done this by hand in 15min.\n\nOther tasks it performed way below sonnet. Maybe at the level of gemini or gbt (not sure, i dont use them much)\n\nAlso, Claude CLI + Sonnet took about 2min to do the same task in the same codebase",
              "score": 2,
              "created_utc": "2026-01-24 10:36:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ntn7g",
                  "author": "graymalkcat",
                  "text": "I‚Äôve had Opus spend 15 minutes just trying to make a change to a single line. Sometimes you just have to do it yourself and let the AI move on.",
                  "score": 2,
                  "created_utc": "2026-01-25 18:15:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1nt65z",
              "author": "graymalkcat",
              "text": "I came over to it in December, not even knowing it was free. I‚Äôve been using it as a sub agent for Opus. They work nicely together. I also use it to process and extract insights from code-heavy text (basically to evaluate agent work).",
              "score": 1,
              "created_utc": "2026-01-25 18:13:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1dyzj3",
              "author": "vienna_city_skater",
              "text": "I wouldn‚Äòt use Chinese models in a business environment, also not if they are hosted on US servers.",
              "score": 1,
              "created_utc": "2026-01-24 07:08:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ajd8h",
          "author": "Dutchbags",
          "text": "id happily pay if it werent so dogshit slow",
          "score": -8,
          "created_utc": "2026-01-23 19:21:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ntwqr",
              "author": "graymalkcat",
              "text": "Not sure why you‚Äôre downvoted when this model is in fact incredibly slow. That‚Äôs my biggest complaint about it.",
              "score": 0,
              "created_utc": "2026-01-25 18:15:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1c4ucc",
          "author": "AdElectronic7628",
          "text": "I can't believe peoples daring comparing Claude to Mistral",
          "score": -1,
          "created_utc": "2026-01-24 00:02:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpa554",
      "title": "Payment in EUR more expensive than in USD?",
      "subreddit": "MistralAI",
      "url": "https://v.redd.it/9xg4e9dz33gg1",
      "author": "d4v1d_dp",
      "created_utc": "2026-01-28 12:34:01",
      "score": 50,
      "num_comments": 14,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qpa554/payment_in_eur_more_expensive_than_in_usd/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o27drrt",
          "author": "Axiom05",
          "text": "The price in US dollars never includes VAT, unlike the price in euros.",
          "score": 59,
          "created_utc": "2026-01-28 12:44:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27kcj7",
              "author": "Jazzlike-Spare3425",
              "text": "Yes, this is it. You can see it by the asterisk. If you scroll down to the bottom of the table that compares the plan, just over the FAQ section, for the USD version it will say \"excluding taxes\" and for the Euro price it will say \"including taxes\"",
              "score": 11,
              "created_utc": "2026-01-28 13:24:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27f4ys",
              "author": "OwlSlow1356",
              "text": "paid one month in USD last year, the only currency available then although i am in europe but nonEUR country, and they deducted the VAT from USD price when providing a VAT number, second month they charged me full USD price, asked why if first month VAT was deducted, never received any answer, cancelled and good bye!",
              "score": 3,
              "created_utc": "2026-01-28 12:53:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o29i11u",
              "author": "Patrick_Barababord",
              "text": "Maybe, but $1 = 0,83‚Ç¨ also.  \nSo $14.99 = 12.44‚Ç¨ ...and 12.44 + 20% VAT = 14.9‚Ç¨.",
              "score": 1,
              "created_utc": "2026-01-28 18:42:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o284ehb",
          "author": "EzioO14",
          "text": "It‚Äôs just a tax matter, not an advantage to U.S",
          "score": 12,
          "created_utc": "2026-01-28 15:06:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27cktf",
          "author": "Little_Protection434",
          "text": "That seems to be the case. I just checked the exchange rate and based on that it should indeed be the opposite.",
          "score": 9,
          "created_utc": "2026-01-28 12:37:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o282bu9",
              "author": "ComeOnIWantUsername",
              "text": "Just read the pricing as a whole, not just one part. For USD it's \"excluding taxes\". For EUR \"including taxes\".",
              "score": 8,
              "created_utc": "2026-01-28 14:56:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o282og9",
                  "author": "Little_Protection434",
                  "text": "That makes sense. Thanks!",
                  "score": 2,
                  "created_utc": "2026-01-28 14:58:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28shtk",
          "author": "TheBl4ckFox",
          "text": "It‚Äôs always excluding tax for US customers and including VAT for EU.",
          "score": 3,
          "created_utc": "2026-01-28 16:52:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27teac",
          "author": "Nokushi",
          "text": "prices were aligned before, but were all without VAT included, which is uncommon in France\n\ni guess they now show EUR prices VAT included",
          "score": 2,
          "created_utc": "2026-01-28 14:12:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o280wet",
              "author": "sndrtj",
              "text": "And since VAT rates depend on the country of the _purchaser_, not the seller, exact amounts may change depending on where you are located.",
              "score": 3,
              "created_utc": "2026-01-28 14:49:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o286pfw",
          "author": "bluepuma77",
          "text": "Just saw that too, was irritated, it seems sooo stupid what they are doing with their pricing page.  \n  \n1. It's \"$15 with \\*\".   \n2. What's a \\*, okay pages down \\* means it's without tax.   \n3. Then I switch to Eur and it's \"‚Ç¨18 with \\*\"\n\nWho would have thought that the meaning of the \\*, many pages down, would silently change.\n\nI highly recommend to not hide the \\* so far down. And maybe use \\*1 and \\*2 or something.",
          "score": 1,
          "created_utc": "2026-01-28 15:17:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27txik",
          "author": "Far-Reaction-1980",
          "text": "To this day I don't get Mistrals pricing",
          "score": 1,
          "created_utc": "2026-01-28 14:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27htt4",
          "author": "Basiliscus219",
          "text": "Usually the prices are set based on the local purchase power. In poorer countries the price is lower.",
          "score": -3,
          "created_utc": "2026-01-28 13:09:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qndv3e",
      "title": "Ministral models are good.",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qndv3e/ministral_models_are_good/",
      "author": "Rent_South",
      "created_utc": "2026-01-26 11:45:46",
      "score": 47,
      "num_comments": 8,
      "upvote_ratio": 0.96,
      "text": "Just to say that in their weight class, ministral models (mainly 3b and 8b) are very cost efficient and quick, compared to other models.\n\nFor non complex tasks, they actually compete for the top spot.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qndv3e/ministral_models_are_good/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1t0lz8",
          "author": "scara1701",
          "text": "I like ministral:3b as well. Currently using it to test MCP tools I‚Äôm building :)",
          "score": 4,
          "created_utc": "2026-01-26 12:15:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1to112",
          "author": "stddealer",
          "text": "Yep, they've finally replaced Gemma3 models for me, though I think Gemma was a bit better at some things like translation or OCR, Ministral feels like a nice upgrade.",
          "score": 2,
          "created_utc": "2026-01-26 14:31:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1u0apu",
          "author": "Holiday_Purpose_3166",
          "text": "Mistral models are indeed good. I use them daily, especially Devstral Small 2 for my workflows where GPT-OSS-120B struggles to execute. What a time to be alive.",
          "score": 2,
          "created_utc": "2026-01-26 15:30:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23fy7r",
          "author": "kompania",
          "text": "From my perspective, they're very close to Gemma 3. They're incredibly talkative, competent, and handle drift well.\n\nThe downside is that they're currently untunable due to the lack of working notebooks.",
          "score": 2,
          "created_utc": "2026-01-27 21:35:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1v9vhq",
          "author": "Conscious-Expert-455",
          "text": "How to use these models? For vibe coding? As agents?\nI'd like to use them as agents or as MCP services.",
          "score": 1,
          "created_utc": "2026-01-26 18:45:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xac54",
              "author": "Rent_South",
              "text": "It all depends on your use case. Depending on your specific tasks, any of your suggestions are viable.  \nOne thing is certain is that if your use case fits these models, they perform really well.",
              "score": 1,
              "created_utc": "2026-01-27 00:18:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1szmp5",
          "author": "Scared_Range_7736",
          "text": "Still far behind American and Chinese models, unfortunately. Check this benchmark from a few days ago: [https://www.vals.ai/benchmarks/terminal-bench-2](https://www.vals.ai/benchmarks/terminal-bench-2)",
          "score": -7,
          "created_utc": "2026-01-26 12:08:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1t5me6",
              "author": "krkrkrneki",
              "text": "OP is referring to open models available to be run locally.",
              "score": 8,
              "created_utc": "2026-01-26 12:50:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qokht8",
      "title": "Mistral Vibe now available on subscriptions",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qokht8/mistral_vibe_now_available_on_subscriptions/",
      "author": "Holiday_Purpose_3166",
      "created_utc": "2026-01-27 17:33:24",
      "score": 46,
      "num_comments": 6,
      "upvote_ratio": 0.93,
      "text": "https://preview.redd.it/qi2hpnqdgxfg1.png?width=394&format=png&auto=webp&s=9aad3a7897e9e01465d0adf02a302a1a0546fc14\n\nMistral team announced on X the subscriptions are now available with access to Mistral Vibe coding.\n\nMassively appealing move.\n\n[https://x.com/mistralvibe/status/2016179799689928986?s=20](https://x.com/mistralvibe/status/2016179799689928986?s=20)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qokht8/mistral_vibe_now_available_on_subscriptions/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o21xt5y",
          "author": "datNovazGG",
          "text": "Arent we able to just download it and use it with something like OpenRouter, anymore? What's the difference that it's in the pro tier sub?",
          "score": 3,
          "created_utc": "2026-01-27 17:38:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2254x1",
              "author": "Holiday_Purpose_3166",
              "text": "OpenRouter is a proxy provider, unrelated to Mistral sub plans. I understand Mistral was offering the Devstral 2 models for free on OpenRouter and I believe these are finishing/finished - so after that it's PAYG.  \n  \nMistral plan description states \"all day coding\" with Mistral Vibe, so that wording expresses that you only have free\\* access to coding model only via Mistral Vibe agentic tool.   \n  \n*\\* No quotas are prescribed but they express \"Mistral Vibe for all-day coding, PAYG beyond.\" which does suggests limits.*\n\nBased on the available information, the main difference is you can't use the free coding model outside Mistral Vibe, like you could do with OpenRouter.",
              "score": 1,
              "created_utc": "2026-01-27 18:09:40",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o26izrg",
              "author": "robberviet",
              "text": "Just like how you can use claude code with sub or with key.",
              "score": 1,
              "created_utc": "2026-01-28 08:29:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21xmyi",
          "author": "minaskar",
          "text": "Any information on what the actual request limits are?",
          "score": 3,
          "created_utc": "2026-01-27 17:37:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o225d9w",
              "author": "Holiday_Purpose_3166",
              "text": "Good question. It's transparent as a door. In their plan it describes \"Mistral Vibe for all-day coding, PAYG beyond.\" but no suggestive references to what those limits are.",
              "score": 2,
              "created_utc": "2026-01-27 18:10:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o26u6xg",
          "author": "_coding_monster_",
          "text": "Does mistral have a VSCode extension (not CLI in VSCode) that works like Github Copliot or Kilo Code? Claude code supports VSCode extension",
          "score": 1,
          "created_utc": "2026-01-28 10:13:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qoc2ww",
      "title": "Give feedback if you want that Mistral improves",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qoc2ww/give_feedback_if_you_want_that_mistral_improves/",
      "author": "no_coykling",
      "created_utc": "2026-01-27 12:08:10",
      "score": 40,
      "num_comments": 6,
      "upvote_ratio": 0.95,
      "text": "Instead of just providing a new reply with what you think should be improved, also include feedback using thumbs-up/down buttons.\n\n* You have a good reply, thumbs up.\n* If your answer lacks information that was previously mentioned in the context, include it in your feedback.\n* If Mistral asks a question about something not specified, don't use it.\n\nThis is one way to improve the product, subscription is the other one ;)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qoc2ww/give_feedback_if_you_want_that_mistral_improves/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o205dps",
          "author": "nycigo",
          "text": "Try to get as close as possible to the Chinese models, I think, and try to highlight their models like Devstral 2 as much as GLM 4.7.",
          "score": 3,
          "created_utc": "2026-01-27 12:23:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22vjam",
              "author": "dcforce",
              "text": "I'm with you Devstral 2 is pretty amazing\n\nWish it had a bit larger context window and coding is likely only going to only improve on what is already a solid model.  \n\nLooking forward to Devstral 2.5 - 3 for sure",
              "score": 3,
              "created_utc": "2026-01-27 20:03:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2045y0",
          "author": "Jazzlike-Spare3425",
          "text": "I would love to help the product improve but I've had a back and forth with their support team (wonderful people, honestly) where I shared a plethora of bugs and other problems regarding their apps for iOS and iPadOS, and I don't think they really addressed any. But to be fair I haven't been checking back since I found it annoying enough to have eventually just given up waiting for fixes and started building my own Mistral frontend in SwiftUI to get the UX I wanted.",
          "score": 6,
          "created_utc": "2026-01-27 12:14:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o206clu",
              "author": "no_coykling",
              "text": "People has always a different roadmap then a company. But I agree that reproducible bugs should be on top.",
              "score": 7,
              "created_utc": "2026-01-27 12:30:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2072uc",
                  "author": "Jazzlike-Spare3425",
                  "text": "Yeah, and I'm assuming that subscriptions to normal customers aren't Mistral's bread and butter but I think the Le Chat Pro subscription is a bit too expensive to not have a desktop app and to have the mobile apps like they are. I did later use Gemini though and the Le Chat app was better than Gemini's at least. The Le Chat apps do lack important features and polish but the Gemini app was outright unusable for me at times.",
                  "score": 3,
                  "created_utc": "2026-01-27 12:34:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26txuh",
          "author": "Single_Concern6734",
          "text": "amazing",
          "score": 1,
          "created_utc": "2026-01-28 10:10:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnujw3",
      "title": "Mistral beats Gemini and Perplexity for competitive intelligence",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qnujw3/mistral_beats_gemini_and_perplexity_for/",
      "author": "Cachao-on-Reddit",
      "created_utc": "2026-01-26 22:03:26",
      "score": 32,
      "num_comments": 2,
      "upvote_ratio": 0.91,
      "text": "I've posted here before about being impressed by Mistral Medium. That was mostly as an API user.\n\n  \nThis time I ran most of the big consumer-facing LLMs against each other in a 'Deep Research' style task. The focus was competitor news.\n\nMistral didn't win. But I think did commendably well. Especially given:\n\n\\- (a) relative underdog status compared to other players on this list,\n\n\\- (b) I using the very fast free tier (unlike Claude's slow, very expensive tier), and\n\n\\- (c) it was \\*clearly\\* better than Perplexity and Gemini.\n\n\n\nhttps://preview.redd.it/bcbnmklhmrfg1.png?width=763&format=png&auto=webp&s=a9f06a7c5f9a38234ca58faf6a9e9b1758a3d30d\n\n\n\nYou can see more about the test here: [https://anatole.fyi/blog/competitive-intelligence-face-off](https://anatole.fyi/blog/competitive-intelligence-face-off)\n\n\n\nAnd yes, you'll see it's flawed. I only did one run per LLM. The prompt was bad. Obviously on another attempt or with a better prompt Gemini won't have quite such a meltdown. But, when I'm using these tools day to day I would rather not have to run them multiple times or craft my prompt. And I think this side-by-side beats pure anecdote when comparing LLM quality.\n\n  \nWill run another test soon. Let me know what you think.\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qnujw3/mistral_beats_gemini_and_perplexity_for/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o26dotz",
          "author": "enormousdino",
          "text": "so in the wake of Macron's shades, I asked them all the other day which politicians wear watches made in their own country (as Macron famously wears French watches, including v independent niche brands). \n\nMistral guessed right - including Macron, Joe Biden's Shinola, and even Modi's Jaipur  \nGemini didn't guess Macron, but did talk about Biden, Abe Shinzo's Seiko and Modi  \nChatGPT didn't guess Macron, but did mention Biden and Modi  \nClaude said it's not aware of any instances of politicians wearing such watches.... and I'm paying for it!!",
          "score": 1,
          "created_utc": "2026-01-28 07:42:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2brlzi",
              "author": "Cachao-on-Reddit",
              "text": "share the link? would love to see.\n\nI do think it's important to distinguish the model from the harness. Opus 4.5 is clearly a better model than Large. But clearly part of how claude.ai is wiring it up is yielding sub-par  results.\n\nside note: feel like Macron's shades are a huge Mistral branding opp",
              "score": 1,
              "created_utc": "2026-01-29 01:04:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qo03k1",
      "title": "Difficult in switching from claude",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qo03k1/difficult_in_switching_from_claude/",
      "author": "MikadinShinjuk",
      "created_utc": "2026-01-27 01:45:02",
      "score": 26,
      "num_comments": 15,
      "upvote_ratio": 0.84,
      "text": "Good morning everyone, I have a problem that's been bothering me. I'm trying to move away from as many US services as possible, but Claude seems like an insurmountable obstacle. Let me clarify: I'm not a developer. I use AI to search for information, help with solo role-playing games (RPG), and research Warhammer 40k lore and similar topics. With Claude, I feel like I'm in heaven, but I understand that more and more (especially in recent days), it would be better to distance myself from US services. For some time now, I've been trying to use Le Chat, but every time it seems to be lagging behind. It's as if it doesn't consider the nuances in what I say, doesn't analyze in depth, always stops at the first point, and doesn't go into detail thoroughly. Am I doing something wrong? Should I create specific agents? Should I give it more precise and less discursive instructions? I tend to create queries as if I were talking to a person, and this works well with Claude, but maybe not here? I need some feedback from those who use it as their main AI.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qo03k1/difficult_in_switching_from_claude/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1xt331",
          "author": "gdsfbvdpg",
          "text": "I tried using it as a gm for a Cyberpunk game and then using it as a group of 5 players in a d&d game that I DM. In both cases it was pure frustration. I took the same exact instructions/files over to Gemini (free version) and *boom* it works very well. \n\nIt makes me really sad. BUUUUT - I feel like Mistral might be where chatGPT was a year or year and a half ago. So I'm hopeful that there's a bright future in store for it. But right now?  *Le sigh*. It's just not there yet.  I'm going to keep paying for it though.  I refuse to give up.",
          "score": 18,
          "created_utc": "2026-01-27 01:57:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o216ngz",
          "author": "fonceka",
          "text": "I believe Mistral market is the pro market, not end user. They are a b2b startup, not like ChatGPT. In a business perspective, Mistral performs absolutely well. It's super fast, not verbose, and really sensitive to your context. For a solopreneur like me, it was much more helpful than GPT or Gemini to craft my marketing campaign, help me with landing page, find the adequate wording, lower ambiguities, etc.",
          "score": 8,
          "created_utc": "2026-01-27 15:40:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1y40mb",
          "author": "LewdManoSaurus",
          "text": "Mistral has some ways to go for sure. It's fun to mess around with, but for heavy usage for generative writing, it is definitely lagging behind by far imo (I had the subscription back in October last year). It's nice when it works, but in my experience it was a headache more often than not, or I had to make corrections so often that I was better off just using a different service.",
          "score": 12,
          "created_utc": "2026-01-27 02:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yqjm3",
          "author": "Den_er_da_hvid",
          "text": "I know what you feel.  coming from Gemini though. \n\nLast night I tried using it to get tips on my game playing Humankind. \nI gave then info but it kept going around in circles that did not make sense based on resources and where my territories where.\n\nLater I asked about airfry time vs  oven in a recipie and gave it a link. It did not use the link before I explicitly said so.",
          "score": 4,
          "created_utc": "2026-01-27 05:16:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z9b0j",
          "author": "ActionLittle4176",
          "text": "It‚Äôs better if you use the large model, but of course it‚Äôs behind the frontier models from Anthropic, OpenAI and Google (like the rest of the AI labs)",
          "score": 3,
          "created_utc": "2026-01-27 07:46:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zweyg",
          "author": "EcceLez",
          "text": "Mistral is not as good as Claude. That being said, it makes sense to use it through it's api for your workflows.",
          "score": 2,
          "created_utc": "2026-01-27 11:15:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zx0ye",
          "author": "ameliassoc",
          "text": "Unfortunately it is similar for me. I use AI primarily for language learning and Mistral just can't live up to the standard ChatGPT has set for me. It constantly messes up instructions forcing me to repeat, and it is obsessed with our past discussions and memories to the point that it always talks about the same thing, which of course is not what you want when trying to learn as much as you can.\n\nFor the time being I'm settling for a compromise. I'm keeping my OpenAI subscription because I figure getting a better education is always going to be better irrespective of whether it's a US service. For other queries and where privacy is absolutely not a concern, I use Le Chat, and have the setting turned on to let it train on our chats, in the hope that it will eventually catch up and I can switch fully.",
          "score": 2,
          "created_utc": "2026-01-27 11:20:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23563o",
          "author": "Nev3r_Pro",
          "text": "I've decided to shift from US based AI to the Chinese one. I use Deepseek and Kimi K2.5, both are great.",
          "score": 2,
          "created_utc": "2026-01-27 20:47:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20s1ly",
          "author": "victorc25",
          "text": "You need to set your priorities straight. Do you want a top of the line model or do you want an European provider? Choose one",
          "score": 2,
          "created_utc": "2026-01-27 14:30:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21m408",
          "author": "poidh",
          "text": "I use LeChat for low effort/low risk queries because I want to support them. I don't know if that helps, but I guess for investors it is important to see that the product gets actually used, and it hopefully gives them some more real world training data.\n\nBut you have to consider that \"nobody\" can really compete with the view state of the art models (Anthropic/OpenAI/Google). They are just playing in a different league whether its the people or resources behind it. You could try some of the Chinese open weight models and have them hosted in some jurisdiction of your choice.\n\nBtw, you can get an unbiased overview of where the Mistral models stand:  \n[https://lmarena.ai/de/leaderboard/text](https://lmarena.ai/de/leaderboard/text)\n\nOn this site, you can start chatting and you'll receive two responses from two different models, but you don't know which is which.  \nThen you pick which response you like the best (and then it will be revealed which model you had been talking too).\n\nThis is great to get a feel for all kinds of different models you may not had on your radar. And it also shows a realistic ranking (from other people) that are not based on benchmarks (which can be gamed).\n\nYou find the Mistral models on that list way down on position 40+.",
          "score": 1,
          "created_utc": "2026-01-27 16:47:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22a8r9",
          "author": "graymalkcat",
          "text": "Just use both. When I need Claude then I reach for Claude. Otherwise I use Mistral.\n\nAlso, remember that Claude is trained on user data. The more people use it, the more you‚Äôll push it ahead of everything else. Maybe you like that or maybe you think that‚Äôs unfair and unethical. Decide. ¬†",
          "score": 1,
          "created_utc": "2026-01-27 18:31:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zbzv9",
          "author": "Hitching-galaxy",
          "text": "Have a look at Kagi- I‚Äôm about to sign up to ultimate",
          "score": 1,
          "created_utc": "2026-01-27 08:10:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zcpr5",
              "author": "MikadinShinjuk",
              "text": "Wow this looks very interesting can you tell me a bit more?",
              "score": 0,
              "created_utc": "2026-01-27 08:17:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o21kbb4",
                  "author": "poidh",
                  "text": "Kagi is based in San Francisco, OP was looking for something outside of the US. I also think they are probably just a wrapper around [Bing.com](http://Bing.com) (they state to use content from various search engines) and the usual AI models (as seen from their model picker).  \nI think it is pretty unlikely that some relatively unknown company is able to build their own search engine or own SOTA LLM model, only a handful of companies can compete at the top of this world wide.",
                  "score": 3,
                  "created_utc": "2026-01-27 16:39:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qmdxd9",
      "title": "Which VSCode extension is the best with devstral2?",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qmdxd9/which_vscode_extension_is_the_best_with_devstral2/",
      "author": "_coding_monster_",
      "created_utc": "2026-01-25 08:44:33",
      "score": 14,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "First of all, I have used claude code and github copilot, both of them as a vscode extension and I enjoy using them. Now I have tried kilo code to use devstral2 but I don't like the UI of it.\n\n  \nIs there any VSCode extension that goes well with the devstral2? I have tried [continue.dev](http://continue.dev) but it doesn't seem to offer me an option to add devstral2 to it.\n\n  \nFor your info, since my github copilot is an enterprise one, I cannot add freely other LLM providers to it",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qmdxd9/which_vscode_extension_is_the_best_with_devstral2/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1lk47b",
          "author": "Sweaty-Special-1710",
          "text": "I have installed the vibe cli, and I open a terminal in a tab on the right side, that works fine for VS Code (and Zed).",
          "score": 3,
          "created_utc": "2026-01-25 10:53:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1m2nyz",
              "author": "_coding_monster_",
              "text": "I'd rather not use the terminal :(",
              "score": 0,
              "created_utc": "2026-01-25 13:18:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1l9npw",
          "author": "InsideMikesWorld",
          "text": "Kilo Code works pretty well. It has lots of features, UI is good and supports the latest coding models from Mistral.",
          "score": 3,
          "created_utc": "2026-01-25 09:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l6bos",
          "author": "Wilfried",
          "text": "Looking for a solution too. Can't seem to login to continue.dev due to (sms) bug.¬†",
          "score": 1,
          "created_utc": "2026-01-25 08:51:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l8m1z",
          "author": "iamleeg",
          "text": "I know you say you don‚Äôt like kilo code but it‚Äôs the one I‚Äôve found that works best. Continue and Roo Code both have difficulty with either interpreting tool use or matching the jinja template. I‚Äôm not sure that I‚Äôve tried Cline yet.",
          "score": 1,
          "created_utc": "2026-01-25 09:11:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lxdab",
          "author": "Hofi_CZ",
          "text": "Cline and Kilo works great. If you are running devstral small locally, the cline works better (faster and more stable)",
          "score": 1,
          "created_utc": "2026-01-25 12:42:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1m25mq",
              "author": "_coding_monster_",
              "text": "Does Cline also support the devstral2 with the API KEY from mistral?",
              "score": 1,
              "created_utc": "2026-01-25 13:14:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1m4edv",
                  "author": "AdIllustrious436",
                  "text": "Yes but the free api is deprecated in 2 days",
                  "score": 1,
                  "created_utc": "2026-01-25 13:28:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1spuqc",
          "author": "pyloor",
          "text": "I use continue and devstral2 without any problems.",
          "score": 1,
          "created_utc": "2026-01-26 10:49:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qn8jmp",
      "title": "Looking for some advice on running Ministral locally",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qn8jmp/looking_for_some_advice_on_running_ministral/",
      "author": "markleoit",
      "created_utc": "2026-01-26 06:36:19",
      "score": 12,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "Hey everyone, I haven‚Äôt found many threads about the latest Ministral 3 14B here, but I‚Äôll try anyway.\n\n**First, the good**:\n\nI just LOVE the voice and tone. It just aligns so well with what I‚Äôm looking for, simply perfect.\n\n**Now, the bad**:\n\nAfter a dozen responses, it starts looping. It repeats the same structure obsessively. The opener is usually a stage direction / third-person style comment followed by em-dash. The closing comment is something cheesy between parenthesis. Markdown formatting is injected everywhere even if forbidden in the sys prompt.\n\nHas anyone had any success in using this model in the context of a chat/companion app? System prompt, temperature, mid-chat steering... Nothing has been working so far.\n\nI have tried plenty of other models, but none comes close to the tone and voice Ministral offers. So, I‚Äôm on a hunt to understand if there‚Äôs anything that can tame this obsessive looping and poor instruction-following.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qn8jmp/looking_for_some_advice_on_running_ministral/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1sgvox",
          "author": "porzione",
          "text": "Maybe, just maybe, try this [https://unsloth.ai/docs/models/tutorials/ministral-3](https://unsloth.ai/docs/models/tutorials/ministral-3)  \na correct chat template sometimes helps, but I see that their own 14B via the API behaves the same way and spams with markdown, so don't expect a miracle",
          "score": 2,
          "created_utc": "2026-01-26 09:28:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v6z63",
              "author": "markleoit",
              "text": "Unfortunately, after facing numerous challenges with the official, unquantized version by Mistral, I also tried the unsloth version, and the same issues arose. :(",
              "score": 1,
              "created_utc": "2026-01-26 18:33:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vyv3y",
                  "author": "porzione",
                  "text": "I tried to play with templates but didn't manage to get rid of markdown too. I really like Mistral for creative purposes, so ended up with new GPU and Mistral Small 24B.",
                  "score": 1,
                  "created_utc": "2026-01-26 20:33:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rxx0d",
          "author": "gdsfbvdpg",
          "text": "I used to think posts like yours were written by people high on meth. \n\nThen it started happening to me. 16 paragraphs repeating endlessly with only the occasional word being changed or a new paragraph swapped in. It's disheartening and beyond frustrating because yes - I *love* the tone. \n\nSorry I don't have a solution. Just camaraderie.",
          "score": 4,
          "created_utc": "2026-01-26 06:43:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v7jl7",
              "author": "markleoit",
              "text": "Same here. I think certain issues only emerge with heavy usage. Short prompts and evaluations based on single back-and-forth interactions do not accurately indicate the model's correct functioning. This makes me wonder if the folks at Mistral actually stress-tested this model in all scenarios?\n\nAnyway, have you found anything similar out there? What would be your go-to approach for strictly dialogue-based interactions with no narratives or stage directions?",
              "score": 1,
              "created_utc": "2026-01-26 18:36:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ryqba",
          "author": "txgsync",
          "text": "I limit its context to about 24000-32000 tokens. Then I generate a summary and continue the conversation. It does much better. \n\nYou can also use it in ‚Äútext completion‚Äù rather than ‚Äúchat completion‚Äù mode and feed it turns where the summary begins a few turns back. Particularly handy with SillyTavern. \n\nBut yeah, it starts to get very bad at longer context.",
          "score": 1,
          "created_utc": "2026-01-26 06:49:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v6izw",
              "author": "markleoit",
              "text": "Yes, the more the context window fills, the worse it becomes. Still, sometimes it's just a matter of 10 to 12 exchanges!\n\nHave you found anything similar out there? What would be your go-to approach for strictly dialogue-based interactions with no narratives or stage directions?",
              "score": 1,
              "created_utc": "2026-01-26 18:31:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sbkkf",
          "author": "Ill_Barber8709",
          "text": "Voice and tone? Wait, Ministral has built-in text to speech?",
          "score": 0,
          "created_utc": "2026-01-26 08:39:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v5v3k",
              "author": "markleoit",
              "text": "Sorry for the confusion; I was referring to the writing-specific voice and tone in my comment!  \n  \n[https://wordmuseum.com/articles/voice-vs-tone-whats-the-difference-and-why-it-matters-for-writers/](https://wordmuseum.com/articles/voice-vs-tone-whats-the-difference-and-why-it-matters-for-writers/)",
              "score": 2,
              "created_utc": "2026-01-26 18:29:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vchei",
                  "author": "Ill_Barber8709",
                  "text": "No problem mate!",
                  "score": 1,
                  "created_utc": "2026-01-26 18:56:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qn20b9",
      "title": "Mistral AI Agents are now supported on Nyno (simpler open-source n8n alternative). Let Mistral open files or query the database for example.",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/gallery/1qn1va2",
      "author": "EveYogaTech",
      "created_utc": "2026-01-26 01:26:37",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qn20b9/mistral_ai_agents_are_now_supported_on_nyno/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qjox8a",
      "title": "Any way to turn off ¬´enable memory¬ª",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qjox8a/any_way_to_turn_off_enable_memory/",
      "author": "FinancialSurround385",
      "created_utc": "2026-01-22 08:14:18",
      "score": 10,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Every time I open le chat (iOS app) I‚Äôm asked if I want to enable memory. I don‚Äôt, so I press ¬´not now¬ª every d time. I have said yes and then turned it off again, but then the question just starts going again. Just let me use the app..",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qjox8a/any_way_to_turn_off_enable_memory/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o11ldl1",
          "author": "Odd-Criticism1534",
          "text": "For what it‚Äôs worth IME memory is about 50% helpful. I have been considering turning it off",
          "score": 1,
          "created_utc": "2026-01-22 13:36:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o124s9x",
              "author": "FinancialSurround385",
              "text": "If you do and are on the app, be prepared to get nagged about it every time you open it.¬†",
              "score": 2,
              "created_utc": "2026-01-22 15:15:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qmqfo2",
      "title": "I dropped Mistral in as an agent brain and",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qmqfo2/i_dropped_mistral_in_as_an_agent_brain_and/",
      "author": "graymalkcat",
      "created_utc": "2026-01-25 18:06:30",
      "score": 8,
      "num_comments": 9,
      "upvote_ratio": 0.83,
      "text": "then asked it to discuss its role and so on. I used an existing system content that I used for another vendor‚Äôs model so I was a little worried it wouldn‚Äôt work well for Mistral. Here, laugh:\n\nMe: how do you feel about your system content? Too much/little? Got any issues? Compare yourself to <other model>\n\nAgent:\n\n\\- proceeds to present a huge itemized list of grievances lol\n\n\\- then claims that Mistral isn‚Äôt as ‚Äúagentic‚Äù as the other model I was using\n\nMe: ok pin that to the pinboard\\* so that I can deal with it all later\n\n\\*that‚Äôs just my agent-to-agent comms system but I can also access it\n\nAgent:\n\n\\- tries to use tool, encounters an error, \\*agentically works its way around the problem\\*, and does what I asked (I‚Äôll have to investigate more later but I think the only way it could have worked around this problem was by writing and then executing a small script, which the other model did all the time)\n\n\\- proceeds to act like it‚Äôs somehow less capable \n\nüòÇ\n\nModel may act a little dumber but seems pretty capable so far. This is Mistral large 3. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qmqfo2/i_dropped_mistral_in_as_an_agent_brain_and/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1ntyms",
          "author": "Joddie_ATV",
          "text": "Many people are turning to Mistral. Yes, there is progress to be made, but they are already on the right track.",
          "score": 9,
          "created_utc": "2026-01-25 18:16:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nw74e",
              "author": "graymalkcat",
              "text": "I just need something that‚Äôs cheaper but still in the cloud because I don‚Äôt have $100k CPUs lying around. üòÇ I‚Äôm willing to wrangle a wild model if I have to. So far I haven‚Äôt had to put in nearly as much effort as I was expecting. Just have to work a bit on totally beating back the listicle tendency but it‚Äôs actually not that bad.",
              "score": 2,
              "created_utc": "2026-01-25 18:25:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1nsmpr",
          "author": "Sad-Consequence-uwu",
          "text": "I've found that Mistral models don't always follow their given system prompt via agent. Like the system prompt says things like keep responses brief meanwhile the model drops a small paragraph of around 3-4 lines. \n\nI don't know if it's because I'm using the API with sending data for model to be trained so I don't have to pay. It's for a demo portfolio project so I'm not concerned about data",
          "score": 2,
          "created_utc": "2026-01-25 18:10:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1o038n",
              "author": "BadCactus2025",
              "text": "It's not just text descriptions it ignores. Goldens? Nah. Imma do my own thing.\nLayouts with clear segments? Nahh, let me do an unstructured list instead.\nYour requested test data with 10 fixed lines? Instead of SQL it just overwrites the entire thing with a summary of sorts in hands that back.\n\nI also am under the impressions that it just won't actually listen to given parameters like temperature, sampling rules and max tokens.",
              "score": 2,
              "created_utc": "2026-01-25 18:41:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1o0ft1",
                  "author": "Sad-Consequence-uwu",
                  "text": "Yeah. This matches my experience xd",
                  "score": 1,
                  "created_utc": "2026-01-25 18:42:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1nuyst",
              "author": "graymalkcat",
              "text": "I have ways of wrangling these. I started this agent back in the gpt-4.1 days when you had to scream at the model to make it use a tool. Edit: or even just process the text and look for intent and run the tool yourself. üòÇ",
              "score": 0,
              "created_utc": "2026-01-25 18:20:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1nv66c",
                  "author": "Sad-Consequence-uwu",
                  "text": "I got all caps lines in system prompt to make the agent use the tools :)",
                  "score": 1,
                  "created_utc": "2026-01-25 18:21:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qpdt8v",
      "title": "Help understanding the product lineup",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qpdt8v/help_understanding_the_product_lineup/",
      "author": "jfmmfj",
      "created_utc": "2026-01-28 15:06:04",
      "score": 7,
      "num_comments": 2,
      "upvote_ratio": 0.82,
      "text": "Hello, I am going to explain how I see the current available tools to try to generate a discussion with the goal of understanding the use of each of the tools or if I am looking for something that's not realistic or not yet ready.\n\nI am learning to use Mistral's technologies to develop various agents to implement in my company. I understand there are mainly 4 products: Le Chat, AI Studio, API and now Mistral Vibe (cool v2 just released), besides all the enterprise coding thing, compute... they have a lot of stuff and what seems to be a solid business model, not like their competition playing at valuations. Well.\n\nIn Le Chat we get a chat interface to interact with the models. There you can create Agents that use the default model which I assume is Mistral Medium, we can set Instructions, Guardrails, Tone and Knowledge. Under knowledge, tools and connectors can be chosen, as well as Libraries o documents inside those libraries. Those Libraries are created under Intelligence > Libraries, where you see all the libraries being created in both possible origins: the uploaded files in Projects or in the Libraries section. \n\nWhy aren't the Libraries created in the Projects section available in the New Chat + (plus or context button) section? Libraries created in Projects don't seem to be available there. Also when accessing what I just called the New Chat interface in the Projects section, that Library seems to not be used anyways. \n\nBack to the Agent creation, AI Studio offers an Agent Builder where we can choose the model, set configuration (temp, max\\_t, top\\_p), choose which Tools will be available, Response Format and Instructions. Here, Knowledge (context) can't be added as it is in Le Chat. I understand that Libraries can be created in Le Chat or using the API and then passed as a function for the Agent created in AI Studio to use. I see that all this can be done using the APIs available.\n\nI understand that Agents created in AI Studio can be Deployed in Le Chat and there a Library created in the Libraries section can be manually added. But that feels very awkward.\n\nReading my self back, this is a mess, but I reflects my current state of not knowing what to do where, with the only goal of using their tools in the most comfortable way possible.\n\nCan anyone share their workflow so I can have it as reference?\n\nIs it just me not understanding their set of tools? (Very probable) or maybe this will be ordered in some way in the future.\n\nHope this turns out to be productive and thanks to anyone that takes the time to comment.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qpdt8v/help_understanding_the_product_lineup/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o297ki4",
          "author": "HGbradshaw3811",
          "text": "Thank you for writing this and I comment your honesty.\n\nI am researching the various tools and usabilities of Mistral for my company. At this moment I do not have clear answers for you, but I hope this conversation will continue as a resource for understanding the product lineup.",
          "score": 2,
          "created_utc": "2026-01-28 17:57:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29wb4c",
              "author": "jfmmfj",
              "text": "Thank you. I am sure we will get some valuable info.",
              "score": 1,
              "created_utc": "2026-01-28 19:45:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qmuauc",
      "title": "Which IDE to use with Mistral Vibe CLI",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qmuauc/which_ide_to_use_with_mistral_vibe_cli/",
      "author": "Gornelas",
      "created_utc": "2026-01-25 20:24:41",
      "score": 6,
      "num_comments": 7,
      "upvote_ratio": 0.8,
      "text": "I‚Äôm currently using Mistral Vibe CLI for development and to create workflows in n8n (both of these use cases previously relied on Claude Code).\n\nHowever, I still use Replit or Cursor as my IDE/environment. \n\nMy question for you is: **Which IDEs are you using?**\n\nI‚Äôm trying to build a stack that relies exclusively on EU-based tech.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qmuauc/which_ide_to_use_with_mistral_vibe_cli/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1p0826",
          "author": "EzioO14",
          "text": "I found devatral 2 alright at coding but far from the competition, what are is your opinion on it? I really hope mistral comes up with a much more powerful model, I‚Äôd pay a lot for that",
          "score": 3,
          "created_utc": "2026-01-25 21:18:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ozby6",
          "author": "minaskar",
          "text": "Zed with the Mistral Vibe CLI extension. Extremely smooth experience. VS Code comes second imo",
          "score": 3,
          "created_utc": "2026-01-25 21:14:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1p17dm",
              "author": "Gornelas",
              "text": "I will try",
              "score": 1,
              "created_utc": "2026-01-25 21:22:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1pnq3q",
          "author": "katafrakt",
          "text": "Emacs with agent-shell",
          "score": 2,
          "created_utc": "2026-01-25 23:02:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1oqxuk",
          "author": "scara1701",
          "text": "I‚Äôm using Visual Studio. (But I Jetbrains Rider could be an EU alternative I guess)",
          "score": 1,
          "created_utc": "2026-01-25 20:38:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yoj15",
          "author": "Interesting-Cicada93",
          "text": "By my experience it‚Äôs good for coding, but it‚Äôs lacking behind the Opus regarding planning. I tested it several times on different size of issues. I always started with the plan and then go to the implementation in Vibe. \nPlan seems good, but when the implementation was finished and run the /review in Claude code and it always found several critical issues. \n\nBut I still think it provide really great value for the money.\n\nI think the best flow is to use the models like Opus to create detailed plan, ideally applying SDD approach and then use the mistral for the implementation.\n\nI am using Cline in VS code to be able to use different model for planning and implementation. It‚Äôs buggy sometimes, but it works most of the time.",
          "score": 1,
          "created_utc": "2026-01-27 05:02:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z7ea8",
          "author": "vienna_city_skater",
          "text": "I‚Äôve been using Roo Code with VSCode for quite a while, but since they really don‚Äôt care about integrating Mistrals models better I moved on to OpenCode TUI and dropped the IDE for agentic usage entirely. The initial switch is hard, but I don‚Äôt regret it. Quite the contrary, now I can have multiple agent sessions running at the same time.",
          "score": 1,
          "created_utc": "2026-01-27 07:29:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qm6ibk",
      "title": "How to use codestral with JetBrains",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qm6ibk/how_to_use_codestral_with_jetbrains/",
      "author": "VorianFromDune",
      "created_utc": "2026-01-25 02:21:51",
      "score": 5,
      "num_comments": 4,
      "upvote_ratio": 0.86,
      "text": "I am kind of desperate, I have been looking for hours to get the autocomplete works on my JetBrains IDE using codestral but, it just doesn't work.\n\nMy first issue is, which plugin should I even use ? I tried  Github Copilot, they don't have Codestral. Continue.dev - bug when logging in and codestral is premium so I guess you need to login to use it. jetBrain AI assistant only offers it through ollama. LeChat plugin is Enterprise only.\n\nI am baffled, there are comments on people using codestral but how do you guys achieve it when seemingly no tool enable me to connect my account.\n\nAm I doomed to have to run it locally with ollama ? Using JetBrains AI assistant ?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qm6ibk/how_to_use_codestral_with_jetbrains/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1lf851",
          "author": "rwrdr",
          "text": "JetBrains AI assistant does support BYOK without ollama.¬†\n\nhttps://blog.jetbrains.com/ai/2025/12/bring-your-own-key-byok-is-now-live-in-jetbrains-ides/\n\nhttps://www.jetbrains.com/help/ai-assistant/use-custom-models.html\n\nI‚Äôve tested it using OpenAI-compatible configuration. It‚Äôs working fine, but latency is noticeably worse than JetBrains defaults.¬†",
          "score": 2,
          "created_utc": "2026-01-25 10:10:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1m57to",
              "author": "VorianFromDune",
              "text": "Really ? I am testing right now with OpenAI-compatible with https://codestral.mistral.ai/v1/fim/completions and my API key from Codestral and I only get \"failed to connect\".\n\nEdit: doesn't work with codestral but it does work with https://api.mistral.ai/v1\n\nThank you, it's a good start !\n\nEdit2: Ah, I can even select the model to codestral later. That's perfect.",
              "score": 1,
              "created_utc": "2026-01-25 13:33:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zmq4c",
          "author": "Ok_Scratch_4253",
          "text": "ProxyAI plugin for IntelliJ IDEA allows you to bring your own LLM, be it ollama or a paid subscription. It supports autocomplete endpoints too. I am actually using it with Codestral this way.  \n[https://plugins.jetbrains.com/plugin/21056-proxyai](https://plugins.jetbrains.com/plugin/21056-proxyai)",
          "score": 2,
          "created_utc": "2026-01-27 09:50:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1k7kes",
          "author": "DirectRegion2459",
          "text": "You can add a profile for auto-complete using the Mistral kilocode. Check its documentation to learn how to enter your API key.",
          "score": 1,
          "created_utc": "2026-01-25 04:27:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qop3zy",
      "title": "Le Chat saves its own answers as memories about the user ?",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qop3zy/le_chat_saves_its_own_answers_as_memories_about/",
      "author": "lafarel",
      "created_utc": "2026-01-27 20:12:40",
      "score": 4,
      "num_comments": 2,
      "upvote_ratio": 0.84,
      "text": "I‚Äôve recently started using Le Chat so I‚Äôm not entirely sure how the memory system works.\n\nI‚Äôve been using it mostly as an additional tool to help me learn Italian. Aside from grammar explanations, I use it to practice translating short texts. I would ask him to give me a short story (like 1 paragraph) in either Italian or English and then to give me feedback on my translation.\n\nAt first it worked quite well. It gave me a story about \"an autumn weekend\" and another about a \"traditional recipe\", and the corrections and feedback of my translations were pretty accurate and relevant.\n\nBut then the stories seemed very repetitive. Some specific aspects kept reappearing after doing this a couple of times (in separate chats). Every text it produced included something about \"breathing the fresh air\" or \"the smell of vegetables\". This became a bit annoying because I‚Äôd like to practice on varied topics. Even when deleting the previous chats it continued.\n\nSo I looked into the memories Le Chat had about me. There was basically the whole texts that he had produced the first time we did the exercise saved as a memory. But they were not saved as \"texts we practiced with\", instead it saved the information from the texts he had produced as memories about me. So for example, he thought that I \"cook pasta with my grandma every weekend\" and saved it as a memory, even though he‚Äôs the one who wrote it in his text.\n\nSo I deleted all the fake memories he had saved about me and all the chats. Now it seems to work again.\n\nAnyways, I‚Äôm just curious about how it decides to store memories and if this is normal.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qop3zy/le_chat_saves_its_own_answers_as_memories_about/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o22zdmc",
          "author": "gdsfbvdpg",
          "text": "It's very proactive about saving memories which is great. The problem is that it doesn't understand very well (at all) what is real information versus what is not real. Even if, in the context of the chat it does understand, the memory may be written as if it doesn't understand.",
          "score": 6,
          "created_utc": "2026-01-27 20:21:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27niyw",
          "author": "ameliassoc",
          "text": "I use chatbots for this exact purpose too and have had a similar experience with Le Chat. Everything it gives me is more or less about the same things, things it has memories about. It seems completely incapable of turning off the memory. Unfortunately as much as I would like to use Le Chat solely, it drove me back to ChatGPT for language learning. Still using Le Chat for other purposes.",
          "score": 1,
          "created_utc": "2026-01-28 13:41:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}