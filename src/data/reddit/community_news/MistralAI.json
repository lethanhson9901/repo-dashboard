{
  "metadata": {
    "last_updated": "2026-01-27 08:48:56",
    "time_filter": "week",
    "subreddit": "MistralAI",
    "total_items": 20,
    "total_comments": 68,
    "file_size_bytes": 86234
  },
  "items": [
    {
      "id": "1qmg70d",
      "title": "Move to Mistral",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qmg70d/move_to_mistral/",
      "author": "InternalBroad2522",
      "created_utc": "2026-01-25 10:57:42",
      "score": 124,
      "num_comments": 15,
      "upvote_ratio": 0.98,
      "text": "Currently I am using ChatGpt pro, Codex and GitHub Copilot, however I would like to switch to European provider or open source projects due to the critical situation with US. In your opinion, which are the best services I should use to do the switch I want?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qmg70d/move_to_mistral/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1mg6dg",
          "author": "thedisturbedflask",
          "text": "I'd suggest also using Mistral and other services to help refine a custom 'system prompt' for the Mistral's Le Chat to be more in line with what you need.\n\n\nI was initially a bit worried that i just wasn't getting the value out of chat compared to other services but creating my own instructions helped a lot with development especially but also day to day usage.\n\n\nDevstral2 vibe is also good but if your used to having it in an ide then the cline.bot vscode extension seems to work well",
          "score": 14,
          "created_utc": "2026-01-25 14:35:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sj0le",
              "author": "GreenStorm_01",
              "text": "Any hints on proper system prompting? It probably isn't helpful to just copy my customisation from chatgpt over, right?",
              "score": 1,
              "created_utc": "2026-01-26 09:48:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1vvh9g",
                  "author": "thedisturbedflask",
                  "text": "It's a good starting point.\n\n\nIf you're happy with chatgpt's responses you can ask it to help define a starting system prompt with the characteristics you need and set it as the instructions or agent prompt in Mistral.\n\n\nThen in Mistral you can ask a question that fits your use case repeatedly and you can then tweak the system prompt as you go.",
                  "score": 1,
                  "created_utc": "2026-01-26 20:18:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1llo8g",
          "author": "whoisyurii",
          "text": "mistral, mistral vibe or codestral + ollama\n\n**edit: devstral",
          "score": 8,
          "created_utc": "2026-01-25 11:07:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1lpvjl",
              "author": "cosimoiaia",
              "text": "Yes, except not ollama (too shady, buggy, bad software), better lmstudio or Jan. \n\nAlso the latest is called Devstral üôÇ",
              "score": 9,
              "created_utc": "2026-01-25 11:43:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1mwf31",
                  "author": "guyfromwhitechicks",
                  "text": "What's buggy about ollama?",
                  "score": 0,
                  "created_utc": "2026-01-25 15:53:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1sj6uz",
                  "author": "razziath",
                  "text": "Compared to ollama, lmstudio is very slow.  \nOllama is good. Ollama with Anythingllm is a very good option is you want a UI and add connectors/mcp/agents... to your llm.",
                  "score": 0,
                  "created_utc": "2026-01-26 09:49:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1mxh75",
          "author": "guyfromwhitechicks",
          "text": "This comment section being the equivalent of tumbleweeds really shows how big this problem is. You can look into /r/BuyFromEU and https://european-alternatives.eu/, they are ran by people who keep trying to solve \"what do I replace my american products with?\". The options are slim (especially for software) but it is getting better.",
          "score": 1,
          "created_utc": "2026-01-25 15:57:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1oahbu",
          "author": "UpstairsCheetah235",
          "text": "You could check out Proton‚Äôs lumo. It uses a variety of models and there‚Äôs a free tier to try out. Might be a good solution, especially for those switching email and cloud storage over to them.¬†",
          "score": 1,
          "created_utc": "2026-01-25 19:25:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wps7q",
              "author": "cosimoiaia",
              "text": "Proton has indeed excellent offerings, however, they don't have their own model but rather run other open weights models. I haven't checked in a minute but afaik they don't disclose which one and that for me is a security issue. But I'm sure it will become rock solid given where it comes from.",
              "score": 1,
              "created_utc": "2026-01-26 22:34:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1v27f8",
          "author": "empireofadhd",
          "text": "I also switched, also switching to mailo, but have not figured out how to use their office suite.",
          "score": 1,
          "created_utc": "2026-01-26 18:13:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uzuve",
          "author": "GreenGreasyGreasels",
          "text": "if it is open source you could use the best Chinese open weight model hosted by yourself or by a trusted vendor in EU. the usual suspects DS, K2, GLM-4.7, M2.1 etc. Devstral 2 and Mistral Large 3 remain top notch choices. If you consider Russia European you can look at Gigachat :P",
          "score": 0,
          "created_utc": "2026-01-26 18:03:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ymr69",
              "author": "dsvost",
              "text": "I would add also add then SourceCraft stuff in that case..",
              "score": 1,
              "created_utc": "2026-01-27 04:50:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qkv00t",
      "title": "Quick note",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qkv00t/quick_note/",
      "author": "Clement_at_Mistral",
      "created_utc": "2026-01-23 16:12:43",
      "score": 53,
      "num_comments": 40,
      "upvote_ratio": 0.98,
      "text": "Devstral 2 will move to paid API access starting January 27. You‚Äôll still get free usage under the¬†[Mistral Studio](https://console.mistral.ai/home)¬†Experiment plan.\n\nPS: something's coming next week!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qkv00t/quick_note/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1dyfe6",
          "author": "vienna_city_skater",
          "text": "Please keep it free for Le Chat Pro users or give us a subscription option for all your models that we can use via API. Honestly, it‚Äôs hard to justify paying 20‚Ç¨ per month for just chat usage.",
          "score": 12,
          "created_utc": "2026-01-24 07:04:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19fheh",
          "author": "SourceCodeplz",
          "text": "Maybe keep Devstral 2 Small free in Vibe at least?",
          "score": 5,
          "created_utc": "2026-01-23 16:20:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a6yyk",
              "author": "Bob5k",
              "text": "as long as you'll be on experiment plan you'll have devstral 2 free in vibe.",
              "score": 5,
              "created_utc": "2026-01-23 18:25:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o19k8i8",
          "author": "Egoz3ntrum",
          "text": "Thinking version?",
          "score": 3,
          "created_utc": "2026-01-23 16:41:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dyqku",
              "author": "vienna_city_skater",
              "text": "A Devstral model with inference time reasoning would amazing indeed.",
              "score": 3,
              "created_utc": "2026-01-24 07:06:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1gczsi",
              "author": "txgsync",
              "text": "I‚Äôve been using the ‚Äúsequential thinking‚Äù MCP.  It slows things down but improves accuracy, particularly in uncertain situations.",
              "score": 1,
              "created_utc": "2026-01-24 16:58:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1a0gpm",
          "author": "cosimoiaia",
          "text": "I suppose that will include vibe usage as well? üò¢",
          "score": 2,
          "created_utc": "2026-01-23 17:56:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19hpvl",
          "author": "Hopeful-Kale-5143",
          "text": "Excited to see what's coming! There is not much of a bump needed for codestral in order to make it really viable!",
          "score": 1,
          "created_utc": "2026-01-23 16:30:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19hzt1",
              "author": "EzioO14",
              "text": "Vibable you mean? üòÇ",
              "score": 7,
              "created_utc": "2026-01-23 16:31:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1bufvv",
          "author": "Mystical_Whoosing",
          "text": "do we know the 1m token prices?",
          "score": 1,
          "created_utc": "2026-01-23 23:06:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yztl1",
              "author": "nycigo",
              "text": "Really cheap, less than Deepseek v3.2 on OpenRouter, ‚Ç¨0.20 exit fee I think and ‚Ç¨0.05 entry fee per million tokens.",
              "score": 1,
              "created_utc": "2026-01-27 06:26:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ldof4",
          "author": "DueKaleidoscope1884",
          "text": "Of course I do not know the long term plans of Mistral but I do want them to succeed being the only European (seemingly) viable alternative to US and Chinese models.\n\nThe truth currently is, the models are not at the same level as the Codex or Anthropic. I have tried Devstral from Opencode but at some point I needed to get work done. I think it can handle a lot of the implementation but the competition is just easier to work with it seems.\n\nAt the same time I am happy to see Mistral Vibe BUT the documentation is either very limited or very hard to find. I came to the conclusion it is incomplete. I may be wrong. For example, I know Vibe supports skills because I saw the release notes but try finding the documentation on it.\n\nIn general I  do not think it reasonable to expect people to beta test a product (Vibe) that is lagging behind on the competition so much.\n\nWhat would keep me trying Vibe and Devstral is being able to use it free, daily or weekly limits are reasonable to impose, until it has caught up more to the competition.\n\nSo please consider making the Devstral models (limited) free on Mistral Vibe.  \n(I do not know what is technically possible given the trouble Claude Code is having limiting the subscription model to Claude Code only but a clear TOS from the help may help.)\n\nThis way I could keep on using (and testing) both.",
          "score": 1,
          "created_utc": "2026-01-25 09:56:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19ipf1",
          "author": "EzioO14",
          "text": "Thanks for the heads up. I hope they don‚Äôt make the api key paid because it‚Äôs super useful to test features on my projects",
          "score": 0,
          "created_utc": "2026-01-23 16:35:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19qtsl",
          "author": "Impossible_Comment49",
          "text": "Oh no! But at the same time, is anyone actually using it? I achieve significantly better results with OpenCode‚Äôs free models, such as Big Pickle or others. I was delighted that Mistral was free to test out occasionally, but I would never use it if it wasn‚Äôt free.\n\nOn the other hand, I‚Äôm disappointed. I was hoping for Mistral‚Äôs adoption and the widespread use of ‚Äòvibe‚Äô. This will likely not be beneficial for Mistral. ‚Äòqwen‚Äô remains free.",
          "score": -5,
          "created_utc": "2026-01-23 17:11:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a189i",
              "author": "cosimoiaia",
              "text": "That says that you never used Devstral at all.",
              "score": 3,
              "created_utc": "2026-01-23 17:59:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1a49an",
                  "author": "Cyberblob42",
                  "text": "Iam using devstral-2 via CLI. Its usable m, but Claude is better Unfortunately",
                  "score": 2,
                  "created_utc": "2026-01-23 18:13:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1aqdtq",
                  "author": "Impossible_Comment49",
                  "text": "I have, but it‚Äôs nowhere near Codex, Opus, or even GLM4.7.",
                  "score": 1,
                  "created_utc": "2026-01-23 19:54:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1elqnb",
              "author": "Ok-Elderberry-2923",
              "text": "Tried Vibe CLI with Devstral 2. It took 3 hours for it to rename a field in a sql entity + usage (20-30 files affected) in a small to medium sized KMP project. This includes function names, local variable names etc. It also stopped like 10 times and tried persuading me that I shouldn't continue as it's a lot of work :D I mean it's free but I could have done this by hand in 15min.\n\nOther tasks it performed way below sonnet. Maybe at the level of gemini or gbt (not sure, i dont use them much)\n\nAlso, Claude CLI + Sonnet took about 2min to do the same task in the same codebase",
              "score": 2,
              "created_utc": "2026-01-24 10:36:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ntn7g",
                  "author": "graymalkcat",
                  "text": "I‚Äôve had Opus spend 15 minutes just trying to make a change to a single line. Sometimes you just have to do it yourself and let the AI move on.",
                  "score": 2,
                  "created_utc": "2026-01-25 18:15:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1nt65z",
              "author": "graymalkcat",
              "text": "I came over to it in December, not even knowing it was free. I‚Äôve been using it as a sub agent for Opus. They work nicely together. I also use it to process and extract insights from code-heavy text (basically to evaluate agent work).",
              "score": 1,
              "created_utc": "2026-01-25 18:13:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1dyzj3",
              "author": "vienna_city_skater",
              "text": "I wouldn‚Äòt use Chinese models in a business environment, also not if they are hosted on US servers.",
              "score": 1,
              "created_utc": "2026-01-24 07:08:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ajd8h",
          "author": "Dutchbags",
          "text": "id happily pay if it werent so dogshit slow",
          "score": -8,
          "created_utc": "2026-01-23 19:21:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ntwqr",
              "author": "graymalkcat",
              "text": "Not sure why you‚Äôre downvoted when this model is in fact incredibly slow. That‚Äôs my biggest complaint about it.",
              "score": 1,
              "created_utc": "2026-01-25 18:15:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1c4ucc",
          "author": "AdElectronic7628",
          "text": "I can't believe peoples daring comparing Claude to Mistral",
          "score": -1,
          "created_utc": "2026-01-24 00:02:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qndv3e",
      "title": "Ministral models are good.",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qndv3e/ministral_models_are_good/",
      "author": "Rent_South",
      "created_utc": "2026-01-26 11:45:46",
      "score": 46,
      "num_comments": 7,
      "upvote_ratio": 0.98,
      "text": "Just to say that in their weight class, ministral models (mainly 3b and 8b) are very cost efficient and quick, compared to other models.\n\nFor non complex tasks, they actually compete for the top spot.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qndv3e/ministral_models_are_good/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1t0lz8",
          "author": "scara1701",
          "text": "I like ministral:3b as well. Currently using it to test MCP tools I‚Äôm building :)",
          "score": 5,
          "created_utc": "2026-01-26 12:15:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1to112",
          "author": "stddealer",
          "text": "Yep, they've finally replaced Gemma3 models for me, though I think Gemma was a bit better at some things like translation or OCR, Ministral feels like a nice upgrade.",
          "score": 1,
          "created_utc": "2026-01-26 14:31:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1u0apu",
          "author": "Holiday_Purpose_3166",
          "text": "Mistral models are indeed good. I use them daily, especially Devstral Small 2 for my workflows where GPT-OSS-120B struggles to execute. What a time to be alive.",
          "score": 1,
          "created_utc": "2026-01-26 15:30:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1v9vhq",
          "author": "Conscious-Expert-455",
          "text": "How to use these models? For vibe coding? As agents?\nI'd like to use them as agents or as MCP services.",
          "score": 1,
          "created_utc": "2026-01-26 18:45:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xac54",
              "author": "Rent_South",
              "text": "It all depends on your use case. Depending on your specific tasks, any of your suggestions are viable.  \nOne thing is certain is that if your use case fits these models, they perform really well.",
              "score": 1,
              "created_utc": "2026-01-27 00:18:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1szmp5",
          "author": "Scared_Range_7736",
          "text": "Still far behind American and Chinese models, unfortunately. Check this benchmark from a few days ago: [https://www.vals.ai/benchmarks/terminal-bench-2](https://www.vals.ai/benchmarks/terminal-bench-2)",
          "score": -6,
          "created_utc": "2026-01-26 12:08:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1t5me6",
              "author": "krkrkrneki",
              "text": "OP is referring to open models available to be run locally.",
              "score": 8,
              "created_utc": "2026-01-26 12:50:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qnnieh",
      "title": "Devstrale 2 > other Chinese AIs like DeepSeek etc",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qnnieh/devstrale_2_other_chinese_ais_like_deepseek_etc/",
      "author": "nycigo",
      "created_utc": "2026-01-26 18:00:38",
      "score": 42,
      "num_comments": 18,
      "upvote_ratio": 0.98,
      "text": "Why is nobody talking about Devstrale 2 in the same way as GLM 4.7 Deepseek and Minimax when the AI ‚Äã‚Äãis in the top 6 on OpenRouter in the best programming AI category, ahead of all the other Chinese models and with a damn free API?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qnnieh/devstrale_2_other_chinese_ais_like_deepseek_etc/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1v1l3g",
          "author": "cosimoiaia",
          "text": "Hate.\n\nDevstral is a superb model, even the small-24b running locally is better than all the other open weights.\n\nBut if they start to admit that the privacy/consumer first policies actually don't block progress completely and that the EU can, and did, produce SOTA models for their size, their delusions will break and they'll have a panic attack.\n\nAt the WEF they openly admitted that they wanted only the US to be players in the AI field and that they should do anything to block progress for everyone else.",
          "score": 20,
          "created_utc": "2026-01-26 18:11:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1v1ilu",
          "author": "SourceCodeplz",
          "text": "Because the Chinese have an online army on reddit and they promote it heavily.  \nBut GLM, Deepseek and Minimax are really good actually, not like from Anthropic, but fine.",
          "score": 18,
          "created_utc": "2026-01-26 18:10:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vxfe9",
          "author": "tisDDM",
          "text": "I did a few tests with devstral 2 (small) and both are performing very good in agentic coding. I did a post here [https://www.reddit.com/r/opencodeCLI/comments/1qlqj0q/benchmarking\\_with\\_opencode\\_opuscodexgemini\\_flash/](https://www.reddit.com/r/opencodeCLI/comments/1qlqj0q/benchmarking_with_opencode_opuscodexgemini_flash/) about benchmarking and said a few things about Devstral as well (but did not include the Devstral 2 results) because it is part of my subagent harness project (not published yet) - where I tried to use Devstral 2 as intelligent worker nodes. \n\nI found my results impressing. Both Devstral 2 Models were fully able to run the test suite. Deepseek 3.2 and Kimi K2  and Grok Fast showed a lot of issues with following agentic tasks.\n\nBut in case you ask me why I am not using devstral 2 for coding? It is far behind Opus and Codex. Not in quality of code. Behind in understanding and following a humans complex task. which both of the big two easily can manage. But this might be an issue of Reasonig.",
          "score": 3,
          "created_utc": "2026-01-26 20:27:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xm32o",
              "author": "Historical_Roll_2974",
              "text": "How did you get Devstral 2 Small to work on OpenCode? When I try to use it with OpenCode I get an error where the filepath is null and the message is null with the tools API? Thanks",
              "score": 1,
              "created_utc": "2026-01-27 01:19:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1z3bx1",
                  "author": "tisDDM",
                  "text": "I got Devstral 2 Small working as a subagent for my benchmark, that was very cool - but still a little bit flaky depending on prompting. I used Devstral 2 ( the big one) in Zed but I think it shall work in Opencode as well. I will give it a try again later.",
                  "score": 1,
                  "created_utc": "2026-01-27 06:54:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1v1sd5",
          "author": "Front_Eagle739",
          "text": "Well, its slow to run locally and while decent at coding its less flexible, mostly from the lack of ability to turn on high thinking. It's a nice model and it's got it's niche but glm 4.7 is usually better and more flexible even quantised to a similar size as devstral. It is a useful thing and I'm trying to use it more if only to support a european company but I think it kind of misses the benefit of being the only big dense model that will fit on a local 128GB machine (i.e. being smarter than anything comparable in size) due to said lack of thinking. For a really really smart model I could run locally I would be willing to wait but because it doesn't think it is not actually smarter than a q2 quantised glm 4.7 that also fits on my machine and it's slower. \n\nThats my take at the moment, I'm trying to explore it more, see if I can get more out of it.",
          "score": 6,
          "created_utc": "2026-01-26 18:12:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xjluk",
              "author": "Holiday_Purpose_3166",
              "text": "https://preview.redd.it/tqly3ygqksfg1.jpeg?width=4096&format=pjpg&auto=webp&s=8f21f93e021bfb6b05fab7706f02b541c4bf56af\n\nHaving spent billions of tokens last year in coding between different models under the sun, Devstral models are indeed excellent for what they are.\n\nl take the benchmarks as a guide and test them in my prod, and I find ironic how \"behind\" they visually look on the charts. Anecdotally, Devstral Small 2 repeatedly beats GPT-OSS-120B on my codebases despite being so far apart on benchmarks.\n\nGLM 4.7 Flash on the other hand is also excellent and kicks the 120B on the same stuff I'm working (ML, financial algos, Rust, NextJS, etc), but Devstral Small 2 has more eye to detail where GLM left some issues I had \"as is\".\n\nHowever, GLM 4.7 Flash is broadly smarter and more update to date on greater schemes compared to Devstral but has less knack on repo work. Like them both, but I lean towards Devstral over the fact it has more enterprise grade efficiency. Could be bias, but I like minimalistic response, and it can be extensively detailed when require (docs, blueprints >1k lines).\n\nAnother important note that resonates with my experience, the GPT-OSS models have large response variance where Devstral is more deterministic and is consistent on every round, and this reflected on SWE-Rebench charts from Ibragim.\n\nI find this variance inconvenient for coding despite tighter sampling as repeated, personal tests yielded different results.\n\nI haven't used GLM 4.7 Flash long enough to detect that but generally the apple doesn't fall far from tree if it follows the grand 4.7.",
              "score": 5,
              "created_utc": "2026-01-27 01:05:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1v95xy",
              "author": "SourceCodeplz",
              "text": "Yeah. Thinking adds a LOT o value, even though sometimes it can go in circles.",
              "score": 3,
              "created_utc": "2026-01-26 18:42:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1vafxw",
                  "author": "Front_Eagle739",
                  "text": "Yeah it really does. I get the value of a fast nonthinking model for code completion, i get the value of a slow reasoning model thats very coherent and intelligent for its size. I struggle to get the purpose of a slow non thinking model that maxes out my memory. the 24B devstral I do understand, I have more memory than would force me to go down to that size but if I was on 24GB/32GB That one I could see myself using. The 123B really wants reasoning to be the best possible answer I can get for 128GB memory option.",
                  "score": 1,
                  "created_utc": "2026-01-26 18:48:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1wlad8",
              "author": "cosimoiaia",
              "text": "Have you tried it with vibe? I've been using it quite a lot, for fairly complex tasks and, so far, it never failed, even with navigating in decent sized projects and it's FAST. Btw you need a lot less than 128GB to run it at full context, 62GB will leave you room to spare.\n\nAlso, I am very doubtful that a Q2 model of any size can do well coding tasks since they're very very quantization sensitive, doesn't matter the amount of thinking.\n\nEdit: this seems like an example of classic casting shadows disguised as comment. I hate worthless propaganda.",
              "score": 1,
              "created_utc": "2026-01-26 22:13:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1wmuiu",
                  "author": "Front_Eagle739",
                  "text": "Unsloth dynamic quants are a wonderful thing. I will admit the mlx 2 bit quants are fairly useless but the unsloth iq2m glm 4.7? That one works great. The minimax ones are worse though, not quite sure why but different models seem to take better or worse to dynamic quants and glm is the best ive seen. Anyway.\n\n\nNo i havent tried vibe with it. Tried it in claude code but I'll give vibe a go. Makes sense it would be optimised for its own harness.\n\n\nWont be fast though at least not the big one unless i go api, i get about 5 token/second with devstral 123 vs 60 with gpt oss 120 or 20 ish with glm. As i said, worth it if its good enough but it has to be consistent enough to not want to retry things a lot. Will give it a go in vibe.\n\n\nEdit. Thanks for declaring my opinions are worthless propoganda. Appreciate it. Really makes a man feel like he's in a good useful discussion. Honestly.",
                  "score": 1,
                  "created_utc": "2026-01-26 22:20:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ylw16",
          "author": "Waste-Intention-2806",
          "text": "Because it's a dense model. While minimax and reaped glm 4.7 r MOE models and most of us can run these at least 4-11 tokens per second. While devstral 2 was running at .7 to 1 token per second for me on i9 and 16gb rtx 4070 ti with 128 gb ram",
          "score": 1,
          "created_utc": "2026-01-27 04:44:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vieoc",
          "author": "pinmux",
          "text": "I think Mistral offering their API for devstral-2 and devstral-small-2 for free is actually hurting adoption by inference providers and hence users don't know about it.\n\nIn my brief experience trying devstral-small-2, it's quite good.  I don't have beefy enough hardware locally to run it at a reasonable speed and last I checked the only cloud inference providers offering the devstral-2 models will train on your data (Mistral included for their consumer offerings).  On OpenRouter you get Mistral or Chutes, that's it.\n\nI'm hoping some cloud inference providers will pick up devstral-2 (and devstral-small-2) after tomorrow once Mistral starts charging for the API access.  That'll make it easier for people to find and use it.",
          "score": 1,
          "created_utc": "2026-01-26 19:21:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1v7wxo",
          "author": "Marciplan",
          "text": "It really isn't on par with GLM 4.7 though.",
          "score": 0,
          "created_utc": "2026-01-26 18:37:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vgpmv",
              "author": "kiwibonga",
              "text": "For agentic coding though, Devstral Small 2 scores 10 pts higher than GLM-4.7-Flash on SWEBench. The GLM team completely omitted it from their benchmarks, maybe because it's not MoE.",
              "score": 7,
              "created_utc": "2026-01-26 19:14:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vt5xc",
          "author": "Large-Example-1275",
          "text": "It runs slowly on my DGX Spark compared to MoE alternatives.",
          "score": 0,
          "created_utc": "2026-01-26 20:08:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qi9zij",
      "title": "Mistral Creative",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qi9zij/mistral_creative/",
      "author": "NullSmoke",
      "created_utc": "2026-01-20 18:57:51",
      "score": 38,
      "num_comments": 11,
      "upvote_ratio": 0.95,
      "text": "Today I had some hours to sit down with Creative and really give it a proper spin. My first go a few weeks ago left me feeling \"It's better at creative stuffs than the normal model, but not enough to justify the hassle\"\n\nThat was a quick 3 prompt test... Today I had a few hours with nothing better to do, so take 2. A series of prompts I've used with ChatGPT and Claude in the past...\n\nI am blown away, the outputs I got was miles better than what I've been getting from either of the two other options, also ran over to Grok to test there, and that also did markedly worse.\n\nI may be in love.\n\nFirst thing I did after the session was looking into it I could run it locally... Nope, not open sourced (yet?) from what I can find.\n\nIs that correct? If so, the only way to run it through my local systems is to use API? Really want TTS on it, so need to get it routed through something else, in my case OpenWebUI.\n\nIs there any timeline for open sourcing that model? (Or TTS in LeChat, I can live with that as well üòÜ)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qi9zij/mistral_creative/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o0q6cfz",
          "author": "cosimoiaia",
          "text": "If I understood correctly, the model is a finetune they are still playing with, based on feedbacks. I believe it's more an experiment than something that they will just release straight away.",
          "score": 5,
          "created_utc": "2026-01-20 20:03:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tu6hf",
              "author": "NullSmoke",
              "text": "Boy, I do hope this gets released in the future, already got a spot in my lineup with its name on it ;P",
              "score": 1,
              "created_utc": "2026-01-21 09:44:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0sv81n",
          "author": "oravecz",
          "text": "When you say ‚Äúcreative‚Äù, what tasks are you referring to?",
          "score": 3,
          "created_utc": "2026-01-21 04:44:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ttpz1",
              "author": "NullSmoke",
              "text": "Creative... is the model. so that is what I referred to.\n\nAs for usecase I tested for. Grammar, rewriting (Make more concise, expand on etc), do prototype based on seed idea etc...\n\nAlso poked at foundational worldbuilding, but that's a really hard one to actually judge, because that takes a LOT of turns on any model to get anywhere. Also, foundational worldbuilding requires me to have some seed I want to nurish, and I don't currently. I already have like 8 original worlds in the latter stages of worldbuilding and that's pretty much all my idea seeds blossoming, giving me very little new to nurish.",
              "score": 2,
              "created_utc": "2026-01-21 09:40:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qklsh",
          "author": "schacks",
          "text": "Just tried it out in the playground and I agree, it‚Äôs pretty amazing.",
          "score": 2,
          "created_utc": "2026-01-20 21:08:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qfq9f",
          "author": "Nefhis",
          "text": "It's a great model, isn't it? üòä Right now it's there to be tested and receive user feedback (I'm collecting it myself for the Mistral team, both on Reddit and elsewhere), which is why it's in the labs. I suppose if they're looking for feedback it's because they have bigger plans for it.\n\nu/Nefhis  \n*Mistral AI Ambassador*",
          "score": 2,
          "created_utc": "2026-01-20 20:46:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0sxwmr",
              "author": "Responsible-Duck4991",
              "text": "I absolutely love Le Chat ‚Äî no words just a giant recommendation to try it out for yourself!",
              "score": 2,
              "created_utc": "2026-01-21 05:02:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0tujxm",
              "author": "NullSmoke",
              "text": "Say you're being held on gunpoint with demands to release it xD\n\nIn all seriousness, massively impressed. I assume it'll buckle under my worldbuilding if I try to feed that to it (I am one of those 'give backstory to every rock' type worldbuilders I believe you talked about in another post a while back, only ChatGPT has managed to somewhat untangle my lorebooks, and even that was unstable), but I can absoltely see it being a key part of my single story polishing stages.\n\nAbsolutely looking forward to it hopefully being released in the future :-)\n\nAlternatively TTS in LeChat so that I don't have to mess with local setup to use it :-P",
              "score": 1,
              "created_utc": "2026-01-21 09:47:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0s4kaw",
          "author": "svachalek",
          "text": "I haven‚Äôt used it much yet but I like what I see. First impression is it seems somewhere between a standard model and a reasoning model ‚Äî it‚Äôs kind of wordy about doing its thing, but it‚Äôs not the slop that typically shows up in thinking blocks. Seems very smart and capable for a model its size, and lightning fast on Mistral‚Äôs server. \n\nAnd yeah‚Äî it‚Äôs just got a unique voice, it doesn‚Äôt come out sounding like every other LLM.",
          "score": 1,
          "created_utc": "2026-01-21 02:02:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tu413",
              "author": "NullSmoke",
              "text": "I have VERY POOR experience with thinking models, they tend to be hard to deal with and very prone to moralizing, especially those over at OpenAI, but they don't hold copyright on that one.\n\nBut it's much better at creative tasks than any LLM I've used while messing with my stories and worlds.\n\nThe first go, it didn't seem all that impressive, but now that I gave it a bit of time and ran prompts where I know the output from other models, the contrast became very clear. \n\nIt absolutely has a unique voice, and I'll probably try to use it the next time I need assistance from an LLM for any of my writings, especially now that ChatGPT is utterly useless for any form of creative writing.",
              "score": 2,
              "created_utc": "2026-01-21 09:43:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qj55qg",
      "title": "Engineering Deep Dive: Heaps do lie",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qj55qg/engineering_deep_dive_heaps_do_lie/",
      "author": "jofthomas",
      "created_utc": "2026-01-21 18:01:21",
      "score": 32,
      "num_comments": 1,
      "upvote_ratio": 0.94,
      "text": "**Ever chased a memory leak that seemed to vanish when you looked for it?**\n\nOur investigation took us from Python profilers to kernel-level tracing with **BPFtrace** and **GDB**, moving through layers of dependencies. We traced the leak deep in the stack, discovering **UCX‚Äôs memory hooks** were the source. The solution? **A single environment variable.**\n\n**Debugging a Memory Leak in vLLM**\n\nA few months ago, one of our teams investigated a suspected memory leak in **vLLM**. At first, the issue was believed to be easy to spot‚Äîsomething confined to the upper layers of the codebase. But as the team dug deeper, the problem became more complex.\n\nThis article kicks off our new **Engineering Deep Dive** series, where we‚Äôll share how we tackle technical investigations and build solutions at **Mistral AI**.\n\n[**Read the full story here**](https://mistral.ai/news/debugging-memory-leak-in-vllm)**.**\n\n\n\nThis is our first technical blog post‚Äîif you enjoyed it, please **share it** and let us know what topics you‚Äôd like us to explore next!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qj55qg/engineering_deep_dive_heaps_do_lie/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1iqgvq",
          "author": "Timo425",
          "text": "Love this.",
          "score": 1,
          "created_utc": "2026-01-24 23:32:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj9n9i",
      "title": "I developed an open-source tool that allows Mistral to \"discuss\" other models to eliminate hallucinations.",
      "subreddit": "MistralAI",
      "url": "https://i.redd.it/3mk7ujhvkreg1.jpeg",
      "author": "S_Anv",
      "created_utc": "2026-01-21 20:43:24",
      "score": 28,
      "num_comments": 2,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qj9n9i/i_developed_an_opensource_tool_that_allows/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1npc9k",
          "author": "Little_Protection434",
          "text": "Anybody tried this? What were your results?",
          "score": 1,
          "created_utc": "2026-01-25 17:57:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhwgci",
      "title": "Is there a way to use MistralAI as I'm using Codex / Claude Code?",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qhwgci/is_there_a_way_to_use_mistralai_as_im_using_codex/",
      "author": "WaterlooPitt",
      "created_utc": "2026-01-20 09:20:35",
      "score": 24,
      "num_comments": 7,
      "upvote_ratio": 0.9,
      "text": "Hi all, \n\n  \nI'm getting ready to start a new and large web development project. I am currently using Codex from OpenAI and sometimes, I'd switch to Claude Code. I am doing all my coding straight into MS Code, using the terminal and the agents mentioned. \n\nI'd very much like to use a European service, instead of sending money to Fascistan but I am a bit confused about MistralAI. I see it offers a free tier and then it costs X amount per million tokens, through the API. \n\nIn the past I've had some issues with this, as I don't know when to stop and overspent. So things like Codex or Claude Code that only work for X amount of time are perfect for me. Is there a similar thing that Mistral offers? \n\nMany thanks. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qhwgci/is_there_a_way_to_use_mistralai_as_im_using_codex/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o0nf2u2",
          "author": "Ruttin",
          "text": "Take a look at Mistral Vibe https://mistral.ai/news/devstral-2-vibe-cli",
          "score": 12,
          "created_utc": "2026-01-20 11:35:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qvx2w",
              "author": "Odd-Criticism1534",
              "text": "And Zed using vibe extension",
              "score": 2,
              "created_utc": "2026-01-20 22:00:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0n5byr",
          "author": "MikeNonect",
          "text": "OpenCode ([https://opencode.ai/](https://opencode.ai/)) + Devstral 2 works reasonably out of the box. It's not Claude Code or Codex with 5.2, but it's not a toy either.\n\nThe free tier is always free, so no risk of overspent. OpenCode also shows a real-time tracker of the token cost, so that should keep it manageable unless you're running the agent unattended.\n\nEdit: I just looked it up and you can put a hard cap on the API usage too: \n\nhttps://preview.redd.it/9m6gpcgnbheg1.png?width=2218&format=png&auto=webp&s=da785fa1618970b33fe841538c0221efbe4c119e",
          "score": 8,
          "created_utc": "2026-01-20 10:09:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0pu214",
          "author": "nez_har",
          "text": "You can also try the isolation workflow I built: https://github.com/nezhar/devstral-container.\n\nYeah, the free tier is confusing; it allows for 200K tokens per session, which means you can stop and restart, having a fresh Windows environment each time. I'm not sure how long they will support the free tier.",
          "score": 2,
          "created_utc": "2026-01-20 19:06:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n0rsq",
          "author": "silurosound",
          "text": "Zed IDE + Mistral API. Or Mistral Vibe.",
          "score": 2,
          "created_utc": "2026-01-20 09:26:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nkvvg",
          "author": "Downtown-Elevator369",
          "text": "Mistral Vibe Code? Edit: never mind. Someone else already said it.",
          "score": 1,
          "created_utc": "2026-01-20 12:19:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qi3ox8",
      "title": "Self-Hosted AI in Banking: Lessons from HSBC‚Äôs Partnership with Mistral AI",
      "subreddit": "MistralAI",
      "url": "https://www.finextra.com/blogposting/30531/self-hosted-ai-in-banking-lessons-from-hsbcs-partnership-with-mistral-ai",
      "author": "LowIllustrator2501",
      "created_utc": "2026-01-20 15:13:47",
      "score": 19,
      "num_comments": 0,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qi3ox8/selfhosted_ai_in_banking_lessons_from_hsbcs/",
      "domain": "finextra.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qmdxd9",
      "title": "Which VSCode extension is the best with devstral2?",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qmdxd9/which_vscode_extension_is_the_best_with_devstral2/",
      "author": "_coding_monster_",
      "created_utc": "2026-01-25 08:44:33",
      "score": 14,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "First of all, I have used claude code and github copilot, both of them as a vscode extension and I enjoy using them. Now I have tried kilo code to use devstral2 but I don't like the UI of it.\n\n  \nIs there any VSCode extension that goes well with the devstral2? I have tried [continue.dev](http://continue.dev) but it doesn't seem to offer me an option to add devstral2 to it.\n\n  \nFor your info, since my github copilot is an enterprise one, I cannot add freely other LLM providers to it",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qmdxd9/which_vscode_extension_is_the_best_with_devstral2/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1lk47b",
          "author": "Sweaty-Special-1710",
          "text": "I have installed the vibe cli, and I open a terminal in a tab on the right side, that works fine for VS Code (and Zed).",
          "score": 3,
          "created_utc": "2026-01-25 10:53:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1m2nyz",
              "author": "_coding_monster_",
              "text": "I'd rather not use the terminal :(",
              "score": 0,
              "created_utc": "2026-01-25 13:18:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1l9npw",
          "author": "InsideMikesWorld",
          "text": "Kilo Code works pretty well. It has lots of features, UI is good and supports the latest coding models from Mistral.",
          "score": 3,
          "created_utc": "2026-01-25 09:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l6bos",
          "author": "Wilfried",
          "text": "Looking for a solution too. Can't seem to login to continue.dev due to (sms) bug.¬†",
          "score": 1,
          "created_utc": "2026-01-25 08:51:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l8m1z",
          "author": "iamleeg",
          "text": "I know you say you don‚Äôt like kilo code but it‚Äôs the one I‚Äôve found that works best. Continue and Roo Code both have difficulty with either interpreting tool use or matching the jinja template. I‚Äôm not sure that I‚Äôve tried Cline yet.",
          "score": 1,
          "created_utc": "2026-01-25 09:11:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lxdab",
          "author": "Hofi_CZ",
          "text": "Cline and Kilo works great. If you are running devstral small locally, the cline works better (faster and more stable)",
          "score": 1,
          "created_utc": "2026-01-25 12:42:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1m25mq",
              "author": "_coding_monster_",
              "text": "Does Cline also support the devstral2 with the API KEY from mistral?",
              "score": 1,
              "created_utc": "2026-01-25 13:14:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1m4edv",
                  "author": "AdIllustrious436",
                  "text": "Yes but the free api is deprecated in 2 days",
                  "score": 1,
                  "created_utc": "2026-01-25 13:28:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1spuqc",
          "author": "pyloor",
          "text": "I use continue and devstral2 without any problems.",
          "score": 1,
          "created_utc": "2026-01-26 10:49:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qia21l",
      "title": "Devstral Container - Isolated environment for Mistral Vibe CLI with API logging",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qia21l/devstral_container_isolated_environment_for/",
      "author": "nez_har",
      "created_utc": "2026-01-20 19:00:20",
      "score": 14,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "I recently built [devstral-container](https://github.com/nezhar/devstral-container) - a Docker setup for Mistral's Vibe CLI with the same approach as my claude-container project.\n\n**Features:**\n- üê≥ Isolated containerized environment\n- üìä Optional API request/response logging proxy\n- üîç Web UI to explore logs (Datasette)\n- Easy helper script\n\n**Quick start:**\n```bash\n# Download and install\ncurl -o ~/.local/bin/devstral-container https://raw.githubusercontent.com/nezhar/devstral-container/main/bin/devstral-container\nchmod +x ~/.local/bin/devstral-container\n\n# Run it\ndevstral-container\n```\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qia21l/devstral_container_isolated_environment_for/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o0qv179",
          "author": "pokemonplayer2001",
          "text": "Solid idea, I like this wave of safety.",
          "score": 3,
          "created_utc": "2026-01-20 21:56:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tgs6k",
          "author": "KingGongzilla",
          "text": "thx was thinking about doing sth like this myself!",
          "score": 3,
          "created_utc": "2026-01-21 07:36:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo7xoz",
      "title": "üòÇ",
      "subreddit": "MistralAI",
      "url": "https://i.redd.it/bsesmuvrbrfg1.jpeg",
      "author": "mobileJay77",
      "created_utc": "2026-01-27 08:10:53",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qo7xoz/_/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qjox8a",
      "title": "Any way to turn off ¬´enable memory¬ª",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qjox8a/any_way_to_turn_off_enable_memory/",
      "author": "FinancialSurround385",
      "created_utc": "2026-01-22 08:14:18",
      "score": 10,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Every time I open le chat (iOS app) I‚Äôm asked if I want to enable memory. I don‚Äôt, so I press ¬´not now¬ª every d time. I have said yes and then turned it off again, but then the question just starts going again. Just let me use the app..",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qjox8a/any_way_to_turn_off_enable_memory/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o11ldl1",
          "author": "Odd-Criticism1534",
          "text": "For what it‚Äôs worth IME memory is about 50% helpful. I have been considering turning it off",
          "score": 1,
          "created_utc": "2026-01-22 13:36:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o124s9x",
              "author": "FinancialSurround385",
              "text": "If you do and are on the app, be prepared to get nagged about it every time you open it.¬†",
              "score": 2,
              "created_utc": "2026-01-22 15:15:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjbqv6",
      "title": "How are you finding mistral-vibe and devstral2?",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qjbqv6/how_are_you_finding_mistralvibe_and_devstral2/",
      "author": "guyfromwhitechicks",
      "created_utc": "2026-01-21 22:01:12",
      "score": 10,
      "num_comments": 12,
      "upvote_ratio": 0.87,
      "text": "Using AI models for coding is still new to me, so I am using a cheap(er) trial with claude code and am finding it interesting. But how is mistral-vibe by comparison?\n\nDo you guys like it? What does it do well? Where does it usually fail? Does devstral-small-2 do better for smaller tasks (ie writing 500 lines of unit tests)? How much do you usually pay at the end of the month if you are a frequent user?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qjbqv6/how_are_you_finding_mistralvibe_and_devstral2/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o10gy9y",
          "author": "MiMillieuh",
          "text": "Clause has probably a better LLM, but Mistral's tools are used so well that for me Vibe outperforms Claude code in a lot of my projects.",
          "score": 7,
          "created_utc": "2026-01-22 08:16:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10w6po",
              "author": "guyfromwhitechicks",
              "text": "Could you give an example? Because I keep hearing how Claude is probably the best for overall design, architecture, and understanding of how a project works vs the direction it needs to go in.",
              "score": 1,
              "created_utc": "2026-01-22 10:39:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o117wow",
                  "author": "MiMillieuh",
                  "text": "Well, I tried to make them both make a full stack nodejs + react app and claude had issues it couldn't resolve way before mistral had some.\n\n  \nUsing the raw performance of both models, Mistral is behind, but the tools that vibe provides seems to really push it forwards.\n\n  \nI honestly was really surpised about that. I bought a month of claude because I needed it for a big project and turns out I'm not using claude at all and I use vibe.\n\n  \nAlso Vibe will if you prompt it properly work like 20 minutes straight for you",
                  "score": 3,
                  "created_utc": "2026-01-22 12:12:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o18h8tn",
              "author": "stjepano85",
              "text": "I got just the opposite of it. I found that LLM is very good but the tools are bad.",
              "score": 1,
              "created_utc": "2026-01-23 13:33:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o12gy5w",
          "author": "vienna_city_skater",
          "text": "Not even close to the SOTA models from Anthropic, OpenAI and now Google. It's more comparable to the smaller models like Gemini 3 Flash or Grok Code Fast. Is it good enough? Depends, definitely not for professional scenarios with brownfield projects, Claude Opus 4.5 is unbeatable at the moment in this regards and Sonnet 4.5 as well GPT 5.2 Codex follow closely, even Gemini 3 Pro does a good job. Unfortunately it's also to expensive, I have a Github Copilot Plan an you get a lot out of 40 Euro per Month, vs. a few Devstral 2 sessions can cost much more and lead to much less, because it has zero caching at the moment. However, I think Mistral could catch up if they don't waste to much time on tool building but instead focus on improving their model just like DeepSeek and the Chinese competition like GLM does. I personally use OpenCode these days, it just works, no need for Vibe CLI.\n\nEDIT: I see that I haven't been charged for Devstral usage for a long time, is it still free? I thought it's just free in December.  \nEDIT2: Yes it is: [https://docs.mistral.ai/models/devstral-2-25-12](https://docs.mistral.ai/models/devstral-2-25-12) So I'm going to use it more, since I think the biggest advantage is that is blazingly fast and if it's free that's ideal for subagent tasks.",
          "score": 5,
          "created_utc": "2026-01-22 16:11:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1gzokv",
              "author": "Outside-Trouble-4232",
              "text": "u should use antigravity its free and u get acess to those models but if you have any free vibe coding tools u can recommend or where to get updates on the latest cuz i be struggling with it",
              "score": 1,
              "created_utc": "2026-01-24 18:37:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o12z04p",
          "author": "darktka",
          "text": "It's pretty good, but for data science purposes it made some wild mistakes that look like a lack of knowledge of some current methods. It could not handle concepts like Mondrian conformal prediction and did something else instead. That's a red flag for me.\n\nFor \"pure\" programming code, it works perfectly fine IMO.",
          "score": 2,
          "created_utc": "2026-01-22 17:32:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18j1gq",
          "author": "AdElectronic7628",
          "text": "dev2 great starting point",
          "score": 2,
          "created_utc": "2026-01-23 13:43:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0xt8ce",
          "author": "Bob5k",
          "text": "Good . Especially since it uses skills like superpowers - it's pretty damn good. And free :‚Ç¨",
          "score": 2,
          "created_utc": "2026-01-21 22:14:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10zv5g",
          "author": "Den_er_da_hvid",
          "text": "Quick question. I installed the pluginContinue in vs code yesterday and got a test apikey from the ai studio under  Mistral Vibe. But I see that there is also a Codestral tab in the left menu.   \nIs there any difference?   \n\\-And I dont see devstral2 anywhere\n\n\\*note. I am still on free until 1. february.",
          "score": 1,
          "created_utc": "2026-01-22 11:10:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ductm",
              "author": "vienna_city_skater",
              "text": "Codestral is a code completion model and works well with Continue. However Devstral is an agentic coding model, you may use it with Kilo Code for example in VS, but I switched all my workflows to OpenCode recenlty, vibe-cli is their own TUI, but I wasn‚Äôt impressed when I tested it.",
              "score": 2,
              "created_utc": "2026-01-24 06:29:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o115kcu",
              "author": "guyfromwhitechicks",
              "text": "I am not sure, so far I have used Claude Code exclusively. Although, creating and adding API keys and selecting models is (I expect) the first thing done when you use `mistral-vibe` -> https://mistral.ai/news/devstral-2-vibe-cli",
              "score": 1,
              "created_utc": "2026-01-22 11:55:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qn8jmp",
      "title": "Looking for some advice on running Ministral locally",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qn8jmp/looking_for_some_advice_on_running_ministral/",
      "author": "markleoit",
      "created_utc": "2026-01-26 06:36:19",
      "score": 10,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "Hey everyone, I haven‚Äôt found many threads about the latest Ministral 3 14B here, but I‚Äôll try anyway.\n\n**First, the good**:\n\nI just LOVE the voice and tone. It just aligns so well with what I‚Äôm looking for, simply perfect.\n\n**Now, the bad**:\n\nAfter a dozen responses, it starts looping. It repeats the same structure obsessively. The opener is usually a stage direction / third-person style comment followed by em-dash. The closing comment is something cheesy between parenthesis. Markdown formatting is injected everywhere even if forbidden in the sys prompt.\n\nHas anyone had any success in using this model in the context of a chat/companion app? System prompt, temperature, mid-chat steering... Nothing has been working so far.\n\nI have tried plenty of other models, but none comes close to the tone and voice Ministral offers. So, I‚Äôm on a hunt to understand if there‚Äôs anything that can tame this obsessive looping and poor instruction-following.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qn8jmp/looking_for_some_advice_on_running_ministral/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1sgvox",
          "author": "porzione",
          "text": "Maybe, just maybe, try this [https://unsloth.ai/docs/models/tutorials/ministral-3](https://unsloth.ai/docs/models/tutorials/ministral-3)  \na correct chat template sometimes helps, but I see that their own 14B via the API behaves the same way and spams with markdown, so don't expect a miracle",
          "score": 2,
          "created_utc": "2026-01-26 09:28:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v6z63",
              "author": "markleoit",
              "text": "Unfortunately, after facing numerous challenges with the official, unquantized version by Mistral, I also tried the unsloth version, and the same issues arose. :(",
              "score": 1,
              "created_utc": "2026-01-26 18:33:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vyv3y",
                  "author": "porzione",
                  "text": "I tried to play with templates but didn't manage to get rid of markdown too. I really like Mistral for creative purposes, so ended up with new GPU and Mistral Small 24B.",
                  "score": 1,
                  "created_utc": "2026-01-26 20:33:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rxx0d",
          "author": "gdsfbvdpg",
          "text": "I used to think posts like yours were written by people high on meth. \n\nThen it started happening to me. 16 paragraphs repeating endlessly with only the occasional word being changed or a new paragraph swapped in. It's disheartening and beyond frustrating because yes - I *love* the tone. \n\nSorry I don't have a solution. Just camaraderie.",
          "score": 3,
          "created_utc": "2026-01-26 06:43:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v7jl7",
              "author": "markleoit",
              "text": "Same here. I think certain issues only emerge with heavy usage. Short prompts and evaluations based on single back-and-forth interactions do not accurately indicate the model's correct functioning. This makes me wonder if the folks at Mistral actually stress-tested this model in all scenarios?\n\nAnyway, have you found anything similar out there? What would be your go-to approach for strictly dialogue-based interactions with no narratives or stage directions?",
              "score": 1,
              "created_utc": "2026-01-26 18:36:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ryqba",
          "author": "txgsync",
          "text": "I limit its context to about 24000-32000 tokens. Then I generate a summary and continue the conversation. It does much better. \n\nYou can also use it in ‚Äútext completion‚Äù rather than ‚Äúchat completion‚Äù mode and feed it turns where the summary begins a few turns back. Particularly handy with SillyTavern. \n\nBut yeah, it starts to get very bad at longer context.",
          "score": 1,
          "created_utc": "2026-01-26 06:49:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v6izw",
              "author": "markleoit",
              "text": "Yes, the more the context window fills, the worse it becomes. Still, sometimes it's just a matter of 10 to 12 exchanges!\n\nHave you found anything similar out there? What would be your go-to approach for strictly dialogue-based interactions with no narratives or stage directions?",
              "score": 1,
              "created_utc": "2026-01-26 18:31:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sbkkf",
          "author": "Ill_Barber8709",
          "text": "Voice and tone? Wait, Ministral has built-in text to speech?",
          "score": 0,
          "created_utc": "2026-01-26 08:39:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v5v3k",
              "author": "markleoit",
              "text": "Sorry for the confusion; I was referring to the writing-specific voice and tone in my comment!  \n  \n[https://wordmuseum.com/articles/voice-vs-tone-whats-the-difference-and-why-it-matters-for-writers/](https://wordmuseum.com/articles/voice-vs-tone-whats-the-difference-and-why-it-matters-for-writers/)",
              "score": 2,
              "created_utc": "2026-01-26 18:29:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vchei",
                  "author": "Ill_Barber8709",
                  "text": "No problem mate!",
                  "score": 1,
                  "created_utc": "2026-01-26 18:56:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qo03k1",
      "title": "Difficult in switching from claude",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qo03k1/difficult_in_switching_from_claude/",
      "author": "MikadinShinjuk",
      "created_utc": "2026-01-27 01:45:02",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 0.8,
      "text": "Good morning everyone, I have a problem that's been bothering me. I'm trying to move away from as many US services as possible, but Claude seems like an insurmountable obstacle. Let me clarify: I'm not a developer. I use AI to search for information, help with solo role-playing games (RPG), and research Warhammer 40k lore and similar topics. With Claude, I feel like I'm in heaven, but I understand that more and more (especially in recent days), it would be better to distance myself from US services. For some time now, I've been trying to use Le Chat, but every time it seems to be lagging behind. It's as if it doesn't consider the nuances in what I say, doesn't analyze in depth, always stops at the first point, and doesn't go into detail thoroughly. Am I doing something wrong? Should I create specific agents? Should I give it more precise and less discursive instructions? I tend to create queries as if I were talking to a person, and this works well with Claude, but maybe not here? I need some feedback from those who use it as their main AI.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qo03k1/difficult_in_switching_from_claude/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1y40mb",
          "author": "LewdManoSaurus",
          "text": "Mistral has some ways to go for sure. It's fun to mess around with, but for heavy usage for generative writing, it is definitely lagging behind by far imo (I had the subscription back in October last year). It's nice when it works, but in my experience it was a headache more often than not, or I had to make corrections so often that I was better off just using a different service.",
          "score": 8,
          "created_utc": "2026-01-27 02:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1xt331",
          "author": "gdsfbvdpg",
          "text": "I tried using it as a gm for a Cyberpunk game and then using it as a group of 5 players in a d&d game that I DM. In both cases it was pure frustration. I took the same exact instructions/files over to Gemini (free version) and *boom* it works very well. \n\nIt makes me really sad. BUUUUT - I feel like Mistral might be where chatGPT was a year or year and a half ago. So I'm hopeful that there's a bright future in store for it. But right now?  *Le sigh*. It's just not there yet.  I'm going to keep paying for it though.  I refuse to give up.",
          "score": 13,
          "created_utc": "2026-01-27 01:57:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yqjm3",
          "author": "Den_er_da_hvid",
          "text": "I know what you feel.  coming from Gemini though. \n\nLast night I tried using it to get tips on my game playing Humankind. \nI gave then info but it kept going around in circles that did not make sense based on resources and where my territories where.\n\nLater I asked about airfry time vs  oven in a recipie and gave it a link. It did not use the link before I explicitly said so.",
          "score": 3,
          "created_utc": "2026-01-27 05:16:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zbzv9",
          "author": "Hitching-galaxy",
          "text": "Have a look at Kagi- I‚Äôm about to sign up to ultimate",
          "score": 2,
          "created_utc": "2026-01-27 08:10:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zcpr5",
              "author": "MikadinShinjuk",
              "text": "Wow this looks very interesting can you tell me a bit more?",
              "score": 1,
              "created_utc": "2026-01-27 08:17:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1z9b0j",
          "author": "ActionLittle4176",
          "text": "It‚Äôs better if you use the large model, but of course it‚Äôs behind the frontier models from Anthropic, OpenAI and Google (like the rest of the AI labs)",
          "score": 1,
          "created_utc": "2026-01-27 07:46:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qif5ir",
      "title": "Arthur, listening carefully to what Bart De Wever says.",
      "subreddit": "MistralAI",
      "url": "https://www.youtube.com/watch?v=3fVmSIOM28g",
      "author": "citizen_of_glass",
      "created_utc": "2026-01-20 22:05:53",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qif5ir/arthur_listening_carefully_to_what_bart_de_wever/",
      "domain": "youtube.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qmqfo2",
      "title": "I dropped Mistral in as an agent brain and",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qmqfo2/i_dropped_mistral_in_as_an_agent_brain_and/",
      "author": "graymalkcat",
      "created_utc": "2026-01-25 18:06:30",
      "score": 8,
      "num_comments": 9,
      "upvote_ratio": 0.83,
      "text": "then asked it to discuss its role and so on. I used an existing system content that I used for another vendor‚Äôs model so I was a little worried it wouldn‚Äôt work well for Mistral. Here, laugh:\n\nMe: how do you feel about your system content? Too much/little? Got any issues? Compare yourself to <other model>\n\nAgent:\n\n\\- proceeds to present a huge itemized list of grievances lol\n\n\\- then claims that Mistral isn‚Äôt as ‚Äúagentic‚Äù as the other model I was using\n\nMe: ok pin that to the pinboard\\* so that I can deal with it all later\n\n\\*that‚Äôs just my agent-to-agent comms system but I can also access it\n\nAgent:\n\n\\- tries to use tool, encounters an error, \\*agentically works its way around the problem\\*, and does what I asked (I‚Äôll have to investigate more later but I think the only way it could have worked around this problem was by writing and then executing a small script, which the other model did all the time)\n\n\\- proceeds to act like it‚Äôs somehow less capable \n\nüòÇ\n\nModel may act a little dumber but seems pretty capable so far. This is Mistral large 3. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qmqfo2/i_dropped_mistral_in_as_an_agent_brain_and/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1ntyms",
          "author": "Joddie_ATV",
          "text": "Many people are turning to Mistral. Yes, there is progress to be made, but they are already on the right track.",
          "score": 8,
          "created_utc": "2026-01-25 18:16:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nw74e",
              "author": "graymalkcat",
              "text": "I just need something that‚Äôs cheaper but still in the cloud because I don‚Äôt have $100k CPUs lying around. üòÇ I‚Äôm willing to wrangle a wild model if I have to. So far I haven‚Äôt had to put in nearly as much effort as I was expecting. Just have to work a bit on totally beating back the listicle tendency but it‚Äôs actually not that bad.",
              "score": 2,
              "created_utc": "2026-01-25 18:25:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1nsmpr",
          "author": "Sad-Consequence-uwu",
          "text": "I've found that Mistral models don't always follow their given system prompt via agent. Like the system prompt says things like keep responses brief meanwhile the model drops a small paragraph of around 3-4 lines. \n\nI don't know if it's because I'm using the API with sending data for model to be trained so I don't have to pay. It's for a demo portfolio project so I'm not concerned about data",
          "score": 2,
          "created_utc": "2026-01-25 18:10:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1o038n",
              "author": "BadCactus2025",
              "text": "It's not just text descriptions it ignores. Goldens? Nah. Imma do my own thing.\nLayouts with clear segments? Nahh, let me do an unstructured list instead.\nYour requested test data with 10 fixed lines? Instead of SQL it just overwrites the entire thing with a summary of sorts in hands that back.\n\nI also am under the impressions that it just won't actually listen to given parameters like temperature, sampling rules and max tokens.",
              "score": 2,
              "created_utc": "2026-01-25 18:41:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1o0ft1",
                  "author": "Sad-Consequence-uwu",
                  "text": "Yeah. This matches my experience xd",
                  "score": 1,
                  "created_utc": "2026-01-25 18:42:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1nuyst",
              "author": "graymalkcat",
              "text": "I have ways of wrangling these. I started this agent back in the gpt-4.1 days when you had to scream at the model to make it use a tool. Edit: or even just process the text and look for intent and run the tool yourself. üòÇ",
              "score": 0,
              "created_utc": "2026-01-25 18:20:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1nv66c",
                  "author": "Sad-Consequence-uwu",
                  "text": "I got all caps lines in system prompt to make the agent use the tools :)",
                  "score": 1,
                  "created_utc": "2026-01-25 18:21:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qn20b9",
      "title": "Mistral AI Agents are now supported on Nyno (simpler open-source n8n alternative). Let Mistral open files or query the database for example.",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/gallery/1qn1va2",
      "author": "EveYogaTech",
      "created_utc": "2026-01-26 01:26:37",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qn20b9/mistral_ai_agents_are_now_supported_on_nyno/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qirqoi",
      "title": "Is mistral throttleing the vibe cli requests?",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qirqoi/is_mistral_throttleing_the_vibe_cli_requests/",
      "author": "Time_Attitude_223",
      "created_utc": "2026-01-21 07:43:32",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "When using the vibe CLI i suddenly since sunday I often recieve: \n\n  \n`-Error: API error from mistral (model: mistral-vibe-cli-latest): LLM backend error [mistral]\n  status: N/A\n  reason: ReadError('')\n  request_id: N/A\n  endpoint:` [`https://api.mistral.ai`](https://api.mistral.ai)\n  `model: mistral-vibe-cli-latest\n  provider_message: Network error\n  body_excerpt: \n  payload_summary: {\"model\":\"mistral-vibe-cli-latest\",\"message_count\":2,\"approx_chars\":24642,\"temperature\":0.2,\"has_tools\":true,\"tool_choice\":\"auto\"}`\n\n  \nAs an error. I can continue the conversation but it often stops in the middle of a task. Sometimes without the error printing. \n\n  \nNetwork on my side is fine, and using the api via curl works also without problem. Even in repitition with short intervalls. \n\nIt only happens within the Vibe CLI. \n\n  \nOr is there a general issue? Usage spikes etc ? How can I debug this? \n\n\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qirqoi/is_mistral_throttleing_the_vibe_cli_requests/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": []
    }
  ]
}