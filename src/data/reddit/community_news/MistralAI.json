{
  "metadata": {
    "last_updated": "2026-02-02 02:59:57",
    "time_filter": "week",
    "subreddit": "MistralAI",
    "total_items": 20,
    "total_comments": 139,
    "file_size_bytes": 164518
  },
  "items": [
    {
      "id": "1qo7xoz",
      "title": "ðŸ˜‚",
      "subreddit": "MistralAI",
      "url": "https://i.redd.it/bsesmuvrbrfg1.jpeg",
      "author": "mobileJay77",
      "created_utc": "2026-01-27 08:10:53",
      "score": 190,
      "num_comments": 5,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qo7xoz/_/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1zru53",
          "author": "Nefhis",
          "text": "Meanwhile, Mistral... ðŸ¤£\n\nhttps://preview.redd.it/h4d8kw56evfg1.png?width=484&format=png&auto=webp&s=52cb4b2667463298c51071ff332a8d2d9fe55473",
          "score": 23,
          "created_utc": "2026-01-27 10:36:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zl238",
          "author": "Wickywire",
          "text": "Claude even has the puckering animation ðŸ˜…",
          "score": 9,
          "created_utc": "2026-01-27 09:34:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f6j8n",
          "author": "Ejbarzallo",
          "text": "Only Mistral is the cat's head. And Le Chat literally means The Cat",
          "score": 3,
          "created_utc": "2026-01-29 15:04:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zt5ba",
          "author": "whoisyurii",
          "text": "LechÃ¡ ðŸ“¢",
          "score": 2,
          "created_utc": "2026-01-27 10:47:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2w2zfv",
          "author": "BiboxyFour",
          "text": "1. Prolapsed anus\n2. HPV\n3. Nice and tight\n4. Anal fissure",
          "score": 1,
          "created_utc": "2026-02-01 01:02:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qoig0q",
      "title": "Vibe 2.0 - Terminally online Mistral Vibe.",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qoig0q/vibe_20_terminally_online_mistral_vibe/",
      "author": "Clement_at_Mistral",
      "created_utc": "2026-01-27 16:22:48",
      "score": 188,
      "num_comments": 35,
      "upvote_ratio": 0.99,
      "text": "Today, we're releasing Mistral Vibe 2.0 - a major upgrade to our terminal-native coding agent, powered by the state-of-the-art Devstral 2 model family. Build custom subagents, clarify before you execute, load skills with slash commands, and configure your own workflows to match how you work.\n\nMistral Vibe isÂ **now available on the Le Chat Pro and Team plans**Â \\- with pay-as-you-go credits for power use, or bring your own API key.  \nDo you already have a Le Chat Pro/Teams plan? Get your Vibe keyÂ [here](https://console.mistral.ai/codestral/cli).\n\n*Learn more about how to use Vibe*Â [*here*](https://docs.mistral.ai/mistral-vibe/introduction)\n\n# Whats New\n\n* Mistral Vibe 2.0: CustomÂ **subagents**,Â **multi-choice clarifications**,Â **slash-command skills**,Â **unified agent modes**, andÂ **automatic updates**.\n* Available today onÂ **Le Chat Pro and Team plans**Â with PAYG for extra usage, or BYOK.\n* Devstral 2 moves toÂ **paid API access**:Â **Free on the Experiment plan**Â in Mistral Studio.\n* Enterprise services:Â **fine-tuning**,Â **reinforcement learning**, andÂ **code modernization**.\n\n*Learn more about*Â [*Vibe 2.0*](https://github.com/mistralai/mistral-vibe)Â *in our*Â [*blog post*](https://mistral.ai/news/mistral-vibe-2-0)Â *and*Â [*product page*](https://mistral.ai/products/vibe)\n\nhttps://reddit.com/link/1qoig0q/video/bx60g52v3xfg1/player\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qoig0q/vibe_20_terminally_online_mistral_vibe/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o21pzju",
          "author": "Gen5nake",
          "text": "I hope the quotas will at least match those of other providers, but this is exactly what I was waiting for to cancel my Caude subscription :)\n\nOne thing I might have missed, though is where we can set an usage limit? How can we tell when the quota has been exceeded and the system switches to API usage? There's a global settings for API's but can't set to 0.",
          "score": 23,
          "created_utc": "2026-01-27 17:04:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21w0sn",
          "author": "sndrtj",
          "text": "Please tell me it keeps the quirky status messages. Loved \"petting le chat\" etc.",
          "score": 15,
          "created_utc": "2026-01-27 17:30:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26zuez",
              "author": "pandora_s_reddit",
              "text": "Yes :)",
              "score": 2,
              "created_utc": "2026-01-28 11:02:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o270lo3",
                  "author": "sndrtj",
                  "text": "Purrrrfect.",
                  "score": 1,
                  "created_utc": "2026-01-28 11:08:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21teez",
          "author": "deegwaren",
          "text": "Is Le Chat Pro vibe-CLI usage in third-party harnesses like OpenCode allowed and supported? Because disallowing use of subscription in other apps was the reason for me to cancel my Claude subscription.\n\nSince GitHub Copilot and GPT Plus/Pro officially support OpenCode, I really hope mistral does the same.",
          "score": 10,
          "created_utc": "2026-01-27 17:19:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22qzjf",
              "author": "DueKaleidoscope1884",
              "text": "this would be an important factor for me too\n\n  \nDoes Mistral typically reply to the questions in this sub?",
              "score": 2,
              "created_utc": "2026-01-27 19:43:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2776tq",
              "author": "vienna_city_skater",
              "text": "You can just use the API, no need to go through the CLI.",
              "score": 0,
              "created_utc": "2026-01-28 11:59:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27c299",
                  "author": "deegwaren",
                  "text": "But is API usage included in the Pro subscription?",
                  "score": 1,
                  "created_utc": "2026-01-28 12:33:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21mzy0",
          "author": "cosimoiaia",
          "text": "Unfortunately that broke completely the local functionality of it. \n\nI was using devstral-24b with local endpoints and after the update it's unable to do anything at all. ðŸ˜¢\n\nDoes this mean that we can't use local models anymore?",
          "score": 8,
          "created_utc": "2026-01-27 16:51:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21nxqx",
              "author": "SourceCodeplz",
              "text": "Probably a bug, in Mistral Vibe they said from the start it should support local deployments of LLMs.",
              "score": 6,
              "created_utc": "2026-01-27 16:55:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o22ikam",
                  "author": "cosimoiaia",
                  "text": "Yeah, I agree. I will debug it more to see what's going on better but switching back to the old version worked as before.",
                  "score": 1,
                  "created_utc": "2026-01-27 19:06:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o21qfj1",
                  "author": "Downtown-Elevator369",
                  "text": "Yeah, the local option is still there under /model for me",
                  "score": 1,
                  "created_utc": "2026-01-27 17:06:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26zzgr",
              "author": "pandora_s_reddit",
              "text": "Hi there - It should be allowed, the team is taking a look but feel free to open an issue if you did not yet. You are free to use local models or any provider.",
              "score": 3,
              "created_utc": "2026-01-28 11:03:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27ftqy",
                  "author": "cosimoiaia",
                  "text": "Thank you for answering!\n\nStrangely enough I did a fresh reinstall and it started working also with the v2.0.0. \n\nMaybe something causes some conflict when just updating and the local model doesn't use any tool call.\n\nIn any case, thank you for the great work, I really love vibe!",
                  "score": 1,
                  "created_utc": "2026-01-28 12:57:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o22hqum",
              "author": "cosimoiaia",
              "text": "I switched back to v1.3.5 and everything started working again with the exact same setup, so it's definitely something with the new release.",
              "score": 1,
              "created_utc": "2026-01-27 19:02:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27jder",
          "author": "NiceTryAmanda",
          "text": "so the pro plan comes with some usage built in? I generated an api key and /terminal shows that I'm spending money, though I have a pro plan",
          "score": 3,
          "created_utc": "2026-01-28 13:18:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2abdqv",
              "author": "Salt-Willingness-513",
              "text": "username checks out",
              "score": 1,
              "created_utc": "2026-01-28 20:52:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2yh4vi",
              "author": "vienna_city_skater",
              "text": "Make sure to use the API key for VIBE CLI you can generate in AI Studio, not the API key from the La Platforme Admin Panel. I made the same mistake and was charged for my usage.",
              "score": 1,
              "created_utc": "2026-02-01 12:01:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21i3f5",
          "author": "fxdev1",
          "text": "Can someone already say something about the usage quota compared to codex, gemini cli, claude code, antigravity?",
          "score": 6,
          "created_utc": "2026-01-27 16:30:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26mlb0",
              "author": "Gen5nake",
              "text": "Since I started testing yesterday, the quota seems much larger than with others. Yesterday, I did hit some API rate limits, but I could continue after a few minutes. Today, Iâ€™ve already been coding non-stop for 1.5 hours and havenâ€™t hit any limits yet. Also the context seems to fill up more slowly compared to Claude.  \nSo far it's great!",
              "score": 1,
              "created_utc": "2026-01-28 09:02:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o239d95",
          "author": "909876b4-cf8c",
          "text": "Is the user's input and data used for training, when using this through Le Chat Pro subscription?",
          "score": 2,
          "created_utc": "2026-01-27 21:06:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fsdoo",
              "author": "pandora_s_reddit",
              "text": "No, we dont train on it ! You can use freely without worrying about data when using a pro subscription.",
              "score": 2,
              "created_utc": "2026-01-29 16:42:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21khjy",
          "author": "Downtown-Elevator369",
          "text": "I just paid for a Pro plan this morning. This is great! Edit: I ran this to upgrade my existing Vibe setup on Mac: uv pip install --upgrade vibe",
          "score": 4,
          "created_utc": "2026-01-27 16:40:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21h6ut",
          "author": "EzioO14",
          "text": "Amazing news ! Canâ€™t wait to test it out",
          "score": 2,
          "created_utc": "2026-01-27 16:26:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21lstg",
          "author": "Hofi_CZ",
          "text": "Is possible to use devstral via Kilo code as part of the Pro plan?",
          "score": 1,
          "created_utc": "2026-01-27 16:46:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21syb6",
              "author": "nordenstrom",
              "text": "Yes, I've been doing that for a while.",
              "score": 1,
              "created_utc": "2026-01-27 17:17:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mx0wb",
                  "author": "vienna_city_skater",
                  "text": "Do you set the User Agent in the request header? API calls arenâ€™t free per se, but the only difference I see looking at the code is the User Agent header.",
                  "score": 1,
                  "created_utc": "2026-01-30 17:16:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o261j2r",
          "author": "Old-Glove9438",
          "text": "Hope itâ€™s better than Codex with GPT-5.2 high",
          "score": 1,
          "created_utc": "2026-01-28 06:03:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mx3mi",
              "author": "vienna_city_skater",
              "text": "Unfortunately not, and Codex is unfortunately not as good as Opus",
              "score": 1,
              "created_utc": "2026-01-30 17:16:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26xvkw",
          "author": "ProdbyTwoFace",
          "text": "Love your models for local inference so definitely gonna try that.",
          "score": 1,
          "created_utc": "2026-01-28 10:45:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a8wou",
          "author": "Sarttek",
          "text": "I'm new to this so sorry for asking but how exactly pricing works? I'm Pro Le Chat buyer, I downloaded the program and noticed that on the right lower corner there is a tokens counter. I assume that because of my pro sub my base token pool is higher? Or do I have to pay for it separately? What will happen if I use all of that? When will it renew?\n\nI tired looking for FAQs but nothing in there really answers my questions",
          "score": 1,
          "created_utc": "2026-01-28 20:41:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2b9flp",
              "author": "Gen5nake",
              "text": "The token counter you see is the context size usage for your current chat session, not your API usage or token balance. It indicates how much of the conversation history is being kept active in this chat. ;)",
              "score": 1,
              "created_utc": "2026-01-28 23:29:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2hdep9",
          "author": "VaginosiBatterica",
          "text": "Hey u/Clement_at_Mistral  can you add som sort of \"study\" mode to le chat?",
          "score": 1,
          "created_utc": "2026-01-29 21:06:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqj58f",
      "title": "Hi Mistral AI - you rock more than you know - lets take it further",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qqj58f/hi_mistral_ai_you_rock_more_than_you_know_lets/",
      "author": "hartmanners",
      "created_utc": "2026-01-29 19:55:08",
      "score": 159,
      "num_comments": 12,
      "upvote_ratio": 0.99,
      "text": "I've been using AI since it sprung out as part of my job. I am a European and the hype has been overseas most of time (OpenAI, Claude, Gemini as the serious products).\n\n  \nI trust Mistral more than the others when matters are crucial. You could still re-train the model a bit more when it comes to technical issues, but it is doing a great job.\n\n  \nThese other LLM models, except Claude maybe, lack what Mistral is bringing to the table in terms of sources trained on and system prompts taiming it. It's objective and not biased.\n\n  \nMistral, please prioritize making a Windows store app, MacOS app to get this ball rolling faster. It's easier to get my crowd going with these simple means and thereby spreading the good work you did. Ppl need convenience - you already deliver on quality.\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qqj58f/hi_mistral_ai_you_rock_more_than_you_know_lets/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o2h6squ",
          "author": "Quiet_Illustrator410",
          "text": "Mistral AI is top-notch and from EU, perfect combo!",
          "score": 28,
          "created_utc": "2026-01-29 20:34:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hk8fo",
          "author": "Suitable-Guest2781",
          "text": "Fully agree, daily (& happy) user of Mistral AI here ! \n\nWould really love to see a collab. with Kyutai (https://kyutai.org) for next level voice interactions with LeChat !",
          "score": 10,
          "created_utc": "2026-01-29 21:38:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2idku5",
              "author": "EveYogaTech",
              "text": "Oh congratulations on the funding from Iliad Group. That's amazing. I'd also be interested in a Collab with you in the long-term with /r/Nyno (EU-only platform + commercial friendly open-source n8n alternative for AI workflows)",
              "score": 1,
              "created_utc": "2026-01-30 00:09:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ipjyy",
          "author": "Wrong_Country_1576",
          "text": "I love LeChat. It's brilliant and has a great personality.",
          "score": 4,
          "created_utc": "2026-01-30 01:13:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ifetz",
          "author": "ShiroCOTA",
          "text": "I ditched ChatGPT the moment I learned they massively funded Trumpâ€˜s fascist MAGA cult and switched to LeChat. Happy ever since. The only things I miss here are the excellent voice chats and the live-video chat feature to ask the AI about the stuff it sees irl.",
          "score": 8,
          "created_utc": "2026-01-30 00:18:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p7x38",
              "author": "ziplin19",
              "text": "Today i cancelled ChatGPT because they fully support Trumps fascism, i will subsribe to LeChat",
              "score": 3,
              "created_utc": "2026-01-30 23:48:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2j5ek8",
              "author": "Gamester1941",
              "text": "Same! Though I do miss the intuitive memoru feature amd tts feature (also mistral could turn up the temprature some so when I regenerate its not the samw thinf?) But those are small gripes. Love mistral",
              "score": 1,
              "created_utc": "2026-01-30 02:41:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2icacu",
          "author": "909876b4-cf8c",
          "text": "Please don't prioritize US commercial, closed-sourced, restrictive, controlling, privacy invasive operating systems. Europe is switching to Linux, please focus on that.",
          "score": 5,
          "created_utc": "2026-01-30 00:01:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iexyi",
          "author": "Hekke1969",
          "text": "Windows app ?? Hell no",
          "score": 3,
          "created_utc": "2026-01-30 00:16:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t63vi",
          "author": "destello89",
          "text": "You can add it to your desktop which works similarly to how the app would. You should give it a go.",
          "score": 1,
          "created_utc": "2026-01-31 16:16:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2u6raa",
              "author": "hartmanners",
              "text": "You mean add the iOS app to macOS?",
              "score": 1,
              "created_utc": "2026-01-31 19:10:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2inr7i",
          "author": "Equivalent-Word-7691",
          "text": "I downloaded Mistral one year ago,yet I sturggle to really support it if nayhting becuas ethe modle are just REALLY infeior  to gemini ,Open Ai adne speically Claude..like the real only good quality is it's an european company ut even chinese's model are better ...",
          "score": -2,
          "created_utc": "2026-01-30 01:03:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq5gss",
      "title": "USD prices in EU, WTF?",
      "subreddit": "MistralAI",
      "url": "https://i.redd.it/qvhynsijq9gg1.png",
      "author": "anzzax",
      "created_utc": "2026-01-29 10:53:37",
      "score": 146,
      "num_comments": 19,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qq5gss/usd_prices_in_eu_wtf/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2e1v23",
          "author": "UnnamedUA",
          "text": "https://preview.redd.it/3tukzc1sr9gg1.jpeg?width=1080&format=pjpg&auto=webp&s=5533758cbc36bd83ade26edf1e0de9008ac53935",
          "score": 56,
          "created_utc": "2026-01-29 10:57:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e36hw",
              "author": "anzzax",
              "text": "https://preview.redd.it/wjkplopqt9gg1.png?width=2492&format=png&auto=webp&s=3d7010e6360d30ad65af24d6382fc0f63321f7ed",
              "score": 7,
              "created_utc": "2026-01-29 11:08:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2etmcg",
                  "author": "FalseRegister",
                  "text": "I am on that page, and it loaded EUR for me\n\nAre your local settings correct? Try on another browser or device as well",
                  "score": 5,
                  "created_utc": "2026-01-29 13:59:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2e3r24",
                  "author": "sndrtj",
                  "text": "Ah I guess they don't default back to USD for any location that doesn't use Euro.",
                  "score": 3,
                  "created_utc": "2026-01-29 11:12:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2e3bmw",
          "author": "OwlSlow1356",
          "text": "last year was only usd, a big turnoff. if you have the account already in place, you will see upgrade only in USD just like me.  if you log out, you will see these prices EUR/USD this year, but once logged in, again just USD. i think they have many costs in USD, bunny .net CDN also has pricing in USD, sorry, i will not pay in USD to any european company! there are so many options around, I did not bother to make a new account just to see if i can upgrade in EUR!",
          "score": 9,
          "created_utc": "2026-01-29 11:09:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e3uvf",
              "author": "anzzax",
              "text": "Yeah, my account created a long ago and I was using/testing API. I checked admin area, billing settings, all possible places - there is no way to switch to EUR, they show USD prices with Poland VAT - a bit infuriating honeslty.",
              "score": 2,
              "created_utc": "2026-01-29 11:13:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o30n4vt",
                  "author": "Whole-Pressure-7396",
                  "text": "its because they prob use stripe, a cus account if paid previoysly in usd, the account wont be able to switch to eur (not sure if stripe is still like that but that was how it was a few years ago) pain in the ass when integrating in a platform in my opinion, especially if you realize this limitation at later point in time.",
                  "score": 1,
                  "created_utc": "2026-02-01 18:50:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2eb9ne",
          "author": "anzzax",
          "text": "BTW ChatGPT has adjusted pricing and local currency. Those details matter. \n\nMistral - Iâ€™d like to see you build a strong business, not only focus on research. Iâ€™m happy to stay loyal and advocate for your models.\n\nhttps://preview.redd.it/wcooytvq4agg1.png?width=2888&format=png&auto=webp&s=b320b95b010a260fb60e8f3957d4cc11d2ce68a1",
          "score": 7,
          "created_utc": "2026-01-29 12:09:46",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2e9dmh",
          "author": "crazyserb89",
          "text": "If youâ€™re using VPN could be. Iâ€™m accessing from Italy and I see EUR primarily",
          "score": 3,
          "created_utc": "2026-01-29 11:56:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ejzf8",
              "author": "stddealer",
              "text": "I think it only supports euros and USD, and if you browse from any country whose currency is not Euros, it defaults to USD.",
              "score": 2,
              "created_utc": "2026-01-29 13:06:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ercu0",
                  "author": "radek432",
                  "text": "Which makes sense, because dollar is cheap now ðŸ˜‰",
                  "score": 2,
                  "created_utc": "2026-01-29 13:47:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gf8yp",
          "author": "Dadinek",
          "text": "I took a student subscription and voluntarily in USD with a VPN because it was cheaper than from Europe. I end up paying 6$ per month",
          "score": 2,
          "created_utc": "2026-01-29 18:25:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fhxaa",
          "author": "Duedeldueb",
          "text": "Polandâ€˜s CEO thinks Zloty is it.",
          "score": 1,
          "created_utc": "2026-01-29 15:56:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hzplc",
          "author": "Outside_Professor647",
          "text": "It also writes dollars when transcribing numbersðŸ¤®",
          "score": 1,
          "created_utc": "2026-01-29 22:54:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2jx7i9",
          "author": "deegwaren",
          "text": "Try this url: https://mistral.ai/pricing",
          "score": 1,
          "created_utc": "2026-01-30 05:33:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2s5a9g",
              "author": "petersemm",
              "text": "It loads both for me but in dollars is somehow cheaper even though exhange rate is 1 eur = 1.19 $",
              "score": 1,
              "created_utc": "2026-01-31 12:52:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2tu9sj",
                  "author": "DeweyQ",
                  "text": "Mathematics. This exchange rate makes prices in US dollars cheaper. A loaf of bread that needs 1 EUR to purchase it needs less than 1 USD if each USD can buy 1.19 loaves of bread.",
                  "score": 1,
                  "created_utc": "2026-01-31 18:12:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ktgym",
          "author": "Zerr0Daay",
          "text": "If you have a VPN on it may influence it",
          "score": 1,
          "created_utc": "2026-01-30 10:09:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrw16s",
      "title": "Mistral Vibe 2.0 vs Codex 5.2 & Claude (Opus 4.5) - First Impressions",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qrw16s/mistral_vibe_20_vs_codex_52_claude_opus_45_first/",
      "author": "l_eo_",
      "created_utc": "2026-01-31 07:34:06",
      "score": 140,
      "num_comments": 36,
      "upvote_ratio": 0.96,
      "text": "Spending most of my time on Claudeâ€™s Max 20x plan (Opus 4.5) and occasionally hitting my weekly limits, I decided to revisit **Mistral Vibe 2.0** and **Le Chat** after a long break. Here are my first impressions, especially compared to Opus 4.5 and Codex CLI.\n\n---\n\n### **1. Codex CLI (5.2), Underwhelming First Impressions**\n\nI tested Codex in a structured folder with lots of context (files, scripts, and dedicated system for specific workflows that are to be followed). On first prompts and even after instructing it to analyze the **folder context**, it struggled to understand or comply and generate based on an understanding of the surrounding context. It felt less like a true CLI tool and more like a system just inlining requests without grasping the environment. Big asterix: It's also my first time trying codex and I need to explore it more, but the first results were disappointing.\n\n---\n\n### **2. Mistral Vibe 2.0**\n\nReally great first impression, I was very pleasantly surprised!\n\nIt really stood out was how thoroughly Mistral Vibe 2.0 tried to understand the context first (even without being told to do so). It didnâ€™t just jump into answering; it checked the surrounding files, analyzed available examples, and tried to understand what a good outcome would look like before starting to work.\n\nAnd I was blown away by how **fast** Mistral Vibe 2.0 is. The response generation is so quick that I canâ€™t even read along as it outputs. This is a game-changer for feedback loops. While I of course believe that Claude Opus 4.5 is currently the king for coding and complex tasks, Iâ€™ll be testing Mistral Vibe 2.0 much more for coding and general tasks to see how it performs. For everyday structured tasks, Mistralâ€™s first impression suggests it could be a **fantastic alternative and fallback system**.\n\n---\n\n### **3. Le Chat App**\nThe Le Chat app has improved significantly since I last tried it:\n* Voice input is now a thing, and itâ€™s seamless! (Claude struggles a lot with this) When I last used Le Chat, this feature didnâ€™t even exist. I tested it with a long, multi-minute transcription, and the accuracy was impressive. I did a few feedback rounds, and Mistral applied my edits smoothly. Iâ€™m not sure how they made it *that* fastâ€”**the turnaround times for voice transcription and immediate answers are incredibly impressive**.\n* Online research seemed also to work great  and possibly now on par with ChatGPT?\nIâ€™m not even sure yet what other awesome features and UX additions there are, but Iâ€™m excited to explore further.\n\n---\n\n### **4. Potential Switch?**\nGiven Mistralâ€™s speed, UX, and awesome voice transcription, etc, Iâ€™m seriously considering making Le Chat + Mistral Vibe 2.0 my daily driver for everyday tasks and as a fallback when Claudeâ€™s limits kick in (and Le Chat possibly always preferred because of the speed and the great voice transcription in multiple languages). Iâ€™ll test-drive it for a few days, and if it proves as powerful as I hope (writing this as a \"wow, just tried this\" post, so maybe lots of honey moon phase involved), I might cancel my ChatGPT subscription and make Le Chat my main driver for everyday use.\n\nIâ€™m all for EU digital sovereignty, so supporting Mistral feels like a win-win and I am incredibly happy & excited about all the progress being made.\n\n---\n\n**TL;DR:**\n\n* **First rough impressions**: Just excited to share my initial enthusiasm for Mistral Vibe 2.0 and Le Chat after revisiting them.\n* **Codex CLI** felt underwhelming as a CLI tool, especially with folder context. Not sure about code quality and similar yet (also very first impression).\n* **Mistral Vibe 2.0** impressed with speed and context handling; Iâ€™ll test it more for coding.\n* **Le Chat app** now includes voice input and shines with great transcription quality, incredible speed (game changer for back and forth loops), and online research\n* While **Opus 4.5 likely remains unmatched for coding**, Mistralâ€™s first impression suggests it could be a **fantastic alternative and fallback system** for everyday tasks.\n\n\nBig thank you to the Mistral team for all the hard work! Rooting for you big time â™¥ï¸ðŸ‡ªðŸ‡º!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qrw16s/mistral_vibe_20_vs_codex_52_claude_opus_45_first/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o2r8s37",
          "author": "l_eo_",
          "text": "Collecting new findings here as I continue testing: \n\n* I am not super happy with it being pretty hard to copy from the CLI progress. Often I need to \"reply\" to something the agents do and the UX loops around that are crucial. It's pretty bad at the moment, the UI needs a long time to react and seems to include some automatic \"copied!\" feature, that appears very late multiple times and that I don't really care about. Easy direct select -> right click -> copy would be best and should be fast and fluid. That's a \"small\" thing that makes a **huge** difference. \n* Scrolling can also be pretty sluggish (very very late start and keys like 'end' also have many seconds of delay). Possibly tracked here: https://github.com/mistralai/mistral-vibe/issues/222\n* There must be a specific system in place for interruptions? The agent reacted much faster than Claude Code Opus to a \"no\" during generation. Basically almost immediate stop\n* CLI text input seems a bit sluggish when much is happening\n* Context usage seems to go up fairly slowly. Not sure if that's a good or bad thing (testing it for some coding right now).\n* Every response / input during generation seems to be treated as an interruption (so no \"next task queuing\" like with Claude Code). Maybe there is a way to queue?",
          "score": 7,
          "created_utc": "2026-01-31 07:59:41",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2rboh5",
              "author": "l_eo_",
              "text": "I really need to stress that so many gains are to be made \"easily\" by improving the UX of the base-line elements that make up the workflow with the CLI tool. \n\nCopy & paste / reacting / replying, how fast typing is, and how responsive scrolling is are just a few facets of this.\n\nUX improvements make a huge difference if the basic quality of the model output is good enough.\n\nI will look into contributing: https://github.com/mistralai/mistral-vibe/blob/main/CONTRIBUTING.md",
              "score": 10,
              "created_utc": "2026-01-31 08:26:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2teisq",
          "author": "Mr_Finious",
          "text": "As a US citizen, I'm considering Mistral because I trust the EU's privacy protections to better protect me than those of US or Chinese companies. \n\n\n\nWe should all be clapping for companies like Mistral in the EU.",
          "score": 7,
          "created_utc": "2026-01-31 16:56:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r7wv2",
          "author": "ZapojKabel",
          "text": "I am interested at the coding using now heavily modified Gemini CLI and quite happy with, but rather use European tool. Does mistral have api key pay as you go and picture generate?",
          "score": 5,
          "created_utc": "2026-01-31 07:51:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2r9lfq",
              "author": "l_eo_",
              "text": "Interesting, in what way did you modify the Gemini CLI? \n\nI must say, that I was always very unhappy with Gemini output regarding code and it often seeming to \"fight\" me / gaslight me or somehow trying to avoid following instructions. Especially in the last weeks instruction following was pretty much non-existent for the few tests I did and I have been avoiding it ever since. \nBut I also mostly used the web interface so far, so maybe it's a different story with the CLI. \n\nI would have very much recommended Claude Code with Opus 4.5 and used to be *insane* around release / December, but has deteriorated massively in terms of quality the last few weeks. Possibly due to a switch from Google GPUs to the ones they developed with AWS, but not sure.  \nStill, it is very much worth a try if you go for the 5x or 20x max plans (they are very worth it). API pricing would be incredibly expensive (easy to burn through 20-80 bucks an hour). \n\nRegarding your questions: \nYes, API key pay as you go exists and I think the subscription now also included some vague notion of \"full day coding\"? \n\nPicture generate would likely mean an additional model, but that should be easy to integrate into your workflow (e.g. continue using Gemini's very powerful nano-banana-pro or flux).\n\nGreat to hear that you care about European tools!",
              "score": 4,
              "created_utc": "2026-01-31 08:07:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2rngmy",
                  "author": "ZapojKabel",
                  "text": "Well I have my boss I call him Nexus he delegate my task to other agents ( I use mostly three for design, code and advertising expert). Every plan I make all 3 discuss how to do and should work, when it is done the 4 agent step in it is called Sentil \"devil advocate\" a trying to find error in there reasoning and aks question \"if...\". When there settle and I agree, Nexus make track list in conductor with plan how to setup. Also use RAG. I am not coder but make app for my e-shop. https://harmony.nonchalant.cz and also now making app for manager stuff on my eshop and dashboard for better statistic on sale etc.\nEdit: also Avery agent must use TOT protocol",
                  "score": 5,
                  "created_utc": "2026-01-31 10:19:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ul5uq",
              "author": "vienna_city_skater",
              "text": "If you have Le Chat Pro you get Devstral included.",
              "score": 1,
              "created_utc": "2026-01-31 20:20:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2recbg",
          "author": "EzioO14",
          "text": "I use le chat pro for all normal chat questions and I have claude max for coding",
          "score": 5,
          "created_utc": "2026-01-31 08:51:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2reopc",
              "author": "l_eo_",
              "text": "That's my plan as well :)\nI love the snappyness of Le Chat. \n\nIs Le Chat strict enough with sources? So doesn't hallucinate if no information found via web search?",
              "score": 2,
              "created_utc": "2026-01-31 08:55:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2rer8d",
                  "author": "EzioO14",
                  "text": "I find it gets better, I was very disappointed at first, last month when I started but now I find the answers better and better",
                  "score": 2,
                  "created_utc": "2026-01-31 08:55:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2r8a66",
          "author": "Bato_Shi",
          "text": "Only thing i noticed with Vibe is that sometimes it falls into infinite loops, like gemini 3 some months ago",
          "score": 4,
          "created_utc": "2026-01-31 07:55:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2r9n86",
              "author": "l_eo_",
              "text": "Should be easy to break it out of though, right? \nHow often does it happen?",
              "score": 1,
              "created_utc": "2026-01-31 08:07:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2syc21",
                  "author": "kiwibonga",
                  "text": "An example I had yesterday (Devstral Small 2 Q4) is it found that a regular expression wasn't capturing strings properly in a test, so it tried to fix the pattern, but it turned out to be a basic escaping issue rather than an error in the regex. It didn't realize that, so it added more and more characters to the pattern until it was huge -- without ever thinking \"is this overkill? That looks weirdly huge\" even though the purpose of the code was just to make sure a line starts with the word \"AGENT:\".\n\nThis is something that could have been autonomously fixed with an orchestrator that includes failure analysis, sees the blind spot, and proposes an alternate approach before launching another attempt. A reasoning model might also have a much easier time course-correcting.\n\nI was using the CLI by itself so I just said \"try to simplify the regular expression by writing it from scratch, and if it still fails, just write a parser\" -- it failed fast and wrote a parser, then got back to work.",
                  "score": 1,
                  "created_utc": "2026-01-31 15:39:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rpkx3",
              "author": "Gen5nake",
              "text": "I also had this issue once",
              "score": 1,
              "created_utc": "2026-01-31 10:39:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ra7pr",
          "author": "theschiffer",
          "text": "Very interesting review. What does your day-to-day workload look like and in what ways do you generally use AI?",
          "score": 2,
          "created_utc": "2026-01-31 08:13:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rarwz",
              "author": "l_eo_",
              "text": "Workload in terms of volume of usage? \n6-14 hours of multiple terminal tabs with claude processes. \nAI is used as a partner both for planning as well as for execution. Often planning and getting everything prepared for implementation takes up most of the time (e.g. 4 hours planning and adapting and architecture work, 2 hours implementing and adapting / fixing), but it depends a bit on what phase a project is in. \n\nDoes this answer your question fully?",
              "score": 3,
              "created_utc": "2026-01-31 08:18:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2tmt2r",
                  "author": "theschiffer",
                  "text": "Nice. So essentially, youâ€™re a software developer/architect? Thatâ€™s quite an intensive level of use.",
                  "score": 1,
                  "created_utc": "2026-01-31 17:36:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2rae0d",
          "author": "Hot_Bake_4921",
          "text": "Does Le chat still use mistral medium 3.1?",
          "score": 2,
          "created_utc": "2026-01-31 08:14:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rcns6",
              "author": "l_eo_",
              "text": "I am trying to find some information about this, but this proves to be surprisingly hard? Nothing in the change logs and most Reddit discussions about this seem to be from quite long ago.",
              "score": 2,
              "created_utc": "2026-01-31 08:36:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tqeke",
          "author": "Sorry_Role_1701",
          "text": "What Iâ€™m noticing with newer models like Mistral isnâ€™t about benchmarks anymore.\n\nThe real difference is **i**nteration speed, how fast you can refine an idea without restarting context every time.\n\nIn workflow-heavy tasks, that matters more than raw output quality.",
          "score": 2,
          "created_utc": "2026-01-31 17:54:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wlr2d",
              "author": "mWo12",
              "text": "Off course they are not about benchmarks, as they have nothing to show.",
              "score": 2,
              "created_utc": "2026-02-01 02:55:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2r7fxx",
          "author": "l_eo_",
          "text": "As I wrote, I am completely newly back again with Mistral vibe. \nAny optimizations / settings I should immediately go for?",
          "score": 1,
          "created_utc": "2026-01-31 07:47:20",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2r85ap",
          "author": "stjepano85",
          "text": "It is very good for agentic development. It has some problems with recursive algorithms and it can go crazy when his context is large (this can be solved by limiting context in vibe configuration).",
          "score": 1,
          "created_utc": "2026-01-31 07:53:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rayhx",
              "author": "l_eo_",
              "text": "Interesting! \nWhat level do you recommend limiting context to?",
              "score": 1,
              "created_utc": "2026-01-31 08:19:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2rcdgm",
                  "author": "stjepano85",
                  "text": "128k should be good. I noticed at 70+% of the default 200k its performance drops significantly.",
                  "score": 2,
                  "created_utc": "2026-01-31 08:33:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2rcps1",
          "author": "nycigo",
          "text": "For voice input, press Windows + H; this will automatically transcribe your microphone input if you are using Windows 11.",
          "score": 1,
          "created_utc": "2026-01-31 08:36:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rcu1q",
              "author": "l_eo_",
              "text": ":O \n\nIn the CLI for mistral vibe?",
              "score": 2,
              "created_utc": "2026-01-31 08:37:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2rhxqy",
                  "author": "nycigo",
                  "text": "Anywhere on Windows, anywhere your mouse is located",
                  "score": 1,
                  "created_utc": "2026-01-31 09:26:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2wll72",
          "author": "mWo12",
          "text": "I tested Mistral Vibe 2.0, but I found it so slow, and I'm not talking about token generation. Just the CLI interface is so slow and sluggish. Maybe because it is all python, unlike other agents.",
          "score": 1,
          "created_utc": "2026-02-01 02:54:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xo0zs",
              "author": "l_eo_",
              "text": "100% and this is my biggest issue with it right now and a real blocker. \nYou can see the kind of potential it has when starting a new session, but it gets bogged down fast. \nFor me personally these would a \"don't release before fixed\".  \n\nBut I am also very optimistic that this will be improved soon, because relatively speaking, these rendering issues should be quite straightforward to fix, so I really hope Mistral will make them a priority.",
              "score": 1,
              "created_utc": "2026-02-01 07:36:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o31bvsf",
          "author": "BastFX",
          "text": "Hey ! Thanks for this detailed review ðŸ‘Œ\nI never used vibe-code tools like this one, but i'm following news about all that world.\nThe most famous tool Claude Code on this category needs Claude.md file, I would like to know if Mistral need something similar to work properly ? What structure build in this file ? I checked the official documentation but any information about that, or I missed it ...",
          "score": 1,
          "created_utc": "2026-02-01 20:48:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqzac4",
      "title": "Switching to Mistral?",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qqzac4/switching_to_mistral/",
      "author": "Absjalon",
      "created_utc": "2026-01-30 07:52:59",
      "score": 92,
      "num_comments": 28,
      "upvote_ratio": 0.98,
      "text": "Hi Mistral users,\n\nI am strongly considering switching my OpenAI subscription to Mistral.  I'm  happy with OpenAIs products,  but for political and GDPR reasons I'm ready to switch. Even if it means less optimal product. \n\nI've tried the free Mistral version for a while now and I am pretty happy about it, but it's not quite at the level of the paid OpenAI models. \n\nCan someone share their experience with the difference between the paid Mistral and OpenAI and how to optimize/personalize Mistrals output?\n\nI work both with API interactions and the chat interface \n\nThank you",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qqzac4/switching_to_mistral/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o2kgooi",
          "author": "thedisturbedflask",
          "text": "The Mistral models are quite capable but ive found you'll have to really work on the instructions and prompt to get the output your looking for.\n\n\nMy process was in defining a starting prompt and then tweaking it with a control question until it reached a point i was happy with. The answers arent deterministic because of how llms work but it helps to see the general kind of response.\n\n\nI saved these as agents in chat which works quite well.\n\n\nFrom the dev perspective i haven't quite been able to have it refer to an instruction file consistently but might just be missing something.",
          "score": 15,
          "created_utc": "2026-01-30 08:12:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kiyuj",
              "author": "Absjalon",
              "text": "Thank you.   \n  \nSo in Mistral chat, you build an agent and then call it in different chats? e.g. an agent that helps to formulate emails, but stays close to your draft and your wording?\n\nAs opposed to making a project and then giving special instructions for project?",
              "score": 4,
              "created_utc": "2026-01-30 08:33:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2kjjpe",
                  "author": "TheMrLexis",
                  "text": "In Le Chat if I remember well, you can call an agent in the same chat by writting \"@\\[Agent Name\\]\"",
                  "score": 4,
                  "created_utc": "2026-01-30 08:38:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2kkv4s",
                  "author": "thedisturbedflask",
                  "text": "That's correct for the agents, i refine it to 'think' the way id like for general interactions or make it more specifically useful like the email example.\n\n\nI think you definitely can define the instructions for an agent to include project ownership, requirements, milestones, etc and skill sets so that youd be able to ask it to complete the project goals etc\n\n\nIn my case for a dev project using zed.dev editor I do that and spend a lot of time back and forth with the model defining the scope and detail then have it output a specific plan.md file with a checklist.\n\n\nI include it as context and ask it to implement the plan, then inevitably fix what it doesnt quite get right, makes it easier to predict and test the output",
                  "score": 2,
                  "created_utc": "2026-01-30 08:50:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2l8jh3",
          "author": "Vontaxis",
          "text": "I have a mistral subscription just to support them but to be honest it is nowhere as good as Claude or ChatGPT.\n\nNaturally, it depends on what you're using it for but even for simpler things I noticed that prompt adherence is at times rather bad.",
          "score": 6,
          "created_utc": "2026-01-30 12:11:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kqv4z",
          "author": "NullSmoke",
          "text": "Yeah... new 4o scare going on over there (including 4.1 this time if I understand it correctly?)...\n\nJumped over when the whole mess became unbearable, and is very happy with it. As for the diff between paid and free, it's rate limits basically. You can test it out in free and have a decent handle on what you can expect in pro.\n\nI see that agents are being discussed down here, basically CustomGPTs if talking OpenAIspeech. You can find several guides over at r/Nefhis_Lumen_Lab that can help you get started :-)",
          "score": 4,
          "created_utc": "2026-01-30 09:45:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mmwih",
          "author": "LewdManoSaurus",
          "text": "I tried a subscription last October and while it was okay, I had to do a lot of correcting, and the amount of hallucination was seriously a deal breaker. If you just use it sparingly it'll probably be fine, but in my experience It's nowhere close to some of the bigger models. The agent customization is an amazing feature, but the other issues are so frequent that these days I just stick to free tiers of other services. I only used Mistral for generative writing.",
          "score": 4,
          "created_utc": "2026-01-30 16:31:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pn7eo",
          "author": "LoadZealousideal7778",
          "text": "I like it but probably for a weird reason. For me, their models sit at the junction between capable and a useful but fundamentally stupid multi tool. Smart enough to do work, not smart enogh to cognitively offload to.\n\nYou can't just barf in a typo riddled, half formed thought and get what you wanted most of the time like with Claude Opus. Bit more manual.",
          "score": 5,
          "created_utc": "2026-01-31 01:13:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rhkj9",
              "author": "Absjalon",
              "text": "I understand this and have a similar experience",
              "score": 1,
              "created_utc": "2026-01-31 09:22:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lko4e",
          "author": "mythrowaway4DPP",
          "text": "Mistral *is* not on the same level as the top models ChatGPT, Gemini or Claude.  \nBeing an European engine, GDPR, and less puritan censorship is nice.\n\nCapabilities wise, think ChatGPT 4.1, maybe better.\n\nPrompting needs to be more precise, but it is doable.\n\nThe libraries are a nice idea, collections of documents to attach to any chat.\n\nOverall, I am not missing a lot using mistral, and openAI isn't getting my money anymore.",
          "score": 8,
          "created_utc": "2026-01-30 13:27:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mlk85",
              "author": "alexgduarte",
              "text": "That's because you've never used GPT-5.2-Pro.",
              "score": -3,
              "created_utc": "2026-01-30 16:25:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mm1rw",
                  "author": "mythrowaway4DPP",
                  "text": "Of course I did. I also said that mistral is not there.",
                  "score": 6,
                  "created_utc": "2026-01-30 16:27:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2o897h",
          "author": "Komgoroth",
          "text": "I used it in the past. Then I switched to ChatGPT for a while. When my Chat GPT subscription was about to expire I subscribed again to Mistral.\n\nI then had some health issues and when I got my report from the doctor, I used it on Mistral to make a summary of what is happening( it was all perfect other than some benign findings) and it made up certain findings which indicated severe issues.\n\nI panicked as the doctor did not mentioned them. I also did not see them. Then asked the AI to tell me where did it find the issue and it apologized.\n\nTried it again and it did the same horrible mistake. Chat gpt did not.\n\nExample two. I asked both Mistral and ChatGPT to make me the shortest roadtrip using highway only from point a to point b and to tell me what vignettes I need to buy and what's the expected time and how often and where I should take breaks.\n\n\nChat gpt did it perfectly and I confirmed myself on maps etc.\nMistral missed several countries when it comes to vignettes, miscalculated the time by more than 30% and suggested that no breaks are needed for the drive( on a 14 hour long drive).\n\nI unsubscribed and continued with Gemini.\n\n\nI love that it is an European company and I'll support them when they get better but I cannot use it as is.",
          "score": 3,
          "created_utc": "2026-01-30 20:49:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kp7dc",
          "author": "sam-watterson",
          "text": "I started switching it, using vibe for day-to-day usage.",
          "score": 2,
          "created_utc": "2026-01-30 09:29:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2omirx",
          "author": "theAbominablySlowMan",
          "text": "Even before all this US shite started I was scolding everyone I heard using other AI, the data privacy approach of US Vs EU countries is just not compatible, and the level of detail of your personality you give these things is just scary in the wrong hands.Â \n\n\nHonestly I suggest just ditching and making the switch for 3 months and figuring it out yourself, like all software we get tied to what we know. Without knowing what areas you rely on it for I can't be specific but it's not a black and white openai being better, I prefer mistrals response on a lot of more science based topics for exampleÂ ",
          "score": 2,
          "created_utc": "2026-01-30 21:57:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rhgzf",
              "author": "Absjalon",
              "text": "Yes.  I've noticed Mistral gives really good answers on Statistical issues (explaining concepts). In this area, I think it is on par with chatGPT 5.2",
              "score": 1,
              "created_utc": "2026-01-31 09:21:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rac2w",
          "author": "Born-Yoghurt-401",
          "text": "Iâ€˜ve been using LeChat free over the last year to track progress and get medicinal advice about a close relatvies glioblastoma stage 4 brain tumor. I shared overall progress including CT and MRI scans, mental health patterns and in the final weeks palliative and necrotic wound care details. LeChat was very helpful in collecting and aggregating health data and putting many of my questions into context. I had no issues with hallucinations or factual errors and felt in good hands. I also would never have shared any of those details with a US based AI solution.",
          "score": 2,
          "created_utc": "2026-01-31 08:14:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rryl0",
          "author": "WitnessEarly7584",
          "text": "I got a Pro subscription for Mistral for free and tried to input some of my ChatGPT/Gemini prompts, which I use for work. What can I say? It is totally unusable. One prompt even caused a recursive loop. How are you guys using it?",
          "score": 2,
          "created_utc": "2026-01-31 11:01:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rzm87",
          "author": "crazyserb89",
          "text": "I'm also on OpenAI searching for an alternative and checking the Mistral. Gave it a shot several times, but it seems it's not there yet to compete with the big ones. It feels unpolished, lacking some fundamental features, and overall seems like a Beta product. I hope they gonna improve it in future and therefore position themselves better on the market though.",
          "score": 2,
          "created_utc": "2026-01-31 12:08:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2l3kqp",
          "author": "MikadinShinjuk",
          "text": "I switched to kagi, is not European but is way better than all the other main services and is very flexible in terms of choosing the model",
          "score": 1,
          "created_utc": "2026-01-30 11:34:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ss499",
          "author": "fonceka",
          "text": "Mistral strategy is to develop vertical models, so they have a bunch of specific models: codestral (coding), devstral (open-source model for coding agents), voxtral (speech2text), mistral small (enterprise ready), mistral medium, mistral large, ministral, magistral (multilingual)â€¦ I have been on the Pro version for one year on, but I have also a paid subscription to Gemini. I have already dropped the OpenAI subscription, and consider dropping the Anthropic one also. I do not use the API anymore since my focus have shifted. Overall I find Mistral very useful, when context is adequate. You must really work on curating your context window neatly.",
          "score": 1,
          "created_utc": "2026-01-31 15:07:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ue2ml",
          "author": "Green-LaManche",
          "text": "I used le chat - in very specific area which difficult to find someone knowledgeable: I am pretty happy with the answer either when asked specifically about dealing breakdowns in highly sophisticated areas or details of very specific historical figures.\nI would say I am much happier then with copilot",
          "score": 1,
          "created_utc": "2026-01-31 19:45:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lix37",
          "author": "officialexaking",
          "text": "You said for 'political reasons'. What do you mean by that? If it is because you want your data to be stay in the EU and not handled by non-US tech companies then you are wrong with Mistral. All your requests (chats) are routed through Microsoft/Google and Cerebras unless you haven't concluded a personal enterprise contract with them.",
          "score": 0,
          "created_utc": "2026-01-30 13:17:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mjxpx",
              "author": "Absjalon",
              "text": "Very important information you bring to the table here.  My concern is  data wise, but also I want my money to support Europe.\n\nI will see if I can find more information about this",
              "score": 4,
              "created_utc": "2026-01-30 16:17:54",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2mbua7",
              "author": "theKurganDK",
              "text": "Could you elaborate please? Routed?",
              "score": 2,
              "created_utc": "2026-01-30 15:41:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rkore",
                  "author": "officialexaking",
                  "text": "Here is everything you need to know about it:\nhttps://www.xprivo.com/blog/en/mistral-is-not-a-european-alternative/",
                  "score": 1,
                  "created_utc": "2026-01-31 09:52:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qpa554",
      "title": "Payment in EUR more expensive than in USD?",
      "subreddit": "MistralAI",
      "url": "https://v.redd.it/9xg4e9dz33gg1",
      "author": "d4v1d_dp",
      "created_utc": "2026-01-28 12:34:01",
      "score": 73,
      "num_comments": 18,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qpa554/payment_in_eur_more_expensive_than_in_usd/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o27drrt",
          "author": "Axiom05",
          "text": "The price in US dollars never includes VAT, unlike the price in euros.",
          "score": 74,
          "created_utc": "2026-01-28 12:44:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27kcj7",
              "author": "Jazzlike-Spare3425",
              "text": "Yes, this is it. You can see it by the asterisk. If you scroll down to the bottom of the table that compares the plan, just over the FAQ section, for the USD version it will say \"excluding taxes\" and for the Euro price it will say \"including taxes\"",
              "score": 14,
              "created_utc": "2026-01-28 13:24:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o29i11u",
              "author": "Patrick_Barababord",
              "text": "Maybe, but $1 = 0,83â‚¬ also.  \nSo $14.99 = 12.44â‚¬ ...and 12.44 + 20% VAT = 14.9â‚¬.",
              "score": 3,
              "created_utc": "2026-01-28 18:42:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2daedd",
                  "author": "aoc145134",
                  "text": "I don't think you'll get a reasonable comparison by using a conversion rate based on a four-year low for the dollar. Even a week ago it would have been $1 = 0.854 giving 15.36 Euros.",
                  "score": 1,
                  "created_utc": "2026-01-29 06:49:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27f4ys",
              "author": "OwlSlow1356",
              "text": "paid one month in USD last year, the only currency available then although i am in europe but nonEUR country, and they deducted the VAT from USD price when providing a VAT number, second month they charged me full USD price, asked why if first month VAT was deducted, never received any answer, cancelled and good bye!",
              "score": 0,
              "created_utc": "2026-01-28 12:53:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o284ehb",
          "author": "EzioO14",
          "text": "Itâ€™s just a tax matter, not an advantage to U.S",
          "score": 15,
          "created_utc": "2026-01-28 15:06:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27cktf",
          "author": "Little_Protection434",
          "text": "That seems to be the case. I just checked the exchange rate and based on that it should indeed be the opposite.",
          "score": 9,
          "created_utc": "2026-01-28 12:37:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o282bu9",
              "author": "ComeOnIWantUsername",
              "text": "Just read the pricing as a whole, not just one part. For USD it's \"excluding taxes\". For EUR \"including taxes\".",
              "score": 9,
              "created_utc": "2026-01-28 14:56:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o282og9",
                  "author": "Little_Protection434",
                  "text": "That makes sense. Thanks!",
                  "score": 2,
                  "created_utc": "2026-01-28 14:58:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28shtk",
          "author": "TheBl4ckFox",
          "text": "Itâ€™s always excluding tax for US customers and including VAT for EU.",
          "score": 3,
          "created_utc": "2026-01-28 16:52:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27teac",
          "author": "Nokushi",
          "text": "prices were aligned before, but were all without VAT included, which is uncommon in France\n\ni guess they now show EUR prices VAT included",
          "score": 2,
          "created_utc": "2026-01-28 14:12:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o280wet",
              "author": "sndrtj",
              "text": "And since VAT rates depend on the country of the _purchaser_, not the seller, exact amounts may change depending on where you are located.",
              "score": 4,
              "created_utc": "2026-01-28 14:49:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o286pfw",
          "author": "bluepuma77",
          "text": "Just saw that too, was irritated, it seems sooo stupid what they are doing with their pricing page.  \n  \n1. It's \"$15 with \\*\".   \n2. What's a \\*, okay pages down \\* means it's without tax.   \n3. Then I switch to Eur and it's \"â‚¬18 with \\*\"\n\nWho would have thought that the meaning of the \\*, many pages down, would silently change.\n\nI highly recommend to not hide the \\* so far down. And maybe use \\*1 and \\*2 or something.",
          "score": 1,
          "created_utc": "2026-01-28 15:17:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ecvg4",
              "author": "Fisherman-63",
              "text": "C'est notÃ© ! Will improve our page soon :)",
              "score": 2,
              "created_utc": "2026-01-29 12:21:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27txik",
          "author": "Far-Reaction-1980",
          "text": "To this day I don't get Mistrals pricing",
          "score": -1,
          "created_utc": "2026-01-28 14:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27htt4",
          "author": "Basiliscus219",
          "text": "Usually the prices are set based on the local purchase power. In poorer countries the price is lower.",
          "score": -4,
          "created_utc": "2026-01-28 13:09:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnnieh",
      "title": "Devstrale 2 > other Chinese AIs like DeepSeek etc",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qnnieh/devstrale_2_other_chinese_ais_like_deepseek_etc/",
      "author": "nycigo",
      "created_utc": "2026-01-26 18:00:38",
      "score": 54,
      "num_comments": 30,
      "upvote_ratio": 0.96,
      "text": "Why is nobody talking about Devstrale 2 in the same way as GLM 4.7 Deepseek and Minimax when the AI â€‹â€‹is in the top 6 on OpenRouter in the best programming AI category, ahead of all the other Chinese models and with a damn free API?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qnnieh/devstrale_2_other_chinese_ais_like_deepseek_etc/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1v1ilu",
          "author": "SourceCodeplz",
          "text": "Because the Chinese have an online army on reddit and they promote it heavily.  \nBut GLM, Deepseek and Minimax are really good actually, not like from Anthropic, but fine.",
          "score": 23,
          "created_utc": "2026-01-26 18:10:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1v1l3g",
          "author": "cosimoiaia",
          "text": "Hate.\n\nDevstral is a superb model, even the small-24b running locally is better than all the other open weights.\n\nBut if they start to admit that the privacy/consumer first policies actually don't block progress completely and that the EU can, and did, produce SOTA models for their size, their delusions will break and they'll have a panic attack.\n\nAt the WEF they openly admitted that they wanted only the US to be players in the AI field and that they should do anything to block progress for everyone else.",
          "score": 22,
          "created_utc": "2026-01-26 18:11:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22wo60",
              "author": "robogame_dev",
              "text": "    even the small-24b running locally is better than all the other open weights.\n\nFor devstral-2-small to beat all open weights it would have to beat devstral-2... \n\nI've got 60 million tokens through Devstral 2, it's a great model - is it better than every other open weight model at everything, even the ones that are 10x is param count? no, definitely not.",
              "score": 1,
              "created_utc": "2026-01-27 20:08:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o23105z",
                  "author": "cosimoiaia",
                  "text": "Well yes, I meant to compare size to size of course, I could have phrased It better. \n\nAlthough I have to say the ones with 10x the params are not mind-blowing better for me, most of the times they still have the same pitfalls, they just go a bit further, so the convenience and efficiency of Devstral remain unbeatable for me.",
                  "score": 1,
                  "created_utc": "2026-01-27 20:28:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1vxfe9",
          "author": "tisDDM",
          "text": "I did a few tests with devstral 2 (small) and both are performing very good in agentic coding. I did a post here [https://www.reddit.com/r/opencodeCLI/comments/1qlqj0q/benchmarking\\_with\\_opencode\\_opuscodexgemini\\_flash/](https://www.reddit.com/r/opencodeCLI/comments/1qlqj0q/benchmarking_with_opencode_opuscodexgemini_flash/) about benchmarking and said a few things about Devstral as well (but did not include the Devstral 2 results) because it is part of my subagent harness project (not published yet) - where I tried to use Devstral 2 as intelligent worker nodes. \n\nI found my results impressing. Both Devstral 2 Models were fully able to run the test suite. Deepseek 3.2 and Kimi K2  and Grok Fast showed a lot of issues with following agentic tasks.\n\nBut in case you ask me why I am not using devstral 2 for coding? It is far behind Opus and Codex. Not in quality of code. Behind in understanding and following a humans complex task. which both of the big two easily can manage. But this might be an issue of Reasonig.",
          "score": 4,
          "created_utc": "2026-01-26 20:27:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xm32o",
              "author": "Historical_Roll_2974",
              "text": "How did you get Devstral 2 Small to work on OpenCode? When I try to use it with OpenCode I get an error where the filepath is null and the message is null with the tools API? Thanks",
              "score": 1,
              "created_utc": "2026-01-27 01:19:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1z3bx1",
                  "author": "tisDDM",
                  "text": "I got Devstral 2 Small working as a subagent for my benchmark, that was very cool - but still a little bit flaky depending on prompting. I used Devstral 2 ( the big one) in Zed but I think it shall work in Opencode as well. I will give it a try again later.\n\nhttps://preview.redd.it/xel4553hrvfg1.png?width=1404&format=png&auto=webp&s=63299ff43b4cafade0e9c046ac8bf38c132d1a6d\n\nI gave it a try. It works OOB with the Mistral-API and an API-Key. It's free.",
                  "score": 2,
                  "created_utc": "2026-01-27 06:54:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1v1sd5",
          "author": "Front_Eagle739",
          "text": "Well, its slow to run locally and while decent at coding its less flexible, mostly from the lack of ability to turn on high thinking. It's a nice model and it's got it's niche but glm 4.7 is usually better and more flexible even quantised to a similar size as devstral. It is a useful thing and I'm trying to use it more if only to support a european company but I think it kind of misses the benefit of being the only big dense model that will fit on a local 128GB machine (i.e. being smarter than anything comparable in size) due to said lack of thinking. For a really really smart model I could run locally I would be willing to wait but because it doesn't think it is not actually smarter than a q2 quantised glm 4.7 that also fits on my machine and it's slower.\n\nThats my take at the moment, I'm trying to explore it more, see if I can get more out of it.\n\nEdit: So... Fun fact! You can actually make devstral 2 123B reason! Accidently had a reasoning forcing jinja template on for another model when I started testing the mlx version of this thing with a couple of reasoning effot = extra high statements in my system prompt because I really wanted more reasoning out of the last model I was using and havving forgotten about that tried devstral 2 and got 2 minutes of reasoning before it answered my test question.\n\nI shall be doing a great deal of further testing.\n\nEdit 2 Devstral 24B also reasons if you set it to do so in the jinja.\n\nTurns out they are both hybrid reasoners if you put {%- set reasoning\\_content = 'High' %} in the jinja. Nice clean logical reasoning as well. That's actually fixed my main issue with these models, sometimes you just really need that extra consistency.",
          "score": 7,
          "created_utc": "2026-01-26 18:12:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xjluk",
              "author": "Holiday_Purpose_3166",
              "text": "https://preview.redd.it/tqly3ygqksfg1.jpeg?width=4096&format=pjpg&auto=webp&s=8f21f93e021bfb6b05fab7706f02b541c4bf56af\n\nHaving spent billions of tokens last year in coding between different models under the sun, Devstral models are indeed excellent for what they are.\n\nl take the benchmarks as a guide and test them in my prod, and I find ironic how \"behind\" they visually look on the charts. Anecdotally, Devstral Small 2 repeatedly beats GPT-OSS-120B on my codebases despite being so far apart on benchmarks.\n\nGLM 4.7 Flash on the other hand is also excellent and kicks the 120B on the same stuff I'm working (ML, financial algos, Rust, NextJS, etc), but Devstral Small 2 has more eye to detail where GLM left some issues I had \"as is\".\n\nHowever, GLM 4.7 Flash is broadly smarter and more update to date on greater schemes compared to Devstral but has less knack on repo work. Like them both, but I lean towards Devstral over the fact it has more enterprise grade efficiency. Could be bias, but I like minimalistic response, and it can be extensively detailed when require (docs, blueprints >1k lines).\n\nAnother important note that resonates with my experience, the GPT-OSS models have large response variance where Devstral is more deterministic and is consistent on every round, and this reflected on SWE-Rebench charts from Ibragim.\n\nI find this variance inconvenient for coding despite tighter sampling as repeated, personal tests yielded different results.\n\nI haven't used GLM 4.7 Flash long enough to detect that but generally the apple doesn't fall far from tree if it follows the grand 4.7.",
              "score": 3,
              "created_utc": "2026-01-27 01:05:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1v95xy",
              "author": "SourceCodeplz",
              "text": "Yeah. Thinking adds a LOT o value, even though sometimes it can go in circles.",
              "score": 3,
              "created_utc": "2026-01-26 18:42:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1vafxw",
                  "author": "Front_Eagle739",
                  "text": "Yeah it really does. I get the value of a fast nonthinking model for code completion, i get the value of a slow reasoning model thats very coherent and intelligent for its size. I struggle to get the purpose of a slow non thinking model that maxes out my memory. the 24B devstral I do understand, I have more memory than would force me to go down to that size but if I was on 24GB/32GB That one I could see myself using. The 123B really wants reasoning to be the best possible answer I can get for 128GB memory option.",
                  "score": 1,
                  "created_utc": "2026-01-26 18:48:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1wlad8",
              "author": "cosimoiaia",
              "text": "Have you tried it with vibe? I've been using it quite a lot, for fairly complex tasks and, so far, it never failed, even with navigating in decent sized projects and it's FAST. Btw you need a lot less than 128GB to run it at full context, 62GB will leave you room to spare.\n\nAlso, I am very doubtful that a Q2 model of any size can do well coding tasks since they're very very quantization sensitive, doesn't matter the amount of thinking.\n\nEdit: this seems like an example of classic casting shadows disguised as comment. I hate worthless propaganda.",
              "score": 1,
              "created_utc": "2026-01-26 22:13:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1wmuiu",
                  "author": "Front_Eagle739",
                  "text": "Unsloth dynamic quants are a wonderful thing. I will admit the mlx 2 bit quants are fairly useless but the unsloth iq2m glm 4.7? That one works great. The minimax ones are worse though, not quite sure why but different models seem to take better or worse to dynamic quants and glm is the best ive seen. Anyway.\n\n\nNo i havent tried vibe with it. Tried it in claude code but I'll give vibe a go. Makes sense it would be optimised for its own harness.\n\n\nWont be fast though at least not the big one unless i go api, i get about 5 token/second with devstral 123 vs 60 with gpt oss 120 or 20 ish with glm. As i said, worth it if its good enough but it has to be consistent enough to not want to retry things a lot. Will give it a go in vibe.\n\n\nEdit. Thanks for declaring my opinions are worthless propoganda. Appreciate it. Really makes a man feel like he's in a good useful discussion. Honestly.",
                  "score": 1,
                  "created_utc": "2026-01-26 22:20:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ylw16",
          "author": "Waste-Intention-2806",
          "text": "Because it's a dense model. While minimax and reaped glm 4.7 r MOE models and most of us can run these at least 4-11 tokens per second. While devstral 2 was running at .7 to 1 token per second for me on i9 and 16gb rtx 4070 ti with 128 gb ram",
          "score": 2,
          "created_utc": "2026-01-27 04:44:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2gdo5u",
          "author": "Costello173",
          "text": "Like said before the Chinese have reddit Armies and Mistral is French my friend",
          "score": 2,
          "created_utc": "2026-01-29 18:18:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ggcfj",
              "author": "nycigo",
              "text": "It's a shame we don't have the same thing in Europe, haha. With the current European propaganda on social media, you'd hope some guys would get involved. I imagine it works just as well in China because it's the state that funds it, but oh well...",
              "score": 1,
              "created_utc": "2026-01-29 18:30:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2gt0fm",
                  "author": "Costello173",
                  "text": "Right? Even compared to American brands they fall short on marketing Europeans overall seem to take longer to get the ball rolling but when they do their usually onto something. China and the US were just good at giving up good ideas.",
                  "score": 2,
                  "created_utc": "2026-01-29 19:28:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ioree",
          "author": "Equivalent-Word-7691",
          "text": "deepseek, kimi ,GLM let people have a free ride without paying ,I a gulty to us more chiens e model rather than msitral becasue I do not have real limits with kimi or deepseek ,and I I want quality I use Claude or gemini",
          "score": 1,
          "created_utc": "2026-01-30 01:08:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t3zp5",
          "author": "stddealer",
          "text": "It's a dense model so it's not as fast compared to MoE models such as GLM 4.7. this is a big thing when it comes to running models locally. Though GLM requires thinking to function properly, which makes it slow in other ways.",
          "score": 1,
          "created_utc": "2026-01-31 16:06:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2te53z",
              "author": "nycigo",
              "text": "It's super fast even on OpenRouter, man.",
              "score": 1,
              "created_utc": "2026-01-31 16:54:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2tzaiy",
                  "author": "stddealer",
                  "text": "Yes I was talking about running it locally. For api services it doesn't matter as much though it can affect the pricing.",
                  "score": 1,
                  "created_utc": "2026-01-31 18:35:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1v7wxo",
          "author": "Marciplan",
          "text": "It really isn't on par with GLM 4.7 though.",
          "score": 2,
          "created_utc": "2026-01-26 18:37:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vgpmv",
              "author": "kiwibonga",
              "text": "For agentic coding though, Devstral Small 2 scores 10 pts higher than GLM-4.7-Flash on SWEBench. The GLM team completely omitted it from their benchmarks, maybe because it's not MoE.",
              "score": 6,
              "created_utc": "2026-01-26 19:14:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vieoc",
          "author": "pinmux",
          "text": "I think Mistral offering their API for devstral-2 and devstral-small-2 for free is actually hurting adoption by inference providers and hence users don't know about it.\n\nIn my brief experience trying devstral-small-2, it's quite good.  I don't have beefy enough hardware locally to run it at a reasonable speed and last I checked the only cloud inference providers offering the devstral-2 models will train on your data (Mistral included for their consumer offerings).  On OpenRouter you get Mistral or Chutes, that's it.\n\nI'm hoping some cloud inference providers will pick up devstral-2 (and devstral-small-2) after tomorrow once Mistral starts charging for the API access.  That'll make it easier for people to find and use it.",
          "score": 1,
          "created_utc": "2026-01-26 19:21:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vt5xc",
          "author": "Large-Example-1275",
          "text": "It runs slowly on my DGX Spark compared to MoE alternatives.",
          "score": 0,
          "created_utc": "2026-01-26 20:08:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qokht8",
      "title": "Mistral Vibe now available on subscriptions",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qokht8/mistral_vibe_now_available_on_subscriptions/",
      "author": "Holiday_Purpose_3166",
      "created_utc": "2026-01-27 17:33:24",
      "score": 51,
      "num_comments": 8,
      "upvote_ratio": 0.95,
      "text": "https://preview.redd.it/qi2hpnqdgxfg1.png?width=394&format=png&auto=webp&s=9aad3a7897e9e01465d0adf02a302a1a0546fc14\n\nMistral team announced on X the subscriptions are now available with access to Mistral Vibe coding.\n\nMassively appealing move.\n\n[https://x.com/mistralvibe/status/2016179799689928986?s=20](https://x.com/mistralvibe/status/2016179799689928986?s=20)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qokht8/mistral_vibe_now_available_on_subscriptions/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o21xmyi",
          "author": "minaskar",
          "text": "Any information on what the actual request limits are?",
          "score": 7,
          "created_utc": "2026-01-27 17:37:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o225d9w",
              "author": "Holiday_Purpose_3166",
              "text": "Good question. It's transparent as a door. In their plan it describes \"Mistral Vibe for all-day coding, PAYG beyond.\" but no suggestive references to what those limits are.",
              "score": 3,
              "created_utc": "2026-01-27 18:10:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o21xt5y",
          "author": "datNovazGG",
          "text": "Arent we able to just download it and use it with something like OpenRouter, anymore? What's the difference that it's in the pro tier sub?",
          "score": 3,
          "created_utc": "2026-01-27 17:38:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2254x1",
              "author": "Holiday_Purpose_3166",
              "text": "OpenRouter is a proxy provider, unrelated to Mistral sub plans. I understand Mistral was offering the Devstral 2 models for free on OpenRouter and I believe these are finishing/finished - so after that it's PAYG.  \n  \nMistral plan description states \"all day coding\" with Mistral Vibe, so that wording expresses that you only have free\\* access to coding model only via Mistral Vibe agentic tool.   \n  \n*\\* No quotas are prescribed but they express \"Mistral Vibe for all-day coding, PAYG beyond.\" which does suggests limits.*\n\nBased on the available information, the main difference is you can't use the free coding model outside Mistral Vibe, like you could do with OpenRouter.",
              "score": 2,
              "created_utc": "2026-01-27 18:09:40",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o26izrg",
              "author": "robberviet",
              "text": "Just like how you can use claude code with sub or with key.",
              "score": 1,
              "created_utc": "2026-01-28 08:29:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26u6xg",
          "author": "_coding_monster_",
          "text": "Does mistral have a VSCode extension (not CLI in VSCode) that works like Github Copliot or Kilo Code? Claude code supports VSCode extension",
          "score": 2,
          "created_utc": "2026-01-28 10:13:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2r6sk1",
              "author": "Holiday_Purpose_3166",
              "text": "The have IDE but for Enterprise only.",
              "score": 1,
              "created_utc": "2026-01-31 07:41:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dk0fm",
          "author": "Salt-Willingness-513",
          "text": "why the fuck did mistral stop prividing proper info? we still have no info on medium 3 on lechat and rate limits are also unclear on vibe...",
          "score": 2,
          "created_utc": "2026-01-29 08:13:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qndv3e",
      "title": "Ministral models are good.",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qndv3e/ministral_models_are_good/",
      "author": "Rent_South",
      "created_utc": "2026-01-26 11:45:46",
      "score": 48,
      "num_comments": 8,
      "upvote_ratio": 0.96,
      "text": "Just to say that in their weight class, ministral models (mainly 3b and 8b) are very cost efficient and quick, compared to other models.\n\nFor non complex tasks, they actually compete for the top spot.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qndv3e/ministral_models_are_good/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1t0lz8",
          "author": "scara1701",
          "text": "I like ministral:3b as well. Currently using it to test MCP tools Iâ€™m building :)",
          "score": 5,
          "created_utc": "2026-01-26 12:15:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1to112",
          "author": "stddealer",
          "text": "Yep, they've finally replaced Gemma3 models for me, though I think Gemma was a bit better at some things like translation or OCR, Ministral feels like a nice upgrade.",
          "score": 2,
          "created_utc": "2026-01-26 14:31:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1u0apu",
          "author": "Holiday_Purpose_3166",
          "text": "Mistral models are indeed good. I use them daily, especially Devstral Small 2 for my workflows where GPT-OSS-120B struggles to execute. What a time to be alive.",
          "score": 2,
          "created_utc": "2026-01-26 15:30:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23fy7r",
          "author": "kompania",
          "text": "From my perspective, they're very close to Gemma 3. They're incredibly talkative, competent, and handle drift well.\n\nThe downside is that they're currently untunable due to the lack of working notebooks.",
          "score": 2,
          "created_utc": "2026-01-27 21:35:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1v9vhq",
          "author": "Conscious-Expert-455",
          "text": "How to use these models? For vibe coding? As agents?\nI'd like to use them as agents or as MCP services.",
          "score": 1,
          "created_utc": "2026-01-26 18:45:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xac54",
              "author": "Rent_South",
              "text": "It all depends on your use case. Depending on your specific tasks, any of your suggestions are viable.  \nOne thing is certain is that if your use case fits these models, they perform really well.",
              "score": 1,
              "created_utc": "2026-01-27 00:18:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1szmp5",
          "author": "Scared_Range_7736",
          "text": "Still far behind American and Chinese models, unfortunately. Check this benchmark from a few days ago: [https://www.vals.ai/benchmarks/terminal-bench-2](https://www.vals.ai/benchmarks/terminal-bench-2)",
          "score": -6,
          "created_utc": "2026-01-26 12:08:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1t5me6",
              "author": "krkrkrneki",
              "text": "OP is referring to open models available to be run locally.",
              "score": 8,
              "created_utc": "2026-01-26 12:50:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qoc2ww",
      "title": "Give feedback if you want that Mistral improves",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qoc2ww/give_feedback_if_you_want_that_mistral_improves/",
      "author": "no_coykling",
      "created_utc": "2026-01-27 12:08:10",
      "score": 48,
      "num_comments": 8,
      "upvote_ratio": 0.98,
      "text": "Instead of just providing a new reply with what you think should be improved, also include feedback using thumbs-up/down buttons.\n\n* You have a good reply, thumbs up.\n* If your answer lacks information that was previously mentioned in the context, include it in your feedback.\n* If Mistral asks a question about something not specified, don't use it.\n\nThis is one way to improve the product, subscription is the other one ;)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qoc2ww/give_feedback_if_you_want_that_mistral_improves/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o205dps",
          "author": "nycigo",
          "text": "Try to get as close as possible to the Chinese models, I think, and try to highlight their models like Devstral 2 as much as GLM 4.7.",
          "score": 5,
          "created_utc": "2026-01-27 12:23:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22vjam",
              "author": "dcforce",
              "text": "I'm with you Devstral 2 is pretty amazing\n\nWish it had a bit larger context window and coding is likely only going to only improve on what is already a solid model.  \n\nLooking forward to Devstral 2.5 - 3 for sure",
              "score": 4,
              "created_utc": "2026-01-27 20:03:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2045y0",
          "author": "Jazzlike-Spare3425",
          "text": "I would love to help the product improve but I've had a back and forth with their support team (wonderful people, honestly) where I shared a plethora of bugs and other problems regarding their apps for iOS and iPadOS, and I don't think they really addressed any. But to be fair I haven't been checking back since I found it annoying enough to have eventually just given up waiting for fixes and started building my own Mistral frontend in SwiftUI to get the UX I wanted.",
          "score": 8,
          "created_utc": "2026-01-27 12:14:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o206clu",
              "author": "no_coykling",
              "text": "People has always a different roadmap then a company. But I agree that reproducible bugs should be on top.",
              "score": 6,
              "created_utc": "2026-01-27 12:30:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2072uc",
                  "author": "Jazzlike-Spare3425",
                  "text": "Yeah, and I'm assuming that subscriptions to normal customers aren't Mistral's bread and butter but I think the Le Chat Pro subscription is a bit too expensive to not have a desktop app and to have the mobile apps like they are. I did later use Gemini though and the Le Chat app was better than Gemini's at least. The Le Chat apps do lack important features and polish but the Gemini app was outright unusable for me at times.",
                  "score": 3,
                  "created_utc": "2026-01-27 12:34:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2evde3",
          "author": "Any-Cicada-7317",
          "text": "Disclaimer, I'm not an engineer so I may be unaware of an existing capability.  \n  \nWith Vibe now able to use my Le Chat pro account for credits etc. It'd be great to set up a specific Le Chat project/projects that Mistral Vibe could natively interact with.   \nI do a lot of my planning/research/scoping in Le Chat, and then download it into a .md and use Vibe to read it. \n\nIt \"works\" but having it able to natively interact with Le Chat would be interesting. Setting system prompts etc there, placing consistently used [skill.md](http://skill.md)",
          "score": 2,
          "created_utc": "2026-01-29 14:08:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ir97z",
          "author": "Equivalent-Word-7691",
          "text": "as a writer mkisttal is  juts  behind  ,both  on output lenght and  in quality comapred to calude (!), opne AI, gemini, or even chinese models like kimi, GML and deepseek ,it's like a cold style ,aesthetically ugly,a dn the output lenght meh..also free pan should be more generous be cause  no way peope will  try mistral and pay easily 20 eyruos for a mdoe that is clealry infeiror wihtout any pro",
          "score": 0,
          "created_utc": "2026-01-30 01:22:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsvmbo",
      "title": "Is Mistral Large 3 actually the best ai writing tool or are we just coping?",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qsvmbo/is_mistral_large_3_actually_the_best_ai_writing/",
      "author": "Fresh_State_1403",
      "created_utc": "2026-02-01 11:02:36",
      "score": 44,
      "num_comments": 21,
      "upvote_ratio": 0.92,
      "text": "Iâ€™ve spent the last month running Mistral Large 3 against the new Claude 4.5 and GPT-5.1 \"Thinking\" modes, and Iâ€™ve come to a conclusion that might annoy the purists here. If I had to pick one for logic-heavy, technical writing where I don't want a \"guidance counselor\" lecturing me on my tone, Mistral is the winner, but only if you aren't paying the $20 \"tax\" for a single-model sub.\n\nMistral Large 3 is fundamentally the best ai writing tool for anyone who needs high-density output without the sycophancy. While GPT-5 tries to guess what I want to hear and Claude gets bogged down in its own safety \"Constitutional\" logic, Mistral just executes the Markdown. It treats the prompt like a set of instructions, not a suggestion.\n\nImportantly, the reasoning depth here (let me elaborate!!) is finally at parity with the frontier models, but without the \"lobotomy\" effect we see after a modelâ€™s been out for six months. Iâ€™ve been testing this by running complex document analysis through writingmate, where I can flip between Mistral and the other models in a single thread. such a  \"hallucination drift\" is significantly lower on Mistral when you're dealing with non-English technical specs or legacy codebases, at least I found this to be true for my workflow.\n\nThe real problem isn't the model; itâ€™s the fragmentation. Most people claim ChatGPT is the best ai writing tool simply because theyâ€™ve already paid the $20 and don't want to admit it's lagging in raw reasoning. But the minute you need to cross-check a hallucination or run a deep search without the \"Exactly!\" and \"Sharp observation!\" fluff, the value of a single-model subscription falls apart.\n\nClaude and Gemini are great for their specific moats (f.e. Claude for narrative, Gemini for the 2M context window), but Mistral is the only one that feels like itâ€™s built for professionals who want a tool, not a friend. My skeptic take? Perhaps, stop overpaying for the \"big brand\" wrappers and start using a brief stack of tools that let you use the right logic for the right task.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qsvmbo/is_mistral_large_3_actually_the_best_ai_writing/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o2yarri",
          "author": "Working-Chemical-337",
          "text": "oh, price consolidation is the only thing that convinced me to try managed platforms. was paying >100 dollars/month for GPT, Claude, and Gemini separately. then tried multi ai platforms (or wrappers with many models) like writing mate, just because i heard it handled the agent side better than the standard APIs, and saving that ammount per month while still getting frontier-level Mistral reasoning is probably the only \"win\" Iâ€™ve had in my tech stack this year that just started",
          "score": 7,
          "created_utc": "2026-02-01 11:06:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2yjlcd",
              "author": "Fresh_State_1403",
              "text": "makes a lot of sense",
              "score": 2,
              "created_utc": "2026-02-01 12:20:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2z78k1",
              "author": "Tuckebarry",
              "text": "Wait so what's the stack then? What's the best multi AI platform that you mentioned?\n\nThanks",
              "score": 1,
              "created_utc": "2026-02-01 14:49:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2yb0jt",
          "author": "One-Risk-4266",
          "text": "doing the same thing and bouncing drafts between Claude and Mistral. I find Claude is better for the initial soul of a piece, but Mistral is the only one I trust to actually follow a style guide without drifting into aispeak after three pages. and itâ€™s hard to argue itâ€™s not a top-tier contender for the title of best ai writing tool in 2026, especially if you value data sovereignty and want something local",
          "score": 4,
          "created_utc": "2026-02-01 11:08:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2yk8ss",
              "author": "Fresh_State_1403",
              "text": "i totally get that. Claude has that \"human\" spark, but it definitely starts to hallucinate its own \"creativity\" after a while. and Mistral is just cold, hard logic, which is a godsend for technical consistency and actually sticking to a style guide without the aispeak fluff; so for me all in one chats like writingmate are just to keep them both in one window because I was getting massive tab fatigue trying to copy-paste between accounts. Itâ€™s way easier to just flip the model toggle when Claude starts getting too \"wordy\" and you need Mistral to tighten things up. Honestly, the best ai writing tool in 2026 isn't a single model anymore, itâ€™s just whichever one isn't currently acting \"lobotomized\" or ignoring your instructions\n\nbtw are you running Mistral locally via Ollama for that data sovereignty piece, or are you just sticking to the API for the speed?",
              "score": 1,
              "created_utc": "2026-02-01 12:26:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2z379l",
          "author": "porzione",
          "text": "How do you actually use Large? Mistral lacks actual tools, except vibe/devstral - the same issue as with Chinese models. I use local Mistral Small as creative scene writer, but everything is planned by Opus, because I need to collect names, facts, timeline, check backstories from Obsidian vault with \\~1500 files and I use Claude Code for this, then CC writes detailed scene description/plan and sends it to local fine tuned Mistral Small.\n\nIn theory OpenCode or Zed may work for this, but my previous attempts to use Mistral modelsâ€™ agentic features have all failed miserably, except Devstral. I hope that Mistral will add all their models to Vibe, so it will work without fiddling with 3rd party tools.",
          "score": 3,
          "created_utc": "2026-02-01 14:27:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30mhi5",
              "author": "Working-Chemical-337",
              "text": "what tools does mistral lack?",
              "score": 1,
              "created_utc": "2026-02-01 18:47:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o30plz7",
                  "author": "porzione",
                  "text": "Vibe with all Mistral models, specifically - cli tool with ACP support.",
                  "score": 1,
                  "created_utc": "2026-02-01 19:01:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o31povj",
              "author": "erizon",
              "text": "GLM family works perfectly fine in Claude Code via ANTHROPIC_BASE_URL environment variable (actually much faster than via OpenAI style //api.z.ai/api/coding/paas/v4). Deepseek also supports it, which Chinese model does not?",
              "score": 1,
              "created_utc": "2026-02-01 21:55:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zmlna",
          "author": "Ambitious_Fee3169",
          "text": "Mistral Large is fantastic. It's the default in our AI chat system. Mistral medium is also great (but more expensive for some reason for output tokens vs large).",
          "score": 2,
          "created_utc": "2026-02-01 16:04:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30sgl1",
              "author": "Fresh_State_1403",
              "text": "I don't like using it per credits / calls, because it never seemed to be economically viable for me, so i used 'all in one' chatbot subscription(s) which in my case seems to work more, like writingmate in either pro or ultimate",
              "score": 2,
              "created_utc": "2026-02-01 19:14:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2yb78z",
          "author": "DrPinguin98",
          "text": "I do a lot with AIs and use a service provider that lets me use pretty much all AIs, and honestly, Mistral Large 3 is definitely no better than GPT 5.2 Thinking.\n\nJust the day before yesterday, I tried to rewrite some of my reports and got a little frustrated with Mistral. Content was constantly missing, it made things up (even though I explicitly forbade it), or it didn't rewrite things the way I defined in the prompt.\n\nBut that could also be due to the languageâ€”in my case, German.\n\nI have to say, though, that a lot is happening in this regard, and we're starting to reach the point where I can and will really use Mistral.\n\nAnd when the time comes, my monthly subscription will belong entirely to Mistral, and I honestly can't wait.",
          "score": 4,
          "created_utc": "2026-02-01 11:10:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30j53c",
              "author": "Fresh_State_1403",
              "text": "by the way, does the need to use German change a lot in how you pick AI tools? what differences do you see when it comes to non-English responses?   \n5.2 thinking is very ok",
              "score": 1,
              "created_utc": "2026-02-01 18:32:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2yb5w2",
          "author": "TonyHMeow",
          "text": "The best thing about OA is the saved memory blocks and profiling aspect of the models in both writing and conversational discussion flow, and as of now still leading compared to other major competitors IMO.\n\nComing from GPT 4oâ€™s writing, how is LeChat saved memory (storage size/utilization) and context tracking (in a project, say) compared to OpenAI? Genuinely considering trying out Mistral after 4o retires, would love some input on this!",
          "score": 1,
          "created_utc": "2026-02-01 11:09:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ykzkx",
              "author": "Fresh_State_1403",
              "text": "OpenAIâ€™s memory is definitely the stickiest part of their ecosystem to me, it's hard to leave once the model knows your specific quirks. Yet if youâ€™re looking at lechat , in my xp  Itâ€™s less \"vibe-based\" and more literal. Their Project Libraries actually beat GPT in my opinion because they index your files properly. Instead of the model just cramming everything into the context window and hallucinating when it gets full, Mistral pulls only whatâ€™s relevant. Itâ€™s way more stable for long-term technical work. So I often use Writingmate to A/B test the recall between them and overall compare models side by side, and while GPT feels more intuitive, Mistral is much more transparent about what itâ€™s actually pulling from your history. You won't get that \"as an AI assistant\" lecture every five minutes either, which is a massive plus. There's also a trade-off which is, Mistral won't remember a random chat from three weeks ago unless you specifically tell it to save that info to your memory or project. Itâ€™s more manual, but somwewhat better for privacy and focus.",
              "score": 2,
              "created_utc": "2026-02-01 12:31:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2zudq7",
                  "author": "TonyHMeow",
                  "text": "ok, thanks for the info!\n\nWhen you said \"Mistral is much more transparent about what itâ€™s actually pulling from your history.\" Do you mean like it literally quotes you (\"word for word\") from another conversation/your uploaded files? Because i think GPT had an update about that a couple of weeks, which I really dislike lol.",
                  "score": 1,
                  "created_utc": "2026-02-01 16:40:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2yhhi4",
          "author": "Alex-plosion",
          "text": "For  ideas, I don't find mistral large 3 to be amazing, but for writing, yeah it's great, especially in French.\nI tend to write the plan, or a whole draft with another model and rewrite it after with mistral.",
          "score": 1,
          "created_utc": "2026-02-01 12:04:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30jzvw",
              "author": "Fresh_State_1403",
              "text": "when I am able to switch within one chat with tools like writingmate, I can interchange mistral with gpt and gemini without changing context, tools, or chatbots. but frm what I know, Mistral is truly very fine at European languages",
              "score": 1,
              "created_utc": "2026-02-01 18:36:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2yqxwq",
          "author": "Charming_Support726",
          "text": "No. Definitely No.\n\nUsed all of them for writing. All of them showed ugly quirks. I tried German, European Portuguese, English. \n\nGemini follows style instructions quite well, but lacks some variance in creativity. Style was very important for me. \n\nAll others still do this damn \"its not only A, it is B\" thing in every second sentence. You can't get rid of it. \n\nThe creative model from Mistral Labs can get European Portuguese right - this is the only model except for Gemini 3 Pro, which is capable. All others fails (Claude, Gpt, Large3 and more). Unfortunately it is not open weights, otherwise I'd like to see an optimized version.",
          "score": 1,
          "created_utc": "2026-02-01 13:14:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30qf27",
              "author": "Working-Chemical-337",
              "text": "you can prompt them not to use negations, and other signs of ai writing, and otherwise follow your natural style of writing. some editing may be required even then, but hey, this can save a lot of time when you have to do some huge outputs",
              "score": 1,
              "created_utc": "2026-02-01 19:05:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2z0vxg",
          "author": "TeeRKee",
          "text": "Both",
          "score": 1,
          "created_utc": "2026-02-01 14:14:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnujw3",
      "title": "Mistral beats Gemini and Perplexity for competitive intelligence",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qnujw3/mistral_beats_gemini_and_perplexity_for/",
      "author": "Cachao-on-Reddit",
      "created_utc": "2026-01-26 22:03:26",
      "score": 35,
      "num_comments": 2,
      "upvote_ratio": 0.91,
      "text": "I've posted here before about being impressed by Mistral Medium. That was mostly as an API user.\n\n  \nThis time I ran most of the big consumer-facing LLMs against each other in a 'Deep Research' style task. The focus was competitor news.\n\nMistral didn't win. But I think did commendably well. Especially given:\n\n\\- (a) relative underdog status compared to other players on this list,\n\n\\- (b) I using the very fast free tier (unlike Claude's slow, very expensive tier), and\n\n\\- (c) it was \\*clearly\\* better than Perplexity and Gemini.\n\n\n\nhttps://preview.redd.it/bcbnmklhmrfg1.png?width=763&format=png&auto=webp&s=a9f06a7c5f9a38234ca58faf6a9e9b1758a3d30d\n\n\n\nYou can see more about the test here: [https://anatole.fyi/blog/competitive-intelligence-face-off](https://anatole.fyi/blog/competitive-intelligence-face-off)\n\n\n\nAnd yes, you'll see it's flawed. I only did one run per LLM. The prompt was bad. Obviously on another attempt or with a better prompt Gemini won't have quite such a meltdown. But, when I'm using these tools day to day I would rather not have to run them multiple times or craft my prompt. And I think this side-by-side beats pure anecdote when comparing LLM quality.\n\n  \nWill run another test soon. Let me know what you think.\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qnujw3/mistral_beats_gemini_and_perplexity_for/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o26dotz",
          "author": "enormousdino",
          "text": "so in the wake of Macron's shades, I asked them all the other day which politicians wear watches made in their own country (as Macron famously wears French watches, including v independent niche brands). \n\nMistral guessed right - including Macron, Joe Biden's Shinola, and even Modi's Jaipur  \nGemini didn't guess Macron, but did talk about Biden, Abe Shinzo's Seiko and Modi  \nChatGPT didn't guess Macron, but did mention Biden and Modi  \nClaude said it's not aware of any instances of politicians wearing such watches.... and I'm paying for it!!",
          "score": 1,
          "created_utc": "2026-01-28 07:42:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2brlzi",
              "author": "Cachao-on-Reddit",
              "text": "share the link? would love to see.\n\nI do think it's important to distinguish the model from the harness. Opus 4.5 is clearly a better model than Large. But clearly part of how claude.ai is wiring it up is yielding sub-par  results.\n\nside note: feel like Macron's shades are a huge Mistral branding opp",
              "score": 1,
              "created_utc": "2026-01-29 01:04:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qry40r",
      "title": "Replace github copilot with Mistral",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qry40r/replace_github_copilot_with_mistral/",
      "author": "InternalBroad2522",
      "created_utc": "2026-01-31 09:39:58",
      "score": 34,
      "num_comments": 10,
      "upvote_ratio": 0.97,
      "text": "Hi all! I would like to replace Github Copilot with Mistral for coding in VSCode IDE. What can I do?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qry40r/replace_github_copilot_with_mistral/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o2rk80x",
          "author": "scara1701",
          "text": "Does it have to be in vscode? I guess you could run mistral vibe in a terminal window.",
          "score": 4,
          "created_utc": "2026-01-31 09:48:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vlko5",
              "author": "LoadZealousideal7778",
              "text": "Or in the Jetbrains IDE of your choice",
              "score": 2,
              "created_utc": "2026-01-31 23:25:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2roc3i",
          "author": "katafrakt",
          "text": "Mistral Vibe supports ACP, so any VSCode extension supporting ACP would do. I'm not too familiar with this ecosystem, so I won't give any recommendations.",
          "score": 4,
          "created_utc": "2026-01-31 10:27:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2szqsy",
          "author": "kiwibonga",
          "text": "There's a mistral vibe extension that lets you have the CLI in VSCode. It's mostly a convenience thing that saves you the trouble of launching the terminal and setting the working directory.",
          "score": 2,
          "created_utc": "2026-01-31 15:45:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2utlgd",
          "author": "Bob5k",
          "text": "Have in mind that you either pay for API usage of devstral or accept the quite mediocre quota allowance on experiment plan.",
          "score": 2,
          "created_utc": "2026-01-31 21:02:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xnbbp",
              "author": "deegwaren",
              "text": "Oh? Didn't they just include vibe-cli usage in their pro plan? Meaning it's included like Claude code usage in the Claude Pro plan.",
              "score": 1,
              "created_utc": "2026-02-01 07:30:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2yd1vv",
                  "author": "Bob5k",
                  "text": "They included cloud vibe usage afaik, don't know about vibe cli tbh and not sure on the quota allowance, i still think the cli falls under Mistral ai studio.",
                  "score": 1,
                  "created_utc": "2026-02-01 11:26:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ro18m",
          "author": "opsmanager",
          "text": "Im using the Continue extension, works reasonably well. I havent yet figured out how to make it as seemless as the copilot extension with regards to accessing the repository files. But im sure its just me missing something obvious.",
          "score": 2,
          "created_utc": "2026-01-31 10:24:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2uj3bq",
              "author": "vienna_city_skater",
              "text": "Continue is soso, good for code completion, but agent mode sucks, Roo Code works well for agentic coding in VS Code, but the TUIs are much better. I switched everything to OpenCode because I mix models a lot. However, so far I have not find a way to get access to the free devstral with my Le Chat Pro subscription outside of vibe CLI.  \nEDIT: I think I found the problem, I used the API key from La Platforme Admin Panel not from the AI Studio VIBE CLI section, so I was charged on-the-go.",
              "score": 3,
              "created_utc": "2026-01-31 20:10:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qqw89j",
      "title": "Made myself a LeChat application on linux",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qqw89j/made_myself_a_lechat_application_on_linux/",
      "author": "MattyGWS",
      "created_utc": "2026-01-30 05:03:06",
      "score": 32,
      "num_comments": 3,
      "upvote_ratio": 0.92,
      "text": "Ok I'm no programmer, but I just had some fun learning how to build 'web apps' with Electron. I made a lil desktop icon too (in gimp). So now I have my own desktop application for Mistral! \n\n\n\nhttps://preview.redd.it/900xz4j75fgg1.png?width=2005&format=png&auto=webp&s=fdbbb04df3c14f515991184693b4fa46119766e0\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qqw89j/made_myself_a_lechat_application_on_linux/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o2lf64l",
          "author": "AdIllustrious436",
          "text": "Great! Just for you to know, you can achieve the same result in one click with Chromium PWA, which install websites as applications on the system.",
          "score": 3,
          "created_utc": "2026-01-30 12:55:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mcd54",
              "author": "MattyGWS",
              "text": "Yea I know that's how I normally do it but I wanted to learn how Electron apps are made, seemed like a great yet simple example to bring LeChat to desktop. :)",
              "score": 3,
              "created_utc": "2026-01-30 15:43:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2xzts3",
          "author": "LearningPodd",
          "text": "Good job! I'm also starting to creat stuff with AI â˜ºï¸ It's so much fun and the more people that creat things themselves, the less power companies will have over our products  ðŸ‘",
          "score": 2,
          "created_utc": "2026-02-01 09:26:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo03k1",
      "title": "Difficult in switching from claude",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qo03k1/difficult_in_switching_from_claude/",
      "author": "MikadinShinjuk",
      "created_utc": "2026-01-27 01:45:02",
      "score": 30,
      "num_comments": 15,
      "upvote_ratio": 0.85,
      "text": "Good morning everyone, I have a problem that's been bothering me. I'm trying to move away from as many US services as possible, but Claude seems like an insurmountable obstacle. Let me clarify: I'm not a developer. I use AI to search for information, help with solo role-playing games (RPG), and research Warhammer 40k lore and similar topics. With Claude, I feel like I'm in heaven, but I understand that more and more (especially in recent days), it would be better to distance myself from US services. For some time now, I've been trying to use Le Chat, but every time it seems to be lagging behind. It's as if it doesn't consider the nuances in what I say, doesn't analyze in depth, always stops at the first point, and doesn't go into detail thoroughly. Am I doing something wrong? Should I create specific agents? Should I give it more precise and less discursive instructions? I tend to create queries as if I were talking to a person, and this works well with Claude, but maybe not here? I need some feedback from those who use it as their main AI.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qo03k1/difficult_in_switching_from_claude/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1xt331",
          "author": "gdsfbvdpg",
          "text": "I tried using it as a gm for a Cyberpunk game and then using it as a group of 5 players in a d&d game that I DM. In both cases it was pure frustration. I took the same exact instructions/files over to Gemini (free version) and *boom* it works very well. \n\nIt makes me really sad. BUUUUT - I feel like Mistral might be where chatGPT was a year or year and a half ago. So I'm hopeful that there's a bright future in store for it. But right now?  *Le sigh*. It's just not there yet.  I'm going to keep paying for it though.  I refuse to give up.",
          "score": 20,
          "created_utc": "2026-01-27 01:57:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o216ngz",
          "author": "fonceka",
          "text": "I believe Mistral market is the pro market, not end user. They are a b2b startup, not like ChatGPT. In a business perspective, Mistral performs absolutely well. It's super fast, not verbose, and really sensitive to your context. For a solopreneur like me, it was much more helpful than GPT or Gemini to craft my marketing campaign, help me with landing page, find the adequate wording, lower ambiguities, etc.",
          "score": 9,
          "created_utc": "2026-01-27 15:40:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1y40mb",
          "author": "LewdManoSaurus",
          "text": "Mistral has some ways to go for sure. It's fun to mess around with, but for heavy usage for generative writing, it is definitely lagging behind by far imo (I had the subscription back in October last year). It's nice when it works, but in my experience it was a headache more often than not, or I had to make corrections so often that I was better off just using a different service.",
          "score": 13,
          "created_utc": "2026-01-27 02:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yqjm3",
          "author": "Den_er_da_hvid",
          "text": "I know what you feel.  coming from Gemini though. \n\nLast night I tried using it to get tips on my game playing Humankind. \nI gave then info but it kept going around in circles that did not make sense based on resources and where my territories where.\n\nLater I asked about airfry time vs  oven in a recipie and gave it a link. It did not use the link before I explicitly said so.",
          "score": 4,
          "created_utc": "2026-01-27 05:16:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z9b0j",
          "author": "ActionLittle4176",
          "text": "Itâ€™s better if you use the large model, but of course itâ€™s behind the frontier models from Anthropic, OpenAI and Google (like the rest of the AI labs)",
          "score": 3,
          "created_utc": "2026-01-27 07:46:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zweyg",
          "author": "EcceLez",
          "text": "Mistral is not as good as Claude. That being said, it makes sense to use it through it's api for your workflows.",
          "score": 3,
          "created_utc": "2026-01-27 11:15:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zx0ye",
          "author": "ameliassoc",
          "text": "Unfortunately it is similar for me. I use AI primarily for language learning and Mistral just can't live up to the standard ChatGPT has set for me. It constantly messes up instructions forcing me to repeat, and it is obsessed with our past discussions and memories to the point that it always talks about the same thing, which of course is not what you want when trying to learn as much as you can.\n\nFor the time being I'm settling for a compromise. I'm keeping my OpenAI subscription because I figure getting a better education is always going to be better irrespective of whether it's a US service. For other queries and where privacy is absolutely not a concern, I use Le Chat, and have the setting turned on to let it train on our chats, in the hope that it will eventually catch up and I can switch fully.",
          "score": 2,
          "created_utc": "2026-01-27 11:20:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22a8r9",
          "author": "graymalkcat",
          "text": "Just use both. When I need Claude then I reach for Claude. Otherwise I use Mistral.\n\nAlso, remember that Claude is trained on user data. The more people use it, the more youâ€™ll push it ahead of everything else. Maybe you like that or maybe you think thatâ€™s unfair and unethical. Decide. Â ",
          "score": 2,
          "created_utc": "2026-01-27 18:31:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23563o",
          "author": "Nev3r_Pro",
          "text": "I've decided to shift from US based AI to the Chinese one. I use Deepseek and Kimi K2.5, both are great.",
          "score": 2,
          "created_utc": "2026-01-27 20:47:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20s1ly",
          "author": "victorc25",
          "text": "You need to set your priorities straight. Do you want a top of the line model or do you want an European provider? Choose one",
          "score": 2,
          "created_utc": "2026-01-27 14:30:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21m408",
          "author": "poidh",
          "text": "I use LeChat for low effort/low risk queries because I want to support them. I don't know if that helps, but I guess for investors it is important to see that the product gets actually used, and it hopefully gives them some more real world training data.\n\nBut you have to consider that \"nobody\" can really compete with the view state of the art models (Anthropic/OpenAI/Google). They are just playing in a different league whether its the people or resources behind it. You could try some of the Chinese open weight models and have them hosted in some jurisdiction of your choice.\n\nBtw, you can get an unbiased overview of where the Mistral models stand:  \n[https://lmarena.ai/de/leaderboard/text](https://lmarena.ai/de/leaderboard/text)\n\nOn this site, you can start chatting and you'll receive two responses from two different models, but you don't know which is which.  \nThen you pick which response you like the best (and then it will be revealed which model you had been talking too).\n\nThis is great to get a feel for all kinds of different models you may not had on your radar. And it also shows a realistic ranking (from other people) that are not based on benchmarks (which can be gamed).\n\nYou find the Mistral models on that list way down on position 40+.",
          "score": 1,
          "created_utc": "2026-01-27 16:47:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zbzv9",
          "author": "Hitching-galaxy",
          "text": "Have a look at Kagi- Iâ€™m about to sign up to ultimate",
          "score": 1,
          "created_utc": "2026-01-27 08:10:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zcpr5",
              "author": "MikadinShinjuk",
              "text": "Wow this looks very interesting can you tell me a bit more?",
              "score": 0,
              "created_utc": "2026-01-27 08:17:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o21kbb4",
                  "author": "poidh",
                  "text": "Kagi is based in San Francisco, OP was looking for something outside of the US. I also think they are probably just a wrapper around [Bing.com](http://Bing.com) (they state to use content from various search engines) and the usual AI models (as seen from their model picker).  \nI think it is pretty unlikely that some relatively unknown company is able to build their own search engine or own SOTA LLM model, only a handful of companies can compete at the top of this world wide.",
                  "score": 3,
                  "created_utc": "2026-01-27 16:39:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qqguur",
      "title": "Mistral OCR Skill",
      "subreddit": "MistralAI",
      "url": "https://skills.sh/parlamento-ai/parlamento-ai/mistral-ocr",
      "author": "antoine849502",
      "created_utc": "2026-01-29 18:33:36",
      "score": 22,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qqguur/mistral_ocr_skill/",
      "domain": "skills.sh",
      "is_self": false,
      "comments": [
        {
          "id": "o2gznci",
          "author": "nnamfuak",
          "text": "Thank you very much! It works really well. I've been using Mistral OCR3 (mistral-ocr-2512) for 3 weeks now, and it consistently delivers top-quality markdown! Love it!",
          "score": 2,
          "created_utc": "2026-01-29 19:59:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2h1dp5",
              "author": "antoine849502",
              "text": "And the free version is so generous; this should be installed by default with any local agent",
              "score": 1,
              "created_utc": "2026-01-29 20:08:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qq3rlf",
      "title": "Anthropic is winning market share in the enterprise LLM space. Google and Anthropic are gaining ground quickly, while OpenAI is currently seeking new investment in Saudi. Mistral's share is in image 2",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/gallery/1qq3okh",
      "author": "neural_core",
      "created_utc": "2026-01-29 09:11:52",
      "score": 22,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qq3rlf/anthropic_is_winning_market_share_in_the/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o2foxcs",
          "author": "Den_er_da_hvid",
          "text": "The colors in the legend for the stacked barchart is really difficult to see. What color is Mistral?",
          "score": 5,
          "created_utc": "2026-01-29 16:27:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2jxm3f",
              "author": "deegwaren",
              "text": "Mistral is baby blue, around 2%.",
              "score": 3,
              "created_utc": "2026-01-30 05:36:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2jzbvl",
                  "author": "Den_er_da_hvid",
                  "text": "It will be interesting to see in a few month as it seems people are shifting from US to European tech.",
                  "score": 4,
                  "created_utc": "2026-01-30 05:49:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qteixn",
      "title": "I love Mistral",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qteixn/i_love_mistral/",
      "author": "Potential_Block4598",
      "created_utc": "2026-02-01 23:31:42",
      "score": 19,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "This is my second post in a long time praising mistral \n\nSo earlier I praised how they train objective models that services Le Mistral \n\nNow I am doing this again, but as I am running and switching between many models for local agentic tasks (using an agent scaffold and and MCP to perform basic static malware analysis tasks for cybersecurity that is essentially copy pasting to and from an LLM model in an automated way!) \n\nI tried many things \n\nFirst â€œfrontierâ€ (local frontier for my setup) according to artificial analysis aggregated benchmarks (that should include tool call, and not just demonstrative tool call but actual consistent real-life tool call!) (note I always wondered why Devstral ranked too low on that benchmark (either the model is too weak or the benchmark is too weak!!!!)\n\nSo I tried \n\nGPT-OSS (both on all kinds of Thinking effort options)\n\nWeird failures (sometimes call format not correct especially when used with cline and/or Goose!) \n\nAnd no instruction following (not even loose instruction following, or proper task management , so they donâ€™t live well inside the scaffold environment (some code todo management complex prompt and things like that!) \n\nGLM-4.7-Flash\n\nSimilar story \n\nThen Cline docs and Jack Dorsey mentioned Qwen3 Coder, I scratch my head why is that small seemingly insignificant model recognized by them no idea\n\nI try it and lo and behold it works very well than others\n\nSo it is not an agent problem or me dosing misconfiguration, these other open models arenâ€™t desgined for that (and for good reasons form the companies perspective)\n\nI am thinking of trying\n\nMinimax M2.1 or GLM-4.5-Air \n\nBut then I think about using Devstral Small 2\n\nAnd it works better than a charm finishes the task methodologically and analyzes the whole sample in like 3-5 hours \n\nA task that would have taken a junior around a month maybe (still a junior can do other stuff but maybe it dis. Better of MCP becoming exposed by default \n\nAnyways thanks Mistral Team for your awesome model and contributions to the open \n\nTL;DR\n\nDevstral Small 2 is the best for Local LLM agentic tasks (beyond being compared to others!)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qteixn/i_love_mistral/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o32cdeb",
          "author": "iongion",
          "text": "Is there a possibility to run it/configure it in claude code like it is possible with zai GLM ?",
          "score": 1,
          "created_utc": "2026-02-01 23:54:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32i448",
              "author": "Potential_Block4598",
              "text": "Havenâ€™t tried that yet",
              "score": 1,
              "created_utc": "2026-02-02 00:26:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o32fqii",
          "author": "Cachao-on-Reddit",
          "text": "Look I don't want to be aggy about this but it's so long it seems like LinkedIn fodder.\n\n\nWhat's the insight, in one line?",
          "score": 0,
          "created_utc": "2026-02-02 00:13:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32gss6",
              "author": "cosimoiaia",
              "text": "Others do what they think it's best for you, Mistral does what you actually say.\n\n(It's my experience too, it has always been the best instruction following model of all)",
              "score": 2,
              "created_utc": "2026-02-02 00:19:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o32i1gy",
              "author": "Potential_Block4598",
              "text": "There is a TL;DR\n\nBest model for local agentic ai stuff",
              "score": 2,
              "created_utc": "2026-02-02 00:25:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qr2987",
      "title": "How do you use vibe effectively?",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qr2987/how_do_you_use_vibe_effectively/",
      "author": "hyper_plane",
      "created_utc": "2026-01-30 10:51:43",
      "score": 13,
      "num_comments": 8,
      "upvote_ratio": 0.88,
      "text": "I am using the Vibe CLI to work on a project. I feel like the coding capabilities of the underlying devstral model are pretty good, but I have the impression that I am not using it right. Has anybody here tips and tricks on how to really take the best out of this tool? What do you put in your instructions? Do you write instructions for each project? How do you smoothly integrate with your IDE or code editor?\n\nI suppose we are all still learning how to use coding agents effectively, but if you have some really good tips, please share them here! ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qr2987/how_do_you_use_vibe_effectively/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o2kyvx2",
          "author": "EzioO14",
          "text": "I only use vibe cli for documentation and nothing else",
          "score": 3,
          "created_utc": "2026-01-30 10:56:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2naf3s",
          "author": "RegrettableBiscuit",
          "text": "Write an AGENTS.md file that explains what your project is, how it is structured, how to extend it, coding guidelines, etc.\n\n\nWhen prompting, provide a path to example code files similar to what you are building so the LLM can emulate your code base's approach.\n\n\nWrite specific prompts that explain exactly what you want the LLM to do in detail.\n\n\nMake each prompt limited in scope and review after the LLM is done to make sure it's on the right track.Â ",
          "score": 3,
          "created_utc": "2026-01-30 18:15:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kzp6q",
          "author": "chebum",
          "text": "This presentation from Jake Nations (Netflix) describes a framework of working with AI assistants which seems to provide best results in my experience : https://youtu.be/eIoohUmYpGI?si=rr5O4GLdLJTzOgPS",
          "score": 5,
          "created_utc": "2026-01-30 11:03:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2l7l6x",
          "author": "AriyaSavaka",
          "text": "General rule of thumb for any coding agent would be effective system instruction that enforce strict test driven development with frequent atomic commit. And have git hooks already set up for running unit tester/formatter/linter/checker after every turn.",
          "score": 1,
          "created_utc": "2026-01-30 12:04:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pieyv",
          "author": "Bob5k",
          "text": "Using Clavix to help with prd and input. \nHowever - i find the limit on experiment plan quite low for serious work, so usually I'm using vibe whenever j just want to switch from Claude code combined with Kimi K2.5.",
          "score": 1,
          "created_utc": "2026-01-31 00:46:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2quc8z",
          "author": "Holiday_Purpose_3166",
          "text": "For context, I spent billions in tokens last year in prototypes and public facing, production work and it's mostly financial models interacting with 6 figures contracts. Shrimp in my circles, small, but produces income. \n\nRoughly, 70-80% of the jobs produced last year, had to be polished using SOTA closed source models, but I'm seeing that gap closing considerably fast and Devstral Small 2 beats GPT-OSS-120B on the same tasks, and has been doing more work than any OSS model up to 235B model I've used last year.\n\nI wish I could speak better about Mistral Vibe, as my experience was short-lived due to context bloat and errors - using the local Devstral Small 2 UD-Q4_K_XL which also had its issues with inference engines and stale quants. This is fixed by now but I've got plenty of experience with it since then, and I've been riding the Q8_0 which has been God. \n\nI assume Devstral 2 will be in the same ballpark with a better punch in knowledge. \n\nWhen Mistral Vibe launched, the context management blew up quickly and it often derailed. It may not be the case now based on what I've seen. \n\nBased on experience, any model is as good as the agentic framework is operating on, and how good is your context engineering for that specific setup. \n\nMost likely it will be your context work. I polished mine by repeating different cases in my codebases, using different prompts styles until I had repeated, positive results. \n\nNo walls of text prompts. Less is more, but not bare. \n\nKeep working out the environment you're using until you get better. It pays long-term, a LOT. \n\nIf you haven't produced reliable outputs every time, and you start jumping around different agentic tools and models, I guarantee you'll be frustrated. \n\nI found Devstral to work better with Kilocode with a custom agent I made, that saves me over 90% of context over original agents. Opencode works very well too, but Kilocode offers me Orchestrator choice which cuts most context bloat into smaller jobs for my custom agent. I might try Mistral Vibe back again later. \n\nMy base template for all repos must always include a folder for docs and that will generally include a blueprint with project spec overview, and a README.md which explains how the project works downstream. \n\nMost folks likely use AGENTS.md for repo context, which is generally fine. I like to break my docs modular enough, that isn't going to bloat the agent, but isn't hard enough to maintain either. \n\nThey complement each other. (How it works and where). Sometimes I ditch the README.md when I know the job doesn't require it, and that keeps my model light.\n\nIf I have a mono repo with different frameworks, I still use a single blueprint for the whole codebase and a single README.md, but I create separate sub-folder docs for each framework with their own framework-specific spec overview and directories. \n\nIn this last case, if I had to work in the front-end, I'd feed only the framework doc and not the whole blueprint and README.md. \n\nIf the work involved another framework, you could add the whole codebase blueprint and the other framework work doc too, and it works itself out. \n\nYou essentially make a modular doc system that doesn't add too much technical debt to maintain, and can be used whenever needed without giving the whole elephant every time. \n\nWhen it comes to prompt style, just simply write one and ask the model to format in a way it would understand and execute. Same with the documents. Devstral 2 models can surprisingly produce extensive documents if needed. \n\nI've used prompt styles from GPT-OSS-120B which broke Devstral. This is important.",
          "score": 1,
          "created_utc": "2026-01-31 05:53:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2le3ca",
          "author": "clayingmore",
          "text": "I'm not sure how 'basic' you're after. Have you got to the point where you are creating an [AGENTS.md](http://AGENTS.md) file in the root directory for any project? Also I use Claude Code and OpenCode rather than Vibe CLI but I am under the impression it works the same way. I have a Chinese Wall in between my IDE and the CLI so I'm mostly using the IDE for my own review and tinkering.\n\nAs a general approach, I try to create a Reason-Act agent pattern on a bigger scale. I create a constitution for the project, describing what the project is doing, why, what the tech stack is. Then I 'discuss' the entire project with my LLM to get to a point where I am confident that the model's semantic understanding is in line with mine. This coincidentally pushes the 'model' to think more and lay out a specific architecture as well as catch things I missed which I then sign off on.\n\nIf my [AGENTS.md](http://AGENTS.md) file, style guide, [ARCHITECTURE.md](http://ARCHITECTURE.md) file, unit tests, and any other supplementary plans are in good shape then the project can 'go'. Coincidentally, the coding LLMs are great at reviewing these documents as well.\n\n\"Yes.\" \"Yes.\" \"Yes.\" \"No change this.\" \"Yes.\" \"Yes.\"\n\nThe coding agent then does a huge portion of the project faster than I could do it, and in many cases better than I could do it. \n\nMake sure to insist on modularity so the whole thing doesn't break at once, effective version control so that you can roll things back, triple check relevant security issues, etc.\n\nThe model WILL make mistakes all the time, but it can also establish its own test driven development cycle in which it is immediately proceeding to fix those mistakes.",
          "score": 1,
          "created_utc": "2026-01-30 12:48:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kzx26",
          "author": "Eastern-Group-1993",
          "text": "I just generate code samples or ask questions or use it as a shortcut to what do I need to search for in documentation.  \nIf gpt-oss:20b isn't enough I either use claude or gpt-5.1 via [duck.ai](http://duck.ai) or I just try to figure it out on my own.\n\nGenerally I don't really full blown rely on it, I tried to make Github Copilot generate 3 python scripts:  \nGear Ratio to RPM converter, RPM to Gear Ratio converter, Gear Ratio generator using amount of frames between 200 RPM periods on a 1.000 gear ratio, which went as well you imagine.  \nI basically used 100% of the free monthly limit in 1-3 hours and the scripts barely worked(they didn't), I don't know how people manage to vibe code anything at all.\n\nAt least gpt-oss:20b is way better than whatever facebook puts into meta AI/whatsapp.  \nI wanted to go to sleep at 3am after playing RDR2, used LLaMa 4 ONCE and it's fully convinced Red Dead Redemption 2 has a \"save and quit\" button.  \nI would have to be more desperate than having access to 8kbps internet(which enables use of meta ai via whatsapp), at least gpt-oss:20b is somewhat usuable for most scenarios(except vibe coding, it's okay for some simple sample code that you have to transform by hand into your data structure) even without RAG.           \n\n  \nMaybe if you use something on the bleeding edge like Claude Opus 4.5/Claude Sonnet 4.5 with a pro/max subscription or a lot of cash spent on API calls you can get anywhere at all with those tools.",
          "score": -2,
          "created_utc": "2026-01-30 11:05:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}