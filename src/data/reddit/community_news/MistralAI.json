{
  "metadata": {
    "last_updated": "2026-02-01 08:57:56",
    "time_filter": "week",
    "subreddit": "MistralAI",
    "total_items": 20,
    "total_comments": 131,
    "file_size_bytes": 146019
  },
  "items": [
    {
      "id": "1qoig0q",
      "title": "Vibe 2.0 - Terminally online Mistral Vibe.",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qoig0q/vibe_20_terminally_online_mistral_vibe/",
      "author": "Clement_at_Mistral",
      "created_utc": "2026-01-27 16:22:48",
      "score": 191,
      "num_comments": 34,
      "upvote_ratio": 0.99,
      "text": "Today, we're releasing Mistral Vibe 2.0 - a major upgrade to our terminal-native coding agent, powered by the state-of-the-art Devstral 2 model family. Build custom subagents, clarify before you execute, load skills with slash commands, and configure your own workflows to match how you work.\n\nMistral Vibe isÂ **now available on the Le Chat Pro and Team plans**Â \\- with pay-as-you-go credits for power use, or bring your own API key.  \nDo you already have a Le Chat Pro/Teams plan? Get your Vibe keyÂ [here](https://console.mistral.ai/codestral/cli).\n\n*Learn more about how to use Vibe*Â [*here*](https://docs.mistral.ai/mistral-vibe/introduction)\n\n# Whats New\n\n* Mistral Vibe 2.0: CustomÂ **subagents**,Â **multi-choice clarifications**,Â **slash-command skills**,Â **unified agent modes**, andÂ **automatic updates**.\n* Available today onÂ **Le Chat Pro and Team plans**Â with PAYG for extra usage, or BYOK.\n* Devstral 2 moves toÂ **paid API access**:Â **Free on the Experiment plan**Â in Mistral Studio.\n* Enterprise services:Â **fine-tuning**,Â **reinforcement learning**, andÂ **code modernization**.\n\n*Learn more about*Â [*Vibe 2.0*](https://github.com/mistralai/mistral-vibe)Â *in our*Â [*blog post*](https://mistral.ai/news/mistral-vibe-2-0)Â *and*Â [*product page*](https://mistral.ai/products/vibe)\n\nhttps://reddit.com/link/1qoig0q/video/bx60g52v3xfg1/player\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qoig0q/vibe_20_terminally_online_mistral_vibe/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o21pzju",
          "author": "Gen5nake",
          "text": "I hope the quotas will at least match those of other providers, but this is exactly what I was waiting for to cancel my Caude subscription :)\n\nOne thing I might have missed, though is where we can set an usage limit? How can we tell when the quota has been exceeded and the system switches to API usage? There's a global settings for API's but can't set to 0.",
          "score": 23,
          "created_utc": "2026-01-27 17:04:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21w0sn",
          "author": "sndrtj",
          "text": "Please tell me it keeps the quirky status messages. Loved \"petting le chat\" etc.",
          "score": 15,
          "created_utc": "2026-01-27 17:30:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26zuez",
              "author": "pandora_s_reddit",
              "text": "Yes :)",
              "score": 2,
              "created_utc": "2026-01-28 11:02:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o270lo3",
                  "author": "sndrtj",
                  "text": "Purrrrfect.",
                  "score": 1,
                  "created_utc": "2026-01-28 11:08:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21teez",
          "author": "deegwaren",
          "text": "Is Le Chat Pro vibe-CLI usage in third-party harnesses like OpenCode allowed and supported? Because disallowing use of subscription in other apps was the reason for me to cancel my Claude subscription.\n\nSince GitHub Copilot and GPT Plus/Pro officially support OpenCode, I really hope mistral does the same.",
          "score": 9,
          "created_utc": "2026-01-27 17:19:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22qzjf",
              "author": "DueKaleidoscope1884",
              "text": "this would be an important factor for me too\n\n  \nDoes Mistral typically reply to the questions in this sub?",
              "score": 2,
              "created_utc": "2026-01-27 19:43:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2776tq",
              "author": "vienna_city_skater",
              "text": "You can just use the API, no need to go through the CLI.",
              "score": 0,
              "created_utc": "2026-01-28 11:59:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27c299",
                  "author": "deegwaren",
                  "text": "But is API usage included in the Pro subscription?",
                  "score": 1,
                  "created_utc": "2026-01-28 12:33:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21mzy0",
          "author": "cosimoiaia",
          "text": "Unfortunately that broke completely the local functionality of it. \n\nI was using devstral-24b with local endpoints and after the update it's unable to do anything at all. ðŸ˜¢\n\nDoes this mean that we can't use local models anymore?",
          "score": 8,
          "created_utc": "2026-01-27 16:51:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21nxqx",
              "author": "SourceCodeplz",
              "text": "Probably a bug, in Mistral Vibe they said from the start it should support local deployments of LLMs.",
              "score": 7,
              "created_utc": "2026-01-27 16:55:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o22ikam",
                  "author": "cosimoiaia",
                  "text": "Yeah, I agree. I will debug it more to see what's going on better but switching back to the old version worked as before.",
                  "score": 1,
                  "created_utc": "2026-01-27 19:06:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o21qfj1",
                  "author": "Downtown-Elevator369",
                  "text": "Yeah, the local option is still there under /model for me",
                  "score": 1,
                  "created_utc": "2026-01-27 17:06:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26zzgr",
              "author": "pandora_s_reddit",
              "text": "Hi there - It should be allowed, the team is taking a look but feel free to open an issue if you did not yet. You are free to use local models or any provider.",
              "score": 3,
              "created_utc": "2026-01-28 11:03:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27ftqy",
                  "author": "cosimoiaia",
                  "text": "Thank you for answering!\n\nStrangely enough I did a fresh reinstall and it started working also with the v2.0.0. \n\nMaybe something causes some conflict when just updating and the local model doesn't use any tool call.\n\nIn any case, thank you for the great work, I really love vibe!",
                  "score": 1,
                  "created_utc": "2026-01-28 12:57:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o22hqum",
              "author": "cosimoiaia",
              "text": "I switched back to v1.3.5 and everything started working again with the exact same setup, so it's definitely something with the new release.",
              "score": 1,
              "created_utc": "2026-01-27 19:02:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27jder",
          "author": "NiceTryAmanda",
          "text": "so the pro plan comes with some usage built in? I generated an api key and /terminal shows that I'm spending money, though I have a pro plan",
          "score": 3,
          "created_utc": "2026-01-28 13:18:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2abdqv",
              "author": "Salt-Willingness-513",
              "text": "username checks out",
              "score": 1,
              "created_utc": "2026-01-28 20:52:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21i3f5",
          "author": "fxdev1",
          "text": "Can someone already say something about the usage quota compared to codex, gemini cli, claude code, antigravity?",
          "score": 6,
          "created_utc": "2026-01-27 16:30:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26mlb0",
              "author": "Gen5nake",
              "text": "Since I started testing yesterday, the quota seems much larger than with others. Yesterday, I did hit some API rate limits, but I could continue after a few minutes. Today, Iâ€™ve already been coding non-stop for 1.5 hours and havenâ€™t hit any limits yet. Also the context seems to fill up more slowly compared to Claude.  \nSo far it's great!",
              "score": 1,
              "created_utc": "2026-01-28 09:02:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o239d95",
          "author": "909876b4-cf8c",
          "text": "Is the user's input and data used for training, when using this through Le Chat Pro subscription?",
          "score": 2,
          "created_utc": "2026-01-27 21:06:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fsdoo",
              "author": "pandora_s_reddit",
              "text": "No, we dont train on it ! You can use freely without worrying about data when using a pro subscription.",
              "score": 2,
              "created_utc": "2026-01-29 16:42:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21khjy",
          "author": "Downtown-Elevator369",
          "text": "I just paid for a Pro plan this morning. This is great! Edit: I ran this to upgrade my existing Vibe setup on Mac: uv pip install --upgrade vibe",
          "score": 4,
          "created_utc": "2026-01-27 16:40:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21h6ut",
          "author": "EzioO14",
          "text": "Amazing news ! Canâ€™t wait to test it out",
          "score": 2,
          "created_utc": "2026-01-27 16:26:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21lstg",
          "author": "Hofi_CZ",
          "text": "Is possible to use devstral via Kilo code as part of the Pro plan?",
          "score": 1,
          "created_utc": "2026-01-27 16:46:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21syb6",
              "author": "nordenstrom",
              "text": "Yes, I've been doing that for a while.",
              "score": 1,
              "created_utc": "2026-01-27 17:17:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mx0wb",
                  "author": "vienna_city_skater",
                  "text": "Do you set the User Agent in the request header? API calls arenâ€™t free per se, but the only difference I see looking at the code is the User Agent header.",
                  "score": 1,
                  "created_utc": "2026-01-30 17:16:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o261j2r",
          "author": "Old-Glove9438",
          "text": "Hope itâ€™s better than Codex with GPT-5.2 high",
          "score": 1,
          "created_utc": "2026-01-28 06:03:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mx3mi",
              "author": "vienna_city_skater",
              "text": "Unfortunately not, and Codex is unfortunately not as good as Opus",
              "score": 1,
              "created_utc": "2026-01-30 17:16:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26xvkw",
          "author": "ProdbyTwoFace",
          "text": "Love your models for local inference so definitely gonna try that.",
          "score": 1,
          "created_utc": "2026-01-28 10:45:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a8wou",
          "author": "Sarttek",
          "text": "I'm new to this so sorry for asking but how exactly pricing works? I'm Pro Le Chat buyer, I downloaded the program and noticed that on the right lower corner there is a tokens counter. I assume that because of my pro sub my base token pool is higher? Or do I have to pay for it separately? What will happen if I use all of that? When will it renew?\n\nI tired looking for FAQs but nothing in there really answers my questions",
          "score": 1,
          "created_utc": "2026-01-28 20:41:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2b9flp",
              "author": "Gen5nake",
              "text": "The token counter you see is the context size usage for your current chat session, not your API usage or token balance. It indicates how much of the conversation history is being kept active in this chat. ;)",
              "score": 1,
              "created_utc": "2026-01-28 23:29:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2hdep9",
          "author": "VaginosiBatterica",
          "text": "Hey u/Clement_at_Mistral  can you add som sort of \"study\" mode to le chat?",
          "score": 1,
          "created_utc": "2026-01-29 21:06:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo7xoz",
      "title": "ðŸ˜‚",
      "subreddit": "MistralAI",
      "url": "https://i.redd.it/bsesmuvrbrfg1.jpeg",
      "author": "mobileJay77",
      "created_utc": "2026-01-27 08:10:53",
      "score": 187,
      "num_comments": 5,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qo7xoz/_/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1zru53",
          "author": "Nefhis",
          "text": "Meanwhile, Mistral... ðŸ¤£\n\nhttps://preview.redd.it/h4d8kw56evfg1.png?width=484&format=png&auto=webp&s=52cb4b2667463298c51071ff332a8d2d9fe55473",
          "score": 23,
          "created_utc": "2026-01-27 10:36:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zl238",
          "author": "Wickywire",
          "text": "Claude even has the puckering animation ðŸ˜…",
          "score": 8,
          "created_utc": "2026-01-27 09:34:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f6j8n",
          "author": "Ejbarzallo",
          "text": "Only Mistral is the cat's head. And Le Chat literally means The Cat",
          "score": 3,
          "created_utc": "2026-01-29 15:04:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zt5ba",
          "author": "whoisyurii",
          "text": "LechÃ¡ ðŸ“¢",
          "score": 2,
          "created_utc": "2026-01-27 10:47:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2w2zfv",
          "author": "BiboxyFour",
          "text": "1. Prolapsed anus\n2. HPV\n3. Nice and tight\n4. Anal fissure",
          "score": 1,
          "created_utc": "2026-02-01 01:02:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqj58f",
      "title": "Hi Mistral AI - you rock more than you know - lets take it further",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qqj58f/hi_mistral_ai_you_rock_more_than_you_know_lets/",
      "author": "hartmanners",
      "created_utc": "2026-01-29 19:55:08",
      "score": 159,
      "num_comments": 12,
      "upvote_ratio": 0.99,
      "text": "I've been using AI since it sprung out as part of my job. I am a European and the hype has been overseas most of time (OpenAI, Claude, Gemini as the serious products).\n\n  \nI trust Mistral more than the others when matters are crucial. You could still re-train the model a bit more when it comes to technical issues, but it is doing a great job.\n\n  \nThese other LLM models, except Claude maybe, lack what Mistral is bringing to the table in terms of sources trained on and system prompts taiming it. It's objective and not biased.\n\n  \nMistral, please prioritize making a Windows store app, MacOS app to get this ball rolling faster. It's easier to get my crowd going with these simple means and thereby spreading the good work you did. Ppl need convenience - you already deliver on quality.\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qqj58f/hi_mistral_ai_you_rock_more_than_you_know_lets/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o2h6squ",
          "author": "Quiet_Illustrator410",
          "text": "Mistral AI is top-notch and from EU, perfect combo!",
          "score": 28,
          "created_utc": "2026-01-29 20:34:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hk8fo",
          "author": "Suitable-Guest2781",
          "text": "Fully agree, daily (& happy) user of Mistral AI here ! \n\nWould really love to see a collab. with Kyutai (https://kyutai.org) for next level voice interactions with LeChat !",
          "score": 9,
          "created_utc": "2026-01-29 21:38:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2idku5",
              "author": "EveYogaTech",
              "text": "Oh congratulations on the funding from Iliad Group. That's amazing. I'd also be interested in a Collab with you in the long-term with /r/Nyno (EU-only platform + commercial friendly open-source n8n alternative for AI workflows)",
              "score": 1,
              "created_utc": "2026-01-30 00:09:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ipjyy",
          "author": "Wrong_Country_1576",
          "text": "I love LeChat. It's brilliant and has a great personality.",
          "score": 4,
          "created_utc": "2026-01-30 01:13:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ifetz",
          "author": "ShiroCOTA",
          "text": "I ditched ChatGPT the moment I learned they massively funded Trumpâ€˜s fascist MAGA cult and switched to LeChat. Happy ever since. The only things I miss here are the excellent voice chats and the live-video chat feature to ask the AI about the stuff it sees irl.",
          "score": 10,
          "created_utc": "2026-01-30 00:18:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p7x38",
              "author": "ziplin19",
              "text": "Today i cancelled ChatGPT because they fully support Trumps fascism, i will subsribe to LeChat",
              "score": 4,
              "created_utc": "2026-01-30 23:48:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2j5ek8",
              "author": "Gamester1941",
              "text": "Same! Though I do miss the intuitive memoru feature amd tts feature (also mistral could turn up the temprature some so when I regenerate its not the samw thinf?) But those are small gripes. Love mistral",
              "score": 1,
              "created_utc": "2026-01-30 02:41:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2icacu",
          "author": "909876b4-cf8c",
          "text": "Please don't prioritize US commercial, closed-sourced, restrictive, controlling, privacy invasive operating systems. Europe is switching to Linux, please focus on that.",
          "score": 5,
          "created_utc": "2026-01-30 00:01:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iexyi",
          "author": "Hekke1969",
          "text": "Windows app ?? Hell no",
          "score": 3,
          "created_utc": "2026-01-30 00:16:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t63vi",
          "author": "destello89",
          "text": "You can add it to your desktop which works similarly to how the app would. You should give it a go.",
          "score": 1,
          "created_utc": "2026-01-31 16:16:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2u6raa",
              "author": "hartmanners",
              "text": "You mean add the iOS app to macOS?",
              "score": 1,
              "created_utc": "2026-01-31 19:10:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2inr7i",
          "author": "Equivalent-Word-7691",
          "text": "I downloaded Mistral one year ago,yet I sturggle to really support it if nayhting becuas ethe modle are just REALLY infeior  to gemini ,Open Ai adne speically Claude..like the real only good quality is it's an european company ut even chinese's model are better ...",
          "score": -2,
          "created_utc": "2026-01-30 01:03:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmg70d",
      "title": "Move to Mistral",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qmg70d/move_to_mistral/",
      "author": "InternalBroad2522",
      "created_utc": "2026-01-25 10:57:42",
      "score": 144,
      "num_comments": 19,
      "upvote_ratio": 0.98,
      "text": "Currently I am using ChatGpt pro, Codex and GitHub Copilot, however I would like to switch to European provider or open source projects due to the critical situation with US. In your opinion, which are the best services I should use to do the switch I want?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qmg70d/move_to_mistral/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1mg6dg",
          "author": "thedisturbedflask",
          "text": "I'd suggest also using Mistral and other services to help refine a custom 'system prompt' for the Mistral's Le Chat to be more in line with what you need.\n\n\nI was initially a bit worried that i just wasn't getting the value out of chat compared to other services but creating my own instructions helped a lot with development especially but also day to day usage.\n\n\nDevstral2 vibe is also good but if your used to having it in an ide then the cline.bot vscode extension seems to work well",
          "score": 18,
          "created_utc": "2026-01-25 14:35:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sj0le",
              "author": "GreenStorm_01",
              "text": "Any hints on proper system prompting? It probably isn't helpful to just copy my customisation from chatgpt over, right?",
              "score": 1,
              "created_utc": "2026-01-26 09:48:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1vvh9g",
                  "author": "thedisturbedflask",
                  "text": "It's a good starting point.\n\n\nIf you're happy with chatgpt's responses you can ask it to help define a starting system prompt with the characteristics you need and set it as the instructions or agent prompt in Mistral.\n\n\nThen in Mistral you can ask a question that fits your use case repeatedly and you can then tweak the system prompt as you go.",
                  "score": 2,
                  "created_utc": "2026-01-26 20:18:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1llo8g",
          "author": "whoisyurii",
          "text": "mistral, mistral vibe or codestral + ollama\n\n**edit: devstral",
          "score": 10,
          "created_utc": "2026-01-25 11:07:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1lpvjl",
              "author": "cosimoiaia",
              "text": "Yes, except not ollama (too shady, buggy, bad software), better lmstudio or Jan. \n\nAlso the latest is called Devstral ðŸ™‚",
              "score": 9,
              "created_utc": "2026-01-25 11:43:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1mwf31",
                  "author": "guyfromwhitechicks",
                  "text": "What's buggy about ollama?",
                  "score": 0,
                  "created_utc": "2026-01-25 15:53:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1sj6uz",
                  "author": "razziath",
                  "text": "Compared to ollama, lmstudio is very slow.  \nOllama is good. Ollama with Anythingllm is a very good option is you want a UI and add connectors/mcp/agents... to your llm.",
                  "score": 0,
                  "created_utc": "2026-01-26 09:49:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1mxh75",
          "author": "guyfromwhitechicks",
          "text": "This comment section being the equivalent of tumbleweeds really shows how big this problem is. You can look into /r/BuyFromEU and https://european-alternatives.eu/, they are ran by people who keep trying to solve \"what do I replace my american products with?\". The options are slim (especially for software) but it is getting better.",
          "score": 2,
          "created_utc": "2026-01-25 15:57:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1oahbu",
          "author": "UpstairsCheetah235",
          "text": "You could check out Protonâ€™s lumo. It uses a variety of models and thereâ€™s a free tier to try out. Might be a good solution, especially for those switching email and cloud storage over to them.Â ",
          "score": 1,
          "created_utc": "2026-01-25 19:25:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wps7q",
              "author": "cosimoiaia",
              "text": "Proton has indeed excellent offerings, however, they don't have their own model but rather run other open weights models. I haven't checked in a minute but afaik they don't disclose which one and that for me is a security issue. But I'm sure it will become rock solid given where it comes from.",
              "score": 1,
              "created_utc": "2026-01-26 22:34:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1v27f8",
          "author": "empireofadhd",
          "text": "I also switched, also switching to mailo, but have not figured out how to use their office suite.",
          "score": 1,
          "created_utc": "2026-01-26 18:13:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22dost",
          "author": "New-era-begins",
          "text": "if too many switches to european they will run out of inference resources. Thats why switch,  but only with sensitive tasks, and the BS chat put to US AI so they loose money on electricity. Dont ever pay anything for US",
          "score": 1,
          "created_utc": "2026-01-27 18:45:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mhvqb",
          "author": "thatHafuGirl",
          "text": "\"Critical situation in the US\" means what, exactly?",
          "score": 1,
          "created_utc": "2026-01-30 16:08:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uzuve",
          "author": "GreenGreasyGreasels",
          "text": "if it is open source you could use the best Chinese open weight model hosted by yourself or by a trusted vendor in EU. the usual suspects DS, K2, GLM-4.7, M2.1 etc. Devstral 2 and Mistral Large 3 remain top notch choices. If you consider Russia European you can look at Gigachat :P",
          "score": 0,
          "created_utc": "2026-01-26 18:03:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ymr69",
              "author": "dsvost",
              "text": "I would add also add then SourceCraft stuff in that case..",
              "score": 1,
              "created_utc": "2026-01-27 04:50:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o274u0j",
          "author": "officialexaking",
          "text": "Or use a european open source AI like https://www.xprivo.com where you can also run a small Mistral model locally on your computer fully offline",
          "score": 0,
          "created_utc": "2026-01-28 11:42:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq5gss",
      "title": "USD prices in EU, WTF?",
      "subreddit": "MistralAI",
      "url": "https://i.redd.it/qvhynsijq9gg1.png",
      "author": "anzzax",
      "created_utc": "2026-01-29 10:53:37",
      "score": 128,
      "num_comments": 18,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qq5gss/usd_prices_in_eu_wtf/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2e1v23",
          "author": "UnnamedUA",
          "text": "https://preview.redd.it/3tukzc1sr9gg1.jpeg?width=1080&format=pjpg&auto=webp&s=5533758cbc36bd83ade26edf1e0de9008ac53935",
          "score": 53,
          "created_utc": "2026-01-29 10:57:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e36hw",
              "author": "anzzax",
              "text": "https://preview.redd.it/wjkplopqt9gg1.png?width=2492&format=png&auto=webp&s=3d7010e6360d30ad65af24d6382fc0f63321f7ed",
              "score": 9,
              "created_utc": "2026-01-29 11:08:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2etmcg",
                  "author": "FalseRegister",
                  "text": "I am on that page, and it loaded EUR for me\n\nAre your local settings correct? Try on another browser or device as well",
                  "score": 6,
                  "created_utc": "2026-01-29 13:59:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2e3r24",
                  "author": "sndrtj",
                  "text": "Ah I guess they don't default back to USD for any location that doesn't use Euro.",
                  "score": 4,
                  "created_utc": "2026-01-29 11:12:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2e3bmw",
          "author": "OwlSlow1356",
          "text": "last year was only usd, a big turnoff. if you have the account already in place, you will see upgrade only in USD just like me.  if you log out, you will see these prices EUR/USD this year, but once logged in, again just USD. i think they have many costs in USD, bunny .net CDN also has pricing in USD, sorry, i will not pay in USD to any european company! there are so many options around, I did not bother to make a new account just to see if i can upgrade in EUR!",
          "score": 9,
          "created_utc": "2026-01-29 11:09:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e3uvf",
              "author": "anzzax",
              "text": "Yeah, my account created a long ago and I was using/testing API. I checked admin area, billing settings, all possible places - there is no way to switch to EUR, they show USD prices with Poland VAT - a bit infuriating honeslty.",
              "score": 2,
              "created_utc": "2026-01-29 11:13:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2eb9ne",
          "author": "anzzax",
          "text": "BTW ChatGPT has adjusted pricing and local currency. Those details matter. \n\nMistral - Iâ€™d like to see you build a strong business, not only focus on research. Iâ€™m happy to stay loyal and advocate for your models.\n\nhttps://preview.redd.it/wcooytvq4agg1.png?width=2888&format=png&auto=webp&s=b320b95b010a260fb60e8f3957d4cc11d2ce68a1",
          "score": 7,
          "created_utc": "2026-01-29 12:09:46",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2e9dmh",
          "author": "crazyserb89",
          "text": "If youâ€™re using VPN could be. Iâ€™m accessing from Italy and I see EUR primarily",
          "score": 3,
          "created_utc": "2026-01-29 11:56:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ejzf8",
              "author": "stddealer",
              "text": "I think it only supports euros and USD, and if you browse from any country whose currency is not Euros, it defaults to USD.",
              "score": 2,
              "created_utc": "2026-01-29 13:06:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ercu0",
                  "author": "radek432",
                  "text": "Which makes sense, because dollar is cheap now ðŸ˜‰",
                  "score": 2,
                  "created_utc": "2026-01-29 13:47:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gf8yp",
          "author": "Dadinek",
          "text": "I took a student subscription and voluntarily in USD with a VPN because it was cheaper than from Europe. I end up paying 6$ per month",
          "score": 2,
          "created_utc": "2026-01-29 18:25:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fhxaa",
          "author": "Duedeldueb",
          "text": "Polandâ€˜s CEO thinks Zloty is it.",
          "score": 1,
          "created_utc": "2026-01-29 15:56:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hzplc",
          "author": "Outside_Professor647",
          "text": "It also writes dollars when transcribing numbersðŸ¤®",
          "score": 1,
          "created_utc": "2026-01-29 22:54:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2jx7i9",
          "author": "deegwaren",
          "text": "Try this url: https://mistral.ai/pricing",
          "score": 1,
          "created_utc": "2026-01-30 05:33:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2s5a9g",
              "author": "petersemm",
              "text": "It loads both for me but in dollars is somehow cheaper even though exhange rate is 1 eur = 1.19 $",
              "score": 1,
              "created_utc": "2026-01-31 12:52:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2tu9sj",
                  "author": "DeweyQ",
                  "text": "Mathematics. This exchange rate makes prices in US dollars cheaper. A loaf of bread that needs 1 EUR to purchase it needs less than 1 USD if each USD can buy 1.19 loaves of bread.",
                  "score": 1,
                  "created_utc": "2026-01-31 18:12:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ktgym",
          "author": "Zerr0Daay",
          "text": "If you have a VPN on it may influence it",
          "score": 1,
          "created_utc": "2026-01-30 10:09:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrw16s",
      "title": "Mistral Vibe 2.0 vs Codex 5.2 & Claude (Opus 4.5) - First Impressions",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qrw16s/mistral_vibe_20_vs_codex_52_claude_opus_45_first/",
      "author": "l_eo_",
      "created_utc": "2026-01-31 07:34:06",
      "score": 119,
      "num_comments": 35,
      "upvote_ratio": 0.95,
      "text": "Spending most of my time on Claudeâ€™s Max 20x plan (Opus 4.5) and occasionally hitting my weekly limits, I decided to revisit **Mistral Vibe 2.0** and **Le Chat** after a long break. Here are my first impressions, especially compared to Opus 4.5 and Codex CLI.\n\n---\n\n### **1. Codex CLI (5.2), Underwhelming First Impressions**\n\nI tested Codex in a structured folder with lots of context (files, scripts, and dedicated system for specific workflows that are to be followed). On first prompts and even after instructing it to analyze the **folder context**, it struggled to understand or comply and generate based on an understanding of the surrounding context. It felt less like a true CLI tool and more like a system just inlining requests without grasping the environment. Big asterix: It's also my first time trying codex and I need to explore it more, but the first results were disappointing.\n\n---\n\n### **2. Mistral Vibe 2.0**\n\nReally great first impression, I was very pleasantly surprised!\n\nIt really stood out was how thoroughly Mistral Vibe 2.0 tried to understand the context first (even without being told to do so). It didnâ€™t just jump into answering; it checked the surrounding files, analyzed available examples, and tried to understand what a good outcome would look like before starting to work.\n\nAnd I was blown away by how **fast** Mistral Vibe 2.0 is. The response generation is so quick that I canâ€™t even read along as it outputs. This is a game-changer for feedback loops. While I of course believe that Claude Opus 4.5 is currently the king for coding and complex tasks, Iâ€™ll be testing Mistral Vibe 2.0 much more for coding and general tasks to see how it performs. For everyday structured tasks, Mistralâ€™s first impression suggests it could be a **fantastic alternative and fallback system**.\n\n---\n\n### **3. Le Chat App**\nThe Le Chat app has improved significantly since I last tried it:\n* Voice input is now a thing, and itâ€™s seamless! (Claude struggles a lot with this) When I last used Le Chat, this feature didnâ€™t even exist. I tested it with a long, multi-minute transcription, and the accuracy was impressive. I did a few feedback rounds, and Mistral applied my edits smoothly. Iâ€™m not sure how they made it *that* fastâ€”**the turnaround times for voice transcription and immediate answers are incredibly impressive**.\n* Online research seemed also to work great  and possibly now on par with ChatGPT?\nIâ€™m not even sure yet what other awesome features and UX additions there are, but Iâ€™m excited to explore further.\n\n---\n\n### **4. Potential Switch?**\nGiven Mistralâ€™s speed, UX, and awesome voice transcription, etc, Iâ€™m seriously considering making Le Chat + Mistral Vibe 2.0 my daily driver for everyday tasks and as a fallback when Claudeâ€™s limits kick in (and Le Chat possibly always preferred because of the speed and the great voice transcription in multiple languages). Iâ€™ll test-drive it for a few days, and if it proves as powerful as I hope (writing this as a \"wow, just tried this\" post, so maybe lots of honey moon phase involved), I might cancel my ChatGPT subscription and make Le Chat my main driver for everyday use.\n\nIâ€™m all for EU digital sovereignty, so supporting Mistral feels like a win-win and I am incredibly happy & excited about all the progress being made.\n\n---\n\n**TL;DR:**\n\n* **First rough impressions**: Just excited to share my initial enthusiasm for Mistral Vibe 2.0 and Le Chat after revisiting them.\n* **Codex CLI** felt underwhelming as a CLI tool, especially with folder context. Not sure about code quality and similar yet (also very first impression).\n* **Mistral Vibe 2.0** impressed with speed and context handling; Iâ€™ll test it more for coding.\n* **Le Chat app** now includes voice input and shines with great transcription quality, incredible speed (game changer for back and forth loops), and online research\n* While **Opus 4.5 likely remains unmatched for coding**, Mistralâ€™s first impression suggests it could be a **fantastic alternative and fallback system** for everyday tasks.\n\n\nBig thank you to the Mistral team for all the hard work! Rooting for you big time â™¥ï¸ðŸ‡ªðŸ‡º!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qrw16s/mistral_vibe_20_vs_codex_52_claude_opus_45_first/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o2r8s37",
          "author": "l_eo_",
          "text": "Collecting new findings here as I continue testing: \n\n* I am not super happy with it being pretty hard to copy from the CLI progress. Often I need to \"reply\" to something the agents do and the UX loops around that are crucial. It's pretty bad at the moment, the UI needs a long time to react and seems to include some automatic \"copied!\" feature, that appears very late multiple times and that I don't really care about. Easy direct select -> right click -> copy would be best and should be fast and fluid. That's a \"small\" thing that makes a **huge** difference. \n* Scrolling can also be pretty sluggish (very very late start and keys like 'end' also have many seconds of delay). Possibly tracked here: https://github.com/mistralai/mistral-vibe/issues/222\n* There must be a specific system in place for interruptions? The agent reacted much faster than Claude Code Opus to a \"no\" during generation. Basically almost immediate stop\n* CLI text input seems a bit sluggish when much is happening\n* Context usage seems to go up fairly slowly. Not sure if that's a good or bad thing (testing it for some coding right now).\n* Every response / input during generation seems to be treated as an interruption (so no \"next task queuing\" like with Claude Code). Maybe there is a way to queue?",
          "score": 7,
          "created_utc": "2026-01-31 07:59:41",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2rboh5",
              "author": "l_eo_",
              "text": "I really need to stress that so many gains are to be made \"easily\" by improving the UX of the base-line elements that make up the workflow with the CLI tool. \n\nCopy & paste / reacting / replying, how fast typing is, and how responsive scrolling is are just a few facets of this.\n\nUX improvements make a huge difference if the basic quality of the model output is good enough.\n\nI will look into contributing: https://github.com/mistralai/mistral-vibe/blob/main/CONTRIBUTING.md",
              "score": 9,
              "created_utc": "2026-01-31 08:26:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2r7wv2",
          "author": "ZapojKabel",
          "text": "I am interested at the coding using now heavily modified Gemini CLI and quite happy with, but rather use European tool. Does mistral have api key pay as you go and picture generate?",
          "score": 4,
          "created_utc": "2026-01-31 07:51:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2r9lfq",
              "author": "l_eo_",
              "text": "Interesting, in what way did you modify the Gemini CLI? \n\nI must say, that I was always very unhappy with Gemini output regarding code and it often seeming to \"fight\" me / gaslight me or somehow trying to avoid following instructions. Especially in the last weeks instruction following was pretty much non-existent for the few tests I did and I have been avoiding it ever since. \nBut I also mostly used the web interface so far, so maybe it's a different story with the CLI. \n\nI would have very much recommended Claude Code with Opus 4.5 and used to be *insane* around release / December, but has deteriorated massively in terms of quality the last few weeks. Possibly due to a switch from Google GPUs to the ones they developed with AWS, but not sure.  \nStill, it is very much worth a try if you go for the 5x or 20x max plans (they are very worth it). API pricing would be incredibly expensive (easy to burn through 20-80 bucks an hour). \n\nRegarding your questions: \nYes, API key pay as you go exists and I think the subscription now also included some vague notion of \"full day coding\"? \n\nPicture generate would likely mean an additional model, but that should be easy to integrate into your workflow (e.g. continue using Gemini's very powerful nano-banana-pro or flux).\n\nGreat to hear that you care about European tools!",
              "score": 3,
              "created_utc": "2026-01-31 08:07:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2rngmy",
                  "author": "ZapojKabel",
                  "text": "Well I have my boss I call him Nexus he delegate my task to other agents ( I use mostly three for design, code and advertising expert). Every plan I make all 3 discuss how to do and should work, when it is done the 4 agent step in it is called Sentil \"devil advocate\" a trying to find error in there reasoning and aks question \"if...\". When there settle and I agree, Nexus make track list in conductor with plan how to setup. Also use RAG. I am not coder but make app for my e-shop. https://harmony.nonchalant.cz and also now making app for manager stuff on my eshop and dashboard for better statistic on sale etc.\nEdit: also Avery agent must use TOT protocol",
                  "score": 3,
                  "created_utc": "2026-01-31 10:19:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ul5uq",
              "author": "vienna_city_skater",
              "text": "If you have Le Chat Pro you get Devstral included.",
              "score": 1,
              "created_utc": "2026-01-31 20:20:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2recbg",
          "author": "EzioO14",
          "text": "I use le chat pro for all normal chat questions and I have claude max for coding",
          "score": 5,
          "created_utc": "2026-01-31 08:51:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2reopc",
              "author": "l_eo_",
              "text": "That's my plan as well :)\nI love the snappyness of Le Chat. \n\nIs Le Chat strict enough with sources? So doesn't hallucinate if no information found via web search?",
              "score": 2,
              "created_utc": "2026-01-31 08:55:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2rer8d",
                  "author": "EzioO14",
                  "text": "I find it gets better, I was very disappointed at first, last month when I started but now I find the answers better and better",
                  "score": 2,
                  "created_utc": "2026-01-31 08:55:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2teisq",
          "author": "Mr_Finious",
          "text": "As a US citizen, I'm considering Mistral because I trust the EU's privacy protections to better protect me than those of US or Chinese companies. \n\n\n\nWe should all be clapping for companies like Mistral in the EU.",
          "score": 4,
          "created_utc": "2026-01-31 16:56:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r8a66",
          "author": "Bato_Shi",
          "text": "Only thing i noticed with Vibe is that sometimes it falls into infinite loops, like gemini 3 some months ago",
          "score": 3,
          "created_utc": "2026-01-31 07:55:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2r9n86",
              "author": "l_eo_",
              "text": "Should be easy to break it out of though, right? \nHow often does it happen?",
              "score": 1,
              "created_utc": "2026-01-31 08:07:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2syc21",
                  "author": "kiwibonga",
                  "text": "An example I had yesterday (Devstral Small 2 Q4) is it found that a regular expression wasn't capturing strings properly in a test, so it tried to fix the pattern, but it turned out to be a basic escaping issue rather than an error in the regex. It didn't realize that, so it added more and more characters to the pattern until it was huge -- without ever thinking \"is this overkill? That looks weirdly huge\" even though the purpose of the code was just to make sure a line starts with the word \"AGENT:\".\n\nThis is something that could have been autonomously fixed with an orchestrator that includes failure analysis, sees the blind spot, and proposes an alternate approach before launching another attempt. A reasoning model might also have a much easier time course-correcting.\n\nI was using the CLI by itself so I just said \"try to simplify the regular expression by writing it from scratch, and if it still fails, just write a parser\" -- it failed fast and wrote a parser, then got back to work.",
                  "score": 1,
                  "created_utc": "2026-01-31 15:39:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rpkx3",
              "author": "Gen5nake",
              "text": "I also had this issue once",
              "score": 1,
              "created_utc": "2026-01-31 10:39:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ra7pr",
          "author": "theschiffer",
          "text": "Very interesting review. What does your day-to-day workload look like and in what ways do you generally use AI?",
          "score": 2,
          "created_utc": "2026-01-31 08:13:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rarwz",
              "author": "l_eo_",
              "text": "Workload in terms of volume of usage? \n6-14 hours of multiple terminal tabs with claude processes. \nAI is used as a partner both for planning as well as for execution. Often planning and getting everything prepared for implementation takes up most of the time (e.g. 4 hours planning and adapting and architecture work, 2 hours implementing and adapting / fixing), but it depends a bit on what phase a project is in. \n\nDoes this answer your question fully?",
              "score": 3,
              "created_utc": "2026-01-31 08:18:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2tmt2r",
                  "author": "theschiffer",
                  "text": "Nice. So essentially, youâ€™re a software developer/architect? Thatâ€™s quite an intensive level of use.",
                  "score": 1,
                  "created_utc": "2026-01-31 17:36:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2rae0d",
          "author": "Hot_Bake_4921",
          "text": "Does Le chat still use mistral medium 3.1?",
          "score": 2,
          "created_utc": "2026-01-31 08:14:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rcns6",
              "author": "l_eo_",
              "text": "I am trying to find some information about this, but this proves to be surprisingly hard? Nothing in the change logs and most Reddit discussions about this seem to be from quite long ago.",
              "score": 2,
              "created_utc": "2026-01-31 08:36:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tqeke",
          "author": "Sorry_Role_1701",
          "text": "What Iâ€™m noticing with newer models like Mistral isnâ€™t about benchmarks anymore.\n\nThe real difference is **i**nteration speed, how fast you can refine an idea without restarting context every time.\n\nIn workflow-heavy tasks, that matters more than raw output quality.",
          "score": 2,
          "created_utc": "2026-01-31 17:54:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wlr2d",
              "author": "mWo12",
              "text": "Off course they are not about benchmarks, as they have nothing to show.",
              "score": 1,
              "created_utc": "2026-02-01 02:55:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2r7fxx",
          "author": "l_eo_",
          "text": "As I wrote, I am completely newly back again with Mistral vibe. \nAny optimizations / settings I should immediately go for?",
          "score": 1,
          "created_utc": "2026-01-31 07:47:20",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2r85ap",
          "author": "stjepano85",
          "text": "It is very good for agentic development. It has some problems with recursive algorithms and it can go crazy when his context is large (this can be solved by limiting context in vibe configuration).",
          "score": 1,
          "created_utc": "2026-01-31 07:53:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rayhx",
              "author": "l_eo_",
              "text": "Interesting! \nWhat level do you recommend limiting context to?",
              "score": 1,
              "created_utc": "2026-01-31 08:19:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2rcdgm",
                  "author": "stjepano85",
                  "text": "128k should be good. I noticed at 70+% of the default 200k its performance drops significantly.",
                  "score": 2,
                  "created_utc": "2026-01-31 08:33:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2rcps1",
          "author": "nycigo",
          "text": "For voice input, press Windows + H; this will automatically transcribe your microphone input if you are using Windows 11.",
          "score": 1,
          "created_utc": "2026-01-31 08:36:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rcu1q",
              "author": "l_eo_",
              "text": ":O \n\nIn the CLI for mistral vibe?",
              "score": 2,
              "created_utc": "2026-01-31 08:37:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2rhxqy",
                  "author": "nycigo",
                  "text": "Anywhere on Windows, anywhere your mouse is located",
                  "score": 1,
                  "created_utc": "2026-01-31 09:26:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2wll72",
          "author": "mWo12",
          "text": "I tested Mistral Vibe 2.0, but I found it so slow, and I'm not talking about token generation. Just the CLI interface is so slow and sluggish. Maybe because it is all python, unlike other agents.",
          "score": 1,
          "created_utc": "2026-02-01 02:54:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xo0zs",
              "author": "l_eo_",
              "text": "100% and this is my biggest issue with it right now and a real blocker. \nYou can see the kind of potential it has when starting a new session, but it gets bogged down fast. \nFor me personally these would a \"don't release before fixed\".  \n\nBut I am also very optimistic that this will be improved soon, because relatively speaking, these rendering issues should be quite straightforward to fix, so I really hope Mistral will make them a priority.",
              "score": 1,
              "created_utc": "2026-02-01 07:36:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qqzac4",
      "title": "Switching to Mistral?",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qqzac4/switching_to_mistral/",
      "author": "Absjalon",
      "created_utc": "2026-01-30 07:52:59",
      "score": 89,
      "num_comments": 28,
      "upvote_ratio": 0.98,
      "text": "Hi Mistral users,\n\nI am strongly considering switching my OpenAI subscription to Mistral.  I'm  happy with OpenAIs products,  but for political and GDPR reasons I'm ready to switch. Even if it means less optimal product. \n\nI've tried the free Mistral version for a while now and I am pretty happy about it, but it's not quite at the level of the paid OpenAI models. \n\nCan someone share their experience with the difference between the paid Mistral and OpenAI and how to optimize/personalize Mistrals output?\n\nI work both with API interactions and the chat interface \n\nThank you",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qqzac4/switching_to_mistral/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o2kgooi",
          "author": "thedisturbedflask",
          "text": "The Mistral models are quite capable but ive found you'll have to really work on the instructions and prompt to get the output your looking for.\n\n\nMy process was in defining a starting prompt and then tweaking it with a control question until it reached a point i was happy with. The answers arent deterministic because of how llms work but it helps to see the general kind of response.\n\n\nI saved these as agents in chat which works quite well.\n\n\nFrom the dev perspective i haven't quite been able to have it refer to an instruction file consistently but might just be missing something.",
          "score": 13,
          "created_utc": "2026-01-30 08:12:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kiyuj",
              "author": "Absjalon",
              "text": "Thank you.   \n  \nSo in Mistral chat, you build an agent and then call it in different chats? e.g. an agent that helps to formulate emails, but stays close to your draft and your wording?\n\nAs opposed to making a project and then giving special instructions for project?",
              "score": 5,
              "created_utc": "2026-01-30 08:33:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2kjjpe",
                  "author": "TheMrLexis",
                  "text": "In Le Chat if I remember well, you can call an agent in the same chat by writting \"@\\[Agent Name\\]\"",
                  "score": 6,
                  "created_utc": "2026-01-30 08:38:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2kkv4s",
                  "author": "thedisturbedflask",
                  "text": "That's correct for the agents, i refine it to 'think' the way id like for general interactions or make it more specifically useful like the email example.\n\n\nI think you definitely can define the instructions for an agent to include project ownership, requirements, milestones, etc and skill sets so that youd be able to ask it to complete the project goals etc\n\n\nIn my case for a dev project using zed.dev editor I do that and spend a lot of time back and forth with the model defining the scope and detail then have it output a specific plan.md file with a checklist.\n\n\nI include it as context and ask it to implement the plan, then inevitably fix what it doesnt quite get right, makes it easier to predict and test the output",
                  "score": 2,
                  "created_utc": "2026-01-30 08:50:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2l8jh3",
          "author": "Vontaxis",
          "text": "I have a mistral subscription just to support them but to be honest it is nowhere as good as Claude or ChatGPT.\n\nNaturally, it depends on what you're using it for but even for simpler things I noticed that prompt adherence is at times rather bad.",
          "score": 4,
          "created_utc": "2026-01-30 12:11:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kqv4z",
          "author": "NullSmoke",
          "text": "Yeah... new 4o scare going on over there (including 4.1 this time if I understand it correctly?)...\n\nJumped over when the whole mess became unbearable, and is very happy with it. As for the diff between paid and free, it's rate limits basically. You can test it out in free and have a decent handle on what you can expect in pro.\n\nI see that agents are being discussed down here, basically CustomGPTs if talking OpenAIspeech. You can find several guides over at r/Nefhis_Lumen_Lab that can help you get started :-)",
          "score": 4,
          "created_utc": "2026-01-30 09:45:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mmwih",
          "author": "LewdManoSaurus",
          "text": "I tried a subscription last October and while it was okay, I had to do a lot of correcting, and the amount of hallucination was seriously a deal breaker. If you just use it sparingly it'll probably be fine, but in my experience It's nowhere close to some of the bigger models. The agent customization is an amazing feature, but the other issues are so frequent that these days I just stick to free tiers of other services. I only used Mistral for generative writing.",
          "score": 3,
          "created_utc": "2026-01-30 16:31:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lko4e",
          "author": "mythrowaway4DPP",
          "text": "Mistral *is* not on the same level as the top models ChatGPT, Gemini or Claude.  \nBeing an European engine, GDPR, and less puritan censorship is nice.\n\nCapabilities wise, think ChatGPT 4.1, maybe better.\n\nPrompting needs to be more precise, but it is doable.\n\nThe libraries are a nice idea, collections of documents to attach to any chat.\n\nOverall, I am not missing a lot using mistral, and openAI isn't getting my money anymore.",
          "score": 8,
          "created_utc": "2026-01-30 13:27:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mlk85",
              "author": "alexgduarte",
              "text": "That's because you've never used GPT-5.2-Pro.",
              "score": -5,
              "created_utc": "2026-01-30 16:25:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mm1rw",
                  "author": "mythrowaway4DPP",
                  "text": "Of course I did. I also said that mistral is not there.",
                  "score": 5,
                  "created_utc": "2026-01-30 16:27:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2o897h",
          "author": "Komgoroth",
          "text": "I used it in the past. Then I switched to ChatGPT for a while. When my Chat GPT subscription was about to expire I subscribed again to Mistral.\n\nI then had some health issues and when I got my report from the doctor, I used it on Mistral to make a summary of what is happening( it was all perfect other than some benign findings) and it made up certain findings which indicated severe issues.\n\nI panicked as the doctor did not mentioned them. I also did not see them. Then asked the AI to tell me where did it find the issue and it apologized.\n\nTried it again and it did the same horrible mistake. Chat gpt did not.\n\nExample two. I asked both Mistral and ChatGPT to make me the shortest roadtrip using highway only from point a to point b and to tell me what vignettes I need to buy and what's the expected time and how often and where I should take breaks.\n\n\nChat gpt did it perfectly and I confirmed myself on maps etc.\nMistral missed several countries when it comes to vignettes, miscalculated the time by more than 30% and suggested that no breaks are needed for the drive( on a 14 hour long drive).\n\nI unsubscribed and continued with Gemini.\n\n\nI love that it is an European company and I'll support them when they get better but I cannot use it as is.",
          "score": 3,
          "created_utc": "2026-01-30 20:49:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pn7eo",
          "author": "LoadZealousideal7778",
          "text": "I like it but probably for a weird reason. For me, their models sit at the junction between capable and a useful but fundamentally stupid multi tool. Smart enough to do work, not smart enogh to cognitively offload to.\n\nYou can't just barf in a typo riddled, half formed thought and get what you wanted most of the time like with Claude Opus. Bit more manual.",
          "score": 3,
          "created_utc": "2026-01-31 01:13:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rhkj9",
              "author": "Absjalon",
              "text": "I understand this and have a similar experience",
              "score": 1,
              "created_utc": "2026-01-31 09:22:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2kp7dc",
          "author": "sam-watterson",
          "text": "I started switching it, using vibe for day-to-day usage.",
          "score": 2,
          "created_utc": "2026-01-30 09:29:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2omirx",
          "author": "theAbominablySlowMan",
          "text": "Even before all this US shite started I was scolding everyone I heard using other AI, the data privacy approach of US Vs EU countries is just not compatible, and the level of detail of your personality you give these things is just scary in the wrong hands.Â \n\n\nHonestly I suggest just ditching and making the switch for 3 months and figuring it out yourself, like all software we get tied to what we know. Without knowing what areas you rely on it for I can't be specific but it's not a black and white openai being better, I prefer mistrals response on a lot of more science based topics for exampleÂ ",
          "score": 2,
          "created_utc": "2026-01-30 21:57:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rhgzf",
              "author": "Absjalon",
              "text": "Yes.  I've noticed Mistral gives really good answers on Statistical issues (explaining concepts). In this area, I think it is on par with chatGPT 5.2",
              "score": 1,
              "created_utc": "2026-01-31 09:21:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rac2w",
          "author": "Born-Yoghurt-401",
          "text": "Iâ€˜ve been using LeChat free over the last year to track progress and get medicinal advice about a close relatvies glioblastoma stage 4 brain tumor. I shared overall progress including CT and MRI scans, mental health patterns and in the final weeks palliative and necrotic wound care details. LeChat was very helpful in collecting and aggregating health data and putting many of my questions into context. I had no issues with hallucinations or factual errors and felt in good hands. I also would never have shared any of those details with a US based AI solution.",
          "score": 2,
          "created_utc": "2026-01-31 08:14:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rryl0",
          "author": "WitnessEarly7584",
          "text": "I got a Pro subscription for Mistral for free and tried to input some of my ChatGPT/Gemini prompts, which I use for work. What can I say? It is totally unusable. One prompt even caused a recursive loop. How are you guys using it?",
          "score": 2,
          "created_utc": "2026-01-31 11:01:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rzm87",
          "author": "crazyserb89",
          "text": "I'm also on OpenAI searching for an alternative and checking the Mistral. Gave it a shot several times, but it seems it's not there yet to compete with the big ones. It feels unpolished, lacking some fundamental features, and overall seems like a Beta product. I hope they gonna improve it in future and therefore position themselves better on the market though.",
          "score": 2,
          "created_utc": "2026-01-31 12:08:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2l3kqp",
          "author": "MikadinShinjuk",
          "text": "I switched to kagi, is not European but is way better than all the other main services and is very flexible in terms of choosing the model",
          "score": 1,
          "created_utc": "2026-01-30 11:34:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ss499",
          "author": "fonceka",
          "text": "Mistral strategy is to develop vertical models, so they have a bunch of specific models: codestral (coding), devstral (open-source model for coding agents), voxtral (speech2text), mistral small (enterprise ready), mistral medium, mistral large, ministral, magistral (multilingual)â€¦ I have been on the Pro version for one year on, but I have also a paid subscription to Gemini. I have already dropped the OpenAI subscription, and consider dropping the Anthropic one also. I do not use the API anymore since my focus have shifted. Overall I find Mistral very useful, when context is adequate. You must really work on curating your context window neatly.",
          "score": 1,
          "created_utc": "2026-01-31 15:07:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ue2ml",
          "author": "Green-LaManche",
          "text": "I used le chat - in very specific area which difficult to find someone knowledgeable: I am pretty happy with the answer either when asked specifically about dealing breakdowns in highly sophisticated areas or details of very specific historical figures.\nI would say I am much happier then with copilot",
          "score": 1,
          "created_utc": "2026-01-31 19:45:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lix37",
          "author": "officialexaking",
          "text": "You said for 'political reasons'. What do you mean by that? If it is because you want your data to be stay in the EU and not handled by non-US tech companies then you are wrong with Mistral. All your requests (chats) are routed through Microsoft/Google and Cerebras unless you haven't concluded a personal enterprise contract with them.",
          "score": -1,
          "created_utc": "2026-01-30 13:17:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mjxpx",
              "author": "Absjalon",
              "text": "Very important information you bring to the table here.  My concern is  data wise, but also I want my money to support Europe.\n\nI will see if I can find more information about this",
              "score": 3,
              "created_utc": "2026-01-30 16:17:54",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2mbua7",
              "author": "theKurganDK",
              "text": "Could you elaborate please? Routed?",
              "score": 2,
              "created_utc": "2026-01-30 15:41:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rkore",
                  "author": "officialexaking",
                  "text": "Here is everything you need to know about it:\nhttps://www.xprivo.com/blog/en/mistral-is-not-a-european-alternative/",
                  "score": 1,
                  "created_utc": "2026-01-31 09:52:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qpa554",
      "title": "Payment in EUR more expensive than in USD?",
      "subreddit": "MistralAI",
      "url": "https://v.redd.it/9xg4e9dz33gg1",
      "author": "d4v1d_dp",
      "created_utc": "2026-01-28 12:34:01",
      "score": 76,
      "num_comments": 18,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qpa554/payment_in_eur_more_expensive_than_in_usd/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o27drrt",
          "author": "Axiom05",
          "text": "The price in US dollars never includes VAT, unlike the price in euros.",
          "score": 73,
          "created_utc": "2026-01-28 12:44:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27kcj7",
              "author": "Jazzlike-Spare3425",
              "text": "Yes, this is it. You can see it by the asterisk. If you scroll down to the bottom of the table that compares the plan, just over the FAQ section, for the USD version it will say \"excluding taxes\" and for the Euro price it will say \"including taxes\"",
              "score": 14,
              "created_utc": "2026-01-28 13:24:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o29i11u",
              "author": "Patrick_Barababord",
              "text": "Maybe, but $1 = 0,83â‚¬ also.  \nSo $14.99 = 12.44â‚¬ ...and 12.44 + 20% VAT = 14.9â‚¬.",
              "score": 3,
              "created_utc": "2026-01-28 18:42:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2daedd",
                  "author": "aoc145134",
                  "text": "I don't think you'll get a reasonable comparison by using a conversion rate based on a four-year low for the dollar. Even a week ago it would have been $1 = 0.854 giving 15.36 Euros.",
                  "score": 1,
                  "created_utc": "2026-01-29 06:49:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27f4ys",
              "author": "OwlSlow1356",
              "text": "paid one month in USD last year, the only currency available then although i am in europe but nonEUR country, and they deducted the VAT from USD price when providing a VAT number, second month they charged me full USD price, asked why if first month VAT was deducted, never received any answer, cancelled and good bye!",
              "score": 2,
              "created_utc": "2026-01-28 12:53:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o284ehb",
          "author": "EzioO14",
          "text": "Itâ€™s just a tax matter, not an advantage to U.S",
          "score": 14,
          "created_utc": "2026-01-28 15:06:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27cktf",
          "author": "Little_Protection434",
          "text": "That seems to be the case. I just checked the exchange rate and based on that it should indeed be the opposite.",
          "score": 8,
          "created_utc": "2026-01-28 12:37:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o282bu9",
              "author": "ComeOnIWantUsername",
              "text": "Just read the pricing as a whole, not just one part. For USD it's \"excluding taxes\". For EUR \"including taxes\".",
              "score": 9,
              "created_utc": "2026-01-28 14:56:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o282og9",
                  "author": "Little_Protection434",
                  "text": "That makes sense. Thanks!",
                  "score": 2,
                  "created_utc": "2026-01-28 14:58:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28shtk",
          "author": "TheBl4ckFox",
          "text": "Itâ€™s always excluding tax for US customers and including VAT for EU.",
          "score": 3,
          "created_utc": "2026-01-28 16:52:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27teac",
          "author": "Nokushi",
          "text": "prices were aligned before, but were all without VAT included, which is uncommon in France\n\ni guess they now show EUR prices VAT included",
          "score": 2,
          "created_utc": "2026-01-28 14:12:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o280wet",
              "author": "sndrtj",
              "text": "And since VAT rates depend on the country of the _purchaser_, not the seller, exact amounts may change depending on where you are located.",
              "score": 4,
              "created_utc": "2026-01-28 14:49:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o286pfw",
          "author": "bluepuma77",
          "text": "Just saw that too, was irritated, it seems sooo stupid what they are doing with their pricing page.  \n  \n1. It's \"$15 with \\*\".   \n2. What's a \\*, okay pages down \\* means it's without tax.   \n3. Then I switch to Eur and it's \"â‚¬18 with \\*\"\n\nWho would have thought that the meaning of the \\*, many pages down, would silently change.\n\nI highly recommend to not hide the \\* so far down. And maybe use \\*1 and \\*2 or something.",
          "score": 1,
          "created_utc": "2026-01-28 15:17:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ecvg4",
              "author": "Fisherman-63",
              "text": "C'est notÃ© ! Will improve our page soon :)",
              "score": 2,
              "created_utc": "2026-01-29 12:21:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27txik",
          "author": "Far-Reaction-1980",
          "text": "To this day I don't get Mistrals pricing",
          "score": 0,
          "created_utc": "2026-01-28 14:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27htt4",
          "author": "Basiliscus219",
          "text": "Usually the prices are set based on the local purchase power. In poorer countries the price is lower.",
          "score": -5,
          "created_utc": "2026-01-28 13:09:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnnieh",
      "title": "Devstrale 2 > other Chinese AIs like DeepSeek etc",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qnnieh/devstrale_2_other_chinese_ais_like_deepseek_etc/",
      "author": "nycigo",
      "created_utc": "2026-01-26 18:00:38",
      "score": 53,
      "num_comments": 30,
      "upvote_ratio": 0.96,
      "text": "Why is nobody talking about Devstrale 2 in the same way as GLM 4.7 Deepseek and Minimax when the AI â€‹â€‹is in the top 6 on OpenRouter in the best programming AI category, ahead of all the other Chinese models and with a damn free API?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qnnieh/devstrale_2_other_chinese_ais_like_deepseek_etc/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1v1ilu",
          "author": "SourceCodeplz",
          "text": "Because the Chinese have an online army on reddit and they promote it heavily.  \nBut GLM, Deepseek and Minimax are really good actually, not like from Anthropic, but fine.",
          "score": 24,
          "created_utc": "2026-01-26 18:10:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1v1l3g",
          "author": "cosimoiaia",
          "text": "Hate.\n\nDevstral is a superb model, even the small-24b running locally is better than all the other open weights.\n\nBut if they start to admit that the privacy/consumer first policies actually don't block progress completely and that the EU can, and did, produce SOTA models for their size, their delusions will break and they'll have a panic attack.\n\nAt the WEF they openly admitted that they wanted only the US to be players in the AI field and that they should do anything to block progress for everyone else.",
          "score": 22,
          "created_utc": "2026-01-26 18:11:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22wo60",
              "author": "robogame_dev",
              "text": "    even the small-24b running locally is better than all the other open weights.\n\nFor devstral-2-small to beat all open weights it would have to beat devstral-2... \n\nI've got 60 million tokens through Devstral 2, it's a great model - is it better than every other open weight model at everything, even the ones that are 10x is param count? no, definitely not.",
              "score": 1,
              "created_utc": "2026-01-27 20:08:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o23105z",
                  "author": "cosimoiaia",
                  "text": "Well yes, I meant to compare size to size of course, I could have phrased It better. \n\nAlthough I have to say the ones with 10x the params are not mind-blowing better for me, most of the times they still have the same pitfalls, they just go a bit further, so the convenience and efficiency of Devstral remain unbeatable for me.",
                  "score": 1,
                  "created_utc": "2026-01-27 20:28:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1vxfe9",
          "author": "tisDDM",
          "text": "I did a few tests with devstral 2 (small) and both are performing very good in agentic coding. I did a post here [https://www.reddit.com/r/opencodeCLI/comments/1qlqj0q/benchmarking\\_with\\_opencode\\_opuscodexgemini\\_flash/](https://www.reddit.com/r/opencodeCLI/comments/1qlqj0q/benchmarking_with_opencode_opuscodexgemini_flash/) about benchmarking and said a few things about Devstral as well (but did not include the Devstral 2 results) because it is part of my subagent harness project (not published yet) - where I tried to use Devstral 2 as intelligent worker nodes. \n\nI found my results impressing. Both Devstral 2 Models were fully able to run the test suite. Deepseek 3.2 and Kimi K2  and Grok Fast showed a lot of issues with following agentic tasks.\n\nBut in case you ask me why I am not using devstral 2 for coding? It is far behind Opus and Codex. Not in quality of code. Behind in understanding and following a humans complex task. which both of the big two easily can manage. But this might be an issue of Reasonig.",
          "score": 4,
          "created_utc": "2026-01-26 20:27:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xm32o",
              "author": "Historical_Roll_2974",
              "text": "How did you get Devstral 2 Small to work on OpenCode? When I try to use it with OpenCode I get an error where the filepath is null and the message is null with the tools API? Thanks",
              "score": 1,
              "created_utc": "2026-01-27 01:19:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1z3bx1",
                  "author": "tisDDM",
                  "text": "I got Devstral 2 Small working as a subagent for my benchmark, that was very cool - but still a little bit flaky depending on prompting. I used Devstral 2 ( the big one) in Zed but I think it shall work in Opencode as well. I will give it a try again later.\n\nhttps://preview.redd.it/xel4553hrvfg1.png?width=1404&format=png&auto=webp&s=63299ff43b4cafade0e9c046ac8bf38c132d1a6d\n\nI gave it a try. It works OOB with the Mistral-API and an API-Key. It's free.",
                  "score": 2,
                  "created_utc": "2026-01-27 06:54:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1v1sd5",
          "author": "Front_Eagle739",
          "text": "Well, its slow to run locally and while decent at coding its less flexible, mostly from the lack of ability to turn on high thinking. It's a nice model and it's got it's niche but glm 4.7 is usually better and more flexible even quantised to a similar size as devstral. It is a useful thing and I'm trying to use it more if only to support a european company but I think it kind of misses the benefit of being the only big dense model that will fit on a local 128GB machine (i.e. being smarter than anything comparable in size) due to said lack of thinking. For a really really smart model I could run locally I would be willing to wait but because it doesn't think it is not actually smarter than a q2 quantised glm 4.7 that also fits on my machine and it's slower.\n\nThats my take at the moment, I'm trying to explore it more, see if I can get more out of it.\n\nEdit: So... Fun fact! You can actually make devstral 2 123B reason! Accidently had a reasoning forcing jinja template on for another model when I started testing the mlx version of this thing with a couple of reasoning effot = extra high statements in my system prompt because I really wanted more reasoning out of the last model I was using and havving forgotten about that tried devstral 2 and got 2 minutes of reasoning before it answered my test question.\n\nI shall be doing a great deal of further testing.\n\nEdit 2 Devstral 24B also reasons if you set it to do so in the jinja.\n\nTurns out they are both hybrid reasoners if you put {%- set reasoning\\_content = 'High' %} in the jinja. Nice clean logical reasoning as well. That's actually fixed my main issue with these models, sometimes you just really need that extra consistency.",
          "score": 7,
          "created_utc": "2026-01-26 18:12:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xjluk",
              "author": "Holiday_Purpose_3166",
              "text": "https://preview.redd.it/tqly3ygqksfg1.jpeg?width=4096&format=pjpg&auto=webp&s=8f21f93e021bfb6b05fab7706f02b541c4bf56af\n\nHaving spent billions of tokens last year in coding between different models under the sun, Devstral models are indeed excellent for what they are.\n\nl take the benchmarks as a guide and test them in my prod, and I find ironic how \"behind\" they visually look on the charts. Anecdotally, Devstral Small 2 repeatedly beats GPT-OSS-120B on my codebases despite being so far apart on benchmarks.\n\nGLM 4.7 Flash on the other hand is also excellent and kicks the 120B on the same stuff I'm working (ML, financial algos, Rust, NextJS, etc), but Devstral Small 2 has more eye to detail where GLM left some issues I had \"as is\".\n\nHowever, GLM 4.7 Flash is broadly smarter and more update to date on greater schemes compared to Devstral but has less knack on repo work. Like them both, but I lean towards Devstral over the fact it has more enterprise grade efficiency. Could be bias, but I like minimalistic response, and it can be extensively detailed when require (docs, blueprints >1k lines).\n\nAnother important note that resonates with my experience, the GPT-OSS models have large response variance where Devstral is more deterministic and is consistent on every round, and this reflected on SWE-Rebench charts from Ibragim.\n\nI find this variance inconvenient for coding despite tighter sampling as repeated, personal tests yielded different results.\n\nI haven't used GLM 4.7 Flash long enough to detect that but generally the apple doesn't fall far from tree if it follows the grand 4.7.",
              "score": 4,
              "created_utc": "2026-01-27 01:05:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1v95xy",
              "author": "SourceCodeplz",
              "text": "Yeah. Thinking adds a LOT o value, even though sometimes it can go in circles.",
              "score": 3,
              "created_utc": "2026-01-26 18:42:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1vafxw",
                  "author": "Front_Eagle739",
                  "text": "Yeah it really does. I get the value of a fast nonthinking model for code completion, i get the value of a slow reasoning model thats very coherent and intelligent for its size. I struggle to get the purpose of a slow non thinking model that maxes out my memory. the 24B devstral I do understand, I have more memory than would force me to go down to that size but if I was on 24GB/32GB That one I could see myself using. The 123B really wants reasoning to be the best possible answer I can get for 128GB memory option.",
                  "score": 1,
                  "created_utc": "2026-01-26 18:48:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1wlad8",
              "author": "cosimoiaia",
              "text": "Have you tried it with vibe? I've been using it quite a lot, for fairly complex tasks and, so far, it never failed, even with navigating in decent sized projects and it's FAST. Btw you need a lot less than 128GB to run it at full context, 62GB will leave you room to spare.\n\nAlso, I am very doubtful that a Q2 model of any size can do well coding tasks since they're very very quantization sensitive, doesn't matter the amount of thinking.\n\nEdit: this seems like an example of classic casting shadows disguised as comment. I hate worthless propaganda.",
              "score": 1,
              "created_utc": "2026-01-26 22:13:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1wmuiu",
                  "author": "Front_Eagle739",
                  "text": "Unsloth dynamic quants are a wonderful thing. I will admit the mlx 2 bit quants are fairly useless but the unsloth iq2m glm 4.7? That one works great. The minimax ones are worse though, not quite sure why but different models seem to take better or worse to dynamic quants and glm is the best ive seen. Anyway.\n\n\nNo i havent tried vibe with it. Tried it in claude code but I'll give vibe a go. Makes sense it would be optimised for its own harness.\n\n\nWont be fast though at least not the big one unless i go api, i get about 5 token/second with devstral 123 vs 60 with gpt oss 120 or 20 ish with glm. As i said, worth it if its good enough but it has to be consistent enough to not want to retry things a lot. Will give it a go in vibe.\n\n\nEdit. Thanks for declaring my opinions are worthless propoganda. Appreciate it. Really makes a man feel like he's in a good useful discussion. Honestly.",
                  "score": 1,
                  "created_utc": "2026-01-26 22:20:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ylw16",
          "author": "Waste-Intention-2806",
          "text": "Because it's a dense model. While minimax and reaped glm 4.7 r MOE models and most of us can run these at least 4-11 tokens per second. While devstral 2 was running at .7 to 1 token per second for me on i9 and 16gb rtx 4070 ti with 128 gb ram",
          "score": 2,
          "created_utc": "2026-01-27 04:44:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2gdo5u",
          "author": "Costello173",
          "text": "Like said before the Chinese have reddit Armies and Mistral is French my friend",
          "score": 2,
          "created_utc": "2026-01-29 18:18:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ggcfj",
              "author": "nycigo",
              "text": "It's a shame we don't have the same thing in Europe, haha. With the current European propaganda on social media, you'd hope some guys would get involved. I imagine it works just as well in China because it's the state that funds it, but oh well...",
              "score": 1,
              "created_utc": "2026-01-29 18:30:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2gt0fm",
                  "author": "Costello173",
                  "text": "Right? Even compared to American brands they fall short on marketing Europeans overall seem to take longer to get the ball rolling but when they do their usually onto something. China and the US were just good at giving up good ideas.",
                  "score": 2,
                  "created_utc": "2026-01-29 19:28:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ioree",
          "author": "Equivalent-Word-7691",
          "text": "deepseek, kimi ,GLM let people have a free ride without paying ,I a gulty to us more chiens e model rather than msitral becasue I do not have real limits with kimi or deepseek ,and I I want quality I use Claude or gemini",
          "score": 1,
          "created_utc": "2026-01-30 01:08:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t3zp5",
          "author": "stddealer",
          "text": "It's a dense model so it's not as fast compared to MoE models such as GLM 4.7. this is a big thing when it comes to running models locally. Though GLM requires thinking to function properly, which makes it slow in other ways.",
          "score": 1,
          "created_utc": "2026-01-31 16:06:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2te53z",
              "author": "nycigo",
              "text": "It's super fast even on OpenRouter, man.",
              "score": 1,
              "created_utc": "2026-01-31 16:54:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2tzaiy",
                  "author": "stddealer",
                  "text": "Yes I was talking about running it locally. For api services it doesn't matter as much though it can affect the pricing.",
                  "score": 1,
                  "created_utc": "2026-01-31 18:35:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1v7wxo",
          "author": "Marciplan",
          "text": "It really isn't on par with GLM 4.7 though.",
          "score": 1,
          "created_utc": "2026-01-26 18:37:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vgpmv",
              "author": "kiwibonga",
              "text": "For agentic coding though, Devstral Small 2 scores 10 pts higher than GLM-4.7-Flash on SWEBench. The GLM team completely omitted it from their benchmarks, maybe because it's not MoE.",
              "score": 6,
              "created_utc": "2026-01-26 19:14:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vieoc",
          "author": "pinmux",
          "text": "I think Mistral offering their API for devstral-2 and devstral-small-2 for free is actually hurting adoption by inference providers and hence users don't know about it.\n\nIn my brief experience trying devstral-small-2, it's quite good.  I don't have beefy enough hardware locally to run it at a reasonable speed and last I checked the only cloud inference providers offering the devstral-2 models will train on your data (Mistral included for their consumer offerings).  On OpenRouter you get Mistral or Chutes, that's it.\n\nI'm hoping some cloud inference providers will pick up devstral-2 (and devstral-small-2) after tomorrow once Mistral starts charging for the API access.  That'll make it easier for people to find and use it.",
          "score": 1,
          "created_utc": "2026-01-26 19:21:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vt5xc",
          "author": "Large-Example-1275",
          "text": "It runs slowly on my DGX Spark compared to MoE alternatives.",
          "score": 0,
          "created_utc": "2026-01-26 20:08:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qokht8",
      "title": "Mistral Vibe now available on subscriptions",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qokht8/mistral_vibe_now_available_on_subscriptions/",
      "author": "Holiday_Purpose_3166",
      "created_utc": "2026-01-27 17:33:24",
      "score": 53,
      "num_comments": 8,
      "upvote_ratio": 0.95,
      "text": "https://preview.redd.it/qi2hpnqdgxfg1.png?width=394&format=png&auto=webp&s=9aad3a7897e9e01465d0adf02a302a1a0546fc14\n\nMistral team announced on X the subscriptions are now available with access to Mistral Vibe coding.\n\nMassively appealing move.\n\n[https://x.com/mistralvibe/status/2016179799689928986?s=20](https://x.com/mistralvibe/status/2016179799689928986?s=20)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qokht8/mistral_vibe_now_available_on_subscriptions/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o21xmyi",
          "author": "minaskar",
          "text": "Any information on what the actual request limits are?",
          "score": 6,
          "created_utc": "2026-01-27 17:37:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o225d9w",
              "author": "Holiday_Purpose_3166",
              "text": "Good question. It's transparent as a door. In their plan it describes \"Mistral Vibe for all-day coding, PAYG beyond.\" but no suggestive references to what those limits are.",
              "score": 3,
              "created_utc": "2026-01-27 18:10:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o21xt5y",
          "author": "datNovazGG",
          "text": "Arent we able to just download it and use it with something like OpenRouter, anymore? What's the difference that it's in the pro tier sub?",
          "score": 4,
          "created_utc": "2026-01-27 17:38:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2254x1",
              "author": "Holiday_Purpose_3166",
              "text": "OpenRouter is a proxy provider, unrelated to Mistral sub plans. I understand Mistral was offering the Devstral 2 models for free on OpenRouter and I believe these are finishing/finished - so after that it's PAYG.  \n  \nMistral plan description states \"all day coding\" with Mistral Vibe, so that wording expresses that you only have free\\* access to coding model only via Mistral Vibe agentic tool.   \n  \n*\\* No quotas are prescribed but they express \"Mistral Vibe for all-day coding, PAYG beyond.\" which does suggests limits.*\n\nBased on the available information, the main difference is you can't use the free coding model outside Mistral Vibe, like you could do with OpenRouter.",
              "score": 2,
              "created_utc": "2026-01-27 18:09:40",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o26izrg",
              "author": "robberviet",
              "text": "Just like how you can use claude code with sub or with key.",
              "score": 1,
              "created_utc": "2026-01-28 08:29:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26u6xg",
          "author": "_coding_monster_",
          "text": "Does mistral have a VSCode extension (not CLI in VSCode) that works like Github Copliot or Kilo Code? Claude code supports VSCode extension",
          "score": 2,
          "created_utc": "2026-01-28 10:13:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2r6sk1",
              "author": "Holiday_Purpose_3166",
              "text": "The have IDE but for Enterprise only.",
              "score": 1,
              "created_utc": "2026-01-31 07:41:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dk0fm",
          "author": "Salt-Willingness-513",
          "text": "why the fuck did mistral stop prividing proper info? we still have no info on medium 3 on lechat and rate limits are also unclear on vibe...",
          "score": 2,
          "created_utc": "2026-01-29 08:13:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qndv3e",
      "title": "Ministral models are good.",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qndv3e/ministral_models_are_good/",
      "author": "Rent_South",
      "created_utc": "2026-01-26 11:45:46",
      "score": 50,
      "num_comments": 8,
      "upvote_ratio": 0.97,
      "text": "Just to say that in their weight class, ministral models (mainly 3b and 8b) are very cost efficient and quick, compared to other models.\n\nFor non complex tasks, they actually compete for the top spot.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qndv3e/ministral_models_are_good/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1t0lz8",
          "author": "scara1701",
          "text": "I like ministral:3b as well. Currently using it to test MCP tools Iâ€™m building :)",
          "score": 4,
          "created_utc": "2026-01-26 12:15:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1to112",
          "author": "stddealer",
          "text": "Yep, they've finally replaced Gemma3 models for me, though I think Gemma was a bit better at some things like translation or OCR, Ministral feels like a nice upgrade.",
          "score": 2,
          "created_utc": "2026-01-26 14:31:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1u0apu",
          "author": "Holiday_Purpose_3166",
          "text": "Mistral models are indeed good. I use them daily, especially Devstral Small 2 for my workflows where GPT-OSS-120B struggles to execute. What a time to be alive.",
          "score": 2,
          "created_utc": "2026-01-26 15:30:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23fy7r",
          "author": "kompania",
          "text": "From my perspective, they're very close to Gemma 3. They're incredibly talkative, competent, and handle drift well.\n\nThe downside is that they're currently untunable due to the lack of working notebooks.",
          "score": 2,
          "created_utc": "2026-01-27 21:35:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1v9vhq",
          "author": "Conscious-Expert-455",
          "text": "How to use these models? For vibe coding? As agents?\nI'd like to use them as agents or as MCP services.",
          "score": 1,
          "created_utc": "2026-01-26 18:45:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xac54",
              "author": "Rent_South",
              "text": "It all depends on your use case. Depending on your specific tasks, any of your suggestions are viable.  \nOne thing is certain is that if your use case fits these models, they perform really well.",
              "score": 1,
              "created_utc": "2026-01-27 00:18:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1szmp5",
          "author": "Scared_Range_7736",
          "text": "Still far behind American and Chinese models, unfortunately. Check this benchmark from a few days ago: [https://www.vals.ai/benchmarks/terminal-bench-2](https://www.vals.ai/benchmarks/terminal-bench-2)",
          "score": -5,
          "created_utc": "2026-01-26 12:08:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1t5me6",
              "author": "krkrkrneki",
              "text": "OP is referring to open models available to be run locally.",
              "score": 9,
              "created_utc": "2026-01-26 12:50:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qoc2ww",
      "title": "Give feedback if you want that Mistral improves",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qoc2ww/give_feedback_if_you_want_that_mistral_improves/",
      "author": "no_coykling",
      "created_utc": "2026-01-27 12:08:10",
      "score": 45,
      "num_comments": 8,
      "upvote_ratio": 0.97,
      "text": "Instead of just providing a new reply with what you think should be improved, also include feedback using thumbs-up/down buttons.\n\n* You have a good reply, thumbs up.\n* If your answer lacks information that was previously mentioned in the context, include it in your feedback.\n* If Mistral asks a question about something not specified, don't use it.\n\nThis is one way to improve the product, subscription is the other one ;)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qoc2ww/give_feedback_if_you_want_that_mistral_improves/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o205dps",
          "author": "nycigo",
          "text": "Try to get as close as possible to the Chinese models, I think, and try to highlight their models like Devstral 2 as much as GLM 4.7.",
          "score": 6,
          "created_utc": "2026-01-27 12:23:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22vjam",
              "author": "dcforce",
              "text": "I'm with you Devstral 2 is pretty amazing\n\nWish it had a bit larger context window and coding is likely only going to only improve on what is already a solid model.  \n\nLooking forward to Devstral 2.5 - 3 for sure",
              "score": 3,
              "created_utc": "2026-01-27 20:03:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2045y0",
          "author": "Jazzlike-Spare3425",
          "text": "I would love to help the product improve but I've had a back and forth with their support team (wonderful people, honestly) where I shared a plethora of bugs and other problems regarding their apps for iOS and iPadOS, and I don't think they really addressed any. But to be fair I haven't been checking back since I found it annoying enough to have eventually just given up waiting for fixes and started building my own Mistral frontend in SwiftUI to get the UX I wanted.",
          "score": 7,
          "created_utc": "2026-01-27 12:14:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o206clu",
              "author": "no_coykling",
              "text": "People has always a different roadmap then a company. But I agree that reproducible bugs should be on top.",
              "score": 5,
              "created_utc": "2026-01-27 12:30:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2072uc",
                  "author": "Jazzlike-Spare3425",
                  "text": "Yeah, and I'm assuming that subscriptions to normal customers aren't Mistral's bread and butter but I think the Le Chat Pro subscription is a bit too expensive to not have a desktop app and to have the mobile apps like they are. I did later use Gemini though and the Le Chat app was better than Gemini's at least. The Le Chat apps do lack important features and polish but the Gemini app was outright unusable for me at times.",
                  "score": 3,
                  "created_utc": "2026-01-27 12:34:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2evde3",
          "author": "Any-Cicada-7317",
          "text": "Disclaimer, I'm not an engineer so I may be unaware of an existing capability.  \n  \nWith Vibe now able to use my Le Chat pro account for credits etc. It'd be great to set up a specific Le Chat project/projects that Mistral Vibe could natively interact with.   \nI do a lot of my planning/research/scoping in Le Chat, and then download it into a .md and use Vibe to read it. \n\nIt \"works\" but having it able to natively interact with Le Chat would be interesting. Setting system prompts etc there, placing consistently used [skill.md](http://skill.md)",
          "score": 2,
          "created_utc": "2026-01-29 14:08:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ir97z",
          "author": "Equivalent-Word-7691",
          "text": "as a writer mkisttal is  juts  behind  ,both  on output lenght and  in quality comapred to calude (!), opne AI, gemini, or even chinese models like kimi, GML and deepseek ,it's like a cold style ,aesthetically ugly,a dn the output lenght meh..also free pan should be more generous be cause  no way peope will  try mistral and pay easily 20 eyruos for a mdoe that is clealry infeiror wihtout any pro",
          "score": 0,
          "created_utc": "2026-01-30 01:22:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnujw3",
      "title": "Mistral beats Gemini and Perplexity for competitive intelligence",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qnujw3/mistral_beats_gemini_and_perplexity_for/",
      "author": "Cachao-on-Reddit",
      "created_utc": "2026-01-26 22:03:26",
      "score": 36,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "I've posted here before about being impressed by Mistral Medium. That was mostly as an API user.\n\n  \nThis time I ran most of the big consumer-facing LLMs against each other in a 'Deep Research' style task. The focus was competitor news.\n\nMistral didn't win. But I think did commendably well. Especially given:\n\n\\- (a) relative underdog status compared to other players on this list,\n\n\\- (b) I using the very fast free tier (unlike Claude's slow, very expensive tier), and\n\n\\- (c) it was \\*clearly\\* better than Perplexity and Gemini.\n\n\n\nhttps://preview.redd.it/bcbnmklhmrfg1.png?width=763&format=png&auto=webp&s=a9f06a7c5f9a38234ca58faf6a9e9b1758a3d30d\n\n\n\nYou can see more about the test here: [https://anatole.fyi/blog/competitive-intelligence-face-off](https://anatole.fyi/blog/competitive-intelligence-face-off)\n\n\n\nAnd yes, you'll see it's flawed. I only did one run per LLM. The prompt was bad. Obviously on another attempt or with a better prompt Gemini won't have quite such a meltdown. But, when I'm using these tools day to day I would rather not have to run them multiple times or craft my prompt. And I think this side-by-side beats pure anecdote when comparing LLM quality.\n\n  \nWill run another test soon. Let me know what you think.\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qnujw3/mistral_beats_gemini_and_perplexity_for/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o26dotz",
          "author": "enormousdino",
          "text": "so in the wake of Macron's shades, I asked them all the other day which politicians wear watches made in their own country (as Macron famously wears French watches, including v independent niche brands). \n\nMistral guessed right - including Macron, Joe Biden's Shinola, and even Modi's Jaipur  \nGemini didn't guess Macron, but did talk about Biden, Abe Shinzo's Seiko and Modi  \nChatGPT didn't guess Macron, but did mention Biden and Modi  \nClaude said it's not aware of any instances of politicians wearing such watches.... and I'm paying for it!!",
          "score": 1,
          "created_utc": "2026-01-28 07:42:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2brlzi",
              "author": "Cachao-on-Reddit",
              "text": "share the link? would love to see.\n\nI do think it's important to distinguish the model from the harness. Opus 4.5 is clearly a better model than Large. But clearly part of how claude.ai is wiring it up is yielding sub-par  results.\n\nside note: feel like Macron's shades are a huge Mistral branding opp",
              "score": 1,
              "created_utc": "2026-01-29 01:04:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qry40r",
      "title": "Replace github copilot with Mistral",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qry40r/replace_github_copilot_with_mistral/",
      "author": "InternalBroad2522",
      "created_utc": "2026-01-31 09:39:58",
      "score": 31,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "Hi all! I would like to replace Github Copilot with Mistral for coding in VSCode IDE. What can I do?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qry40r/replace_github_copilot_with_mistral/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o2rk80x",
          "author": "scara1701",
          "text": "Does it have to be in vscode? I guess you could run mistral vibe in a terminal window.",
          "score": 4,
          "created_utc": "2026-01-31 09:48:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vlko5",
              "author": "LoadZealousideal7778",
              "text": "Or in the Jetbrains IDE of your choice",
              "score": 1,
              "created_utc": "2026-01-31 23:25:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2roc3i",
          "author": "katafrakt",
          "text": "Mistral Vibe supports ACP, so any VSCode extension supporting ACP would do. I'm not too familiar with this ecosystem, so I won't give any recommendations.",
          "score": 3,
          "created_utc": "2026-01-31 10:27:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2szqsy",
          "author": "kiwibonga",
          "text": "There's a mistral vibe extension that lets you have the CLI in VSCode. It's mostly a convenience thing that saves you the trouble of launching the terminal and setting the working directory.",
          "score": 2,
          "created_utc": "2026-01-31 15:45:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ro18m",
          "author": "opsmanager",
          "text": "Im using the Continue extension, works reasonably well. I havent yet figured out how to make it as seemless as the copilot extension with regards to accessing the repository files. But im sure its just me missing something obvious.",
          "score": 2,
          "created_utc": "2026-01-31 10:24:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2uj3bq",
              "author": "vienna_city_skater",
              "text": "Continue is soso, good for code completion, but agent mode sucks, Roo Code works well for agentic coding in VS Code, but the TUIs are much better. I switched everything to OpenCode because I mix models a lot. However, so far I have not find a way to get access to the free devstral with my Le Chat Pro subscription outside of vibe CLI.  \nEDIT: I think I found the problem, I used the API key from La Platforme Admin Panel not from the AI Studio VIBE CLI section, so I was charged on-the-go.",
              "score": 2,
              "created_utc": "2026-01-31 20:10:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2utlgd",
          "author": "Bob5k",
          "text": "Have in mind that you either pay for API usage of devstral or accept the quite mediocre quota allowance on experiment plan.",
          "score": 1,
          "created_utc": "2026-01-31 21:02:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xnbbp",
              "author": "deegwaren",
              "text": "Oh? Didn't they just include vibe-cli usage in their pro plan? Meaning it's included like Claude code usage in the Claude Pro plan.",
              "score": 1,
              "created_utc": "2026-02-01 07:30:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qqw89j",
      "title": "Made myself a LeChat application on linux",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qqw89j/made_myself_a_lechat_application_on_linux/",
      "author": "MattyGWS",
      "created_utc": "2026-01-30 05:03:06",
      "score": 28,
      "num_comments": 2,
      "upvote_ratio": 0.9,
      "text": "Ok I'm no programmer, but I just had some fun learning how to build 'web apps' with Electron. I made a lil desktop icon too (in gimp). So now I have my own desktop application for Mistral! \n\n\n\nhttps://preview.redd.it/900xz4j75fgg1.png?width=2005&format=png&auto=webp&s=fdbbb04df3c14f515991184693b4fa46119766e0\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qqw89j/made_myself_a_lechat_application_on_linux/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o2lf64l",
          "author": "AdIllustrious436",
          "text": "Great! Just for you to know, you can achieve the same result in one click with Chromium PWA, which install websites as applications on the system.",
          "score": 3,
          "created_utc": "2026-01-30 12:55:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mcd54",
              "author": "MattyGWS",
              "text": "Yea I know that's how I normally do it but I wanted to learn how Electron apps are made, seemed like a great yet simple example to bring LeChat to desktop. :)",
              "score": 2,
              "created_utc": "2026-01-30 15:43:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qo03k1",
      "title": "Difficult in switching from claude",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qo03k1/difficult_in_switching_from_claude/",
      "author": "MikadinShinjuk",
      "created_utc": "2026-01-27 01:45:02",
      "score": 28,
      "num_comments": 15,
      "upvote_ratio": 0.85,
      "text": "Good morning everyone, I have a problem that's been bothering me. I'm trying to move away from as many US services as possible, but Claude seems like an insurmountable obstacle. Let me clarify: I'm not a developer. I use AI to search for information, help with solo role-playing games (RPG), and research Warhammer 40k lore and similar topics. With Claude, I feel like I'm in heaven, but I understand that more and more (especially in recent days), it would be better to distance myself from US services. For some time now, I've been trying to use Le Chat, but every time it seems to be lagging behind. It's as if it doesn't consider the nuances in what I say, doesn't analyze in depth, always stops at the first point, and doesn't go into detail thoroughly. Am I doing something wrong? Should I create specific agents? Should I give it more precise and less discursive instructions? I tend to create queries as if I were talking to a person, and this works well with Claude, but maybe not here? I need some feedback from those who use it as their main AI.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qo03k1/difficult_in_switching_from_claude/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1xt331",
          "author": "gdsfbvdpg",
          "text": "I tried using it as a gm for a Cyberpunk game and then using it as a group of 5 players in a d&d game that I DM. In both cases it was pure frustration. I took the same exact instructions/files over to Gemini (free version) and *boom* it works very well. \n\nIt makes me really sad. BUUUUT - I feel like Mistral might be where chatGPT was a year or year and a half ago. So I'm hopeful that there's a bright future in store for it. But right now?  *Le sigh*. It's just not there yet.  I'm going to keep paying for it though.  I refuse to give up.",
          "score": 20,
          "created_utc": "2026-01-27 01:57:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o216ngz",
          "author": "fonceka",
          "text": "I believe Mistral market is the pro market, not end user. They are a b2b startup, not like ChatGPT. In a business perspective, Mistral performs absolutely well. It's super fast, not verbose, and really sensitive to your context. For a solopreneur like me, it was much more helpful than GPT or Gemini to craft my marketing campaign, help me with landing page, find the adequate wording, lower ambiguities, etc.",
          "score": 9,
          "created_utc": "2026-01-27 15:40:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1y40mb",
          "author": "LewdManoSaurus",
          "text": "Mistral has some ways to go for sure. It's fun to mess around with, but for heavy usage for generative writing, it is definitely lagging behind by far imo (I had the subscription back in October last year). It's nice when it works, but in my experience it was a headache more often than not, or I had to make corrections so often that I was better off just using a different service.",
          "score": 11,
          "created_utc": "2026-01-27 02:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yqjm3",
          "author": "Den_er_da_hvid",
          "text": "I know what you feel.  coming from Gemini though. \n\nLast night I tried using it to get tips on my game playing Humankind. \nI gave then info but it kept going around in circles that did not make sense based on resources and where my territories where.\n\nLater I asked about airfry time vs  oven in a recipie and gave it a link. It did not use the link before I explicitly said so.",
          "score": 5,
          "created_utc": "2026-01-27 05:16:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z9b0j",
          "author": "ActionLittle4176",
          "text": "Itâ€™s better if you use the large model, but of course itâ€™s behind the frontier models from Anthropic, OpenAI and Google (like the rest of the AI labs)",
          "score": 3,
          "created_utc": "2026-01-27 07:46:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zweyg",
          "author": "EcceLez",
          "text": "Mistral is not as good as Claude. That being said, it makes sense to use it through it's api for your workflows.",
          "score": 3,
          "created_utc": "2026-01-27 11:15:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zx0ye",
          "author": "ameliassoc",
          "text": "Unfortunately it is similar for me. I use AI primarily for language learning and Mistral just can't live up to the standard ChatGPT has set for me. It constantly messes up instructions forcing me to repeat, and it is obsessed with our past discussions and memories to the point that it always talks about the same thing, which of course is not what you want when trying to learn as much as you can.\n\nFor the time being I'm settling for a compromise. I'm keeping my OpenAI subscription because I figure getting a better education is always going to be better irrespective of whether it's a US service. For other queries and where privacy is absolutely not a concern, I use Le Chat, and have the setting turned on to let it train on our chats, in the hope that it will eventually catch up and I can switch fully.",
          "score": 2,
          "created_utc": "2026-01-27 11:20:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22a8r9",
          "author": "graymalkcat",
          "text": "Just use both. When I need Claude then I reach for Claude. Otherwise I use Mistral.\n\nAlso, remember that Claude is trained on user data. The more people use it, the more youâ€™ll push it ahead of everything else. Maybe you like that or maybe you think thatâ€™s unfair and unethical. Decide. Â ",
          "score": 2,
          "created_utc": "2026-01-27 18:31:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23563o",
          "author": "Nev3r_Pro",
          "text": "I've decided to shift from US based AI to the Chinese one. I use Deepseek and Kimi K2.5, both are great.",
          "score": 2,
          "created_utc": "2026-01-27 20:47:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20s1ly",
          "author": "victorc25",
          "text": "You need to set your priorities straight. Do you want a top of the line model or do you want an European provider? Choose one",
          "score": 2,
          "created_utc": "2026-01-27 14:30:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21m408",
          "author": "poidh",
          "text": "I use LeChat for low effort/low risk queries because I want to support them. I don't know if that helps, but I guess for investors it is important to see that the product gets actually used, and it hopefully gives them some more real world training data.\n\nBut you have to consider that \"nobody\" can really compete with the view state of the art models (Anthropic/OpenAI/Google). They are just playing in a different league whether its the people or resources behind it. You could try some of the Chinese open weight models and have them hosted in some jurisdiction of your choice.\n\nBtw, you can get an unbiased overview of where the Mistral models stand:  \n[https://lmarena.ai/de/leaderboard/text](https://lmarena.ai/de/leaderboard/text)\n\nOn this site, you can start chatting and you'll receive two responses from two different models, but you don't know which is which.  \nThen you pick which response you like the best (and then it will be revealed which model you had been talking too).\n\nThis is great to get a feel for all kinds of different models you may not had on your radar. And it also shows a realistic ranking (from other people) that are not based on benchmarks (which can be gamed).\n\nYou find the Mistral models on that list way down on position 40+.",
          "score": 1,
          "created_utc": "2026-01-27 16:47:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zbzv9",
          "author": "Hitching-galaxy",
          "text": "Have a look at Kagi- Iâ€™m about to sign up to ultimate",
          "score": 1,
          "created_utc": "2026-01-27 08:10:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zcpr5",
              "author": "MikadinShinjuk",
              "text": "Wow this looks very interesting can you tell me a bit more?",
              "score": 0,
              "created_utc": "2026-01-27 08:17:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o21kbb4",
                  "author": "poidh",
                  "text": "Kagi is based in San Francisco, OP was looking for something outside of the US. I also think they are probably just a wrapper around [Bing.com](http://Bing.com) (they state to use content from various search engines) and the usual AI models (as seen from their model picker).  \nI think it is pretty unlikely that some relatively unknown company is able to build their own search engine or own SOTA LLM model, only a handful of companies can compete at the top of this world wide.",
                  "score": 3,
                  "created_utc": "2026-01-27 16:39:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qqguur",
      "title": "Mistral OCR Skill",
      "subreddit": "MistralAI",
      "url": "https://skills.sh/parlamento-ai/parlamento-ai/mistral-ocr",
      "author": "antoine849502",
      "created_utc": "2026-01-29 18:33:36",
      "score": 21,
      "num_comments": 2,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qqguur/mistral_ocr_skill/",
      "domain": "skills.sh",
      "is_self": false,
      "comments": [
        {
          "id": "o2gznci",
          "author": "nnamfuak",
          "text": "Thank you very much! It works really well. I've been using Mistral OCR3 (mistral-ocr-2512) for 3 weeks now, and it consistently delivers top-quality markdown! Love it!",
          "score": 2,
          "created_utc": "2026-01-29 19:59:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2h1dp5",
              "author": "antoine849502",
              "text": "And the free version is so generous; this should be installed by default with any local agent",
              "score": 1,
              "created_utc": "2026-01-29 20:08:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qq3rlf",
      "title": "Anthropic is winning market share in the enterprise LLM space. Google and Anthropic are gaining ground quickly, while OpenAI is currently seeking new investment in Saudi. Mistral's share is in image 2",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/gallery/1qq3okh",
      "author": "neural_core",
      "created_utc": "2026-01-29 09:11:52",
      "score": 20,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qq3rlf/anthropic_is_winning_market_share_in_the/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o2foxcs",
          "author": "Den_er_da_hvid",
          "text": "The colors in the legend for the stacked barchart is really difficult to see. What color is Mistral?",
          "score": 4,
          "created_utc": "2026-01-29 16:27:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2jxm3f",
              "author": "deegwaren",
              "text": "Mistral is baby blue, around 2%.",
              "score": 2,
              "created_utc": "2026-01-30 05:36:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2jzbvl",
                  "author": "Den_er_da_hvid",
                  "text": "It will be interesting to see in a few month as it seems people are shifting from US to European tech.",
                  "score": 3,
                  "created_utc": "2026-01-30 05:49:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qn8jmp",
      "title": "Looking for some advice on running Ministral locally",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/r/MistralAI/comments/1qn8jmp/looking_for_some_advice_on_running_ministral/",
      "author": "markleoit",
      "created_utc": "2026-01-26 06:36:19",
      "score": 12,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "Hey everyone, I havenâ€™t found many threads about the latest Ministral 3 14B here, but Iâ€™ll try anyway.\n\n**First, the good**:\n\nI just LOVE the voice and tone. It just aligns so well with what Iâ€™m looking for, simply perfect.\n\n**Now, the bad**:\n\nAfter a dozen responses, it starts looping. It repeats the same structure obsessively. The opener is usually a stage direction / third-person style comment followed by em-dash. The closing comment is something cheesy between parenthesis. Markdown formatting is injected everywhere even if forbidden in the sys prompt.\n\nHas anyone had any success in using this model in the context of a chat/companion app? System prompt, temperature, mid-chat steering... Nothing has been working so far.\n\nI have tried plenty of other models, but none comes close to the tone and voice Ministral offers. So, Iâ€™m on a hunt to understand if thereâ€™s anything that can tame this obsessive looping and poor instruction-following.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qn8jmp/looking_for_some_advice_on_running_ministral/",
      "domain": "self.MistralAI",
      "is_self": true,
      "comments": [
        {
          "id": "o1sgvox",
          "author": "porzione",
          "text": "Maybe, just maybe, try this [https://unsloth.ai/docs/models/tutorials/ministral-3](https://unsloth.ai/docs/models/tutorials/ministral-3)  \na correct chat template sometimes helps, but I see that their own 14B via the API behaves the same way and spams with markdown, so don't expect a miracle",
          "score": 2,
          "created_utc": "2026-01-26 09:28:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v6z63",
              "author": "markleoit",
              "text": "Unfortunately, after facing numerous challenges with the official, unquantized version by Mistral, I also tried the unsloth version, and the same issues arose. :(",
              "score": 1,
              "created_utc": "2026-01-26 18:33:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vyv3y",
                  "author": "porzione",
                  "text": "I tried to play with templates but didn't manage to get rid of markdown too. I really like Mistral for creative purposes, so ended up with new GPU and Mistral Small 24B.",
                  "score": 1,
                  "created_utc": "2026-01-26 20:33:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rxx0d",
          "author": "gdsfbvdpg",
          "text": "I used to think posts like yours were written by people high on meth. \n\nThen it started happening to me. 16 paragraphs repeating endlessly with only the occasional word being changed or a new paragraph swapped in. It's disheartening and beyond frustrating because yes - I *love* the tone. \n\nSorry I don't have a solution. Just camaraderie.",
          "score": 3,
          "created_utc": "2026-01-26 06:43:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v7jl7",
              "author": "markleoit",
              "text": "Same here. I think certain issues only emerge with heavy usage. Short prompts and evaluations based on single back-and-forth interactions do not accurately indicate the model's correct functioning. This makes me wonder if the folks at Mistral actually stress-tested this model in all scenarios?\n\nAnyway, have you found anything similar out there? What would be your go-to approach for strictly dialogue-based interactions with no narratives or stage directions?",
              "score": 1,
              "created_utc": "2026-01-26 18:36:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ryqba",
          "author": "txgsync",
          "text": "I limit its context to about 24000-32000 tokens. Then I generate a summary and continue the conversation. It does much better. \n\nYou can also use it in â€œtext completionâ€ rather than â€œchat completionâ€ mode and feed it turns where the summary begins a few turns back. Particularly handy with SillyTavern. \n\nBut yeah, it starts to get very bad at longer context.",
          "score": 1,
          "created_utc": "2026-01-26 06:49:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v6izw",
              "author": "markleoit",
              "text": "Yes, the more the context window fills, the worse it becomes. Still, sometimes it's just a matter of 10 to 12 exchanges!\n\nHave you found anything similar out there? What would be your go-to approach for strictly dialogue-based interactions with no narratives or stage directions?",
              "score": 1,
              "created_utc": "2026-01-26 18:31:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sbkkf",
          "author": "Ill_Barber8709",
          "text": "Voice and tone? Wait, Ministral has built-in text to speech?",
          "score": 0,
          "created_utc": "2026-01-26 08:39:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v5v3k",
              "author": "markleoit",
              "text": "Sorry for the confusion; I was referring to the writing-specific voice and tone in my comment!  \n  \n[https://wordmuseum.com/articles/voice-vs-tone-whats-the-difference-and-why-it-matters-for-writers/](https://wordmuseum.com/articles/voice-vs-tone-whats-the-difference-and-why-it-matters-for-writers/)",
              "score": 2,
              "created_utc": "2026-01-26 18:29:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vchei",
                  "author": "Ill_Barber8709",
                  "text": "No problem mate!",
                  "score": 1,
                  "created_utc": "2026-01-26 18:56:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qn20b9",
      "title": "Mistral AI Agents are now supported on Nyno (simpler open-source n8n alternative). Let Mistral open files or query the database for example.",
      "subreddit": "MistralAI",
      "url": "https://www.reddit.com/gallery/1qn1va2",
      "author": "EveYogaTech",
      "created_utc": "2026-01-26 01:26:37",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/MistralAI/comments/1qn20b9/mistral_ai_agents_are_now_supported_on_nyno/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": []
    }
  ]
}