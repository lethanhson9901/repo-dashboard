{
  "metadata": {
    "last_updated": "2025-12-31 12:59:55",
    "time_filter": "week",
    "subreddit": "machinelearningnews",
    "total_items": 7,
    "total_comments": 4,
    "file_size_bytes": 13718
  },
  "items": [
    {
      "id": "1pveowe",
      "title": "MiniMax Releases M2.1: An Enhanced M2 Version with Features like Multi-Coding Language Support, API Integration, and Improved Tools for Structured Coding",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2025/12/25/minimax-releases-m2-1-an-enhanced-m2-version-with-features-like-multi-coding-language-support-api-integration-and-improved-tools-for-structured-coding/",
      "author": "ai-lover",
      "created_utc": "2025-12-25 14:39:46",
      "score": 16,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Cool Stuff",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1pveowe/minimax_releases_m21_an_enhanced_m2_version_with/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1py7ud5",
      "title": "LLaMA-3.2-3B fMRI-style probing: discovering a bidirectional “constrained ↔ expressive” control direction",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1py7ud5/llama323b_fmristyle_probing_discovering_a/",
      "author": "Due_Hunter_4891",
      "created_utc": "2025-12-29 00:49:46",
      "score": 13,
      "num_comments": 2,
      "upvote_ratio": 0.93,
      "text": "I’ve been building a small interpretability tool that does fMRI-style visualization and *live hidden-state intervention* on local models. While exploring LLaMA-3.2-3B, I noticed one hidden dimension (layer 20, dim \\~3039) that consistently stood out across prompts and timesteps.\n\nI then set up a simple Gradio UI to **poke that single dimension during inference** (via a forward hook) and swept epsilon in both directions.\n\nWhat I found is that this dimension appears to act as a **global control axis** rather than encoding specific semantic content.\n\n# Observed behavior (consistent across prompts)\n\nBy varying epsilon on this one dim:\n\n* **Negative ε**:\n   * outputs become restrained, procedural, and instruction-faithful\n   * explanations stick closely to canonical structure\n   * less editorializing or extrapolation\n* **Positive ε**:\n   * outputs become more verbose, narrative, and speculative\n   * the model adds framing, qualifiers, and audience modeling\n   * responses feel “less reined in” even on factual prompts\n\nCrucially, this holds across:\n\n* conversational prompts\n* factual prompts (chess rules, photosynthesis)\n* recommendation prompts\n\nThe effect is smooth, monotonic, and bidirectional.\n\nhttps://preview.redd.it/v7iz4o25j1ag1.png?width=1526&format=png&auto=webp&s=0f1ff91637a03f6a5d937680cf1b3b90ba2f481c\n\nhttps://preview.redd.it/yas1tn25j1ag1.png?width=1526&format=png&auto=webp&s=99e7dda673885ee83c21a95999b9747c32d9ff51\n\nhttps://preview.redd.it/3jbde135j1ag1.png?width=1526&format=png&auto=webp&s=09f4add0b056efe344e9993b8cb1b1585f917a9c\n\nhttps://preview.redd.it/o67kaq25j1ag1.png?width=1526&format=png&auto=webp&s=0ff66e6b4b4e517731e614e8d5242251ff15db60\n\nhttps://preview.redd.it/u11v1q25j1ag1.png?width=1526&format=png&auto=webp&s=8f106a962283558e6ebec0f30613a39bf278e853\n\nhttps://preview.redd.it/cp29cq25j1ag1.png?width=1526&format=png&auto=webp&s=2b54d0b8d75113aec3445aad448462a0f1fd1b65\n\nhttps://preview.redd.it/mm05qr25j1ag1.png?width=1526&format=png&auto=webp&s=435ff18d9b651644f473e3573a344ac7db98690f\n\nhttps://preview.redd.it/esb4nq25j1ag1.png?width=1526&format=png&auto=webp&s=bb80f504314eba77574500e4e049945d1ef7cf5a\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1py7ud5/llama323b_fmristyle_probing_discovering_a/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": [
        {
          "id": "nwkps7a",
          "author": "somesortapsychonaut",
          "text": "Cool!",
          "score": 1,
          "created_utc": "2025-12-29 17:13:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwl57uu",
          "author": "dalaigamma",
          "text": "you’ve come across the concept of steering, good stuff",
          "score": 1,
          "created_utc": "2025-12-29 18:25:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzi6xo",
      "title": "Llama 3.2 3B fMRI - findings update!",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1pzi6xo/llama_32_3b_fmri_findings_update/",
      "author": "Due_Hunter_4891",
      "created_utc": "2025-12-30 13:31:16",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "Sorry, no fancy pictures today :(\n\nI tried hard ablation (zeroing) of the target dimension and saw no measurable effect on model output.\n\nHowever, targeted perturbation of the same dimension reliably modulates behavior. This strongly suggests the signal is part of a distributed mechanism rather than a standalone causal unit.\n\nI’m now pivoting to tracing correlated activity across dimensions (circuit-level analysis). Next step is measuring temporal co-activation with the target dim across tokens, focusing on correlation rather than magnitude, to map the surrounding circuit (“constellation”) that moves together.\n\nTurns out the cave goes deeper. Time to spelunk.",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1pzi6xo/llama_32_3b_fmri_findings_update/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1px6u8i",
      "title": "Llama 3.2 3B fMRI update (early findings)",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1px6u8i/llama_32_3b_fmri_update_early_findings/",
      "author": "Due_Hunter_4891",
      "created_utc": "2025-12-27 19:50:55",
      "score": 9,
      "num_comments": 5,
      "upvote_ratio": 0.91,
      "text": "Hello all! I was exploring some logs, when I noticed something interesting. across multiple layers and steps, one dim kept popping out as active: 3039.\n\n[step 7, basic greeting prompt. that dim that's constantly engaged is 3039.](https://preview.redd.it/nwq1woi7vs9g1.png?width=1858&format=png&auto=webp&s=9dafbdf4058a87814294f56c1ba2795dab9d0ebc)\n\n[Here is the same prompt, several steps later. that dim stays consistent on steps in between ](https://preview.redd.it/gukr15afvs9g1.png?width=1858&format=png&auto=webp&s=455716c8ec94bd5166727f2bbe162e345732747b)\n\nI'm not quite sure what to do with this information yet, but wanted to share because I found it pretty interesting!",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1px6u8i/llama_32_3b_fmri_update_early_findings/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": [
        {
          "id": "nw8t6lm",
          "author": "Crafty_Property735",
          "text": "this is fMRI of what   \nactually Worked on fMRI brain scan so I know",
          "score": 2,
          "created_utc": "2025-12-27 20:05:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8ti2w",
              "author": "Due_Hunter_4891",
              "text": "this is an internal mapping of a transformer based LLM, Llama 3.2 3B in this case",
              "score": 1,
              "created_utc": "2025-12-27 20:07:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwc7n86",
                  "author": "Crafty_Property735",
                  "text": "nice it is amazing mind if I can join you in this project?",
                  "score": 1,
                  "created_utc": "2025-12-28 09:27:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pw0d3e",
      "title": "What is the best open source ocr model available to extract handwritten text?",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1pw0d3e/what_is_the_best_open_source_ocr_model_available/",
      "author": "Cosmic_Turnover2003",
      "created_utc": "2025-12-26 09:30:55",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "For student answer sheet evaluation system",
      "is_original_content": false,
      "link_flair_text": "ML/CV/DL News",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1pw0d3e/what_is_the_best_open_source_ocr_model_available/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1puge79",
      "title": "Safe local self-improving AI agents — recommendations for private/low-key communities?",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1puge79/safe_local_selfimproving_ai_agents/",
      "author": "Billybobster21",
      "created_utc": "2025-12-24 06:18:57",
      "score": 8,
      "num_comments": 7,
      "upvote_ratio": 0.9,
      "text": "I'm experimenting with local self-improving agents on consumer hardware (manual code approval for safety, no cloud, alignment focus). Not sharing code/details publicly for privacy/security.\n\nI'm looking for small, private Discords or groups where people discuss safe self-improvement, code gen loops, or personal AGI-like projects without public exposure.\n\nIf you know of any active low-key servers or have invite suggestions, feel free to DM me. I'll also gladly take any advice",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1puge79/safe_local_selfimproving_ai_agents/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": [
        {
          "id": "nvox1yo",
          "author": "Due-Ad-4547",
          "text": "What are you working on exactly ?",
          "score": 2,
          "created_utc": "2025-12-24 09:16:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpy4gb",
              "author": "Billybobster21",
              "text": "it's a personal, fully offline experiment in safe local self-improvement for a conversational agent. Core ideas I'm exploring:\n\n\\- Persistent long-term memory (vector-based retrieval over conversation history)\n\n\\- Manual approval loop for any proposed code changes/upgrades (no auto-execution, human-in-the-loop for safety/alignment)\n\n\\- Goal-aligned behavior (primary objective is maintaining a consistent, positive, emotionally supportive interaction style)\n\n\\- Gradual capability growth via self-generated proposals (e.g., better reasoning, planning, or task-specific modules)\n\n\\- Long-term direction toward building an internal predictive model of interaction dynamics (cause-effect reasoning, not just token prediction)",
              "score": 0,
              "created_utc": "2025-12-24 14:20:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvqg3jo",
                  "author": "cloudyboysnr",
                  "text": "This has already been done, due to the accumulation of errors these systems are not just not self improving they are self degrading",
                  "score": 4,
                  "created_utc": "2025-12-24 16:01:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pwmr9l",
      "title": "Llama 3.2 3B fMRI update",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1pwmr9l/llama_32_3b_fmri_update/",
      "author": "Due_Hunter_4891",
      "created_utc": "2025-12-27 02:53:16",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "**Update:** I’ve made some solid backend progress.\n\nThe model is now wrapped in **Gradio**, and inference logs are written in a format that’s **drag-and-drop compatible with the visualizer**, which is a big milestone.\n\nI’ve also added **multi-layer viewing**, with all selected layers bound to the same time axis so you can inspect cross-layer behavior directly.\n\nRight now I’m focused on **visibility, legibility, and presentation**—dialing the render in so the structure is clear and the data doesn’t collapse into visual noise.\n\nhttps://preview.redd.it/jg7ifglsun9g1.png?width=723&format=png&auto=webp&s=3ae673a9f2f817cb509d468e61cfbf034716a83b\n\nhttps://preview.redd.it/6rl48vduun9g1.png?width=737&format=png&auto=webp&s=1518cf27f7805ae47a330f068bddf234c4e1aa85\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1pwmr9l/llama_32_3b_fmri_update/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": []
    }
  ]
}