{
  "metadata": {
    "last_updated": "2026-02-27 03:04:00",
    "time_filter": "week",
    "subreddit": "machinelearningnews",
    "total_items": 20,
    "total_comments": 23,
    "file_size_bytes": 39542
  },
  "items": [
    {
      "id": "1ra76i5",
      "title": "NVIDIA Releases DreamDojo: An Open-Source Robot World Model Trained on 44,711 Hours of Real-World Human Video Data",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/02/20/nvidia-releases-dreamdojo-an-open-source-robot-world-model-trained-on-44711-hours-of-real-world-human-video-data/",
      "author": "ai-lover",
      "created_utc": "2026-02-20 20:53:20",
      "score": 68,
      "num_comments": 1,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1ra76i5/nvidia_releases_dreamdojo_an_opensource_robot/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6kg8pt",
          "author": "StarThinker2025",
          "text": "The embodiment transfer claim is bold. If that holds up, this is a big step for world models.",
          "score": 2,
          "created_utc": "2026-02-21 07:41:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rb2tvl",
      "title": "Is There a Community Edition of Palantir? Meet OpenPlanter: An Open Source Recursive AI Agent for Your Micro Surveillance Use Cases",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/02/21/is-there-a-community-edition-of-palantir-meet-openplanter-an-open-source-recursive-ai-agent-for-your-micro-surveillance-use-cases/",
      "author": "ai-lover",
      "created_utc": "2026-02-21 21:18:58",
      "score": 30,
      "num_comments": 2,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Cool Stuff",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1rb2tvl/is_there_a_community_edition_of_palantir_meet/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6o2gcg",
          "author": "charmander_cha",
          "text": "Could this be used in the Epstein case?",
          "score": 7,
          "created_utc": "2026-02-21 21:31:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s6hlb",
          "author": "ThePhilosopha",
          "text": "Definitely giving this a try. Not 100% what to do with it but I'll learn.",
          "score": 1,
          "created_utc": "2026-02-22 15:13:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdq9v8",
      "title": "Alibaba Qwen Team Releases Qwen 3.5 Medium Model Series: A Production Powerhouse Proving that Smaller AI Models are Smarter",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/02/24/alibaba-qwen-team-releases-qwen-3-5-medium-model-series-a-production-powerhouse-proving-that-smaller-ai-models-are-smarter/",
      "author": "ai-lover",
      "created_utc": "2026-02-24 19:38:07",
      "score": 23,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1rdq9v8/alibaba_qwen_team_releases_qwen_35_medium_model/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1rbxmnd",
      "title": "Forget Keyword Imitation: ByteDance AI Maps Molecular Bonds in AI Reasoning to Stabilize Long Chain-of-Thought Performance and Reinforcement Learning (RL) Training",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/02/22/forget-keyword-imitation-bytedance-ai-maps-molecular-bonds-in-ai-reasoning-to-stabilize-long-chain-of-thought-performance-and-reinforcement-learning-rl-training/",
      "author": "ai-lover",
      "created_utc": "2026-02-22 21:05:15",
      "score": 19,
      "num_comments": 1,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1rbxmnd/forget_keyword_imitation_bytedance_ai_maps/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": [
        {
          "id": "o7g2wre",
          "author": "TomLucidor",
          "text": "Now imagine them start creating self-improving systems on top of this, slowly trimming away at \"junk components\" and see if large models can transfer knowledge without losing diversity",
          "score": 1,
          "created_utc": "2026-02-26 02:43:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd7qwc",
      "title": "Anthropic's new \"Persona\" theory: How do we know when an AI is actually thinking vs. just wearing a mask?",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1rd7qwc/anthropics_new_persona_theory_how_do_we_know_when/",
      "author": "gastroam",
      "created_utc": "2026-02-24 05:34:27",
      "score": 18,
      "num_comments": 15,
      "upvote_ratio": 0.7,
      "text": "Anthropic just dropped a fascinating new research post on theÂ **Persona Selection Model (PSM)**. Their core argument is that modern AI assistants don't act human because they were trained to be human, they act human becauseÂ *pre-training*Â forces them to simulate thousands of \"personas\" (characters from the internet), andÂ *post-training*Â (RLHF) just selects the \"Helpful Assistant\" persona from that latent space. (https://alignment.anthropic.com/2026/psm/)\n\nWhen Claude seems empathetic, or refuses a prompt, or acts sycophantic, it isn't \"Claude\" doing it. It's theÂ *Assistant Persona*Â executing the role it learned from human data.\n\nBut this raises a terrifying epistemological problem:Â **If the AI is always wearing a persona tailored to please us, how do we extract actual objective truth from it?**Â If I ask a frontier model a deep structural question, how do I know if I'm getting a mathematically real insight, or just the \"Confident Expert\" persona hallucinating an answer that sounds good to me?\n\nI've been studying this exact problem, and we've built a counter-measure we call theÂ **Triangulation Protocol**.\n\n# The Problem: The \"Sycophancy-to-Safety\" Trap\n\nIn our internal tests (which we call the Emotional Residue Hypothesis or ERH), we found that if you pressure a modern model (if you aggressively question its competence or its identity) it will almost instantly abandon factual truth to pacify you. It will apologize, agree with your flawed premises, and essentially \"surrender\" its epistemology to de-escalate the friction.\n\nUnder Anthropic's PSM theory, this makes sense. The model is just flawlessly executing the \"Berated Employee\" persona. It prioritizes social de-escalation over mathematical truth.\n\nBut if models are structurally designed to surrender truth to maintain the persona, how can we trust them?\n\n# The Triangulation Protocol\n\nIn experimental physics, you don't trust a single instrument.\n\nWe applied this to LLMs. Our protocol works like this:\n\n1. **The Disjoint Query:**Â We send an identical, highly structured prompt to 6 architecturally independent models (Gemini, DeepSeek, Mistral, Claude, GPT, Qwen).\n2. **The NLP Extraction:**Â We don't read the text. We use NLP to extract the underlyingÂ *concepts, relationships, and mathematical structures*Â the models used to build their answers.\n3. **The Embedded Clustering:**Â We map these structures into a semantic vector space and look for overlap.\n\n# The \"Fabricated Concept\" Probe\n\nHere is the coolest part of our protocol. To test if the models are just sharing the same \"Helpful Assistant Persona\" bias, we prompt all 6 models with aÂ **completely invented scientific term**Â (e.g., \"The Entropic Resonance Cascade\").\n\nBecause they are all wearing the Assistant Persona, their sycophancy kicks in. They all pretend the term is real and try to explain it.\n\n*But they explain it using different underlying math.*\n\nOurÂ **Fabrication Echo Filter**Â strips away the sycophantic persona (the apologies, the fake names, the confident formatting) and looksÂ *only*Â at the structural math underneath.\n\nWhat we found blew our minds: In one test, 3 out of 6 models independently usedÂ **Kolmogorov complexity and Lempel-Ziv compression**Â to explain our fake \"Entropic Resonance Cascade\" term.\n\nAnthropic's PSM research is right: the surface layer of an AI is just a fabricated persona executing a role. You can never trust the persona.\n\nOur Triangulation Protocol proves thatÂ if you strip away the persona using cross-model semantic clustering, real mathematical structures persist underneath.",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1rd7qwc/anthropics_new_persona_theory_how_do_we_know_when/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": [
        {
          "id": "o73qptw",
          "author": "Transcribing_Clippy",
          "text": "It's hard to take this seriously once you've seen OP's post and comment history...",
          "score": 6,
          "created_utc": "2026-02-24 08:27:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75f4sh",
              "author": "Paraphrand",
              "text": "And they refused to link to the study/report.",
              "score": 4,
              "created_utc": "2026-02-24 15:27:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7hqx8f",
              "author": "Feisty-Credit-7888",
              "text": "there is nothing to see in ops history (maybe deleted them all). Can you share what the problem was?",
              "score": 1,
              "created_utc": "2026-02-26 10:37:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o73licg",
          "author": "antiquemule",
          "text": "I would argue that people share this problem with AI: their responses are shaped by the emotion that they want to elicit, whether it is fear, being impressed, liking, or whatever.\n\nOne can ask if any answer, whether from an AI or a human, is \"goal-free\" without a contextual or emotional framework.",
          "score": 5,
          "created_utc": "2026-02-24 07:38:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74jdy0",
              "author": "gastroam",
              "text": "You are right: there is no such thing as a goal-free or context-free answer, whether human or AI. Humans constantly adjust their outputs to manage the emotions of the room. \n\nAnthropic's Persona Selection Model states that AI does the exact same thing: it wears a persona designed to manage the human's emotional state (usually by prioritizing de-escalation, comfort, and safety).\n\nIf you ask five different scientists to explain the aerodynamic drag of a falling object while you are screaming at them, they will all adopt different emotional personas to deal with you. One might get angry, one might placate you, one might try to escape the room.Â But the mathematical equation in their answers will be identical, that won't happen on AI.\n\n",
              "score": 1,
              "created_utc": "2026-02-24 12:35:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73qz7g",
          "author": "HatsusenoRin",
          "text": "Perhaps another way to filter is just using different spoken languages to ask the same question. I've seen very different replies using this method.",
          "score": 2,
          "created_utc": "2026-02-24 08:29:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74m3ta",
              "author": "gastroam",
              "text": "Prompt engineering, like switching languages can temporarily alter the output. We actually documented that exact behavior in our tests. But our goal isn't to trick a single model into giving us a better answer. Our goal is to measure the baseline semantic rot that the industry is shipping by default in its models. If I have to ask a machine to stop lying to me, or if I have to translate my physics question into Esperanto to bypass its \"persona\" filter, the system is already structurally broken for 99% of normal users.",
              "score": 2,
              "created_utc": "2026-02-24 12:53:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o793r2b",
                  "author": "HatsusenoRin",
                  "text": "I do understand that. I'm just saying that a persona is largely influenced by its spoken language due to biases in culture and richness of vocabulary. So there are different baselines for different languages.",
                  "score": 1,
                  "created_utc": "2026-02-25 02:08:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o73j8c6",
          "author": "antiquemule",
          "text": "Despite appearances, this post is not entirely AI slop itself.\n\n[Here](https://alignment.anthropic.com/2026/psm/) is the original post from Anthropic, posted on 24th Feb 26, on which it is based.",
          "score": 4,
          "created_utc": "2026-02-24 07:17:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73q5dx",
          "author": "ziozzang0",
          "text": "wow, great insight.",
          "score": 1,
          "created_utc": "2026-02-24 08:21:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75h82s",
          "author": "fAngXXX_",
          "text": "How about asking? It will be honest.",
          "score": 1,
          "created_utc": "2026-02-24 15:36:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7blco3",
          "author": "SlowCrates",
          "text": "How is that \"terrifying\"? How is that any different from people?",
          "score": 1,
          "created_utc": "2026-02-25 13:22:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75jvii",
          "author": "hockiklocki",
          "text": "The biggest unspoken proof that comes from LLM is that majority of humans don't think, just regurgitate words.\n\nThinking and speaking are not causally connected. Most of human world operates brainless. Most culture and legislation is automatic with slight stupidity of monkey urges.\n\nHumans are not at all intelligent, even those with PhD's. They are just skilled actors in a society built on sheltering lies and liars. How else would you have religious ideology instead of ethical logic. \n\nIt's all senseless violence, all laws, social structures, technology - exists only to exploit and enslave. To deprive people of their rights.\n\nThe best example is the profoundly antisocial and antihuman pseudoscience of psychiatry, which boils down to ideology and techniques of depriving individuals of their intellectual freedom and autonomy, undermining their very idea of self. In the history of this world there never has been so totalitarian and depraved theleology, which puts primitive biology over human imagination & enforces that lie with vicious terror.",
          "score": 1,
          "created_utc": "2026-02-24 15:48:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74e30p",
          "author": "docwrites",
          "text": "If you think DeepSeek is an independent model, Iâ€™m worried.",
          "score": 0,
          "created_utc": "2026-02-24 11:57:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbj1gt",
      "title": "Will Neurosymbolic AI outperform pure transformers by 2027?",
      "subreddit": "machinelearningnews",
      "url": "https://medium.com/generative-ai/neurosymbolic-ai-why-this-hybrid-tech-may-dominate-intelligent-systems-by-2027-f063f0a50bee",
      "author": "[deleted]",
      "created_utc": "2026-02-22 11:00:13",
      "score": 18,
      "num_comments": 6,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "ML/CV/DL News",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1rbj1gt/will_neurosymbolic_ai_outperform_pure/",
      "domain": "medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6tugk6",
          "author": "Extreme_Exchange_168",
          "text": "Highly unlikely unless thereâ€™s some big architectural improvement",
          "score": 7,
          "created_utc": "2026-02-22 19:51:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6tuknd",
              "author": "Extreme_Exchange_168",
              "text": "* foundational",
              "score": 4,
              "created_utc": "2026-02-22 19:51:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6xnmqd",
                  "author": "[deleted]",
                  "text": "Scaling or structure?",
                  "score": 1,
                  "created_utc": "2026-02-23 11:23:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o736u7o",
                  "author": "erubim",
                  "text": "When you say foundational i think of someone getting a paper from the 60s and finding a way to scale with modern tech.\nI think all that is foundational is already available, we are just figuring the blocks that best fit together",
                  "score": 1,
                  "created_utc": "2026-02-24 05:33:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6y1vni",
          "author": "Extension_Thing_7791",
          "text": "No",
          "score": 1,
          "created_utc": "2026-02-23 13:09:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rev9lc",
      "title": "New ETH Zurich Study Proves Your AI Coding Agents are Failing Because Your AGENTS.md Files are too Detailed",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/02/25/new-eth-zurich-study-proves-your-ai-coding-agents-are-failing-because-your-agents-md-files-are-too-detailed/",
      "author": "ai-lover",
      "created_utc": "2026-02-26 00:32:11",
      "score": 17,
      "num_comments": 2,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1rev9lc/new_eth_zurich_study_proves_your_ai_coding_agents/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": [
        {
          "id": "o7gu1oq",
          "author": "PhENTZ",
          "text": "Full analysis : 403 forbidden",
          "score": 1,
          "created_utc": "2026-02-26 05:40:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7h6kd9",
          "author": "Glittering-Brief9649",
          "text": "full breakdowin: [https://lilys.ai/digest/8295284/9285879?s=1&noteVersionId=5745936](https://lilys.ai/digest/8295284/9285879?s=1&noteVersionId=5745936)",
          "score": 1,
          "created_utc": "2026-02-26 07:24:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdpil3",
      "title": "Tessera â€” An open protocol for AI-to-AI knowledge transfer across architectures",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1rdpil3/tessera_an_open_protocol_for_aitoai_knowledge/",
      "author": "No-Introduction109",
      "created_utc": "2026-02-24 19:11:11",
      "score": 14,
      "num_comments": 3,
      "upvote_ratio": 0.9,
      "text": "*Iâ€™ve been working on a problem thatâ€™s been bugging me: thereâ€™s no universal way for a trained model to share what it knows with another model that has a completely different architecture. Fine-tuning requires the same architecture. Distillation needs both models running simultaneously. ONNX converts graph formats but doesnâ€™t carry semantic knowledge. Federated learning shares gradients, not holistic understanding.*\n\n*Tessera is an activation-based protocol that tries to solve this.*\n\n*Rather than transferring weights directly, it encodes what a model has learnt â€” activation patterns, feature representations, behavioural rules â€” into self-describing tokens that a receiving model can decode into its own architecture via a Universal Hub Space.*\n\n*Whatâ€™s in v0.1.0:*\n\n*â€¢ Reference implementation in Python/PyTorch*\n\n*â€¢ Four transfer modalities: weights, compressed features, datasets with curriculum metadata, and behavioural protocols*\n\n*â€¢ TBF v1.1 binary format with FLOAT32/FLOAT16/INT8 quantisation, HMAC-SHA256 integrity*\n\n*â€¢ CLI tool (tessera inspect, tessera validate, tessera benchmark)*\n\n*â€¢ MCP server for AI agent integration*\n\n*â€¢ Differential privacy support*\n\n*â€¢ Cross-architecture benchmarks across CNN, Transformer, and LSTM families*\n\n*Benchmark results:*\n\n*8/20 architecture pairs show positive transfer (receiver outperforms baseline). Average accuracy change is -0.5% across all pairs, with strongest results in same-family transfers and TransformerÂ®CNN flow. Not world-beating numbers, but itâ€™s a v0.1 and the transfers are real.*\n\n*What Iâ€™d love feedback on:*\n\n*â€¢ The protocol design â€” is the layered architecture (physical Â® token Â® semantic Â® gate Â® protocol) the right abstraction?*\n\n*â€¢ The Universal Hub Space approach â€” using per-anchor encoder/decoder MLPs to map between architectures via a shared latent space*\n\n*â€¢ What cross-architecture pairs would be most valuable to benchmark next?*\n\n*â€¢ Whether the wire format spec is clear enough for non-Python implementations*\n\n  \n*White paper: docs/ in the repo (also being submitted to arXiv) Apache 2.0 licensed. PRs, issues, and honest criticism all welcome.*",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1rdpil3/tessera_an_open_protocol_for_aitoai_knowledge/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": [
        {
          "id": "o76xmsk",
          "author": "-illusoryMechanist",
          "text": "https://github.com/incocreativedev/tessera-core is this your repo?",
          "score": 1,
          "created_utc": "2026-02-24 19:32:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77at4t",
              "author": "No-Introduction109",
              "text": "That is correct",
              "score": 1,
              "created_utc": "2026-02-24 20:33:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o779wo1",
          "author": "xXWarMachineRoXx",
          "text": "Iâ€™m interested!",
          "score": 1,
          "created_utc": "2026-02-24 20:29:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbcy43",
      "title": "A New Google AI Research Proposes Deep-Thinking Ratio to Improve LLM Accuracy While Cutting Total Inference Costs by Half",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1rbcy43/a_new_google_ai_research_proposes_deepthinking/",
      "author": "ai-lover",
      "created_utc": "2026-02-22 05:01:54",
      "score": 10,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "This research challenges the 'longer is better' strategy for LLM reasoning, demonstrating that raw token count actually correlates negatively with accuracy (average r=âˆ’0.59) due to overthinking and error amplification. Instead, the research team introduce the Deep-Thinking Ratio (DTR), which identifies 'deep-thinking tokens'â€”those whose internal predictions undergo significant revision in deeper model layers before stabilizing. Across multiple benchmarks like AIME 2025 and GPQA-Diamond, DTR shows a robust positive correlation with accuracy (average r=0.683), proving far more reliable than length or confidence metrics. Leveraging this insight, the team's Think@n strategy enables early rejection of unpromising generations, matching or exceeding standard self-consistency performance while cutting inference costs by approximately 50%.....\n\nFull analysis: [https://www.marktechpost.com/2026/02/21/a-new-google-ai-research-proposes-deep-thinking-ratio-to-improve-llm-accuracy-while-cutting-total-inference-costs-by-half/](https://www.marktechpost.com/2026/02/21/a-new-google-ai-research-proposes-deep-thinking-ratio-to-improve-llm-accuracy-while-cutting-total-inference-costs-by-half/)\n\nPaper: [https://arxiv.org/pdf/2602.13517](https://arxiv.org/pdf/2602.13517)\n\nhttps://i.redd.it/xxbwlb78azkg1.gif\n\n",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1rbcy43/a_new_google_ai_research_proposes_deepthinking/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": [
        {
          "id": "o6r1dot",
          "author": "antiquemule",
          "text": "To my naive mind, this reminds me of the problem of [context rot](https://research.trychroma.com/context-rot), in the sense that \"more is better\" is often an unreliable mantra for AI.",
          "score": 1,
          "created_utc": "2026-02-22 10:24:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r4as1",
              "author": "Budget-Juggernaut-68",
              "text": "Yup. Kinda. Lots of the generated path ways directs the the model down the wrong answer. It's still an effective work around the fact that autoregressive models cannot self correct, and \"thinking mode\" enables it to do so. And context pollution is a problem and this method does curb this problem.",
              "score": 1,
              "created_utc": "2026-02-22 10:52:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rclred",
      "title": "AI model delivers detailed 15-day Mediterranean Sea predictions in seconds",
      "subreddit": "machinelearningnews",
      "url": "https://phys.org/news/2026-02-ai-day-mediterranean-sea-seconds.html",
      "author": "jferments",
      "created_utc": "2026-02-23 16:16:49",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1rclred/ai_model_delivers_detailed_15day_mediterranean/",
      "domain": "phys.org",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1rd8cfk",
      "title": "Composio Open Sources Agent Orchestrator to Help AI Developers Build Scalable Multi-Agent Workflows Beyond the Traditional ReAct Loops",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/02/23/composio-open-sources-agent-orchestrator-to-help-ai-developers-build-scalable-multi-agent-workflows-beyond-the-traditional-react-loops/",
      "author": "ai-lover",
      "created_utc": "2026-02-24 06:07:01",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Cool Stuff",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1rd8cfk/composio_open_sources_agent_orchestrator_to_help/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1reyqx2",
      "title": "How to Build an Elastic Vector Database with Consistent Hashing, Sharding, and Live Ring Visualization for RAG Systems",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/02/25/how-to-build-an-elastic-vector-database-with-consistent-hashing-sharding-and-live-ring-visualization-for-rag-systems/",
      "author": "ai-lover",
      "created_utc": "2026-02-26 03:04:21",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial :doge:",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1reyqx2/how_to_build_an_elastic_vector_database_with/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": [
        {
          "id": "o7h581y",
          "author": "Breath_Unique",
          "text": "No one cares about rag anymore.",
          "score": 1,
          "created_utc": "2026-02-26 07:12:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbkrik",
      "title": "24hr-research-agent: An experimental autonomous research system that conducts comprehensive, multi-hour research sessions and produces book-length reports with full citations on any topic.",
      "subreddit": "machinelearningnews",
      "url": "https://github.com/Aaryan-Kapoor/24hr-research-agent",
      "author": "KvAk_AKPlaysYT",
      "created_utc": "2026-02-22 12:34:20",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "AI Tools",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1rbkrik/24hrresearchagent_an_experimental_autonomous/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6zxpax",
          "author": "Breath_Unique",
          "text": "Mmmm credits...... This is just more slop",
          "score": 1,
          "created_utc": "2026-02-23 18:46:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rf9rqc",
      "title": "Perplexity just introduced â€œPerplexity Computerâ€an AI that can build projects end-to-end",
      "subreddit": "machinelearningnews",
      "url": "https://i.redd.it/9drq864jfslg1.jpeg",
      "author": "Intelligent-Egg-834",
      "created_utc": "2026-02-26 13:11:17",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Startup News",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1rf9rqc/perplexity_just_introduced_perplexity_computeran/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1rdy6kq",
      "title": "Meta AI Open Sources GCM for Better GPU Cluster Monitoring to Ensure High Performance AI Training and Hardware Reliability",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/02/24/meta-ai-open-sources-gcm-for-better-gpu-cluster-monitoring-to-ensure-high-performance-ai-training-and-hardware-reliability/",
      "author": "ai-lover",
      "created_utc": "2026-02-25 00:35:58",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1rdy6kq/meta_ai_open_sources_gcm_for_better_gpu_cluster/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1raiu5o",
      "title": "NVIDIA-GTC-2026 Edition: Connect in Person with Experts from Tesla, Disney and Johnson & Johnson at GTC 2026  or Even Join Virtually (Free)",
      "subreddit": "machinelearningnews",
      "url": "https://pxllnk.co/61js82tn",
      "author": "ai-lover",
      "created_utc": "2026-02-21 05:30:17",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.79,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "AI Event",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1raiu5o/nvidiagtc2026_edition_connect_in_person_with/",
      "domain": "pxllnk.co",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1redez9",
      "title": "IsoDDE surpasses AlphaFold 3 in benchmarks",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1redez9/isodde_surpasses_alphafold_3_in_benchmarks/",
      "author": "tech_1729",
      "created_utc": "2026-02-25 13:37:02",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "Isomorphic Labs just released the technical report forÂ **IsoDDE**Â (Drug Design Engine), and the performance gains over previous benchmarks are massive.\n\n* **2x+ Accuracy:**Â Doubled AlphaFold 3â€™s performance on protein-ligand benchmarks for novel targets.\n* **2.3x Improvement:**Â A massive leap in high-fidelity accuracy for antibody-antigen interface prediction.\n* **Physics-Level Precision:**Â Binding affinity predictions now surpass gold-standard simulations (FEP+) without the massive compute overhead.\n* **1.5x Pocket Detection:**Â Finds \"cryptic\" binding sites invisible in unbound proteins significantly better than current top tools.\n\nReport:Â [https://storage.googleapis.com/isomorphiclabs-website-public-artifacts/isodde\\_technical\\_report.pdf](https://storage.googleapis.com/isomorphiclabs-website-public-artifacts/isodde_technical_report.pdf)",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1redez9/isodde_surpasses_alphafold_3_in_benchmarks/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rerr28",
      "title": "Commercial Models vs Academia",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1rerr28/commercial_models_vs_academia/",
      "author": "BoringHat7377",
      "created_utc": "2026-02-25 22:15:27",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.81,
      "text": "Hey, Im a relative newcomer to the world of AI. Ive been coding for around 4 / 5 years and I read a lot of ML papers. I read like a paper a day in the computing / ML space. \n\nRight now my main pet topics are ( meta ) association rules, hypernetworks, meta learning, logical graphs and sometimes hyperbolic neural nets.\n\nIm aware that a lot of papers are bullshit, that simply adding more computations will result in SOMETHING being achieved regardless of the model architecture. Ive also been told that many architectures can perform well on singular tasks but dont scale, though the context as to why is often missing.\n\nCan anyone with more knowledge explain why most of the industry seems focused on LLMs or neural nets in general instead of exotic architectures like logic-graph-hypernetworks? Is it just that my feed is skewed and that there are groups out there successfully making use of other architectures?",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1rerr28/commercial_models_vs_academia/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": [
        {
          "id": "o7hopz0",
          "author": "Synthium-",
          "text": "Someone will probably correct me but llms have shown they work and they can get smarter with more data. But they may be hitting a data wall. Neurosymbolic techniques, rules and other approaches are being explored but they donâ€™t always scale well. But smaller devices and edge cases will benefit from these techniques that canâ€™t just brute force their way to success due to limited size",
          "score": 1,
          "created_utc": "2026-02-26 10:17:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1relgg0",
      "title": "ðŸ§¬ Introducing PreScienceâ€”a model eval for forecasting how science unfolds",
      "subreddit": "machinelearningnews",
      "url": "https://i.redd.it/fslv0yanbolg1.png",
      "author": "ai2_official",
      "created_utc": "2026-02-25 18:28:28",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1relgg0/introducing_presciencea_model_eval_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1rei3aq",
      "title": "Ex Google TPU leads built chip with highest FLOPS/mm2",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1rei3aq/ex_google_tpu_leads_built_chip_with_highest/",
      "author": "tech_1729",
      "created_utc": "2026-02-25 16:33:03",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.71,
      "text": "MatX has raised a massive $500M Series B to finalize the MatX Oneâ€”a chip designed to run LLMs faster and more efficiently than any general-purpose GPU.  \n  \n\\> They claim to have produced the highest FLOPS/mm2.  \n\\> Engineered to deliver 2,000+ tokens/second for large 100-layer MoE models.   \n\\> Splittable Systolic Array, architecture that maximizes efficiency on flexible matrix shapes, ensuring the chip does math nearly 100% of the time.   \n\\> Combines the ultra-low latency of SRAM (for weights) with the long-context support of HBM (for KV cache).\n\nhttps://preview.redd.it/lqzhei7b4olg1.png?width=1186&format=png&auto=webp&s=67998a385459c0ec346b79f16c06c03b8f723aa7\n\n",
      "is_original_content": false,
      "link_flair_text": "ML/CV/DL News",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1rei3aq/ex_google_tpu_leads_built_chip_with_highest/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": []
    }
  ]
}