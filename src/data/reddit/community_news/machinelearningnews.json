{
  "metadata": {
    "last_updated": "2026-01-07 16:56:07",
    "time_filter": "week",
    "subreddit": "machinelearningnews",
    "total_items": 14,
    "total_comments": 17,
    "file_size_bytes": 39254
  },
  "items": [
    {
      "id": "1q65i6l",
      "title": "NVIDIA AI Released Nemotron Speech ASR: A New Open Source Transcription Model Designed from the Ground Up for Low-Latency Use Cases like Voice Agents",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/01/06/nvidia-ai-released-nemotron-speech-asr-a-new-open-source-transcription-model-designed-from-the-ground-up-for-low-latency-use-cases-like-voice-agents/",
      "author": "ai-lover",
      "created_utc": "2026-01-07 04:17:27",
      "score": 17,
      "num_comments": 0,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Cool Stuff",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1q65i6l/nvidia_ai_released_nemotron_speech_asr_a_new_open/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pzqdyd",
      "title": "Alibaba Tongyi Lab Releases MAI-UI: A Foundation GUI Agent Family that Surpasses Gemini 2.5 Pro, Seed1.8 and UI-Tars-2 on AndroidWorld",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2025/12/30/alibaba-tongyi-lab-releases-mai-ui-a-foundation-gui-agent-family-that-surpasses-gemini-2-5-pro-seed1-8-and-ui-tars-2-on-androidworld/",
      "author": "ai-lover",
      "created_utc": "2025-12-30 18:54:11",
      "score": 15,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Cool Stuff",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1pzqdyd/alibaba_tongyi_lab_releases_maiui_a_foundation/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1py7ud5",
      "title": "LLaMA-3.2-3B fMRI-style probing: discovering a bidirectional â€œconstrained â†” expressiveâ€ control direction",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1py7ud5/llama323b_fmristyle_probing_discovering_a/",
      "author": "Due_Hunter_4891",
      "created_utc": "2025-12-29 00:49:46",
      "score": 14,
      "num_comments": 2,
      "upvote_ratio": 0.94,
      "text": "Iâ€™ve been building a small interpretability tool that does fMRI-style visualization and *live hidden-state intervention* on local models. While exploring LLaMA-3.2-3B, I noticed one hidden dimension (layer 20, dim \\~3039) that consistently stood out across prompts and timesteps.\n\nI then set up a simple Gradio UI to **poke that single dimension during inference** (via a forward hook) and swept epsilon in both directions.\n\nWhat I found is that this dimension appears to act as a **global control axis** rather than encoding specific semantic content.\n\n# Observed behavior (consistent across prompts)\n\nBy varying epsilon on this one dim:\n\n* **Negative Îµ**:\n   * outputs become restrained, procedural, and instruction-faithful\n   * explanations stick closely to canonical structure\n   * less editorializing or extrapolation\n* **Positive Îµ**:\n   * outputs become more verbose, narrative, and speculative\n   * the model adds framing, qualifiers, and audience modeling\n   * responses feel â€œless reined inâ€ even on factual prompts\n\nCrucially, this holds across:\n\n* conversational prompts\n* factual prompts (chess rules, photosynthesis)\n* recommendation prompts\n\nThe effect is smooth, monotonic, and bidirectional.\n\nhttps://preview.redd.it/v7iz4o25j1ag1.png?width=1526&format=png&auto=webp&s=0f1ff91637a03f6a5d937680cf1b3b90ba2f481c\n\nhttps://preview.redd.it/yas1tn25j1ag1.png?width=1526&format=png&auto=webp&s=99e7dda673885ee83c21a95999b9747c32d9ff51\n\nhttps://preview.redd.it/3jbde135j1ag1.png?width=1526&format=png&auto=webp&s=09f4add0b056efe344e9993b8cb1b1585f917a9c\n\nhttps://preview.redd.it/o67kaq25j1ag1.png?width=1526&format=png&auto=webp&s=0ff66e6b4b4e517731e614e8d5242251ff15db60\n\nhttps://preview.redd.it/u11v1q25j1ag1.png?width=1526&format=png&auto=webp&s=8f106a962283558e6ebec0f30613a39bf278e853\n\nhttps://preview.redd.it/cp29cq25j1ag1.png?width=1526&format=png&auto=webp&s=2b54d0b8d75113aec3445aad448462a0f1fd1b65\n\nhttps://preview.redd.it/mm05qr25j1ag1.png?width=1526&format=png&auto=webp&s=435ff18d9b651644f473e3573a344ac7db98690f\n\nhttps://preview.redd.it/esb4nq25j1ag1.png?width=1526&format=png&auto=webp&s=bb80f504314eba77574500e4e049945d1ef7cf5a\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1py7ud5/llama323b_fmristyle_probing_discovering_a/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": [
        {
          "id": "nwkps7a",
          "author": "somesortapsychonaut",
          "text": "Cool!",
          "score": 1,
          "created_utc": "2025-12-29 17:13:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwl57uu",
          "author": "dalaigamma",
          "text": "youâ€™ve come across the concept of steering, good stuff",
          "score": 1,
          "created_utc": "2025-12-29 18:25:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q35wec",
      "title": "I took Bernard Widrowâ€™s machine learning & neural networks classes in the early 2000s. Some recollections.",
      "subreddit": "machinelearningnews",
      "url": "https://i.redd.it/uncdscf2x6bg1.png",
      "author": "DueKitchen3102",
      "created_utc": "2026-01-03 20:56:44",
      "score": 14,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "ML/CV/DL News",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1q35wec/i_took_bernard_widrows_machine_learning_neural/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pzi6xo",
      "title": "Llama 3.2 3B fMRI - findings update!",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1pzi6xo/llama_32_3b_fmri_findings_update/",
      "author": "Due_Hunter_4891",
      "created_utc": "2025-12-30 13:31:16",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Sorry, no fancy pictures today :(\n\nI tried hard ablation (zeroing) of the target dimension and saw no measurable effect on model output.\n\nHowever, targeted perturbation of the same dimension reliably modulates behavior. This strongly suggests the signal is part of a distributed mechanism rather than a standalone causal unit.\n\nIâ€™m now pivoting to tracing correlated activity across dimensions (circuit-level analysis). Next step is measuring temporal co-activation with the target dim across tokens, focusing on correlation rather than magnitude, to map the surrounding circuit (â€œconstellationâ€) that moves together.\n\nTurns out the cave goes deeper. Time to spelunk.",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1pzi6xo/llama_32_3b_fmri_findings_update/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q5d4a9",
      "title": "We (this subreddit's admin team) have Released 'AI2025Dev': A Structured Intelligence Layer for AI Models, Benchmarks, and Ecosystem Signals",
      "subreddit": "machinelearningnews",
      "url": "https://ai2025.dev/",
      "author": "ai-lover",
      "created_utc": "2026-01-06 08:28:13",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Cool Stuff",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1q5d4a9/we_this_subreddits_admin_team_have_released/",
      "domain": "ai2025.dev",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q4enix",
      "title": "Tencent Researchers Release Tencent HY-MT1.5: A New Translation Models Featuring 1.8B and 7B Models Designed for Seamless on-Device and Cloud Deployment",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/01/04/tencent-researchers-release-tencent-hy-mt1-5-a-new-translation-models-featuring-1-8b-and-7b-models-designed-for-seamless-on-device-and-cloud-deployment/",
      "author": "ai-lover",
      "created_utc": "2026-01-05 06:47:05",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Cool Stuff",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1q4enix/tencent_researchers_release_tencent_hymt15_a_new/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q5nrx8",
      "title": "Liquid AI Releases LFM2.5: A Compact AI Model Family For Real On Device Agents",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/01/06/liquid-ai-releases-lfm2-5-a-compact-ai-model-family-for-real-on-device-agents/",
      "author": "ai-lover",
      "created_utc": "2026-01-06 16:46:50",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Cool Stuff",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1q5nrx8/liquid_ai_releases_lfm25_a_compact_ai_model/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": [
        {
          "id": "ny19dyu",
          "author": "macromind",
          "text": "This is pretty interesting, especially the focus on \"real on-device agents\". Curious if anyone has benchmarked tool-use style workflows (planner/executor, function calling, etc) on LFM2.5 yet, and how it compares to small Llama variants when latency really matters.\n\nIf youre exploring agentic patterns, Ive been collecting some practical notes and examples here too: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-01-06 16:49:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q17yu4",
      "title": "Llame 3.2 3B fMRI LOAD BEARING DIM FOUND",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1q17yu4/llame_32_3b_fmri_load_bearing_dim_found/",
      "author": "[deleted]",
      "created_utc": "2026-01-01 16:21:16",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 0.82,
      "text": "Iâ€™ve been building a local interpretability toolchain to explore **hidden-dimension coupling** in small LLMs (Llama-3.2-3B-Instruct). This started as visualization (â€œconstellationsâ€ of co-activating dims), but the visuals alone were too noisy to move beyond theory.\n\nSo I rebuilt the pipeline to answer a more specific question:\n\n>\n\n# TL;DR\n\nYes.  \nAnd perturbing the top one causes **catastrophic loss of semantic commitment** while leaving fluency intact.\n\n# Step 1 â€” Reducing noise upstream (not in the renderer)\n\nInstead of rendering everything, I tightened the experiment:\n\n* **Deterministic decoding** (no sampling)\n* **Stratified prompt suite** (baseline, constraints, reasoning, commitment, transitions, etc.)\n* **Event-based logging**, not frame-based\n\nI only logged events where:\n\n* the hero dim was **active**\n* the hero dim was **moving** (std gate)\n* Pearson correlation with another dim was **strong**\n* polarity relationship was consistent\n\nMetrics logged per event:\n\n* Pearson correlation (centered)\n* Cosine similarity (raw geometry)\n* Dot/energy\n* Polarity agreement\n* Classification: `FEATURE` (structural) vs `TRIGGER` (functional)\n\nThis produced a *hostile filter*: most dims disappear unless they matter repeatedly.\n\n# Step 2 â€” Persistence analysis across runs\n\nInstead of asking â€œwhat lights up,â€ I counted:\n\n>\n\nThe result was a sharp hierarchy, not a cloud.\n\nTop hits (example):\n\n* **DIM 1731 â€” \\~14k hits**\n* **DIM 221 â€” \\~10k hits**\n* then a steep drop-off into the long tail\n\nThis strongly suggests a **small structural core** \\+ many conditional â€œguestâ€ dims.\n\n# Step 3 â€” Causal test (this is the key part)\n\nI then built a small UI to **intervene on individual hidden dimensions** during generation:\n\n* choose layer\n* choose dim\n* apply epsilon bias (not hard zero)\n* apply to attention output + MLP output\n\nWhen I biased **DIM 1731** (layer \\~20) with Îµ â‰ˆ +3:\n\n* grammar stayed intact\n* tokens kept flowing\n* **semantic commitment collapsed**\n* reasoning failed completely\n* output devolved into repetitive, affect-heavy, indecisive text\n\nThis was *not* random noise or total model failure.  \nIt looks like the model can still â€œtalkâ€ but **cannot commit to a trajectory**.\n\nThat failure mode was consistent with what the persistence analysis predicted.\n\n# Interpretation (carefully stated)\n\nDIM 1731 does *not* appear to be:\n\n* a topic neuron\n* a style feature\n* a lexical unit\n\nIt behaves like part of a **decision-stability / constraint / routing spine**:\n\n* present whenever the hero dim is doing real work\n* polarity-stable\n* survives across prompt classes\n* causally load-bearing when perturbed\n\nIâ€™m calling it â€œThe Kingâ€ internally because removing or overdriving it destabilizes everything downstream â€” but thatâ€™s just a nickname, not a claim.\n\n# Why I think this matters\n\n* This is a concrete example of **persistent, high-centrality hidden dimensions**\n* It suggests a path toward:\n   * targeted pruning\n   * hallucination detection (hero activation without core engagement looks suspect)\n   * mechanistic comparison across models\n* It bridges visualization â†’ aggregation â†’ **causal confirmation**\n\nIâ€™m not claiming universality or that this generalizes yet.  \nNext steps are sign-flip tests, ablations on the next-ranked dim (â€œthe Queenâ€), and cross-model replication.\n\nHappy to hear critiques, alternative explanations, or suggestions for better controls.\n\n*(Screenshots attached below â€” constellation persistence, hit distribution, and causal intervention output.)*\n\nDIM 1731: 13,952 hits (The King)\n\nDIM 221: 10,841 hits (The Queen)\n\nDIM 769: 4,941 hits\n\nDIM 1935: 2,300 hits\n\nDIM 2015: 2,071 hits\n\nDIM 1659: 1,900 hits\n\nDIM 571: 1,542 hits\n\nDIM 1043: 1,536 hits\n\nDIM 1283: 1,388 hits\n\nDIM 642: 1,280 hits\n\nhttps://preview.redd.it/qzd0agu1krag1.png?width=1542&format=png&auto=webp&s=1f4ca0fc58909bbd0c51ef28643b0c082c633e56\n\n",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1q17yu4/llame_32_3b_fmri_load_bearing_dim_found/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": [
        {
          "id": "nxs3scp",
          "author": "angie_akhila",
          "text": "neat",
          "score": 1,
          "created_utc": "2026-01-05 07:27:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q39g8n",
      "title": "Constraint Accumulation & the Emergence of a Plateau",
      "subreddit": "machinelearningnews",
      "url": "https://i.redd.it/yl3trianw7bg1.jpeg",
      "author": "Harryinkman",
      "created_utc": "2026-01-03 23:20:07",
      "score": 6,
      "num_comments": 2,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Agentic AI",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1q39g8n/constraint_accumulation_the_emergence_of_a_plateau/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxk8gob",
          "author": "gnv_gandu",
          "text": "Broken link",
          "score": 2,
          "created_utc": "2026-01-04 03:20:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxk8p6c",
              "author": "Harryinkman",
              "text": "https://doi.org/10.5281/zenodo.18141539 thanks for letting me know",
              "score": 1,
              "created_utc": "2026-01-04 03:22:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pz01z9",
      "title": "Roast my Career Strategy: 0-Exp CS Grad pivoting to \"Agentic AI\" (4-Month Sprint)",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1pz01z9/roast_my_career_strategy_0exp_cs_grad_pivoting_to/",
      "author": "Substantial_Sky_8167",
      "created_utc": "2025-12-29 22:19:41",
      "score": 5,
      "num_comments": 11,
      "upvote_ratio": 0.78,
      "text": "Roast my Career Strategy: 0-Exp CS Grad pivoting to \"Agentic AI\" (4-Month Sprint)\n\n\n\nI am a Computer Science senior graduating in May 2026. I have 0 formal internships, so I know I cannot compete with Senior Engineers for traditional Machine Learning roles (which usually require Masters/PhD + 5 years exp).\n\n\n\n\\> \\*\\*My Hypothesis:\\*\\*\n\n\\> The market has shifted to \"Agentic AI\" (Compound AI Systems). Since this field is <2 years old, I believe I can compete if I master the specific \"Agentic Stack\" (Orchestration, Tool Use, Planning) rather than trying to be a Model Trainer.\n\n\n\nI have designed a 4-month \"Speed Run\" using O'Reilly resources. I would love feedback on if this stack/portfolio looks hireable.\n\n\n\n\\## 1. The Stack (O'Reilly Learning Path)\n\n\\* \\*\\*Design:\\*\\* \\*AI Engineering\\* (Chip Huyen) - For Eval/Latency patterns.\n\n\\* \\*\\*Logic:\\*\\* \\*Building GenAI Agents\\* (Tom Taulli) - For LangGraph/CrewAI.\n\n\\* \\*\\*Data:\\*\\* \\*LLM Engineer's Handbook\\* (Paul Iusztin) - For RAG/Vector DBs.\n\n\\* \\*\\*Ship:\\*\\* \\*GenAI Services with FastAPI\\* (Alireza Parandeh) - For Docker/Deployment.\n\n\n\n\\## 2. The Portfolio (3 Projects)\n\nI am building these linearly to prove specific skills:\n\n\n\n1.  \\*\\*Technical Doc RAG Engine\\*\\*\n\n\\* \\*Concept:\\* Ingesting messy PDFs + Hybrid Search (Qdrant).\n\n\\* \\*Goal:\\* Prove Data Engineering & Vector Math skills.\n\n\n\n2.  \\*\\*Autonomous Multi-Agent Auditor\\*\\*\n\n\\* \\*Concept:\\* A Vision Agent (OCR) + Compliance Agent (Logic) to audit receipts.\n\n\\* \\*Goal:\\* Prove Reasoning & Orchestration skills (LangGraph).\n\n\n\n3.  \\*\\*Secure AI Gateway Proxy\\*\\*\n\n\\* \\*Concept:\\* A middleware proxy to filter PII and log costs before hitting LLMs.\n\n\\* \\*Goal:\\* Prove Backend Engineering & Security mindset.\n\n\n\n\\## 3. My Questions for You\n\n1.  Does this \"Portfolio Progression\" logically demonstrate a Senior-level skill set despite having 0 years of tenure?\n\n2.  Is the 'Secure Gateway' project impressive enough to prove backend engineering skills?\n\n3.  Are there mandatory tools (e.g., Kubernetes, Terraform) missing that would cause an instant rejection for an \"AI Engineer\" role?\n\n\n\n\\*\\*Be critical. I am a CS student soon to be a graduateï¿½do not hold back on the current plan.\\*\\*\n\n\n\nAny feedback is appreciated!",
      "is_original_content": false,
      "link_flair_text": "Agentic AI",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1pz01z9/roast_my_career_strategy_0exp_cs_grad_pivoting_to/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": [
        {
          "id": "nwswxa2",
          "author": "SeattleChrisCode",
          "text": "I'm watching for responses.",
          "score": 3,
          "created_utc": "2025-12-30 21:29:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtsauk",
          "author": "Exotic-Mongoose2466",
          "text": "From what I understand, you plan to learn the basics of machine learning (AI engineering is based on ML + ML-OPS) in less than 4 months?\nIt takes 2 years just for the basics of ML, and ML-OPS is senior-level.\n\nYou want to level the playing field with people who are either experienced developers, experienced data scientists, or ML engineers in just 4 months?\n4 months to learn what you want won't be enough.\n\nAnd 4 months to complete 3 projects won't demonstrate senior-level or even intermediate-level work at all.\nIf you're spending 4 months on 3 projects, it means you haven't planned any proper structure (industrialization, automation, infrastructure, etc.) for them.\n\nThey'll just be junior side projects.\n\nFocus on aiming for jobs that match your experience and skill level (junior) rather than aiming for intermediate or senior positions, and you'll be fine.\n\nPS: Your basic calculation is incorrect and shows that you're definitely a junior.\n\nThe tools are less than two years old, but the underlying logic is much older.\n\nThe tools can be learned very quickly when you understand the logic behind them.",
          "score": 3,
          "created_utc": "2025-12-31 00:10:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8umrr",
              "author": "That_Ability_7126",
              "text": "It really depends on the person but 4 months is very aggressive but in my opinion it really depends on his foundation and â€œsmartsâ€",
              "score": 1,
              "created_utc": "2026-01-02 12:50:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxa84fu",
                  "author": "Jakamo77",
                  "text": "No the smartest people in the world are spending years in this field. Ur not gonna be competing with 4 months even if ur smarter than ur competition. Iq wont bridge the learning gap from 10k-20k hours in a specific subject.",
                  "score": 1,
                  "created_utc": "2026-01-02 17:14:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvm6g8",
          "author": "Stunning_Habit_6411",
          "text": "I think youâ€™re misunderstanding what seniority means. I am an engineer who interviews and hires developers for years, all levels. When Iâ€™m hiring for a senior role im looking for basics that you canâ€™t learn without being a part of an organization or a team. Things like working with product managers or designers. How to manage tech debt. Did you ever have to dive into a giant 8 year old code base and do some infra work?\n\nThe language and tool usage are important but itâ€™s not really the most important thing youâ€™ll bring, especially if youâ€™re a smart person and more importantly in a world of coding agents.\n\nFinish your project, I think itâ€™s great and would definitely put you ahead of other juniors. But it wonâ€™t make you look senior, Iâ€™m sorry.\n\nKeep this attitude of learning and building your skills. If you do youâ€™ll have a great career. \n\nGood luck with the job search. Itâ€™s not a great time to be a junior dev I know, but itâ€™s possible, the opportunities are out there and it sounds like youâ€™re ready to work hard for it so i will allow myself to predict youâ€™ll be a great engineer very soon.\n\nEdits: I tried to be clearer ðŸ˜…",
          "score": 2,
          "created_utc": "2025-12-31 07:17:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4jx12",
          "author": "TomatoInternational4",
          "text": "The idea that you need to show what you can do is sound. The things chatgpt chose for you to do make no sense though. They are generic, low effort, hype word projects that don't actually mean anything.\n\n If you really want to get a good job then I would first try to show consistent and enthusiastic work in the field. This means your portfolio or GitHub should be active and show a history of things you've been working on. The asterisk here is that you should work on things you truly find interesting. None of this generic RAG chatbot crap. If you make sure you're creating things you enjoy making it will naturally show through your work. You'll be able to talk about it without effort and in great detail. People will see you're happy doing it, it excites you, and there is a drive within you to do it. \n\nThe degree isn't meaningless either. It just comes second to showing people what you can do. Your portfolio is your chance to shine so you don't want to half ass it with AI. Take your time on it and make it pretty. \n\nYour job right now is to compel the client or employer to hire you.",
          "score": 2,
          "created_utc": "2026-01-01 19:27:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1zr89",
          "author": "notAllBits",
          "text": "I agree that LLMs can accelerate pointed learning, but the internalized knowledge from personal experience is not sped up by it without considerable engagement. Your coverage is very thin and does not reflect senior level ambition or knowledge. Agentic orchestration does not operate as an alternative to the old way of doing things, it is at best an extension with deep stack dependencies and unprecedented regulatory overhead. Having said that, here some tips:\n\nThis field is currently under very high pressure to innovate, both from LLM capability directly (learning, coding) and their integration into higher level services (value generation).\n\nYou need a stack to orchestrate. A future-stable stack, which does not amount to much these days.\n\nUtilize hyper-scalers. Get accustomed with deploying with IaC (fx. terraform modules) to Amazon or Google or Azure. Their inertia will provide the deepest grooves against dependency rot and tech debt in these accelerated technologies. \n\nThink about how to build genAI systems compliant with internal idiosyncrasies and heritage and external regulation in repetitive planning exercises. Pick old technologies and assume flawed system architecture and blocked upgrade paths due to abandoned products and a history of abandoned modernization projects. Develop a sense of smell for those to steer clear of them. \n\nThink about how you would implement compliant orchestration systems using off the shelf tech and services. There will always be both internal and external governance and constraints. LLMs are mediocre at identifying these and fail at comprehensive work on them, because of scaling complexity, diversity, and privileged use case access. \n\nFigure out a way to utilize systems design to operationalize complexity. Pick a field, catalogue its constraints, and design solutions for it as a strategic role play, where you explore possible synergies and solutions for each and all. LLMs can help here if you are disciplined with it. Beware, they will like to convince you very early of poor solutions being sufficient. LLMOps is deeply integrated with local and cloud architecture (deep grooves).\n\nPlay with synthetic data and budget calculations to identify gaps, bottlenecks and service cost scaling.\n\nManual devOps steps such as \"deploying with docker\" do not represent backend expertise; they look either naive, or ancient. This step should be embedded into automated pipelines. Deployment is gated on several layers for secure, compliant, and reliable releases, where the actions on the targeted machines should be completely automatic. At the very least also list CI/CD, development stages with dedicated isolated landing zones, and managed services. Similarly gateways are hyper-scrutinized attack vector nodes. Mentioning them will trigger immune-response follow ups like: where in our cloud is it deployed? And who can connect to what when?     \n\nLast, but certainly not least: orchestration is not free of legacy concerns, it is extending all the old- alongside a set of new concerns into an under-integrated (unstable tech landscape) and under-regulated (will follow) service space. Cover your basics before diving into orchestration. In regulated businesses you may be responsible personally for designs you sign off on.\n\nedits: clarifications",
          "score": 1,
          "created_utc": "2026-01-01 08:56:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8hq2p",
          "author": "DmitryPavol",
          "text": "What will you do when, in a couple of years, there is a breakthrough in quantum computers and there is a shortage of quantum programmers, and you have already spent time and energy on AI?",
          "score": 1,
          "created_utc": "2026-01-02 11:04:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa0p8t",
          "author": "Muted_Ad6114",
          "text": "Maybe donâ€™t trust ChatGPT with your career plan",
          "score": 1,
          "created_utc": "2026-01-02 16:39:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxapy6s",
          "author": "AsukaMLEnjoyer",
          "text": "Have you considered Georgia Tech's [OMSCS](https://pe.gatech.edu/degrees/computer-science) program where they offer a MSCS degree for under $10k online? \n\nThat program has a Machine Learning specialization. This education alone won't land a new job but it will make your resume more marketable when combined with projects, certifications, and any relevant job experiences.\n\nHunt for an internship or software role and further specialize in your downtime. Employers want to see proven experience.",
          "score": 1,
          "created_utc": "2026-01-02 18:36:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxbjzq2",
          "author": "peterxsyd",
          "text": "Basically you will get arse fucked by AI, and look like a complete tool in the process. People who are hyper productive with AI are the ones who know when to say no to it, and, my experience has been, on non-trivial projects requiring it, is roughly 30-50% of the time, with the rest spent steering it and providing context for it to perform really well.\n\nInvest that time learning and building things either yourself or for an average company first so you can get the fundamentals otherwise you are lost poor child.",
          "score": 1,
          "created_utc": "2026-01-02 21:00:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2qz5z",
      "title": "ISON: 70% fewer tokens than JSON. Built for LLM context stuffing.",
      "subreddit": "machinelearningnews",
      "url": "/r/AIMemory/comments/1q19dya/ison_70_fewer_tokens_than_json_built_for_llm/",
      "author": "Immediate-Cake6519",
      "created_utc": "2026-01-03 10:03:58",
      "score": 5,
      "num_comments": 5,
      "upvote_ratio": 0.67,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "AI Tools",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1q2qz5z/ison_70_fewer_tokens_than_json_built_for_llm/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "nxkd1op",
          "author": "Budget-Juggernaut-68",
          "text": "And have you tested the performance?\n\nedit:\n\n[https://ison.dev/benchmark.html](https://ison.dev/benchmark.html)\n\noh. nice.",
          "score": 2,
          "created_utc": "2026-01-04 03:47:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxklcow",
              "author": "Immediate-Cake6519",
              "text": "You can check in the repo with full benchmark results",
              "score": 1,
              "created_utc": "2026-01-04 04:39:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxh3zh6",
          "author": "TomHale",
          "text": "Is there a converter from JSON for pipeline usage without needing to recompile tools?\n\nAlso, does it convert losslessly in both directions?",
          "score": 1,
          "created_utc": "2026-01-03 17:50:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxkl73o",
              "author": "Immediate-Cake6519",
              "text": "Sorry replied in the main chat instead of direct reply\n\nYes to both.\n\nConverter:\n\nâ€¢\tâ ison-py: ison.from_json(data) / ison.to_json(data)\nâ€¢\tâ ison-ts: ISON.fromJSON() / ISON.toJSON()\nâ€¢\tâ CLI: ison convert input.json output.ison\n\nNo recompilation needed â€” it's a runtime conversion. Drop it into your pipeline wherever you're serializing context for the LLM.\n\nLossless: Yes, bidirectional and lossless. JSON â†’ ISON â†’ JSON roundtrips preserve all data. The token savings come from syntax reduction, not data loss:\n\nJSON {\"users\": [{\"id\": 1, \"name\": \"Alice\"}]}\n\nISON table.users id name 1 Alice\n\nSame data, different encoding. Types are preserved (strings, numbers, booleans, nulls, nested objects, arrays).\n\nGitHub has examples: github.com/maheshvaikri-code/ison",
              "score": 2,
              "created_utc": "2026-01-04 04:38:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxiatma",
          "author": "Immediate-Cake6519",
          "text": "Yes to both.\n\nConverter:\n- `ison-py`: `ison.from_json(data)` / `ison.to_json(data)`\n- `ison-ts`: `ISON.fromJSON()` / `ISON.toJSON()`\n- CLI: `ison convert input.json output.ison`\n\nNo recompilation needed â€” it's a runtime conversion. Drop it into \nyour pipeline wherever you're serializing context for the LLM.\n\nLossless:\nYes, bidirectional and lossless. JSON â†’ ISON â†’ JSON roundtrips \npreserve all data. The token savings come from syntax reduction, \nnot data loss:\n\nJSON\n{\"users\": [{\"id\": 1, \"name\": \"Alice\"}]}\n\nISON\ntable.users\nid name\n1 Alice\n\n\nSame data, different encoding. Types are preserved (strings,\nnumbers, booleans, nulls, nested objects, arrays).\n\nGitHub has examples: github.com/maheshvaikri-code/ison",
          "score": 1,
          "created_utc": "2026-01-03 21:13:02",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2ntdf",
      "title": "Autonomous Dodging of Stochastic-Adversarial Traffic Without a Safety Driver",
      "subreddit": "machinelearningnews",
      "url": "https://youtu.be/JGN-HXj1K3w",
      "author": "shani_786",
      "created_utc": "2026-01-03 06:53:54",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Startup News",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1q2ntdf/autonomous_dodging_of_stochasticadversarial/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q5nv4s",
      "title": "Circuit Tracing Methodology",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1q5nv4s/circuit_tracing_methodology/",
      "author": "[deleted]",
      "created_utc": "2026-01-06 16:50:06",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "T-Scan Methodology Summary\n\nOverview\n\nT-scan is a mechanistic interpretability technique for mapping load-bearing infrastructure in transformer models by using individual dimensions as \"heroes\" to reveal network topology through co-activation analysis.\n\nCore Methodology\n\n1. Hero Dimension Selection\n\n\n\nSelected 73 dimensions from Llama 3.2 3B (3072-dimensional residual stream)\n\nHeroes chosen based on preliminary screening for high co-activation counts\n\nEach hero acts as a \"perspective\" for viewing the network\n\n\n\n2. Window-Based Correlation Analysis\n\n\n\nRolling 15-token window during generation\n\nCompute three metrics per dimension pair:\n\n\n\nPearson correlation: Centered, normalized sync (temporal co-activation)\n\nCosine similarity: Raw directional alignment\n\nEnergy: Scaled dot product (interaction strength)\n\n\n\n\n\n\n\n3. Phase Lock Detection\n\n\n\nTrack whether target dimension's sign matches expected polarity\n\nExpected sign = sign(hero) Ã— sign(correlation)\n\nlock\\_ratio = proportion of observations where polarity is correct\n\nMeasures relationship stability/reliability\n\n\n\n4. Multi-Prompt Aggregation\n\n\n\nRun each hero across 88 diverse prompts\n\nAggregate statistics per dimension pair:\n\n\n\nTotal co-activation count (weight)\n\nNet polarity (positive - negative observations)\n\nAverage energy\n\nPhase lock consistency\n\nHero visibility (which heroes see each connection)\n\n\n\n\n\n\n\n5. Consensus Analysis (Overlay)\n\n\n\nCompare all 73 hero perspectives\n\nCalculate consensus metrics:\n\n\n\nNode consensus: Which dimensions are universally visible\n\nEdge consensus: Which connections appear across multiple heroes\n\n\n\n\n\nDiscovered: Universal nodes, hero-specific edges\n\n\n\nKey Findings\n\nNetwork Structure:\n\n\n\n3072 nodes with near-universal visibility (all heroes agree on WHICH dimensions matter)\n\n161,385 edges with hero-specific visibility (different heroes reveal different connection patterns)\n\n0 edges visible to >50% of heroes (connections are perspective-dependent)\n\n\n\nInfrastructure Tiers:\n\n\n\n8 universal nodes visible to all 53 heroes (network skeleton)\n\nCritical dimensions (221, 1731, 3039) show highest infrastructure scores\n\nInfrastructure score = geometric mean of hero performance Ã— network mass\n\n\n\nMethodological Innovation:\n\n\n\nTraditional interp: analyze model from outside\n\nT-scan: use model's own dimensions to reveal internal structure\n\nEach hero dimension acts as a \"sensor\" revealing different network facets\n\n\n\nData Products\n\n\n\nIndividual hero constellation maps (73 files)\n\nAggregated network topology (constellation\\_final.json)\n\nConsensus overlay analysis (identifies universal vs. hero-specific structure)\n\nVoltron analysis (merges hero performance with network topology)",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1q5nv4s/circuit_tracing_methodology/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": []
    }
  ]
}