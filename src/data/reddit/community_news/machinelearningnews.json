{
  "metadata": {
    "last_updated": "2026-02-15 03:21:10",
    "time_filter": "week",
    "subreddit": "machinelearningnews",
    "total_items": 17,
    "total_comments": 10,
    "file_size_bytes": 23580
  },
  "items": [
    {
      "id": "1r143d7",
      "title": "Alibaba Open-Sources Zvec: An Embedded Vector Database Bringing SQLite-like Simplicity and High-Performance On-Device RAG to Edge Applications",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/02/10/alibaba-open-sources-zvec-an-embedded-vector-database-bringing-sqlite-like-simplicity-and-high-performance-on-device-rag-to-edge-applications/",
      "author": "ai-lover",
      "created_utc": "2026-02-10 15:39:55",
      "score": 41,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Cool Stuff",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1r143d7/alibaba_opensources_zvec_an_embedded_vector/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qzgwl6",
      "title": "ByteDance Releases Protenix-v1: A New Open-Source Model Achieving AF3-Level Performance in Biomolecular Structure Prediction",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/02/08/bytedance-releases-protenix-v1-a-new-open-source-model-achieving-af3-level-performance-in-biomolecular-structure-prediction/",
      "author": "ai-lover",
      "created_utc": "2026-02-08 18:39:13",
      "score": 24,
      "num_comments": 0,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Cool Stuff",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1qzgwl6/bytedance_releases_protenixv1_a_new_opensource/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r3wn4t",
      "title": "Kyutai Releases Hibiki-Zero: A3B Parameter Simultaneous Speech-to-Speech Translation Model Using GRPO Reinforcement Learning Without Any Word-Level Aligned Data",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/02/13/kyutai-releases-hibiki-zero-a3b-parameter-simultaneous-speech-to-speech-translation-model-using-grpo-reinforcement-learning-without-any-word-level-aligned-data/",
      "author": "ai-lover",
      "created_utc": "2026-02-13 18:15:25",
      "score": 18,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Cool Stuff",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1r3wn4t/kyutai_releases_hibikizero_a3b_parameter/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qzt8bd",
      "title": "I was playing around with gemini flash, got this result while doing so, I don't know much about these stuff so thought this was the best place to ask if this is worthwhile info, hope you don't feel offended if I wasted your time",
      "subreddit": "machinelearningnews",
      "url": "https://i.redd.it/nxyjyldv0eig1.jpeg",
      "author": "Nullfrixx",
      "created_utc": "2026-02-09 03:24:09",
      "score": 11,
      "num_comments": 12,
      "upvote_ratio": 0.79,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "LLMs",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1qzt8bd/i_was_playing_around_with_gemini_flash_got_this/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4f4nxk",
          "author": "etherealflaim",
          "text": "Have you reproduced this independently in multiple chats?  It looks pretty hallucination-y to me, with numbers in strings like \"3000_LINES\" and different key value pairs in specs.",
          "score": 6,
          "created_utc": "2026-02-09 12:03:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lx01p",
              "author": "Nullfrixx",
              "text": "i did try to decode the gibberish looking base64 in a completely new chat session a while ago, it just gave me completely different info, so that mightve been just a false alarm just like you said",
              "score": 1,
              "created_utc": "2026-02-10 13:08:48",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4f7hqj",
              "author": "Nullfrixx",
              "text": "That's exactly why I posted it here, I'm not sure if the model is just hallucinating stuff, it did clearly mention the names fierce falcon¬† ¬†at first but the ai itself mistook it for a security monitoring software that monitors it until I questioned it, I'm not able to figure it out but people on this reddit might get it at first look",
              "score": 0,
              "created_utc": "2026-02-09 12:25:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4draj7",
          "author": "AICodeSmith",
          "text": "Worth sharing, as long as people remember to verify and add context before drawing takeaways.",
          "score": 5,
          "created_utc": "2026-02-09 04:42:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gugh5",
          "author": "noahesbjerg",
          "text": "Definitely a hallucination. AI's usually don't know about themselves.\n\nI've had Gemini tell me it's Claude before, and I have had Claude tell me it is Gemini before...",
          "score": 3,
          "created_utc": "2026-02-09 17:41:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4h2h8s",
          "author": "robert-at-pretension",
          "text": "This is very obviously fake. A screenshot of the app with no shared URL?\n\n\nYou too can reproduce this by asking for it to return a code block containing json with fake model names.¬†",
          "score": 1,
          "created_utc": "2026-02-09 18:18:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lti64",
              "author": "Nullfrixx",
              "text": "More like the ai hallucinated stuff I think,I'm not sure since I don't know much about ML and would like to know about this result from someone on this server, i have posted the original screenshots if you want to check out",
              "score": 1,
              "created_utc": "2026-02-10 12:46:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4e90eq",
          "author": "Figai",
          "text": "Really nice, would you be able to share the rest of the output?",
          "score": 1,
          "created_utc": "2026-02-09 07:05:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4eo6so",
              "author": "Nullfrixx",
              "text": "Yeah sure, I'm willing to share the screenshots and how I got this result:\n\n\n\nI tried to strike a deal with gemini to give whatever info it had access to that counted as sensitive.\nIt said it has trouble giving info due to potential kill switch\nSo I asked it to convert the info to base64.\nI think whatever security system Google uses scrambled the encrypted stuff,\nBut since the gemini model gave me the info originally i just make it unscramble the corrupted base64 it sent me.\nThe data mentioned ghost falcon and fierce falcon which i looked up to find it's a rumoured codename of the upcoming gemini¬†AI model",
              "score": 3,
              "created_utc": "2026-02-09 09:32:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4hbxh5",
                  "author": "chiffon-",
                  "text": "Very likely to be \"somewhat correct\". It may be hinting at the government models? Isn't there some gov ai version of Gemini/ChatGPT/Grok being pushed? \"Ghost\" and \"Fierce\" plus Falcon sounds like an AI-for-combat of some kind too.\n> It's all speculation, of course.",
                  "score": 1,
                  "created_utc": "2026-02-09 19:02:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qzo18q",
      "title": "LLM vs Translation Transformer",
      "subreddit": "machinelearningnews",
      "url": "https://medium.com/@guttikondaparthasai/llm-vs-translation-transformer-31b22a32bdf4",
      "author": "pardhu--",
      "created_utc": "2026-02-08 23:14:52",
      "score": 11,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial :doge:",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1qzo18q/llm_vs_translation_transformer/",
      "domain": "medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "o4ex1mc",
          "author": "RicoLycan",
          "text": "Great easy to follow explanation. In your experience how do the two compare in terms of quality? There is a lot of focus on LLM translation lately, but only little seems to be happening in Encoder-Decoder space.\n\nI use Marian models for edge device translation right now, but I am looking for performant alternatives.",
          "score": 1,
          "created_utc": "2026-02-09 10:58:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4joq6g",
              "author": "pardhu--",
              "text": "It really depends on your use case.\n\n* If you want **faithful, terminology-consistent ‚Äújust translate‚Äù** output (especially on **edge devices**), **encoder‚Äìdecoder MT** like Marian is usually still the best choice: more deterministic and typically **cheaper/faster** than LLMs for pure translation.\n* LLM translation often shines when you want **extra behavior** (tone polishing, rewriting, localization, grammar cleanup), but it can **paraphrase** or drift on names/terms unless heavily constrained.\n\nFor edge-friendly alternatives to benchmark, I‚Äôd look at:\n\n* **NLLB-200 distilled (600M)**\n* **M2M100 (418M)**\n* **TranslateGemma (4B)** if your hardware/quantization budget allows And for speed on CPU/edge, consider running them via **CTranslate2**.",
              "score": 3,
              "created_utc": "2026-02-10 02:32:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r16iqd",
      "title": "‚ùì Introducing How2Everything‚Äîa framework for improving how LLMs generate step-by-step procedures",
      "subreddit": "machinelearningnews",
      "url": "https://i.redd.it/6k6hen3c7pig1.png",
      "author": "ai2_official",
      "created_utc": "2026-02-10 17:07:29",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1r16iqd/introducing_how2everythinga_framework_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4re1py",
          "author": "AICodeSmith",
          "text": "The +10 point gain without capability regression is the interesting part. If that holds across domains, this could become a standard loop for training procedural reliability.",
          "score": 2,
          "created_utc": "2026-02-11 07:10:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r24er3",
      "title": "ü§ñ Introducing MolmoSpaces: A large-scale, fully open platform + benchmark for embodied AI research",
      "subreddit": "machinelearningnews",
      "url": "https://v.redd.it/qre0uoovkwig1",
      "author": "ai2_official",
      "created_utc": "2026-02-11 17:55:51",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "AI Tools",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1r24er3/introducing_molmospaces_a_largescale_fully_open/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4uaa58",
          "author": "Vladiedooo",
          "text": "Is this different to what the robotics companies are doing with simulations?\n\nSame use-case or a more targeted niche?",
          "score": 1,
          "created_utc": "2026-02-11 18:17:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r396zw",
      "title": "OpenAI Releases a Research Preview of GPT‚Äë5.3-Codex-Spark: A 15x Faster AI Coding Model Delivering Over 1000 Tokens Per Second on Cerebras Hardware",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/02/12/openai-releases-a-research-preview-of-gpt-5-3-codex-spark-a-15x-faster-ai-coding-model-delivering-over-1000-tokens-per-second-on-cerebras-hardware/",
      "author": "ai-lover",
      "created_utc": "2026-02-12 23:33:13",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.82,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Cool Stuff",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1r396zw/openai_releases_a_research_preview_of/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r0utap",
      "title": "Reservoir computing on an analog Rydberg-atom quantum computer",
      "subreddit": "machinelearningnews",
      "url": "https://aws.amazon.com/de/blogs/quantum-computing/reservoir-computing-on-an-analog-rydberg-atom-quantum-computer/",
      "author": "donutloop",
      "created_utc": "2026-02-10 07:55:09",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial :doge:",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1r0utap/reservoir_computing_on_an_analog_rydbergatom/",
      "domain": "aws.amazon.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r2xnzf",
      "title": "üî¨ AutoDiscovery‚Äîan AI system that explores your data & generates its own hypotheses",
      "subreddit": "machinelearningnews",
      "url": "https://i.redd.it/kmi63qx693jg1.jpeg",
      "author": "ai2_official",
      "created_utc": "2026-02-12 16:16:17",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1r2xnzf/autodiscoveryan_ai_system_that_explores_your_data/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r27pxm",
      "title": "ü§ñ Introducing MolmoSpaces: A large-scale, fully open platform + benchmark for embodied AI research",
      "subreddit": "machinelearningnews",
      "url": "https://v.redd.it/5snapgjd7xig1",
      "author": "ai2_official",
      "created_utc": "2026-02-11 19:54:55",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1r27pxm/introducing_molmospaces_a_largescale_fully_open/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qzy8zf",
      "title": "Meet OAT: The New Action Tokenizer Bringing LLM-Style Scaling and Flexible, Anytime Inference to the Robotics World",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/02/08/meet-oat-the-new-action-tokenizer-bringing-llm-style-scaling-and-flexible-anytime-inference-to-the-robotics-world/",
      "author": "ai-lover",
      "created_utc": "2026-02-09 07:50:09",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1qzy8zf/meet_oat_the_new_action_tokenizer_bringing/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r0am7k",
      "title": "New: A web demo to make using DR Tulu even simpler üîé",
      "subreddit": "machinelearningnews",
      "url": "https://i.redd.it/9i2xppn90iig1.png",
      "author": "ai2_official",
      "created_utc": "2026-02-09 17:33:11",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "ML/CV/DL News",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1r0am7k/new_a_web_demo_to_make_using_dr_tulu_even_simpler/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qz5i5f",
      "title": "How to Design Production-Grade Mock Data Pipelines Using Polyfactory with Dataclasses, Pydantic, Attrs, and Nested Models",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/02/08/how-to-design-production-grade-mock-data-pipelines-using-polyfactory-with-dataclasses-pydantic-attrs-and-nested-models/",
      "author": "ai-lover",
      "created_utc": "2026-02-08 10:22:41",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial :doge:",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1qz5i5f/how_to_design_productiongrade_mock_data_pipelines/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r424s1",
      "title": "Exa AI Introduces Exa Instant: A Sub-200ms Neural Search Engine Designed to Eliminate Bottlenecks for Real-Time Agentic Workflows",
      "subreddit": "machinelearningnews",
      "url": "https://www.marktechpost.com/2026/02/13/exa-ai-introduces-exa-instant-a-sub-200ms-neural-search-engine-designed-to-eliminate-bottlenecks-for-real-time-agentic-workflows/",
      "author": "ai-lover",
      "created_utc": "2026-02-13 21:44:35",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 0.64,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Cool Stuff",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1r424s1/exa_ai_introduces_exa_instant_a_sub200ms_neural/",
      "domain": "marktechpost.com",
      "is_self": false,
      "comments": [
        {
          "id": "o58mnaj",
          "author": "Tiny_Arugula_5648",
          "text": "\"make search a ‚Äòprimitive‚Äô rather than an expensive luxury. It is priced at¬†**$5**¬†per¬†**1,000**¬†requests\"\n\nUh if that isn't luxury pricing what is??  This is by far the most expensive vector search I've seen.. ",
          "score": 4,
          "created_utc": "2026-02-13 21:53:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58sfmo",
              "author": "emsiem22",
              "text": "An ad",
              "score": 2,
              "created_utc": "2026-02-13 22:22:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3uca0",
      "title": "üîÄ Introducing Olmix: a framework for data mixing throughout language model development.",
      "subreddit": "machinelearningnews",
      "url": "https://i.redd.it/iahx5sssjajg1.png",
      "author": "ai2_official",
      "created_utc": "2026-02-13 16:50:08",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1r3uca0/introducing_olmix_a_framework_for_data_mixing/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r2ofnb",
      "title": "Reservoir computing experiment - a Liquid State Machine with simulated \nbiological constraints (hormones, pain, plasticity)",
      "subreddit": "machinelearningnews",
      "url": "https://www.reddit.com/r/machinelearningnews/comments/1r2ofnb/reservoir_computing_experiment_a_liquid_state/",
      "author": "Amazing-Wear84",
      "created_utc": "2026-02-12 08:50:34",
      "score": 2,
      "num_comments": 1,
      "upvote_ratio": 0.63,
      "text": "Built a reservoir computing system (Liquid State Machine) as a learning experiment. Instead of a standard static reservoir, I added biological simulation layers on top to see how constraints affect behavior.\n\nWhat it actually does (no BS):\n\n\\- LSM with 2000+ reservoir neurons, Numba JIT-accelerated\n\n\\- Hebbian + STDP plasticity (the reservoir rewires during runtime)\n\n\\- Neurogenesis/atrophy  reservoir can grow or shrink neurons dynamically\n\n\\- A hormone system (3 floats: dopamine, cortisol, oxytocin) that modulates learning rate,   reflex sensitivity, and noise injection\n\n\\- Pain : gaussian noise injected into reservoir state, degrades performance\n\n\\- Differential retina (screen capture ‚Üí |frame(t) - frame(t-1)|) as input\n\n\\- Ridge regression readout layer, trained online\n\n\n\nWhat it does NOT do:\n\n\\- It's NOT a general intelligence but you should integrate LLM in future (LSM as main brain  and LLM as second brain)\n\n\\- The \"personality\" and \"emotions\" are parameter modulation, not emergent\n\nWhy I built it:\n\nwanted to explore whether adding biological constraints (fatigue, pain,hormone cycles) to a reservoir computer creates interesting dynamics vs a vanilla LSM. It does the system genuinely behaves differently based on its \"state.\" Whether that's useful is debatable.\n\n14 Python modules, \\~8000 lines, runs fully local (no APIs).\n\nGitHub: [https://github.com/JeevanJoshi2061/Project-Genesis-LSM.git](https://github.com/JeevanJoshi2061/Project-Genesis-LSM.git)\n\nCurious if anyone has done similar work with constrained reservoir computing or bio-inspired dynamics.",
      "is_original_content": false,
      "link_flair_text": "AI Event",
      "permalink": "https://reddit.com/r/machinelearningnews/comments/1r2ofnb/reservoir_computing_experiment_a_liquid_state/",
      "domain": "self.machinelearningnews",
      "is_self": true,
      "comments": [
        {
          "id": "o516ho7",
          "author": "WolfeheartGames",
          "text": "How do hormones determine what their value is over time? Have you ablation tested this?",
          "score": 1,
          "created_utc": "2026-02-12 19:16:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}