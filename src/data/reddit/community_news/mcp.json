{
  "metadata": {
    "last_updated": "2026-01-06 08:56:04",
    "time_filter": "week",
    "subreddit": "mcp",
    "total_items": 28,
    "total_comments": 77,
    "file_size_bytes": 117956
  },
  "items": [
    {
      "id": "1pynehb",
      "title": "MCP for a coffee machine.. Worked!",
      "subreddit": "mcp",
      "url": "https://i.redd.it/t50uyzldj5ag1.jpeg",
      "author": "Any-Way-2765",
      "created_utc": "2025-12-29 14:18:02",
      "score": 164,
      "num_comments": 19,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1pynehb/mcp_for_a_coffee_machine_worked/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwll7sj",
          "author": "livecodelife",
          "text": "As a fellow weird coffee person and engineer I love this. I also love seeing someone use AI tools for fun life things as a respite from the constant â€œmillion dollar ideasâ€ everywhere. Well done, I may give this a try!",
          "score": 15,
          "created_utc": "2025-12-29 19:39:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwm9ron",
              "author": "WantDollarsPlease",
              "text": "I love it as well. Especially since it's not slop",
              "score": 2,
              "created_utc": "2025-12-29 21:39:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwpt9ni",
              "author": "TheAtlasMonkey",
              "text": "You cannot get million dollar ideas if you are sleepy. you need coffee.\n\nAnd now you can use just ask : Claude brew me some coffee.",
              "score": 2,
              "created_utc": "2025-12-30 11:55:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwvxpbp",
              "author": "Electrical-Taro-4058",
              "text": "Fellow weird coffee engineer unite! Finally, AI being used for something that matters: nailing the perfect shot instead of drafting a 20-slide pitch deck for a \"disruptive\" coffee subscription SaaS bro scam. That $0 machine hack is the crema on top of this perfect project.",
              "score": 1,
              "created_utc": "2025-12-31 09:05:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwltsqj",
          "author": "Ok-Bedroom8901",
          "text": "Dude, you should submit this writeup for Make magazine. This is right up their alley",
          "score": 11,
          "created_utc": "2025-12-29 20:21:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnyav7",
          "author": "ParamedicAble225",
          "text": "If anyone is curious, itâ€™s basically a $1500 coffee machine that has smart features.\n\nItâ€™s a simple mcp server that connects and calls those features.Â ",
          "score": 7,
          "created_utc": "2025-12-30 03:04:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrewh5",
              "author": "Any-Way-2765",
              "text": "It's actually a $0 coffee machine ðŸ˜",
              "score": 3,
              "created_utc": "2025-12-30 17:15:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwvqwcw",
                  "author": "ParamedicAble225",
                  "text": "send one over",
                  "score": 1,
                  "created_utc": "2025-12-31 08:01:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpke08",
          "author": "0MEGALUL-",
          "text": "Iâ€™m not fully grasping what AI is adding to the proces, arenâ€™t they predefined rules? \n\nVery cool project!",
          "score": 2,
          "created_utc": "2025-12-30 10:37:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpn4or",
              "author": "Any-Way-2765",
              "text": "Just brings \"3rd opinion\" if you get stuck and don't know how to tune",
              "score": 1,
              "created_utc": "2025-12-30 11:02:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwpq5uj",
                  "author": "0MEGALUL-",
                  "text": "How does it generate the 3rd option?\n\nAs I understand now, it is based on predefined rules or am i missing something",
                  "score": 2,
                  "created_utc": "2025-12-30 11:29:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpkf8z",
          "author": "Kulitorum2",
          "text": "Implemented this in Decenza|DE1:\n\n  \n[https://github.com/Kulitorum/de1-qt/releases](https://github.com/Kulitorum/de1-qt/releases)",
          "score": 2,
          "created_utc": "2025-12-30 10:38:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnc0rz",
          "author": "TurmoilX",
          "text": "This is super cool, thanks for sharing.",
          "score": 1,
          "created_utc": "2025-12-30 01:02:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwplgx6",
          "author": "Psychological_Cry920",
          "text": "OMG",
          "score": 1,
          "created_utc": "2025-12-30 10:47:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvvb7h",
          "author": "Electrical-Taro-4058",
          "text": "Finally, AI being used for the actual important work: perfecting my morning espresso instead of pitching another \"disruptive SaaS bro\" idea. That $0 coffee machine flex just makes this even better. Chefâ€™s kiss.",
          "score": 1,
          "created_utc": "2025-12-31 08:43:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx02nln",
          "author": "keinsaas-navigator",
          "text": "Achestra + keinsaas Navigator looks like the ultimate open source Ai implementation solution for companies!",
          "score": 1,
          "created_utc": "2026-01-01 00:02:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6nw1j",
          "author": "TwoOk866",
          "text": "love it! I tuned my Rocket to my espresso beans struggling with a google sheets table of data to get the right extraction. Great job!",
          "score": 1,
          "created_utc": "2026-01-02 02:23:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo3avp",
          "author": "some1else42",
          "text": "I have read a lot today, and this was hands down my favorite thing I have read. Well done!",
          "score": 1,
          "created_utc": "2025-12-30 03:33:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2p343",
      "title": "Run Claude Code with ollama without losing any single feature offered by Anthropic backend",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1q2p343/run_claude_code_with_ollama_without_losing_any/",
      "author": "Dangerous-Dingo-5169",
      "created_utc": "2026-01-03 08:07:14",
      "score": 62,
      "num_comments": 16,
      "upvote_ratio": 0.95,
      "text": "Hey folks! Sharing an open-source project that might be useful:\n\nLynkr connects AI coding tools (like Claude Code) to multiple LLM providers with intelligent routing.  \nKey features:\n\n\\- Route between multiple providers: Databricks, Azure Ai Foundry, OpenRouter, Ollama,llama.cpp, OpenAi\n\n\\- Cost optimization through hierarchical routing, heavy prompt caching\n\n\\- Production-ready: circuit breakers, load shedding, monitoring\n\n\\- It supports all the features offered by claude code like sub agents, skills , mcp , plugins etc unlike other proxies which only supports basic tool callings and chat completions.\n\nGreat for:\n\n\\- Reducing API costs as it supports hierarchical routing where you can route requstes to smaller local models and later switch to cloud LLMs automatically.\n\n\\- Using enterprise infrastructure (Azure)\n\n\\-Â  Local LLM experimentation\n\n\\`\\`\\`bash\n\nnpm install -g lynkr\n\n\\`\\`\\`\n\nGitHub: [https://github.com/Fast-Editor/Lynkr](https://github.com/Fast-Editor/Lynkr) (Apache 2.0)\n\nWould love to get your feedback on this one. Please drop a star on the repo if you found it helpful",
      "is_original_content": false,
      "link_flair_text": "resource",
      "permalink": "https://reddit.com/r/mcp/comments/1q2p343/run_claude_code_with_ollama_without_losing_any/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "nxh5y9i",
          "author": "wtgserpant",
          "text": "any plan on adding bedrock as model provider?",
          "score": 2,
          "created_utc": "2026-01-03 17:59:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxiqueh",
              "author": "extra_specticles",
              "text": "Yes, please, that would be massive. /u/Dangerous-Dingo-5169",
              "score": 1,
              "created_utc": "2026-01-03 22:31:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxj0igd",
                  "author": "Dangerous-Dingo-5169",
                  "text": "Sure ðŸ‘ the next changes would be to support bedrock and LM studio",
                  "score": 2,
                  "created_utc": "2026-01-03 23:21:45",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhw906",
          "author": "Scary-Difference630",
          "text": "Why do these projects usually not support LM Studio?",
          "score": 1,
          "created_utc": "2026-01-03 20:00:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhwsar",
              "author": "Dangerous-Dingo-5169",
              "text": "Hi thanks for the feedback \nI have nothing against LM studio and will integrate it in the near future. \nI integrated ollama and llama.cpp for supporting local models \nFrankly speaking I integrated ollama after I got a feedback asking the same \nBecause ollama was damn slow on mac m1 with 16 gb",
              "score": 2,
              "created_utc": "2026-01-03 20:03:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxidhx6",
          "author": "Nshx-",
          "text": "but i still need a subscription tu enter claude code haha. mmm",
          "score": 1,
          "created_utc": "2026-01-03 21:26:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxj0p4k",
              "author": "Dangerous-Dingo-5169",
              "text": "No \nYou would just need to connect your claude account with 0 credits",
              "score": 1,
              "created_utc": "2026-01-03 23:22:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxj5kec",
                  "author": "Nshx-",
                  "text": "**Claude Code can be used with your Claude subscription or billed based on API usage through your Console account.**\n\nSelect login method:\n\nÂ â¯ 1. Claude account with subscription Â· Pro, Max, Team, or Enterprise\n\nÂ Â  2. Anthropic Console account Â· API usage billing",
                  "score": 1,
                  "created_utc": "2026-01-03 23:48:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxektxi",
          "author": "macromind",
          "text": "This is cool, especially the \"donâ€™t lose Claude Code features\" part, most proxies break on the weird edge cases (agents, plugins, MCP, etc). How are you deciding routing by default, is it rules-based (model A for tool calls, model B for long context) or do you score requests dynamically?\n\nAlso, do you have any benchmarks on latency overhead from the proxy layer?\n\nSide note, I have been collecting examples of agent routing patterns and guardrails lately, a few notes here if useful: https://blog.promarkia.com/",
          "score": 1,
          "created_utc": "2026-01-03 08:09:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxeldv3",
              "author": "Dangerous-Dingo-5169",
              "text": "Thank you u/macromind I will definitely check the blog  \nTo answer your question on routing  \nFor now  \nits kinda rule based if you configure all 3 levels models. I have divided the models into 3 categories\n\n1. Local LLM's\n2. OpenRouter\n3. All the other cloud LLMs\n4. The reason why I did this is because for a free user local llms are not so efficient in tool use as most machines can only host smaller models. OpenRouter as well gives strict limits for free tier but its good with moderate tool calls and context length.\n5. Although you can configure individuals ones as well. Like if you only want to use local llms you can do so\n6. I hope this answers your question. Open for feedback for any of this\n7. Please drop a star on the repo if you found it helpful\n8. Thanks",
              "score": 1,
              "created_utc": "2026-01-03 08:14:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxemzkq",
          "author": "Nshx-",
          "text": "im interested... where is the proyect?",
          "score": 0,
          "created_utc": "2026-01-03 08:28:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxen5o2",
              "author": "Dangerous-Dingo-5169",
              "text": "https://github.com/Fast-Editor/Lynkr",
              "score": 2,
              "created_utc": "2026-01-03 08:30:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0onvr",
      "title": "Built a MCP server that bridges WhatsApp and AI assistants. Messages stay local, Claude/AI assistant gets real context.",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1q0onvr/built_a_mcp_server_that_bridges_whatsapp_and_ai/",
      "author": "felipe-adeildo",
      "created_utc": "2025-12-31 22:05:35",
      "score": 35,
      "num_comments": 12,
      "upvote_ratio": 0.97,
      "text": "Built a MCP server that bridges WhatsApp and AI assistants. Messages stay local, Claude gets real context.\n\n**What it does??**\n\n* Cross-chat search: \"Find everything Arthur said about budget\" -> searches DMs, groups, everywhere\n* Context analysis: \"Summarize my Tech Team group this week\"\n* Send messages: \"Tell Maria I'll be 10 minutes late\" ->  finds chat, sends contextually\n* Reply with context: Claude reads conversation history before responding\n* Person lookup: See all messages from someone across all chats\n* On-demand history: Load older messages from WhatsApp servers when needed\n\n**Stack:**\n\n* Go + whatsmeow (WhatsApp Web reverse engineering)\n* SQLite with full-text search indexes\n* 6 tools, 4 prompts, 4 resource guides\n* Docker deployment\n* Streamable HTTP (not SSE, neither STDIO)\n\n**Why?**\n\nTired of copy-pasting messages manually. Now Claude searches my full WhatsApp history, understands relationships, and sends contextual replies.\n\n**Repo:** [https://github.com/felipeadeildo/whatsapp-mcp](https://github.com/felipeadeildo/whatsapp-mcp)\n\n[claude.ai using the whatsapp-mcp server](https://preview.redd.it/46o3udzi9mag1.png?width=1918&format=png&auto=webp&s=5c48fc66329d19c1245d3a14fa0b5bcf123b1ac9)\n\n**Roadmap:** Voice transcription, image OCR, GraphRAG integration.",
      "is_original_content": false,
      "link_flair_text": "server",
      "permalink": "https://reddit.com/r/mcp/comments/1q0onvr/built_a_mcp_server_that_bridges_whatsapp_and_ai/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "nx1xqxt",
          "author": "Revolutionary_Sir140",
          "text": "Use both mcp and utcp",
          "score": 3,
          "created_utc": "2026-01-01 08:35:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx377y0",
              "author": "felipe-adeildo",
              "text": "I could be a good feature request on the repository!\n\nI really don't know utcp (until now), i can't test for now. But i'll keep it in mind!\n\nThanks",
              "score": 1,
              "created_utc": "2026-01-01 15:14:47",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx7hazl",
              "author": "Special-Click-7607",
              "text": "what is utcp?",
              "score": 1,
              "created_utc": "2026-01-02 05:35:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7ib5l",
                  "author": "Revolutionary_Sir140",
                  "text": "Universal Tool Calling Protocol, alternative to Model Context Protocol, an ai tooling that enables ai agent to call any API.",
                  "score": 2,
                  "created_utc": "2026-01-02 05:42:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7h846",
          "author": "Special-Click-7607",
          "text": "good one. built a personal WhatsApp scheduler using Go + whatsmeow too.  \nI was sending WhatsApp messages at 3am and I just wished something to schedule WhatsApp messages like you can do with email.",
          "score": 3,
          "created_utc": "2026-01-02 05:34:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwzkfac",
          "author": "qwer1627",
          "text": "Good work! I built a thing like that for Reddit/Threads/GroupMe, didnt release it though because I fear the privacy implications. Innit wild how easy it is to exfil data out of these messaging apps??",
          "score": 2,
          "created_utc": "2025-12-31 22:14:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwzll5v",
              "author": "felipe-adeildo",
              "text": "Yeah, WhatsApp is actually well-designed from a security architecture perspective.\n\nMost messaging apps are way easier to reverse engineer. WhatsApp has proper E2E encryption, session management, and complex sync protocols.\n\nThat's why there are so few libraries that do this ***well***. Most WhatsApp automation tools rely on browser automation (Puppeteer/Selenium) which is fragile and breaks constantly (ok, i know it occurs on bare requests too, but less...)\n\nwhatsmeow is one of the rare libraries that properly reverse-engineered the protocol and maintains compatibility. That's why this project works, and why whatsapp-mcp is written in go :p",
              "score": 5,
              "created_utc": "2025-12-31 22:21:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx17iu5",
                  "author": "admiller07",
                  "text": "Love this!  How does your mcp differ from others for WhatsApp already out there?",
                  "score": 2,
                  "created_utc": "2026-01-01 04:32:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1py1t6z",
      "title": "[Release] Skill Seekers v2.5.0 - Multi-Platform Support: Convert docs to skills for Claude, Gemini, ChatGPT, or any LLM",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1py1t6z/release_skill_seekers_v250_multiplatform_support/",
      "author": "Critical-Pea-8782",
      "created_utc": "2025-12-28 20:38:35",
      "score": 30,
      "num_comments": 4,
      "upvote_ratio": 0.94,
      "text": "Hey ðŸ‘‹\n\n  Released **Skill Seekers v2.5.0** with universal LLM support - convert any documentation into structured markdown skills.\n\n  ## What It Does\n\n  Automatically scrapes documentation websites and converts them into organized, categorized reference files with extracted code examples. Works with any LLM (local or remote).\n\n  ## New in v2.5.0: Universal Format Support\n\n  - âœ… **Generic Markdown export** - works with ANY LLM\n  - âœ… **Claude AI** format (if you use Claude)\n  - âœ… **Google Gemini** format (with grounding)\n  - âœ… **OpenAI ChatGPT** format (with vector search)\n\n  ## Why This Matters for Local LLMs\n\n  Instead of context-dumping entire docs, you get:\n  - **Organized structure**: Categorized by topic (getting-started, API, examples, etc.)\n  - **Extracted patterns**: Code examples pulled from docs with syntax highlighting\n  - **Portable format**: Pure markdown ZIP - use with Ollama, llama.cpp, or any local model\n  - **Reusable**: Build once, use with any LLM\n\n  ## Quick Example\n\n  ```bash\n  # Install\n  pip install skill-seekers\n\n  # Scrape any documentation\n  skill-seekers scrape --config configs/react.json\n\n  # Export as universal markdown\n  skill-seekers package output/react/ --target markdown\n\n  # Result: react-markdown.zip with organized .md files\n```\n\n  The output is just structured markdown files - perfect for feeding to local models or adding to your RAG pipeline.\n\n  Features\n\n  - ðŸ“„ Documentation scraping with smart categorization\n  - ðŸ™ GitHub repository analysis\n  - ðŸ“• PDF extraction (for PDF-based docs)\n  - ðŸ”€ Multi-source unified (docs + code + PDFs in one skill)\n  - ðŸŽ¯ 24 preset configs (React, Vue, Django, Godot, etc.)\n\n  Links\n\n  - GitHub: https://github.com/yusufkaraaslan/Skill_Seekers\n  - PyPI: https://pypi.org/project/skill-seekers/\n  - Release: https://github.com/yusufkaraaslan/Skill_Seekers/releases/tag/v2.5.0\n\n  MIT licensed, contributions welcome! Would love to hear what documentation you'd like to see supported.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1py1t6z/release_skill_seekers_v250_multiplatform_support/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "nwigyk0",
          "author": "Stock-Protection-453",
          "text": "Nice",
          "score": 2,
          "created_utc": "2025-12-29 08:12:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwineu8",
              "author": "Critical-Pea-8782",
              "text": "Thanks ðŸ˜Ž",
              "score": 1,
              "created_utc": "2025-12-29 09:13:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwijiqn",
          "author": "UnderstandingOwn4448",
          "text": "Nice! I have used this for 3 libraries now and it seems to work great",
          "score": 2,
          "created_utc": "2025-12-29 08:36:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwinip4",
              "author": "Critical-Pea-8782",
              "text": "Good to hear that if there are any problems or improvements you can think always happy to hear them ðŸ™‚",
              "score": 1,
              "created_utc": "2025-12-29 09:14:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzh09o",
      "title": "Six Patterns for Connecting LLM Agents to Stateful Tools",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1pzh09o/six_patterns_for_connecting_llm_agents_to/",
      "author": "Low-Efficiency-9756",
      "created_utc": "2025-12-30 12:34:40",
      "score": 22,
      "num_comments": 10,
      "upvote_ratio": 0.96,
      "text": "LLMs are stateless. Your database isn't. After building several MCP servers, I distilled six patterns that make the bridge work:\n\n1. **Externalize all state**Â â€” The agent isn't smart; the database is\n2. **Rich query tools**Â â€” Let the LLM reconstruct context on demand\n3. **Composite operations**Â â€” Batch actions to reduce coordination overhead\n4. **Fuzzy input validation**Â â€” Levenshtein matching for LLM tolerance\n5. **Explicit synchronization**Â â€” Fork/snapshot models for complex state\n6. **Chat-first output**Â â€” ASCII art, visual hierarchy, fits in a chat window\n\nFull write-up with code examples in comments.",
      "is_original_content": false,
      "link_flair_text": "resource",
      "permalink": "https://reddit.com/r/mcp/comments/1pzh09o/six_patterns_for_connecting_llm_agents_to/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "nwpyh6i",
          "author": "Low-Efficiency-9756",
          "text": "check out the full writeup here, no paywall: [https://mnehmos.github.io/Mnehmos/blog/stateful-mcp-architecture/](https://mnehmos.github.io/Mnehmos/blog/stateful-mcp-architecture/)",
          "score": 4,
          "created_utc": "2025-12-30 12:35:15",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwuibk3",
          "author": "jovial1991",
          "text": "Nice article!\n\nI was also wondering about the high token usage when an agent needs to call several tools, and I started writing down some of my thoughts and experiments. I came here to talk with more experienced people and check if Iâ€™m on the right path.\n\nIf you have some time, could you take a look at this draft?  \n\nhttps://gist.github.com/josealmada/27060317da5fc858d0d2efa2d3a16511\n\nThis is one way of composing operations, similar to your third pattern.\n\nIâ€™ve never written anything like this before, so any feedback is appreciated.",
          "score": 2,
          "created_utc": "2025-12-31 02:38:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv0e8h",
              "author": "Low-Efficiency-9756",
              "text": "Hi, id love to review the draft, however its 404ing for me! idk if its public or not lmk!\n\nAs per your question, high token usage can be an issue. You have to be very careful at times and build safe guards into your tools. For example in my mcp server I had a read\\_file tool. simple except for the fact it read a file much larger than it should have, the context immediately jumped to 800k tokens and the chat was unrecoverable.\n\nfrom there i started to build in hard coded limits like limiting the max lines readable to 500 for read\\_file. Then when we build composite tools, we can think of different ways to do them. My favorite is enum menus where tools can be more modular and one tool can serve many different functions.\n\nThen we can also add in batch tools.  \nbatch\\_read\\_files  \nbatch\\_string\\_replace  \nbatch\\_read\\_lines  \netc\n\nthis allow us to turn 20 calls into 1. A time saver and a context saver for the misc info we get from tool outputs.",
              "score": 2,
              "created_utc": "2025-12-31 04:30:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwv0xnt",
                  "author": "jovial1991",
                  "text": "Just fixed the link!",
                  "score": 1,
                  "created_utc": "2025-12-31 04:34:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwv37fl",
              "author": "Low-Efficiency-9756",
              "text": "worrying about ingredient not found is a bit of design smell imo  \nThe server should validate plausibility not exact match, we can treat unknown ingredients with confidence scores rather than failures. \n\n    // Bad: Rigid lookup\n    ingredient(\"KRAFT Smooth Peanut Butter\") // Fails if not in DB\n    \n    // Your pattern: Flexible with validation\n    ingredient({\n      name: \"peanut butter\",\n      brand: \"KRAFT\",  // Optional refinement\n      fallback: \"generic_peanut_butter\",\n      nutritional_override: { /* user-provided if unknown */ }\n    })\n\nErrors as guidance is non negotiable in our loop. \n\n    // Don't negotiate. Educate.\n    {\n      error: \"INGREDIENT_AMBIGUOUS\",\n      message: \"Multiple matches for 'peanut butter'\",\n      suggestions: [\n        { id: \"pb_001\", name: \"Generic Peanut Butter\", confidence: 0.95 },\n        { id: \"pb_kraft\", name: \"KRAFT Smooth\", confidence: 0.87 }\n      ],\n      hint: \"Re-call with specific id, or provide nutritional_override for custom entry\"\n    }\n\nthe error should tell the model exactly how to fix it.",
              "score": 1,
              "created_utc": "2025-12-31 04:49:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwutohl",
          "author": "RoboCopsGoneMad",
          "text": "very interesting that you chose an RPG domain, Im working on something similar, but am focused more on RAG for rules reference and generating examples",
          "score": 2,
          "created_utc": "2025-12-31 03:47:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv0zrg",
              "author": "Low-Efficiency-9756",
              "text": "I think rpgs are a great culmination between the stochastic nature of LLM's (telling stories and narratives) \n\nand ensuring the game is a game that follows rules (the determistic nature of tooling) \n\nI'm currently working on translating SRD 5.2 and The World's Largest Dungeon Book 1 into a RAG rules and reference server. I'm thinking a dual approach. One traditional RAG, and one an SQL mcp server. \n\nMaybe those tools in combination can give both a broad and a granular level of control for models to use Rules and Reference material.",
              "score": 1,
              "created_utc": "2025-12-31 04:34:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwq0lnj",
          "author": "JohnLebleu",
          "text": "Batching operations was the biggest game changer for my mcp servers. Speeds up the interaction a lot and it was very simple to implement. All mcp servers should have that as a feature.\n\n\nBut I do prefer sending back information in json format and instead guiding the llm on how to display the information. This way I can create a custom mcp client that doesn't use llm but can still interact with a mcp server. This can be very useful for testing.Â ",
          "score": 1,
          "created_utc": "2025-12-30 12:50:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwq2wui",
              "author": "Low-Efficiency-9756",
              "text": "You can absolutely have both! I do this with a layered approach:\nThe tool returns structured JSON internally, but the final response includes a pre-formatted display string alongside the raw data or just a reformatting.",
              "score": 3,
              "created_utc": "2025-12-30 13:06:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pya8yo",
      "title": "Why I'm building my own CLIs for agents",
      "subreddit": "mcp",
      "url": "https://martinalderson.com/posts/why-im-building-my-own-clis-for-agents/",
      "author": "malderson",
      "created_utc": "2025-12-29 02:38:18",
      "score": 21,
      "num_comments": 5,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/mcp/comments/1pya8yo/why_im_building_my_own_clis_for_agents/",
      "domain": "martinalderson.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwhyzdl",
          "author": "KeithLeague",
          "text": "Hey, me too! https://enact.tools. This is a demonstration of it using playwright:  \n[https://enact.tools/blog/claude-code-superpowers](https://enact.tools/blog/claude-code-superpowers)\n\nEnact uses the \"skills\" standard but also defines a command to be executed as proposed here: https://github.com/anthropics/skills/issues/157 \n\nBasically you can define any tools `user/my-tools/playwright` or whatever and publish them so they can be searched semantically and executed via cli.\n\nI still believe MCP is the future regarding interfacing with agents, but the main tools in your context window will be for searching, registering and executing tools.",
          "score": 10,
          "created_utc": "2025-12-29 05:38:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwkhsx9",
              "author": "malderson",
              "text": "Very interesting - DMed you!",
              "score": 2,
              "created_utc": "2025-12-29 16:35:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhn34u",
          "author": "Orpheusly",
          "text": "I've also been eyeballing the MCP world recently and.. yeah it just seems like a horizontal abstraction that really isn't necessary except in certain cases.",
          "score": 3,
          "created_utc": "2025-12-29 04:17:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiwvdn",
          "author": "Cumak_",
          "text": "Yeah, this is the way",
          "score": 1,
          "created_utc": "2025-12-29 10:41:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkkdl6",
          "author": "circamidnight",
          "text": "Isn't it more of a problem with MCP clients though. They can, and sometimes do, allow you to assign mcp tools to certain subagents using skills or similiar mechanisms to optionaly add to context. This is more of an agent context managment problem more than something innate about MCP.\n\nI do agree that in some usecases, a simple cli tool is better than a full MCP server. But it is limited to mostly coding or technical usecases. There are many agents that we don't want to have access to a bash tool to execute our cli's.",
          "score": 1,
          "created_utc": "2025-12-29 16:47:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q3cb3r",
      "title": "PlanoA3B - fast, predictable multi-agent orchestration LLM for agentic apps",
      "subreddit": "mcp",
      "url": "https://i.redd.it/qcvp6fjfi8bg1.png",
      "author": "AdditionalWeb107",
      "created_utc": "2026-01-04 01:22:38",
      "score": 21,
      "num_comments": 10,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "resource",
      "permalink": "https://reddit.com/r/mcp/comments/1q3cb3r/planoa3b_fast_predictable_multiagent/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxjr4ks",
          "author": "mt-beefcake",
          "text": "Ha im literally building this now, but for my home llm and agents. Im assuming it isnt free...\n\nIt is, omg thank you, saved me days to make a shittier version",
          "score": 2,
          "created_utc": "2026-01-04 01:43:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxjrqzv",
              "author": "AdditionalWeb107",
              "text": "it is absolutely free of use (just not packaged in a proxy/gateway like framework that is being distributed for commercial adoption). That's the only catch, else would love for you to build and explore with this LLM\n\nAnd of course Iâ€™d love for you to try Plano the substrate . And if you like what you see drop us a star",
              "score": 4,
              "created_utc": "2026-01-04 01:47:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxk3dex",
                  "author": "mt-beefcake",
                  "text": "Hell yes dude, I will!",
                  "score": 3,
                  "created_utc": "2026-01-04 02:51:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxl1brn",
          "author": "ExtentOdd",
          "text": "great work!",
          "score": 2,
          "created_utc": "2026-01-04 06:35:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxl2477",
              "author": "AdditionalWeb107",
              "text": "ðŸ™",
              "score": 1,
              "created_utc": "2026-01-04 06:42:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxyk9l6",
          "author": "appakaradi",
          "text": "Is this a fine tune of Qwen?",
          "score": 2,
          "created_utc": "2026-01-06 05:38:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxyliyo",
              "author": "AdditionalWeb107",
              "text": "yes it is. Qwen FTW! We also have a LLama-based version that performs relatively well, will be releasing that fine-tune soon",
              "score": 1,
              "created_utc": "2026-01-06 05:48:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxyotj8",
                  "author": "appakaradi",
                  "text": "Thank you.",
                  "score": 1,
                  "created_utc": "2026-01-06 06:14:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxm7lkl",
          "author": "SatoshiNotMe",
          "text": "Sounds very useful. What metric are you comparing, in the table shown?\nMaybe that part got cutoff",
          "score": 1,
          "created_utc": "2026-01-04 12:44:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxo3ox9",
              "author": "AdditionalWeb107",
              "text": "thank you. For orchestration performance measurements, we used the agent-bench as a primary metric. But we had to develop some additional benchmarks which are discussed more in detail on the HF models page",
              "score": 2,
              "created_utc": "2026-01-04 18:34:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q1yupu",
      "title": "Built a tool to make MCP execution visible, shipping v1.1.0",
      "subreddit": "mcp",
      "url": "https://v.redd.it/gsatfx60wxag1",
      "author": "hack_the_developer",
      "created_utc": "2026-01-02 13:39:40",
      "score": 18,
      "num_comments": 0,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/mcp/comments/1q1yupu/built_a_tool_to_make_mcp_execution_visible/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q2py06",
      "title": "Claude Team MCP â€“ Enables collaboration between multiple AI models (GPT, Claude, Gemini) to work together on complex tasks, with intelligent task distribution and role-based expert assignment for code development, review, and optimization.",
      "subreddit": "mcp",
      "url": "https://glama.ai/mcp/servers/@7836246/claude-team-mcp",
      "author": "modelcontextprotocol",
      "created_utc": "2026-01-03 09:00:05",
      "score": 18,
      "num_comments": 3,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "server",
      "permalink": "https://reddit.com/r/mcp/comments/1q2py06/claude_team_mcp_enables_collaboration_between/",
      "domain": "glama.ai",
      "is_self": false,
      "comments": [
        {
          "id": "nxeqlg1",
          "author": "modelcontextprotocol",
          "text": "This server has 18 tools:\n\n- [analyze_project_structure](https://glama.ai/mcp/servers/@7836246/claude-team-mcp/tools/analyze_project_structure) â€“ Analyze project structure to identify technology stacks and architecture patterns for development planning and optimization.\n- [ask_expert](https://glama.ai/mcp/servers/@7836246/claude-team-mcp/tools/ask_expert) â€“ Consult specialized experts (frontend, backend, or QA) to get targeted answers for technical questions within the Claude Team MCP collaborative environment.\n- [code_review](https://glama.ai/mcp/servers/@7836246/claude-team-mcp/tools/code_review) â€“ Get expert code reviews from frontend, backend, or QA specialists to improve code quality and identify issues before deployment.\n- [cost_estimate](https://glama.ai/mcp/servers/@7836246/claude-team-mcp/tools/cost_estimate) â€“ Estimate token usage and execution time for AI tasks to help plan resources before running complex operations.\n- [explain_plan](https://glama.ai/mcp/servers/@7836246/claude-team-mcp/tools/explain_plan) â€“ Analyze how a Tech Lead would distribute tasks for a given project requirement to optimize team workflow and resource allocation.\n- [fix_bug](https://glama.ai/mcp/servers/@7836246/claude-team-mcp/tools/fix_bug) â€“ Fix bugs in code by analyzing problematic code snippets and error descriptions to provide corrected solutions.\n- [generate_commit_message](https://glama.ai/mcp/servers/@7836246/claude-team-mcp/tools/generate_commit_message) â€“ Generate Git commit messages from code changes using conventional, simple, or detailed styles to document version history.\n- [history_context](https://glama.ai/mcp/servers/@7836246/claude-team-mcp/tools/history_context) â€“ Retrieve recent collaboration context to continue previous work by accessing the last few interactions between AI models.\n- [history_get](https://glama.ai/mcp/servers/@7836246/claude-team-mcp/tools/history_get) â€“ Retrieve detailed records of specific AI collaboration sessions to review task distribution, expert assignments, and workflow outcomes for analysis and optimization.\n- [history_list](https://glama.ai/mcp/servers/@7836246/claude-team-mcp/tools/history_list) â€“ Retrieve team collaboration history records to track project progress and review past interactions between AI models.",
          "score": 3,
          "created_utc": "2026-01-03 09:00:05",
          "is_submitter": true,
          "replies": [
            {
              "id": "nxfeqn3",
              "author": "volcanotnt",
              "text": "Hey man! Nice, nice! I have a questionâ€”maybe you know a solution. I want a single window where different agents can log in and collaborate between multiple AI models like GPT, Claude, Gemini, and ChatGPT to have long discussions and debates. Iâ€™d prefer a setup with login and password for the agents, but I canâ€™t seem to find one. Maybe an IDE window could work. Iâ€™ve been looking for this for a long time :D Iâ€™m willing to pay for a working version!",
              "score": 1,
              "created_utc": "2026-01-03 12:22:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxi5jce",
                  "author": "punkpeye",
                  "text": "So you have a single window, and each agent is a persona representing a different model? Who is driving the conversation?",
                  "score": 1,
                  "created_utc": "2026-01-03 20:46:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q2b0i6",
      "title": "MCP works great â€” until you actually ship.",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1q2b0i6/mcp_works_great_until_you_actually_ship/",
      "author": "_dremnik",
      "created_utc": "2026-01-02 21:21:23",
      "score": 15,
      "num_comments": 21,
      "upvote_ratio": 0.75,
      "text": "I wrote a blog post discussing some of the limitations of MCP.\n\nI'd love to get your guys' thoughts.\n\n* What has been your experience building apps with MCP?\n* Have you experienced the problems I discuss? How did you solve them, if so?\n\n[https://dremnik.substack.com/p/mcp-works-great-until-you-actually](https://dremnik.substack.com/p/mcp-works-great-until-you-actually)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1q2b0i6/mcp_works_great_until_you_actually_ship/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "nxcebxb",
          "author": "anirishafrican",
          "text": "Honestly, I've had an incredible experience building MCP.\n\nThe thing that makes it so useful is that you can use the MCP directly from Claude code or from any other instance. And when anything goes wrong, it's got full context of what it requested and what it expected.  \n  \nYou can get an effortless stack trace and full suggestion of what to fix straight away. (Particularly powerful from Claude Code ofc)",
          "score": 3,
          "created_utc": "2026-01-02 23:35:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxceunt",
              "author": "_dremnik",
              "text": "totally agreed on the usage in Claude Code. the protocol was definitely catered towards those use cases like i mentioned in the post \n\n\\> It works well for the use cases around which it was designed (Claude Code, Cursor â€” local, single-tenant clients with a clear user scope)\n\nthe criticisms that i laid out are more geared towards a builders perspective, for people who are building agents in production systems :)",
              "score": 1,
              "created_utc": "2026-01-02 23:38:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxcfygw",
                  "author": "anirishafrican",
                  "text": "So as a consumer of the MCP server as well, it's a fair point that it's an LLM, contractless best effort approach. I've been accepting that as a given, and creating skills (playbooks in my platform - [xtended.ai](https://xtended.ai) if curious)\n\nIt has allowed consistent usage of many MCP integrations across a range of AI clients. I've baked that in now into a recommended system prompt I suggest for any users of the platform (a strange new world)\n\nHow does your tool marketplace fit into who ecosystem? e.g. Claude Web / Mobile / ChatGPT",
                  "score": 2,
                  "created_utc": "2026-01-02 23:44:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxcojsv",
          "author": "lexxwern",
          "text": "* MCP is glorified APIs for all intents and purposes\n* There's nothing an agent can't do with OpenAPI documented services vs. MCPs\n* For non-read-only agents, both APIs and MCPs need to figure out transactions a d rollback mechanisms.",
          "score": 2,
          "created_utc": "2026-01-03 00:31:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxcqh43",
              "author": "JohnLebleu",
              "text": "The big advantge of MCP is offering a \"bring your own AI\" interface to a software.Â ",
              "score": 3,
              "created_utc": "2026-01-03 00:41:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdydze",
          "author": "[deleted]",
          "text": "Are you looking for an LSP?",
          "score": 1,
          "created_utc": "2026-01-03 05:08:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxff77v",
          "author": "Hofi2010",
          "text": "None of the problems you are citing in your article are MCP problems. MCP is, as people noted, a standardized tool interface. Very similar to rest apiâ€™s. Anything else is up to the architect and/or developer. Your REST API also has no clue about your schema, but the developer has. In the context of MCP you need to give that information either to the LLM and it will create queries based on the schema or you need to define an interface that (parameters) that are implicitly including your schema fields and tables, eg save_sale(company, thing_to_order, amount , delivery_date)",
          "score": 1,
          "created_utc": "2026-01-03 12:26:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfgqa4",
              "author": "_dremnik",
              "text": "iâ€™d love to hear how you would solve the code sandbox problem deterministically :) \n\ni donâ€™t consider passing prompts to the LLM without runtime schema checks a good solution personally..\n\nif we are supposed to build reliable agents on this then i think its reasonable to expect to be able to eliminate non-determinism as much as possible, seeing that its already so hard to build production systems reliably.",
              "score": 1,
              "created_utc": "2026-01-03 12:37:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxfhdj1",
                  "author": "Hofi2010",
                  "text": "No they are not. You are mis-understanding what MCP is trying to solve imo.",
                  "score": 1,
                  "created_utc": "2026-01-03 12:42:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxbxcib",
          "author": "Automatic-Step-9756",
          "text": "To answer your questions:\n\n1. Experience has been very exhausting with rapidly changing MCP spec, but I think now spec has been mostly stable. My servers is HTTP transport and generated from user DB schema.\n2. But having MCP server gave advantage of being able to perform CRUD operations on database using AI powered chat.\n3. We are using OAuth flow for user authentication and backed MCP server is supported with RBAC to check user's authorizations for perform activities.\n\nCheckout how we use MCP. It has simplified a user interaction a lot -Â [https://medium.com/@crudler/stop-clicking-forms-start-talking-mcp-in-crudler-45d2624fd06f](https://medium.com/@crudler/stop-clicking-forms-start-talking-mcp-in-crudler-45d2624fd06f)",
          "score": 1,
          "created_utc": "2026-01-02 22:05:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxby6r1",
              "author": "_dremnik",
              "text": "hmm what kind of MCP server did you implement? and what do you think about some of the issues around schema variability + context propagation that I discussed? \n\nyou can have CRUD operations against a database without MCP, and my argument is that in a lot of cases its a lot simpler to do this instead of fighting the protocol's limitations",
              "score": 1,
              "created_utc": "2026-01-02 22:09:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxc2p1i",
                  "author": "Automatic-Step-9756",
                  "text": "We do handle tool schema variability in CRUD list operation(where user is able to provide any filter condition on table columns in plain english), through instructing LLM in tool description. For context propagation have you explored around MCP sessions(mcp-session-id)?\n\nBy the way - dont want to self promote, but would recommend to try [crudler.com](http://crudler.com) to see how it takes CRUD beyond simple db operations to complicated transaction.",
                  "score": 2,
                  "created_utc": "2026-01-02 22:32:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxcfw7u",
                  "author": "JohnLebleu",
                  "text": "You can't do crud without mcp if you aren't controlling the AI agent that makes the call, that's the real advantage of mcp.Â ",
                  "score": 2,
                  "created_utc": "2026-01-02 23:43:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q01twd",
      "title": "Built a leaner Microsoft Graph MCP - 7 tools instead of 37, with direct multi-tenant access",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1q01twd/built_a_leaner_microsoft_graph_mcp_7_tools/",
      "author": "BTForIT",
      "created_utc": "2025-12-31 02:53:27",
      "score": 15,
      "num_comments": 1,
      "upvote_ratio": 0.86,
      "text": "Been using Claude with Microsoft 365 via MCP and hit two frustrations:\n\n**Problem 1: Context bloat**\n\nThe popular MS Graph MCPs expose 30-40 specialized tools like `list-mail-messages`, `create-calendar-event`, `get-user`, etc. That's ~12KB of context eaten up before you even start talking. Claude already knows the Graph API - it doesn't need 37 hand-holding tools.\n\n**Problem 2: Multi-tenant switching sucks**\n\nI manage multiple M365 tenants (work, clients, personal). Every time I wanted to query a different tenant, I had to call `select-account`, wait, then make my request. Constantly switching context.\n\n**The fix:**\n\nForked the Softeria MCP and stripped it down to 7 tools:\n- `login`, `logout`, `verify-login` (auth)\n- `list-accounts`, `select-account`, `remove-account` (account management)\n- `graph-request` (one tool for ALL Graph API calls)\n\nThe `graph-request` tool takes an `accountId` parameter, so you can query any tenant directly without switching:\n\n```json\n{\n  \"endpoint\": \"/me/messages\",\n  \"accountId\": \"client-tenant-abc123\"\n}\n```\n\nQuery work and personal calendars in the same conversation. No switching dance.\n\n~1KB context instead of ~12KB. Same capabilities.\n\n**Repo:** https://github.com/ForITLLC/forit-microsoft-graph\n\nMIT licensed, fork of Softeria's work. Just a different philosophy - less is more.\n\nAnyone else running into context bloat issues with MCPs?\n",
      "is_original_content": false,
      "link_flair_text": "server",
      "permalink": "https://reddit.com/r/mcp/comments/1q01twd/built_a_leaner_microsoft_graph_mcp_7_tools/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "nxekw6n",
          "author": "FancyConversation555",
          "text": "Thanks for sharing. I did a similar thing with Databricks MCP. It was easy to write a â€œproxyâ€ mcp with limited  set of tools by combining tool calls into one.",
          "score": 1,
          "created_utc": "2026-01-03 08:10:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q4iyuh",
      "title": "Reticle: a local â€œtraffic inspectorâ€ for MCP (stdio/HTTP), looking for feedback on debugging + tracing",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1q4iyuh/reticle_a_local_traffic_inspector_for_mcp/",
      "author": "PutPurple844",
      "created_utc": "2026-01-05 11:09:48",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 0.92,
      "text": "**Disclosure: Iâ€™m the author.**  \nThis is **Reticle**, a local debugging/observability proxy for the Model Context Protocol.\n\nhttps://preview.redd.it/5ic276c5libg1.png?width=1922&format=png&auto=webp&s=b217224bda7b38d846108b020e700d22b1a4f98f\n\n  \nMCP is awesome, but day-to-day debugging can feel like building web apps without devtools:\n\n* a tool server crashes over **stdio** â†’ the agent \"hangs\" and you donâ€™t know why\n* errors like `-32600 Invalid Request` Often arrive without enough context to fix quickly\n* â€œcontext bloatâ€ is real once you connect multiple servers, itâ€™s hard to attribute where tokens/time are going\n* And honestly, running random `npx` servers with broad local permissions can feel doubtful even when you trust the author.\n\n**What Reticle does**  \nIt sits *between* an MCP client â†” MCP servers and shows, in real time:\n\n* raw **JSON-RPC** stream (requests / responses / notifications)\n* request â†” response **correlation**\n* **latency** per call + slow-call highlighting\n* **token estimates** per message/method (so you can find the hogs)\n* **stderr capture** (tracebacks, crash logs, debug prints)\n* sessions + recording + export (handy for bug reports / repros)\n\n**What itâ€™s not**\n\n* Not a sandbox. It wonâ€™t magically make untrusted servers safe, itâ€™s visibility, not isolation.\n* Not another hosted platform / waitlist, itâ€™s an OSS devtool you run locally.\n\n**Where Iâ€™d love community input**\n\n1. What are your most common â€œMCP went silentâ€ failure modes (stdio, streamable HTTP, SSE)?\n2. Would you want a *standard* tracing story in MCP (e.g., correlation IDs, structured error envelopes, tool-call spans), or should this stay out-of-band?\n3. For â€œrandom serversâ€: whatâ€™s your current safety posture (containers, allowlists, restricted FS, separate user accounts, etc.)?\n\nGitHub: [https://github.com/LabTerminal/mcp-reticle](https://github.com/LabTerminal/mcp-reticle?utm_source=chatgpt.com)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1q4iyuh/reticle_a_local_traffic_inspector_for_mcp/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "nxthsq2",
          "author": "ParamedicAble225",
          "text": "Thatâ€™s awesome. Itâ€™s like wireshark but for mcp.Â \nI have the same thing but just a console log wrapped around my transport layer with no visuals. Helped me find several problems like duplicate initializations/tool calls.\n\nShould test it out on my mcp server with oauth https://tree.tabors.site/mcp\n\nIf you add that url as a connector on ChatGPT with oauth enabled it should automatically connectÂ ",
          "score": 1,
          "created_utc": "2026-01-05 14:08:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxufrkt",
              "author": "PutPurple844",
              "text": "Ok, Thanks",
              "score": 1,
              "created_utc": "2026-01-05 16:55:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxvnww1",
          "author": "matt8p",
          "text": "How are you estimating token usage?",
          "score": 1,
          "created_utc": "2026-01-05 20:17:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxylf44",
              "author": "PutPurple844",
              "text": "Token estimation:Â `text.len() / 4`. That's it. No tiktoken, no BPE tokenizer, no WASM blob.\n\nWhy? For a debugging tool showing 'this message is \\~2k tokens', being off by 15% doesn't matter. What matters is instantly spotting that one tool dumping 60k tokens of context. The 4 bytes/token ratio is surprisingly accurate for English JSON-RPC, close to OpenAI's \\~4 chars/token average.\n\nÂ Would I use this for billing? Absolutely not. For a latency-sensitive proxy that processes thousands of messages?Â Sure.",
              "score": 1,
              "created_utc": "2026-01-06 05:47:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q3s83j",
      "title": "Hi everyone I need help on how to create MCP servers",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1q3s83j/hi_everyone_i_need_help_on_how_to_create_mcp/",
      "author": "Hot-Finger3903",
      "created_utc": "2026-01-04 15:08:56",
      "score": 10,
      "num_comments": 18,
      "upvote_ratio": 0.86,
      "text": "Hello everyone here, i  am currently working  with locallm and would like to create own MCP servers ,i don't know where to start so I need help ,any kind of resources or suggestions would be helpful ....Thankyou",
      "is_original_content": false,
      "link_flair_text": "resource",
      "permalink": "https://reddit.com/r/mcp/comments/1q3s83j/hi_everyone_i_need_help_on_how_to_create_mcp/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "nxn2t2t",
          "author": "arvindand1695",
          "text": "Are you proficient in a certain language? If so, look up libraries or sdks in that language to start. Otherwise if you're vibe coding it then you can tell your ai agent to use typescript or python and I would suggest using context7 mcp server with your agent while building so it uses the latest library versions and associated docs :)",
          "score": 2,
          "created_utc": "2026-01-04 15:45:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxn3ia5",
              "author": "Hot-Finger3903",
              "text": "Oh cool thank you I am currently working most with python and py 3.12 to be precise like do they provide documentation for specific version too!?",
              "score": 1,
              "created_utc": "2026-01-04 15:49:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxn4d1g",
                  "author": "arvindand1695",
                  "text": "For python - https://modelcontextprotocol.github.io/python-sdk/ this should help :)",
                  "score": 1,
                  "created_utc": "2026-01-04 15:53:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxu67yn",
                  "author": "rex-scalekit",
                  "text": "I would recommend using FastMCP if you are using python , as it is super simple and you can focus just on your functionality rather than worry about MCP protocol.",
                  "score": 1,
                  "created_utc": "2026-01-05 16:11:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxn7xia",
          "author": "r4mp_",
          "text": "For different languages (including Python) you can find examples here â€”> https://modelcontextprotocol.io/docs/develop/build-server#python",
          "score": 2,
          "created_utc": "2026-01-04 16:10:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxne3yz",
              "author": "Hot-Finger3903",
              "text": "Thanks for the examples",
              "score": 1,
              "created_utc": "2026-01-04 16:38:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxoz381",
          "author": "Purple-Print4487",
          "text": "This is a course I've developed to learn how to build high quality MCP servers:Introduction - Advanced MCP: Enterprise-Grade AI Integration with Rust https://share.google/YDAoHArYzwgzOVQt4\n\nYou can adapt the concepts to other languages if Rust is not for you.",
          "score": 1,
          "created_utc": "2026-01-04 20:55:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxrkwts",
              "author": "Hot-Finger3903",
              "text": "Thank you for this ,would ya mind if I dm you",
              "score": 1,
              "created_utc": "2026-01-05 04:59:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxterqn",
          "author": "Stock-Protection-453",
          "text": "Readable and maintainable way is to use photon, write simple typescript class with pure business logic. Photon runtime exposes it as a MCP server and also as a CLI tool\n\nSee https://github.com/portel-dev/photon",
          "score": 1,
          "created_utc": "2026-01-05 13:51:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxud673",
          "author": "ninadpathak",
          "text": "FastMCP is the way to go if you want to ship fast. But honestly, the best approach is to ask an AI to build it from scratch, then read through the generated code to understand the patterns. You'll learn the MCP protocol faster by seeing a working implementation than reading docs. The future of MCP adoption depends on tools making it easy for non-experts to contribute.",
          "score": 1,
          "created_utc": "2026-01-05 16:43:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxmxhln",
          "author": "Opinion-Former",
          "text": "Tell your llm to create it for you. An mcp server is either stdio meaning it runs on command line commanded by the llm or as an http server. If youâ€™ve never done one, start with stdio.\n\nThe first function in mcp simply lists the tools available to the llm with descriptive text to provide enough info to them to access the tools. The rest of the protocol is accessing the tools themselves.\n\nHave your llm create an mcp server for an api youâ€™re familiar with and read through the code",
          "score": 1,
          "created_utc": "2026-01-04 15:19:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmzgc0",
              "author": "Hot-Finger3903",
              "text": "It's kinda funny and thank you abt this",
              "score": 1,
              "created_utc": "2026-01-04 15:29:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxnac3e",
          "author": "Tricky-Report-1343",
          "text": "I built one click mcp deployment agent at https://pingu.audn.ai give any github url and get it deployed to vercel, render or on a vm through hopx all through agentic AI .\n\nThere's one day trial. Reach out to me on Intercom there if you run into issues",
          "score": 0,
          "created_utc": "2026-01-04 16:21:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxne25f",
              "author": "Hot-Finger3903",
              "text": "Thank you about this this sounds helpful",
              "score": 1,
              "created_utc": "2026-01-04 16:38:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0eyfc",
      "title": "Homework Grading MCP â€“ Enables automated grading of student homework images using Qwen3-VL multimodal model, supporting multiple subjects and question types with detailed feedback and batch processing capabilities.",
      "subreddit": "mcp",
      "url": "https://glama.ai/mcp/servers/@pickstar-2002/homework-grading-mcp",
      "author": "modelcontextprotocol",
      "created_utc": "2025-12-31 15:00:07",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "server",
      "permalink": "https://reddit.com/r/mcp/comments/1q0eyfc/homework_grading_mcp_enables_automated_grading_of/",
      "domain": "glama.ai",
      "is_self": false,
      "comments": [
        {
          "id": "nwx8fdi",
          "author": "modelcontextprotocol",
          "text": "This server has 1 tool:\n\n- [grade_homework](https://glama.ai/mcp/servers/@pickstar-2002/homework-grading-mcp/tools/grade_homework) â€“ Grade student homework by analyzing images to automatically score answers and provide detailed feedback using AI-powered assessment.",
          "score": 1,
          "created_utc": "2025-12-31 15:00:07",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1q11aqv",
      "title": "Protecting Your Privacy_ RedactAI MCP server",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1q11aqv/protecting_your_privacy_redactai_mcp_server/",
      "author": "Gullible-Relief-5463",
      "created_utc": "2026-01-01 10:32:58",
      "score": 9,
      "num_comments": 14,
      "upvote_ratio": 1.0,
      "text": "Do you send confidential documents directly to LLMs?\n\nThat means sensitive information often gets shared unfiltered by default.\n\nI built **RedactAI**, an MCP server that acts as a privacy firewall for PDFs. It detects and permanently redacts sensitive data before the document ever reaches the LLM, while preserving layout and providing an audit-friendly preview.\n\nEverything runs locally using Ollama. No cloud calls.\n\nBuilt using MCP (Anthropic) to explore how privacy can be enforced at the tool layer instead of being an afterthought.\n\nRepo: [https://github.com/AtharvSabde/RedactAI]()  \nDemo/context: [https://www.linkedin.com/posts/atharv-sabde](https://www.linkedin.com/posts/atharv-sabde-4aa272222_%F0%9D%97%97%F0%9D%97%BC-%F0%9D%98%86%F0%9D%97%BC%F0%9D%98%82-%F0%9D%98%80%F0%9D%97%B2%F0%9D%97%BB%F0%9D%97%B1-%F0%9D%97%BF%F0%9D%97%AE%F0%9D%98%84-%F0%9D%97%A3%F0%9D%97%97%F0%9D%97%99%F0%9D%98%80-%F0%9D%98%84%F0%9D%97%B6-activity-7412434987058130945-nAvk)\n\nCurious how others are handling privacy in LLM-based document workflows.",
      "is_original_content": false,
      "link_flair_text": "resource",
      "permalink": "https://reddit.com/r/mcp/comments/1q11aqv/protecting_your_privacy_redactai_mcp_server/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "nx2wkzi",
          "author": "Afraid-Today98",
          "text": "Local redaction before LLM access is smart. Way better than trusting cloud providers with sensitive docs.",
          "score": 3,
          "created_utc": "2026-01-01 14:05:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx34az8",
              "author": "Gullible-Relief-5463",
              "text": "Thanks, that was exactly the goal, enforce privacy before the document ever reaches an LLM.\nIf you like the approach, a star on the repo would really help, and feel free to share it with anyone working on LLM document workflows.",
              "score": 1,
              "created_utc": "2026-01-01 14:56:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx37s0u",
          "author": "DavidAntoon",
          "text": "This is really solid work ðŸ‘\nRedacting before the document ever touches the LLM is exactly the right layer to enforce privacy.\n\nIf youâ€™re open to it, this feels like a great fit as a FrontMCP plugin.\nFrontMCP is an open-source MCP runtime with a plugin system designed specifically for tool-layer guardrails like this, so RedactAI could be easily reused across LLM document workflows without re-implementing the logic.\n\nPlugin docs: https://docs.agentfront.dev/docs/plugins/overview\n\nFrontMCP: https://github.com/agentfront/frontmcp\n\nHappy to help wire this up and contribute it back as an open-source plugin if youâ€™re interested.\n\nLove the local-only + audit-friendly approach â€” privacy by default, not by policy ðŸ‘",
          "score": 2,
          "created_utc": "2026-01-01 15:18:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx38qhd",
              "author": "Gullible-Relief-5463",
              "text": "Yes, what not!\nLet's connect and work together..also don't forget to star the repo",
              "score": 2,
              "created_utc": "2026-01-01 15:23:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx3ba3k",
                  "author": "DavidAntoon",
                  "text": "Starred,  you are more than welcome to star our frontmcp repo ðŸ™",
                  "score": 1,
                  "created_utc": "2026-01-01 15:38:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx37mif",
          "author": "chill-botulism",
          "text": "Do you plan to add support for other file types?",
          "score": 1,
          "created_utc": "2026-01-01 15:17:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx38icl",
              "author": "Gullible-Relief-5463",
              "text": "Yes ofc.\nFor now thinking of deepseek OCR for scanned docs",
              "score": 1,
              "created_utc": "2026-01-01 15:22:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3h0yb",
          "author": "General-Ear-8056",
          "text": "Looks quite interesting. Do u know the minimum hardware requirements?",
          "score": 1,
          "created_utc": "2026-01-01 16:09:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3koeu",
              "author": "Gullible-Relief-5463",
              "text": "I have noted the minimum setup in the repo. I have tested it with a 1B parameter model, which works fine even on modest hardware.\nThe exact requirements mainly depend on which Ollama model you choose, smaller models run comfortably on CPU, larger ones benefit from more RAM or a GPU.\nIf you find it useful, feel free to check out the repo and drop a star.",
              "score": 2,
              "created_utc": "2026-01-01 16:28:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6yaea",
          "author": "CaptainMalikk",
          "text": "awesome will try it boss",
          "score": 1,
          "created_utc": "2026-01-02 03:27:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx766cn",
              "author": "Gullible-Relief-5463",
              "text": "Thanks ðŸ˜Ž",
              "score": 1,
              "created_utc": "2026-01-02 04:18:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx75bm5",
          "author": "Long-Chemistry-5525",
          "text": "Curious on how this would work, do you upload the document to the mcp directly outside of the llm then reference them frm Claude? As if you are telling Claude to upload to the mcp you are already exposed",
          "score": 1,
          "created_utc": "2026-01-02 04:13:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7628o",
              "author": "Gullible-Relief-5463",
              "text": "You pass a file path to the MCP server through claude, the document is processed locally, and Claude only issues the tool call. The raw document is never exposed. You should check the repo, I have added some examples in readme..give a star on the repo if you liked it.",
              "score": 1,
              "created_utc": "2026-01-02 04:17:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q1przm",
      "title": "[Feedback] Counsel MCP Server: a new \"deep research\" workflow via MCP (research + synthesis with structured debates)",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1q1przm/feedback_counsel_mcp_server_a_new_deep_research/",
      "author": "baradas",
      "created_utc": "2026-01-02 05:04:55",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "Hey folks,\n\nKept looking for aÂ **deep research**Â workflow that acts like a good analyst team aka : gather sources, generate hypotheses, challenges/critiques, and stitch a crisp answer.Â \n\nMost DR products (or modes) end up with 1-shot DR.   \n  \nNot to forget :   \n(a) single model hallucinations (made up links anyone?)  \norÂ   \n(b) a pile of unstructured notes with lil accountability\n\nI often keep running the output copy pasting from one model to another to validate the hypothesis and synthesis.Â \n\nthe current work is inspired a ton by Karpathyâ€™s work on the LLM-council repo - over the holidays, builtÂ **Counsel MCP Server**: an MCP server that runsÂ **structured debates**Â across aÂ **family of LLM agents**Â toÂ **research + synthesize**Â with fewer silent errors. The council emphasizes: a debuggable artifact trail and a MCP integration surface that can be plugged in into any assistant.\n\nIf you want to try it, thereâ€™s aÂ **playground assistant**Â with Counsel MCP already wired up:Â [**https://counsel.getmason.io**](https://counsel.getmason.io/)\n\n# What it does ?\n\n* You submit a research question or task.\n* The server runs a structured loop with multiple LLM agents (examples: propose, critique, synthesize, optional judge).\n* You get back artifacts that make it inspectable:\n   * **final**Â synthesis (answer or plan)\n   * **critiques**Â (what got challenged and why)\n   * **decision record**Â (assumptions, key risks, what changed)\n   * **trace**Â (run timeline, optional per-agent messages, cost/latency)\n\nThis is not just \"N models voting.â€ in a round robin pattern - the council will do structured arguments and critique aimed at better research outcomes.\n\n# Have 3 top of mind questions - any feedback here would be great?\n\n1. Whatâ€™s a useful API variant here ?\n   * A singleÂ `counsel.research()`Â orÂ `counsel.debate()`Â tool plus resources?\n   * Or multiple tools (run, stream, explain, get)?\n2. Whatâ€™s the right pattern for research runs that take 10â€“60 seconds?\n   * streaming events\n   * polling resources\n   * returning everything inline\n3. What should the final artifact contain?\n   * final output only\n   * final + critiques\n   * full trace + decision record\n   * whatâ€™s the minimum that still makes this debuggable and trustworthy?\n\nGive it a spin & tell me what gives\n\nPlayground:Â [**https://counsel.getmason.io**](https://counsel.getmason.io/)\n\nIf you try it, Iâ€™d love to hear any feedback good, blahhhh, meh?",
      "is_original_content": false,
      "link_flair_text": "question",
      "permalink": "https://reddit.com/r/mcp/comments/1q1przm/feedback_counsel_mcp_server_a_new_deep_research/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "nx8shvk",
          "author": "baradas",
          "text": "Got asked this in a comment   \n=======================  \nReally interesting setup with the research council workflow! The \"debuggable artifact trail\" requirement resonated. Interestingly, we're seeing this a lot as critical for multi-agent MCP systems.  \n  \nHow are you currently validating that the debate structure produces reliable results, and what's been your approach to debugging when the council output is unexpected?\n\n  \nthought of throwing in some views in here  \n===================================  \n  \nOur approach has 3 layers:\n\n\\#1 - Validation: Strict JSON schemas per phase, mandatory evidence citations (no unsourced claims), and a locked Crux Registry after the Attack phase to prevent semantic drift. We validate at the structure level (schema compliant) and semantic level (crux stability, agreement thresholds per tension).\n\n\\#2 - Debugging: Every debate produces an append-only artifact trail - transcripts per role/round, the exact context each role saw (including argument shuffle order), schema repair attempts, and crux evolution. When output is out of bounds (unexpected), we trace back through - which role drove the conclusion   \nâ†’ what evidence was cited   \nâ†’ whether the crux positions shifted unexpectedly   \nâ†’ whether validation repair led to loss in nuance.\n\n\\#3 - We also support human-in-the-loop intervention (pause, steer, modify\\_tensions) for real-time course correction when you see the debate heading somewhere wrong. the \"debuggable artifact trail\" you talk abt is what this is optimized for - a multi-agent debate synthesis is only trustworthy if you can audit the reasoning chain.",
          "score": 2,
          "created_utc": "2026-01-02 12:34:19",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nx7jo32",
          "author": "Agreeable-Gur-7525",
          "text": "I'd be interested in trying it out but I'm currently working on an app. Would it be able to help with architecture and code evaluation/decisions as well?",
          "score": 1,
          "created_utc": "2026-01-02 05:53:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7k0ug",
              "author": "baradas",
              "text": "Absolutely give it a spin for this. Right now am still working on enabling direct code context via Github integrations - but if you give it a spec (e.g. markdown or PDF docs) it should work just fine.",
              "score": 2,
              "created_utc": "2026-01-02 05:56:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7rzmd",
          "author": "akhil_agrawal08",
          "text": "This looks pretty awesome. Thanks for sharing.",
          "score": 1,
          "created_utc": "2026-01-02 07:02:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxbk6qx",
          "author": "beepdarpledoo",
          "text": "Hi. Very interesting. What debate model are you using for structured debates? Are they just prompts?",
          "score": 1,
          "created_utc": "2026-01-02 21:01:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbmw5i",
              "author": "baradas",
              "text": "This is derived off the DACI framework - am getting the protocol reviewed, it's an open counsel protoocl. You can go ahead and configure your own protocol btw with the admin and APIs",
              "score": 1,
              "created_utc": "2026-01-02 21:14:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q1qtxf",
      "title": "Swiss Health MCP Server â€“ Provides AI assistants access to 1.6 million Swiss health insurance premium records from 55 insurers across 11 years (2016-2026), enabling price comparisons, historical analysis, and finding the cheapest insurance options based on location, age, and coverage preferences.",
      "subreddit": "mcp",
      "url": "https://glama.ai/mcp/servers/@remoprinz/swiss-health-mcp",
      "author": "modelcontextprotocol",
      "created_utc": "2026-01-02 06:00:04",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "server",
      "permalink": "https://reddit.com/r/mcp/comments/1q1qtxf/swiss_health_mcp_server_provides_ai_assistants/",
      "domain": "glama.ai",
      "is_self": false,
      "comments": [
        {
          "id": "nx7khpc",
          "author": "modelcontextprotocol",
          "text": "This server has 4 tools:\n\n- [compare_insurers](https://glama.ai/mcp/servers/@remoprinz/swiss-health-mcp/tools/compare_insurers) â€“ Compare health insurance premiums from multiple Swiss insurers based on canton, year, age group, and franchise to identify cost-effective options.\n- [get_cheapest_insurers](https://glama.ai/mcp/servers/@remoprinz/swiss-health-mcp/tools/get_cheapest_insurers) â€“ Compare Swiss health insurance premiums to find the most affordable options based on canton, age group, deductible amount, and coverage preferences.\n- [get_database_stats](https://glama.ai/mcp/servers/@remoprinz/swiss-health-mcp/tools/get_database_stats) â€“ Retrieve database statistics including entry counts, available years, and insurers to understand the scope of Swiss health insurance premium data for analysis.\n- [get_price_history](https://glama.ai/mcp/servers/@remoprinz/swiss-health-mcp/tools/get_price_history) â€“ Track Swiss health insurance premium changes over time by insurer, canton, age group, and franchise amount to analyze cost trends and make informed coverage decisions.",
          "score": 1,
          "created_utc": "2026-01-02 06:00:04",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzscgg",
      "title": "Best MCP servers for AI Agents",
      "subreddit": "mcp",
      "url": "https://i.redd.it/u25442aqeeag1.png",
      "author": "Worldly_Ad_2410",
      "created_utc": "2025-12-30 20:08:54",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "server",
      "permalink": "https://reddit.com/r/mcp/comments/1pzscgg/best_mcp_servers_for_ai_agents/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q3yrni",
      "title": "mcpc: a new universal MCP command-line client",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1q3yrni/mcpc_a_new_universal_mcp_commandline_client/",
      "author": "jancurn",
      "created_utc": "2026-01-04 19:17:51",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 0.79,
      "text": "*Processing img cpj2b9rnsdbg1...*\n\nHey all,\n\nOver the holidays, I finished a new tool, which I believe many of you here will find helpful:\n\n`mcpc`Â is a universal CLI client for MCP that maps MCP operations to intuitive commands for interactive shell use, scripts, and AI coding agents.\n\n**Key features:**\n\n* Swiss Army knife for MCP: supports stdio and streamable HTTP, server config files and zero config, OAuth 2.1, HTTP headers, and main MCP features.\n* Persistent sessions for interaction with multiple servers simultaneously.\n* Structured text output enables AI agents to explore and interact with MCP servers.\n* `--json` output and `--schema` validation allow stable integration with other CLI tools like `jq`, scripting, and MCPÂ [**code mode**](https://blog.cloudflare.com/code-mode/)Â in a shell.\n* `--proxy` MCP server to provide AI code sandboxes with secure access to authenticated MCP sessions, without sharing credentials.\n\nThe source code and docs are on [https://github.com/apify/mcpc](https://github.com/apify/mcpc)\n\nI'd love to hear your thoughts!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1q3yrni/mcpc_a_new_universal_mcp_commandline_client/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "nxoecwv",
          "author": "macromind",
          "text": "This is a really nice idea, MCP tooling feels like its finally getting the CLI ergonomics it needed. The persistent sessions + structured output combo seems especially useful for agentic workflows (like letting an AI agent probe capabilities safely, then execute a narrow command).\n\nCurious, do you have a recommended pattern for managing auth tokens when you run mcpc in CI or inside an automation runner?\n\nAlso, if anyone is collecting practical patterns for agentic AI automation (routing, retries, tool contracts, etc), I have been bookmarking notes here: https://www.agentixlabs.com/blog/",
          "score": 4,
          "created_utc": "2026-01-04 19:21:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxogppy",
              "author": "jancurn",
              "text": "Thanks! There's no way to export OAuth profiles (by design, for security), but you can use the MCP server proxy (\\`--proxy host:port\\` option) to provide access to an authenticated MCP server via an open proxy.",
              "score": 1,
              "created_utc": "2026-01-04 19:31:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0befz",
      "title": "ForIT Microsoft Graph â€“ Provides direct access to Microsoft Graph API with multi-tenant account management, allowing users to interact with Microsoft 365 services across multiple tenants through a single flexible graph-request tool.",
      "subreddit": "mcp",
      "url": "https://glama.ai/mcp/servers/@ForITLLC/forit-microsoft-graph",
      "author": "modelcontextprotocol",
      "created_utc": "2025-12-31 12:00:09",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 0.83,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "server",
      "permalink": "https://reddit.com/r/mcp/comments/1q0befz/forit_microsoft_graph_provides_direct_access_to/",
      "domain": "glama.ai",
      "is_self": false,
      "comments": [
        {
          "id": "nwwglvt",
          "author": "modelcontextprotocol",
          "text": "This server has 7 tools:\n\n- [graph-request](https://glama.ai/mcp/servers/@ForITLLC/forit-microsoft-graph/tools/graph-request) â€“ Execute Microsoft Graph API requests to access Microsoft 365 services. Target specific accounts without switching and use any Graph endpoint with query parameters.\n- [list-accounts](https://glama.ai/mcp/servers/@ForITLLC/forit-microsoft-graph/tools/list-accounts) â€“ Retrieve all available Microsoft accounts for managing Microsoft 365 services across multiple tenants.\n- [login](https://glama.ai/mcp/servers/@ForITLLC/forit-microsoft-graph/tools/login) â€“ Authenticate with Microsoft Graph API using device code flow to access Microsoft 365 services across multiple tenants. Manage multi-tenant accounts through a single flexible interface.\n- [logout](https://glama.ai/mcp/servers/@ForITLLC/forit-microsoft-graph/tools/logout) â€“ Terminate active Microsoft account sessions to end access to Microsoft 365 services and protect account security.\n- [remove-account](https://glama.ai/mcp/servers/@ForITLLC/forit-microsoft-graph/tools/remove-account) â€“ Remove cached Microsoft accounts from the ForIT Microsoft Graph server to manage multi-tenant access and maintain account security.\n- [select-account](https://glama.ai/mcp/servers/@ForITLLC/forit-microsoft-graph/tools/select-account) â€“ Choose a specific Microsoft account to access Microsoft Graph API services across multiple tenants, enabling interaction with Microsoft 365 resources.\n- [verify-login](https://glama.ai/mcp/servers/@ForITLLC/forit-microsoft-graph/tools/verify-login) â€“ Check current Microsoft authentication status to verify login validity before accessing Microsoft 365 services across multiple tenants.",
          "score": 3,
          "created_utc": "2025-12-31 12:00:09",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1q3gb6e",
      "title": "Pattern Iâ€™m seeing in ChatGPT MCP apps: tool descriptions behaving like system prompts",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1q3gb6e/pattern_im_seeing_in_chatgpt_mcp_apps_tool/",
      "author": "Trevadir",
      "created_utc": "2026-01-04 04:26:22",
      "score": 7,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "In ChatGPT Apps that are powered by MCP (not MCP in general), Iâ€™m seeing a pattern where MCP Tool descriptions are more like system prompts and a lot of workflow semantics that are crucial for SaaS apps seem to be getting baked in to the tool description.\n\nDocumented a few interesting ones here \\~Â [https://blog.koundinya.xyz/posts/b2b-chatgpt-apps-mcp-patterns/](https://blog.koundinya.xyz/posts/b2b-chatgpt-apps-mcp-patterns/)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1q3gb6e/pattern_im_seeing_in_chatgpt_mcp_apps_tool/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "nxklpq0",
          "author": "Successful_Grand_784",
          "text": "Great, resource. Bookmarked it.  I just created my first app in developer mode on ChatGPT -and have been analyzing the live apps and tool descriptors.",
          "score": 1,
          "created_utc": "2026-01-04 04:41:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxkm90a",
              "author": "Trevadir",
              "text": "oh nice!! Best of luck with your app! happy to test it if you need a hand",
              "score": 1,
              "created_utc": "2026-01-04 04:45:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxn3uw0",
          "author": "Low-Efficiency-9756",
          "text": "Great breakdown. IMO this pattern is exactly right, and it maps to what Iâ€™ve found building MCP servers from the other side (implementation rather than analysis).\n\nThe key insight: **tool descriptions are the only guaranteed injection point**. System prompts can be overridden, context windows fill up, but tool descriptions are read fresh every time the model considers calling that tool.\n\nYour pattern catalog maps cleanly to what Iâ€™ve implemented:\n\n|Your Pattern                |Implementation Reality                                                                                                                       |\n|----------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|\n|**Prerequisite Enforcement**|I call these â€œcomposite toolsâ€ â€” instead of hoping the LLM chains correctly, bundle the workflow into one tool that does steps 1-3 internally|\n|**Tiered Fallback**         |Fuzzy validation with ranked suggestions â€” donâ€™t just reject bad input, return â€œdid you mean X?â€                                             |\n|**Context Injection**       |â€œDynamic loaderâ€ pattern â€” one tool that loads only the context needed for the current operation                                             |\n|**Negative Examples**       |Essential for anti-hallucination â€” â€œthis tool does NOT create characters, use create_character for thatâ€                                     |\n\nThe Salesforce 500-word description is the brute force approach. It works, but thereâ€™s an architectural alternative: **move workflow logic into the tool itself**. Instead of telling the LLM â€œyou MUST query both VoiceCall AND VideoCall first,â€ make that the toolâ€™s internal behavior. The description becomes: â€œSummarizes conversation transcripts. Handles both voice and video calls automatically.â€\n\nSame outcome, fewer tokens, less reliance on LLM compliance.\n\nRelated reading if youâ€™re going deeper:\n\n- [Six Patterns for Connecting LLM Agents to Stateful Tools](https://mnehmos.github.io/Mnehmos/blog/stateful-mcp-architecture) â€” covers the implementation side\n- [The Scalpel, Not the Hammer](https://mnehmos.github.io/Mnehmos/blog/scalpel-not-hammer) â€” scope constraints as token optimization\n\nThe â€œHTTP spec evolving in real-timeâ€ framing is apt. Weâ€™re watching tool-calling conventions crystallize.",
          "score": 1,
          "created_utc": "2026-01-04 15:50:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxq7xdd",
              "author": "Trevadir",
              "text": "Thank you for sharing! The Scalpel, Not the Hammer was a good read.",
              "score": 1,
              "created_utc": "2026-01-05 00:30:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxgy30",
          "author": "Quiet_Pudding8805",
          "text": "[CartoGopher.com](https://www.CartoGopher.com) (my own mcp) works like this where on the first run it does llm_instructions and displays a super short summary of the tools and steps to use it while simultaneously mapping the codebase",
          "score": 1,
          "created_utc": "2026-01-06 01:41:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pz76u0",
      "title": "I made this MCP server to cover almost any cybersecurity topic in a VPS.",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1pz76u0/i_made_this_mcp_server_to_cover_almost_any/",
      "author": "exitcactus",
      "created_utc": "2025-12-30 03:27:54",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "For the ones into ai stuff, I made this MCP all about cybersecurity when you self host something.\n\nhttps://github.com/girste/mcp-cybersec-watchdog\n\nEdit- if you know what to do, it's literally the same you do everyday, in hours.\n\nIf you know how to make ai (CLI) do it for you, it takes time, tokens and sometimes results \"may vary\"...\n\nWith this, in 5 seconds you have an almost complete checkup of your VPS.\n\nPlanning to expand to SSL certs and more \"corporate\" CS stuff.",
      "is_original_content": false,
      "link_flair_text": "server",
      "permalink": "https://reddit.com/r/mcp/comments/1pz76u0/i_made_this_mcp_server_to_cover_almost_any/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q4prnm",
      "title": "Deco MCP Mesh - OSS runtime gateways for MCP that prevent tool-bloat",
      "subreddit": "mcp",
      "url": "https://v.redd.it/o6edet5yzjbg1",
      "author": "babydecocx",
      "created_utc": "2026-01-05 16:09:20",
      "score": 7,
      "num_comments": 4,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1q4prnm/deco_mcp_mesh_oss_runtime_gateways_for_mcp_that/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxuimjj",
          "author": "ninadpathak",
          "text": "This is what production MCP infrastructure looks like. The tool bloat problem was always going to be real. Smart gateway + binding layer architecture scales way better than 'just load all the servers'. Well executed.",
          "score": 1,
          "created_utc": "2026-01-05 17:09:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxuoey5",
              "author": "babydecocx",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-01-05 17:36:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxvdim9",
          "author": "gadr90",
          "text": "Oh look at that, it's a me!",
          "score": 1,
          "created_utc": "2026-01-05 19:29:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvdy6m",
              "author": "gadr90",
              "text": "BTW my co-founder rations my food based on the number of daily GitHub Stars, please help me not starve ðŸ¥¹ [https://github.com/decocms/mesh](https://github.com/decocms/mesh)",
              "score": 1,
              "created_utc": "2026-01-05 19:31:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q4gspt",
      "title": "RAG Systems Are Crashing at Scale Thanks to 'Semantic Collapse' â€“ Stanford's Wake-Up Call",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/gallery/1q1qqrg",
      "author": "sibraan_",
      "created_utc": "2026-01-05 08:59:21",
      "score": 7,
      "num_comments": 5,
      "upvote_ratio": 0.77,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1q4gspt/rag_systems_are_crashing_at_scale_thanks_to/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nxwr4fb",
          "author": "Competitive-Ad-5081",
          "text": "You have to implement strategies to segment your vectorized data, for example, by using categories and subcategories. Your LLM could first perform a general search and then begin a specific search according to the user's instructions. If you want to answer user requests using semantic search across all your documents at once, the LLM will likely begin to mix information, and its answer will not be accurate Â¯â \\â _â (â ãƒ„â )â _â /â Â¯\n\nThe results depend on your setup configuration:\n[ System Prompt ] + [ RAG tool definition and search strategy ] + [ Vector database scheme ] + [ User's needs ]",
          "score": 3,
          "created_utc": "2026-01-05 23:25:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxuckfb",
          "author": "charlyAtWork2",
          "text": "This is why is a bit usless to have 100% of your documents in the same area to query.\n\nIt's not too hard to make severals small group and query the good one.",
          "score": 2,
          "created_utc": "2026-01-05 16:41:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsynz4",
          "author": "Crafty_Disk_7026",
          "text": "Yup I see this as well. As soon as the data goes from being directly in context to using some form of rag, the quality drops.\n\nFor example I had thousands of recipe data and the direct full context can pick up subtle connections like \"sriracha being mentioned in a sauce\", but not listed as the main recipe ingredient.  However if the same recipes are analyzed using RAG is missed these points.  \n\nChanging chunking behavior or embedding logic does not seem to help with these cases.",
          "score": 1,
          "created_utc": "2026-01-05 12:04:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxuth2f",
              "author": "mfreeze77",
              "text": "Would a directory index help refine the search?",
              "score": 2,
              "created_utc": "2026-01-05 17:59:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxxkjq8",
                  "author": "Crafty_Disk_7026",
                  "text": "That's essentially what rag does, it creates embeddings which it uses to index your data to retrieve it later.  The problem, which this graph shows, is that as the document set grows, the lest comprehensive and accurate the reasoning gets.  This is basically saying our current rag strategies are deficient.   Better strategies perform better with large documents but they all get worse the more docs you hold.\n\nI see this all the time with people building agents and it usually results in a custom fine tuned tag solution for a use case.  So a specific rag for processing text in a textbook versus a different rag for processing text from a novel.  This creates a ton of work as each category of data needs a tailored solution which is not engineering scalable.",
                  "score": 1,
                  "created_utc": "2026-01-06 02:01:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q2thef",
      "title": "Built an MCP that replicates the exact animations from real websites",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1q2thef/built_an_mcp_that_replicates_the_exact_animations/",
      "author": "TrickAd9980",
      "created_utc": "2026-01-03 12:30:43",
      "score": 6,
      "num_comments": 5,
      "upvote_ratio": 0.88,
      "text": "Iâ€™ve been developing an MCP tool that lets agentic coding AI agents pull rich UI context from a single URL. It captures pixel-accurate sizing, spacing, typography, and layout, then generates Tailwind configuration, identifies animation libraries in use, and extracts the exact animations and keyframes needed to recreate the experience. It also produces screenshots for every section and component, plus additional structured metadata to make replication and iteration fast and reliable.\n\nIâ€™m planning to open-source it and I want blunt feedback from people who would actually use it. Iâ€™m especially interested in the hard problems and real-world failure cases you hit when trying to clone or modernize production UIs, and what would make this tool genuinely indispensable. Thanks.\n\nhttps://reddit.com/link/1q2thef/video/i7dl7gdno4bg1/player\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1q2thef/built_an_mcp_that_replicates_the_exact_animations/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "nxhxx69",
          "author": "EveryoneForever",
          "text": "I would. I tried to build something by similar for animations that use web gl but it didnâ€™t quiet work. Would love to give this a try",
          "score": 2,
          "created_utc": "2026-01-03 20:08:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxid2oa",
              "author": "Old-Beginning-8892",
              "text": "Really curious what was ur approach and why it didnâ€™t work? I tried too",
              "score": 1,
              "created_utc": "2026-01-03 21:24:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxq1ils",
          "author": "HisokaMoreau",
          "text": "Sick",
          "score": 2,
          "created_utc": "2026-01-04 23:59:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhk6n5",
          "author": "TrickAd9980",
          "text": "Really curios u might find this helpfull:)",
          "score": 1,
          "created_utc": "2026-01-03 19:03:45",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nxw0ak8",
          "author": "WriterSeveral7904",
          "text": "id uset it for sure",
          "score": 1,
          "created_utc": "2026-01-05 21:15:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyo6v0",
      "title": "A look at Gemini Function Calling architecture and connecting it to an MCP Tool Router",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1pyo6v0/a_look_at_gemini_function_calling_architecture/",
      "author": "cyber_harsh",
      "created_utc": "2025-12-29 14:51:23",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hey everyone ðŸ‘‹,\n\nIâ€™ve been experimenting with the new Gemini SDKs and Googleâ€™s Agent Development Kit (ADK) recently. Particularly Function Calling differences.  \n  \nI wanted to break down exactly how the function calling loop works under the hood and share a workflow for connecting it to a Model Context Protocol (MCP) Tool Router.\n\nI figured this might be useful for anyone trying to bridge Geminiâ€™s native tooling with MCP servers, so sharing here.\n\n\n\n**The Core Loop: How Gemini handles tools (earlier)**\n\nIf you're coming from other ecosystems, the Gemini loop is standard but strict on data structures. It essentially works like this:\n\n1. ***Declaration*****:** You pass `FunctionDeclaration` objects (*OpenAPI*\\-like schemas) in the `GenerateContentConfig`.\n2. **The Prompt:** Gemini ingests the prompt + tool schemas.\n3. **The Stop:** Instead of text, the model halts and returns a `functionCall` object in `content.parts`.\n4. **Execution (The Missing Link):** The generic SDK *doesn't* execute code for you. You have to parse the `functionCall`, execute your local code/API, and wrap the result.\n5. **The Return:** You send a new message with `role=\"tool\"` and the structured result.\n\nThe interesting part (and often the pain point) is handling the routing logic when you have dozens of tools.\n\n**Integrating an MCP Router (new flow)**\n\nWriting manual handlers for every tool gets messy fast. This is where using an MCP (Model Context Protocol) Tool Router helps abstract the execution layer.\n\nHere is a pattern for using Google ADK with an MCP Toolset. \n\n    from google.adk.agents.llm_agent import Agent\n    from google.adk.tools.mcp_tool.mcp_toolset import McpToolset\n    from google.adk.tools.mcp_tool.mcp_session_manager import StreamableHTTPConnectionParams\n    \n    # ... (Standard Auth Setup) ...\n    \n    # 1. Create the MCP Session\n    # In this example, we use a router that aggregates search tools\n    composio_session = composio_client.experimental.tool_router.create_session(\n        user_id=COMPOSIO_USER_ID,\n        toolkits=[\"duckduckgo\", \"google_search\"] \n    )\n    \n    # 2. Connect ADK to the MCP URL\n    composio_mcp_url = composio_session.url\n    composio_toolset = McpToolset(\n        connection_params=StreamableHTTPConnectionParams(url=composio_mcp_url)\n    )\n    \n    # 3. Inject into the Agent\n    root_agent = Agent(\n        model=\"gemini-2.5-pro\",\n        name=\"research_agent\",\n        # Instruction is critical for ADK to trust the tool output\n        instruction=(\n            \"You are a helpful assistant. \"\n            \"Use the available tools to answer user queries. \"\n            \"Execute the tools directly.\"\n        ),\n        tools=[composio_toolset], # <--- The MCP Toolset\n    )\n    \n\nThis example uses a router (via Composio in this instance, but the logic applies to other   \nMCP clients) to let the agent search/plan across available tools dynamically.\n\n**Why these matters?**\n\nUsing the `McpToolset` class in Google ADK allows the agent to treat the MCP server as a unified tool interface. \n\nYou don't need to manually define the schema for every single search function; the MCP handshake handles the capability discovery.\n\n  \n**Common Pitfalls**\n\nIf you are building this out, here are a few things that tripped me up:\n\n* **Schema Strictness:** Gemini is very sensitive to parameter types. If your MCP server returns a loose schema, Gemini might hallucinate arguments. Use Enums where possible.\n* **Error Propagation:** If the MCP tool fails, *return the error as a string* in the function response. Don't crash the app. Gemini can often read the error message (\"Error: 404 not found\") and self-correct or ask the user for clarification.\n* **Routing Latency:** Passing everything through a router adds a hop. For critical, low-latency tools, native function calling might still be faster than MCP.\n\nIf you want to grab the full source code for the agent setup or see a more detailed walkthrough of the environment configuration, I wrote up a guide here: [Function Calling & MCP using Google ADK](https://medium.com/composiohq/function-calling-mcp-using-google-adk-bf861421638e?postPublishedType=repub).\n\nHas anyone else here played with Google ADK's `McpToolset` yet? I'm curious how it compares to using the standard `sse`transport with other clients like Claude Desktop.\n\n",
      "is_original_content": false,
      "link_flair_text": "resource",
      "permalink": "https://reddit.com/r/mcp/comments/1pyo6v0/a_look_at_gemini_function_calling_architecture/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q0af08",
      "title": "Calorie-Tracking â€“ A MCP server that tracks daily calorie intake through natural language interaction, providing meal recording, daily summary, weekly report generation, and food search functions.",
      "subreddit": "mcp",
      "url": "https://glama.ai/mcp/servers/@xiaobenyang-com/Calorie-Tracking",
      "author": "modelcontextprotocol",
      "created_utc": "2025-12-31 11:00:05",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "server",
      "permalink": "https://reddit.com/r/mcp/comments/1q0af08/calorietracking_a_mcp_server_that_tracks_daily/",
      "domain": "glama.ai",
      "is_self": false,
      "comments": [
        {
          "id": "nww9ti6",
          "author": "modelcontextprotocol",
          "text": "This server has 4 tools:\n\n- [add_meal](https://glama.ai/mcp/servers/@xiaobenyang-com/Calorie-Tracking/tools/add_meal) â€“ Log meals with food items and calories to track daily intake. Record descriptions and meal types for accurate calorie monitoring.\n- [get_daily_summary](https://glama.ai/mcp/servers/@xiaobenyang-com/Calorie-Tracking/tools/get_daily_summary) â€“ Retrieve today's calorie intake summary to monitor daily nutritional goals and track progress in your diet plan.\n- [get_weekly_report](https://glama.ai/mcp/servers/@xiaobenyang-com/Calorie-Tracking/tools/get_weekly_report) â€“ Generate a weekly calorie consumption report to track nutritional intake patterns and monitor progress toward health goals.\n- [search_food](https://glama.ai/mcp/servers/@xiaobenyang-com/Calorie-Tracking/tools/search_food) â€“ Find calorie information for specific foods to support dietary tracking and nutrition management.",
          "score": 2,
          "created_utc": "2025-12-31 11:00:05",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0t31y",
      "title": "Google Cloud Docs MCP Server â€“ Enables AI assistants to search and access Google Cloud Platform documentation in real-time, supporting 20+ GCP services with natural language queries and smart content extraction.",
      "subreddit": "mcp",
      "url": "https://glama.ai/mcp/servers/@longngo192/gcpdoc-mcp",
      "author": "modelcontextprotocol",
      "created_utc": "2026-01-01 02:00:06",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "server",
      "permalink": "https://reddit.com/r/mcp/comments/1q0t31y/google_cloud_docs_mcp_server_enables_ai/",
      "domain": "glama.ai",
      "is_self": false,
      "comments": [
        {
          "id": "nx0l98t",
          "author": "modelcontextprotocol",
          "text": "This server has 4 tools:\n\n- [fetch_google_cloud_doc](https://glama.ai/mcp/servers/@longngo192/gcpdoc-mcp/tools/fetch_google_cloud_doc) â€“ Extract content from specific Google Cloud documentation pages by providing the exact path to retrieve detailed technical information in structured format.\n- [get_api_reference](https://glama.ai/mcp/servers/@longngo192/gcpdoc-mcp/tools/get_api_reference) â€“ Retrieve REST API documentation for Google Cloud services to access endpoints, methods, and parameters for integration development.\n- [list_google_cloud_products](https://glama.ai/mcp/servers/@longngo192/gcpdoc-mcp/tools/list_google_cloud_products) â€“ Discover available Google Cloud products and their documentation paths to explore services or find correct product IDs for further research.\n- [search_google_cloud_docs](https://glama.ai/mcp/servers/@longngo192/gcpdoc-mcp/tools/search_google_cloud_docs) â€“ Search Google Cloud documentation to find configuration guides, best practices, and troubleshooting steps for GCP services like Compute Engine, Cloud Storage, BigQuery, and Kubernetes.",
          "score": 2,
          "created_utc": "2026-01-01 02:00:06",
          "is_submitter": true,
          "replies": []
        }
      ]
    }
  ]
}