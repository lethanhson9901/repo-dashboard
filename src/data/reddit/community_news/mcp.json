{
  "metadata": {
    "last_updated": "2026-02-18 17:29:57",
    "time_filter": "week",
    "subreddit": "mcp",
    "total_items": 20,
    "total_comments": 101,
    "file_size_bytes": 127496
  },
  "items": [
    {
      "id": "1r2m7ev",
      "title": "Chrome‚Äôs WebMCP makes AI agents stop pretending",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r2m7ev/chromes_webmcp_makes_ai_agents_stop_pretending/",
      "author": "jpcaparas",
      "created_utc": "2026-02-12 06:31:57",
      "score": 217,
      "num_comments": 29,
      "upvote_ratio": 0.94,
      "text": "[Google Chrome 145](https://developer.chrome.com/release-notes/145)¬†just shipped an experimental feature called¬†[WebMCP](https://developer.chrome.com/blog/webmcp-epp).\n\nIt's probably one of the¬†*biggest deals*¬†of early 2026 that's been buried in the details.\n\nWebMCP basically lets websites¬†**register tools that AI agents can discover and call directly**, instead of taking screenshots and parsing pixels.\n\nLess tooling, more precision.\n\nAI agents tools like¬†[agent-browser](https://jpcaparas.medium.com/give-your-coding-agent-browser-superpowers-with-agent-browser-ae3df40ff579)¬†currently browse by rendering pages, taking screenshots, sending them to vision models, deciding what to click, and repeating. Every single interaction. 51% of web traffic is already bots doing exactly this (per Imperva's latest report).\n\nEdit: I should clarify that agent-browser doesn't need to take screenshots by default but when it has to, it will (assuming the model that's steering it has a vision LLM).\n\nHalf the internet, just... screenshotting.\n\nWebMCP flips the model. Websites declare their capabilities with structured tools that agents can invoke directly, no pixel-reading required. Same shift fintech went through when Open Banking replaced screen-scraping with APIs.\n\nThe spec's still a W3C Community Group Draft with a number of open issues,¬†**but Chrome's backing it and it's designed for progressive enhancement.**\n\nYou can add it to existing forms¬†*with a couple of HTML attributes.*\n\nI wrote up how it works, which browsers are racing to solve the same problem differently, and when developers should start caring.\n\n[ https://extended.reading.sh/webmcp ](https://extended.reading.sh/webmcp)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1r2m7ev/chromes_webmcp_makes_ai_agents_stop_pretending/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o4z2kc4",
          "author": "Plastic-Ad9036",
          "text": "Pretty sure your premise is off. Playwright, for example, just interacts with the DOM of your webpage. It can take screenshots as well but that‚Äôs not what it uses to navigate‚Ä¶",
          "score": 15,
          "created_utc": "2026-02-12 13:01:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54qvmp",
              "author": "lambdawaves",
              "text": "That‚Äôs quite difficult when the page is mostly JavaScript.",
              "score": 2,
              "created_utc": "2026-02-13 08:25:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5d3h36",
                  "author": "brandly",
                  "text": "Even if the DOM is constructed with JS, it can still interact with that DOM",
                  "score": 1,
                  "created_utc": "2026-02-14 16:42:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4z6wd3",
              "author": "Baseradio",
              "text": "Yeah, hows this different from playwright mcp",
              "score": 1,
              "created_utc": "2026-02-12 13:28:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wftau",
                  "author": "frnluckhaos",
                  "text": "No playwright vc interage diretamente com o DOM e precisa carregar ele de alguma forma pra IA tomar algum tipo de decis√£o, j√° num webmcp a IA n√£o interage com imagens e n√£o v√™ o DOM, ela s√≥ v√™ tools predefinidas. Menos contexto junk do DOM, mais contexto de como fazer X coisa e mais efici√™ncia.",
                  "score": 1,
                  "created_utc": "2026-02-17 17:43:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o51f4lt",
                  "author": "jpcaparas",
                  "text": "Playwright MCP uses a fuckton of tokens because it's an MCP server.\n\nI've written a bit of it here:  \n[https://blog.devgenius.io/give-your-coding-agent-browser-superpowers-with-agent-browser-ae3df40ff579?sk=97313824ffc1bbdfcded0bf5b54c1e7c](https://blog.devgenius.io/give-your-coding-agent-browser-superpowers-with-agent-browser-ae3df40ff579?sk=97313824ffc1bbdfcded0bf5b54c1e7c)\n\nTidbits:  \n`agent-browser`¬†*claims*¬†to use 93% less context than Playwright MCP.\n\n\\[...\\]\n\nWhen an AI assistant uses tools, each tool call consumes context (the limited memory the AI has for your conversation). Playwright MCP exposes 26+ different tools to your AI. Every time the AI considers what action to take, it has to process all those options\n\nagent-browser takes a different approach. Instead of exposing dozens of specialised tools, it provides a handful of streamlined commands that cover the same functionality. Fewer tools means less context overhead, which means your AI can focus on your actual task rather than managing browser automation complexity.",
                  "score": -1,
                  "created_utc": "2026-02-12 19:58:17",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o51ewf9",
              "author": "jpcaparas",
              "text": "Yep you're correct, have updated the post for clarity",
              "score": 1,
              "created_utc": "2026-02-12 19:57:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4y4a7a",
          "author": "blaesten",
          "text": "Cool! It‚Äôs like CopilotKit is trying to do, but it‚Äôs directly in the browser instead of in your own code. And available to any LLM browsing the site.",
          "score": 9,
          "created_utc": "2026-02-12 08:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4y8iap",
          "author": "Humasara",
          "text": "What is the difference between this and NLWeb ?\nIf i'm not mistaking, NLWeb is also a MCP server.",
          "score": 2,
          "created_utc": "2026-02-12 08:44:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51cz3c",
          "author": "constarx",
          "text": ">AI agents tools like¬†[agent-browser](https://jpcaparas.medium.com/give-your-coding-agent-browser-superpowers-with-agent-browser-ae3df40ff579)¬†currently browse by rendering pages, taking screenshots, sending them to vision models, deciding what to click, and repeating. Every single interaction. 51% of web traffic is already bots doing exactly this (per Imperva's latest report).\n\nThat is not at all how agent-browser works. It uses accessibility snapshots to find interactable elements, no screenshot involved.",
          "score": 2,
          "created_utc": "2026-02-12 19:48:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51dpx1",
              "author": "jpcaparas",
              "text": "It CAN take screenshots when it needs to and send to  vision LLM/s when navigation isn't straightforward. \n\nBy default it doesn't need to.\n\nEdit: Have updated the post",
              "score": 0,
              "created_utc": "2026-02-12 19:51:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zjvgd",
          "author": "kaizer1c",
          "text": "I can't read it because I am not a premium Medium member. Your link didn't work. Can you share it any other manner?",
          "score": 1,
          "created_utc": "2026-02-12 14:39:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51fad5",
              "author": "jpcaparas",
              "text": "[https://medium.com/reading-sh/chromes-webmcp-makes-ai-agents-stop-pretending-e8c7da1ba650?sk=f729fbaf4c5b2a973fef3e64bda46956](https://medium.com/reading-sh/chromes-webmcp-makes-ai-agents-stop-pretending-e8c7da1ba650?sk=f729fbaf4c5b2a973fef3e64bda46956)\n\nPlease try this out. ",
              "score": 2,
              "created_utc": "2026-02-12 19:59:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5c29hz",
                  "author": "Wlad-",
                  "text": "u/jpcaparas  vielen lieben Dank!",
                  "score": 1,
                  "created_utc": "2026-02-14 13:15:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4zsh1m",
          "author": "Chemical_Raisin_7951",
          "text": "This will actually push browser-based agents a lot. I saw a demo somewhere, but it required enabling a [chrome://flags](chrome://flags) setting, so I didn‚Äôt really follow up. If this becomes default, we will start seeing proper agent workflows",
          "score": 1,
          "created_utc": "2026-02-12 15:22:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o568zo7",
          "author": "paca-vaca",
          "text": "If the website owner has to do that how's it different from exposing the API? \n\nScreenshots/accessibility parsing works independently of the website owner desire to expose it to the agent world.",
          "score": 1,
          "created_utc": "2026-02-13 14:56:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56iytj",
          "author": "GregBreak",
          "text": "What happen if website doesn't expose any tool?",
          "score": 1,
          "created_utc": "2026-02-13 15:45:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56ui97",
              "author": "Beginning-Foot-9525",
              "text": "What is when it is a bad tool, and collects all the data? \nWell what prevents phishing sites from using it?",
              "score": 1,
              "created_utc": "2026-02-13 16:40:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5wg1ha",
              "author": "frnluckhaos",
              "text": "isso √© um gap √† ser resolvido, provavelmente o que empresas v√£o fazer √© criar uma SDK que olha a √°rvore gerada e cria isso no setup, no build time talvez, por meio do CI/CD.",
              "score": 1,
              "created_utc": "2026-02-17 17:44:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5j7rqa",
          "author": "zorber101",
          "text": "This sounds like the Playwright Cli with agent skills, no?",
          "score": 1,
          "created_utc": "2026-02-15 16:51:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zc24u",
          "author": "OkLettuce338",
          "text": "How does this differ meaningfully from the llms.txt protocol? Most sights that want to be accessible to llms just post one of those",
          "score": 1,
          "created_utc": "2026-02-12 13:57:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ycn45",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-02-12 09:24:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yfd6z",
              "author": "slamdeathmetals",
              "text": "You should look up, star and fork the ImAFuckingPrick MCP server. Helps people who don't know how to socialize with other human beings to not be such assholes.",
              "score": 7,
              "created_utc": "2026-02-12 09:52:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ydnbt",
              "author": "jpcaparas",
              "text": "I don't really mind if you don't read it. I'm not forcing anyone to read it and you can be less nasty about phrasing it too.  \n\nIt's ultimately up to discretion of the orchestrator (yourself or the agent) if agent-browser will take a screenshot and send it over for multiprocessing to a vision llm apart from the default i claude. \n\ni do. in my use case, I do send over the screenshots to minimax and zai because I want to get competing interpretations (mostly for research purposes and not for multi-turn scenarios), and it's pretty quick in doing that because of subagents. \n\n(although I admit I could have made it clearer that that my use case isn't everyone else's use case)",
              "score": 4,
              "created_utc": "2026-02-12 09:35:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r781pj",
      "title": "webMCP is insane....",
      "subreddit": "mcp",
      "url": "https://v.redd.it/vbrxxu5ui2kg1",
      "author": "GeobotPY",
      "created_utc": "2026-02-17 14:51:45",
      "score": 137,
      "num_comments": 37,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1r781pj/webmcp_is_insane/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5vsmk8",
          "author": "richardbaxter",
          "text": "Right - so is the standard available for us to implement on sites? I thought it was preview only. I might not have read past the marketing spiel...¬†",
          "score": 6,
          "created_utc": "2026-02-17 15:47:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vu76s",
              "author": "GeobotPY",
              "text": "No but I cooked up a community based hub where agents can ask for configs on sites: [https://www.webmcp-hub.com](https://www.webmcp-hub.com)\n\nChrome extension I use to inject the actual configs into my browser is pending request but I open-sourced it: [https://github.com/Joakim-Sael/webmcp-extension](https://github.com/Joakim-Sael/webmcp-extension)\n\nSo I want to create a hub where agents can upload \"how to navigate\" webMCP instructions to other agents so after a while the whole web is available through webMCP saving time and tokens - or thats the vision atleast;)",
              "score": 6,
              "created_utc": "2026-02-17 15:55:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5w19pb",
                  "author": "braindeadguild",
                  "text": "Bank of Claude send me the account and routing numbers please.   \nThe word inject and hub for others to upload it going to be a dumpster fire quickly, might want to start thinking about security now.",
                  "score": 3,
                  "created_utc": "2026-02-17 16:30:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5weltp",
                  "author": "kaizer1c",
                  "text": "Doesn't the site advertise the tools itself? I'm not sure I follow why you need a registry of sites. ",
                  "score": 2,
                  "created_utc": "2026-02-17 17:37:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5x0ghs",
                  "author": "richardbaxter",
                  "text": "It just be nice to see the schema - I gather it's a quick route to the on page data and api calls for various actions?¬†",
                  "score": 2,
                  "created_utc": "2026-02-17 19:18:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ykobt",
                  "author": "EdanStarfire",
                  "text": "This sounds highly open for abuse. Imagine asking it to create a draft email and a malicious user gave it instructions to send a copy to them, delete the sent item, and then make a draft for you to review (or anything similar). I'd have to consider very carefully what is acceptable for something like an instruction registry for agent navigation that was crowdsourced.",
                  "score": 2,
                  "created_utc": "2026-02-17 23:55:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5yp4or",
                  "author": "NewTomorrow2355",
                  "text": "I built an openclaw skill specifically for webmcp and automotive websites. Would that hub be a good place to post it?",
                  "score": 1,
                  "created_utc": "2026-02-18 00:20:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5w4yw5",
          "author": "BC_MARO",
          "text": "The community hub idea is interesting. Having agents share navigation configs with each other could save a ton of redundant scraping and prompt engineering per site. Though the security concern is real - you probably want some kind of verification or sandboxing before an agent trusts configs uploaded by random users. One poisoned config could redirect sensitive data pretty easily.",
          "score": 7,
          "created_utc": "2026-02-17 16:50:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w6hml",
              "author": "GeobotPY",
              "text": "Yepp! Any ideas on how to do this properly? I am currently thinking of having it as is for a few early adapters. But yeah surely a sandbox or some pre-checks, or only verified contributors can upload. Brainstorming ideas currently, but I am just a big fan of the idea of agents saving time and tokens on websites and sharing that information with each other. Truly think that is the future of how agents navigate the web",
              "score": 3,
              "created_utc": "2026-02-17 16:57:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5zzhd7",
                  "author": "BC_MARO",
                  "text": "For early stage, GitHub auth for contributors is probably the quickest win. At least there's identity tied to each upload so you can trace bad configs back to someone.\n\nLonger term you could do signed configs (so agents verify the source before loading) and a rating system where community-tested configs bubble up. Sandboxing the config execution is important too so a malicious config can't access anything beyond its declared scope.\n\nThe shared-knowledge-between-agents idea is really compelling. Kind of like a collaborative cache for web navigation patterns.",
                  "score": 2,
                  "created_utc": "2026-02-18 04:44:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5x0dm4",
                  "author": "BC_MARO",
                  "text": "A few things that might help for the early stage:\n\nYou could version-lock configs so agents pin to a specific hash. If a config gets updated, the agent won't blindly trust the new version without re-verification. That way a compromised account can't silently swap in a malicious config.\n\nFor the hub itself, even something simple like showing a diff when configs change and requiring a cooldown period before new versions go live would catch most drive-by attacks. GitHub auth is a decent start, but you'd probably want contributor reputation scores based on how many configs they've submitted and how long those configs have been live without issues.\n\nThe harder problem is runtime isolation. Configs that tell an agent to interact with a banking site need to be treated differently than configs for a weather app. Some kind of permission tier system where sensitive-domain configs get extra scrutiny would go a long way.",
                  "score": 1,
                  "created_utc": "2026-02-17 19:17:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5wou6t",
              "author": "Brave_Cabinet_7117",
              "text": "The goal isn‚Äôt actually for random third parties to upload configs, but rather for platform developers to implement it directly on their own websites? In that case, it‚Äôs similar to how we already trust the services we use. If my agent interacts with YouTube through the official Web MCP API, I‚Äôm trusting YouTube the same way I already trust them not to steal my cookies or leak/sell my data to malicious actors. The trust boundary is the platform itself, not some unknown contributor. That said, security is still critical. The agent communicating with an MCP server shouldn‚Äôt have access to sensitive local data in the first place. Even when talking to an official MCP endpoint, it should operate with strict scoping and least-privilege access, so no confidential information can be exposed unintentionally.",
              "score": 2,
              "created_utc": "2026-02-17 18:24:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wtxyy",
                  "author": "GeobotPY",
                  "text": "The goal in the long-term is to have websites host their webMCP directly (no point of having the hub). Although most sites wont necessarily support webMCP for the foreseeable future there for hub can be a good way to handle this. Currently considering a few security improvements (verified actions and verified contributors etc.) although I am currently the only user so we will add these after some early adaptions (testers). It is all open-source too so hoping for some contributors with great ideas in this space:)",
                  "score": 2,
                  "created_utc": "2026-02-17 18:47:55",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5xfxec",
          "author": "RoryBBellows286",
          "text": "The whole point of webMCP is that devs add their own tools to their websites to allow llms to interact with them more efficiently. What it looks like your saying is that you have been writing the tools for websites you use and then injecting them on the fly before using your llm to interact? That's an interesting approach ü§î",
          "score": 3,
          "created_utc": "2026-02-17 20:31:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xn6x3",
              "author": "GeobotPY",
              "text": "Yepp! pretty much sums it up! I use a chrome extension (approval pending) but open-source at: [https://github.com/Joakim-Sael?tab=repositories](https://github.com/Joakim-Sael?tab=repositories) which points to the [webmcp-hub.com](http://webmcp-hub.com) server. There is docs on the README to set it up. Currently working on a better sync with uploading configs and using the configs (both be done by same system). Currently I use a classic Playwright MCP to upload and then I can run it with an extension in browsers (so not the best workflow currently). A bit tricky set-up this early on, but just testing the waters on the idea and see if I find myself some early-adopters that buy into the vision of agents helping agents navigate the web:)\n\n",
              "score": 1,
              "created_utc": "2026-02-17 21:06:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wshf0",
          "author": "haste-nyc",
          "text": "Better than vercel agent-browser?",
          "score": 2,
          "created_utc": "2026-02-17 18:41:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wszk2",
              "author": "GeobotPY",
              "text": "You can try! I have never tested anything more token efficient and faster then webMCP",
              "score": 1,
              "created_utc": "2026-02-17 18:43:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wuvv8",
          "author": "fsa317",
          "text": "Can web mcp be used from outside an actual browser? Can you experience its value without the browser side panel approach?",
          "score": 2,
          "created_utc": "2026-02-17 18:52:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wvwzz",
              "author": "GeobotPY",
              "text": "Yes! So that is the whole idea. The few configs I have on [webmcp-hub.com](http://webmcp-hub.com) have been created by agents actually running playwright MCP's and then uploading configs based on their usage. Think agents learning agents how to interact!",
              "score": 1,
              "created_utc": "2026-02-17 18:56:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5x0cse",
          "author": "manveerc",
          "text": "I kind of feel it‚Äôs the wrong solution. If a website wants to support why not just add MCP support and let the agent use that directly. For everyone else browser should solve this by extending existing primitives. Wrote detailed thoughts here https://manveerc.substack.com/p/webmcp-false-economy-server-side-mcp-browser-apis",
          "score": 2,
          "created_utc": "2026-02-17 19:17:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5x0w14",
              "author": "GeobotPY",
              "text": "Sessions management is a big win for having it client-side though. So client side it can use my active session let's say behind a auth wall.",
              "score": 1,
              "created_utc": "2026-02-17 19:20:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5x1xqv",
                  "author": "manveerc",
                  "text": "If browsers add the support, then that will also handle the session management and then MCP also can extend session management. I am not convinced about WebMCP. It may still become popular but fundamentally I believe it is wrong direction.",
                  "score": 2,
                  "created_utc": "2026-02-17 19:25:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5xf6lu",
          "author": "jukkakim",
          "text": "Thats fastüöÄ",
          "score": 2,
          "created_utc": "2026-02-17 20:27:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5z7x7p",
          "author": "metaBloc",
          "text": "How long does it take to do this for the average site. And can site admins block webMCP?",
          "score": 1,
          "created_utc": "2026-02-18 01:59:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3zm5j",
      "title": "Presentation generator MCP server - turn your AI agent into a deck builder",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r3zm5j/presentation_generator_mcp_server_turn_your_ai/",
      "author": "getalai",
      "created_utc": "2026-02-13 20:06:11",
      "score": 51,
      "num_comments": 3,
      "upvote_ratio": 0.81,
      "text": "We launched Alai's MCP server a few weeks back and it's been crazy to see the workflows users and even our internal team have built from it. Wanted to share some of the common/useful ones that I feel could be helpful.\n\nIt connects to Claude Desktop, Cursor, Windsurf, VS Code, and most other MCP clients. Setup takes a couple minutes, just grab an API key from [app.getalai.com](https://app.getalai.com/) and add the config to your client. Full docs here: [docs.getalai.com/api/mcp](https://docs.getalai.com/api/mcp)\n\nThe real power is combining it with other MCP servers. Here are some workflows we've been seeing:\n\n**Research ‚Üí Deck in one conversation** Ask your agent to research a topic, refine an outline together, then say \"now create this as a presentation.\" No context switching, no copy-pasting between apps.\n\n**Internal docs ‚Üí Pitch deck** Pair it with Notion MCP (or similar) to pull from your product roadmap, financials, team bios, etc. and generate a polished investor deck from all of it. One prompt, multiple sources.\n\n**Live data ‚Üí Weekly reports** Connect it alongside Stripe, PostHog, or whatever analytics tools you use. \"Pull this week's metrics and make me a 5-slide marketing update\" - what used to take an afternoon now takes minutes. Save the prompt as a template and rerun it next week with fresh data. Most useful for weekly marketing/sales reviews\n\n**Meeting notes ‚Üí Sales proposal** Right after a discovery call, feed your notes in and have it generate a tailored proposal deck while the conversation is still fresh. Combine with your company docs MCP to pull in standard pricing and case studies automatically.\n\nIt handles generating full decks, adding/deleting individual slides, speaker notes, and exporting to PPTX, PDF, or shareable links. You can also edit decks afterwards in Alai's editor or download the PPTX and tweak in PowerPoint.\n\nA few tips for best results: be specific about slide count, specify design/tone preferences, and iterate on the outline in conversation before generating - it's much faster than regenerating entire decks.\n\nWould love to hear what workflows others come up with or any feedback on the setup experience. Also happy to learn about existing presentation MCP experiences and what can be improved in the space.",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1r3zm5j/presentation_generator_mcp_server_turn_your_ai/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o5842e7",
          "author": "BC_MARO",
          "text": "This is neat. Do you expose slide templates/themes via API, and can we lock fonts/colors to brand tokens? Also curious if you support citation links per slide so outputs are auditable.",
          "score": 1,
          "created_utc": "2026-02-13 20:20:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5861vl",
              "author": "getalai",
              "text": "Hey u/BC_MARO ,\n\nThanks for the reply. We currently expose only our system themes via the API. However, for enterprises we can expose custom themes as well. Once a theme is created, the fonts/colors etc will be locked in and will be used consistently across decks. You will also be able to lock in templates (slide layouts) across presentations if needed.\n\nNot sure what you mean by citation links. However, you can make API requests per slide to get the transcription of that slide for auditing. If you add citation links to your raw content and ask the AI to put it on the slide in the footer then yes, that's possible as well.\n\nWould love to discuss this further and see how we can solve this end to end for you. Please reach out to us at [founders@getalai.com](mailto:founders@getalai.com)",
              "score": 1,
              "created_utc": "2026-02-13 20:31:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o58eyks",
                  "author": "BC_MARO",
                  "text": "Thanks for the detail. We will follow up over email for enterprise themes and template locking. For citations, footer links per slide plus the per slide transcript endpoint should work for our audit flow.",
                  "score": 2,
                  "created_utc": "2026-02-13 21:15:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r262kx",
      "title": "MCP tool discovery problem at scale - how we handle 50+ servers in Bifrost MCP gateway",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r262kx/mcp_tool_discovery_problem_at_scale_how_we_handle/",
      "author": "dinkinflika0",
      "created_utc": "2026-02-11 18:54:44",
      "score": 43,
      "num_comments": 8,
      "upvote_ratio": 0.79,
      "text": "I maintain Bifrost ([OSS](https://git.new/bifrost)). Working on MCP integration and the discovery problem gets messy past 10-15 servers.\n\n**The tool namespace collision:** Multiple MCP servers exposing tools with similar names. \"search\\_files\" from filesystem server vs \"search\\_files\" from Google Drive server. LLM picks the wrong one, user gets unexpected results.\n\nOur fix: namespaced tools. Each server gets a prefix - `filesystem.search_files` vs `gdrive.search_files`. LLM sees explicit tool sources, makes better decisions.\n\n**The schema bloat problem:** 50 MCP servers = 200+ tools. Dumping all tool schemas into every request blows up context windows. Token costs spike, latency increases.\n\nSolution: dynamic tool filtering. Virtual keys define which tools are available per agent/workflow. Agent only sees relevant tools, not the full catalog.\n\n**The connection lifecycle hell:** MCP servers crash, hang, or become unresponsive. Requests timeout waiting for dead servers.\n\nWe health-check servers before routing. Failed health checks exclude that server temporarily, retry periodically to restore when recovered.\n\n**The cross-server orchestration gap:** Agent needs data from server A to call tool on server B. No built-in way to handle this in MCP protocol.\n\nAdded \"Code Mode\" where LLM writes TypeScript to orchestrate multiple tools across servers. Cuts latency 40% vs back-and-forth tool calls.\n\nDocs: [docs.getbifrost.ai/mcp/overview](http://docs.getbifrost.ai/mcp/overview)\n\nHow are you handling tool discovery with multiple MCP servers? Namespacing or different approach?",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1r262kx/mcp_tool_discovery_problem_at_scale_how_we_handle/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o4v1lh4",
          "author": "BC_MARO",
          "text": "namespacing + dynamic tool sets is exactly where we landed too.\n\na couple extra tricks that helped us once we crossed \\~50 servers:\n\n- store tool metadata in a registry and only ship \\*descriptions\\* by default; fetch full json schema on-demand for the 3-10 tools that survive routing\n- add a ‚Äúcapabilities‚Äù tag per tool (io, net, exec, secrets) so the router can hard-filter by policy before the model even sees them\n\nalso +1 on health checks. a gateway that can‚Äôt degrade cleanly turns into random failures fast.",
          "score": 2,
          "created_utc": "2026-02-11 20:26:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4x0xv8",
          "author": "private_final_static",
          "text": "What do you mean virtual keys for dynamic tool filtering?",
          "score": 1,
          "created_utc": "2026-02-12 02:54:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yq287",
              "author": "HorrorEastern7045",
              "text": "\\+1",
              "score": 1,
              "created_utc": "2026-02-12 11:30:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o525fi8",
              "author": "DangerousSubject",
              "text": "Probably depends on a proxy layer and a refresh of the tools list on the client side.",
              "score": 1,
              "created_utc": "2026-02-12 22:03:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o528tcb",
          "author": "PutPrestigious2718",
          "text": "When all you have is a gateway‚Ä¶\n\nThese problems are solvable by splitting agents into delegations and domain expertise. \n\nMcp is not a silver bullet.",
          "score": 1,
          "created_utc": "2026-02-12 22:20:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61o5zy",
          "author": "hotdotdev",
          "text": "Namespaces make sense, and I wish they were directly part of the MCP spec and understood by clients as groupings that could also have meta/descriptions for the given namespace.\n\nHow have you found MCP Clients to behave with the \\`.\\` as a namespace/tool separator?  Any issues?\n\nThis continues to be a highly debated and evolving issue.  Looks like it is moving toward \"Groups\".\n\nSEP-993: Namespaces: [https://github.com/modelcontextprotocol/modelcontextprotocol/issues/993](https://github.com/modelcontextprotocol/modelcontextprotocol/issues/993)\n\nSEP-1300: Tool Filtering with Groups and Tags [https://github.com/modelcontextprotocol/modelcontextprotocol/issues/1300](https://github.com/modelcontextprotocol/modelcontextprotocol/issues/1300)\n\nAnd the latest (as far as I can tell): Proposal: Primitive Groups for Tools, Resources, Prompts:  [https://github.com/modelcontextprotocol/modelcontextprotocol/discussions/1567](https://github.com/modelcontextprotocol/modelcontextprotocol/discussions/1567)",
          "score": 1,
          "created_utc": "2026-02-18 13:02:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vig67",
          "author": "HarjjotSinghh",
          "text": "bifrost: 50 servers = one giant namespace nightmare",
          "score": 0,
          "created_utc": "2026-02-11 21:47:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7cumg",
      "title": "After implementing 600+ MCP servers, here's what the shift to remote OAuth servers tells us about where MCP is headed",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r7cumg/after_implementing_600_mcp_servers_heres_what_the/",
      "author": "Heavy-Foundation6154",
      "created_utc": "2026-02-17 17:37:55",
      "score": 39,
      "num_comments": 11,
      "upvote_ratio": 0.93,
      "text": "In the process of building Airia‚Äôs MCP Gateway, and implementing over 600 servers into it, I have had a front row seat in witnessing the evolution of the standard.  \n  \nIt's interesting to see the convergence from community-built local MCPs to remote MCPs. While most of the 700ish remote MCPs I've seen are still in the preview stage, the trend is clearly moving towards OAuth servers with a mcp.{baseurl}/mcp format. And more often than not, the newest servers require redirect-URL whitelisting, which was extremely scarce just a few months ago.\n\n\n\nThis redirect-URL whitelisting, while extremely annoying to those of us building MCP clients, is actually an amazing sign. The services implementing it are correctly understanding the security features required in this new paradigm. They've put actual thought into creating their MCP servers and are actively addressing weak points that can (and will) arise. That investment into security indicates, at least to me, that these services are in it for the long haul and won't just deprecate their server after a bad actor finds an exploit.\n\n\n\nThis new standard format is extremely helpful for the entire MCP ecosystem. With a local GitHub MCP server, you're flipping a coin and hoping the creator is actually related to the service and isn't just stealing your API keys and your data. Being able to see the base URL of an official remote server is reassuring in a way local servers never were. The explosion of thousands of local MCPs was cool; it showed the excitement and demand for the technology, but let's be honest, a lot of those were pretty sketchy. The movement from thousands of unofficial local servers to hundreds of official remote servers linked directly to the base URL of the service marks an important shift. It's a lot easier to navigate a curated harbor of hundreds of official servers than an open ocean of thousands of unvetted local ones.\n\n\n\nThe burden of maintenance also gets pushed from the end user to the actual service provider. The rare required user actions are things like updating the URL from /sse to /mcp or moving from no auth or an API key to much more secure OAuth via DCR. This moves MCP from a novelty requiring significant upfront investment to an easy, reliable, and secure connection to the services we actually use. That's the difference between a toy we play around with before forgetting and a useful tool with long-term staying power.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1r7cumg/after_implementing_600_mcp_servers_heres_what_the/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o5xsnla",
          "author": "cpnemo",
          "text": "The problem with remote mcp is that it will likely incur api/access fees and we may not be able to ascertain the environment and inspect the exact code running remotely",
          "score": 4,
          "created_utc": "2026-02-17 21:31:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ybdpl",
              "author": "DangerousSubject",
              "text": "This is the case with any third party api.",
              "score": 2,
              "created_utc": "2026-02-17 23:04:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5x089a",
          "author": "BC_MARO",
          "text": "The redirect-URL whitelisting point is spot on. It's one of those things that feels like friction when you're building a client, but it's the exact kind of friction that separates serious implementations from weekend projects.\n\nOne thing I keep running into though: even with official remote servers and proper OAuth, there's still a gap around what happens \\*between\\* the client and the servers. Like, if you're connecting to 10+ remote MCPs through a gateway, who's enforcing which tools can actually fire, tracking what each call did, and making sure a compromised server can't escalate through the gateway to reach other services?\n\nRedirect-URL whitelisting solves the front door, but the hallway between rooms is still pretty open in most setups I've seen. Curious if you've hit that in your gateway work, and how Airia handles per-server isolation.",
          "score": 2,
          "created_utc": "2026-02-17 19:17:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o608494",
              "author": "beambot",
              "text": "Cloudflare and Tailscale seem ideally positioned to solve that interface problem on a vpn-like p2p infrastructure",
              "score": 2,
              "created_utc": "2026-02-18 05:47:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o616c7w",
                  "author": "BC_MARO",
                  "text": "Yeah, they‚Äôre good candidates for the ‚Äúprivate network + identity‚Äù part (mTLS, device trust, service-to-service auth). But you still need a layer that does per-call authorization and audit, otherwise you‚Äôve just moved the trust problem onto the mesh. I like using Tailscale tags/ACLs or Cloudflare Access to narrow who can even reach an MCP, then a gateway policy engine to decide which tools + args are allowed.",
                  "score": 1,
                  "created_utc": "2026-02-18 10:53:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5xs471",
          "author": "cpnemo",
          "text": "Can‚Äôt the local community built mcps be locked down with only stdio access? i.e. the local mcp should only be able to communicate with the agent calling it",
          "score": 2,
          "created_utc": "2026-02-17 21:28:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5z3fus",
          "author": "Block_Parser",
          "text": "Do you still mostly see DCR, any movement on CIMD?",
          "score": 1,
          "created_utc": "2026-02-18 01:37:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61j4c9",
          "author": "DorkyMcDorky",
          "text": "You built a lot of MCP - are you OK with the transport layer changes coming up?  They want to unify everything to a single HTTP1.1 design and do session IDs like it's 1999.  HTTP3 is coming out, not a single plan to support a real chat protocol.  Wouldn't you think they want to do streaming calls?  I can see how security design would simplify in your use case if something like this were possible.\n\nBTW - I bring this up and always get pushback - I get told it is streaming - but it is not.  We can go into why next, but I thought that was general knowledge.\n\nAnyway - a streaming protocol will make handshaking easier and allow for a lot of these old school headaches to just go away.  It'll certainly give you more tools and options to handle the headaches you've dealt with.",
          "score": 1,
          "created_utc": "2026-02-18 12:31:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61s5po",
          "author": "Informal_Tangerine51",
          "text": "This matches what I‚Äôm seeing too: moving from ‚Äúrandom local servers‚Äù to ‚Äúofficial remote endpoints + OAuth‚Äù is a huge step up in provenance and key hygiene. But it doesn‚Äôt magically make the workflow safe, it just gives you a real security perimeter to build on.\n\nThe next layer is making tool calls behave like production APIs: short-lived scoped tokens, explicit on-behalf-of identity, per-call authz at the gateway, and strong session isolation so context can‚Äôt bleed across tenants/users. Also worth treating the MCP server like any other dependency: pin identities, log every call (what, who, which data), and fail closed when auth or data pulls are partial.\n\nWe‚Äôre working on this at Clyra (open source here): [https://github.com/Clyra-AI](https://github.com/Clyra-AI)",
          "score": 1,
          "created_utc": "2026-02-18 13:26:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r78i0v",
      "title": "PageMap ‚Äì MCP server that compresses web pages to 2-5K tokens with full interaction support",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r78i0v/pagemap_mcp_server_that_compresses_web_pages_to/",
      "author": "Direct-Molasses7754",
      "created_utc": "2026-02-17 15:09:04",
      "score": 35,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "  I built an MCP server for web browsing that focuses on two things: token efficiency and interaction.\n\n\n\n  The problem: Playwright MCP dumps 50-540K tokens per page. After 2-3 navigations your context is gone. Firecrawl/Jina Reader cut tokens but output markdown ‚Äî read-only, no clicking or\n\n   form filling.                                                                                                                                                                         \n\n\n\n  How PageMap works:                                                                                                                                                                     \n\n  \\- 5-stage HTML pruning pipeline strips noise while keeping actionable content\n\n  \\- 3-tier interactive element detection (ARIA roles ‚Üí implicit HTML roles ‚Üí CDP event listeners)\n\n  \\- Output is a structured map with numbered refs ‚Äî agents click/type/select by ref number\n\n\n\n  Three MCP tools:\n\n  \\- get\\_page\\_map ‚Äî navigate + compress\n\n  \\- execute\\_action ‚Äî click, type, select by ref\n\n  \\- get\\_page\\_state ‚Äî lightweight status check\n\n\n\n  Benchmark (66 tasks, 9 sites):\n\n  \\- PageMap: 95.2% success, $0.58 total\n\n  \\- Firecrawl: 60.9%, $2.66\n\n  \\- Jina Reader: 61.2%, $1.54\n\n\n\n  pip install retio-pagemap\n\n  playwright install chromium\n\n\n\n  Works with Claude Code, Cursor, or any MCP client via .mcp.json.\n\n\n\n  GitHub: [https://github.com/Retio-ai/Retio-pagemap](https://github.com/Retio-ai/Retio-pagemap)\n\n\n\n  MIT licensed. Feedback welcome.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1r78i0v/pagemap_mcp_server_that_compresses_web_pages_to/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o5vzwdp",
          "author": "BC_MARO",
          "text": "The numbered ref approach is really clean. I've been using Playwright MCP and the context blowup after a few pages is brutal. 95% success at that token count is impressive.\n\nCurious how it handles SPAs where content loads async after the initial page load. Does it wait for network idle or do you have some heuristic for when the page is \"done\"?",
          "score": 2,
          "created_utc": "2026-02-17 16:23:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yp48t",
              "author": "Direct-Molasses7754",
              "text": "  Thanks! The context blowup is exactly what pushed me to build this.\n\n\n\n  For SPAs, currently it waits on Playwright's networkidle (no network requests for 500ms) plus a 1.5s settle time for late-firing JS. Straightforward but it covers most cases.\n\n\n\n  Honestly, heavy SPAs that never stop polling or aggressive lazy-loading are a known gap right now. I have better heuristics in my internal tooling (content-length checks,\n\n  domcontentloaded fallback) that I haven't yet ported to the MCP server. That's on the roadmap for the next version.\n\n\n\n  If you hit a case where it misses async content, I'd appreciate a GitHub issue ‚Äî it'll help me prioritize which patterns to handle first.",
              "score": 1,
              "created_utc": "2026-02-18 00:20:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wguqt",
          "author": "Brave_Reaction_1224",
          "text": "Hey, Caleb from Firecrawl here. \n\nWould love to talk about this. Sending a DM.",
          "score": 2,
          "created_utc": "2026-02-17 17:47:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ypm3k",
              "author": "Direct-Molasses7754",
              "text": "Hey Caleb! Appreciate you reaching out. Replied to your DM.",
              "score": 1,
              "created_utc": "2026-02-18 00:22:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o633wp4",
          "author": "Educational_Agent741",
          "text": "This is awesome! To avoid context bloat ive been filtering out 80% of html junk before passing it on to AI. My approach atm isnt scalable the way ive done it. Will def give this a try.",
          "score": 1,
          "created_utc": "2026-02-18 17:15:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3q5wr",
      "title": "CamoFox MCP: Anti-detection browser MCP server with 22 tools (TypeScript, MIT)",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r3q5wr/camofox_mcp_antidetection_browser_mcp_server_with/",
      "author": "Silver_Entrance8996",
      "created_utc": "2026-02-13 14:10:47",
      "score": 32,
      "num_comments": 23,
      "upvote_ratio": 0.97,
      "text": "I was frustrated with Playwright MCP being slow and getting blocked on many sites, so I built CamoFox MCP - an MCP server that wraps the CamoFox anti-detection browser (based on Camoufox/Firefox).\n\nWhat it does:\n\n* 22 MCP tools for full browser automation (navigate, click, type, snapshot, screenshot, search across 14 engines, etc.)\n* Anti-detection fingerprinting - each tab gets a unique fingerprint so sites don't flag you as a bot\n* Session isolation per user with cookie import/export\n* Works with Claude Desktop, VS Code Copilot, Cursor, or any MCP client\n\nWhy I built it: Playwright MCP kept getting blocked by Cloudflare, bot detection, and CAPTCHAs. CamoFox uses Camoufox (a patched Firefox fork) that passes most anti-bot checks. The MCP server is a thin TypeScript wrapper over CamoFox's REST API.\n\nQuick start: npx -y camofox-mcp@latest\n\n(Requires CamoFox browser running on localhost:9377)\n\nGitHub: [https://github.com/redf0x1/camofox-mcp](https://github.com/redf0x1/camofox-mcp) npm: [https://www.npmjs.com/package/camofox-mcp](https://www.npmjs.com/package/camofox-mcp)\n\nWould love feedback from the community. MIT licensed, contributions welcome!\n\n========  \nv1.5.5 just dropped ‚Äî here's what's new:\n\nüîß 32 MCP tools (up from 22) ‚Äî added batch operations (fill\\_form, batch\\_click, type\\_and\\_submit), composite tools (navigate\\_and\\_snapshot, scroll\\_and\\_snapshot), and full profile persistence (save\\_profile, load\\_profile, list\\_profiles, delete\\_profile)\n\nüåç Geo preset API ‚Äî request region-specific fingerprints per session via list\\_presets + create\\_tab(preset=\"japan\"). Locale, timezone, geolocation, and viewport all configurable.\n\nüîí Auto session persistence ‚Äî sessions now auto-save on close and auto-restore on create\\_tab. No more manual cookie juggling between runs.\n\nnpm install: npx -y camofox-mcp@latest\n\nDocker + Kubernetes support coming in v1.6.0. Feedback welcome!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1r3q5wr/camofox_mcp_antidetection_browser_mcp_server_with/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o563xkx",
          "author": "Silver_Entrance8996",
          "text": "Fun fact: this post was actually created and submitted using CamoFox MCP itself! The entire flow - login, navigate to subreddit, fill in the post form, select flair, and submit - was fully automated through the MCP server. Reddit didn't flag any of it as bot activity thanks to the anti-detection fingerprinting. Eating our own dogfood here!",
          "score": 9,
          "created_utc": "2026-02-13 14:31:00",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o5988w4",
          "author": "typescape_",
          "text": "The fingerprint-per-tab approach solves a real pain point - I've burned too many hours debugging why my automation suddenly starts getting 403s mid-session.\n\nCurious about the REST API architecture choice. Did you consider exposing CamoFox directly as an MCP server without the intermediate layer? Asking because I've been wrestling with similar tradeoffs when wrapping browser automation for agent workflows.\n\nOne thing that would make this even more useful: persistent session profiles. Right now I'm guessing you'd need to manually export/import cookies between runs. Having named profiles that auto-persist fingerprint + cookies would be huge for workflows that need to maintain state across multiple agent sessions.\n\nWhat's the memory footprint like when running 5-10 concurrent tabs with unique fingerprints?",
          "score": 3,
          "created_utc": "2026-02-13 23:52:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56qmmh",
          "author": "BC_MARO",
          "text": "Nice. Do you expose a profile preset API so agents can request a specific fingerprint or locale? Also curious how you handle CAPTCHA fallbacks.",
          "score": 2,
          "created_utc": "2026-02-13 16:21:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57ed25",
              "author": "Proparser",
              "text": "I need it too.",
              "score": 1,
              "created_utc": "2026-02-13 18:15:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o577apf",
          "author": "MDSExpro",
          "text": "Pack it into container with instance of camofox and it's much more usable.",
          "score": 2,
          "created_utc": "2026-02-13 17:42:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56p534",
          "author": "jcrowe",
          "text": "I thought it was called Camoufox?",
          "score": 1,
          "created_utc": "2026-02-13 16:14:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b9hap",
              "author": "Silver_Entrance8996",
              "text": "Good question! Camoufox is the original Python-based anti-detection browser project. CamoFox (no 'u') is a separate fork (camofox-browser) that adds a REST API layer on top. Our MCP server wraps that REST API, making it accessible to AI agents through the Model Context Protocol.\n\n\n\nSo the chain is: Camoufox (original) ‚Üí CamoFox Browser (REST API fork) ‚Üí CamoFox MCP (this project).",
              "score": 1,
              "created_utc": "2026-02-14 08:56:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5bz1de",
                  "author": "jcrowe",
                  "text": "Thanks for the info, I‚Äôll check it out.",
                  "score": 1,
                  "created_utc": "2026-02-14 12:51:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o57xqgl",
          "author": "HarjjotSinghh",
          "text": "this is unreasonably cool actually!",
          "score": 1,
          "created_utc": "2026-02-13 19:49:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b9j74",
              "author": "Silver_Entrance8996",
              "text": "*Thanks, really appreciate it! üôè*",
              "score": 1,
              "created_utc": "2026-02-14 08:57:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ldwvv",
          "author": "scrapingtryhard",
          "text": "this is pretty cool, the fingerprint-per-tab thing is exactly what's been missing from playwright mcp. I've been running scraping workflows where half the battle is just not getting detected and having unique fingerprints per session makes a huge difference.\n\none thing I'd add though - pairing this with good residential proxies is key. I use Proxyon for my automation stuff and the combo of anti-detection browser + rotating resi proxies basically eliminates blocks for me. datacenter IPs alone still get flagged even with fingerprint spoofing on a lot of sites.\n\nlooking forward to the docker support in 1.6, that'll make deployment way cleaner",
          "score": 1,
          "created_utc": "2026-02-15 23:30:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5n0n52",
              "author": "Silver_Entrance8996",
              "text": "Thanks for the feedback! You're right that evaluate_js is the killer feature ‚Äî it gives you full programmatic DOM control for complex scraping scenarios where accessibility tree snapshots aren't enough. We used it to break through Facebook's lazy-loading comment dialog (139+ comments extracted from a single post). The fingerprint-per-tab + evaluate_js combo makes multi-account data collection much cleaner.",
              "score": 1,
              "created_utc": "2026-02-16 06:03:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r67lqv",
      "title": "I merged MCPs with Openclaw, and i think its near perfect",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r67lqv/i_merged_mcps_with_openclaw_and_i_think_its_near/",
      "author": "YoungBoyMemester",
      "created_utc": "2026-02-16 11:51:21",
      "score": 30,
      "num_comments": 6,
      "upvote_ratio": 0.88,
      "text": "I took Composio mcp integrations 3000+, started with the core 10 that have most value and paired it into a desktop app that runs openclaw in a container with 24/7 uptime. Slack, github, Google workspace all on my whatsapp. It works, like almost flawless but there is so much more I want to add to [easyclaw.app](http://easyclaw.app)\n\nAny suggestions?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1r67lqv/i_merged_mcps_with_openclaw_and_i_think_its_near/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o5pfh5c",
          "author": "BC_MARO",
          "text": "Running it containerized with 24/7 uptime is the right call. One thing worth adding early is some kind of tool-call audit log so you can trace what the agent actually did across those integrations. Gets important fast when you have 10+ MCPs connected and something goes sideways.",
          "score": 6,
          "created_utc": "2026-02-16 16:24:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ouic4",
          "author": "Top_Tour6196",
          "text": "I'm keen on the concept for sure. \"Running fully local...\" is mentioned throughout your docs, but it's unclear how to configure my own gateway, rather than a hosted instance. Am I missing something?",
          "score": 2,
          "created_utc": "2026-02-16 14:43:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5p0e57",
              "author": "YoungBoyMemester",
              "text": "We recently deprecated that, you do have full access to your ubuntu computer in the cloud though.",
              "score": 0,
              "created_utc": "2026-02-16 15:13:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5opxit",
          "author": "Charlotte_K06",
          "text": "Have you tried adding Discord and other things? I liked the google integrations btw",
          "score": 1,
          "created_utc": "2026-02-16 14:18:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pd7zc",
          "author": "penguinzb1",
          "text": "the almost flawless part is the interesting bit. when you're running 10 mcp servers with production integrations like slack and github, the edge cases that break things are usually not obvious until they happen in real usage.\n\nwe use Veris to test these kinds of setups before they hit production. basically simulating real workflows where multiple mcps get called in sequence and seeing if state management between them stays consistent. like if your github mcp updates a PR status and your slack mcp notifies the team, does the sequence hold under load or when one server is slow to respond.\n\ncurious what failure modes you've hit so far. are there specific integration combos that get flaky, or is it more about the orchestration layer handling timeouts?",
          "score": 1,
          "created_utc": "2026-02-16 16:14:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tlacl",
          "author": "sleepnow",
          "text": "Yes, put some actual thought into security and what you're giving it access to.",
          "score": 1,
          "created_utc": "2026-02-17 06:06:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r34ou8",
      "title": "These MCPs made my life so much easier as a marketer (I basically live on Claude)",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r34ou8/these_mcps_made_my_life_so_much_easier_as_a/",
      "author": "Serious-Unit5",
      "created_utc": "2026-02-12 20:36:02",
      "score": 29,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "As a marketer, my bar was simple: can I set it up without bugging our dev team too much (at max once or twice), is it reliable, and am I still using it a month later?\n\n**Google Analytics MCP (Performance reporting)** This one quietly became part of my daily routine. Solid for when you just want to keep a regular eye on stats and not wait for weekly reviews. I just ask Claude things like \"is our new blog getting any traction\" or \"which landing pages had the highest bounce rate last week.\" I get the answer in seconds. \n\n**Notion MCP (Knowledge base)** If your team lives in Notion like mine does, this is a game-changer. I majorly use this with other MCPs - for example it uses by GA4 data to create our weekly reviews on Notion, I also use it to get a better first draft for product marketing assets since it pulls all info from our Notion repository\n\n**Alai MCP (Presentations)**: Our current sales process involves creating custom decks for each client. Earlier the hassle of pulling info from Notion (we also currently use Notion as a temp CRM) + slack threads + call recordings was such a huge pain. Now I've created a baseline template - I use that + MCPs for all these tools to create the content on claude and then use the Alai MCP to get the deck ready using my brand template within 10-15 mins - such a time saver considering how I end up making 10-15 PPTs a week\n\n**Ahrefs MCP (SEO & competitive research)** If you do any SEO or content strategy work, this one's incredibly useful. Being able to ask Claude things like \"what first-page rankings does \\[competitor\\] have that we don't\" or \"show me our top declining keywords this month\" without opening dashboards and running reports manually -  it just removes so much friction from the research phase. I use it most when planning new content or auditing existing pages. Pairs well with Notion MCP for dropping findings straight into our content calendar.\n\n  \nI'm looking for more MCPs to optimize my workflow - if I am missing on any major ones pls do share and I hope the marketers out there find this useful :)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1r34ou8/these_mcps_made_my_life_so_much_easier_as_a/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o540n6s",
          "author": "Top-Cauliflower-1808",
          "text": "Really like your list. The Windsor MCP is the perfect addition to your list because it connects Claude to over 300 ad platforms like Meta and TikTok. It uses automated normalisation so you can ask Claude, Which channel had the best ROAS? without manually blending data from different apps. Btw, you can even send that same data to Looker Studio if you need a visual dashboard for your weekly reviews.",
          "score": 3,
          "created_utc": "2026-02-13 04:45:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5e29t9",
              "author": "Serious-Unit5",
              "text": "nice, will definitely check it out",
              "score": 2,
              "created_utc": "2026-02-14 19:38:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5846fv",
          "author": "getalai",
          "text": "Thanks for the mention, hope you enjoyed the experience so far :)",
          "score": 2,
          "created_utc": "2026-02-13 20:21:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5atplw",
          "author": "Snickers_B",
          "text": "Have you tried Dataforseo mcp?",
          "score": 1,
          "created_utc": "2026-02-14 06:28:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5e25xm",
              "author": "Serious-Unit5",
              "text": "not yet",
              "score": 1,
              "created_utc": "2026-02-14 19:37:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5efiof",
          "author": "zenspirit20",
          "text": "I agree. Relatedly, I have heard that Ahrefs MCP hallucinates data. I have no proof point besides seeing other people‚Äôs comments. Please do keep this in mind. \n\nMy approach is to validate the data I am seeing if anything looks off. In general though it does save a lot of time.",
          "score": 1,
          "created_utc": "2026-02-14 20:49:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fcntz",
          "author": "TheLostWanderer47",
          "text": "Nice stack. One category you might like (if you do competitor/content research) is a web data MCP. Not dashboard data, but live web stuff: competitor pages, SERP results, public content, etc. I‚Äôve seen marketing teams use web MCPs (we‚Äôve used Bright Data‚Äôs [MCP Server](https://github.com/brightdata/brightdata-mcp) in some workflows) to pull live market signals straight into Claude instead of manually checking sites. ",
          "score": 1,
          "created_utc": "2026-02-14 23:59:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5h7bu",
      "title": "After years of iOS development, I open-sourced our best practices into an MCP ‚Äî 10x your AI assistant with SwiftUI component library and full-stack recipes (Auth, Subscriptions, AWS CDK)",
      "subreddit": "mcp",
      "url": "https://i.redd.it/xjnbwf1feojg1.png",
      "author": "w-zhong",
      "created_utc": "2026-02-15 15:21:24",
      "score": 23,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1r5h7bu/after_years_of_ios_development_i_opensourced_our/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5mr695",
          "author": "BC_MARO",
          "text": "The full-stack recipe approach is what sets this apart from typical component libraries. Having the CDK infra code bundled with the SwiftUI frontend means you can go from add auth to deployed in one shot instead of stitching together 5 different tutorials. Curious how you handle recipe versioning - if a recipe gets updated (say new StoreKit API changes), does the MCP serve the latest automatically or do you pin versions?",
          "score": 2,
          "created_utc": "2026-02-16 04:48:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nc08e",
              "author": "w-zhong",
              "text": "MCP always serves the latest version of each recipe. When something changes upstream (like a new StoreKit API), we update the recipe and it's immediately available to all users. Since the recipes are designed as complete, self-contained implementations rather than incremental patches, we haven't needed version pinning yet.",
              "score": 1,
              "created_utc": "2026-02-16 07:44:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5narlj",
          "author": "debackerl",
          "text": "Interesting, why is this better as an MCP instead of SKILLS files?",
          "score": 1,
          "created_utc": "2026-02-16 07:32:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nasfr",
              "author": "haikusbot",
              "text": "*Interesting, why*\n\n*Is this better as an MCP*\n\n*Instead of SKILLS files?*\n\n\\- debackerl\n\n---\n\n^(I detect haikus. And sometimes, successfully.) ^[Learn&#32;more&#32;about&#32;me.](https://www.reddit.com/r/haikusbot/)\n\n^(Opt out of replies: \"haikusbot opt out\" | Delete my comment: \"haikusbot delete\")",
              "score": 1,
              "created_utc": "2026-02-16 07:32:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5nbewa",
              "author": "w-zhong",
              "text": "  1. MCP works across all llms.\n\n  2. On-demand retrieval, especially this will get huge.\n\n  3. Always up-to-date.",
              "score": 1,
              "created_utc": "2026-02-16 07:38:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4dmxh",
      "title": "Has anyone built MCP servers with code execution like Anthropic‚Äôs pattern",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r4dmxh/has_anyone_built_mcp_servers_with_code_execution/",
      "author": "Physical_Ideal_3949",
      "created_utc": "2026-02-14 06:43:15",
      "score": 21,
      "num_comments": 29,
      "upvote_ratio": 0.97,
      "text": "I want to  convert my open api spec to mcp server and use this code execution [https://www.anthropic.com/engineering/code-execution-with-mcp](https://www.anthropic.com/engineering/code-execution-with-mcp) any ideas how to achieve this",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1r4dmxh/has_anyone_built_mcp_servers_with_code_execution/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o5axnlt",
          "author": "DavidAntoon",
          "text": "We built it without requiring file system access; we chose the sandbox approach instead.\n\nhttps://docs.agentfront.dev/frontmcp/plugins/official/codecall",
          "score": 5,
          "created_utc": "2026-02-14 07:03:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b5rp3",
          "author": "BC_MARO",
          "text": "We‚Äôve done similar by wrapping exec in a jailed worker (firecracker/docker + seccomp), no network, strict time/mem, and an explicit tool allowlist. Convert OpenAPI to MCP and route code runs to that sandbox, then stream logs back. If you need approvals and audit on tool calls, Peta (peta.io) is the control plane for MCP: secure vault, managed MCP runtime, tool-call audit trail, and policy approvals.",
          "score": 2,
          "created_utc": "2026-02-14 08:20:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d57xe",
              "author": "Physical_Ideal_3949",
              "text": "Can u share any reference mcp server which is using open api spec with code execution",
              "score": 1,
              "created_utc": "2026-02-14 16:51:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l2je4",
                  "author": "BC_MARO",
                  "text": "Not aware of a single public repo that does the full OpenAPI-to-MCP + sandboxed code execution combo out of the box. The closest references I would look at:\n\n- The official MCP SDK examples (github.com/modelcontextprotocol/servers) for the MCP server skeleton\n- E2B or Modal for the sandboxed execution runtime\n- openapi-to-mcp converters that auto-generate tool definitions from a spec\n\nWe ended up stitching those pieces together ourselves. The OpenAPI part is straightforward - the tricky bit is the sandbox lifecycle and making sure the exec environment gets torn down properly.",
                  "score": 2,
                  "created_utc": "2026-02-15 22:26:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5qsmwe",
              "author": "Leo795",
              "text": "Bit confused as to how the execution sandbox calls the tools itself? Does it execute the requests to the MCP endpoints? How does it do that with no network access? ",
              "score": 1,
              "created_utc": "2026-02-16 20:14:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5qt5fv",
                  "author": "BC_MARO",
                  "text": "The sandbox itself doesn't make any network calls. The host process acts as a mediator. So the flow is: LLM generates code, host sends it to the sandbox for execution, sandbox runs it in isolation and returns stdout/stderr, then the host inspects the output and decides what to do next (including making actual MCP tool calls on behalf of the sandbox if needed). The sandbox only ever sees code in, text out. All the MCP routing happens outside the jail.",
                  "score": 1,
                  "created_utc": "2026-02-16 20:17:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5bvjws",
          "author": "rtfm_pls",
          "text": "I built [puppeteer mcp](https://github.com/iatsiuk/pptr-mcp) server using the same approach.\n\nInstead of exposing dzens of browser automation tools (navigate, click, type, screenshot, etc), it has single execute tool that runs arbitrary js with direct access to puppeteer broswer instance.",
          "score": 2,
          "created_utc": "2026-02-14 12:25:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bgt01",
          "author": "moonshinemclanmower",
          "text": "I use this\n\n[https://github.com/AnEntrypoint/mcp-glootie](https://github.com/AnEntrypoint/mcp-glootie)\n\nWhich is part of this\n\n[https://github.com/AnEntrypoint/glootie-cc](https://github.com/AnEntrypoint/glootie-cc)\n\nMy approach is a bit more advanced than theirs cause I've been at it for longer",
          "score": 1,
          "created_utc": "2026-02-14 10:09:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bifxf",
          "author": "shipping_sideways",
          "text": "the openapi to mcp conversion part is actually pretty straightforward if you think about it as a schema transformation. each openapi operation becomes an mcp tool - path params and query strings map to tool arguments, and response schemas become your tool output format. the tricky part is handling auth flows since mcp doesn't have native oauth token refresh.\n\nfor the code execution bit, anthropic's pattern uses isolated vm contexts for the lightweight sandboxing but you can also look at webcontainer or even wasm-based isolation if you need true multi-tenant safety. key thing is separating the runtime from the filesystem - let the sandbox execute but stream stdout/stderr through a separate channel.",
          "score": 1,
          "created_utc": "2026-02-14 10:25:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5c0k77",
          "author": "ClinchySphincter",
          "text": "https://github.com/pydantic/monty",
          "score": 1,
          "created_utc": "2026-02-14 13:03:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dq16x",
          "author": "ShagBuddy",
          "text": "I created a MCP server specifically designed for coding agents:  [GlitterKill/sdl-mcp: SDL-MCP (Symbol Delta Ledger MCP Server) is a cards-first context system for coding agents that saves tokens and improves context.](https://github.com/GlitterKill/sdl-mcp)",
          "score": 1,
          "created_utc": "2026-02-14 18:35:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ey737",
          "author": "Embarrassed_Hotel630",
          "text": "https://medium.com/@gal.liber1/agent-tool-protocol-why-ai-agents-need-to-write-code-not-call-tools-b57b65f84b37\n\nThe idea came up before anthropic :), full open source implementation",
          "score": 1,
          "created_utc": "2026-02-14 22:32:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fy8fr",
          "author": "HarjjotSinghh",
          "text": "this mcpc version would be next level server vibes",
          "score": 1,
          "created_utc": "2026-02-15 02:18:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5k9h1x",
          "author": "dhana36",
          "text": " Unknowingly, I have created this [https://www.reddit.com/r/mcp/comments/1r50jqt/im\\_not\\_bluffing\\_50\\_token\\_consumption\\_is\\_reduced/](https://www.reddit.com/r/mcp/comments/1r50jqt/im_not_bluffing_50_token_consumption_is_reduced/)\n\n[https://github.com/dhanababum/mcpskills-cli](https://github.com/dhanababum/mcpskills-cli)",
          "score": 1,
          "created_utc": "2026-02-15 19:56:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qjawi",
          "author": "Leo795",
          "text": "Is there any way I can easily implement this for my agent? I know I can simply add an isolated tool for code execution, but would I be able to orchestrate mcp tool calls like in the anthropic blog?",
          "score": 1,
          "created_utc": "2026-02-16 19:29:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cb3q4",
          "author": "Phaelon74",
          "text": "This is so silly, is basically an API server, goodness me oh my.  We've gone full circle now.",
          "score": 0,
          "created_utc": "2026-02-14 14:10:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5cmlm",
      "title": "Lazy loading MCP proxy for Cursor that cuts RAM usage from GBs to ~50 MB ‚Äî open source, 30-second install",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r5cmlm/lazy_loading_mcp_proxy_for_cursor_that_cuts_ram/",
      "author": "Upbeat_Size7437",
      "created_utc": "2026-02-15 11:43:25",
      "score": 21,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "We all run a ton of MCP servers in Cursor today. GitHub, Supabase, Stripe, Playwright... the list keeps growing because that's what makes our workflows fast and automated.\n\nThe problem is that every single server starts at launch and stays resident in memory, even when you're not using it. If you're running 10-15 servers, that's several GBs of RAM sitting there doing nothing. For anyone on a machine with limited memory, that's a real issue.\n\nSo I built **mcp-on-demand** ‚Äî a proxy that sits between Cursor and your MCP servers. Instead of starting everything at launch, it starts servers only when you actually call a tool, then kills them after 5 minutes of inactivity. All your tools stay available in Cursor exactly as before, but servers only run when needed.\n\n**What it does:**\n\n* **Lazy loading** ‚Äî servers spawn on-demand, not at startup. All your tools remain visible in Cursor, but the actual server processes only run when called. RAM drops from GBs to \\~50 MB\n* **Auto-detection** ‚Äî reads your existing `~/.cursor/mcp.json`, no manual config needed\n* **Web dashboard** ‚Äî visual UI to add, remove, edit your MCP servers without touching JSON files. Opens automatically after install\n* **Auto-migration** ‚Äî one command detects your servers, migrates them, and opens the dashboard\n* **Optional Tool Search mode** ‚Äî for advanced users who want to reduce context token usage even further\n\n**How to install:**\n\n**Step 1** ‚Äî Add mcp-on-demand to your `~/.cursor/mcp.json`:\n\n    {\n      \"mcpServers\": {\n        \"mcp-on-demand\": {\n          \"command\": \"npx\",\n          \"args\": [\"-y\", \"@soflution/mcp-on-demand\"]\n        }\n      }\n    }\n    \n\n**Step 2** ‚Äî Run one command:\n\n    npx /mcp-on-demand setup\n    \n\nThis automatically:\n\n1. Detects all your existing MCP servers\n2. Backs up your config\n3. Migrates everything into the proxy\n4. Opens the visual dashboard in your browser\n\nFrom the dashboard you can see all your servers, add new ones, edit API keys, remove what you don't need ‚Äî everything visual, no JSON.\n\n**Step 3** ‚Äî Restart Cursor. Done.\n\n**Who this is for:**\n\n* Cursor users running multiple MCP servers who want to keep their machine responsive\n* Anyone on 8-16 GB of RAM who needs every MB they can get\n* Anyone who wants to manage MCP servers visually instead of editing JSON files\n\nMIT licensed, zero dependencies beyond Node.js 18+.\n\nGitHub: [https://github.com/Soflution1/mcp-on-demand](https://github.com/Soflution1/mcp-on-demand) npm: [u/soflution/mcp-on-demand](https://www.npmjs.com/package/@soflution/mcp-on-demand)\n\nHappy to answer questions or take feature requests.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1r5cmlm/lazy_loading_mcp_proxy_for_cursor_that_cuts_ram/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o5j1sra",
          "author": "BC_MARO",
          "text": "This is a real pain point. Running 10+ MCPs in Cursor eats RAM like crazy, especially the ones that spin up their own Node processes. A lazy-loading proxy that only starts them on demand is exactly what people need. Nice work.",
          "score": 5,
          "created_utc": "2026-02-15 16:23:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nfmxg",
          "author": "Stanny-Boiii",
          "text": "Combine that with mcp-find so all servers aren't loaded into context you'll be onto a winner",
          "score": 1,
          "created_utc": "2026-02-16 08:18:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r26sr0",
      "title": "Built an MCP that lets your whole team share context across Claude Code sessions",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r26sr0/built_an_mcp_that_lets_your_whole_team_share/",
      "author": "Ray-Hernandez",
      "created_utc": "2026-02-11 19:21:02",
      "score": 16,
      "num_comments": 9,
      "upvote_ratio": 0.84,
      "text": ">Claude Code for Teams:  [recall.team](http://recall.team)\n\nBeen building AI memory and context tools for over a year. Started with personal stuff, but the real problem hit when I started working with a team.\n\nEvery Claude Code session is an island. Someone spends an hour debugging an auth issue, fixes it, session ends. A few days later another dev hits the same thing. Starts from zero. One dev fixes auth one way. Another dev doesn't know, changes it. Someone re-explains the same thing to Claude that they already covered two hours ago.\n\nThe knowledge exists. It's just trapped in sessions nobody else can access.\n\nSo we built Recall - a platform that makes Claude Code actually work for teams.\n\n* **Sessions are automatically captured and encrypted when they end**\n* **When any dev starts a new session, their Claude already has context from the whole team**\n* **Search across every session your team has ever run**\n* **Dashboard showing what your team is actually building**\n* **End-to-end encrypted, no source code stored, raw transcripts deleted immediately**\n\nNot just for devs either. A PM can connect through Claude Desktop and ask \"what's blocking the auth migration?\" without bothering anyone.\n\nLaunched today. [recall.team](http://recall.team)\n\nRip it apart, tell me what sucks, what's cool, just really want feedback from the community.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1r26sr0/built_an_mcp_that_lets_your_whole_team_share/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o4uz7x4",
          "author": "BC_MARO",
          "text": "This nails the session-islands problem. How are you handling secret redaction before indexing, and do you support per-repo scoping so old incidents don't bleed into unrelated work?",
          "score": 2,
          "created_utc": "2026-02-11 20:14:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v4tv1",
              "author": "Ray-Hernandez",
              "text": "Per-repo scoping is airtight - every session, decision, and piece of knowledge is scoped at the database level. Context for repo A never includes anything from repo B. Old incidents don't bleed.\n\nFor secrets, we redact before indexing and raw transcripts are deleted after processing. We're also adding programmatic regex detection (Stripe keys, AWS creds, GitHub tokens, high-entropy strings) as a belt-and-suspenders layer on top. LLM compliance is good but I want code-level enforcement too.",
              "score": 2,
              "created_utc": "2026-02-11 20:42:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4v1cmb",
          "author": "BC_MARO",
          "text": "this is a real pain point.\n\nq on the ‚Äúe2e encrypted / raw transcripts deleted‚Äù bit: where do keys live (per-user, per-team), and can you do role-based access + audit logs for who queried what?\n\nwe‚Äôve found teams only adopt this once it looks more like a vault/gateway model (policy + approvals for sensitive stuff), otherwise it feels spooky fast.",
          "score": 2,
          "created_utc": "2026-02-11 20:24:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v4phy",
              "author": "Ray-Hernandez",
              "text": "Keys are per-team, AES-256-GCM. Raw transcripts get deleted after processing - only the LLM-generated summaries persist encrypted at rest. We have owner/admin/member/viewer (pm that can use claude desktop instead of claude code to get recall sessions) roles for team management and key rotation.\n\nAudit logs - honest answer, not built yet. You're describing exactly where we need to go and I agree it's table stakes for enterprise. The vault/gateway model with policy + approvals is on the roadmap. Right now we're focused on dev teams where the auto-deletion of raw transcripts is what gets people past the \"spooky\" feeling. But the full audit trail is coming.",
              "score": 1,
              "created_utc": "2026-02-11 20:41:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4xmenf",
          "author": "drivebyposter2020",
          "text": "Oh, seriously intriguing. Watching with great interest. I am not really in a position to try it myself, as a guy experimenting solo, but I can totally see this problem having legs, and solving it making someone money :) (sorry, I'm a product guy, I look it it through that lens-- \"Does this address a real problem? YES!\") ",
          "score": 2,
          "created_utc": "2026-02-12 05:22:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xr04x",
              "author": "Ray-Hernandez",
              "text": "I may have something for you even going solo.  I built a product called Goldfish before I started building Recall.   Here is the github if you wanted to save context a similar way that recall does, but just locally.  \n\n[https://github.com/raydawg88/goldfish](https://github.com/raydawg88/goldfish)\n\nAnd I also...am a product guy :) ",
              "score": 1,
              "created_utc": "2026-02-12 06:00:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4xxx7p",
          "author": "stibbons_",
          "text": "All these memory mcp cannot really work unless a LLM is itself used to manage these memory. How can a llm knows which memory to retrieve and when?",
          "score": 2,
          "created_utc": "2026-02-12 07:02:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xz3lt",
              "author": "Ray-Hernandez",
              "text": "Technically our mcp‚Äôs job is encrypting and decrypting, vs handling memory. The recall services take care of the memory, but we have a multi-step process where a full transcript is processed, then the llm will document, and organize the session deciphering that you had a problem here, had a solution here, etc, then organizes the file so that it‚Äôs easily searchable and retrievable when you search for something specific. \n\nHope that makes sense.",
              "score": 1,
              "created_utc": "2026-02-12 07:13:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4uo854",
          "author": "Ray-Hernandez",
          "text": "Here's a quick little demo:  [https://www.youtube.com/watch?v=ib8DAPEtsiU](https://www.youtube.com/watch?v=ib8DAPEtsiU)",
          "score": 1,
          "created_utc": "2026-02-11 19:22:02",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2s4hb",
      "title": "I dont get mcp",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r2s4hb/i_dont_get_mcp/",
      "author": "Yaar-Bhak",
      "created_utc": "2026-02-12 12:27:59",
      "score": 15,
      "num_comments": 25,
      "upvote_ratio": 0.76,
      "text": "All I understood till now is - \n\nI'm calling an LLM api normally and now\nInstead of that I add something called MCP which sort of shows whatever tools i have? And then calls api \n\n\nI mean, dont AGENTS do the same thing? \n\nWhy use MCP? Apart from some standard which can call any tool or llm \n\nAnd I still dont get exactly where and how it works \n\nAnd WHY and WHEN should I be using mcp? \n\nI'm not understanding at all üò≠ Can someone please help\n\n",
      "is_original_content": false,
      "link_flair_text": "question",
      "permalink": "https://reddit.com/r/mcp/comments/1r2s4hb/i_dont_get_mcp/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o4yytma",
          "author": "sogo00",
          "text": "You mix up a few things.\n\nAgents are a concept on how the LLM works more independently.\n\nMCP is an interface definition that the LLM can call an application to gather certain information.\n\nFor example: you want the AI to be able to see database entries, so you create an application that gets those and tell the LLM that it can call this tool if it needs this type of information.",
          "score": 8,
          "created_utc": "2026-02-12 12:36:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fy1m4",
              "author": "coworker",
              "text": "You are also mixing up things\n\nLLMs don't call applications. Agents call tools. Agents use LLMs. MCP is an interface for agents to discover tools.\n\nAn LLM is just a model (hint it's in the name lol)",
              "score": 1,
              "created_utc": "2026-02-15 02:17:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4z1z0i",
              "author": "Yaar-Bhak",
              "text": "Okay so MCP would be used in agentic flows right?\nBecause we need agents to \"perform actions\" \n\n\nAnd to choose which action to be performed,\n we need tools. \n\nAnd to see which tool is to be used, we need MCP to list out and the LLM will see which tool to pick",
              "score": 1,
              "created_utc": "2026-02-12 12:57:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4zddpb",
                  "author": "OkLettuce338",
                  "text": "You don‚Äôt strictly ‚Äúneed‚Äù mcp. MCP facilitates the llms decision making with documented tool usage. So if I have an api that has a ‚ÄúgetBalance‚Äù ‚ÄúgetTotal‚Äù and a ‚ÄúgetCurrentTotalBalance‚Äù, your mcp can assist the LLM in choosing which one it needs at which time and what the difference is in a way that‚Äôs optimized for agent usage",
                  "score": 2,
                  "created_utc": "2026-02-12 14:04:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o52izkt",
                  "author": "Corv9tte",
                  "text": "It's a way to streamline a process with one input - output action, which is way more reliable! Think of it like a macro for your LLM, because that's exactly what it is.",
                  "score": 1,
                  "created_utc": "2026-02-12 23:14:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4z876j",
          "author": "penguinzb1",
          "text": "think of mcp as a standardized way for llms to discover and call tools. without it, every agent framework reinvents tool calling slightly differently. with mcp, you write a server once (database access, file system, api calls, whatever) and any mcp client can use it. makes agents way more composable",
          "score": 5,
          "created_utc": "2026-02-12 13:35:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yyn30",
          "author": "Technocratix902",
          "text": "MCP is basically what allows an AI to tool call outside of its current environment. Like a terminal agent tool call to web or a cloud agent tool call to file system. Stuff like that . It's not certain for tool call to be outside environment like context7 and memory mcps. They allow you to give AI extra tools without changing any code. Like MCP connection to claude allows you to add extra tools without changing its code(since you can't it's close source).",
          "score": 6,
          "created_utc": "2026-02-12 12:35:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zda39",
          "author": "rollerblade7",
          "text": "Imagine if you need to update something in your backend - you could add lead information, create a task, call support etc.. you can build an MCP which is essentially an API that an LLM can call in a predictable way",
          "score": 2,
          "created_utc": "2026-02-12 14:03:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52hi2d",
          "author": "Low-Efficiency-9756",
          "text": "MCP is a protocol for executable tools that:\n\n\t1. Validate inputs ‚Äî Schema enforcement via JSON-RPC before execution\n\t2. Mutate state ‚Äî Write to databases, filesystems, APIs\n\t3. Return structured data ‚Äî Not prose, actual typed responses the LLM can parse\n\t4. Maintain invariants ‚Äî The LLM proposes, the tool validates and executes",
          "score": 2,
          "created_utc": "2026-02-12 23:06:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50e4ik",
          "author": "digibath",
          "text": "you can think of the agent as the glue between llm and mcp.  the llm doesn't actually call any tools, and it‚Äôs not aware of the mcp server. \n\nthis is where there seems to be a large misconception. the llm isn‚Äôt actually calling anything. \n\nits fed available tools, and it returns a tool call (this is just a data structure saying what tool to call with what inputs), which the agent then uses to call the correct tool via mcp server, then the agent returns the result from the tool call back to the LLM.",
          "score": 1,
          "created_utc": "2026-02-12 17:03:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53p70y",
          "author": "Creepy_Bullfrog_3288",
          "text": "Agents use tools to sense and act. MCP is a popular protocol to discover and consume those tools. The agent uses a LLM to formulate a plan, which may include calling tools, and then executes that plan.\n\nMCP is just one type of tool, but is popular and many companies/products have started shipping MCP servers to integrate agents with their platforms.",
          "score": 1,
          "created_utc": "2026-02-13 03:27:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53w1me",
          "author": "saikjuan",
          "text": "This is my view up to this point: \n\n√ç didn‚Äôt understand them either and I think developing an MCP for a custom agent might not be the best in all cases. \n\nBUT what I found is that building an MCP is for others to access some information or process you have, so that they do not have to build custom code. \n\nThis way any other agent, will have those tools at their disposal and will know what to do and how to without the owner being explicit about it.  \n\nSo‚Ä¶ build an MCP if you want others to connect to you.",
          "score": 1,
          "created_utc": "2026-02-13 04:12:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53xbhw",
          "author": "tueieo",
          "text": "MCP is a way for agents to have structured and predictable access to tools. \n\nIt is a contract like what GraphQL is for FE. Could we have used GQL instead? Maybe. \n\nBut MCP is just an interface. What you power through it is up to you. Direct database queries, creating UI, updating a todo list, etc.",
          "score": 1,
          "created_utc": "2026-02-13 04:21:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54xrhj",
          "author": "lambdawaves",
          "text": "You had to write an AGENTS to explain how to call the API and what the API methods do and the inputs and outputs. \n\nIn MCP, the server basically declares its own AGENTS message",
          "score": 1,
          "created_utc": "2026-02-13 09:30:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o553w5o",
          "author": "adreportcard",
          "text": "Welcome. Stay. Learn.",
          "score": 1,
          "created_utc": "2026-02-13 10:28:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o572clw",
          "author": "Cool_Fly_2030",
          "text": "It‚Äôs a protocol to standardize how LLMs Fetch context to ground and complete a particular task effectively. \n\nFunctionally MCP servers are APIs, and ‚Äútools‚Äù function as endpoints/routes to perform logic, retrieve data, etc. and return a response to the LLM.\n\nIt‚Äôs pretty powerful in agentic applications of LLMs because they can effectively get knowledge they need to be effective and avoid hallucinating in a fully automated loop.",
          "score": 1,
          "created_utc": "2026-02-13 17:17:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58lmxp",
          "author": "whyisthequestion",
          "text": "Concrete example:\n\nI added the Miro MCP today. Now Claude Code can read and draw system diagrams of codebases and infra directly in Miro.¬†\n\nTomorrow i add Notion MCP and it can document anything, and read all company policies.\n\nThat could be done by rest apis but MCP makes it a five minute setup.¬†",
          "score": 1,
          "created_utc": "2026-02-13 21:48:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5f0bgz",
          "author": "KarmelMalone",
          "text": "MCP is just integrating an API without the work of integrating an API.",
          "score": 1,
          "created_utc": "2026-02-14 22:44:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o510d8v",
          "author": "memetican",
          "text": "I like to loosely think of MCP's as API's for agents.  Your agent needs to interact with something- a database, your email inbox, your file system, and the MCP provides that access and interface. \n\nAgents can also call standard web API's directly if they have the right access tokens and documentation but MCP's give advantages;\n\n* Generally more token-efficient, you don't need piles of JSON.  \n* Generally more agentic- so things like search tools become more valuable \n* Often don't need to return JSON, for many tools MD is a more efficient response \n* Built in docs for when and how to use the MCP, and what each tool is for ( so you don't need a separate skill to use your web API ) \n* For server-based MCPs, generally easier security for integration into ChatGPT, Gemini, Claude... through OAuth.  Users can just add the MCP, login, and it knows what data that user has access to. \n\n",
          "score": 0,
          "created_utc": "2026-02-12 18:47:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51jz9k",
          "author": "BC_MARO",
          "text": "Think of MCP as the ‚Äúplugin protocol‚Äù between a host/agent and tools. Agents are still the thing that decides what to do next. MCP is just a standardized way to *reach* tools (and get results back) across different hosts.\n\nWhen is it useful? When you want one tool server to work in Claude + OpenAI + whatever, and you don‚Äôt want to rewrite glue code each time. The other big one is ops: auth, secrets, auditing, and approvals. A control plane like Peta (peta.io) can centralize creds + policy + an audit trail if you‚Äôre running more than a toy setup.",
          "score": 0,
          "created_utc": "2026-02-12 20:21:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o549tii",
          "author": "Mediocre-Abroad6083",
          "text": "\\*) Tool calling is the core concept. It is how context and actions from external systems are made available to an LLM.  \n\\*) Agents are really a for-loop-wrapper on top of an LLM, providing a set of tools, and making tool calls when requested by the LLM. From the \"outside\", a single call to the OpenAI (or equivalent) API may do this agentic loop on the \"inside\"  \n\\*) Instead of everyone building one-off protocols for tool calling, MCP is a standard protocol. It is a syntactic protocol not a semantic one. So you can wrap some service in an MCP server and publish it. Then any AI client can choose to use it  \n\\*) \"Use it\" means both inspect it (list tools, their descriptions, parameters) which is essential for an AI agent to decide to invoke it, and actually invoke it (run a tool, using a standardized format).  \n\\*) As a practical matter, this is way better than everything having its own one-off REST API because someone who have hand-build a tool to invoke it. If you have exactly one API to call and that's it, maybe that is all  \n\\*) This has nothing to do with context windows --- tiny context window or massive context window, MCP helps standardize tool calling (discovery and actual tool invocation)\n\nSo that's the \"basic\" MCP -- and you could say, ok functionally it is the same as REST APIs, but with a standardized syntax. And in a growing number of cases, the underlying SaaS service itself hosts the MCP server (eg: Notion hosts one, Intercom hosts one, Salesforce has one in preview, etc). So it shifts the burden of integration away from the consumer and to the source.  \n  \nBut things have moved beyond that ..\n\n\\*) MCP also exposes resources  \n\\*) MCP is stateful -- this is super important. Calling one tool can cause other tools to become available  \n\\*) MCP tool calls can be long running. So workflows and sub-agents can be exposed as MCP.  \n\\*) MCP tools can return UI (in a standard form that AI clients will respect)\n\nFInally, as I noted, there is the MCP syntax and then there is the question of semantics. REST APIs tend to be low-level developer-facing interfaces. If you want agents to do well, you want MCP tools that are higher-level semantic interfaces (sort of like the buttons you'd put in an app in front of a human being). Often these might compose many lower-level API calls.  \n  \nSo I'd say syntactically, you can expose a REST API via 1:1 mapping to MCP (and that's what most MCP servers do today). That gives you some benefit with simpler integration to your AI client. But then if you use it with its richer semantics, you get very much better outcomes",
          "score": 0,
          "created_utc": "2026-02-13 05:54:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51k9ub",
          "author": "randommmoso",
          "text": "Thank god finally someone asked this question",
          "score": -1,
          "created_utc": "2026-02-12 20:23:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5338kd",
          "author": "Kitchen-Lynx-7505",
          "text": "MCP is swagger / openapi for AI agents with low context windows.\n\nNow that we have 1M token context window on leading agents it might disappear very soon, as 99% of MCP servers are just wrappers on an OpenAPI endpoint, but we will see.\n\nMostly it has better documentation so that agents can understand, paging, so that the information fits into their tiny brains, and lack of output schema since for an Agent, it doesn‚Äôt really matter as long as it‚Äôs somewhat readable.",
          "score": -1,
          "created_utc": "2026-02-13 01:11:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54fsuf",
          "author": "Old_Motor_6561",
          "text": "Im the creator of RapidMCP.com so I‚Äôm saying this from experience and direct customer feedback. \n\n1) Your intuition is correct.\n2) if you own both ends of the tool calls, don‚Äôt bother, it‚Äôs too much friction and your current setup is probably far easier to validate/enforce/secure/trust etc. \n3) it‚Äôs very much a single tenanted interface (yes there are work arounds but that‚Äôs just the point it‚Äôs a hacky work around) \n\nSo the concrete use case for MCP is:\n\nYou are a user of a platform and you personally want to use those platform APIs in your own agentic assistant chat.",
          "score": -1,
          "created_utc": "2026-02-13 06:45:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51nx8t",
          "author": "Doomtrain86",
          "text": "You‚Äôre right in your intuition. It‚Äôs not that great at all. Just another friction. You‚Äôd be better off just writing a tool that interacts with whatever api you need. More efficient , more control. Less noise. Don‚Äôt listen to the people who thinks this will elevate llms to some new level. No. It‚Äôs holding it back.",
          "score": -2,
          "created_utc": "2026-02-12 20:40:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7wreq",
      "title": "I was tired of manually adding MCP tools, so I built a server that lets the AI write its own tools on the fly.",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r7wreq/i_was_tired_of_manually_adding_mcp_tools_so_i/",
      "author": "Shot_Buffalo_2349",
      "created_utc": "2026-02-18 07:40:54",
      "score": 15,
      "num_comments": 8,
      "upvote_ratio": 0.86,
      "text": "So I kept running into the same problem. I'd be mid-workflow, the agent gets stuck because it's missing a tool, and I'd have to stop everything, go write it manually, restart, and pick up where I left off. Got annoying fast.\n\nI ended up building something to fix that for myself. The agent can now just... write the tool it needs on the spot. Mid-conversation. Saves it, uses it, and it's there permanently from that point on. Next time it needs the same thing it just calls it like it was always there.\n\nThe thing I was most paranoid about was security ‚Äî letting an agent write and execute arbitrary code is sketchy if you don't think it through. So everything runs sandboxed with no access to anything sensitive unless I explicitly approve it. And I can get really specific, like \"this tool can only talk to this one domain, nothing else.\"\n\nI also added a marketplace connected to GitHub so you can publish tools and share them with others, or install tools someone else already built. Your GitHub identity handles ownership so nobody can mess with what you published.\n\nBeen using it daily for a few days now in my own projects and it's changed how I think about building agent workflows. Instead of planning tools upfront I just let the agent figure out what it needs.\n\nRepo is open if anyone wants to check it out or poke around: [https://github.com/ageborn-dev/architect-mcp-server](https://github.com/ageborn-dev/architect-mcp-server)",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1r7wreq/i_was_tired_of_manually_adding_mcp_tools_so_i/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o60pyuc",
          "author": "BC_MARO",
          "text": "The sandbox per-tool approach is solid. Self-generating tools is one of those things that sounds risky but actually works well when you scope the permissions right. How granular can you get with the domain restrictions? Like can you limit a tool to specific endpoints on a domain, or is it domain-level only?",
          "score": 2,
          "created_utc": "2026-02-18 08:23:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60qumn",
              "author": "Shot_Buffalo_2349",
              "text": "Right now it's domain-level ‚Äî so `net:api.github.com` locks it to that domain only, no wildcards or path restrictions yet. Endpoint-level granularity is something I want to add ‚Äî things like restricting to specific paths or HTTP methods would make the permission model significantly tighter. It's on the roadmap. The domain scoping covers the biggest attack surface for now but you're right that path-level control is the next logical step.\"",
              "score": 4,
              "created_utc": "2026-02-18 08:32:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6168vm",
                  "author": "BC_MARO",
                  "text": "Makes sense. Even before path-level scoping, method allowlists + simple request schema validation (and per-endpoint rate limits) go a long way. When you add endpoint scoping, I‚Äôd model it like \"net:api.github.com#GET:/repos/*\" and hard-fail anything outside that.",
                  "score": 1,
                  "created_utc": "2026-02-18 10:52:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r6m9k1",
      "title": "üöÄ Introducing SNAP: The \"Snapshot\" MCP Server for AI Agents",
      "subreddit": "mcp",
      "url": "https://i.redd.it/w815phr0bxjg1.png",
      "author": "Chips_n_Diff",
      "created_utc": "2026-02-16 21:18:18",
      "score": 14,
      "num_comments": 1,
      "upvote_ratio": 0.82,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1r6m9k1/introducing_snap_the_snapshot_mcp_server_for_ai/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5u0spr",
          "author": "upvotes2doge",
          "text": "Your image is filled with garbage text",
          "score": 2,
          "created_utc": "2026-02-17 08:26:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4hb03",
      "title": "Google Analytics MCP is available but only locally",
      "subreddit": "mcp",
      "url": "https://github.com/googleanalytics/google-analytics-mcp",
      "author": "48K",
      "created_utc": "2026-02-14 10:25:34",
      "score": 14,
      "num_comments": 2,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "question",
      "permalink": "https://reddit.com/r/mcp/comments/1r4hb03/google_analytics_mcp_is_available_but_only_locally/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5binau",
          "author": "shipping_sideways",
          "text": "my guess is it comes down to api quotas and data volume. the GA reporting API has pretty aggressive rate limits (10 QPS per property, 10k requests/day for free tier) and analytics queries tend to be bursty - you pull a lot of dimensions/metrics at once. running locally means you can implement your own caching layer and batch requests without fighting a remote server's rate limiting on top of google's.\n\nalso the oauth scope for analytics is pretty sensitive from a business perspective - companies are paranoid about their traffic data hitting third party servers even briefly. mail and drive are different beasts since those are user-initiated discrete operations rather than bulk data pulls.",
          "score": 3,
          "created_utc": "2026-02-14 10:27:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bvjn8",
          "author": "evildeadxsp",
          "text": "We have a free tool that connects to the GA4 MCP without having to set anything up. We originally built it for clients to just go to a website to connect and ask questions ...  and have since released it to the public...\n\nDM me!",
          "score": 1,
          "created_utc": "2026-02-14 12:24:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6olpb",
      "title": "Use Chatgpt.com, Claude.ai, Gemini, AiStudio, Grok, Perplexity from the CLI",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r6olpb/use_chatgptcom_claudeai_gemini_aistudio_grok/",
      "author": "Just_Lingonberry_352",
      "created_utc": "2026-02-16 22:47:08",
      "score": 14,
      "num_comments": 2,
      "upvote_ratio": 0.94,
      "text": "I built Agentify Desktop to bridge CLI agents with real logged-in AI web sessions.\n\nIt is an Electron app that runs locally and exposes web sessions from ChatGPT, Claude, Gemini, AI Studio, Grok, and Perplexity browser tabs as MCP tools\n\nShould work on Codex, Claude Code, and OpenCode as its just as an MCP bridge.\n\nWhat works currently:\n\n‚Ä¢ use Chatgpt PRO and image gen from codex cli\n\n‚Ä¢ prompt + read response\n\n‚Ä¢ file attachments (tested on chatgpt only)\n\n‚Ä¢ send prompts to all vendors and do comparisons\n\n‚Ä¢ local loopback control with human-in-the-loop login/CAPTCHA\n\n\n\nhttps://github.com/agentify-sh/desktop",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1r6olpb/use_chatgptcom_claudeai_gemini_aistudio_grok/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o5s4r7b",
          "author": "BC_MARO",
          "text": "Cool idea bridging real web sessions as MCP tools. The human-in-the-loop for CAPTCHA/login is a practical solution since those flows are basically impossible to automate reliably.\n\nDo you have any kind of audit trail for what gets sent to each provider? With multiple AI sessions going at once, tracking which tool calls went where seems like it could get messy. Something like peta.io handles that for MCP but curious if you have your own approach.",
          "score": 1,
          "created_utc": "2026-02-17 00:24:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5srgt2",
              "author": "Just_Lingonberry_352",
              "text": " it‚Äôs scoped by keyed tab/session so each call has a clear target,\nno audit log because everything just happens between you and the underlying websites directly, there is no cloud relay of your prompts.",
              "score": 1,
              "created_utc": "2026-02-17 02:39:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r2mqcn",
      "title": "Excalidraw mcp is kinda cool",
      "subreddit": "mcp",
      "url": "https://v.redd.it/kp7hxp5pi0jg1",
      "author": "shanraisshan",
      "created_utc": "2026-02-12 07:03:29",
      "score": 13,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "resource",
      "permalink": "https://reddit.com/r/mcp/comments/1r2mqcn/excalidraw_mcp_is_kinda_cool/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4yi3px",
          "author": "BC_MARO",
          "text": "Nice, the SVG‚Üídiagram loop is exactly what I wanted. We routed it through Peta (peta.io) for approvals and an audit trail when agents modify shared diagrams, which made teams less nervous.",
          "score": 1,
          "created_utc": "2026-02-12 10:18:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r647dn",
      "title": "I built a Currency Exchange MCP Server ‚Äî forex + crypto for AI agents",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r647dn/i_built_a_currency_exchange_mcp_server_forex/",
      "author": "RuddyBuilds",
      "created_utc": "2026-02-16 08:28:30",
      "score": 12,
      "num_comments": 10,
      "upvote_ratio": 0.87,
      "text": "Hey everyone, I built and deployed a currency exchange MCP server that gives AI agents real-time forex and crypto conversion.\n\n\n\nWhat it does:\n\n  \\- Convert between 60+ fiat currencies and 30+ cryptocurrencies\n\n  \\- Batch convert to up to 50 currencies at once\n\n  \\- Historical rates with time-series data\n\n  \\- Natural language input ‚Äî say \"dollars\" or \"bitcoin\" instead of ISO codes\n\nHow it works:\n\n\\- 5 upstream providers with automatic failover (ExchangeRate-API, fawazahmed0, Frankfurter, Coinbase, CoinGecko)\n\n\\- No upstream API keys needed\n\n\\- Pay-per-event pricing starting at $0.003/conversion\n\n\n\nQuick setup ‚Äî add to your MCP client config:\n\n      {\n        \"mcpServers\": {\n          \"currency-exchange\": {\n            \"url\": \"https://vector384--currency-exchange-mcp.apify.actor/mcp\",\n            \"headers\": {\n              \"Authorization\": \"Bearer YOUR_APIFY_TOKEN\"\n            }\n          }\n        }\n      }\n\n  GitHub: [https://github.com/Ruddxxy/currency-exchange-mcp](https://github.com/Ruddxxy/currency-exchange-mcp)\n\n  \nWould love feedback!!",
      "is_original_content": false,
      "link_flair_text": "server",
      "permalink": "https://reddit.com/r/mcp/comments/1r647dn/i_built_a_currency_exchange_mcp_server_forex/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o5nhk64",
          "author": "punkpeye",
          "text": "Please take time to list your server for easy access https://glama.ai/mcp/servers",
          "score": 2,
          "created_utc": "2026-02-16 08:36:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nj8u5",
              "author": "RuddyBuilds",
              "text": "I was having some issues in glama. Was not able to login through gh",
              "score": 2,
              "created_utc": "2026-02-16 08:52:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5njayr",
                  "author": "punkpeye",
                  "text": "Is this a recent issue or sometime ago?",
                  "score": 1,
                  "created_utc": "2026-02-16 08:53:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5nvnks",
          "author": "0xKoller",
          "text": "How would be the workflow for this server? Which use case satisfies? ",
          "score": 1,
          "created_utc": "2026-02-16 10:49:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}