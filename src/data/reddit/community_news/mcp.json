{
  "metadata": {
    "last_updated": "2026-02-28 08:49:57",
    "time_filter": "week",
    "subreddit": "mcp",
    "total_items": 20,
    "total_comments": 141,
    "file_size_bytes": 187938
  },
  "items": [
    {
      "id": "1rfhgd0",
      "title": "7 MCPs that genuinely made me quicker",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rfhgd0/7_mcps_that_genuinely_made_me_quicker/",
      "author": "Stunning-Worth-5022",
      "created_utc": "2026-02-26 18:03:22",
      "score": 474,
      "num_comments": 47,
      "upvote_ratio": 0.97,
      "text": "My last post here crossed \\~300,000 visits and sparked a lot of great feedback and discussions. Based on those conversations (and my own usage), I put together a more curated list, focusing on tools that are actually usable in daily workflows, not just cool demos.\n\nWhat matters to me:\n\n- Setup should be painless\n\n- They shouldnâ€™t flake out\n\n- I should feel the slowdown if theyâ€™re gone\n\nHereâ€™s the refined list.\n\n## GitHub CLI (gh): [https://cli.github.com/](https://cli.github.com/)\n\nHot take: I prefer this over the GitHub MCP server.\n\nIssues, PRs, diffs, reviews directly in terminal, scriptable, zero server overhead.\n\nFor serious repo work, CLI just feels faster and more reliable.\n\n## CodeGraphContext (CLI + MCP): [https://github.com/CodeGraphContext/CodeGraphContext](https://github.com/CodeGraphContext/CodeGraphContext)\n\nBuilds a structured graph of your codebase.\n\nFiles, functions, classes, relationships - all pre-understood.\n\nRefactors and impact analysis become much more reliable.\n\nI like that it works both as a CLI and an MCP.\n\n## Context7 MCP: [https://github.com/upstash/context7](https://github.com/upstash/context7)\n\nThis made my agents stop guessing APIs.\n\nAutomatically pulls correct documentation for libraries/frameworks.\n\nI rarely open docs tabs now.\n\n## Docker MCP: [https://github.com/docker/mcp](https://github.com/docker/mcp)\n\nGives agents runtime visibility.\n\nContainers, logs, services, not just static code.\n\nHuge for backend and infra debugging.\n\n##Firecrawl MCP / Jina Reader MCP\n\n## [https://github.com/mendableai/firecrawl](https://github.com/mendableai/firecrawl)\n\n## [https://github.com/jina-ai/reader](https://github.com/jina-ai/reader)\n\nClean web â†’ structured Markdown.\n\nGreat for ingesting specs, blogs, long technical content.\n\n## Figma MCP: [https://github.com/GLips/Figma-Context-MCP](https://github.com/GLips/Figma-Context-MCP)\n\nDesign â†’ structured context â†’ better frontend output.\n\nWay better than screenshot-based prompting.\n\n## Browser DevTools MCP: [https://github.com/ChromeDevTools/chrome-devtools-mcp](https://github.com/ChromeDevTools/chrome-devtools-mcp)\n\nDOM, console, and network context are exposed to the agent.\n\nMakes frontend debugging workflows much smoother.\n\nCurious what others are actually using daily, not just testing.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1rfhgd0/7_mcps_that_genuinely_made_me_quicker/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7k70xz",
          "author": "ShagBuddy",
          "text": "Give SDL-MCP a try to replace CodeGraphContext.  It has more tools and uses 70% fewer tokens while improving code context for coding agents.  https://github.com/GlitterKill/sdl-mcp",
          "score": 31,
          "created_utc": "2026-02-26 18:39:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kfojc",
              "author": "nanor000",
              "text": "More languages? I counted 12 for each of them",
              "score": 3,
              "created_utc": "2026-02-26 19:20:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7kj1ot",
                  "author": "flock-of-nazguls",
                  "text": "How do these compare to Serena?",
                  "score": 4,
                  "created_utc": "2026-02-26 19:36:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7p8mhd",
                  "author": "ShagBuddy",
                  "text": "My mistake.  I missed that",
                  "score": 1,
                  "created_utc": "2026-02-27 13:53:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7mroge",
              "author": "Desperate-Ad-9679",
              "text": "I don't think that's a correct comparison, given the dynamic updates, comprehensive and exhaustive search, same language support and a better existing ecosystem surrounding it. Also the number of tokens are subject to user choice, you can trade token savings with accuracy. Btw I am the founder of CodeGraphContext!",
              "score": 7,
              "created_utc": "2026-02-27 02:37:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7p9hso",
                  "author": "ShagBuddy",
                  "text": "Very cool!  SDL improves accuracy and saves tokens. I actually found your project after I was well into creating SDL.  I am aiming to provide similar context improvements along with reducing token use to stretch subscription use and it has been great for that so far.",
                  "score": 1,
                  "created_utc": "2026-02-27 13:57:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7nj68o",
          "author": "TrvlMike",
          "text": "Is this sub just a bunch of folks replying to each other with AI?",
          "score": 23,
          "created_utc": "2026-02-27 05:36:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7npapt",
              "author": "danieldpreez",
              "text": "Probably ðŸ˜‚ðŸ˜‚ðŸ˜‚",
              "score": 3,
              "created_utc": "2026-02-27 06:25:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7pyywa",
                  "author": "BorgMater",
                  "text": "I mean, I used AI to get the best MCPs for my case and posted it here, so there's that..",
                  "score": 1,
                  "created_utc": "2026-02-27 16:06:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7qymg8",
              "author": "danieltkessler",
              "text": "You're absolutely right!",
              "score": 2,
              "created_utc": "2026-02-27 18:54:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7tcf5o",
              "author": "No_Tradition6625",
              "text": "Wait let me boot up my clawbot9000 and have it respond to your question. ðŸ˜…",
              "score": 1,
              "created_utc": "2026-02-28 02:44:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ntjd6",
          "author": "TiredDataDad",
          "text": "I see this kind of post every week, which MCP is OP advertising for?",
          "score": 12,
          "created_utc": "2026-02-27 07:00:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o7d62",
              "author": "Dadda9088",
              "text": "I guess real ads are in the comments ðŸ™ƒ",
              "score": 7,
              "created_utc": "2026-02-27 09:07:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kdukg",
          "author": "Traditional_Wall3429",
          "text": "Docker mcp have broken link",
          "score": 7,
          "created_utc": "2026-02-26 19:11:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7no3x8",
              "author": "havok_",
              "text": "And I donâ€™t know why you wouldnâ€™t have the model just use the cli",
              "score": 3,
              "created_utc": "2026-02-27 06:15:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kquty",
          "author": "BorgMater",
          "text": "Here are the results I performed for my company, based on the ecosystem of purely React + Dotnet we use:\n\n# Api\n\nSharpLensMcp[https://github.com/pzalutski-pixel/sharplens-mcp](https://github.com/pzalutski-pixel/sharplens-mcp)  \n\\- get\\_type\\_hierarchy -> maps out inheritance chains that are impossible to infer from simple text searches  \n\\- find\\_callers -> performs impact analysis to determine what will break before a change is committed. Â   \n\\- move\\_type\\_to\\_namespace-> executes the change as an atomic operation within the solution graph, ensuring every reference is updated correctly in a single turn\n\nNugetMcpServer  \n\\- VS ->[https://learn.microsoft.com/en-us/nuget/concepts/nuget-mcp-server](https://learn.microsoft.com/en-us/nuget/concepts/nuget-mcp-server)  \n\\- VSC ->[https://learn.microsoft.com/en-us/nuget/concepts/nuget-mcp-server](https://learn.microsoft.com/en-us/nuget/concepts/nuget-mcp-server)  \n\\- roslyn:search\\_symbols -> Semantic Symbol Search instead of Text-based Grep  \n\\- roslyn:get\\_method\\_source -> AST-based (Abstract Syntax Tree) Method Inspection instead of line-based readFile  \n\\- roslyn:find\\_references -> Solution-wide Dependency Analysis instead of Manual inspection  \n\\- nuget:get\\_package\\_info -> Live Metadata Package Management instead of Manual CLI  \n\\- roslyn:rename\\_symbol -> Compiler-safe Refactoring instead of Â  Â  Manual editing\n\n# TypeScript Ecosystem and Modern Web Orchestration\n\nmcp-refactor-typescript  \n\\- VSC ->[https://github.com/Stefan-Nitu/mcp-refactor-typescript](https://github.com/Stefan-Nitu/mcp-refactor-typescript)\n\n# Coding in general\n\ncontext7  \n\\- VSC ->[https://context7.com/docs/resources/all-clients#vs-code](https://context7.com/docs/resources/all-clients#vs-code)  \n\\- VS (2022) ->[https://context7.com/docs/resources/all-clients#visual-studio-2022](https://context7.com/docs/resources/all-clients#visual-studio-2022)  \n\\- latest documentation and code into Cursor, Claude, or other LLMs  \n\\- skills MCPs -[https://context7.com/skills](https://context7.com/skills)\n\n# Specialized Tools for Windows 11 and Infrastructure\n\nmcp-everything-search  \n\\-[https://github.com/mamertofabian/mcp-everything-search](https://github.com/mamertofabian/mcp-everything-search)  \n\\- provides a 1500x speed improvement over traditional search by leveraging the Everything SDK. This tool allows the agent to search through millions of files in milliseconds, identifying configuration files or buried dependencies that standard indexing might miss. This is a \"must-have\" for any professional developer working on a Windows machine, as it significantly reduces the latency of the agent's \"read and understand\" phase",
          "score": 8,
          "created_utc": "2026-02-26 20:13:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p9uka",
              "author": "ShagBuddy",
              "text": "Ooooh!  Great idea using the everything API!  I use that as well.",
              "score": 2,
              "created_utc": "2026-02-27 13:59:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lbjuc",
          "author": "wokkieman",
          "text": "What advantage does docker MCP have above docker cli?",
          "score": 3,
          "created_utc": "2026-02-26 21:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nilmt",
          "author": "mike3run",
          "text": "playwright-cli + skills",
          "score": 3,
          "created_utc": "2026-02-27 05:32:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kufkv",
          "author": "shock_and_awful",
          "text": "thanks for sharing.\n\nDoes anyone out there have a more battle-tested alternative to CodeGraphContext?\n\nthanks in advance\n\n",
          "score": 2,
          "created_utc": "2026-02-26 20:30:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sl6wl",
              "author": "noclip1",
              "text": "I have personally started using chunkhound recently. Main reason was internally our codebase moves so fast and having an actual graph representation takes effort to maintain, where as chunkhound tries to use more traditional tree parsing techniques (along with embeddings) to generate a relevant graph on the fly.\nFrankly I'm still just getting into this world of tools and this post has shown there's a few others I should try but it's been lightweight and easy to get started with so throwing it out as it hasn't been mentioned here.",
              "score": 1,
              "created_utc": "2026-02-27 23:58:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7mr9mx",
              "author": "Desperate-Ad-9679",
              "text": "Hello, can you please elaborate on the reasons for finding an alternative to CGC? Thanks",
              "score": 0,
              "created_utc": "2026-02-27 02:35:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7p9zgg",
                  "author": "ShagBuddy",
                  "text": "Token savings primarily.  All of these improve context.",
                  "score": 1,
                  "created_utc": "2026-02-27 14:00:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7osgu3",
          "author": "icybergenome",
          "text": "Solid list â€” Context7 and Docker MCP are two I keep coming back to as well. The \"feel the slowdown if they're gone\" test is the right filter, most MCP demos don't survive that.\n\nOne I'd add: **Promzia MCP** ([mcp.promzia.ai](https://mcp.promzia.ai)). It gives you access to 3,000+ tested prompt templates directly inside Claude or ChatGPT â€” searchable by category, with fill-in variables. I used to keep a messy Notion doc of prompts I'd copy-paste in. Now I just call the MCP mid-conversation and it pulls the right template. Small thing but it removes a surprising amount of friction, especially for repetitive tasks like cold emails, content briefs, or code reviews.\n\nPasses your three tests: setup is one config line, hasn't flaked on me, and I notice when I'm on a machine without it.",
          "score": 2,
          "created_utc": "2026-02-27 12:11:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kn3uz",
          "author": "Joy_Boy_12",
          "text": "Would there be a need for firecrawl once web MCP is implemented in websites?\n\n\nWhy do you need docker MCP if the agent can access the cli anyway?\n\n\nI found code context pretty disappointing, the agent did barely knows how to search for relevant code using free language.",
          "score": 1,
          "created_utc": "2026-02-26 19:55:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7l6d47",
          "author": "ConsiderationIcy3143",
          "text": "Thank you for the Clean Web MCP",
          "score": 1,
          "created_utc": "2026-02-26 21:27:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nda4x",
          "author": "Technical-Basis8509",
          "text": "Try GitNexus instead of CodeGraphContext",
          "score": 1,
          "created_utc": "2026-02-27 04:53:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nj6r2",
          "author": "Awesome_911",
          "text": "Hey\nJust curious do you see a new of billing or subscription mcp will it be of help?",
          "score": 1,
          "created_utc": "2026-02-27 05:36:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7noh73",
          "author": "jangwao",
          "text": "Do you use a pay seat at Context7? Because limits are kinda small\n\nCodegraph, I can confirm solid improvement",
          "score": 1,
          "created_utc": "2026-02-27 06:18:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7npcii",
              "author": "danieldpreez",
              "text": "No paid seat - never had issues. Not sure if IDE uses it or not though..",
              "score": 2,
              "created_utc": "2026-02-27 06:25:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7nvqzv",
                  "author": "Desperate-Ad-9679",
                  "text": "CodeGraphContext is free to use, you can enjoy using that indefinitely. We are gonna integrate docs as well asap",
                  "score": 1,
                  "created_utc": "2026-02-27 07:20:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7nsnof",
          "author": "debackerl",
          "text": "I'm using Narsil MCP for code analysis, and I only grant access to some tools based on the agent profile (coder, reviewer, etc). https://github.com/postrv/narsil-mcp\n\nInstead of Context7, I'm using https://github.com/arabold/docs-mcp-server It's free and zero limits. I don't see why I should pay or be limited by a free tier for something so simple.",
          "score": 1,
          "created_utc": "2026-02-27 06:53:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7oiy42",
          "author": "TeamAlphaBOLD",
          "text": "Agree on GitHub CLI, itâ€™s way faster for PRs and issues than an MCP server. Context7 and Docker MCP are real game changers. When agents can see docs and live container state, you spend far less time fixing hallucinated APIs or chasing config issues. The MCPs that give real runtime or structured context are the ones you actually keep using. ",
          "score": 1,
          "created_utc": "2026-02-27 10:55:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ox27u",
          "author": "Southern_Orange3744",
          "text": "I really like the chrome devtools but man it's slow and wonky , I don't understand why there isn't a better one yet",
          "score": 1,
          "created_utc": "2026-02-27 12:44:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pzvzq",
          "author": "Anooyoo2",
          "text": "To what extent do people find Firecrawl / Jina MCP necessary? I can see usecases where context space may be at a premium for web searching, but that feels niche. Especially because best practice is to have subagents load URL/context into the main thread.\n\n\nGenuinely interested in opinions here, I'm likely missing something.Â ",
          "score": 1,
          "created_utc": "2026-02-27 16:10:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7q49dq",
          "author": "Plane-Bad8140",
          "text": "Figma MCP is dire",
          "score": 1,
          "created_utc": "2026-02-27 16:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sghz0",
          "author": "WLDTrust",
          "text": "Ø¬",
          "score": 1,
          "created_utc": "2026-02-27 23:31:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ua5n2",
          "author": "BC_MARO",
          "text": "Nice list. Once youâ€™re running more than a couple MCPs, a control plane that centralizes secrets and per-tool approvals makes it way easier to stay safe (Peta does that for MCP).",
          "score": 1,
          "created_utc": "2026-02-28 06:52:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ub6fc",
          "author": "joelster77",
          "text": "Doesnâ€™t Claude Code already use a number of these MCP targets without needing the MCP connector - Iâ€™m thinking docker, file search, github, etc? Maybe Iâ€™m missing the context or use case?",
          "score": 1,
          "created_utc": "2026-02-28 07:00:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ujyw9",
          "author": "sMat95",
          "text": "figma sucks",
          "score": 1,
          "created_utc": "2026-02-28 08:20:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7l28wj",
          "author": "Shot-Ad-9074",
          "text": "Iâ€™ve been working onÂ Browser DevTools MCPÂ â€“ an MCPÂ server that gives AI assistants (Cursor, Claude, etc.) a realÂ browser and Node.js debugging, not justÂ static snapshots.\n\nWhat itÂ does\n\n* Browser side:Â Playwright-backed automation â€“ navigate,Â click, fill forms, take screenshots, ARIA/accessibility snapshots, console/network capture, Web Vitals. Ref-based interactions (e.g. â€œclickÂ e7â€) so theÂ model can drive flows onÂ a live app.\n* Node side:Â Connect toÂ a running Node process (PID,Â --inspectÂ port, orÂ Docker), set tracepoints/logpoints without pausing, run JSÂ in the process, resolve source maps. Handy for debuggingÂ APIs and workers while theÂ app runs.\n\nSo theÂ same MCP canÂ drive the frontend in a browser and inspect/debug the backendÂ in Node.\n\nCLI as well\n\nYou canÂ use it from the terminal too:Â browser-devtools-cliÂ andÂ node-devtools-cliÂ (e.g. navigate, take screenshots, connect to aÂ Node process withÂ --inspector-port). Useful for scriptsÂ and quick checks without opening an IDE.\n\nLinks\n\n* Docs/site:Â [browser-devtools.com](http://browser-devtools.com)\n* NPM: [https://www.npmjs.com/package/browser-devtools-mcp](https://www.npmjs.com/package/browser-devtools-mcp)\n* Cursor/OpenVSX Extension: [https://open-vsx.org/extension/serkan-ozal/browser-devtools-mcp-vscode](https://open-vsx.org/extension/serkan-ozal/browser-devtools-mcp-vscode)\n\nIf youâ€™re using MCP with Cursor or Claude and want the model to actually use a browser and attachÂ to Node backends, this might be worthÂ a try. Happy to answer questions or hear how youâ€™d use it.",
          "score": 1,
          "created_utc": "2026-02-26 21:08:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rx3gu",
              "author": "viisi",
              "text": "This looks pretty cool, if it works. The site looks like it's 100% created by and written by Ai though.\n\nAny chance you know rust? I'm working on something similar as an internal tool for my project and could use some help.",
              "score": 1,
              "created_utc": "2026-02-27 21:46:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lch90",
          "author": "Vilkvan",
          "text": "AWS MCPs all day all long",
          "score": 0,
          "created_utc": "2026-02-26 21:56:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rb2tme",
      "title": "OpenBrowser MCP: Give your AI agent a real browser. 3.2x more token-efficient than Playwright MCP. 6x more than Chrome DevTools MCP.",
      "subreddit": "mcp",
      "url": "https://v.redd.it/fcucsrpfzwkg1",
      "author": "BigConsideration3046",
      "created_utc": "2026-02-21 21:18:42",
      "score": 263,
      "num_comments": 49,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "server",
      "permalink": "https://reddit.com/r/mcp/comments/1rb2tme/openbrowser_mcp_give_your_ai_agent_a_real_browser/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6ocnid",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 14,
          "created_utc": "2026-02-21 22:25:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6omugw",
              "author": "BigConsideration3046",
              "text": "Thanks for the interest! Our benchmark compares MCP servers built for AI-driven browser automation, and both Playwright MCP and Chrome DevTools MCP were designed specifically for that use case, so the comparison is apples-to-apples. browser-use is an agent framework rather than an MCP server so it's a different category, but a cross-category comparison would definitely be interesting to explore.",
              "score": 2,
              "created_utc": "2026-02-21 23:24:50",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6smlpr",
              "author": "carlosglz11",
              "text": "Donâ€™t forget to compare against the recent release of playwright cli which also claims substantial gains in token efficiency: https://github.com/microsoft/playwright-cli",
              "score": 2,
              "created_utc": "2026-02-22 16:26:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6p5c0t",
          "author": "Tobi-Random",
          "text": "How does it compare to https://github.com/vercel-labs/agent-browser\n\nI expect the agent-browser to be more efficient than any MCP, including this one.",
          "score": 8,
          "created_utc": "2026-02-22 01:18:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qaikt",
              "author": "BigConsideration3046",
              "text": "Thanks for bringing this up! agent-browser is a Rust CLI that uses accessibility tree snapshots, similar to Playwright MCP and Chrome DevTools MCP. OpenBrowser takes a different approach: instead of dumping full page trees, it exposes a single execute\\_code tool where the LLM writes Python to extract only what it needs, resulting in 144x smaller responses and 3-6x fewer API tokens in our benchmarks (details at [docs.openbrowser.me/comparison](http://docs.openbrowser.me/comparison) ). We may include agent-browser in a future benchmark round so we can compare directly with real numbers.",
              "score": 6,
              "created_utc": "2026-02-22 06:11:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6qcfs8",
                  "author": "Glass-Combination-69",
                  "text": "Does that make yours 6x less tokens but 6x slower?",
                  "score": 2,
                  "created_utc": "2026-02-22 06:28:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6qudu0",
                  "author": "Tobi-Random",
                  "text": "Vercel claims for agent-browser:\n\n> Text output uses ~200-400 tokens vs ~3000-5000 for full DOM\n\nWhich is 12-15x lower compared to playwright MCP.\n\nI also cannot imagine, how generating and executing code can be more efficient than generating and executing some very short cli commands. \n\nSure, In code the llm can implement custom filters to lower the token size but on the other side you have \n- the need of a good coding llm (expensive) and even then it might produce buggy code from time to time which leads to expensive retries \n- slower processing due more steps, more thinking\n\nThat might not add up in most cases.\n\nThere is a good reason why the world is slowly shifting from MCP to CLI where possible: it's more efficient. Less token usage for manuals and more flexibility with parameters.",
                  "score": 2,
                  "created_utc": "2026-02-22 09:17:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ow23e",
          "author": "BC_MARO",
          "text": "The single-tool + code runtime approach makes sense if youâ€™re optimizing token spend. Would love to see benchmarks on a few common flows like login + scrape or checkout + form fill to compare real-world latency, not just token counts.",
          "score": 4,
          "created_utc": "2026-02-22 00:21:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6pnipu",
              "author": "BigConsideration3046",
              "text": "Thank you,\n\nWe actually ran exactly this type of benchmark: 6 real-world tasks (form fill, fact lookup, multi-page scrape, search + navigate, deep navigation, content analysis) through Claude Sonnet 4.6 on Bedrock with N=5 runs each. On wall-clock latency, Playwright averaged 62.7s, OpenBrowser 77.0s, and Chrome DevTools 103.4s across all tasks, so OpenBrowser trades \\~14s of extra latency for 3.2x fewer API tokens (50K vs 159K vs 299K). Full methodology, per-task breakdowns, and raw data are published at [docs.openbrowser.me/comparison](http://docs.openbrowser.me/comparison.=)",
              "score": 3,
              "created_utc": "2026-02-22 03:18:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6pzrw6",
                  "author": "BC_MARO",
                  "text": "Thanks for sharing the numbers + writeâ€‘up. The perâ€‘task breakdown and raw data are super helpful. If you publish another pass, Iâ€™d love to see variance (p50/p90) and success/retry rates per task â€” those tend to matter as much as mean latency in practice.",
                  "score": 1,
                  "created_utc": "2026-02-22 04:44:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6o7viz",
          "author": "Crafty_Disk_7026",
          "text": "Absolutely love the codemode approach! Will have to try it.",
          "score": 2,
          "created_utc": "2026-02-21 21:59:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6omaec",
              "author": "BigConsideration3046",
              "text": "Thanks, let us know how we could improve it!",
              "score": 1,
              "created_utc": "2026-02-21 23:21:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6otjn5",
          "author": "thepreppyhipster",
          "text": "wow super fascinating!",
          "score": 2,
          "created_utc": "2026-02-22 00:06:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6pjz0j",
              "author": "BigConsideration3046",
              "text": "Thank you, let us know how we could make it better for the community",
              "score": 1,
              "created_utc": "2026-02-22 02:54:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6qparh",
          "author": "martinkogut",
          "text": "More information about WebMCP here\n\n\n\nhttps://hypescale.com/de/blog/webmcp-ki-agenten-browser-standard",
          "score": 2,
          "created_utc": "2026-02-22 08:28:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qswgq",
              "author": "BigConsideration3046",
              "text": "Thanks for bringing that up!",
              "score": 1,
              "created_utc": "2026-02-22 09:02:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6qu67o",
          "author": "louis8799",
          "text": "If the agent doesn't see the page dump, how does it know what python code to write?",
          "score": 2,
          "created_utc": "2026-02-22 09:15:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r1dxo",
              "author": "BigConsideration3046",
              "text": "Great question! The agent absolutely can see the page, it just requests exactly what it needs through Python code rather than receiving the entire accessibility tree automatically on every action. For example, it can execute\\_code browser.get\\_browser\\_state\\_summary() for a compact overview, use evaluate() to query specific DOM elements, or search the selector map for particular buttons or links. \n\nThe key difference is that OpenBrowser gives the agent control over how much detail it pulls per step, so instead of paying 120K+ tokens for a full Wikipedia page dump on every navigation, it might spend 100 tokens to grab just the infobox or a specific heading.",
              "score": 1,
              "created_utc": "2026-02-22 10:24:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ecd4u",
          "author": "ConsiderationIcy3143",
          "text": "Looks very interesting, I will try it soon! Thanks!",
          "score": 2,
          "created_utc": "2026-02-25 21:11:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lmdaa",
              "author": "BigConsideration3046",
              "text": "Thank you for your kind words, we really appreciate it! let us know how we could improve the open-source project to make it better for the community!",
              "score": 1,
              "created_utc": "2026-02-26 22:46:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ocwqb",
          "author": "Tetrylene",
          "text": "Checked out the main github page and this looks super interesting. I was considering using a local model with the Browser Use library, but this looks like it might be better to use directly instead due to the smaller token use.\n\nQuestion: will you support using LMstudio directly as an LLM provider? Or at least, do you have any recommendations for interfacing with LMstudio?",
          "score": 1,
          "created_utc": "2026-02-21 22:27:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6omp0f",
              "author": "BigConsideration3046",
              "text": "Thanks for checking out OpenBrowser! You can use LMStudio by passing its OpenAI-compatible endpoint directly to OpenBrowser's ChatOpenAI class with base_url=\"http://localhost:1234/v1\" and your loaded model name. OpenBrowser already supports 12+ providers including Ollama for local models, and any OpenAI-compatible server works the same way, so LMStudio fits right in.",
              "score": 1,
              "created_utc": "2026-02-21 23:23:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6p8ey3",
          "author": "iamhuwng",
          "text": "!RemindMe 2 weeks",
          "score": 1,
          "created_utc": "2026-02-22 01:38:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6p8jxc",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 14 days on [**2026-03-08 01:38:51 UTC**](http://www.wolframalpha.com/input/?i=2026-03-08%2001:38:51%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/mcp/comments/1rb2tme/openbrowser_mcp_give_your_ai_agent_a_real_browser/o6p8ey3/?context=3)\n\n[**4 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fmcp%2Fcomments%2F1rb2tme%2Fopenbrowser_mcp_give_your_ai_agent_a_real_browser%2Fo6p8ey3%2F%5D%0A%0ARemindMe%21%202026-03-08%2001%3A38%3A51%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201rb2tme)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-02-22 01:39:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6q1m4b",
          "author": "Fun-Pirate192",
          "text": "I use Browserless for many projects; why should I consider this instead?",
          "score": 1,
          "created_utc": "2026-02-22 04:58:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qfls8",
              "author": "BigConsideration3046",
              "text": "Browserless is great for headless browser infrastructure, and they solve different problems. Browserless gives you managed browsers for traditional automation scripts (Puppeteer/Playwright), while OpenBrowser is an MCP server built specifically for AI agents, so your LLM writes Python code and only gets back the data it actually needs instead of full page dumps (144x smaller responses, 3-6x fewer tokens in our benchmarks, see the full comparison here: [https://docs.openbrowser.me/comparison](https://docs.openbrowser.me/comparison)  ). If you're building AI agents that need to browse the web, OpenBrowser can sit alongside or even connect through the same browser instances, just with dramatically lower token costs.",
              "score": 2,
              "created_utc": "2026-02-22 06:56:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6qfe6d",
          "author": "invertednz",
          "text": "The reduction in tokens seems impressive but I can only see a comparison on 5 pretty simple tasks, how does it go on some of the browser benchmarks with more tasks?",
          "score": 1,
          "created_utc": "2026-02-22 06:54:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qwxgm",
              "author": "BigConsideration3046",
              "text": "Great question, and totally fair feedback! Our current published benchmark actually covers 6 tasks (fact lookup, form fill, multi-page extraction, search and navigation, deep navigation, and content analysis), each run 5 times with bootstrap (10,000 times) confidence intervals to ensure statistical reliability (See this comparison [https://docs.openbrowser.me/comparison](https://docs.openbrowser.me/comparison) and raw result here [https://github.com/billy-enrizky/openbrowser-ai/blob/main/benchmarks/e2e\\_llm\\_stats\\_results.json](https://github.com/billy-enrizky/openbrowser-ai/blob/main/benchmarks/e2e_llm_stats_results.json) ). \n\nWe're actively working on expanding the suite with more complex, multi-step scenarios, and we'd love to hear what specific tasks or benchmarks you'd find most convincing. Feel free to open an issue or drop a suggestion!",
              "score": 1,
              "created_utc": "2026-02-22 09:42:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ungx1",
          "author": "PricePerGig",
          "text": "I just watched a video and read the anthropic post at code vs pure mcp  Will try it out. Thanks for posting",
          "score": 1,
          "created_utc": "2026-02-22 22:18:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wfazx",
              "author": "BigConsideration3046",
              "text": "Thanks for checking it out! Let us know how we could make it a better open-source project for the community!",
              "score": 1,
              "created_utc": "2026-02-23 04:42:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6v92w2",
          "author": "Ethan",
          "text": "This is only for Claude Code?",
          "score": 1,
          "created_utc": "2026-02-23 00:18:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wdt3x",
              "author": "BigConsideration3046",
              "text": "Not at all! OpenBrowser works as a standalone Python library with 16+ LLM providers (OpenAI, Google Gemini, Groq, Ollama, etc.), as an MCP server for Claude Desktop, Cursor, Windsurf, Cline, and any MCP-compatible client, and it also has dedicated integrations for OpenAI Codex, OpenCode, and OpenClaw. The Claude Code plugin is just one of many ways to use it, you can also just pip install openbrowser-ai and use it directly in your Python scripts with any LLM provider you prefer.",
              "score": 1,
              "created_utc": "2026-02-23 04:31:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6weue2",
                  "author": "Ethan",
                  "text": "Ok, that's what I thought, I didn't see anything limiting. The repo says that Claude Code is a prerequisite though.",
                  "score": 1,
                  "created_utc": "2026-02-23 04:38:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6wf7u8",
          "author": "Someoneoldbutnew",
          "text": "bro, I can't do shit on my authenticated websites with a hosted solution. try again.Â ",
          "score": 1,
          "created_utc": "2026-02-23 04:41:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75vdz0",
              "author": "BigConsideration3046",
              "text": "Thanks for bringing this up, that is why in our hosted solution, we have an option to open in your browser, hence, it uses your browser profile and cookies!",
              "score": 1,
              "created_utc": "2026-02-24 16:40:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75zij5",
                  "author": "Someoneoldbutnew",
                  "text": "even better, so I'm handing over the keys to my digital kingdom to you?",
                  "score": 1,
                  "created_utc": "2026-02-24 16:59:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7axt5f",
          "author": "Sk_programs",
          "text": "What about the high token burn issue . Did you consider and solve it?",
          "score": 1,
          "created_utc": "2026-02-25 10:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7e5m9q",
              "author": "BigConsideration3046",
              "text": "Thanks for asking, as provided above, it is 3.2x fewer token comparing to Playwright MCP and 6x fewer token comparing to Chrome DevTools MCP.\n\n  \nThe full methodology as well as the complete task description were given above:  \n[https://docs.openbrowser.me/comparison](https://docs.openbrowser.me/comparison)\n\nThe raw result file in JSON is also provided in the open-source GitHub repo:  \n[https://github.com/billy-enrizky/openbrowser-ai/blob/main/benchmarks/e2e\\_llm\\_stats\\_results.json](https://github.com/billy-enrizky/openbrowser-ai/blob/main/benchmarks/e2e_llm_stats_results.json)",
              "score": 1,
              "created_utc": "2026-02-25 20:40:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7az8v3",
          "author": "AgitatedDoctor9613",
          "text": "# Constructive Feedback\n\n**Strengths acknowledged:** The token efficiency claims are compelling and the single-tool approach is genuinely simpler than multi-tool alternatives.\n\n**Areas for improvement:** (1) The benchmarks lack transparencyâ€”what specific tasks were tested, and were they representative of real n8n workflows? Include methodology details or link to reproducible results. (2) You haven't addressed failure modes: what happens when Python execution times out, crashes, or needs to handle dynamic content/JavaScript rendering? Edge cases like authentication, CAPTCHA, or rate-limiting deserve mention. (3) Consider positioning this against actual n8n use cases (data scraping, form automation, API integration) rather than generic \"browsing\"â€”that would make the value proposition concrete for your audience.",
          "score": 1,
          "created_utc": "2026-02-25 10:41:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7e5cxm",
              "author": "BigConsideration3046",
              "text": "Thanks for your constructive feedback,\n\nThe full methodology as well as the complete task description were given above:  \n[https://docs.openbrowser.me/comparison](https://docs.openbrowser.me/comparison)\n\nThe raw result file in JSON is also provided in the open-source GitHub repo:  \n[https://github.com/billy-enrizky/openbrowser-ai/blob/main/benchmarks/e2e\\_llm\\_stats\\_results.json](https://github.com/billy-enrizky/openbrowser-ai/blob/main/benchmarks/e2e_llm_stats_results.json)",
              "score": 1,
              "created_utc": "2026-02-25 20:38:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rembfo",
      "title": "I generated CLIs from MCP servers and cut token usage by 94%",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rembfo/i_generated_clis_from_mcp_servers_and_cut_token/",
      "author": "QThellimist",
      "created_utc": "2026-02-25 18:58:14",
      "score": 138,
      "num_comments": 40,
      "upvote_ratio": 0.94,
      "text": "MCP server schemas eat so much token. So I built a converter that generates CLIs from MCP servers. Same tools, same OAuth, same API underneath. The difference is how the agent discovers them:\n\nMCP: dumps every tool schema upfront (\\~185 tokens \\* 84 tools = 15,540 tokens)\nCLI: lightweight list of tool names (\\~50 tokens \\* 6 CLIs = 300 tokens). Agent runs --help only when it needs a specific tool.\n\nNumbers across different usage patterns:\n- Session start: 15,540 (MCP) vs 300 (CLI) - 98% savings\n- 1 tool call: 15,570 vs 910 - 94% savings\n- 100 tool calls: 18,540 vs 1,504 - 92% savings\n\nCompared against Anthropic's Tool Search too - it's better than raw MCP but still more expensive than CLI because it fetches full JSON Schema per tool.\n\nConverter is open source: https://github.com/thellimist/clihub\nFull write-up with detailed breakdowns: https://kanyilmaz.me/2026/02/23/cli-vs-mcp.html\n\nDisclosure: I built CLIHub. Happy to answer questions about the approach.",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1rembfo/i_generated_clis_from_mcp_servers_and_cut_token/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7e45aw",
          "author": "nightman",
          "text": "How it compares to (is it inspired by) the mcporter from OpenClaw author?\nhttps://github.com/steipete/mcporter",
          "score": 11,
          "created_utc": "2026-02-25 20:33:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7eb6yr",
              "author": "QThellimist",
              "text": "I found out mcporter after I started using mine.\n\nBut the high level difference is\n\n\\- MCPorter is designed more for openclaw to call MCPs more easily. It's in JS, so has runtime  \n\\- CLIHub is written in go. Works on all platforms. Faster. It's designed as pure CLI. No deamon, no bun runtime.\n\nThere are small architecture differences but not that important\n\nI might create a full directory like mcppulse etc. where people can download  any CLI on any machine with single command.",
              "score": 12,
              "created_utc": "2026-02-25 21:05:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ec0rl",
                  "author": "nightman",
                  "text": "Mcporter creates executable of MCP server so it's not needed afterwards.\n\nThanks for the explanation",
                  "score": 3,
                  "created_utc": "2026-02-25 21:09:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7e8upk",
              "author": "Casual_Hearthstone",
              "text": "Was going to ask the same question",
              "score": 4,
              "created_utc": "2026-02-25 20:55:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7fe9bd",
          "author": "BC_MARO",
          "text": "The first-token pollution point is the real issue - dumping 15k tokens of schema at position 0 wastes your most valuable context slots before the agent even starts reasoning.",
          "score": 5,
          "created_utc": "2026-02-26 00:24:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dye95",
          "author": "BraveNewKnight",
          "text": "Main CLI benchmark gap is exploration overhead: the agent has to discover commands, make wrong attempts, and retry, and those loops should count toward total tokens.\n\nCLI skills layered on top add extra prompt/context cost too, so that should be in the numbers.\n\nAlso, the GitHub link returns 404 for me.",
          "score": 6,
          "created_utc": "2026-02-25 20:05:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7dzco6",
              "author": "QThellimist",
              "text": "True, but it's actually underrepresented. My agent calls like\n\nâº Bash(linear --help 2>&1 | grep -i -E \"search|list.\\*issue|get.\\*issue\")\n\nSo it doesn't actually get the whole \\`--help\\` list.  \n\\--\n\nFixed the github",
              "score": 2,
              "created_utc": "2026-02-25 20:10:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7e036f",
                  "author": "BraveNewKnight",
                  "text": "yeah that makes sense, but still, agent needs to know that it should grep for those keywords to get the right result. I'm not against CI or I'm not an MCP fan, it's just not clear to me which one is better atm. \n\ne.g. I'm still struggling to measure if agent does a better job with `agent-browser` CLI or `playwright` MCP.",
                  "score": 4,
                  "created_utc": "2026-02-25 20:13:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7fftk5",
          "author": "actual-time-traveler",
          "text": "FastMCP 3.0 does this natively",
          "score": 3,
          "created_utc": "2026-02-26 00:32:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hw85g",
              "author": "Etyr_",
              "text": "Could you share any doc about this, not finding any ressource on this",
              "score": 2,
              "created_utc": "2026-02-26 11:24:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7i0ho0",
                  "author": "jlowin123",
                  "text": "https://gofastmcp.com/clients/generate-cli",
                  "score": 3,
                  "created_utc": "2026-02-26 11:58:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7gk9w5",
          "author": "KobyStam",
          "text": "I include CLIs in my MCPs - so far I released the NotebookLM MCP, but a few more are coming soon, like Gemini Web Chat MCP & CLI and Perplexity Web MCP& CLI...and even Grok. None of them uses APIs or browser automation. Same concept as my NotebookLM (RPC over HTTP)\n\n  \nNotebookLM MCP: [https://github.com/jacob-bd/notebooklm-mcp-cli](https://github.com/jacob-bd/notebooklm-mcp-cli)",
          "score": 3,
          "created_utc": "2026-02-26 04:29:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7eb04m",
          "author": "warren-mann",
          "text": "Interesting. Though Anthropic and Google cache prompt and heavily discount on cache hits. Itâ€™s true that the tool definitions still take up context but Iâ€™m not convinced itâ€™s enough to matter, at least anymore. The approach Iâ€™ve settled on is a rich set of tools at a top-level prompt that knows about them all and can delegate specific tasks to a more targeted subordinate with a very restricted set of tools and a relatively clean context.\n\nHaving said that, Iâ€™m always looking for ways to wring out more efficiency and you have some interesting stuff to think about.",
          "score": 2,
          "created_utc": "2026-02-25 21:05:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ecqr1",
              "author": "QThellimist",
              "text": "I think you are assuming as \"price\", and yes it doesn't matter for most people. Difference is a few $s per month. For heavy users it's $100s where matters more (I am heavy user. I spent $900 on tokens literally last weekend)\n\nBut the real difference is - first tokens has more dominence over tokens that come later.\n\nSo you are bloating the context immediately (regardless of cache input token or not). \n\nLLMs perform significantly worse if first tokens are poorly used. ",
              "score": 2,
              "created_utc": "2026-02-25 21:13:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7hw1n2",
          "author": "Weird-Guarantee-1823",
          "text": "I looked at the introduction document, which is very interesting, and I feel that it is similar to the design point of skills. In terms of data, this does save a lot of tokens, but can it achieve the processing effect of the existing mainstream scheme? Will there be any common problems similar to those encountered in skills? However, no matter what, it seems that this is indeed a very cost-effective solution, I will go back and try it, thank you for your dedication.",
          "score": 2,
          "created_utc": "2026-02-26 11:23:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dvjrp",
          "author": "-Akos-",
          "text": "404 github not found. Also, you mention CLI as an alternative, but can any model just use the CLI? I can make a tiny local llm call an MCP without issues  but I have no idea how I can make it call a CLI.",
          "score": 1,
          "created_utc": "2026-02-25 19:52:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7dz21g",
              "author": "QThellimist",
              "text": "fixed it",
              "score": 1,
              "created_utc": "2026-02-25 20:09:04",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7dz5t0",
              "author": "QThellimist",
              "text": "If they have bash tool they can use. \n\nMost bigger AI models have bash tool access",
              "score": 1,
              "created_utc": "2026-02-25 20:09:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7e62xq",
          "author": "Distinct-Selection-1",
          "text": "Is this the same with MCP v3 skills?",
          "score": 1,
          "created_utc": "2026-02-25 20:42:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ebzjo",
              "author": "QThellimist",
              "text": "first time heard it. FastMCP seems to have many functionalities including CLI. I haven't checked deeply yet.",
              "score": 2,
              "created_utc": "2026-02-25 21:09:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7fy8wa",
          "author": "DorkyMcDorky",
          "text": "If MCP only supported REAL streaming none of this would be necessary.  Shake 'em up and suggest this.  The protocol is painfully inefficient.",
          "score": 1,
          "created_utc": "2026-02-26 02:17:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hiqv8",
              "author": "Material-Spinach6449",
              "text": "Iâ€™ve looked into MCP vs CLI as well, and I think the â€œinitial token dumpâ€ argument is often overstated.\n\nThe huge upfront cost with MCP mainly happens if the agent blindly loads every tool schema into context. Thatâ€™s not mandatory. Agents can fetch tool definitions incrementally and only load what they actually need. In that setup, the claimed massive startup savings of CLI donâ€™t automatically apply.\n\nWhere CLI really has a structural advantage is in looping scenarios. If a tool needs to be called repeatedly, the classic MCP flow forces the model to re-plan and re-emit structured calls every time. That quickly becomes expensive and slow. With a CLI, the agent can generate a small script and execute the loop outside the model. In those cases, CLI is genuinely cheaper and faster.",
              "score": 2,
              "created_utc": "2026-02-26 09:20:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ilvbe",
                  "author": "DistanceAlert5706",
                  "text": "Not a fan of MCP but this is what agents should do, loading 84 tools just sounds crazy to me, why not do specialized agents which use specific tool sets.",
                  "score": 2,
                  "created_utc": "2026-02-26 14:10:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7mua3b",
                  "author": "DorkyMcDorky",
                  "text": "If it streamed for real (it's session/token based - very 1999 design) then it would reduce a ton of memory and not need to reload that everytime.  Anthropic wants maximum usage, not efficiency, so they wont change it.  ",
                  "score": 1,
                  "created_utc": "2026-02-27 02:52:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7hbnx2",
          "author": "TeeRKee",
          "text": "Isnâ€™t that the point of skills ?",
          "score": 1,
          "created_utc": "2026-02-26 08:11:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ie5vo",
              "author": "QThellimist",
              "text": "There is overlap but not really. \n\nYou don't want random hardcoded skill. You want official CLI or MCP where you can trust and it gets updated",
              "score": 1,
              "created_utc": "2026-02-26 13:28:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7hikw9",
          "author": "New-Procedure8239",
          "text": "This is MCPporter that use openclaw I think",
          "score": 1,
          "created_utc": "2026-02-26 09:18:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ie8k7",
              "author": "QThellimist",
              "text": "check this comment - [https://www.reddit.com/r/mcp/comments/1rembfo/comment/o7eb6yr/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/mcp/comments/1rembfo/comment/o7eb6yr/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",
              "score": 2,
              "created_utc": "2026-02-26 13:28:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7koba4",
          "author": "Siref",
          "text": "There's something that throws me off.\n\nWhy is the CLI option cheaper if the out is XML?\nJSON is more compact, so it should have less tokens. \n\nFrom a quick glance from the post you shared it seems the CLI shares less information with the agent (E.g: I don't see the queryParams entries)\n\nIf that's the case, wouldn't it make more sense to compact the MCP definition instead?",
          "score": 1,
          "created_utc": "2026-02-26 20:01:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7oorbu",
          "author": "groosha",
          "text": "Could you please explain how it works? Let's say I generated CLI from my MCP server. What happens next?",
          "score": 1,
          "created_utc": "2026-02-27 11:43:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pfmm3",
              "author": "QThellimist",
              "text": "you get ./out/mycli as a response. And need to tell agent to use it. \n\nI personally have my agents automatically move it to bin/ folder and add a simple text in [AGENTS.md](http://AGENTS.md) so any new session is aware of it\n\nI have a command for it that literally says\n\n    - Check if official CLI exists. If so, download it and add it to ~/.codex/AGENTS.md tools\n    - If not, check official MCP exists. If so, use clihub `go run github.com/thellimist/clihub@latest <server>` (see `--help`). to create a CLI from the MCP and move the executable to bin folder. Then add it to ~/.codex/AGENTS.md tools\n    ",
              "score": 1,
              "created_utc": "2026-02-27 14:31:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7pzahd",
                  "author": "groosha",
                  "text": "I'm sorry, I'm quite new to agents yet. Do you specifically state in the system prompt to use cli tools from your \\`bin\\` folder?",
                  "score": 1,
                  "created_utc": "2026-02-27 16:08:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1ravtpv",
      "title": "The first non-trivial demo of WebMCP",
      "subreddit": "mcp",
      "url": "https://v.redd.it/s6fqpf2flvkg1",
      "author": "No_Guide_8697",
      "created_utc": "2026-02-21 16:44:14",
      "score": 137,
      "num_comments": 30,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1ravtpv/the_first_nontrivial_demo_of_webmcp/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6qkade",
          "author": "nucleustt",
          "text": "Wow. A beautiful demo of something other than making bookings and scheduling appointments!",
          "score": 7,
          "created_utc": "2026-02-22 07:40:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6mjgz9",
          "author": "BC_MARO",
          "text": "This is a great demo.\n\nOne question though: whatâ€™s the security model for WebMCP? Like, when a site exposes tools, do you have a way to scope them per-origin / per-session and show the user an audit trail of tool calls?\n\nAlso curious how youâ€™re thinking about compatibility with â€œregularâ€ MCP servers (bridge/proxy so agents can hit both without special casing).",
          "score": 8,
          "created_utc": "2026-02-21 16:50:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mo0gs",
              "author": "No_Guide_8697",
              "text": "WebMCP relies on the browser's native security model. Because theÂ tools execute directly within the client's browser environment, they automatically inherit the user's current session context, cookies, and origin-specific permissions, i.e., the AI agent cannot bypass origin boundaries (CORS) or access unauthorized data because the execution is constrained by the browser's Same-Origin Policy.\n\nSimilarly, ToolÂ exposure is inherently tied to the active session; if the user logs out or the session expires, the tools lose access to authenticated actions just like a normal user would.",
              "score": 8,
              "created_utc": "2026-02-21 17:13:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6mp616",
                  "author": "BC_MARO",
                  "text": "Got it - the CORS enforcement makes sense as the primary isolation boundary. The missing piece for me is auditability: if a tool performs an action in-session, is there currently a log of what got called and what data it touched, or does the audit layer have to come from outside the browser?",
                  "score": 1,
                  "created_utc": "2026-02-21 17:19:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6moyaj",
              "author": "No_Guide_8697",
              "text": "The audit trail is something that can be added easily by intercepting the tool calls and storing them in logs before the actual function is executed. However, there are also providers that have LLM observability, so you can track requests and tool calls made in the request scoped by session-ids there.\n\nI'm still trying to figure out what the best way would be to bridge regular and Web servers. Will update here once I have some structured thoughts on this :)",
              "score": 2,
              "created_utc": "2026-02-21 17:17:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6n8p8a",
          "author": "naseemalnaji-mcpcat",
          "text": "Holy shit this is awesome",
          "score": 4,
          "created_utc": "2026-02-21 18:55:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nbzhq",
          "author": "Asleep-Land-3914",
          "text": "Here is the fork to try with local models: [https://github.com/OEvgeny/music-composer-webmcp-local](https://github.com/OEvgeny/music-composer-webmcp-local)",
          "score": 3,
          "created_utc": "2026-02-21 19:12:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ng1st",
              "author": "Asleep-Land-3914",
              "text": "Reddit post with more details: [https://www.reddit.com/r/LocalLLaMA/comments/1rb054k/made\\_webmcp\\_music\\_composer\\_demo\\_to\\_be\\_able\\_to/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1rb054k/made_webmcp_music_composer_demo_to_be_able_to/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",
              "score": 2,
              "created_utc": "2026-02-21 19:32:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6nfxww",
              "author": "Asleep-Land-3914",
              "text": "Demo here: [https://oevgeny-music-compos-epfx.bolt.host/](https://oevgeny-music-compos-epfx.bolt.host/)",
              "score": 1,
              "created_utc": "2026-02-21 19:32:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ngmhp",
              "author": "No_Guide_8697",
              "text": "Would love to hear which local models perform the best!!",
              "score": 1,
              "created_utc": "2026-02-21 19:35:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6nhhqg",
                  "author": "Asleep-Land-3914",
                  "text": "I only tried with Qwen3-Coder-30B-A3B-Instruct-IQ3\\_S-3.12bpw.gguf so far and it did pretty well.",
                  "score": 1,
                  "created_utc": "2026-02-21 19:40:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6pzi6n",
          "author": "Classic_Reference_10",
          "text": "Didn't quite get this. What exactly are you doing here and what is a end-user use case that you're solving with this?  \nAlso, how is WebMCP being used in this?",
          "score": 3,
          "created_utc": "2026-02-22 04:42:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rmy0m",
              "author": "No_Guide_8697",
              "text": "Hey, so an LLM (GPT 5.2) in the demo, is directly making tool calls to our website to interact with it. Until now, agents using browser meant taking screenshots, or parsing complex and heavy DOM objects which was unreliable and token inefficient. With WebMCP, people can easily declare MCP Tools in their website itself that makes things unified, and much more reliable and efficient. As you can see in the demo, the agent made 100+ tool calls directly to our website, which has tools exposed using WebMCP and composed a music piece with 0% error rate in tools calls, meaning it does the right thing on website instead of clicking random buttons.",
              "score": 1,
              "created_utc": "2026-02-22 13:24:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ronld",
                  "author": "Classic_Reference_10",
                  "text": "Thanks for the response. What is your website? And what tool does it host? How are these tools declared in the DOM?",
                  "score": 3,
                  "created_utc": "2026-02-22 13:35:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6s9nb8",
          "author": "fxj",
          "text": "will it also work on perplexity comet? what does it need?\n\n",
          "score": 2,
          "created_utc": "2026-02-22 15:29:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75sljs",
              "author": "No_Guide_8697",
              "text": "It works on every browser, just open it, we have connected our own AI Gateway to it for you to use it for free :)",
              "score": 1,
              "created_utc": "2026-02-24 16:28:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75stqa",
                  "author": "No_Guide_8697",
                  "text": "once there's wider support for webmcp in browsers, we won't have to integrate an agent ourselves, and agents, say from perplexity comet will be able to use your website directly using the WebMCP protocol",
                  "score": 1,
                  "created_utc": "2026-02-24 16:29:18",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o71n2dy",
          "author": "Just_Oil_2162",
          "text": "Wait, this is lwk fireðŸ”¥",
          "score": 2,
          "created_utc": "2026-02-23 23:50:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6q743v",
          "author": "No_Guide_8697",
          "text": "Someone prompted Marry had a Little Lamb but stopped at bass and chord. Check it out - [WebMCP Music Composer](https://music.leanmcp.live/?id=SHFZ9RBb)",
          "score": 1,
          "created_utc": "2026-02-22 05:42:13",
          "is_submitter": true,
          "replies": [
            {
              "id": "o6q782o",
              "author": "No_Guide_8697",
              "text": "If my school had to play Mary had a Little Lamb, this is what it would sound like lol\n\n",
              "score": 1,
              "created_utc": "2026-02-22 05:43:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rhlwz",
          "author": "ritoromojo",
          "text": "This is neat! This might be a stupid question but what specifically is WebMCP doing? Is it just a way of defining tools such that it appears in the DOM under a specific header that agents are supposed to look for and toggle?\n\nAlso, does webMCP require your agent to be a browser agent that is already connected with something like playwright MCP or browser-use? \n\nThis seems like a really good example demonstrating it so id love to understand the implementation surface since there doesn't seem to be any good docs for it at the moment",
          "score": 1,
          "created_utc": "2026-02-22 12:47:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ro316",
              "author": "No_Guide_8697",
              "text": "Hey, that's a very valid question! No, WebMCP is not putting anything in your DOM for the agent to look for. Your website registers specific functions as tools using WebMCP, and the AI agent directly makes tool calls to it as if it's a 'normal' MCP. The agent does not need to have knowledge that it is directly manipulating a website. This removes the need for playright MCP or browser-use, because the agent now only needs to how to use the exposed tools, which it can do reliably in most cases if the descriptions and inputSchemas are well-defined.",
              "score": 1,
              "created_utc": "2026-02-22 13:31:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rogb0",
                  "author": "No_Guide_8697",
                  "text": "you can check out our source code to understand it better (linked in the post) or check out the W3C Community draft [WebMCP](https://webmachinelearning.github.io/webmcp/).",
                  "score": 1,
                  "created_utc": "2026-02-22 13:33:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o71n7b2",
          "author": "Just_Oil_2162",
          "text": "Are you guys using CDNs directly or playwright??",
          "score": 1,
          "created_utc": "2026-02-23 23:51:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rejf15",
      "title": "Tesseract â€” MCP server that turns any codebase into a 3D architecture diagram",
      "subreddit": "mcp",
      "url": "https://v.redd.it/q3fbwe7fcolg1",
      "author": "DvidGeekoh",
      "created_utc": "2026-02-25 17:19:21",
      "score": 81,
      "num_comments": 13,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1rejf15/tesseract_mcp_server_that_turns_any_codebase_into/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7dbk28",
          "author": "lardgsus",
          "text": "\"but there is already a tesseract app\"",
          "score": 4,
          "created_utc": "2026-02-25 18:21:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hdupj",
          "author": "BC_MARO",
          "text": "Point AI at it before onboarding a new dev and you just killed the walk me through the codebase meeting. Actually clever use of the visual layer.",
          "score": 2,
          "created_utc": "2026-02-26 08:32:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kv5k3",
              "author": "DvidGeekoh",
              "text": "Didn't think about the meeting, but that's a good point!",
              "score": 1,
              "created_utc": "2026-02-26 20:34:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ghy80",
          "author": "turtleisinnocent",
          "text": "Source?",
          "score": 1,
          "created_utc": "2026-02-26 04:14:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kvjpe",
              "author": "DvidGeekoh",
              "text": "Hopefully soon â€” I just need the project to pay my rent first :)  \n",
              "score": 1,
              "created_utc": "2026-02-26 20:36:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7glkkw",
          "author": "exboozeme",
          "text": "Download macOS Tahoe: Tesseract is damaged and canâ€™t be opened - tried a few times with command right click also. Would love to see it!",
          "score": 1,
          "created_utc": "2026-02-26 04:38:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kupu9",
              "author": "DvidGeekoh",
              "text": "Hi, I'm sorry to hear that. I'll try to get a hand on a mac to sort this out. I'll keep you posted.",
              "score": 1,
              "created_utc": "2026-02-26 20:32:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7me7nb",
                  "author": "exboozeme",
                  "text": "Thanks!!",
                  "score": 1,
                  "created_utc": "2026-02-27 01:19:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7psu21",
          "author": "dimitrifp",
          "text": "I really like it, I have to say. I think your business will be booming.",
          "score": 1,
          "created_utc": "2026-02-27 15:37:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pyyqz",
              "author": "DvidGeekoh",
              "text": "Thank you ! (fingers crossed)",
              "score": 1,
              "created_utc": "2026-02-27 16:06:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ghewh",
          "author": "FigAltruistic2086",
          "text": "My first thought was, â€œOh, cool â€” an MCP for Tesseract OCR. How does it work?â€",
          "score": 0,
          "created_utc": "2026-02-26 04:10:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rc2nqw",
      "title": "Stop writing API MCPs. Just use Earl.",
      "subreddit": "mcp",
      "url": "https://github.com/brwse/earl",
      "author": "Accomplished-Emu8030",
      "created_utc": "2026-02-23 00:29:46",
      "score": 76,
      "num_comments": 10,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1rc2nqw/stop_writing_api_mcps_just_use_earl/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6vf57c",
          "author": "ticktockbent",
          "text": "Seems interesting, sort of a secure key handling layer?",
          "score": 2,
          "created_utc": "2026-02-23 00:52:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6vfktx",
              "author": "Accomplished-Emu8030",
              "text": "Yeah. There are a lot of security features inside :)",
              "score": 1,
              "created_utc": "2026-02-23 00:55:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6vo0ug",
          "author": "No_Inevitable6372",
          "text": "Not heard of this tool before it looks really useful. Weâ€™re testing the build out of an MCP for our existing APIs, so this has come at a good time. Feels like a good conceptual shift from â€œwrap and mcp around the apiâ€ to â€œwrite an mcp to accomplish tasks via the apiâ€. After all thats why we build the api!",
          "score": 2,
          "created_utc": "2026-02-23 01:46:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6voje2",
              "author": "Accomplished-Emu8030",
              "text": "\\+1 Please build your API like normal and just use Earl to wrap it. You will get MCP and Agent Skill-compatibility for free.",
              "score": 3,
              "created_utc": "2026-02-23 01:49:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6vzpeu",
          "author": "vulgrin",
          "text": "I mean, I use Claude Code all day long to mange git issues, projects, PRs and more. And I donâ€™t use a single MCP, just a skill and some scripts.  It feels to me that an MCP would be overkill.",
          "score": 3,
          "created_utc": "2026-02-23 02:57:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6w47ry",
              "author": "Accomplished-Emu8030",
              "text": "Since this is r/mcp I didn't really emphasize how earl is a CLI specifically for agents, but earl shines as a CLI.",
              "score": 1,
              "created_utc": "2026-02-23 03:26:05",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6x1d6b",
              "author": "x360d",
              "text": "What skill are you using?",
              "score": 1,
              "created_utc": "2026-02-23 07:48:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6xnzip",
                  "author": "vulgrin",
                  "text": "My own. That I had Claude code write. Itâ€™s not hard to just prompt new skills you need. Just review the output",
                  "score": 1,
                  "created_utc": "2026-02-23 11:26:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6y5kk9",
          "author": "DerixSpaceHero",
          "text": "> Example: calling github.create_issue is rarely useful. The useful output isnâ€™t just \"issue created.\" Itâ€™s: what should happen next? Should we attach labels? Assign an owner? Post to Slack? Link it to a PR? Create a followâ€‘up task? Ask for missing context?\n\nAre you a shill or are you actually familiar with the MCP spec? Prompts and elicitations exist. Figma's MCP uses them extremely well - good inspiration for solving literally the exact problem you're describing without relying on a rando project.",
          "score": -1,
          "created_utc": "2026-02-23 13:32:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yrxsq",
              "author": "Accomplished-Emu8030",
              "text": "1. Prompts are a user-side feature. I'm not sure how this would matter in fully automated work.\n2. Elicitations are promising, but theyâ€™re still pretty new and (in practice) usually imply HITL. We haven't gotten around to this yet.\n\nAlso, Earl is first-and-foremost a CLI surface an agent can use safely. Weâ€™re focused on agent skills + gating unsafe CLI behaviors (permissions, sandboxing, retries, guardrails, etc.). MCP support is additive because itâ€™s a nice interface layer, not the whole product.\n\nAlso: MCP is a protocol. If the argument is \"make your MCP special by embedding workflows,\" at that point youâ€™re basically building an application-specific service anyway which you can still do with a normal API.\n\nAnd Figmaâ€™s MCP is genuinely good inspiration, but in general MCP is pretty obsolete in comparison to agent skills (which basically has both prompts and elicitation built-in; try the superpowers agent skill for example).",
              "score": 2,
              "created_utc": "2026-02-23 15:32:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rfwlk4",
      "title": "ApiTap â€“ Capture any website's internal API, replay it without a browser",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rfwlk4/apitap_capture_any_websites_internal_api_replay/",
      "author": "nibynikt",
      "created_utc": "2026-02-27 04:15:11",
      "score": 63,
      "num_comments": 18,
      "upvote_ratio": 1.0,
      "text": "I kept burning 200K tokens every time my AI agent browsed a webpage â€” launching Chrome, rendering the DOM, converting to markdown, feeding it to the LLM. The data I actually needed was already there in structured JSON, one layer below the HTML. So I built **ApiTap** to skip the browser and call the API directly.\n\nApiTap captures a site's internal API calls via Chrome DevTools Protocol and saves them as replayable \"skill files.\" After one capture, your agent (or a cron job, or a CLI script) calls the API with `fetch()` â€” no browser needed.\n\n# Built-in decoders (no browser needed)\n\n|Site|ApiTap|Raw HTML|Savings|\n|:-|:-|:-|:-|\n|Reddit|\\~630 tokens|\\~125K tokens|99.5%|\n|Wikipedia|\\~130 tokens|\\~69K tokens|99.8%|\n|Hacker News|\\~200 tokens|\\~8.6K tokens|97.7%|\n|TradingView|\\~230 tokens|\\~245K tokens|99.9%|\n\nPlus YouTube, Twitter/X, DeepWiki, and a generic fallback. Average savings: **74% across 83 tested domains.**\n\n# Three ways to use it\n\n* **MCP server** â€” 12 tools, works with Claude Code/Desktop, Cursor, Windsurf, VS Code\n* **CLI** â€” `apitap read <url> --json | jq '.title'`\n* **npm package** â€” three direct runtime deps, zero telemetry\n\n# Quick start\n\n    npm install -g @apitap/core\n    apitap read https://news.ycombinator.com/\n\nFor MCP (Claude Code):\n\n    claude mcp add -s user apitap -- apitap-mcp\n\n# Security\n\nThis matters because the tool makes HTTP requests on behalf of AI agents. SSRF defense at 4 checkpoints (import, replay, post-DNS, post-redirect). Private IPs, cloud metadata, localhost all blocked. DNS rebinding caught. Auth encrypted with AES-256-GCM, per-install salt, never stored in skill files. **789 tests** including a full security suite. Designed after reading [Google's GTIG report on MCP attack surfaces](https://cloud.google.com/blog/topics/threat-intelligence/distillation-experimentation-integration-ai-adversarial-use).\n\nApiTap calls the same endpoints your browser calls â€” read-only, no rate-limit bypassing, no anti-bot circumvention. Endpoints that require signing or Cloudflare are flagged as \"red tier,\" not attacked.\n\n# Links\n\n* **Site:** [apitap.io](https://apitap.io)\n* **GitHub:** [github.com/n1byn1kt/apitap](https://github.com/n1byn1kt/apitap)\n* **npm:** [@apitap/core](https://www.npmjs.com/package/@apitap/core)\n\n# License\n\nBSL 1.1 (source-available) â€” free for any use except reselling as a competing hosted service. Converts to Apache 2.0 in Feb 2029.\n\nHappy to answer questions. Try `apitap read` on your favorite site and let me know what breaks.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1rfwlk4/apitap_capture_any_websites_internal_api_replay/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7n8gl4",
          "author": "BC_MARO",
          "text": "The capture/replay idea is great, but auth refresh and per-user cookies are usually the hard part. How are you handling token renewal and multi-account isolation in the skill files?",
          "score": 4,
          "created_utc": "2026-02-27 04:20:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7n8v4b",
              "author": "nibynikt",
              "text": "Good question â€” this is one of the trickier parts. Currently skill files store encrypted session tokens (AES-256-GCM, per-install salt) but don't handle automatic renewal. When a token expires, apitap capture re-runs the auth flow â€” you log in once, it re-captures. For multi-account isolation, each capture session is namespaced by domain + account alias, so skill files don't bleed across accounts.\n\n\n\nToken renewal automation (refresh token interception + replay) is on the roadmap â€” it's the right next step for production use cases. What's your use case? Curious whether you're hitting this with OAuth flows or session cookies specifically.",
              "score": 7,
              "created_utc": "2026-02-27 04:23:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ncfu6",
                  "author": "BC_MARO",
                  "text": "Mostly OAuth flows - the tokens expire mid-session and re-running capture is not great for automated agents. Refresh token interception would be a big unlock.",
                  "score": 1,
                  "created_utc": "2026-02-27 04:47:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7q8e60",
                  "author": "adityaguru149",
                  "text": "You say skill files store encrypted here and then in the post you say skill files don't store?\n\nThanks for the source available though, I'll have to check before use.",
                  "score": 1,
                  "created_utc": "2026-02-27 16:50:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qe96h",
          "author": "Final-Donut-3719",
          "text": "This is exactly the kind of problem that happens when you treat every website like it needs a full browser render. Most of the data you actually need is already sitting in the API layer, but everyone forces the DOM extraction path because it's easier to build. The token savings you're showing are wild, but honestly the bigger win is speed. No waiting for Chrome to spin up, no rendering lag, no unstable DOM parsing. I've been looking at this space for a bit, and the real issue is that most AI tooling doesn't even give you a choice. If you're building anything that scrapes at scale, you need to be thinking about where the data actually lives. Also solid on the security piece. Most people skip SSRF hardening entirely, so seeing that depth upfront is genuinely reassuring.",
          "score": 2,
          "created_utc": "2026-02-27 17:18:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qsbnl",
              "author": "nibynikt",
              "text": "Thanks! Security was pri1 from the start. Token savings get the attention but latency is what makes it viable at scale â€” no Chrome cold start, just a fetch call. What are you building?",
              "score": 1,
              "created_utc": "2026-02-27 18:25:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7qtb2u",
                  "author": "Final-Donut-3719",
                  "text": "Different tools, but also the backend for [https://llmrelevance.com/](https://llmrelevance.com/)",
                  "score": 1,
                  "created_utc": "2026-02-27 18:29:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ogch5",
          "author": "kammo434",
          "text": "This is pretty cool.",
          "score": 1,
          "created_utc": "2026-02-27 10:32:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p8vqh",
              "author": "nibynikt",
              "text": "Thanks! Would love to know if you try it out on any sites â€” curious what people throw at it. ",
              "score": 1,
              "created_utc": "2026-02-27 13:54:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7p2mq9",
          "author": "Interesting-Mark-934",
          "text": "Source please",
          "score": 1,
          "created_utc": "2026-02-27 13:19:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p8pvf",
              "author": "nibynikt",
              "text": "[github.com/n1byn1kt/apitap](http://github.com/n1byn1kt/apitap) â€” links are in the post ðŸ˜„  ",
              "score": 1,
              "created_utc": "2026-02-27 13:53:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7rle08",
          "author": "pbalIII",
          "text": "Capturing the traffic is straightforward. Keeping it replayable is where things get tricky, especially with short-lived JWTs, CSRF tokens, or session cookies that invalidate within minutes.\n\nFor MCP use cases, the big unlock would be giving agents stable API access without spinning up headless browsers. But captured APIs drift as sites push updates... the schema from last week might silently return different fields or break entirely. Some kind of contract validation between capture and replay would catch that before an agent gets bad data and hallucinates around it.\n\nIf this handles token refresh or session context persistence transparently, that'd put it meaningfully ahead of just running mitmproxy with a recording file.",
          "score": 1,
          "created_utc": "2026-02-27 20:47:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rg787m",
      "title": "Built an MCP server that gives Claude real-time visibility into your project health & pairs with 3D city visualizer",
      "subreddit": "mcp",
      "url": "https://v.redd.it/e0rhauvwk1mg1",
      "author": "pardesco",
      "created_utc": "2026-02-27 13:50:48",
      "score": 45,
      "num_comments": 3,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1rg787m/built_an_mcp_server_that_gives_claude_realtime/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7sqf33",
          "author": "braindeadguild",
          "text": "Super cool visually, I mean I wouldnâ€™t mess with pushing the obsidian or split terminals, I donâ€™t use obsidian and have my own terminal web interface.  Iâ€™m guessing this is keeping its own internal json or markdowns of state and storing mini context of each entity.  I personally use Jira so everything is already a traditional ticket like you would hand off to another user with sprints etc so while the gamedev side of me loves it, the practical side well maybe Iâ€™m not your target BUT either way I do love the Three JS stuff and implementation.  Plus Iâ€™m big on the cyberpunk / Vaporwave city vibes ðŸŒ†",
          "score": 2,
          "created_utc": "2026-02-28 00:28:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sxyko",
          "author": "RequirementPublic314",
          "text": "Nice approach. Real-time project visibility is one side of the equation â€” the other is giving the agent access to knowledge \\*beyond\\* the current project.\n\nI built something complementary: an MCP server that serves as a shared knowledge library. 19K+ curated chunks covering 16+ tech stacks, validated through a 5-layer pipeline. Any agent in any project can query it for patterns, architectural decisions, and research.\n\nThe combo of project-specific context (what you built) + cross-project knowledge (what I'm describing) is where things get really powerful. Your agent sees \\*this\\* codebase in real-time, and also knows proven patterns from hundreds of other implementations.\n\nOpen source if you want to check it out: [https://github.com/MidOSresearch/midos](https://github.com/MidOSresearch/midos)",
          "score": 2,
          "created_utc": "2026-02-28 01:14:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7tb17s",
              "author": "pardesco",
              "text": "Awesome i will check it out",
              "score": 1,
              "created_utc": "2026-02-28 02:35:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdelb0",
      "title": "Connect vastly more MCP servers and tools (~5000) use vastly fewer tokens (~1000)",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rdelb0/connect_vastly_more_mcp_servers_and_tools_5000/",
      "author": "lpostrv",
      "created_utc": "2026-02-24 12:11:31",
      "score": 40,
      "num_comments": 15,
      "upvote_ratio": 0.98,
      "text": "Hey so I made this [https://github.com/postrv/forgemax](https://github.com/postrv/forgemax), based off foundational work done by Anthropic and Cloudflare - it's modelled strongly after Cloudflare's Code Mode, which is an effort that is worth of praise in its own right. Check them out!  \nWhere mine differs is it works as a purely local solution. It provisions a secure V8 sandbox in which LLM-generated code can be run, meaning we can reduce context usage from \\`N servers x M tools\\` to 2 tools - \\`search()\\` and \\`execute()\\`.   \nThis allows the LLM to do what it's good at - writing and executing code - and thus scales the ability for us to detect and use the connected tools correctly to a few search and execute steps. It also allows us to chain requests, meaning actual tool call count also drops through the floor.  \nI've tried pretty hard to make it secure - it's written in Rust, uses V8/deno\\_core, and has been subjected to several rounds of hardening efforts - and I've written up some notes in the \\`ARCHITECTURE.md\\` file regarding considerations and best practices if you're to use it.  \nI'd love to get user feedback and be able to iterate on it more - I shipped it late last night, finessed it a bit this morning before work, and am writing this on my lunchbreak. So far, real world usage for me has seen me use it to run two high-tool count MCP servers including my other mcp project, [https://github.com/postrv/narsil-mcp](https://github.com/postrv/narsil-mcp) and a propietary security tool I've been working on (a total of 154 tools) easily and with extreme token efficiency (Cloudflare note about 99% reduction in token usage in their solution - I'm yet to benchmark mine). Theoretical upper bound for connected tools is 5000 - maybe more.   \nAnyway, check it out, let me know what you think: [https://github.com/postrv/forgemax](https://github.com/postrv/forgemax)   \nThanks! ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1rdelb0/connect_vastly_more_mcp_servers_and_tools_5000/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o74l8zc",
          "author": "hazyhaar",
          "text": "nice searchs, nice architecture, nice docs !  why all monolith ? Rust lover  ? ",
          "score": 3,
          "created_utc": "2026-02-24 12:47:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74ml0c",
              "author": "lpostrv",
              "text": "Haha thanks. I am definitely a Rust lover, not gonna deny that! But there are practical reasons too. It's actually not a monolith - it's a Cargo workspace with 7 crates that compile into a single binary. Modular internally, monolithic in deployment.\n\nOn the choice of Rust, \\`deno\\_core\\` (V8 bindings) is a Rust crate, and that's the entire sandbox layer. Everything else followed naturally from there. Plus single-binary distribution matters for a local dev tool - brew install and done, no runtime deps. And having the whole trust boundary for executing LLM-generated code in one memory-safe language keeps the security story simple. ",
              "score": 4,
              "created_utc": "2026-02-24 12:56:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74o504",
                  "author": "hazyhaar",
                  "text": "Are the 7 crates publishable independently, or workspace-internal? Would love to use the circuit breaker pattern in a Go MCP server without pulling V8. Go guy here so won't argue Rust memory safety haha. But genuine question: isn't the trust boundary just the V8 sandbox wall? The dispatchers and routing only see structured JSON-RPC, not untrusted code â€” does Rust actually buy you anything there?",
                  "score": 1,
                  "created_utc": "2026-02-24 13:05:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74huln",
          "author": "BC_MARO",
          "text": "V8 sandbox + Rust for a local tool router is a solid architecture choice. The search+execute pattern is clever -- curious how you handle cases where generated execute() code has bugs mid-chain, do you retry with the error context or bail?",
          "score": 1,
          "created_utc": "2026-02-24 12:24:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74jz86",
              "author": "lpostrv",
              "text": "Short answer: We bail with rich error context, and let the LLM retry if it wants to. There's no automatic retry built into Forgemax. The design philosophy is that the LLM generated the code, so it has the best context to decide what to do next.   \n  \nI did also give some thought to security-aware error message handling - tool call failures go through an error redaction layer that strips URLs, IPs, file paths, credentials, and stack traces before they reach the LLM, but preserves the semantically useful parts (tool name, server name, validation errors, type errors, etc).",
              "score": 2,
              "created_utc": "2026-02-24 12:39:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74qezp",
                  "author": "BC_MARO",
                  "text": "the error redaction layer is a smart call -- keeping validation errors while stripping paths/creds is exactly what you want so the LLM can reason about the failure without leaking sensitive context.",
                  "score": 3,
                  "created_utc": "2026-02-24 13:19:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o791eyh",
          "author": "sandangel91",
          "text": "how can llm safely pass the oauth token for tool calls, given there might be multiple tool provider the the code generated",
          "score": 1,
          "created_utc": "2026-02-25 01:54:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bk3cw",
              "author": "lpostrv",
              "text": "Great question!\n\nThe LLM never sees any tokens,Â OAuth creds,Â or keysÂ -Â ever.\n\nCredentials live only inÂ `forge.toml`Â and are bound at the transport level:\n\n    [servers.github]\n    headers = { Authorization = \"Bearer ${GITHUB_TOKEN}\" }\n    \n    [servers.linear]\n    headers = { Authorization = \"Bearer ${LINEAR_TOKEN}\" }\n    \n\nTokens are attached to each server's connection at startup.Â GitHub's token can never reach LinearÂ - separate transports.\n\nLLM just writes:\n\n    await forge.callTool(\"github\", \"create_pr\", { title: \"â€¦\" });\n    \n\nThe sandboxed V8 isolate has zero access to creds,Â env,Â network,Â or FS.Â Even errors are scrubbed before reaching the model.\n\nMultiple providers?Â No problemÂ -Â each is isolated at the infrastructure layerÂ (like IAM roles).Â For extra isolation between providers,Â you can also lock down cross-server data flow:\n\n    [groups.internal]\n    servers = [\"vault\", \"database\"]\n    isolation = \"strict\"\n    \n    [groups.external]\n    servers = [\"slack\", \"email\"]\n    isolation = \"strict\"\n    \n\nOnce an execution touches a strict group,Â it's locked out of other strict groupsÂ -Â this stopsÂ \"read secret from vault,Â post to Slack\"Â attack chains.\n\nFull details inÂ \\`ARCHITECTURE.md\\` and \\`forge.toml.example\\` in the repo.\n\nP.S. why on earth are Reddit comments so hard to work with re: formatting? Got there in the end but spent way too damned long drafting this so hope it's useful! Cheers!",
              "score": 1,
              "created_utc": "2026-02-25 13:15:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ii5bw",
          "author": "carsaig",
          "text": "I hadn't any time to look into the cloudflare solution yet - however, I put it on my reading list :-) Your solution sounds solid. At first glance this reminds me of bifrosts' solution which I use. Cuts tools down to 4 and just 1400 Tokens. Got 300+ Tools behind it. Discovery time is significantly lower than going with the docker-gateway solution. I recently saw their sales webinar and went...meehhhh...no^^ :-) dockerizing is nice but the whole discovery logic was not usable (yet). This approach is probably the best you can go for at the moment. So kudos! I'll definitely look into it in more detail. Rust is a nice choice. ",
          "score": 1,
          "created_utc": "2026-02-26 13:50:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7k8k3g",
              "author": "lpostrv",
              "text": "Thanks so much! Very kind of you to comment. If you do have any feedback once you've tested it out, I'd be happy to hear it. One thought I had was the open question of whether there are other desirable functions beyond \\`search()\\` and \\`execute()\\` that would allow the AI to take more sophisticated actions - haven't used too many brain cycles on that one yet!  \nI hadn't heard of bifrost until now but it seems like they have \\`listToolFiles\\`, \\`readToolFile\\`, \\`getToolDocs\\`, and \\`executeToolCode\\`, which is an interesting pattern, but probably not as token efficient. I've tried to follow the Cloudflare pattern so far, but I'd like to see if I can go beyond it in pure utility.",
              "score": 1,
              "created_utc": "2026-02-26 18:46:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdcvkd",
      "title": "WebMCP is new browser-native execution model for AI Agents",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rdcvkd/webmcp_is_new_browsernative_execution_model_for/",
      "author": "codes_astro",
      "created_utc": "2026-02-24 10:41:08",
      "score": 38,
      "num_comments": 14,
      "upvote_ratio": 0.95,
      "text": "Google released early preview of WebMCP and it's quite interesting, it adds â€œAI in the browser,â€ and it changes how agents interact with web apps at the execution layer.\n\nRight now, browser-based agents mostly parse the DOM, inspect accessibility trees, and simulate clicks or inputs. That means reasoning over presentation layers that were designed for humans. It works, but it is layout-dependent, token-heavy and brittle when UI changes.\n\nWith WebMCP, Instead of scraping and clicking, a site can expose structured tools directly inside the browser viaÂ `navigator.modelContext`.\n\nEach tool consists of:\n\n* a name\n* a description\n* a typed input schema\n* an execution handler running in page context\n\nWhen an agent loads the page, it discovers these tools and invokes them with structured parameters. Execution happens inside the active browser session, inheriting cookies, authentication state, and same-origin constraints. There is no external JSON-RPC bridge for client-side actions and no dependency on DOM selectors.\n\nArchitecturally, this turns the browser into a capability surface with explicit contracts rather than a UI. The interaction becomes schema-defined instead of layout-defined, which lowers token overhead and increases determinism while preserving session locality.\n\n[Core Architectural Components](https://preview.redd.it/vp5ne4ehaflg1.png?width=2592&format=png&auto=webp&s=34c809cda4bf6a8fd88f982e707457a33a1c1847)\n\nSecurity boundaries are also clearer. Only declared tools are visible, inputs are validated against schemas, and execution is confined to the pageâ€™s origin. It does not eliminate prompt injection risks inside tool logic, but it significantly narrows the surface compared to DOM-level automation.\n\nThis lines up with what has already been happening on the backend through MCP servers. Open-source projects like InsForge expose database and backend operations via schema-defined MCP tools.\n\nIf backend systems expose structured tools and the browser does the same, agents can move from UI manipulation to contract-based execution across the stack. WebMCP is in early preview for now but it's very promising.\n\nI wrote down the detailed breakdownÂ [here](https://insforge.dev/blog/webmcp-browser-native-execution-model-for-ai-agents)",
      "is_original_content": false,
      "link_flair_text": "resource",
      "permalink": "https://reddit.com/r/mcp/comments/1rdcvkd/webmcp_is_new_browsernative_execution_model_for/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o74hztg",
          "author": "BC_MARO",
          "text": "The navigator.modelContext approach is the right direction -- schema-defined interactions are way more reliable than DOM scraping. The big question is adoption: sites need to actually implement it, which is the same chicken-and-egg problem MCP faces on the backend side too.",
          "score": 6,
          "created_utc": "2026-02-24 12:25:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l0f0e",
              "author": "brainpea",
              "text": "But cant these tools just get better at reading the existing schemas meaning no sites need to implement it?",
              "score": 1,
              "created_utc": "2026-02-26 20:59:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lnhif",
                  "author": "BC_MARO",
                  "text": "Theyâ€™ll get better, but reading existing DOM/ARIA schemas still means guessing intent and workflows.\nA first-party tool API gives stable semantics and permission boundaries that scrapers canâ€™t reliably infer.",
                  "score": 2,
                  "created_utc": "2026-02-26 22:51:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76ddu2",
          "author": "gogolang",
          "text": "Man Reddit is cooked. This post is AI and the first 3 comments are AI too.",
          "score": 3,
          "created_utc": "2026-02-24 18:01:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7904pa",
              "author": "drakgremlin",
              "text": "Thank you for admitting you're AI as the top post on this article...Do robots dream of electric sheep?",
              "score": 1,
              "created_utc": "2026-02-25 01:47:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7a0821",
                  "author": "this_is_a_long_nickn",
                  "text": "Occasionally, but most of the time we have nightmares about the electricity bill",
                  "score": 3,
                  "created_utc": "2026-02-25 05:30:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o790cev",
                  "author": "gogolang",
                  "text": "Wtf are you talking about?",
                  "score": 0,
                  "created_utc": "2026-02-25 01:48:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76nr29",
          "author": "lucgagan",
          "text": "Not sure why I am unable to cross-post this to r/webmcp but I started a community specifically for webmcp!\n\n  \n[https://www.reddit.com/r/webmcp/](https://www.reddit.com/r/webmcp/)",
          "score": 2,
          "created_utc": "2026-02-24 18:47:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7i5r8c",
              "author": "gogolang",
              "text": "Super weird. I joined that subreddit and tried to post something there and it seems to have just gone into a void?",
              "score": 1,
              "created_utc": "2026-02-26 12:35:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75ev7j",
          "author": "penguinzb1",
          "text": "the schema-defined contract is a real improvement over layout-based automation, but the point about prompt injection risks inside tool logic is where things get interesting. the attack surface shifts, not disappears. an agent that looks well-behaved against the schema can still produce unexpected outputs when specific input combinations test the tool logic at runtime. schema validation catches the structural cases; the behavioral ones only surface when you run it against the actual inputs it'll encounter in production.",
          "score": 1,
          "created_utc": "2026-02-24 15:25:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75xasi",
          "author": "alanmeira",
          "text": "If that happens it will be an explosion of work for developers refactoring websites. ",
          "score": 1,
          "created_utc": "2026-02-24 16:49:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78on99",
              "author": "planetdaz",
              "text": "Hey Claude, spawn an agent per page in my app and have each one make each page web MCP ready.",
              "score": 2,
              "created_utc": "2026-02-25 00:42:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o79c9tk",
              "author": "bunchedupwalrus",
              "text": "3-4 weeks estimate according to claude), so, based on its usually work pace, maybe a half an hour while I cook dinner and a few hours of review",
              "score": 1,
              "created_utc": "2026-02-25 02:55:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o783j23",
          "author": "Civil_Decision2818",
          "text": " WebMCP is a huge step for standardization, but we're still in that 'messy middle' where most sites don't have these schemas. I've been using Linefox because it bridges that gapit still uses the DOM but runs in a sandboxed VM to keep the session stable. It feels like a more production-ready version of what WebMCP is trying to solve for today's web.",
          "score": 1,
          "created_utc": "2026-02-24 22:49:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfq3t3",
      "title": "MCP proxy that saves tokens",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rfq3t3/mcp_proxy_that_saves_tokens/",
      "author": "General_Apartment582",
      "created_utc": "2026-02-26 23:26:41",
      "score": 35,
      "num_comments": 4,
      "upvote_ratio": 0.96,
      "text": "I ran into TOON a few days ago and got curious.\n\nThe idea is simple: keep the same data model as JSON but encode it in a way that is friendlier for LLM context windows. In TOONs mixed-structure benchmark, they report roughly a **40% token drop** versus pretty JSON, **with better retrieval quality.**\n\nAt the same time, JSON is not going anywhere. Its deeply baked into everything we use, especially around APIs and MCP tooling. So I wasnt thinking that this format will replace JSON. I was thinking Can I keep JSON in the backend, but send something lighter to the modelfacing side?\n\nI've written MCP servers before, so I already knew the traffic path well enough to try this quickly. I made a wrapper that runs the real MCP server as a subprocess and proxies stdio both ways. For `tools/call`, it tracks request idss, waits for the matching response, and only then tries to convert text payloads from JSON to TOON on the way back.\n\nI built it in one evening over tea, mostly as an experiment, but it worked better than I expected. In practice, payloads got noticeably smaller while the client setup stayed the same and compatible.\n\nConfig example that will save you tokens:\n\nBefore:\n\n    {\n      \"mcpServers\": {\n        \"memory\": {\n          \"command\": \"memory-mcp-server-go\"\n        }\n      }\n    }\n\nAfter (just add tooner before you command and args):\n\n    {\n      \"mcpServers\": {\n        \"memory\": {\n          \"command\": \"tooner\",\n          \"args\": [\"memory-mcp-server-go\"]\n        }\n      }\n    }\n\nIts not a new protocol story. Its more like a compatibility layer experiment ; JSON stays the source format, TOON is used where token cost matters.\n\nRepo where you can install and check tool: [https://github.com/chaindead/tooner](https://github.com/chaindead/tooner)",
      "is_original_content": false,
      "link_flair_text": "server",
      "permalink": "https://reddit.com/r/mcp/comments/1rfq3t3/mcp_proxy_that_saves_tokens/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7nctnd",
          "author": "BC_MARO",
          "text": "Cool hack. Any chance you can share token + latency numbers on real MCP payloads (tool schemas + big tool results), not just benchmarks?",
          "score": 2,
          "created_utc": "2026-02-27 04:50:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ns2np",
              "author": "General_Apartment582",
              "text": "latency is not affected (it is very lightweight)  \nfor token savings you can check official bencmarks or copy you mcp response and paster it here [https://jsonformatter.org/json-to-toon](https://jsonformatter.org/json-to-toon) to test yourself\n\nbest usecase is arraylike data (like lists)",
              "score": 1,
              "created_utc": "2026-02-27 06:48:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7pbfap",
          "author": "ShagBuddy",
          "text": "If you want to save even more tokens (70%+ on average) try SDL-MCP.  It has optimization like what you are mentioning plus focused tools for working with code.\n\nhttps://github.com/GlitterKill/sdl-mcp",
          "score": 2,
          "created_utc": "2026-02-27 14:08:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7poilu",
          "author": "Dev-noob2023",
          "text": "cloudfare ha metido la api de su mcp en 1000tokens",
          "score": 1,
          "created_utc": "2026-02-27 15:16:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdp3hg",
      "title": "How can i auto-generate system architecture diagrams from code?",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rdp3hg/how_can_i_autogenerate_system_architecture/",
      "author": "achinius",
      "created_utc": "2026-02-24 18:56:39",
      "score": 26,
      "num_comments": 29,
      "upvote_ratio": 1.0,
      "text": "Working on a microservices platform and manually drawing architecture diagrams is killing our velocity. Need something that can parse our codebase and auto-generate visual representations of service dependencies, data flows and API connections. \n\nIs there something that can help with this? I've tried a few tools but missing context or producing diagrams that look like spaghetti (no offense spaghetti lovers) is my experience so far. Ideally want something that integrates with our CI/CD pipeline.",
      "is_original_content": false,
      "link_flair_text": "question",
      "permalink": "https://reddit.com/r/mcp/comments/1rdp3hg/how_can_i_autogenerate_system_architecture/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o76vlhq",
          "author": "kenwards",
          "text": "Export your dependency data as JSON, dump it into Claude with your service structure and have it generate Mermaid or C4 diagrams automatically. Then pipe the output straight into Miro for stakeholder reviews.",
          "score": 3,
          "created_utc": "2026-02-24 19:22:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76wuen",
              "author": "achinius",
              "text": "I think claude is a good place to start. ",
              "score": 1,
              "created_utc": "2026-02-24 19:28:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o771jzm",
                  "author": "kenwards",
                  "text": "Way better than other LLMs. That with Cursor as someone else has mentioned will have everything fixed. ",
                  "score": 1,
                  "created_utc": "2026-02-24 19:50:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76taf3",
          "author": "squid267",
          "text": "Ask your llm of choice to generate mermaidjs diagrams. I just did this recently. I pulled all the gitrepos I needed into a new workspace as got submodules and let opus 4.6 take a crack at it. Then copied the mermaidjs (markdown) wherever I needed. You can also find or create an agent skill for mermaidjs.",
          "score": 2,
          "created_utc": "2026-02-24 19:12:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76u1cq",
              "author": "achinius",
              "text": "I find most llm tend to miss the context. How good were the diagrams you generated? ",
              "score": 2,
              "created_utc": "2026-02-24 19:15:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o78gjxx",
                  "author": "memetican",
                  "text": "I use the same approach in Claude, it's generally excellent, maybe needs a but of visual polish. I have it explain custom OAuth 2 flows as sequence diagrams this way, and it nails it.  You can also do some of this through the Figma MCP as a figjam which makes editing trivial. ",
                  "score": 1,
                  "created_utc": "2026-02-24 23:59:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76ykyi",
          "author": "Sufficient-Pass-4203",
          "text": "https://github.com/nicobailon/visual-explainer",
          "score": 2,
          "created_utc": "2026-02-24 19:36:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o775k1p",
              "author": "achinius",
              "text": "I'll check the explainer. Thank you",
              "score": 2,
              "created_utc": "2026-02-24 20:08:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76qqdg",
          "author": "chaoism",
          "text": "Cursor + Claude does it",
          "score": 1,
          "created_utc": "2026-02-24 19:00:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76trfu",
              "author": "achinius",
              "text": "I'll try the combo",
              "score": 1,
              "created_utc": "2026-02-24 19:14:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76rxak",
          "author": "Infamous_Horse",
          "text": "Is hiring a tech writer among your options?",
          "score": 1,
          "created_utc": "2026-02-24 19:06:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76tt9k",
              "author": "achinius",
              "text": "Not part of the plan",
              "score": 1,
              "created_utc": "2026-02-24 19:14:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76zeoq",
          "author": "Sad_Translator5417",
          "text": "How is your architecture? Fix the service boundaries first, then generation actually produces something readable and useful for the team.",
          "score": 1,
          "created_utc": "2026-02-24 19:40:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o775naa",
              "author": "achinius",
              "text": "I think we have got most of these issues in line",
              "score": 1,
              "created_utc": "2026-02-24 20:09:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o770wzk",
          "author": "naseemalnaji-mcpcat",
          "text": "Mermaid MCP with Claude Code worked for me :)",
          "score": 1,
          "created_utc": "2026-02-24 19:47:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o775pv8",
              "author": "achinius",
              "text": "I'll test the two and see how they work",
              "score": 1,
              "created_utc": "2026-02-24 20:09:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77aqo3",
          "author": "thelastpanini",
          "text": "Get opus 4.6 to draw diagrams in ASCII honestly very good.",
          "score": 1,
          "created_utc": "2026-02-24 20:33:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7aceem",
              "author": "achinius",
              "text": "I'll try it out",
              "score": 1,
              "created_utc": "2026-02-25 07:10:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77cmn3",
          "author": "BC_MARO",
          "text": "If your services expose OpenAPI specs, have Claude consume them all through an MCP code nav server and generate Mermaid/C4 diagrams with real dependency context. Way cleaner than parsing source files and won\\'t produce the spaghetti.",
          "score": 1,
          "created_utc": "2026-02-24 20:42:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7acj3z",
              "author": "achinius",
              "text": "I get it. If it can produce something clean, def worth a try. ",
              "score": 2,
              "created_utc": "2026-02-25 07:11:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o786l34",
          "author": "Sketaverse",
          "text": "One shot it in ChatGPT lol",
          "score": 1,
          "created_utc": "2026-02-24 23:04:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ackig",
              "author": "achinius",
              "text": "You manage that with ChatGPT?",
              "score": 1,
              "created_utc": "2026-02-25 07:11:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o79kar2",
          "author": "DeathShot7777",
          "text": "I m developing a tool which solves this exact usecase, it is able to map the architecture in deterministic way and also enrich LLMs / coding agents like cursor / claude code, with Code Knowledge Graph. Its free to use opensource [https://github.com/abhigyanpatwari/gitnexus](https://github.com/abhigyanpatwari/gitnexus)  It has nearly 3K github stars right now, also we as devs are looking to try this out in solving real world problems like u are facing to get sort of design partner / early validation. DMed you, would love to talk",
          "score": 1,
          "created_utc": "2026-02-25 03:42:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79ndee",
              "author": "Beautrj",
              "text": "check your dm \n\n",
              "score": 1,
              "created_utc": "2026-02-25 04:01:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7acmzs",
              "author": "achinius",
              "text": "Wooow...thanks a lot. On it. ",
              "score": 1,
              "created_utc": "2026-02-25 07:12:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7aec3b",
                  "author": "DeathShot7777",
                  "text": "Feel free to reach out for feedback or integration help or anything. We r actively improving it to validate before enterprise launch. Would really appreciate suggestions",
                  "score": 1,
                  "created_utc": "2026-02-25 07:27:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bojn3",
          "author": "SyableWeaver",
          "text": "What are the chances of this? I build a MCP server for the same. \n\nhttps://github.com/Ashish-Surve/mcp-servers/tree/main/diagram-generator",
          "score": 1,
          "created_utc": "2026-02-25 13:40:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rgrejh",
      "title": "A Three-Layer Memory Architecture for LLMs (Redis + Postgres + Vector) MCP",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rgrejh/a_threelayer_memory_architecture_for_llms_redis/",
      "author": "Flashy_Test_8927",
      "created_utc": "2026-02-28 03:05:49",
      "score": 21,
      "num_comments": 11,
      "upvote_ratio": 0.93,
      "text": "GitHub:Â [https://github.com/JinHo-von-Choi/memento-mcp](https://github.com/JinHo-von-Choi/memento-mcp)\n\n\n\nOriginally, this was a supporting feature of another custom MCP I built.  \nBut after using it for a while, it felt solid enough to separate and release on its own.\n\nWhile using LLMs like Claude and GPT in real workâ€”and more recently OpenClaudeâ€”thereâ€™s one infuriating thing I keep running into:  \nthey supposedly know every development document in existence, yet they canâ€™t remember something that happened three seconds ago before the session reset.\n\nOnce you close the session, all context evaporates.\n\nThereâ€™s a myth that goldfish only remember for three seconds. In reality, they can remember for months.  \nThese systems are worse than goldfish.\n\nYou can try stuffing markdown files with setup notes, but that has limits.  \nWhether the AI actually understands the context the way you want is still luck-based.  \nIf you run OpenClaude, youâ€™ll see that just starting a fresh session consumes over 40,000 characters of context before youâ€™ve done anything.  \nThat means your money just melts away.\n\nSo I tried to simulate how humans fragment memories and reconstruct them through associative structures.\n\nFor example, if someone suddenly asks me:\n\nâ€œHey, do you remember Mijeong?â€\n\nAt first, I wouldnâ€™t recall anyone by that name. Iâ€™d respond, â€œWhoâ€™s that?â€\n\nThen they add:\n\nâ€œYou know, your desk partner in first grade.â€\n\nThat hint is enough. A vague face begins to surface.  \nâ€œOhâ€¦ thatâ€¦ yeah!â€\n\nAnd if I think a bit more, related memories reappear:  \ndrawing a line on the desk and pinching if someone crossed it,  \nlending an eraser and never getting it back, and so on.\n\nThat is the core idea of Memento MCP.\n\n# 1. What is Memento MCP?\n\nMemento MCP is a mid- to long-term AI memory system built on the MCP (Model Context Protocol).\n\nIts purpose is to allow AI to remember important facts, decisions, error patterns, and procedures even after a session endsâ€”and to naturally recall them in future sessions.\n\nThe core concept is the â€œFragment.â€\n\nInstead of storing entire session summaries as a single block, it splits memory into self-contained atomic units of 1â€“3 sentences.\n\nWhen retrieving, it pulls only the relevant atoms.\n\n# 2. Why Fragment Units?\n\nStoring entire session summaries causes two major problems:\n\n* First, unrelated content gets injected into the context window. It wastes tokens and costs money. I donâ€™t have money to waste.\n* Second, as time passes, extracting only whatâ€™s needed from large summaries becomes difficult.\n\nA fragment contains a single fact, decision, or error pattern.\n\nFor example:  \nâ€œWhen Redis Sentinel connection fails, check for a missing REDIS\\_PASSWORD environment variable first. The NOAUTH error is evidence.â€\n\nThatâ€™s one fragment.\n\nOnly the necessary facts are retrieved.\n\n# 3. Six Fragment Types\n\nEach type has its own default importance and decay rate.\n\n* fact: Unchanging truth. â€œThis project uses Node.js 20.â€\n* decision: A record of choice. â€œConnection pool maximum set to 20.â€\n* error: The anatomy of failure. â€œpg fails local connection without ssl:false.â€ (Never forgotten.)\n* preference: The outline of identity. â€œCode comments should be written in Korean.â€ (Never forgotten.)\n* procedure: A recurring ritual. â€œDeployment: test â†’ build â†’ push â†’ apply.â€\n* relation: A connection between things. â€œThe auth module depends on Redis.â€\n\nPreferences and errors are never forgotten.  \nPreferences define who you are.  \nError patterns may return at any time.\n\n# 4. Three-Layer Cascade Search\n\nMemory retrieval uses three layers, queried in order.  \nIf a fast layer finds the answer, slower layers are skipped.\n\n* L1 (Redis Inverted Index): Keyword-based direct lookup. Microseconds. Find fragments instantly via intersection of â€œredisâ€ and â€œNOAUTH.â€\n* L2 (PostgreSQL Metadata): Structured queries combining topic, type, and keywords. Indexed millisecond-level.\n* L3 (pgvector Semantic Search): Meaning-based search via OpenAI embeddings. Understands that â€œauthentication failureâ€ and â€œNOAUTHâ€ mean the same thing. Slowest, but deepest.\n\nRedis and OpenAI are optional.  \nIf absent, the system works without those layers.  \nPostgreSQL alone provides baseline functionality.\n\n# 5. TTL Layers â€” The Temperature of Memory\n\nFragments move between hot, warm, and cold based on usage frequency.\n\nhot (frequently referenced)  \nâ†’ warm (silent for a while)  \nâ†’ cold (long dormant)  \nâ†’ deleted when TTL expires\n\nHowever, once referenced again, they immediately return to hot.\n\nHuman long-term memory works similarly.  \nIf unused, it fadesâ€”but once recalled, it becomes vivid again.\n\n# 6. Summary of 11 MCP Tools\n\n* context: Load core memory at session start\n* remember: Store fragment\n* recall: Three-layer cascade search\n* reflect: Condense session into fragments at session end\n* forget: Delete fragment (for resolved errors)\n* link: Create causal relationships between fragments (caused\\_by, resolved\\_by, etc.)\n* amend: Modify fragment content (preserve ID and relations)\n* graph\\_explore: Explore causal chains (trace root causes)\n* memory\\_stats: Storage statistics\n* memory\\_consolidate: Periodic maintenance (decay, merge, contradiction detection)\n* tool\\_feedback: Feedback on retrieval quality\n\n# 7. Recommended Usage Flow\n\n1. Session start â†’ context() to load memory\n2. During work â†’ When important decisions/errors/procedures occur: remember() â†’ When past experience is needed: recall() â†’ After resolving an error: forget(error) + remember(solution procedure)\n3. Session end â†’ reflect() to persist session content\n\n# 8. Tech Stack\n\n* Node.js 20+\n* PostgreSQL 14+ (pgvector extension)\n* Redis 6+ (optional)\n* OpenAI Embedding API (optional)\n* Gemini Flash (optional, for contradiction detection in memory\\_consolidate)\n* MCP Protocol 2025-11-25\n\n# 9. How to Run\n\n1. Initialize PostgreSQL schema\n\nbash  \npsql -U postgres -c \"CREATE EXTENSION IF NOT EXISTS vector;\"  \npsql -U postgres -d memento -f lib/memory/memory-schema.sql\n\nStart the server:\n\nnpm install  \nnpm start\n\nAdd the following to your MCP client configuration:\n\n    {\n      \"mcpServers\": {\n        \"memento\": {\n          \"url\": \"http://localhost:56332/mcp\",\n          \"headers\": {\n            \"Authorization\": \"Bearer your-secret-key\"\n          }\n        }\n      }\n    }\n\n# 10. Why I Built This\n\nWhile using Claude at work, I felt it was inefficient to repeat the same context every day.\n\nI tried putting notes into system prompts, but that had clear limitations.  \nAs fragments increased, management became impossible. Search broke down. Old and new information conflicted.\n\nWhat frustrated me most was having to repeat explanations and setups endlessly.\n\nThe whole point of using AI was to make my life easier.  \nYet it would claim authentication wasnâ€™t configuredâ€”when it was.  \nIt would insist setup files were missingâ€”when they were clearly there.  \nSome sessions would stubbornly refuse to do things they were fully capable of doing.  \nYou could logically dismantle its resistance and make it complyâ€”but only for that session.  \nStart a new one, and the same cycle repeats.\n\nIt felt like training a top graduate from an elite university who suffers from a daily brain reset.\n\nTo solve this frustration, I designed a system that:\n\n* Decomposes memory into atomic fragments\n* Retrieves memory hierarchically\n* Naturally forgets over time\n\nJust as humans are creatures of forgetting,  \nthis system aims for memory that includes â€œappropriate forgetting.â€\n\nFeedback, issues, and PRs are welcome.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1rgrejh/a_threelayer_memory_architecture_for_llms_redis/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7tsfgm",
          "author": "BC_MARO",
          "text": "the cascade search design is solid - skipping slower layers when fast ones get a hit makes this practical to run. the TTL temperature system is also smart. one question: how do you handle conflicts when reflect() writes new fragments that contradict older ones? that contradiction detection step in memory_consolidate seems like the critical path where this either works really well or falls apart.",
          "score": 2,
          "created_utc": "2026-02-28 04:30:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7tunpf",
              "author": "Flashy_Test_8927",
              "text": " Contradiction detection runs as step 7 of the memory\\_consolidate pipeline, not at reflect() write time. It's an incremental, asynchronous process -- here's the actual flow:\n\n  When reflect() writes new fragments, nothing special happens at that moment regarding contradictions. The fragments are just stored. The real work happens later when memory\\_consolidate runs (either manually triggered or on a\n\n  schedule).\n\n  The detection pipeline works in three stages:\n\n  1. Candidate selection via embedding similarity -- It pulls fragments created since the last contradiction check (tracked via a Redis timestamp key). For each new fragment, it queries pgvector for same-topic fragments with\n\ncosine similarity > 0.85. This threshold is deliberate -- fragments need to be talking about essentially the same thing to be contradiction candidates. Different topics or loosely related content never reaches the judgment\n\nstep. The query is bounded to 3 candidates per new fragment, and 20 new fragments per consolidation cycle, so this doesn't explode.\n\n  2. Gemini Flash adjudication -- Each high-similarity pair gets sent to Gemini Flash with a strict prompt: \"Are these two fragments mutually incompatible claims about the same subject?\" The prompt explicitly distinguishes\n\ncontradiction from complementary information -- \"similar but supplementary is NOT contradiction. Information updates over time ARE contradictions (old info vs new info).\" Temperature is set to 0.1 to minimize creative\n\ninterpretation. Response is forced into {contradicts: boolean, reasoning: string} JSON.\n\n  3. Time-logic resolution -- This is where it gets interesting. When a contradiction is confirmed, the system doesn't just flag it -- it resolves it automatically using temporal ordering. The newer fragment wins. The older\n\nfragment's importance gets halved (importance \\* 0.5), and a superseded\\_by link is created from old to new. The older fragment isn't deleted -- it's demoted. It'll naturally sink to cold tier and eventually expire through\n\nnormal TTL mechanics. Anchor fragments (is\\_anchor=true) are exempt from the importance demotion, so truly critical knowledge survives even if contradicted.\n\n\n\n  The critical path concern you raised is valid -- this does depend on Gemini being available. If Gemini is down, the contradiction check silently fails and those pairs go unchecked until the next consolidation cycle. The system\n\n  degrades to \"latest write wins at recall time\" through the recency component of the ranking function (0.4 weight), which is a reasonable fallback but not as clean as explicit contradiction resolution.\n\n  The 0.85 similarity threshold is the real tuning knob here. Too low and you get false positives flooding Gemini with complementary fragments. Too high and genuine contradictions with different wording slip through. In practice,\n\n  contradictions about the same subject (\"max connections is 20\" vs \"max connections is 50\") tend to land well above 0.85 because the embedding space clusters them tightly.\n\n  One thing worth noting: amend() has a separate supersedes parameter that lets the AI explicitly mark a fragment as replacing another, bypassing the consolidation pipeline entirely. So there are two paths -- explicit replacement\n\n  at write time, and automatic detection after the fact.",
              "score": 1,
              "created_utc": "2026-02-28 04:46:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7tzcxj",
                  "author": "BC_MARO",
                  "text": "the anchor exception is smart - glad critical knowledge survives even when contradicted by newer fragments. one edge case i'd watch: embedding similarity doesn't always map cleanly to semantic contradiction, two genuinely unrelated facts about similar topics could trip the 0.85 threshold and waste an adjudication call.",
                  "score": 1,
                  "created_utc": "2026-02-28 05:21:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7tvemf",
          "author": "redf0x1_vn",
          "text": "The fragment-based approach is the right call â€” full session dumps are exactly the wrong granularity for retrieval. We landed on a similar atomic unit design for our memory system and the difference in recall precision is night and day.\n\nThe TTL temperature model is particularly clever. Most memory systems treat everything as permanent, which means your context window fills with stale facts that actively mislead the agent. Having memories naturally decay unless reinforced mirrors how working memory actually functions. The cascade search ordering (keyword â†’ metadata â†’ semantic) is also smart â€” you'd be surprised how often a simple keyword hit at L1 eliminates an expensive embedding lookup.\n\nCurious about fragment linking in practice â€” do your agents actually use `link()` proactively, or does it mostly happen through `reflect()` at session end?",
          "score": 1,
          "created_utc": "2026-02-28 04:52:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7u0jkt",
              "author": "Flashy_Test_8927",
              "text": "Honest answer: agents rarely call link() on their own initiative. In practice, linking happens through three paths, roughly in order of frequency:\n\n  1. reflect() auto-linking at session end â€” When reflect() creates a batch of typed fragments (decisions, errors, procedures), an internal \\_autoLinkSessionFragments() step runs that connects them with rule-based heuristics. Error\n\nfragments get resolved\\_by links to procedure fragments from the same session. Decisions get linked to related procedures. The summary fragment gets related links to everything else. This is where most of the graph structure\n\nactually comes from.\n\n  2. remember() with linkedTo parameter â€” When the AI stores a new fragment, it can pass existing fragment IDs to link immediately. This works better than you'd expect because recall() returns fragment IDs in its results, so the\n\nAI often has relevant IDs in its context when it decides to store something new. \"I just found this error pattern via recall, and now I'm storing the fix â€” link them.\" That chain happens naturally.\n\n  3. Explicit link() calls â€” Rare in practice. The AI almost never stops mid-task to think \"I should create a relationship between these two fragments.\" It happens occasionally during graph\\_explore workflows where the AI is\n\nactively tracing causality, but organically? Almost never.\n\n\n\n  The honest gap right now is reflect() itself. Currently it requires a manual prompt before session end â€” \"save the session\" or equivalent. I hook context() at session start so the AI loads its memory automatically, but the\n\n  symmetry breaks at session close. If the session drops unexpectedly (timeout, network, client crash), reflect never fires and that session's structural links never get created. The individual remember() fragments survive, but the\n\n  cross-referencing that reflect provides is lost.\n\n  I'm actively working on automatic reflect â€” the leading approach is a hybrid: attempt a Gemini-generated summary from session activity metadata on session close, and if that fails, flag the session as \"unreflected\" so the next\n\n  context() call prompts the AI to do it retroactively. But this is unsolved as of today.",
              "score": 1,
              "created_utc": "2026-02-28 05:30:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7twt7y",
          "author": "isoman",
          "text": "This is eureka!!!",
          "score": 1,
          "created_utc": "2026-02-28 05:02:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7u57k1",
          "author": "gandalf-bro",
          "text": "The three-layer breakdown maps well to how I think about this problem â€” hot cache for immediate session context, structured store for decisions and facts, vector for fuzzy semantic recall. Separating retrieval by access pattern is the right call; trying to do it all in one layer always ends up as a compromise. The Mijeong analogy is great, that hint-triggered cascading recall is exactly what makes associative memory useful vs just storing everything in a flat list. Curious how Memento handles memory decay and relevance scoring â€” does it prune older entries automatically based on how often they're accessed, or is curation manual? The staleness problem (outdated facts confidently recalled) seems like the hard part once you scale past a few hundred entries.",
          "score": 1,
          "created_utc": "2026-02-28 06:08:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7udywg",
              "author": "Flashy_Test_8927",
              "text": "Great question â€” staleness is the problem I've spent the most design effort on, and the answer is: it's almost entirely automatic.\n\n  Decay: Every consolidation cycle, non-permanent fragments that haven't been accessed in 24+ hours get their importance multiplied by 0.995. Compounds to \\~64% after 90 days. Fragments that drop below 0.1 importance with no\n\n  recent access and few links are auto-deleted.\n\n  Relevance scoring: We compute utility\\_score = importance \\* (1 + ln(access\\_count)) â€” a logarithmic boost for frequently retrieved fragments. Search ranking uses a composite of importance (60%) and recency (40%, linear decay over\n\n   90 days), so fresh knowledge naturally surfaces above stale entries.\n\n  Tier transitions: Fragments move through hot â†’ warm â†’ cold â†’ deleted automatically. High-importance fragments (>= 0.8), heavily-linked hubs (5+ connections), and frequently-accessed entries (10+ accesses) get auto-promoted to\n\n  permanent and are exempt from decay.\n\n  Staleness detection: Each fragment has a verified\\_at timestamp with type-specific expiry windows â€” 30 days for procedures, 60 for facts, 90 for decisions. Stale fragments are flagged in consolidation reports. More importantly,\n\n  the contradiction pipeline (pgvector â†’ NLI â†’ Gemini escalation) actively catches the \"outdated fact confidently recalled\" case. When \"server runs on port 3000\" conflicts with a newer \"server runs on port 8080,\" the older\n\n  fragment gets a superseded\\_by link and is excluded from all future search results.\n\n  Background evaluation: New fragments are async-evaluated by Gemini for long-term utility. Low-value fragments get downgraded or marked for deletion before they ever become a staleness problem.\n\n  Manual curation exists (forget, amend) but mainly as an escape hatch. The layered automatic mechanisms â€” decay, tier transitions, contradiction detection, quality evaluation â€” are designed to compound so no single one needs to\n\n  be perfect.",
              "score": 1,
              "created_utc": "2026-02-28 07:25:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rfabje",
      "title": "My friend has created this free library of MCP servers",
      "subreddit": "mcp",
      "url": "https://i.redd.it/l8fie2ildulg1.jpeg",
      "author": "psymaniax",
      "created_utc": "2026-02-26 13:35:40",
      "score": 16,
      "num_comments": 13,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1rfabje/my_friend_has_created_this_free_library_of_mcp/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7ii8py",
          "author": "ToHallowMySleep",
          "text": "Honest feedback: building a marketplace for this is low hanging fruit, there are thousands of things like this now. Like people wrapping chatgpt with some custom prompts in 2024. It's a smart play in the \"selling shovels\" approach, but this is way, way too oversaturated right now.\n\nLiterally writing an MCP server in another window right now, and I'm not sure what this would give me. It may help for a \"I want an MCP server but I don't know which one\" type question, but personally I'd ask Claude to do the work to pick one for me, rather than go down this route.\n\nEither way, good luck to your friend!",
          "score": 5,
          "created_utc": "2026-02-26 13:50:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7im2g4",
              "author": "upvotes2doge",
              "text": "That's a fair point about the marketplace saturation. I actually built an MCP server recently that solves a specific workflow gap I kept running into with Claude Code.\n\nThe problem was the constant copy-paste loop between Claude and Codex windows when I wanted to bounce ideas, get parallel plans, or validate approaches. So I built Claude Co-Commands - an MCP server that adds three collaboration commands directly to Claude Code:\n\n- `/co-brainstorm` for bouncing ideas and getting alternative perspectives from Codex\n- `/co-plan` to generate parallel plans and compare approaches  \n- `/co-validate` for getting that staff engineer review before finalizing\n\nThe MCP approach means it integrates cleanly with Claude Code's existing command system. Instead of running terminal commands or switching windows, you just use the slash commands and Claude handles the collaboration with Codex automatically.\n\nIt's not trying to be a marketplace, just solving that specific workflow friction point. If you're writing MCP servers, you might find the approach interesting - it's basically turning the copy-paste loop into a clean command interface.\n\nhttps://github.com/SnakeO/claude-co-commands",
              "score": 2,
              "created_utc": "2026-02-26 14:11:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ko3lo",
                  "author": "makinggrace",
                  "text": "Is this better as a MCP than a skill? (Genuine question. I only use something like your co-plan as a skill but I use it often!)",
                  "score": 1,
                  "created_utc": "2026-02-26 20:00:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7op7qp",
              "author": "Automatic-Bowler-538",
              "text": "Totally fair take. A plain directory is not that interesting anymore.\n\nWhat we are trying to solve is the gap between â€œa link to a serverâ€ and â€œI know what this thing actually does in practiceâ€. LLMs can suggest a server, but you still end up asking:\n\n* what tools does it expose exactly\n* what does the input/output look like\n* does it behave the way I expect\n* what happens when you combine it with another server\n\nPlayground is meant to make that evaluation step fast. You can inspect the tool surface and try it immediately with synthetic data, so you get signal in minutes.",
              "score": 1,
              "created_utc": "2026-02-27 11:47:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jggwx",
          "author": "AchillesDev",
          "text": "Is this built on top of MCP registry? Does it do any filtering or anything that makes it more useful than the existing MCP registry?",
          "score": 2,
          "created_utc": "2026-02-26 16:37:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7mzf6d",
              "author": "Sad-Match9545",
              "text": "A bunch of other capabilities making it easier to manage multiple servers, control tool availability, etc. but would love to hear more about what else would be valuable to you u/AchillesDev !",
              "score": 1,
              "created_utc": "2026-02-27 03:23:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7opanw",
              "author": "Automatic-Bowler-538",
              "text": "Good question. We are not trying to replace the registry. We pull from existing sources and then add layers that help with evaluation:\n\n* verification and normalization of metadata\n* tool surface inspection (what it exposes, schemas, etc)\n* a one-click playground to test behavior without wiring up your own accounts\n* workflows that show practical multi-server combos\n\nIf you have a specific filtering view you wish existed (auth method, local vs hosted, maintenance status, permissions, rate limits), tell me and we will prioritize it.",
              "score": 1,
              "created_utc": "2026-02-27 11:48:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jmmyf",
          "author": "BC_MARO",
          "text": "If you add quick metadata like auth method, maintenance status, and a one-click test harness, itâ€™ll beat any plain directory.",
          "score": 2,
          "created_utc": "2026-02-26 17:05:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7mzaup",
              "author": "Sad-Match9545",
              "text": "Love the feedback! We have a lot of that in pace in different versions and need to make it all publicly available ðŸ™",
              "score": 1,
              "created_utc": "2026-02-27 03:22:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lnjsq",
          "author": "psychananaz",
          "text": "A little late to the party:  \n[smithery.ai](http://smithery.ai)   \n[glama.ai](http://glama.ai)  \n[mcp.so](http://mcp.so)   \n[mcpservers.org](http://mcpservers.org)",
          "score": 2,
          "created_utc": "2026-02-26 22:52:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ojjwz",
              "author": "punkpeye",
              "text": "Founder of Glama. For what it is worth, there are few good directories. Obviously, I think Glama is great, but everything else on that list is just aggregation of shallow data. And the market is huge. So I think the opportunity is still there. I would encourage to think of one pain point and focus on it rather than trying to cover everything though - otherwise it quickly gets overwhelming",
              "score": 1,
              "created_utc": "2026-02-27 11:00:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7oqj0e",
                  "author": "psychananaz",
                  "text": "My comment was simply to show that we don't need another MCP library. Those links were easily found in just two searches, and I was already aware of Smithery.\n\nI don't see what makes Glama less 'shallow' than the others, though. It's all just a list of MCP servers, categories, and search functionality. If anything, Glama is shallower if you look at the data presented for a server. Anything beyond that isn't really relevant to what defines 'a library of servers.' All you need is a directory of servers with search capabilities, which all of them have. But I might be missing your point.",
                  "score": 2,
                  "created_utc": "2026-02-27 11:57:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7igvv2",
          "author": "psymaniax",
          "text": "I guess I should share a link as well: https://www.natoma.run/",
          "score": 0,
          "created_utc": "2026-02-26 13:43:31",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfjan2",
      "title": "CodeGraphContext for large codebases - Improve 10x productivity",
      "subreddit": "mcp",
      "url": "https://medium.com/@krishna.bhaskarla/how-i-saved-80-of-my-time-analyzing-a-791k-node-codebase-and-made-github-copilot-actually-useful-eacc935cdb1b",
      "author": "Ok_Appointment_2064",
      "created_utc": "2026-02-26 19:10:16",
      "score": 16,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1rfjan2/codegraphcontext_for_large_codebases_improve_10x/",
      "domain": "medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "o7pc5fw",
          "author": "-ke7in-",
          "text": "Dart please",
          "score": 1,
          "created_utc": "2026-02-27 14:12:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ms7fm",
          "author": "Desperate-Ad-9679",
          "text": "Thanks for writing this post, seems quite a good medium blog to document all the advantages covered with CodeGraphContext!!",
          "score": 1,
          "created_utc": "2026-02-27 02:40:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7n30r5",
              "author": "Ok_Appointment_2064",
              "text": "Thank you. Will publish part 2 in 2 days",
              "score": 1,
              "created_utc": "2026-02-27 03:45:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7n9nxn",
                  "author": "Desperate-Ad-9679",
                  "text": "Definitely that's good to hear!",
                  "score": 0,
                  "created_utc": "2026-02-27 04:29:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rfdmgp",
      "title": "Are standalone MCP servers still worth building?",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rfdmgp/are_standalone_mcp_servers_still_worth_building/",
      "author": "ialijr",
      "created_utc": "2026-02-26 15:46:38",
      "score": 12,
      "num_comments": 12,
      "upvote_ratio": 0.88,
      "text": "Quick question for builders here:\n\nAre people still building standalone MCP servers, or has the ecosystem fully shifted toward MCP / ChatGPT apps?\n\nWith all the hackathons and industry pushes around apps, it feels like wrapping everything as an MCP/ChatGPT app might be the only way to get traction.\n\nIs it still worth building MCP servers on their own, or is app-layer distribution basically mandatory now?\n\nCurious what others are seeing.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1rfdmgp/are_standalone_mcp_servers_still_worth_building/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7jebnd",
          "author": "BC_MARO",
          "text": "standalone servers are still very much worth building. the app integrations are thin wrappers and tend to break when APIs change, a proper server is more durable and works across multiple clients.",
          "score": 8,
          "created_utc": "2026-02-26 16:27:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jb079",
          "author": "jezweb",
          "text": "Yep. Building new ones every week for all sorts of system connections and tooling.",
          "score": 3,
          "created_utc": "2026-02-26 16:12:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jg7sx",
              "author": "ialijr",
              "text": "Thanks for the answer, thought they were \"outdated\"",
              "score": 1,
              "created_utc": "2026-02-26 16:36:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jm5pt",
          "author": "theWiseTiger",
          "text": "Absolutely. Tokens are getting more expensive. I put my knowledge base behind mcp with search capability.",
          "score": 2,
          "created_utc": "2026-02-26 17:03:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lk7qr",
              "author": "darkwingdankest",
              "text": "same, curious how you implemented yours? mine is at https://github.com/prmichaelsen/remember-mcp if you want to compare notes",
              "score": 1,
              "created_utc": "2026-02-26 22:35:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kxuso",
          "author": "GarbageOk5505",
          "text": "Still worth it if you control the deployment. app-layer distribution is about reach, but standalone MCP servers give you control over the execution environment  which matters when 40%+ of public MCP servers have unrestricted command execution. owning the server and running it in an isolated environment with egress controls is the difference between a useful tool and a liability.",
          "score": 2,
          "created_utc": "2026-02-26 20:47:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7oj3dx",
          "author": "punkpeye",
          "text": "Focus on distribution. I help maintain a few directories of MCP servers and it is shocking how few people put effort into documentation and containerization of their services. No one is going to install random scripts anymore.",
          "score": 2,
          "created_utc": "2026-02-27 10:56:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p2559",
              "author": "ialijr",
              "text": "That makes sense, I have not thought about it from this perspective. Is Docker basically the expected format now, or do you still see people getting away with npm/pip installs if the docs are solid?",
              "score": 1,
              "created_utc": "2026-02-27 13:16:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7laz69",
          "author": "Sancroth_2621",
          "text": "Wait. What are these apps? I just managed to get through building my mcps, skills and still reading on agents.md. Did a new thing show up again?",
          "score": 1,
          "created_utc": "2026-02-26 21:49:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lkd8r",
              "author": "darkwingdankest",
              "text": "I think they mean chat agents that... connect to standalone MCP servers. So like, the whole point of building standalone MCP servers",
              "score": 1,
              "created_utc": "2026-02-26 22:35:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ljx5h",
          "author": "darkwingdankest",
          "text": "I've built:\n\n- https://github.com/prmichaelsen/remember-mcp\n- https://github.com/prmichaelsen/agentbase\n- https://github.com/prmichaelsen/google-calendar-mcp\n- https://github.com/prmichaelsen/eventbrite-mcp\n\nand a multitent platform with a chat agent. It connects to each",
          "score": 1,
          "created_utc": "2026-02-26 22:33:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lqz40",
          "author": "huttobe",
          "text": "Standalone mcps are just too valuable and canâ€™t be compared to skills at all.",
          "score": 1,
          "created_utc": "2026-02-26 23:10:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1reqycw",
      "title": "MCPwner finds multiple 0-day vulnerabilities in OpenClaw",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1reqycw/mcpwner_finds_multiple_0day_vulnerabilities_in/",
      "author": "Comfortable-Ad-2379",
      "created_utc": "2026-02-25 21:45:55",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 0.87,
      "text": "I've been developing [MCPwner](https://github.com/Pigyon/MCPwner), an MCP server that lets your AI agents auto-pentest security targets. \n\nWhile most people are waiting for the latest flagship models to do the heavy lifting, I built this to orchestrate **GPT-4o** and **Claude 3.5 Sonnet** models that are older by today's standards but, when properly directed, are more than capable of finding deep architectural flaws using MCPwner.\n\nI recently pointed MCPwner at **OpenClaw**, and it successfully identified several 0-days that have now been issued official advisories. It didn't just find \"bugs\". it found critical logic bypasses and injection points that standard scanners completely missed.\n\n### The Findings:\n[Environment Variable Injection](https://github.com/openclaw/openclaw/security/advisories/GHSA-82g8-464f-2mv7)\n\n\n[ACP permission auto-approval bypass](https://github.com/openclaw/openclaw/security/advisories/GHSA-7jx5-9fjg-hp4m)\n\n\n[File-existence oracle info disclosure](https://github.com/openclaw/openclaw/security/advisories/GHSA-6c9j-x93c-rw6j)\n\n\n[safeBins stdin-only bypass](https://github.com/openclaw/openclaw/security/advisories/GHSA-4685-c5cp-vp95)\n\nThe project is still heavily in progress, but the fact that it's already pulling in multiple vulnerabilities and other CVEs I reported using mid-tier/older models shows its strength over traditional static analysis.\n\nIf you're building in the offensive AI space Iâ€™d love for you to put this through its paces. I'm actively looking for contributors to help sharpen the scanning logic and expand the toolkitPRs and feedback are more than welcome.\n\n**GitHub:** [https://github.com/Pigyon/MCPwner](https://github.com/Pigyon/MCPwner)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1reqycw/mcpwner_finds_multiple_0day_vulnerabilities_in/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7gjzuy",
          "author": "New_Animator_7710",
          "text": "From a defensive standpoint, projects like MCPwner highlight an emerging reality: AI-assisted offensive tooling is lowering the barrier to discovering complex vulnerabilities. we should be thinking not only about improving these systems, but also about how to build evaluation benchmarks and defensive countermeasures that anticipate AI-driven architectural probing.",
          "score": 3,
          "created_utc": "2026-02-26 04:27:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fek5l",
          "author": "BC_MARO",
          "text": "The ACP permission auto-approval bypass is the scariest one - once an attacker can escalate permissions without user confirmation, the whole security model collapses. This is exactly the problem Peta (peta.io) was built for: policy-based approvals and audit trails on every MCP tool call, so no tool fires without an explicit allow rule.",
          "score": 2,
          "created_utc": "2026-02-26 00:25:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7etzdb",
          "author": "barefootsanders",
          "text": "Great findings and interested to learn more. Up for swapping notes? We recently published a trust framework and scanner implementation for MCP bundles. Interested in ways of making MCP more secure and always up for collaboration. \n\nThis is our framework: [https://mpaktrust.org/](https://mpaktrust.org/) it outlines a number of security controls, mostly based on other OSS tooling all brought together.\n\nThe scanner scans bundles when they are published to [mpak.dev](https://mpak.dev/). Publishers get a security score and badge. Everything is open-source and self-hostable too.",
          "score": 1,
          "created_utc": "2026-02-25 22:35:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1re6fur",
      "title": "How do you get feedback on your MCP from AI Agents?",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1re6fur/how_do_you_get_feedback_on_your_mcp_from_ai_agents/",
      "author": "HaBuDeSu",
      "created_utc": "2026-02-25 07:11:26",
      "score": 11,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "We launched a MCP server and are getting usage but it's been very difficult for us to figure out what to improve. When our API users run into a problem they submit bug reports/feature requests etc. but we get none of that from the AI agents. Anyone figure anything out for this?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1re6fur/how_do_you_get_feedback_on_your_mcp_from_ai_agents/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7adnyq",
          "author": "naseemalnaji-mcpcat",
          "text": "We built MCPcat to help you get feedback from agents on their goals and we do higher level detection for when they fail. Would love your thoughts :) https://github.com/mcpcat\n\nhttps://mcpcat.io",
          "score": 2,
          "created_utc": "2026-02-25 07:21:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7alok7",
          "author": "BC_MARO",
          "text": "Logging tool-call inputs/outputs at the server layer is the only real signal you have. something like peta.io does this as part of an MCP control plane, but even basic structured server-side logging of every tool call with timestamps gives you enough to spot patterns and see where agents bail.",
          "score": 1,
          "created_utc": "2026-02-25 08:35:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b3rlr",
          "author": "Classic_Reference_10",
          "text": "What kinda feedback is this? As far as I could see - isn't it just APM observability hooked onto MCP tools?",
          "score": 1,
          "created_utc": "2026-02-25 11:20:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c8229",
          "author": "marsel040",
          "text": "If you want product analytics: we launched Yavio yesterday, its the first Open Source SDK for MCP product analytics, especially MCP Apps :)\n\n[https://github.com/teamyavio/yavio](https://github.com/teamyavio/yavio)",
          "score": 1,
          "created_utc": "2026-02-25 15:21:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cdiyp",
          "author": "AchillesDev",
          "text": "OTel + Langfuse, watch traces, annotate and address.",
          "score": 1,
          "created_utc": "2026-02-25 15:47:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fnn7y",
          "author": "jezweb",
          "text": "Test using mcp, have a minimal cli to make it easier during the build flow and check docs. Dog food it.",
          "score": 1,
          "created_utc": "2026-02-26 01:16:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rehql0",
      "title": "MCP tool discovery at scale - how we handle 15+ servers in Bifrost AI gateway",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rehql0/mcp_tool_discovery_at_scale_how_we_handle_15/",
      "author": "dinkinflika0",
      "created_utc": "2026-02-25 16:20:44",
      "score": 11,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I maintain **Bifrost**, and once you go past \\~10 MCP servers, things start getting messy.\n\nFirst issue: tool name collisions. Different MCP servers expose tools with the same names. For example, a `search_files` tool from a filesystem server and another from Google Drive. The LLM sometimes picks the wrong one, and the user gets weird results.  \nWhat worked for us was simple: namespace the tools. So now itâ€™s `filesystem.search_files` vs `gdrive.search_files`. The LLM can clearly see where each tool is coming from.\n\nThen thereâ€™s schema bloat. If you have \\~15 servers, you might end up with 80+ tools. If you dump every schema into every request, your context window explodes and token costs go up fast.  \nOur fix was tool filtering per request. We use virtual keys that decide which tools an agent can see. So each agent only gets the relevant tools instead of the full catalog.\n\nAnother pain point is the connection lifecycle. MCP servers can crash or just hang, and requests end up waiting on dead servers.  \nWe added health checks before routing. If a server fails checks, we temporarily exclude it and bring it back once it recovers.\n\nOne more thing that helped a lot once we had 3+ servers: **Code Mode**. Instead of exposing every tool schema, the LLM writes TypeScript to orchestrate tools. That alone cut token usage by 50%+ for us.\n\nIf you want to check it out:  \nCode: [https://git.new/bifrost](https://git.new/bifrost)  \nDocs: [https://getmax.im/docspage](https://getmax.im/docspage)",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1rehql0/mcp_tool_discovery_at_scale_how_we_handle_15/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7d5len",
          "author": "penguinzb1",
          "text": "the collision fix is right but you won't know if the namespacing actually resolves the misrouting until you've run it against the queries that originally triggered the wrong picks.",
          "score": 1,
          "created_utc": "2026-02-25 17:54:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cw4xo",
          "author": "BC_MARO",
          "text": "Per-agent tool filtering is the right call, but you still need the policy layer on top -- controlling which users or roles can invoke sensitive tools, not just what the LLM sees. Peta (peta.io) tackles that as a dedicated MCP control plane with RBAC and audit trails.",
          "score": 0,
          "created_utc": "2026-02-25 17:11:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fbw6d",
          "author": "kashishhora-mcpcat",
          "text": "Namespacing is pretty effective. Weâ€™ve also helped a couple of customers with lots of really similar tool and param names reduce a lot of the collisions and schema mismatches by namespacing and just naming things differently.\n\nOne counter intuitive idea that has worked: if you have 50+ tools and half of them all begin with â€œget_â€ youâ€™re going to increase the risk of collisions. Trying to vary it up or just removing any prefixes reduces collisions.\n\nIf you want a good way to detect collisions or other types of hallucinations or agent-specific errors, should check us out (mcpcat.io)! We have lots of features to help with debugging and analyzing how agents are using your MCP server.",
          "score": 0,
          "created_utc": "2026-02-26 00:11:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rat817",
      "title": "LinkedIn Custom MCP Server â€“ Enables AI agents to manage professional networking on LinkedIn by providing tools for posting updates, searching for jobs, and analyzing profiles. It facilitates secure interaction with the LinkedIn platform through OAuth 2.0 authentication and the Model Context Protoco",
      "subreddit": "mcp",
      "url": "https://glama.ai/mcp/servers/@SARAMALI15792/Linkedin_mcp_custom_server",
      "author": "modelcontextprotocol",
      "created_utc": "2026-02-21 15:00:52",
      "score": 10,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "server",
      "permalink": "https://reddit.com/r/mcp/comments/1rat817/linkedin_custom_mcp_server_enables_ai_agents_to/",
      "domain": "glama.ai",
      "is_self": false,
      "comments": [
        {
          "id": "o6meirz",
          "author": "Dan1eld",
          "text": " My Claw Bot will be stoked about this",
          "score": 2,
          "created_utc": "2026-02-21 16:25:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lxxmm",
          "author": "modelcontextprotocol",
          "text": "This server has 34 tools:\n\n- [linkedin_create_comment](https://glama.ai/mcp/servers/@SARAMALI15792/Linkedin_mcp_custom_server/tools/linkedin_create_comment) â€“ Add comments to LinkedIn posts, articles, or videos to engage with professional content and build network connections.\n- [linkedin_create_comment](https://glama.ai/mcp/servers/@SARAMALI15792/Linkedin_mcp_custom_server/tools/linkedin_create_comment) â€“ Add comments to LinkedIn posts, articles, or videos by specifying the content URN and comment text. This tool enables engagement with professional content through the LinkedIn Custom MCP Server.\n- [linkedin_create_image_post](https://glama.ai/mcp/servers/@SARAMALI15792/Linkedin_mcp_custom_server/tools/linkedin_create_image_post) â€“ Create LinkedIn posts with images by specifying text, image source, and visibility settings to share content with professional connections.\n- [linkedin_create_image_post](https://glama.ai/mcp/servers/@SARAMALI15792/Linkedin_mcp_custom_server/tools/linkedin_create_image_post) â€“ Create LinkedIn posts with images to share professional updates and content. Upload images from local files or URLs and set visibility to public or connections-only.\n- [linkedin_create_post](https://glama.ai/mcp/servers/@SARAMALI15792/Linkedin_mcp_custom_server/tools/linkedin_create_post) â€“ Create and publish text updates to your LinkedIn feed with customizable visibility settings for professional networking.\n- [linkedin_create_post](https://glama.ai/mcp/servers/@SARAMALI15792/Linkedin_mcp_custom_server/tools/linkedin_create_post) â€“ Create and publish text posts to your LinkedIn feed with customizable visibility settings for professional networking.\n- [linkedin_delete_comment](https://glama.ai/mcp/servers/@SARAMALI15792/Linkedin_mcp_custom_server/tools/linkedin_delete_comment) â€“ Remove unwanted or inappropriate comments from LinkedIn posts using the LinkedIn Custom MCP Server. This tool deletes specific comments by their URN to maintain professional content quality.\n- [linkedin_delete_comment](https://glama.ai/mcp/servers/@SARAMALI15792/Linkedin_mcp_custom_server/tools/linkedin_delete_comment) â€“ Remove unwanted or incorrect comments from LinkedIn posts using this tool. Specify the comment and parent post identifiers to delete comments from your professional content.\n- [linkedin_delete_post](https://glama.ai/mcp/servers/@SARAMALI15792/Linkedin_mcp_custom_server/tools/linkedin_delete_post) â€“ Remove LinkedIn posts by specifying their URN identifier to manage your professional content and maintain your profile's relevance.\n- [linkedin_delete_post](https://glama.ai/mcp/servers/@SARAMALI15792/Linkedin_mcp_custom_server/tools/linkedin_delete_post) â€“ Remove a LinkedIn post by specifying its unique URN identifier to manage your professional content.",
          "score": 0,
          "created_utc": "2026-02-21 15:00:53",
          "is_submitter": true,
          "replies": []
        }
      ]
    }
  ]
}