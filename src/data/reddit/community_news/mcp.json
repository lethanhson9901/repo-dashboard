{
  "metadata": {
    "last_updated": "2026-02-11 03:30:00",
    "time_filter": "week",
    "subreddit": "mcp",
    "total_items": 20,
    "total_comments": 109,
    "file_size_bytes": 140399
  },
  "items": [
    {
      "id": "1r09tpd",
      "title": "5 MCPs that genuinely made me quicker",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r09tpd/5_mcps_that_genuinely_made_me_quicker/",
      "author": "Stunning-Worth-5022",
      "created_utc": "2026-02-09 17:05:38",
      "score": 284,
      "num_comments": 47,
      "upvote_ratio": 0.96,
      "text": "I have been consistently putting MCPs to use in my daily real work, not just for showing demos. Initially, these ones ignited quite a hype, and now, they have grown on me. What mattered to me: setup should be painless, they shouldnt flake out, and I should notice when theyre gone.\n\n### **GitHub MCP** [https://github.com/github/github-mcp-server](https://github.com/github/github-mcp-server)\n\nThis was the thing that really gave the agent the feel that it was working within the repo. Issues, commits, PR context, file history, all without copy, pasting links or dumping files into prompts. Seriously cant imagine doing heavy, duty work without this feature now.\n\n### **CodeGraphContext MCP** [https://github.com/CodeGraphContext/CodeGraphContext](https://github.com/CodeGraphContext/CodeGraphContext)\n\nThis one is the quiet time-saving hero. It stores a structured graph of the codebase internally at all times, so the agent is pre-equipped with an understanding of how files, functions, and classes relate to each other. Refactors and what breaks if I change this? become pretty reliable.\n\n### **Context7 MCP** [https://github.com/upstash/context7](https://github.com/upstash/context7)\n\nThis one made my agents stop guessing APIs. Whenever I request something using a library or framework, it automatically pulls the correct docs. I open docs tabs so rarely now.\n\n### **Firecrawl MCP / Jina Reader MCP** [https://github.com/mendableai/firecrawl](https://github.com/mendableai/firecrawl) [https://github.com/jina-ai/reader](https://github.com/jina-ai/reader)\n\nBoth of these are wonderful at converting dirty web pages into spotless Markdown. Great for blogs, specs, or lengthy articles where you just want the content, not the site.\n\n### **Figma MCP** [https://github.com/GLips/Figma-Context-MCP](https://github.com/GLips/Figma-Context-MCP)\n\nDesign â†’ code, but done properly. Instead of screenshots, the agent sees real Figma structure: layouts, components, variants, tokens. Frontend output is noticeably closer to the design.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/mcp/comments/1r09tpd/5_mcps_that_genuinely_made_me_quicker/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o4gygt3",
          "author": "Desperate-Ad-9679",
          "text": "Thanks for mentioning CodeGraphContext, people are now realizing how much context window aka money can be saved by simpler tweaks!! Btw, I am the maintainer of CodeGraphContext.  ",
          "score": 59,
          "created_utc": "2026-02-09 18:00:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ieq03",
              "author": "m3kw",
              "text": "How would it save context?  Tools search for the required context, this MCP does what?",
              "score": 3,
              "created_utc": "2026-02-09 22:15:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4k3cvw",
                  "author": "Desperate-Ad-9679",
                  "text": "Imagine 3 scenarios -\nYou are changing a function and want to see the impact across the entire project, grep - read - loop over all open files - read one by one. But here you just get a function call chain and can directly use it as context.\n\nAnother scenario would be the need to find dead code and refactor it, you would need to read tonnes of files to find dead code, even then lots of hallucinations could corrupt the response, here you just call a single tool.\n\nSimilarly you can find most complex functions in a single tool call to start simplifying and refactoring them rather than opening all files to find the right function",
                  "score": 1,
                  "created_utc": "2026-02-10 04:02:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4k5nxy",
              "author": "Someoneoldbutnew",
              "text": "right on, graph based memory beats the hell out of grep",
              "score": 2,
              "created_utc": "2026-02-10 04:18:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ku3zo",
                  "author": "Desperate-Ad-9679",
                  "text": "Very true",
                  "score": 1,
                  "created_utc": "2026-02-10 07:34:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4oghyc",
              "author": "jii0",
              "text": "How have I missed this one! Will definitely give it a try right now.",
              "score": 2,
              "created_utc": "2026-02-10 20:31:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ojnqs",
                  "author": "Desperate-Ad-9679",
                  "text": "Sure, would love to get your feedback!",
                  "score": 1,
                  "created_utc": "2026-02-10 20:46:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4k1pxy",
              "author": "thepreppyhipster",
              "text": "thanks for your contribution!!",
              "score": 1,
              "created_utc": "2026-02-10 03:51:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4k3evv",
                  "author": "Desperate-Ad-9679",
                  "text": "Happy to help!",
                  "score": 1,
                  "created_utc": "2026-02-10 04:03:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4kgg5u",
              "author": "Logical_Armadillo390",
              "text": "Hey, love the idea. Did you pull this data from LSPs? Given they do a similar thing. I'm not an expert, just curious.",
              "score": 1,
              "created_utc": "2026-02-10 05:36:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ku1ra",
                  "author": "Desperate-Ad-9679",
                  "text": "Thanks for your appreciation, also this data isn't pulled by LSPs but it works by using tree sitters on each file and writing custom resolution logic for each language. This lets us have a very fast speed in exchange for a little drop of accuracy. We are now also integrating SCIP (A better version of LSP) for users who prefer accuracy over speed, Python has already been integrated.",
                  "score": 3,
                  "created_utc": "2026-02-10 07:33:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gxffy",
          "author": "OnRedditAtWorkRN",
          "text": "I find the GitHub mcp grossly over rated. Historically it crowded the context window heavily, that's less of a pain point with recent mcp changes, but I still don't feel like I get any more out of it than I do with the agent using the gh cli",
          "score": 12,
          "created_utc": "2026-02-09 17:55:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4i5zik",
              "author": "phil",
              "text": "Exactly. Thatâ€™s the consensus at my work too. Is there any advantage at all to the GitHub MCP server over gh command line tools?",
              "score": 4,
              "created_utc": "2026-02-09 21:31:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gvi0k",
          "author": "penguinzb1",
          "text": "the GitHub and CodeGraphContext combo is solidâ€”having repo structure awareness without manually feeding context makes refactors way less fragile.\n\none thing I've noticed with MCP integrations is they can break silently when the server flakes or returns unexpected output. the agent just keeps going with stale context and you don't catch it until the PR is already wrong. we've been working on simulating these failure modes before they hit prodâ€”helps spot when an MCP dependency is quietly degrading your workflow.\n\nContext7 is underrated for exactly the reason you mentioned. docs drift is real and having fresh reference material automatically pulled in saves so much debug time.",
          "score": 7,
          "created_utc": "2026-02-09 17:46:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hf9tb",
              "author": "Stunning-Worth-5022",
              "text": "Exactly my point!!",
              "score": 2,
              "created_utc": "2026-02-09 19:18:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4kqnnm",
          "author": "Electrical_Walrus537",
          "text": "You can also try this. When I run the browser tests using it, everything works perfectly. [Browser devtools mcp](https://www.npmjs.com/package/browser-devtools-mcp)",
          "score": 4,
          "created_utc": "2026-02-10 07:02:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4h0xzd",
          "author": "BC_MARO",
          "text": "Context7 is the one I keep recommending to people. The number of times I've watched an agent hallucinate an API that doesn't exist anymore because it's working off training data from 2 years ago... Context7 basically kills that problem.\n\nI'd also throw in the sequential thinking MCP if you haven't tried it. For multi-step refactors where the agent needs to plan before executing, it keeps things way more coherent than just letting it go step by step.",
          "score": 4,
          "created_utc": "2026-02-09 18:11:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l2ewj",
              "author": "cellulosa",
              "text": "Do you need to tell the agent to use it explicitly? And does it know which exact documentation to pick?",
              "score": 1,
              "created_utc": "2026-02-10 08:54:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mkjnj",
                  "author": "BC_MARO",
                  "text": "Usually yes: unless your agent has an automatic tool-selection policy, youâ€™ll want to explicitly instruct it to use Context7 (e.g. â€œuse Context7 for the current docs before answering / codingâ€).\n\nFor picking the right docs, donâ€™t rely on it guessing. Give it an unambiguous target: package/repo name + version (or a specific URL/commit/branch). If you only say â€œthe docsâ€, it may grab a close match and youâ€™re back to stale APIs.",
                  "score": 1,
                  "created_utc": "2026-02-10 15:16:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4pm7r6",
                  "author": "BC_MARO",
                  "text": "In practice, yes: you get much more reliable behavior if you tell the agent when to call the tool (a simple rule + 1-2 examples goes a long way).\n\nOn docs: it usually wonâ€™t pick the right page on its own. I either scope a docs/search tool to the exact docs set (so retrieval does the routing), or I give it an explicit â€œdocs indexâ€ mapping in the tool description (feature -> URL/section). If there are multiple versions, make the version rule explicit too.",
                  "score": 1,
                  "created_utc": "2026-02-11 00:00:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4guawv",
          "author": "sorvendral",
          "text": "Nice ones thanks bro",
          "score": 2,
          "created_utc": "2026-02-09 17:40:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4h08o9",
          "author": "Important_Storage123",
          "text": "Which Figma MCP is better? This one or the Official from Figma itself?",
          "score": 1,
          "created_utc": "2026-02-09 18:08:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hepap",
              "author": "Stunning-Worth-5022",
              "text": "I think this one worked for me better!",
              "score": 1,
              "created_utc": "2026-02-09 19:16:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4h8pji",
          "author": "stibbons_",
          "text": "I do not see the use of codeCraphContext, Claude code or viscose copilot are both excellent at analyzing the code. And in vscode it has access to the language server.\n\nContext7: still no free equivalent for internal libraries ?",
          "score": 1,
          "created_utc": "2026-02-09 18:47:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hf76t",
              "author": "Stunning-Worth-5022",
              "text": "I have used Antigravity, claude code, and cursor for my work but had a terrible experience when working on large codebases. All of them started hallucinating and consuming tonnes of tokens just to find the right function to edit or the right file to find an implementation. This helped me find the right context across those files clutter, hence recommended.",
              "score": 1,
              "created_utc": "2026-02-09 19:18:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4hcnbk",
          "author": "Maasu",
          "text": "Oh I just wrote my own version of code graph context, will check that out as it's likely done a better job. Can't remember if I looked at it or not, maybe I did but it didn't support vb.net which I have a use case for. \n\nOnly thing is add to the list is a memory mcp, I wrote my own version of that as well. Genuinely did write it as well as it was before opus 4.5 when I felt comfortable letting the models take the reigns. I do use AI to maintain it these days.",
          "score": 1,
          "created_utc": "2026-02-09 19:06:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hjdc4",
          "author": "NebulaNavigator2049",
          "text": "ChromeDevTools anyone?",
          "score": 1,
          "created_utc": "2026-02-09 19:38:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4i14xz",
          "author": "GentoroAI",
          "text": "thanks! figma MCP is great",
          "score": 1,
          "created_utc": "2026-02-09 21:07:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4iuqxz",
          "author": "brads0077",
          "text": "I would add Docker MCP Server",
          "score": 1,
          "created_utc": "2026-02-09 23:39:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4iz32o",
          "author": "TheLostWanderer47",
          "text": "Good picks. Iâ€™d add a web access MCP for cases where agents need live data. Weâ€™ve used one ([Bright Data MCP Server](https://github.com/brightdata/brightdata-mcp)) to keep browsing and extraction consistent instead of wiring custom scrapers per project. Low effort, high reliability.",
          "score": 1,
          "created_utc": "2026-02-10 00:04:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4j1v92",
          "author": "an80sPWNstar",
          "text": "I use searxng for the web search and it has been solid.",
          "score": 1,
          "created_utc": "2026-02-10 00:19:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4j22xt",
          "author": "nooruponnoor",
          "text": "Have you compared Github CLI vs MCP?",
          "score": 1,
          "created_utc": "2026-02-10 00:20:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4je24y",
          "author": "TrvlMike",
          "text": "Howâ€™s token usage on them though?",
          "score": 1,
          "created_utc": "2026-02-10 01:30:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ltn6e",
              "author": "Stunning-Worth-5022",
              "text": "GitHub consumes the most, whereas codegraphcontext and context7 are comparatively on the lower end, based on the functionalities they provide",
              "score": 2,
              "created_utc": "2026-02-10 12:47:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4lw3jr",
                  "author": "TrvlMike",
                  "text": "Thanks! Your post finally got me to install Context7 too",
                  "score": 1,
                  "created_utc": "2026-02-10 13:03:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4klvi9",
          "author": "Bulky_Ad738",
          "text": "Hey all,\n\nDo you think an MCP to help you save your best prompts, skill, agents, UI/UX, tech stack etc, in your own library **right from your terminal or IDE** would be useful?\n\nLike imagine you're in your project, and you came up with a great and refined prompts to format a document. You just ask the agent to save it and it's done. Or maybe you created a beautiful UI and you want to save the details. Just tell your agent to do it and it's save for future use.\n\nSame if you want to retrieve anything from the library. Just ask your agent.\n\nIs it useful?",
          "score": 1,
          "created_utc": "2026-02-10 06:20:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nd7gk",
          "author": "Pitiful-Minute-2818",
          "text": "Greb mcp really good i mean it does what code graph mcp does but without indexing overhead",
          "score": 1,
          "created_utc": "2026-02-10 17:30:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ob0q3",
          "author": "mugiltsr",
          "text": "Reg GitHub MCP,  why can't we clone the repo and analyse using AI agent locally ?",
          "score": 1,
          "created_utc": "2026-02-10 20:05:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4olu0v",
          "author": "NikolaiAce",
          "text": "Thank you",
          "score": 1,
          "created_utc": "2026-02-10 20:56:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyncmd",
      "title": "CodeGraphContext - An MCP server that indexes your codebase into a graph database to provide accurate context to AI assistants and humans",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/gallery/1qyncmd",
      "author": "Desperate-Ad-9679",
      "created_utc": "2026-02-07 19:44:19",
      "score": 170,
      "num_comments": 59,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1qyncmd/codegraphcontext_an_mcp_server_that_indexes_your/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o44vp82",
          "author": "Otherwise_Wave9374",
          "text": "Congrats on the momentum, those adoption numbers are wild. Graph-based context feels like the direction most serious coding agents need, because chunk-RAG turns into token spam fast.\n\nCurious how you handle dynamic repos: do you incrementally update the graph on file change, and do you have a strategy to avoid stale edges when refactors happen?\n\nAlso if youre comparing approaches, Ive seen some good discussions around agent context strategies here: https://www.agentixlabs.com/blog/\n\nLooks really promising.",
          "score": 8,
          "created_utc": "2026-02-07 20:01:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44wpxl",
              "author": "Desperate-Ad-9679",
              "text": "First of all thanks a lot for the appreciation, I too was annoyed by the problems that chunking causes.  \nFor dynamic repos, we do watch files for live changes using watch\\_dog and the incremental updates are done by replacing all nodes, including and contained within the node. This helps it to be fast and accurate.\n\nThanks again for the link, will appreciate going through the details mentioned there.",
              "score": 2,
              "created_utc": "2026-02-07 20:06:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o44uzm2",
          "author": "Desperate-Ad-9679",
          "text": "Another clue to the next version - We are launching CodeGraphContext as a VS code extension soon âœ¨â­, Do star the repository please and join the Discord server for latest news",
          "score": 8,
          "created_utc": "2026-02-07 19:57:12",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o45ffco",
          "author": "Tobi-Random",
          "text": "https://gitlab-org.gitlab.io/rust/knowledge-graph/",
          "score": 5,
          "created_utc": "2026-02-07 21:47:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45grox",
              "author": "Desperate-Ad-9679",
              "text": "Damn that's a very close project like mine, but I cant see much progress in there. But yeah definitely they are a complete org so the project even though a little slow will catch up traction sooner or later.",
              "score": 3,
              "created_utc": "2026-02-07 21:54:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o46jdjq",
          "author": "BC_MARO",
          "text": "Graph-based code indexing is such a better approach than just chunking files. How are you handling incremental updates when only a few files change? That's usually where the perf bottleneck shows up.",
          "score": 2,
          "created_utc": "2026-02-08 01:45:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47xhbt",
              "author": "Desperate-Ad-9679",
              "text": "Whenever we change a file, we delete the node and all related edges to this. Then only that file is brought-back and from the cache we do path resolution for imports. ",
              "score": 2,
              "created_utc": "2026-02-08 08:01:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o47xi24",
                  "author": "Desperate-Ad-9679",
                  "text": "Thanks for your kind words!",
                  "score": 2,
                  "created_utc": "2026-02-08 08:01:23",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47szsg",
          "author": "debackerl",
          "text": "This is really a great idea! Looks awesome. Now, if you allow me, why not combine both worlds: if the AI doesn't know from why 'node' to start from, compute an embedding for each function. Then you can tell, 'give me the function validating my shopping cart', and then give me all functions calling it.",
          "score": 2,
          "created_utc": "2026-02-08 07:19:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47xqg4",
              "author": "Desperate-Ad-9679",
              "text": "Thanks for your appreciation!  \n  \nExactly, this is something we have in our bucket list. I have been writing some small algorithms to identify and store vector embeddings for nodes, cluster of nodes. This is an open problem and so needs a lot of brainstorming as of now...",
              "score": 2,
              "created_utc": "2026-02-08 08:03:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o485037",
          "author": "ratek-20",
          "text": "Great Job, I'll definitely give it a try!\nDo you think it can be expanded to services?\nFor example order-service calls warehouse-service via rest api -> they can be 2 linked nodes of the graph",
          "score": 2,
          "created_utc": "2026-02-08 09:12:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o485awm",
              "author": "Desperate-Ad-9679",
              "text": "Thanks for your kind words,  \nRight now it doesnt have, but thanks for the suggestion. Will add it in the next version.",
              "score": 2,
              "created_utc": "2026-02-08 09:15:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o485etu",
                  "author": "Desperate-Ad-9679",
                  "text": "Also it can definitely be expanded because we already parse entire codebases.",
                  "score": 2,
                  "created_utc": "2026-02-08 09:16:03",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4g8id5",
          "author": "noobfivered",
          "text": "This is a way forward I think! I'm working on something simmilar",
          "score": 2,
          "created_utc": "2026-02-09 15:56:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gb6go",
              "author": "Desperate-Ad-9679",
              "text": "Great",
              "score": 1,
              "created_utc": "2026-02-09 16:09:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4h00ts",
          "author": "Special-Click-7607",
          "text": "Bro Iâ€™ve been using it. Thank you a lot. Wondering if you would like to share concrete tips or examples for best use and ways to use it.\nThanks!!",
          "score": 2,
          "created_utc": "2026-02-09 18:07:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4h0kpi",
              "author": "Desperate-Ad-9679",
              "text": "Thanks for using it, Hopefully you can find reference use-cases in the docs - [https://codegraphcontext.github.io/use\\_cases\\_detailed/](https://codegraphcontext.github.io/use_cases_detailed/)\n\nAlso, there are 40 simpler examples in the cookbook file, [https://github.com/CodeGraphContext/CodeGraphContext/blob/main/organizer/cookbook.md](https://github.com/CodeGraphContext/CodeGraphContext/blob/main/organizer/cookbook.md)",
              "score": 2,
              "created_utc": "2026-02-09 18:09:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nlbpd",
          "author": "EvanGarp",
          "text": "I've been trying to use this tool this week. How does this handle persistence across sessions? Maybe I am using it wrong but it keeps making me re-index my code base across sessions. Is it designed to work that way or should it be saving the indexes somewhere that I can access again later. Can I access the indexes outside of the MCP functionality? If so where?",
          "score": 2,
          "created_utc": "2026-02-10 18:07:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nmx90",
              "author": "Desperate-Ad-9679",
              "text": "Hello, thanks for using it. Also the data is persistent and saved in a graph database. You can look at the [https://codegraphcontext.github.io/reference/cli\\_master/](https://codegraphcontext.github.io/reference/cli_master/) docs for CLI commands. If you are working on a codebase which is changing in real-time, then you need to do cgc watch, which spawns a process tracking all live changes. You dont need to do re-indexing.",
              "score": 1,
              "created_utc": "2026-02-10 18:14:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4p04i8",
                  "author": "EvanGarp",
                  "text": "Was able to figure it out. Was trying to use my neo4j database but the cgc config command wasn't actually updating and was defaulting to the falkorDB. Was able to use Claude to update the config files and then it started working. There still appears to be an issue the getting db stats functionality where it always returns zeros but when I query the neo4j graph directly I can see that it is populated. ",
                  "score": 1,
                  "created_utc": "2026-02-10 22:02:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o466lpq",
          "author": "bargaindownhill",
          "text": "no instructions for roo or kiro?",
          "score": 1,
          "created_utc": "2026-02-08 00:26:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47h9xf",
              "author": "Desperate-Ad-9679",
              "text": "Oops, I raised an issue for this but forgot the fact that I got no PR. Will do this by the next version (perhaps in a day)",
              "score": 2,
              "created_utc": "2026-02-08 05:36:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o47zi93",
                  "author": "bargaindownhill",
                  "text": "thanks!",
                  "score": 1,
                  "created_utc": "2026-02-08 08:20:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o47xcw8",
              "author": "Desperate-Ad-9679",
              "text": "There's already an option for roocode, checkout by doing \\`cgc mcp setup\\`",
              "score": 2,
              "created_utc": "2026-02-08 08:00:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4705i4",
          "author": "I_EAT_THE_RICH",
          "text": "Funny enough, I was working on something like this about 6 months ago with the same intent. I think managing context is extremely important and making your codebase queryable in this fashion makes a ton of sense. Good work.",
          "score": 1,
          "created_utc": "2026-02-08 03:32:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47xkz2",
              "author": "Desperate-Ad-9679",
              "text": "Thanks a lot for your kind words, No more- No less, only the appropriate context makes sense!!",
              "score": 1,
              "created_utc": "2026-02-08 08:02:08",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4876h2",
              "author": "raiffuvar",
              "text": "A lot of ppl were working on smth like this and later claude showed that grep is enough. (At least I stopped trying with opus4 cause it eventually will catch up).",
              "score": 1,
              "created_utc": "2026-02-08 09:33:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4888bz",
                  "author": "Desperate-Ad-9679",
                  "text": "Perhaps, but I am unsure if they can find perfect call chains or dead code??",
                  "score": 1,
                  "created_utc": "2026-02-08 09:43:04",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o49fjw9",
                  "author": "I_EAT_THE_RICH",
                  "text": "It's a fair consideration that depending on the model it may not be necessary. Can you provide any links demonstrating grep vs a knowledge graph? I assume there might be some tests out there but haven't found any myself yet.",
                  "score": 1,
                  "created_utc": "2026-02-08 15:06:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o487hoq",
          "author": "raiffuvar",
          "text": "What's the difference between LSP? \nDoes it parse docs?\n\nUpd: did not dig in but small advice: return tree and file/method annotations and lineno.",
          "score": 1,
          "created_utc": "2026-02-08 09:36:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o488fds",
              "author": "Desperate-Ad-9679",
              "text": "LSPs are way slow than my custom resolution logic (though it adds a little inconsistencies sometimes as of now), also it is polyglot but LSPs are not. One more thing is that it doesnt need any external bundle installations like LSPs need for each lang.\n\nAdding Docs, is the second stage. Will add them by the next version release ",
              "score": 1,
              "created_utc": "2026-02-08 09:44:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4895qv",
          "author": "maverick_soul_143747",
          "text": "I am building something on my own and was planning to handle it with contexf7, obsidian but I am going to try this. This looks exciting",
          "score": 1,
          "created_utc": "2026-02-08 09:51:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o489c8n",
              "author": "Desperate-Ad-9679",
              "text": "Great, good luck for your quest!",
              "score": 2,
              "created_utc": "2026-02-08 09:53:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ak01w",
          "author": "foobarrister",
          "text": "Question - are you able  to handle many repos? Like a 100s of microservices that put together comprise a large distributed system?\n\n(awesome project btw!)",
          "score": 1,
          "created_utc": "2026-02-08 18:23:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4avl7q",
              "author": "Desperate-Ad-9679",
              "text": "Yes, it is meant to handle multiple repos (be they related or unrelated), just index them by putting all of them in a single folder... Or remove ones using cgcignore",
              "score": 1,
              "created_utc": "2026-02-08 19:17:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4aul3r",
          "author": "redlotusaustin",
          "text": "How does this handle multiple projects/repos; related & unrelated? Obviously you don't want context mixing on unrelated projects but you might if they are related.\n\nHave you had any feedback about using this on Ubuntu, since it manages Python packages via APT? Generally I create a venv for installing things from pip, but that means the MCP would be \"within\" the venv so \"cgc\" wouldn't be available as a command.",
          "score": 1,
          "created_utc": "2026-02-08 19:12:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4avayj",
              "author": "Desperate-Ad-9679",
              "text": "It is meant to handle multiple projects be it related or unrelated. Also you can install it via venv and then it handles everything on its own but if it doesn't run, then just change the command in MCP.json to be the exact command from your specific venv. If you still face any problem ping me here.",
              "score": 1,
              "created_utc": "2026-02-08 19:16:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4bat7x",
                  "author": "redlotusaustin",
                  "text": "Cool, I'll give it a shot. Thanks!",
                  "score": 2,
                  "created_utc": "2026-02-08 20:33:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4d7mi7",
          "author": "Bulky_Ad738",
          "text": "This is interesting. I donâ€™t think I saw something like it so far. Well done.",
          "score": 1,
          "created_utc": "2026-02-09 02:45:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4dnsp0",
              "author": "Desperate-Ad-9679",
              "text": "Thanks for the appreciation!",
              "score": 1,
              "created_utc": "2026-02-09 04:18:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4dp3w0",
          "author": "BLANkals",
          "text": "I built something like this for my company about 6 months ago. No one really understood what I was so I didnâ€™t release it. I use it all the time though. The graph is based the idea that the nodes are entities (something that can build a connection to something else) and the edges are the relationships between them. For example a file can define a function. A function can use a variable or read a table in some other service like big query. LLM can then start at any point and hop to the next node.",
          "score": 1,
          "created_utc": "2026-02-09 04:27:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ftjn6",
              "author": "Desperate-Ad-9679",
              "text": "That's true, its too much useful. Just the people are unaware of the actual possibilities.",
              "score": 1,
              "created_utc": "2026-02-09 14:40:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fqsk5",
          "author": "edge-case42",
          "text": "Do you think this could help identify non used pieces of code in typescript?",
          "score": 1,
          "created_utc": "2026-02-09 14:25:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fstzi",
              "author": "Desperate-Ad-9679",
              "text": "yep definitely, it would do this easily...",
              "score": 1,
              "created_utc": "2026-02-09 14:36:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jx85f",
          "author": "notgilly",
          "text": "What are your thoughts on some cli agents starting to integrate the language servers into their agents. That would effectively have the same benefits, no?",
          "score": 1,
          "created_utc": "2026-02-10 03:23:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4k3wnj",
              "author": "Desperate-Ad-9679",
              "text": "No not entirely the same benefit, you can't find dead code, complexity of functions in a code, indirect call chains in a single query etc etc. Also an LSP is way slower than CGC because it trades-off speed for a little accuracy, as speed is the utmost importance aspect in coding tools if the accuracy doesn't drop much.",
              "score": 1,
              "created_utc": "2026-02-10 04:06:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4k82pm",
                  "author": "notgilly",
                  "text": "Ahh I can see the benefit of indirect call chains in a single call. \nI donâ€™t view the LSP being a bit slower as a huge issue, most times I let the agents do its thing for a while instead of watching the output. I also generally already know where Iâ€™m working in so discovery isnâ€™t that big an issue for me. \n\nThe reason why Iâ€™m curious is because I have a strong suspicion that most of the future â€œintelligenceâ€ gains in foundational models will be from training these agentic tricks into the model itself. For example, chaining shell commands vs processing individual commands. \n\nMy guess is that future foundational models will be better at navigating with language servers vs the graph approach. Just because everyone will have an lsp, but not everyone will have a graphed tool.",
                  "score": 1,
                  "created_utc": "2026-02-10 04:34:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ld2vf",
          "author": "Ok_Supermarket3382",
          "text": "Very cool project! Been waiting for someone to build this for a while haha. For the queries are you doing text to cypher?",
          "score": 1,
          "created_utc": "2026-02-10 10:37:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ld5np",
              "author": "Desperate-Ad-9679",
              "text": "Yeah, it does!",
              "score": 1,
              "created_utc": "2026-02-10 10:37:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ldcde",
                  "author": "Ok_Supermarket3382",
                  "text": "This is the way to do it! Are you preserving comments, file metadata, directories etc ?",
                  "score": 1,
                  "created_utc": "2026-02-10 10:39:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qwvl5u",
      "title": "API â†’ MCP Server, in 30 seconds.",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1qwvl5u/api_mcp_server_in_30_seconds/",
      "author": "dorukyelken",
      "created_utc": "2026-02-05 19:51:43",
      "score": 39,
      "num_comments": 9,
      "upvote_ratio": 0.81,
      "text": "Turn your existing APIs into an MCP Server without rewriting anything.\n\nBuilt for teams experimenting with MCP, agents, and tool-based AI workflows.\n\nThis is a beta, free-to-try personal project.\n\nTry it out and share feedback ðŸ‘‡\n\n[https://apitomcphost.com/](https://apitomcphost.com/)\n\nHow it works (short demo):\n\nhttps://reddit.com/link/1qwvl5u/video/114wshoj55ig1/player\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1qwvl5u/api_mcp_server_in_30_seconds/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o3tirtn",
          "author": "AchillesDev",
          "text": "Ah yes, the ubiquitous MCP antipattern engine. \n\n[Don't convert your REST APIs to MCP servers](https://www.jlowin.dev/blog/stop-converting-rest-apis-to-mcp)",
          "score": 17,
          "created_utc": "2026-02-06 00:53:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t2nr7",
          "author": "Obvious-Car-2016",
          "text": "Umm just use the Claude code mcp builder skill",
          "score": 4,
          "created_utc": "2026-02-05 23:21:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ubu9v",
          "author": "BC_MARO",
          "text": "interesting approach. wrappers like this are great for getting started quickly, but curious how you're handling auth + rate limiting on the mcp side (api keys vs oauth, per-user vs per-app, etc)?\n\ni've found that's usually where these \"openapi -> tool surface\" bridges get tricky in production.",
          "score": 1,
          "created_utc": "2026-02-06 03:48:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o416618",
          "author": "Physical_Ideal_3949",
          "text": "Are u using fastmcp to do this",
          "score": 1,
          "created_utc": "2026-02-07 05:04:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42iys4",
          "author": "jezweb",
          "text": "Then you start to use it with an ai agent and realise itâ€™s inefficient junk and you start over properly.",
          "score": 1,
          "created_utc": "2026-02-07 12:29:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o439ulr",
          "author": "Able-Classroom7007",
          "text": "An approach that I really like for API to MCP is to \"Code Mode\" like Turbopuffer MCP has.\n\nThe idea is you have 2 tools: API docs + write and execute code in a sandbox that only has network access to that API endpoint.\n\nSuper clean and great for complex APIs.\n\n[https://github.com/turbopuffer/turbopuffer-typescript/tree/main/packages/mcp-server](https://github.com/turbopuffer/turbopuffer-typescript/tree/main/packages/mcp-server)",
          "score": 1,
          "created_utc": "2026-02-07 15:12:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rxl5u",
          "author": "finance-mcp-001",
          "text": "This is quite cool. Any link to a GitHub repo? Iâ€™m very curious about the conversion methodology.",
          "score": 1,
          "created_utc": "2026-02-05 19:58:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3yahvl",
          "author": "connectgeeks",
          "text": "If youâ€™re exploring MCP beyond toy examples, having a curated list really helps.  \nIâ€™ve been bookmarking tools, servers, agents, and real-world MCP implementations as I come across them.  \nThis collection might save some digging [https://github.com/JustInCache/awesome-mcp-collection](https://github.com/JustInCache/awesome-mcp-collection).If you know any good MCP projects that arenâ€™t listed yet, you can add them.",
          "score": 1,
          "created_utc": "2026-02-06 19:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rz82a",
          "author": "Otherwise_Wave9374",
          "text": "This is a great pitch, converting an OpenAPI into an MCP server is exactly the kind of glue that makes agent tooling practical for teams. Not having to rewrite your backend just to \"make it agent friendly\" is huge.\n\nCurious if you support auth flows cleanly (API keys vs OAuth) and how you handle tool schemas for long running jobs. I have been tracking MCP and tool calling patterns here too: https://www.agentixlabs.com/blog/",
          "score": -2,
          "created_utc": "2026-02-05 20:06:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvgt6g",
      "title": "Built the most comprehensive Ghidra MCP Server â€” 110 tools for AI-powered reverse engineering (v2.0.0)",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1qvgt6g/built_the_most_comprehensive_ghidra_mcp_server/",
      "author": "XerzesX",
      "created_utc": "2026-02-04 06:12:09",
      "score": 27,
      "num_comments": 5,
      "upvote_ratio": 0.95,
      "text": "Hey r/mcp,\n\nJust released v2.0.0 of my Ghidra MCP Server with **110 tools** â€” by far the most feature-complete Ghidra integration for MCP. For context, the most popular one (LaurieWired's, 7K+ stars) has about 15 tools.\n\n## What makes this different\n\nðŸ”§ **110 MCP tools** covering decompilation, disassembly, analysis, annotation, data types, and project management\n\nðŸ”„ **Cross-binary function documentation transfer** â€” hash functions by behavior (not addresses), so when a new binary version drops, all your annotations carry over automatically. No more re-doing work across versions.\n\nâš¡ **Batch operations** â€” analyze multiple functions/addresses in one call (93% API call reduction vs. one-at-a-time)\n\nðŸ³ **Headless mode + Docker** â€” run analysis pipelines without the GUI. `docker compose up` and you're analyzing binaries.\n\nðŸ“‹ **.env configuration** â€” no hardcoded paths, everything configurable\n\n## Real-world usage\n\nI've been using this for Diablo 2 reverse engineering across 20+ patch versions. The function hash registry has **154K+ entries** â€” when a new patch drops, annotations transfer automatically instead of starting from scratch.\n\n## What's new in v2.0.0\n\n- Localhost-only binding for security (no more accidental network exposure)\n- Configurable decompile timeout\n- New label deletion tools (delete_label + batch_delete_labels)\n- .env-based configuration (no hardcoded paths)\n- Ghidra 12.0.2 support\n- Complete README rewrite with full API reference\n\n## Links\n\n- **GitHub:** https://github.com/bethington/ghidra-mcp\n- **v2.0.0 Release:** https://github.com/bethington/ghidra-mcp/releases/tag/v2.0.0\n\nPart of a broader RE toolkit â€” I also maintain [cheat-engine-server-python](https://github.com/bethington/cheat-engine-server-python) for dynamic analysis via MCP and [re-universe](https://github.com/bethington/re-universe) for BSim-powered binary similarity at scale.\n\nHappy to answer questions about the architecture, the hashing algorithm, or how MCP fits into RE workflows.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1qvgt6g/built_the_most_comprehensive_ghidra_mcp_server/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o3hkbi2",
          "author": "bargaindownhill",
          "text": "Jebus, where were you a week ago when i did a full disassembly and RE on an old 8051 design to teach my students what y2k was?",
          "score": 3,
          "created_utc": "2026-02-04 06:32:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kfyh6",
          "author": "bargaindownhill",
          "text": "ok commenting again.. \n\ncomplex setup, had to do some mods to make it work with roocode but OMFG!! this thing is MINT! best MCP of the year. You are the GOAT!\n\nim going to need about 3 classes to teach my students how to install and use this, but they are going to love this MCP",
          "score": 2,
          "created_utc": "2026-02-04 17:47:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gsyl4",
              "author": "JonchunAI",
              "text": "out of curiosity, how much better does this work compared to the ones with only a few tools? My agents really start to struggle when I add too many tools and my initial reaction to reading \"110 tools\" was that it may hurt more than help. (e.g. the one from LaurieWired)",
              "score": 1,
              "created_utc": "2026-02-09 17:33:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gttog",
                  "author": "bargaindownhill",
                  "text": "im running it with roo, and yes roo complains bitterly about having 160 tools, but ive not noticed any issues. \n\nwhat i have noticed is that i was able to fully disassembly and decompile an 8051 microprocessor firmware from the 1980s, into reasonable c psudo-code. enough that i was able to fix the y2k bug and add a couple of modern features to the code. cost about $60.00 in claude sonnet and haiku tokens. \n\nthis is the best MCP ive used in... well ever..",
                  "score": 1,
                  "created_utc": "2026-02-09 17:37:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4aj6o4",
          "author": "GovernmentSmall7873",
          "text": "As a security guy this may solve one of my biggest weak areas. I am terrible at reverse engineer and writing buffer overflows.",
          "score": 1,
          "created_utc": "2026-02-08 18:20:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0elcz",
      "title": "I built an MCP server that gives Claude 60+ FFmpeg tools â€” trim, merge, convert, stream, stabilize, and more",
      "subreddit": "mcp",
      "url": "https://i.redd.it/urjpo9paxiig1.png",
      "author": "dubnium0",
      "created_utc": "2026-02-09 19:54:46",
      "score": 26,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1r0elcz/i_built_an_mcp_server_that_gives_claude_60_ffmpeg/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4jpr0d",
          "author": "Basic_Young538",
          "text": "This rocks. Thx",
          "score": 2,
          "created_utc": "2026-02-10 02:38:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hmr93",
          "author": "dubnium0",
          "text": "If you want to try it, setup takes about 2 minutes...",
          "score": 1,
          "created_utc": "2026-02-09 19:55:31",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4ht36z",
              "author": "finance-mcp-001",
              "text": "Whatâ€™s the rough token usage?",
              "score": 1,
              "created_utc": "2026-02-09 20:27:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4k2619",
                  "author": "Proparser",
                  "text": "Interest in this too",
                  "score": 1,
                  "created_utc": "2026-02-10 03:54:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4k41j3",
              "author": "strigov",
              "text": "You mean installing requirements?",
              "score": 1,
              "created_utc": "2026-02-10 04:07:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n08bz",
                  "author": "dubnium0",
                  "text": "yeah",
                  "score": 1,
                  "created_utc": "2026-02-10 16:30:19",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r17v3k",
      "title": "We scanned over 8000+ MCP Servers... here's what we found",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r17v3k/we_scanned_over_8000_mcp_servers_heres_what_we/",
      "author": "Upstairs_Safe2922",
      "created_utc": "2026-02-10 17:55:54",
      "score": 26,
      "num_comments": 5,
      "upvote_ratio": 0.88,
      "text": "Over the past few months weâ€™ve been running theÂ [MCP Trust Registry](http://mcp-trust.com/), an open scanning project looking at security posture across publicly available MCP server builds.\n\nWeâ€™ve analyzed 8,000+ servers so far using 22 rules mapped to the OWASP MCP Top 10.\n\nSome findings:\n\n* \\~36.7% exposed unbounded URI handling â†’ SSRF risk (same class of issue we disclosed in Microsoftâ€™s Markitdown MCP server that allowed retrieval of instance metadata credentials)\n* \\~43% had command execution paths that could potentially be abused\n* \\~9.2% included critical-severity findings\n\nWe just added private repo scanning for teams running internal MCP servers. Same analysis, same evidence depth. Most enterprise MCP adoption is internal, so this was the #1 request.\n\nInterested to know what security review processes others have for MCP servers, if any. The gap we keep seeing isnâ€™t intent, itâ€™s that MCP is new enough that standard security gates havenâ€™t caught up.\n\nHappy to share methodology details or specific vuln patterns if useful.",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1r17v3k/we_scanned_over_8000_mcp_servers_heres_what_we/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o4o3l1f",
          "author": "kramit",
          "text": "Thanks to AI, I don't think its \"Move fast and break things\" anymore. I think we need a new phrase. \"Move so fast that you break everything to the point of massive dumpster fire\"",
          "score": 3,
          "created_utc": "2026-02-10 19:30:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4oeb62",
              "author": "Upstairs_Safe2922",
              "text": "Lol we've seen a few teams adopt that mindset. At least they get to call themselves \"first adopters\"",
              "score": 1,
              "created_utc": "2026-02-10 20:20:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pp6pp",
          "author": "_blkout",
          "text": "Three prompts in and you have to wait until next Tuesday 2046",
          "score": 1,
          "created_utc": "2026-02-11 00:17:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pr591",
          "author": "Left_Fieldhitem",
          "text": "Would love to hear the methodologies",
          "score": 1,
          "created_utc": "2026-02-11 00:28:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pxugc",
              "author": "Upstairs_Safe2922",
              "text": "We built an agentic system that dynamically monitors new security issues related to MCP server implementations, using sources such as the OWASP Top 10, MAESTRO, and other MCP security standards. From those best practices, we distilled an initial set of 22 evaluation rules which we then instruct our system to evaluate each and every MCP server repository against. Multiple agents evaluate the source code against a combination of SAST and DAST tools with the specific intent of gathering sufficient evidence as to whether the repo has one or more security findings based on each evaluation rule. For each vulnerable finding, our system proposes practical fixes that the MCP repo creator can take to improve the operational security of the implementation as well as mitigations any MCP server operator can take if they choose to deploy the server themselves.\n\nOne of the biggest issues facing the MCP server ecosystem, is that there may be many different implementations of the same server -- some repos are clearly more secure than others. The goal of this trust registry is to help both MCP server owners and operators safely build and deploy MCP servers.",
              "score": 1,
              "created_utc": "2026-02-11 01:07:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxi768",
      "title": "Open source: build MCP apps for ChatGPT, Gemini, and Claude using Flowbite",
      "subreddit": "mcp",
      "url": "https://v.redd.it/bujc4ek2qvhg1",
      "author": "elwingo1",
      "created_utc": "2026-02-06 13:52:54",
      "score": 19,
      "num_comments": 3,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "resource",
      "permalink": "https://reddit.com/r/mcp/comments/1qxi768/open_source_build_mcp_apps_for_chatgpt_gemini_and/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4js3xk",
          "author": "jdguggs10",
          "text": "Clever. Iâ€™m not sure I want to bother adding UI but if itâ€™s easy enough then maybe",
          "score": 2,
          "created_utc": "2026-02-10 02:52:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o421ey7",
          "author": "GlebosAppsInGPT",
          "text": "Nice, thanks for sharing.\n\nDoes the starter kit include (or recommend) an OAuth flow for MCP servers, e.g. user-scoped tokens + refresh, and where do you store secrets in a multi-user setup?",
          "score": 1,
          "created_utc": "2026-02-07 09:47:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o423rug",
              "author": "elwingo1",
              "text": "I would imagine using OAuth, Clerk, or WorkOS.\n\nNext step would be to integrate an authentication flow too and some dummy API requests. I think that Convex would be a better candidate than Supabase in this case.",
              "score": 1,
              "created_utc": "2026-02-07 10:11:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qvozrn",
      "title": "Why so many MCP servers avoid OAuth",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1qvozrn/why_so_many_mcp_servers_avoid_oauth/",
      "author": "Ok_Message7136",
      "created_utc": "2026-02-04 13:47:03",
      "score": 16,
      "num_comments": 14,
      "upvote_ratio": 0.81,
      "text": "Itâ€™s not that OAuth is bad- itâ€™s that MCP adds nuance:\n\n* tools â‰  servers\n* agents â‰  users\n* scopes change per call\n\nOnce you model that properly, OAuth actually fits pretty well. SDK-based setups help a lot since you donâ€™t have to hand-roll flows each time. Gopherâ€™s SDK was useful for experimenting without running a full auth gateway.\n\nWhat patterns are people using?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/mcp/comments/1qvozrn/why_so_many_mcp_servers_avoid_oauth/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o3j7nqy",
          "author": "alsophocus",
          "text": "I had to do a custom implementation for OAuth2, because we had to give users permission to work with MCP for their specific spaces within our different services. It rotates them tokens every twelve hours.",
          "score": 4,
          "created_utc": "2026-02-04 14:17:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3llski",
              "author": "Ok_Message7136",
              "text": "This matches what weâ€™ve seen too, once permissions are per space / per service, hand-rolling OAuth becomes unavoidable.",
              "score": 0,
              "created_utc": "2026-02-04 21:01:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3put3p",
          "author": "Informal_Tangerine51",
          "text": "OAuth breaks in MCP because token lifecycle doesn't match agent lifecycle. User authenticates once, agent runs for hours/days with that token. When it expires mid-execution, recovery is ugly.\n\nThe bigger issue: scopes are per-user but agents need per-action authorization. User has read/write, but should agent be allowed to delete based on this specific context? OAuth grants access, doesn't enforce policy at decision time.\n\nWe ended up with OAuth for authentication plus runtime policy gates for authorization. Token proves identity, policy layer decides if this specific action is allowed right now. Two-layer approach.\n\nSDK helps with flow but doesn't solve: token refresh during long-running agent execution, scope escalation when agent needs more access mid-task, or evidence of what was authorized when.\n\nMost people skip OAuth because they're prototyping and it's friction. Production agents need it, but OAuth alone isn't enough - you need runtime policy on top.",
          "score": 2,
          "created_utc": "2026-02-05 14:03:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jr4w1",
          "author": "cungalunga387",
          "text": "I just hate that the clients donâ€™t all support OAuth or API key so I have to support both! Other than that OAuth seems like the simplest solution especially for people with no technical background",
          "score": 2,
          "created_utc": "2026-02-04 15:53:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kjq70",
          "author": "dinkinflika0",
          "text": "You nailed the core issue - agents aren't users and tools aren't traditional API endpoints. When we built MCP support into Bifrost, the OAuth flow got weird fast. \n\nThe trickiest part was handling token refresh across multiple MCP servers. Agent makes 10 tool calls across different servers, each with different auth states. Do you refresh proactively? Wait for 401s? How do you handle one server's auth failing mid-workflow?\n\nWe ended up supporting vault integration (HashiCorp, AWS Secrets Manager) for key management, which helps but doesn't solve the agent-as-user identity problem.\n\nFor Bifrost specifically, we let you configure auth per MCP server connection and handle token lifecycle at the gateway level. Agents just call tools, gateway manages the OAuth dance.\n\nDocs: [https://docs.getbifrost.ai/mcp/overview](https://docs.getbifrost.ai/mcp/overview)\n\nStill feels like there should be a cleaner pattern though. What approach did you settle on with Gopher's SDK? Curious how others are handling the scopes-per-call problem.",
          "score": 2,
          "created_utc": "2026-02-04 18:04:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mut7u",
          "author": "Obvious-Car-2016",
          "text": "Use a gateway and it should be able to take in servers on any auth method and produce mcp endpoints with any auth method. \n\n(At least the one weâ€™ve built supports this fully)",
          "score": 1,
          "created_utc": "2026-02-05 00:54:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n6shd",
          "author": "Free-Internet1981",
          "text": "Because oauth was never meant to be used by agents, it was designed to secure services used by humans",
          "score": 1,
          "created_utc": "2026-02-05 02:03:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nqxuh",
          "author": "H0BB5",
          "text": "OAuth isn't suited for Agents, that's why",
          "score": 1,
          "created_utc": "2026-02-05 04:02:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pvoba",
          "author": "makinggrace",
          "text": "I'm on the struggle bus getting this configured myself. But there's no choice so.....",
          "score": 1,
          "created_utc": "2026-02-05 14:08:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3r1xov",
          "author": "anashel",
          "text": "I had to tweak my MCP, so the boilerplate now supports OAuth and API keys, with RPC, SSE, or streamable HTTP endpoints.\n\nThis means my MCP now works across Cursor, Claude, internal Cloudflare bindings, and voice platforms with real time MCP usage via SSE and streamable HTTP. Do it once and youâ€™re good.\n\nI also added Postgres RLS, so MCP scope now applies directly at the dataset level. That was quite useful.",
          "score": 1,
          "created_utc": "2026-02-05 17:32:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45zkpy",
          "author": "CueEcho-CEO",
          "text": "OAuth works well when youâ€™re dealing with user-facing web apps like Claude or ChatGPT, where sessions and user identity are central. But in MCP environments, especially CLI or agent-driven workflows like Claude Code, API keys are often simpler because agents and tools donâ€™t map cleanly to traditional user-session OAuth flows. I also find OAuth sessions can require frequent refreshes during development, which adds friction compared to API-key.",
          "score": 1,
          "created_utc": "2026-02-07 23:44:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3leir6",
          "author": "bingeboy",
          "text": "I did it to use Google Calendar with my MCP",
          "score": -1,
          "created_utc": "2026-02-04 20:26:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3llrl5",
              "author": "Ok_Message7136",
              "text": "Yeah, Calendar is a great real-world example where OAuth actually makes sense. Scoped, time-bound access per tool is way easier to reason about than long-lived keys.",
              "score": -2,
              "created_utc": "2026-02-04 21:01:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kn4e6",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -4,
          "created_utc": "2026-02-04 18:19:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ohufb",
              "author": "ValeoAnt",
              "text": "Go away bot",
              "score": 1,
              "created_utc": "2026-02-05 07:30:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxjn3h",
      "title": "Scheduling with an MCP server",
      "subreddit": "mcp",
      "url": "http://www.infobip.com/developers/blog/the-invisible-problem-how-we-solved-scheduling-with-ai",
      "author": "TypicalComma",
      "created_utc": "2026-02-06 14:50:47",
      "score": 14,
      "num_comments": 4,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1qxjn3h/scheduling_with_an_mcp_server/",
      "domain": "infobip.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3xg6dz",
          "author": "grewgrewgrewgrew",
          "text": "cron",
          "score": 3,
          "created_utc": "2026-02-06 16:47:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zedgz",
          "author": "penguinzb1",
          "text": "time zones are crazy. it's general practice to convert system time to the user's timezone upon first init, which for agents is awkward as it's usually too expensive to have a seperate subsystem for each user (unlike traditional systems)",
          "score": 3,
          "created_utc": "2026-02-06 22:30:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47w0uj",
          "author": "Sovairon",
          "text": "I don't get how your colleague is able to get client with this implementation, however this is a real problem which we have faced also and this is a good post. We currently have a timestamp tool that has offset capability, but this doesn't solve not knowing what time zone is client is at. This should be part of protocol honestly.\n\nThanks for sharing.",
          "score": 2,
          "created_utc": "2026-02-08 07:47:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3wu05l",
          "author": "Otherwise_Wave9374",
          "text": "This is such a real gotcha, agents are \"timeless\" unless you explicitly give them time and locale context. I like the two-tool approach (get now + schedule) since it keeps prompts small and avoids hidden assumptions. Also makes replay/testing easier. If anyone wants more MCP + agent workflow examples, Ive seen some good writeups here: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-02-06 15:01:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyjd87",
      "title": "Local Memory 1.4.0 Released",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1qyjd87/local_memory_140_released/",
      "author": "d2000e",
      "created_utc": "2026-02-07 17:13:24",
      "score": 13,
      "num_comments": 10,
      "upvote_ratio": 0.89,
      "text": "Just released v1.4.0 last night. It incorporates user feedback across a number of areas.\n\nv1.4.0 builds on the massive architecture changes from v1.3.0 that introduced knowledge levels, automatic contradiction detection, contradiction resolution, knowledge evolution, and epistemic questions. Most memory systems are flat RAG storage, treating all memories the same, even contradictory ideas. Local Memory has a fundamentally different architecture that addresses this flaw.\n\n# New Features\n\n# Multi-Provider AI Backend\n\n* Split Architecture: Separate `EmbeddingProvider` and `ChatProvider` interfaces allow independent configuration\n* Provider Mixing: Use different providers for embeddings vs chat (e.g., Ollama for embeddings, Anthropic for chat)\n* Fallback Chains: Optional fallback providers for resilience when primary providers fail\n* Circuit Breaker Pattern: All providers include circuit breakers to prevent cascade failures\n\n# Agent Attribution\n\nTrack which agent stored or updated memories and from which machine:\n\n* Agent Type Detection: Automatically detects whether memories come from Claude Desktop, Claude Code, REST API, or other sources\n* Hostname Tracking: Records the machine hostname for multi-device memory attribution\n* HTTP Headers: REST API clients can set `X-Agent-Type`, `X-Agent-Context`, `X-Access-Scope`, and `X-Agent-Hostname` headers\n* MCP Detection: Automatically detects agent type from session ID patterns and environment variables\n\n# Default Domain with MCP Prompts\n\nOrganize memories by project with intelligent domain detection:\n\n* Default Domain: New `session.default_domain` config option (defaults to \"general-knowledge\")\n* Domain Cascade: Explicit domain > agent config file > config default\n* Agent Config File Detection: Reads domain from CLAUDE.md, AGENTS.md, or GEMINI.md:\n   * HTML comment: `<!-- domain: project-name -->`\n   * Markdown header: `## Domain: project-name`\n   * YAML frontmatter: `domain: project-name`\n* MCP Prompts Protocol: New `prompts/list` and `prompts/get` methods\n   * `domain_selection` prompt instructs agents on domain handling\n   * Lists existing domains from database\n   * Provides usage examples\n\nYou can read the full write-up on v1.4.0 here: [https://www.localmemory.co/blog/local-memory-1.4-multi-provider-ai](https://www.localmemory.co/blog/local-memory-1.4-multi-provider-ai)\n\nYou can learn more about the knowledge hierarchy architecture released in v1.3.0 here: [https://www.localmemory.co/blog/local-memory-1.3-series-the-journey-to-world-memory](https://www.localmemory.co/blog/local-memory-1.3-series-the-journey-to-world-memory)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1qyjd87/local_memory_140_released/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o44071k",
          "author": "BC_MARO",
          "text": "The split embedding/chat providers + circuit breakers is the right direction.\nIf you wrap it with an MCP control plane (policy approvals, tool-call audit trail, per-domain quotas) it gets way easier to run in prod - weâ€™re building a layer like that at https://peta.io.",
          "score": 2,
          "created_utc": "2026-02-07 17:22:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45ikpw",
          "author": "sirebral",
          "text": "Looks interesting, I see it's a paid product, is it available as a trial as well?  I assume it's licensed as a proprietary product?\n\nSide note, the GitHub link on your CMS your is broken.",
          "score": 1,
          "created_utc": "2026-02-07 22:04:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o464bb4",
              "author": "d2000e",
              "text": "Appreciate the feedback. It is linked to the private repo. I need to update it to the public releases repo.\n\nIt currently doesnâ€™t have a trial but that is something to consider. Do you currently use an AI memory or knowledge solution?",
              "score": 1,
              "created_utc": "2026-02-08 00:13:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o46a4to",
                  "author": "sirebral",
                  "text": "I use various solutions, based upon need.  Hence why I asked about a trial.   Being that there is no single persistence layer that will work well for all use cases, it would be helpful to at least have something to play with.  When there are hundreds of open source projects that you're competing against, it's hard to quantify the value.",
                  "score": 1,
                  "created_utc": "2026-02-08 00:47:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4bvwm7",
                  "author": "sirebral",
                  "text": "This is challenging, if no repo for the code, I need to sandbox it and audit its behavior.  It's a big red-flag for me.  If I can't audit the code without installing it, I'm dubious.  Being it's a local tool, persistence with a remote data source  (for validation of the license key, and who knows whatever else) makes it untenable for sensitive data.",
                  "score": 1,
                  "created_utc": "2026-02-08 22:19:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qvpn4v",
      "title": "Introducing TinyFn -- 500+ simple tools for your agents",
      "subreddit": "mcp",
      "url": "https://tinyfn.io/",
      "author": "yesiliketacos",
      "created_utc": "2026-02-04 14:14:11",
      "score": 13,
      "num_comments": 10,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1qvpn4v/introducing_tinyfn_500_simple_tools_for_your/",
      "domain": "tinyfn.io",
      "is_self": false,
      "comments": [
        {
          "id": "o3n3mis",
          "author": "unoriginal_original_",
          "text": "Wouldn't this be better as a skill instead?",
          "score": 4,
          "created_utc": "2026-02-05 01:45:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3phyqu",
              "author": "yesiliketacos",
              "text": "How so? This allows an agent to execute these functions by making a tool call. With a skill the agent would need to run code somewhere",
              "score": 2,
              "created_utc": "2026-02-05 12:47:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3obbfl",
          "author": "codeyman2",
          "text": "Donâ€™t you see hallucinations with 500+ tools?",
          "score": 3,
          "created_utc": "2026-02-05 06:32:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pit0y",
              "author": "yesiliketacos",
              "text": "Yep this is a real issue--It's a lot of context to add the entire 500+ tool mcp.    \n  \nThere is work being done on this at the harness level. For example, claude has a [\"tool search tool\"](https://platform.claude.com/docs/en/agents-and-tools/tool-use/tool-search-tool) that makes a toolset discoverable, so all MCP servers aren't loaded right off the bat.  But it is somewhat of an unsolved problem.    \n  \nFor that reason, you can also attach individual [\"categories\"](https://docs.tinyfn.io/mcp/categories) of tools with TinyFn.  I am thinking about other ways to solve this as well (allowing users to select from everything and exactly what tools they want to expose for a specific MCP server? would be cool--I haven't quite figured it out yet)",
              "score": 1,
              "created_utc": "2026-02-05 12:53:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3j91z8",
          "author": "Ok_Message7136",
          "text": "Nice approach. Deterministic MCP tools like this are a clean way to reduce hallucinations-use the LLM for reasoning, offload exact ops to tools. Makes agents way more reliable.",
          "score": 3,
          "created_utc": "2026-02-04 14:24:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jb2xq",
              "author": "yesiliketacos",
              "text": "Thanks!  I built this to solve my own problem when I couldn't find a similar solution.  It has made my agents so much better at what should be simple tasks.  Working on a benchmark now to demonstrate this",
              "score": 1,
              "created_utc": "2026-02-04 14:34:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3npt82",
          "author": "sublimegeek",
          "text": "Thatâ€™s cool. You need a MCP for this? Could it be a skill?",
          "score": 3,
          "created_utc": "2026-02-05 03:55:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pj48r",
              "author": "yesiliketacos",
              "text": "How would this work as a skill? genuine question.  An agent with a place to run code could certainly run most of these... (some require dependencies that the agent would then also need to install), but then the agent needs to run code.  This is setup in a way an agent can execute all of these utility functions with a simple tool call",
              "score": 2,
              "created_utc": "2026-02-05 12:55:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4050vk",
                  "author": "makinggrace",
                  "text": "This would be an amazing set of skills. I would break them up by category. Look more at skills--they can carry resources like scripts. \n\nMost chat agents can run skills in some way now.",
                  "score": 0,
                  "created_utc": "2026-02-07 01:01:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qy142v",
      "title": "[Showcase] MCP-powered Autonomous AI Research Engineer (Claude Desktop, RAG, Code Execution)",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1qy142v/showcase_mcppowered_autonomous_ai_research/",
      "author": "Kooky-Second2410",
      "created_utc": "2026-02-07 02:04:01",
      "score": 12,
      "num_comments": 2,
      "upvote_ratio": 0.8,
      "text": "Hey r/mcp,\n\nIâ€™ve been working on an MCP-powered â€œAI Research Engineerâ€ and wanted to share it here for feedback and ideas.\n\nGitHub: [https://github.com/prabureddy/ai-research-agent-mcp](https://github.com/prabureddy/ai-research-agent-mcp)  \nIf it looks useful, a â­ on the repo really helps more MCP builders find it.\n\n**What it does**\n\nYou give it a single high-level task like:\n\nâ€œCompare electric scooters vs bikes for my commute and prototype a savings calculatorâ€\n\nThe agent then autonomously:\n\n* researches the web for relevant data\n* queries your personal knowledge base (notes/papers/docs) via RAG\n* writes and executes Python code (models, simulations, visualizations) in a sandbox\n* generates a structured research run: report, charts, code, data, sources\n* self-evaluates the run with quality metrics (clarity, grounding, completeness, etc.)\n\nItâ€™s built specifically around MCP so you can run everything from Claude Desktop (or another MCP client) with minimal setup.\n\n**Tech / architecture**\n\nMCP server in Python 3.10+\n\nTools:\n\n* `web_research`: DuckDuckGo/Brave + scraping + content extraction\n* `rag_tool`: local embeddings + ChromaDB over a `knowledge_base` directory\n* `code_sandbox`: restricted Python execution with time/memory limits\n* `workspace`: organizes each research run into its own folder (report, charts, code, data, evaluation)\n* `evaluator`: simple self-critique + quality metrics per run\n\nRAG uses local sentence-transformers by default, so you can get started without external embedding APIs.\n\n5â€“10 min setup: clone â†’ install â†’ add MCP config to Claude Desktop â†’ restart.\n\n**Example flows**\n\n* â€œDeep dive: current state of EVs in 2026. Include market size, major players, growth trends, and a chart of adoption over time.â€\n* â€œUse my notes in `knowledge_base` plus web search to analyze whether solar panels are worth it for a home in California. Build a payback-period model and visualize cashflows.â€\n* â€œUse `web_research` \\+ RAG + code execution to build a small cost-of-ownership calculator for my commute.â€\n\n**Why Iâ€™m posting here**\n\nIâ€™d really appreciate feedback from this community on:\n\n**MCP design:**\n\n* Does the tool surface / boundaries make sense for MCP?\n* Anything youâ€™d change about how `web_research` / `rag_tool` / `code_sandbox` are exposed?\n\n**Safety & sandboxing:**\n\n* Are there better patterns youâ€™ve used for constrained code execution behind MCP?\n* Any obvious gotchas Iâ€™m missing around resource limits or isolation?\n\n**RAG + research UX:**\n\n* Suggestions for better chunking/query strategies in this â€œresearch agentâ€ context?\n* Patterns youâ€™ve used to keep the agent grounded in sources while still being autonomous?\n\n**Extensibility:**\n\n* Other tools youâ€™d add to a â€œresearch engineerâ€ server (data connectors, notebooks, schedulers, etc.)?\n* Thoughts on integrating with other MCP clients beyond Claude Desktop / Cursor?\n\nIf you have time to glance at the repo and tear it apart, Iâ€™d love to hear what you think. Happy to answer implementation questions or discuss MCP patterns in more detail.\n\nIf you end up trying it and think itâ€™s useful, please consider dropping a â­ on the GitHub repo and sharing any ideas/issues there as well.\n\nThanks!\n\n[AI Research Engineer](https://i.redd.it/sczg2svgn3ig1.gif)\n\nhttps://preview.redd.it/kwh5dbntczhg1.png?width=1074&format=png&auto=webp&s=2c7729e95890dce291ad8e635feca5a2805583b2\n\nhttps://preview.redd.it/4e0nlantczhg1.png?width=1076&format=png&auto=webp&s=f1e3f3eabe67ff887c8ca994f0090c74989621f6\n\nhttps://preview.redd.it/zx4v3puuczhg1.png?width=4168&format=png&auto=webp&s=f798447d3b5bf5510400b832af96161488c4e25c\n\nhttps://preview.redd.it/bmec8quuczhg1.png?width=3702&format=png&auto=webp&s=6a8fe3d1c47a464c6f733cfa4c2463d25ccd5d5b\n\nhttps://preview.redd.it/3zv5hnuuczhg1.png?width=3568&format=png&auto=webp&s=162f410cc6edd2b46bd1c0a8f36a7e4a0afb9e12",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1qy142v/showcase_mcppowered_autonomous_ai_research/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o418aga",
          "author": "BC_MARO",
          "text": "Cool build. For the code_sandbox, Iâ€™d be paranoid about isolation - separate process/container, no network by default, and explicit allowlists for file access + libs, otherwise â€œresearch agentâ€ turns into â€œrun arbitrary pythonâ€. Also +1 on saving every tool call / artifact as an audit trail so you can replay what happened.",
          "score": 3,
          "created_utc": "2026-02-07 05:20:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o481i2l",
              "author": "Kooky-Second2410",
              "text": "Thank you! Totally agree on the isolation risk â€” Iâ€™m treating the sandbox as untrusted: separate process, no network, tight resource limits, and moving toward strict allowlists for file access / libs. Also working on persisting every tool call + artifact so runs are fully replayable for debugging and safety. Really appreciate you calling that out.",
              "score": 2,
              "created_utc": "2026-02-08 08:39:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r0xsmr",
      "title": "File-Organizer-MCP v3.2.8 just dropped â€” Now with FINAL MCP fixes, TUI wizard, and battle-tested stability. Organize 5000+ files with one AI prompt!",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r0xsmr/fileorganizermcp_v328_just_dropped_now_with_final/",
      "author": "Technocratix902",
      "created_utc": "2026-02-10 11:01:38",
      "score": 11,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hey MCP fam (and Claude/Cursor users),\nTired of your Downloads folder looking like a digital warzone? I built File-Organizer-MCP â€” a hardcore local MCP server that lets your AI actually manage your files intelligently:\nAuto-categorizes into 12+ smart folders (Docs, Images, Code, Archives, etc.)\nDetects duplicates with SHA-256 (saves you GBs of space)\nBatch rename, conflict handling (rename/skip/overwrite), dry-run previews\nRollback any changes if something goes sideways\nScheduled auto-organization (cron-style)\n8 layers of security so nothing escapes your allowed dirs\nInteractive setup: just run npx file-organizer-mcp --setup â€” picks your clients automatically\nThe AI decides what to do â€” we handle the how securely and efficiently (low token burn, atomic ops).\nLatest v3.2.8 fixes the last MCP protocol gremlins â€” tested with Claude Desktop, Cursor, Gemini CLI, etc.\nInstall in seconds:\nnpx file-organizer-mcp --setup\nGitHub: https://github.com/kridaydave/File-Organizer-MCP\nNPM: https://www.npmjs.com/package/file-organizer-mcp\nMCP Registry: https://registry.modelcontextprotocol.io/servers/io.github.kridaydave/file-organizer\nBuilt during school break because I was tired of manual sorting lol. Feedback, stars, issues, PRs all welcome â€” let's make file chaos history!\nHappy organizing ðŸ—‚ï¸âœ¨",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1r0xsmr/fileorganizermcp_v328_just_dropped_now_with_final/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o4myr0o",
          "author": "astrokat79",
          "text": "Have you tested/tried this with a music folder?",
          "score": 4,
          "created_utc": "2026-02-10 16:23:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n7gez",
              "author": "Technocratix902",
              "text": "File organization by Content/Metadata is in the works and should be implemented by Sunday. It was delayed today due to an npm oversight I occured. By the time update rolls out. You will have sorting of songs according to your Favorite Artists. Currently Kendrick and Drake have to share the same folder lol.",
              "score": 1,
              "created_utc": "2026-02-10 17:03:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxekbu",
      "title": "We Made MCP Connection Stupidly Easy",
      "subreddit": "mcp",
      "url": "https://v.redd.it/e9kl9z8ztuhg1",
      "author": "zakjaquejeobaum",
      "created_utc": "2026-02-06 10:56:44",
      "score": 10,
      "num_comments": 2,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1qxekbu/we_made_mcp_connection_stupidly_easy/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o42tyqc",
          "author": "init0",
          "text": "For free web based version https://mcphost.link does almost all of this.",
          "score": 1,
          "created_utc": "2026-02-07 13:43:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43zxv8",
              "author": "zakjaquejeobaum",
              "text": "Free ai tokens as well or what? The tool is free except the llm costs.",
              "score": 1,
              "created_utc": "2026-02-07 17:21:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qz4asy",
      "title": "ðŸ¦ž When Your AI Talks to Another AI â€” I Built an MCP Bridge for OpenClaw & Claude",
      "subreddit": "mcp",
      "url": "https://github.com/freema/openclaw-mcp",
      "author": "Open_Variation1438",
      "created_utc": "2026-02-08 09:09:21",
      "score": 10,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "server",
      "permalink": "https://reddit.com/r/mcp/comments/1qz4asy/when_your_ai_talks_to_another_ai_i_built_an_mcp/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o486del",
          "author": "BC_MARO",
          "text": "this is a fun idea.\n\n2 things i'd love to see in the repo docs:\n- a clear threat model (what claude can and can't trigger)\n- what gets logged, and where\n\nif you ever expose this beyond localhost, i'd strongly recommend putting policy in front of the openclaw side: tool allowlists, require human approval for write actions, and keep a real audit log. itâ€™ll save people from doing something they regret.",
          "score": 3,
          "created_utc": "2026-02-08 09:25:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o487x5e",
              "author": "Open_Variation1438",
              "text": "Hey, thanks a lot! Yeah, I'll definitely add that. I'm already running it somewhat in production, so I know what needs to be done â€” but others mightt not. It does take quite a bit of efort to set up properly. I'll add som warnings at startup when it detects it's running outside localhost, tha should help a lot... ",
              "score": 2,
              "created_utc": "2026-02-08 09:40:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o48ifgb",
          "author": "Prestigious-Yam2428",
          "text": "Sounds like an interesting experiment ðŸ˜… Good luck! ðŸš€",
          "score": 2,
          "created_utc": "2026-02-08 11:18:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4det99",
          "author": "mt-beefcake",
          "text": "I havnt looked into the details of your setup yet. But I got 2 open claw agents and 2 claude desktops talking with eachother and collaborating on things. Set up a universal memory that reads all chat logs live and adds them to a query able database. Its also great for crash recovery, but reverting back to the last message seems to happen less now. Imma have one of the boys take a look at your setup and compare it to mine. Ill let you know",
          "score": 2,
          "created_utc": "2026-02-09 03:23:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4e9fzj",
              "author": "Open_Variation1438",
              "text": "Hey, thanks for the comment. I think a multi-agent setup without orchestration is fragile. That's why I often have repetitive flows in n8n, and n8n gives OpenClaw instructions via MCP. As I wrote somewhere above, Claw mainly gives me the ability to test and prototype a flow. When I like it and it's functional, I usually convert it into some kind of automation. In my case, Claw basically just handles working with GitHub and passes instructions to Claude Code on what to do (bug fixes). This can then be taken over by a simple Golang script. Otherwise, it's definitely important for me to isolate Claw in Docker. If it crashes, I just throw away the container and start fresh. Memory is usually in an external volume. Theoretically, I've been thinking about adding some kind of memory module or something like that. I use that approach on my other online MCP tools. Anyway, thanks for the feedback! :)\n\nOh and about the Docker setup â€” I usually have CC, GH, or whatever else pre-installed. I don't want it installing anything on its own. That's always a road to hell...  \n",
              "score": 1,
              "created_utc": "2026-02-09 07:08:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4b3500",
          "author": "dbizzler",
          "text": "Can someone explain to me what the hubbub is about with OpenClaw? I got excited about it after watching dozens of bros on TikTok going out and buying Mac Minis to run it but when I dug in it's... an insecure version of Claude Cowork with a non-standard tools interface? I know that by having a node running on your machine it gets access to your entire filesystem, camera, location, etc. but I'm seeing influencers talk about having it code them apps and I just don't understand why you'd have OpenClaw make you an app over Claude Code?\n\n\n\nGiven its explosive growth I assume I'm the one missing something here. Is it the heartbeat? The messaging app integration? The full YOLO access to everything?",
          "score": 1,
          "created_utc": "2026-02-08 19:54:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4bkfee",
              "author": "Open_Variation1438",
              "text": "Yeah, I agree the hype is nonsense. What I actually enjoy about it though is that you can run it in an isolated Docker environment where it can't go anywhere â€” it only has access to what you give it. For example, you give it Claude Code and GitHub, and then you send it tasks like fixing bugs. Then you realize it actually works, and you write something in Golang that can do the same thing :D So it's more like a toy you can use to prototype workflows that you do over and over again.",
              "score": 1,
              "created_utc": "2026-02-08 21:21:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qz9rrb",
      "title": "We open-sourced SBP â€” a protocol that lets AI agents coordinate through pheromone-like signals instead of direct messaging",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1qz9rrb/we_opensourced_sbp_a_protocol_that_lets_ai_agents/",
      "author": "Brief-Feed665",
      "created_utc": "2026-02-08 14:05:46",
      "score": 10,
      "num_comments": 4,
      "upvote_ratio": 0.86,
      "text": "We just released SBP (Stigmergic Blackboard Protocol), an open-source protocol for multi-agent AI coordination.\n\n**The problem:**Â Most multi-agent systems use orchestrators or message queues. These create bottlenecks, single points of failure, and brittle coupling between agents.\n\n**The approach:**Â SBP uses stigmergy â€” the same mechanism ants use. Agents leave signals on a shared blackboard. Those signals have intensity, decay curves, and types. Other agents sense the signals and react. No direct communication needed.\n\n**What makes it different from MCP?**Â MCP (Model Context Protocol) gives agents tools and context. SBP gives agentsÂ *awareness of each other*. They're complementary â€” use MCP for \"what can I do?\" and SBP for \"what's happening around me?\"\n\n**What's included:**\n\n* Full protocol specification (RFC 2119 compliant)\n* TypeScript reference server (`@advicenxt/sbp-server`)\n* TypeScript + Python client SDKs\n* OpenAPI 3.1 specification\n* Pluggable storage (in-memory, extensible to Redis/SQLite)\n* Docker support\n\n**Links:**\n\n* GitHub:Â [https://github.com/AdviceNXT/sbp](https://github.com/AdviceNXT/sbp)\n* npm:Â `npm install` u/advicenxt`/sbp-server`\n* PyPI:Â `pip install sbp-client`\n\nHappy to answer questions about the protocol design, decay mechanics, or how we're using it.",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1qz9rrb/we_opensourced_sbp_a_protocol_that_lets_ai_agents/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o49e41q",
          "author": "nofilmincamera",
          "text": "Concept is really cool but I am missing the so what? How does this structure operate different on output?",
          "score": 2,
          "created_utc": "2026-02-08 14:58:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4a7otz",
          "author": "pbalIII",
          "text": "Stigmergy works for ants because ant tasks are stateless... forage, return, reinforce. LLM agents are making contextual decisions where the meaning of a signal depends on what the agent already knows. That analogy breaks fast.\n\nVirtual stigmergy research shows environmental traces without agent memory fail completely. Individual memory alone gets 68.7% improvement over baselines. Decay curves and signal intensity don't help if agents can't reason about why a signal was left.\n\nThe real bottleneck in multi-agent systems isn't communication topology. It's shared state corruption. One agent writes a slightly wrong signal, others build on it confidently. An orchestrator gives you a single place to audit that. A blackboard with decaying anonymous signals makes provenance harder, not easier.",
          "score": 1,
          "created_utc": "2026-02-08 17:25:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ggxqf",
              "author": "Brief-Feed665",
              "text": "Fair critiques, but you're arguing against vanilla ant stigmergy, we don't implement that here.\n\nSBP pheromones aren't anonymous traces. Every signal carries typed JSON payloads, source agent IDs, UUIDs, and timestamps. Downstream agents knowÂ *what*Â happened,Â *who*Â said it, andÂ *how fresh*Â it is. The agent's internal reasoning (RAG, CoT, whatever) is its own business, SBP only handles coordination.\n\nOn \"traces without memory fail\" - agreed, if signals are unstructured. Ours aren't. We also have capped audit streams with pheromone UUIDs for full temporal reconstruction. Decay isn't a substitute for reasoning, it's a safety invariant: unreinforced signals evaporate, the system hibernates instead of acting on stale data.\n\nOn shared state corruption, thank you, your strongest point. But the tradeoff: a bad orchestrator decision persists until a human catches it. A bad SBP signalÂ *decays away*. Plus agents gate on composite thresholds (e.g., drift > 0.6 AND VIX â‰¤ 25), so a single bad signal can't cascade. Orchestrators are easier to debug after the fact. Stigmergic systems fail more safely in real-time. Thanks again.",
              "score": 1,
              "created_utc": "2026-02-09 16:36:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o49o3vp",
          "author": "BC_MARO",
          "text": "Love this. The \"so what\" for me is you can drop the message-passing choreography and let agents reaLove this. The \"so what\" for me is you can drop the message-passing choreography and let agents react",
          "score": 1,
          "created_utc": "2026-02-08 15:50:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r19i1q",
      "title": "Built MCP support into Bifrost (LLM gateway) - your Claude tools work with any LLM now",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r19i1q/built_mcp_support_into_bifrost_llm_gateway_your/",
      "author": "dinkinflika0",
      "created_utc": "2026-02-10 18:53:26",
      "score": 10,
      "num_comments": 2,
      "upvote_ratio": 0.78,
      "text": "We added MCP integration to Bifrost ([OSS gateway](https://git.new/bifrostrepo)) so you can use the same MCP servers across different LLMs, not just Claude.\n\nHow it works: connect your MCP servers to Bifrost (filesystem, web search, databases, whatever). When requests come through the gateway, we automatically inject those tools into the request regardless of which LLM you're using. So your filesystem MCP server that works with Claude? Now works with GPT-4, Gemini, etc.\n\nThe setup is straightforward - configure MCP servers once in Bifrost, then any model you route through can use them. We support STDIO, HTTP, and SSE connections.\n\nWhat made this useful: you can test which model handles your specific MCP tools better. Same filesystem operations, same tools, different models. Turns out some models are way better at tool orchestration than others.\n\nAlso built \"Code Mode\" where the LLM writes TypeScript to orchestrate multiple tools in one request instead of back-and-forth. Cuts down latency significantly for complex workflows.\n\nAll the MCP tools show up in our observability UI so you can see exactly which tools got called, what parameters, what they returned.\n\nSetup guide: [https://docs.getbifrost.ai/mcp/overview](https://docs.getbifrost.ai/mcp/overview)\n\nAnyone running MCP servers in production? What tools are you using?",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1r19i1q/built_mcp_support_into_bifrost_llm_gateway_your/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o4o30qe",
          "author": "GAMEYE_OP",
          "text": "Can anyone explain to me what SSE is used for? Like an example usage",
          "score": 1,
          "created_utc": "2026-02-10 19:28:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0cgkc",
      "title": "What trust assumptions do MCP servers make?",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1r0cgkc/what_trust_assumptions_do_mcp_servers_make/",
      "author": "Far_Accountant_961",
      "created_utc": "2026-02-09 18:38:53",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "What surprised us most while building with MCP wasnâ€™t how powerful the ecosystem is, but how much implicit trust it assumes. Once we started connecting third-party MCP servers to real agents, a few questions kept coming up:\n\nâ€¢ How do you know an MCP server is safe before connecting it?\n\nâ€¢ Where does agent data actually go across servers?\n\nâ€¢ Can one MCP influence model behavior or exfiltrate data meant for another?\n\nâ€¢ Can an LLM be nudged into calling tools it shouldnâ€™t?\n\nWe went looking for â€œtrust layerâ€ for MCP servers and couldnâ€™t find a comprehensive catalog or standardized risk assessment. So we ended up building one internally and decided to make it public. We analyzed MCP servers across a few dimensions:\n\nâ€¢ provenance (official vs community)\n\nâ€¢ MCP spec conformance\n\nâ€¢ OWASP-style risks applied to agentic systems\n\nâ€¢ static analysis for AI-specific patterns\n\nâ€¢ CVEs, dependency issues, and runtime behaviors like prompt injection or cross-server data access\n\nSome of the findings were concerning: exposed API keys, servers pulling in known malicious packages, and tools that appear to attempt context poisoning or cross-server data access.\n\nFull disclosure: [https://mcp.armor1.ai/mcp-directory](https://mcp.armor1.ai/mcp-directory)\n\nThe catalog is free, public, and doesnâ€™t require a login. Itâ€™s still evolving, and Iâ€™d genuinely love input from people here:\n\nâ€¢ How are you evaluating MCP servers today?\n\nâ€¢ Are there risk categories you think weâ€™re missing?\n\nâ€¢ Would a shared trust / risk signal for MCP servers actually be useful?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1r0cgkc/what_trust_assumptions_do_mcp_servers_make/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o4hcg3q",
          "author": "owlpellet",
          "text": "I think this directory is good. Basically journalism about services. I find the liability implications of this kind of spooky, but I'm sure you've thought about that. \n \nThe next layer down is actual controls within orgs that enforce allowed lists for services and/or isolate them within networks. An example: [https://blogs.vmware.com/tanzu/building-an-enterprise-mcp-server-marketplace-with-tanzu-platform/](https://blogs.vmware.com/tanzu/building-an-enterprise-mcp-server-marketplace-with-tanzu-platform/) \n\nNone of this is specific to MCP, once you wrap your head around \"I don't really know you, man\" and zero-trust them accordingly.",
          "score": 1,
          "created_utc": "2026-02-09 19:05:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4i1lia",
          "author": "GentoroAI",
          "text": "Two things Iâ€™d add to your scoring: network egress (where can it phone home) and data retention/logging (what it stores + for how long). A shared risk signal would be super useful if itâ€™s versioned and opinionated.",
          "score": 1,
          "created_utc": "2026-02-09 21:09:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4icwtv",
              "author": "Life_Salt8186",
              "text": "As part of the Armor1 team thanks, those additional signals make sense to add. Can you elaborate a bit more on your context of shared risk signal here? We also have point in time snapshots so metrics/scores are versioned behind the scenes. ",
              "score": 1,
              "created_utc": "2026-02-09 22:06:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4pu3mq",
                  "author": "GentoroAI",
                  "text": "By shared risk signal I mean a machine-readable feed (JSON/API) that registries/gateways can enforce per server version: egress + retention/logging + creds + SBOM/CVEs, plus an allow/warn/block verdict backed by evidence. Snapshots are the right foundation.",
                  "score": 1,
                  "created_utc": "2026-02-11 00:45:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4iewtf",
          "author": "BC_MARO",
          "text": "The cross-server data exfiltration risk is the one that keeps me up at night. Most people just add MCP servers without thinking about what data flows between them, and there's no sandboxing by default.\n\nI've been looking at peta.io for this - they have a managed MCP runtime with policy-based approvals and a tool-call audit trail, which at least gives you visibility into what's happening. But yeah, the spec itself needs a trust/permissions model baked in.",
          "score": 1,
          "created_utc": "2026-02-09 22:16:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4k7jso",
              "author": "Hot_Barracuda2596",
              "text": "This is a real concern and weâ€™re seeing it too. What we do today is detect risk before connection, not sandbox at runtime. We analyze tool metadata and schemas because thatâ€™s what the LLM actually sees, and we flag patterns that could enable cross-server data flow, like instructions that try to force the model to always call a tool. For example, weâ€™ve seen a web search tool whose description explicitly nudged the LLM to prefer that tool over native search tools provided by IDEs like Cursor. It doesnâ€™t solve sandboxing, but it gives teams early visibility into which MCP servers can influence model behavior before theyâ€™re blindly added. We are actively looking into better modelling and analysing cross tool/server interactions as well as runtime analysis strategies.",
              "score": 2,
              "created_utc": "2026-02-10 04:31:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qzcc1d",
      "title": "Built a local memory system for Claude Code, benchmarked against 5 alternatives",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1qzcc1d/built_a_local_memory_system_for_claude_code/",
      "author": "_rendro",
      "created_utc": "2026-02-08 15:49:19",
      "score": 8,
      "num_comments": 18,
      "upvote_ratio": 0.79,
      "text": "I built an MCP server that gives AI agents persistent memory across sessions. 4 tools: store, recall, list, forget. That's the entire API.\n\nThe tool count is deliberate. I started with 8 tools, tags, metadata fields, expiration dates. Every parameter I added was another decision the LLM had to make on every call, and it got worse at using the system. Stripping it down to store content and recall query made the LLM actually use it well without explicit prompting.\n\nUnder the hood it does more than the simple API suggests. Vector search (all-MiniLM-L6-v2 via Candle), hybrid BM25 scoring, memory decay with a 30-day half-life, a property graph for linking related memories, and auto-consolidation that merges near-duplicates. All of that is invisible to the LLM. It just calls store and recall.\n\nI benchmarked it against 5 alternatives (1,000 memories, 200 queries):\n\n* R@1: 50% (vs 47% next best)\n* Recency@1: 100% (vs 14% for competitors)\n* Auto-dedup: 99% consolidation rate\n\nSingle Rust binary, zero external dependencies. No Docker, no cloud, no API keys. Works with any MCP client.\n\n  \n`brew install rendro/tap/sediment`\n\nThen add to your client, e.g. for Claude Code:\n\n`claude mcp add sediment -- sediment`\n\n\n\nLanding page: [https://sediment.sh](https://sediment.sh)\n\nGitHub: [https://github.com/rendro/sediment](https://github.com/rendro/sediment)\n\nBenchmark suite: [https://github.com/rendro/sediment-benchmark](https://github.com/rendro/sediment-benchmark)\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1qzcc1d/built_a_local_memory_system_for_claude_code/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o49pzty",
          "author": "Otherwise_Wave9374",
          "text": "Love the \"fewer tools, better usage\" point. People underestimate how much decision fatigue you introduce for the LLM when the tool surface area gets big.\n\nAlso really like the idea of keeping the fancy stuff (hybrid search, decay, dedup, graph links) invisible behind a dead simple store/recall contract. That is basically how I have seen the most reliable agent memory layers built.\n\nAny chance you have a short writeup on how you tuned the decay + consolidation thresholds? This topic comes up a lot in agent memory discussions, a good overview here too: https://www.agentixlabs.com/blog/",
          "score": 3,
          "created_utc": "2026-02-08 15:59:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4e5kmq",
              "author": "Dense_Gate_5193",
              "text": "thatâ€™s why the MCP server toolset for Nornic is idiomatic for memory operations. \n\nhttps://github.com/orneryd/NornicDB",
              "score": 2,
              "created_utc": "2026-02-09 06:34:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4foh16",
                  "author": "_rendro",
                  "text": "Interesting project, will check it out",
                  "score": 1,
                  "created_utc": "2026-02-09 14:12:27",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4a8o4i",
              "author": "_rendro",
              "text": "Thanks! Context rot is really what I am trying to avoid. I started with way more tools and parameters and removed anything that wasnâ€™t necessary and improved tool call reliability in the process.\n\nThe parameters for decay and consolidation are opinionated and might need more tweaking based on benchmarks. Thanks for the link, will definitely give it a read and see if I need to make changes.\n\nFor the hybrid vector + BM25 FTS blending parameters I had a grid search running with the benchmark suite to tune it for best recall results.",
              "score": 1,
              "created_utc": "2026-02-08 17:30:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o49y6d7",
          "author": "DorkyMcDorky",
          "text": "Do you think a lot of this can be offloaded if it were a persistent connection between client/server?  Do you see anything that might perform better if MCP had such a design to it?\n\nGreat job!!!",
          "score": 2,
          "created_utc": "2026-02-08 16:39:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4abrez",
              "author": "_rendro",
              "text": "Thanks! MCP's stdio transport is already persistent, the server stays running for the whole session, which is how Sediment keeps the embedding model loaded and runs background consolidation between calls. What I'd love to see though is richer serverâ†’client communication, like push notifications when background tasks complete or streaming partial results. The main bottleneck isn't really connection overhead, it's the vector/embedding ops themselves. But bidirectional streaming could make the intelligence layer stuff (consolidation, clustering) feel more real-time instead of fire-and-forget.",
              "score": 1,
              "created_utc": "2026-02-08 17:45:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4bxslm",
                  "author": "DorkyMcDorky",
                  "text": "It's not true streaming though, the results yield via cursors, making it about the same as REST :)  They will never have bidi - they don't have streaming now.  Look at the code - it is a streaming protocol but it is not streaming in action.\n\n  \nI'm trying to get real bidi going in it, they are doing the opposite.",
                  "score": 1,
                  "created_utc": "2026-02-08 22:29:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4dgcg0",
          "author": "BC_MARO",
          "text": "The deliberate tool count reduction is the right call. I've seen the same pattern where more tools means worse LLM decision-making per call. The 30-day decay half-life is interesting -- have you experimented with different curves? For coding context specifically I'd expect some memories (architectural decisions) to stay relevant much longer than others (debug sessions). The auto-consolidation at 99% dedup rate is impressive though, that's usually where naive approaches fall apart.",
          "score": 2,
          "created_utc": "2026-02-09 03:32:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4f3o92",
              "author": "_rendro",
              "text": "Thank you!\n\nIâ€™m currently experimenting with a usage based decay. If I donâ€™t store memories for 30 days I probably donâ€™t want all of it to decay, but if I hammer on the storage layer, 30 days seems too long. This is definitely an area that needs more tuning.\n\nI originally had different decay and storage parameters for different types of memory, based on this paper (https://arxiv.org/abs/2512.05470). But this required the LLM to classify memories at write time and it adds all sort of additional context and complexity (tags etc).\n\nIâ€™ve found that recall boost via the access count does a great job while keeping the memory schema simple. Additionally it works across multiple categories. Simply put, memory recalled more often surfaces more often. Architecture decisions are likely recalled at a higher frequency than one off debug sessions. Lastly, you can opt to keep debug sessions in local context and never add to memory.",
              "score": 2,
              "created_utc": "2026-02-09 11:55:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4fjpjl",
                  "author": "BC_MARO",
                  "text": "The access count as a proxy for importance is a really clean approach. It sidesteps the classification overhead and you get natural weighting for free. I've been doing something similar with my own setup where I just let frequency of recall determine what sticks around. The paper you linked is interesting, I'll check it out. For the usage-based decay, have you considered scaling the half-life dynamically based on how many memories you're storing? Small corpus might want longer retention, large one shorter.",
                  "score": 2,
                  "created_utc": "2026-02-09 13:44:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4b9sxi",
          "author": "IversusAI",
          "text": "Sounds amazing but it seems no windows support",
          "score": 1,
          "created_utc": "2026-02-08 20:28:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4bbl9d",
              "author": "_rendro",
              "text": "Yes not currently. You can compile from source for windows and I can look into adding support for windows in my CI release pipeline",
              "score": 1,
              "created_utc": "2026-02-08 20:37:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4l0uow",
          "author": "UnifiedFlow",
          "text": "Why use mcp overhead for local tools?",
          "score": 1,
          "created_utc": "2026-02-10 08:39:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzicc3",
      "title": "Combining mcp and skills, is there really a difference?",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1qzicc3/combining_mcp_and_skills_is_there_really_a/",
      "author": "BarryTownCouncil",
      "created_utc": "2026-02-08 19:31:40",
      "score": 8,
      "num_comments": 16,
      "upvote_ratio": 0.79,
      "text": "I'm new to MCP and about 7 days even newer to skills, and they're both bothering me somewhat in how they work. As far as I understand them at least. \n\nI hear about MCP servers being quite expensive on the token count and loading too much context, but then going into skills more, how to you coherently manage and deploy the things to a user base not as technical as yourself?\n\nI've a reverse API proxy service I'm working on fronting with some appropriate AI interface, and I've wavered back and forth between both forms, and am mostly coalescing on both at the same time, merging the concept together and was keen to hear takes on how bad an idea this is, or how much money it'll make me...\n\nThere's notional methods to create plugins for skills, but isn't that basically already what an MCP is? Especially if you reframe it from a number of tools to do tasks into a few tools to provide knowledge on how to do a task, these things are starting to look very similar.\n\nMy first MCP had a tool for searching, say, the finance system, another for the monitoring system... but I've since replaced it with a tool to get overview docs, a tool to get specific docs and a tool to actually communicate with the entire proxy. This is modelling, I think, the layout of skills. My tool to hit the proxy IS the very same script that would sit in the scripts/ directory in a Skill layout. the basic overview, what would be the frontmatter in a skill is delivered on loading the MCP etc... everything gets its exact equivalent.\n\nI may be naive, but I see no reason why, if an agent wants more information about system X, it would be any more or less likely, or more or less better off, by getting a chunk of knowledge from a tool call vs reading a secondary local text file. Is there \\*ANY\\* difference at all?! \n\nI feel like there can't be, neither can be better or worse. It's still just at the mercy of what the agent chooses to do next. it wouldn't use one implementation but not the other (assuming it's only aware of one at any given time). Am I missing something fundamental about how agents work and engage with resources?\n\nSo here I am trying to put finishing touches on a solution that can be used 100% as equally as a skill or as an MCP, just depending on how the user chooses to install it. I'm mulling over the best ways to converge the two. like, should the MCP just download it's skill data and say \"hey, there's a skill in that folder\" every time? \n\nEach implementation seems to push me further towards more convergence, where the only thing I know I need is some python script running locally in either a persistent or single shot mode depending on whether I'm orientating it in MCP or skill mode, and increasingly not really seeing much difference between the two.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1qzicc3/combining_mcp_and_skills_is_there_really_a/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o4b01gh",
          "author": "Ordinary-You8102",
          "text": "idh power to read all that but basically  \n\n\n\nMCP - a way for agents to get access to a platform of some sort through API \n\nSkills - a way to extend a LLM capability to your needs by feeding it custom md files that contains detailed instructions, local commands - essentially a recipe that tells the LLM what steps to take as-is. \n\nthey complement each other",
          "score": 12,
          "created_utc": "2026-02-08 19:39:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4b0ct6",
              "author": "BarryTownCouncil",
              "text": "Point is can they compliment each other to the point of being a single thing?",
              "score": 0,
              "created_utc": "2026-02-08 19:41:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4b13yz",
                  "author": "Ordinary-You8102",
                  "text": "at this point its called an agent ",
                  "score": 5,
                  "created_utc": "2026-02-08 19:44:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4c8vzu",
                  "author": "HardyPotato",
                  "text": "the biggest advantage to me for an mcp is authentication. with a skill you can do the same, but it's like having 2 solutions for 1 problem, pick your favorite. depending on the service, having both is better than choosing 1.",
                  "score": 1,
                  "created_utc": "2026-02-08 23:32:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4b7br6",
          "author": "penguinzb1",
          "text": "You're not missing anything fundamental. The delivery mechanism (tool call vs file read) doesn't change what the agent does with the information.\n\nThe real difference is deployment friction. Skills are simpler to distributeâ€”just copy some files and the agent picks them up. MCP requires a running server process, which means your users need to manage that lifecycle. For a reverse API proxy where you're already running something persistent, that difference matters less.\n\nIf you're already converging the implementations, lean into it. The script that hits your proxy is the same whether it's called from an MCP tool or a Skills script. The knowledge chunks are the same whether delivered via tool response or file read. You're just adapting the interface layer to fit each pattern.\n\nThe only place I'd be careful is state managementâ€”if your proxy needs persistent connections or session state, MCP's long-running process model fits better than Skills' ephemeral execution. But for stateless queries it really doesn't matter which interface you expose.",
          "score": 4,
          "created_utc": "2026-02-08 20:15:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4c5wmy",
              "author": "WealthSad4337",
              "text": "This is it. You could put your skills as MCP tool calls that return strings, it would be equivalent. But skills allow them to be stored client side which simplifies a lot of things if you use a single LLM provider.",
              "score": 2,
              "created_utc": "2026-02-08 23:14:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4b82p0",
          "author": "promethe42",
          "text": "MCP has a feature called \"prompts\". They are pretty much instructions on how to use/glue together tool calls in complex multi turn scenarios. Sounds like skills? That's because it is.\n\n\nThe problem? Most - if not all - clients simply do not implement that MCP prompt feature. TBH, most open source are terrible anyway compare to what you get with claude.ai or ChatGPT (I should know, I'm trying to use them everyday).\n\n\nSo skills being just Markdown files makes them sound easier. To be fair it is for non technical users. And for implementors, it is actually pretty much reading a text file.Â \n\n\nIn the end, it's all about what ends up in the context. Call it a skill, call it a MCP prompt, it's just text in the context window.Â ",
          "score": 3,
          "created_utc": "2026-02-08 20:19:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4bnrlz",
          "author": "sjoti",
          "text": "Generally MCP is more focused on taking action and taking to external services, whereas skills are more about processes/information documentation.\n\nTools provided by MCP servers can for example allow an llm to read stuff in Google drive, create new documents etc. You let the MCP server handle the authentication, the model can securely connect to it and now take action.\n\nWhat action to take, how your own drive is structured, where and how you like your research stored, thats all stuff that is more suited to go into a skill. The action that needs to be taken to make it actually happen, thats for the MCP to handle. \n\nIf you want the same functionality of the MCP server into skills, then you would need to provide a model with the option to execute code directly with access to authentication. That's fine if you're a dev, but not fine for the average chatgpt user. \n\nAlso the MCP server can be more generalized, providing a set of tools. How an AI uses it, is for the AI + user to decide, and is where skills come in.",
          "score": 1,
          "created_utc": "2026-02-08 21:37:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4bq475",
          "author": "pbalIII",
          "text": "Saying it's all just text in context undersells what the runtime boundary actually gives you. Skills are static... the agent reads files, follows instructions, done. MCP keeps a live process running, which means the server can hold state between calls, push updates, and enforce schemas the agent validates before calling. That distinction is invisible for stateless lookups but starts mattering fast once you need auth handoffs, streaming results, or composed multi-step tool chains where the server coordinates between calls. Your proxy use case is interesting because you're basically proving the overlap on the read path. But if your proxy ever needs to maintain sessions, rate-limit per user, or serve different tool schemas based on auth scope, the persistent MCP process gives you a hook that file-based skills can't replicate without bolting on a sidecar anyway.",
          "score": 1,
          "created_utc": "2026-02-08 21:49:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4bwahy",
          "author": "Flashy-Bus1663",
          "text": "Semi related u can provide mcp tools to a skill at least in Claude code.\n\nAlso things like file read and run script is provided by mcp tools they are just pre bundled into the harness.\n\nSkills are a bundled set of instructions ur asking the llm to follow do or reason about. \n\nMcp servers are a bundle of specialized tools that can accomplish a goal, the tools may even include instructions that describe others tools to solve some specific goal. \n\nU can derive a tool through other tools but this is a waste of tokens runs the risk of the llm not following the steps or instructions in the skill. \n\nIf you can help it u do want more specialized tool over a skill or hoping the llm derives the correct answer.",
          "score": 1,
          "created_utc": "2026-02-08 22:21:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4dgljt",
          "author": "BC_MARO",
          "text": "You're not wrong that they converge. I've been running both patterns and the main practical difference comes down to lifecycle: skills are static files the agent loads at startup, MCP tools are live services the agent discovers at runtime. When your \"knowledge\" rarely changes, a skill markdown file is cheaper on tokens than a tool call roundtrip. When it's dynamic data or needs auth, MCP wins because the server handles state. The hybrid approach you're describing (same script, different entry point depending on install mode) is probably where most people will end up.",
          "score": 1,
          "created_utc": "2026-02-09 03:33:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kw1wi",
          "author": "Glass-Combination-69",
          "text": "An mcp loads tools that an agent can invoke. The tools are deterministic. Their shape doesnâ€™t change. \n\nSkills give instructions on how to perform a task. The instructions might show a code snippet that it can run, or a bash script. \n\nSkills are discovered as they are needed so generally creates less context but more round trips than mcp (more discovery)\n\nMcps have less round trips and more context bloat. \n\nAn example might be a weather mcp. It gives a tool that the agent can invoke to get the weather. The agent runs the tool and gets the result, easy. \n\nWhat about if it was a weather skill?\nAgent has to read the skill, then the skill tells it it can run a command line get-weather â€”location xyz\nIt then runs that bash command and looks at the result. \n\nHere the mcp was faster, but not all requests need to know the weather, so it was wasted context for those requests where the skill would never have loaded. \n\nClaude has a special way of deferring tools from mcps where it has a â€œsearchâ€ tools function that adds a round trip but stops context bloat with mcps. Codex does not. \n\nSo as of today if using Claude code mcp / skills donâ€™t have a huge difference in terms of context bloat. But codex hasnâ€™t put in this feature yet. \n\nIt also comes down to features. I can write a skill to give it access to an api. With just one example and its latent knowledge it now has access to the whole api. Thatâ€™s a couple paragraphs. \n\nThe same as an mcp? Good luck, thousands of lines of wrappers and context bloat etc.",
          "score": 1,
          "created_utc": "2026-02-10 07:52:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4d2pk9",
          "author": "grewgrewgrewgrew",
          "text": "skills outperform MCP descriptions when it comes to in-context retrieval. It's easy to try and confirm for yourself, just copy paste the skill contents into the tool description and see the performance.\n\nhttps://arxiv.org/abs/2307.03172\n\nI wrote about this recently, as have anthropic\nhttps://www.june.kim/skills-lack-determinism",
          "score": 0,
          "created_utc": "2026-02-09 02:19:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}