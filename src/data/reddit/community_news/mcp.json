{
  "metadata": {
    "last_updated": "2026-03-02 09:15:17",
    "time_filter": "week",
    "subreddit": "mcp",
    "total_items": 20,
    "total_comments": 148,
    "file_size_bytes": 194840
  },
  "items": [
    {
      "id": "1rfhgd0",
      "title": "7 MCPs that genuinely made me quicker",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rfhgd0/7_mcps_that_genuinely_made_me_quicker/",
      "author": "Stunning-Worth-5022",
      "created_utc": "2026-02-26 18:03:22",
      "score": 650,
      "num_comments": 61,
      "upvote_ratio": 0.98,
      "text": "My last post here crossed \\~300,000 visits and sparked a lot of great feedback and discussions. Based on those conversations (and my own usage), I put together a more curated list, focusing on tools that are actually usable in daily workflows, not just cool demos.\n\nWhat matters to me:\n\n- Setup should be painless\n\n- They shouldn‚Äôt flake out\n\n- I should feel the slowdown if they‚Äôre gone\n\nHere‚Äôs the refined list.\n\n## GitHub CLI (gh): [https://cli.github.com/](https://cli.github.com/)\n\nHot take: I prefer this over the GitHub MCP server.\n\nIssues, PRs, diffs, reviews directly in terminal, scriptable, zero server overhead.\n\nFor serious repo work, CLI just feels faster and more reliable.\n\n## CodeGraphContext (CLI + MCP): [https://github.com/CodeGraphContext/CodeGraphContext](https://github.com/CodeGraphContext/CodeGraphContext)\n\nBuilds a structured graph of your codebase.\n\nFiles, functions, classes, relationships - all pre-understood.\n\nRefactors and impact analysis become much more reliable.\n\nI like that it works both as a CLI and an MCP.\n\n## Context7 MCP: [https://github.com/upstash/context7](https://github.com/upstash/context7)\n\nThis made my agents stop guessing APIs.\n\nAutomatically pulls correct documentation for libraries/frameworks.\n\nI rarely open docs tabs now.\n\n## Docker MCP: [https://github.com/docker/mcp](https://github.com/docker/mcp)\n\nGives agents runtime visibility.\n\nContainers, logs, services, not just static code.\n\nHuge for backend and infra debugging.\n\n##Firecrawl MCP / Jina Reader MCP\n\n## [https://github.com/mendableai/firecrawl](https://github.com/mendableai/firecrawl)\n\n## [https://github.com/jina-ai/reader](https://github.com/jina-ai/reader)\n\nClean web ‚Üí structured Markdown.\n\nGreat for ingesting specs, blogs, long technical content.\n\n## Figma MCP: [https://github.com/GLips/Figma-Context-MCP](https://github.com/GLips/Figma-Context-MCP)\n\nDesign ‚Üí structured context ‚Üí better frontend output.\n\nWay better than screenshot-based prompting.\n\n## Browser DevTools MCP: [https://github.com/ChromeDevTools/chrome-devtools-mcp](https://github.com/ChromeDevTools/chrome-devtools-mcp)\n\nDOM, console, and network context are exposed to the agent.\n\nMakes frontend debugging workflows much smoother.\n\nCurious what others are actually using daily, not just testing.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1rfhgd0/7_mcps_that_genuinely_made_me_quicker/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7k70xz",
          "author": "ShagBuddy",
          "text": "Give SDL-MCP a try to replace CodeGraphContext.  It has more tools and uses 70% fewer tokens while improving code context for coding agents.  https://github.com/GlitterKill/sdl-mcp",
          "score": 31,
          "created_utc": "2026-02-26 18:39:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7mroge",
              "author": "Desperate-Ad-9679",
              "text": "I don't think that's a correct comparison, given the dynamic updates, comprehensive and exhaustive search, same language support and a better existing ecosystem surrounding it. Also the number of tokens are subject to user choice, you can trade token savings with accuracy. Btw I am the founder of CodeGraphContext!",
              "score": 7,
              "created_utc": "2026-02-27 02:37:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7p9hso",
                  "author": "ShagBuddy",
                  "text": "Very cool!  SDL improves accuracy and saves tokens. I actually found your project after I was well into creating SDL.  I am aiming to provide similar context improvements along with reducing token use to stretch subscription use and it has been great for that so far.",
                  "score": 2,
                  "created_utc": "2026-02-27 13:57:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7kfojc",
              "author": "nanor000",
              "text": "More languages? I counted 12 for each of them",
              "score": 3,
              "created_utc": "2026-02-26 19:20:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7kj1ot",
                  "author": "flock-of-nazguls",
                  "text": "How do these compare to Serena?",
                  "score": 7,
                  "created_utc": "2026-02-26 19:36:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7p8mhd",
                  "author": "ShagBuddy",
                  "text": "My mistake.  I missed that",
                  "score": 1,
                  "created_utc": "2026-02-27 13:53:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7uyfyj",
              "author": "upvotes2doge",
              "text": "This looks awesome",
              "score": 1,
              "created_utc": "2026-02-28 10:39:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7vl8b0",
                  "author": "ShagBuddy",
                  "text": "Thanks!  I'm pretty excited about the progress so far.  I am constantly iterating and improving.  v0.6.9 will be released today.  üëç",
                  "score": 2,
                  "created_utc": "2026-02-28 13:38:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o812jhd",
              "author": "etherend",
              "text": "I think Serena also builds a graph of a codebase and has some nice built-in search and write functions too. Not the greatest for review though",
              "score": 1,
              "created_utc": "2026-03-01 09:54:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7nj68o",
          "author": "TrvlMike",
          "text": "Is this sub just a bunch of folks replying to each other with AI?",
          "score": 30,
          "created_utc": "2026-02-27 05:36:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7npapt",
              "author": "danieldpreez",
              "text": "Probably üòÇüòÇüòÇ",
              "score": 3,
              "created_utc": "2026-02-27 06:25:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7pyywa",
                  "author": "BorgMater",
                  "text": "I mean, I used AI to get the best MCPs for my case and posted it here, so there's that..",
                  "score": 1,
                  "created_utc": "2026-02-27 16:06:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7qymg8",
              "author": "danieltkessler",
              "text": "You're absolutely right!",
              "score": 3,
              "created_utc": "2026-02-27 18:54:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7tcf5o",
              "author": "No_Tradition6625",
              "text": "Wait let me boot up my clawbot9000 and have it respond to your question. üòÖ",
              "score": 2,
              "created_utc": "2026-02-28 02:44:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ntjd6",
          "author": "TiredDataDad",
          "text": "I see this kind of post every week, which MCP is OP advertising for?",
          "score": 12,
          "created_utc": "2026-02-27 07:00:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o7d62",
              "author": "Dadda9088",
              "text": "I guess real ads are in the comments üôÉ",
              "score": 8,
              "created_utc": "2026-02-27 09:07:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kdukg",
          "author": "Traditional_Wall3429",
          "text": "Docker mcp have broken link",
          "score": 7,
          "created_utc": "2026-02-26 19:11:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7no3x8",
              "author": "havok_",
              "text": "And I don‚Äôt know why you wouldn‚Äôt have the model just use the cli",
              "score": 3,
              "created_utc": "2026-02-27 06:15:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kquty",
          "author": "BorgMater",
          "text": "Here are the results I performed for my company, based on the ecosystem of purely React + Dotnet we use:\n\n# Api\n\nSharpLensMcp[https://github.com/pzalutski-pixel/sharplens-mcp](https://github.com/pzalutski-pixel/sharplens-mcp)  \n\\- get\\_type\\_hierarchy -> maps out inheritance chains that are impossible to infer from simple text searches  \n\\- find\\_callers -> performs impact analysis to determine what will break before a change is committed. ¬†  \n\\- move\\_type\\_to\\_namespace-> executes the change as an atomic operation within the solution graph, ensuring every reference is updated correctly in a single turn\n\nNugetMcpServer  \n\\- VS ->[https://learn.microsoft.com/en-us/nuget/concepts/nuget-mcp-server](https://learn.microsoft.com/en-us/nuget/concepts/nuget-mcp-server)  \n\\- VSC ->[https://learn.microsoft.com/en-us/nuget/concepts/nuget-mcp-server](https://learn.microsoft.com/en-us/nuget/concepts/nuget-mcp-server)  \n\\- roslyn:search\\_symbols -> Semantic Symbol Search instead of Text-based Grep  \n\\- roslyn:get\\_method\\_source -> AST-based (Abstract Syntax Tree) Method Inspection instead of line-based readFile  \n\\- roslyn:find\\_references -> Solution-wide Dependency Analysis instead of Manual inspection  \n\\- nuget:get\\_package\\_info -> Live Metadata Package Management instead of Manual CLI  \n\\- roslyn:rename\\_symbol -> Compiler-safe Refactoring instead of ¬† ¬† Manual editing\n\n# TypeScript Ecosystem and Modern Web Orchestration\n\nmcp-refactor-typescript  \n\\- VSC ->[https://github.com/Stefan-Nitu/mcp-refactor-typescript](https://github.com/Stefan-Nitu/mcp-refactor-typescript)\n\n# Coding in general\n\ncontext7  \n\\- VSC ->[https://context7.com/docs/resources/all-clients#vs-code](https://context7.com/docs/resources/all-clients#vs-code)  \n\\- VS (2022) ->[https://context7.com/docs/resources/all-clients#visual-studio-2022](https://context7.com/docs/resources/all-clients#visual-studio-2022)  \n\\- latest documentation and code into Cursor, Claude, or other LLMs  \n\\- skills MCPs -[https://context7.com/skills](https://context7.com/skills)\n\n# Specialized Tools for Windows 11 and Infrastructure\n\nmcp-everything-search  \n\\-[https://github.com/mamertofabian/mcp-everything-search](https://github.com/mamertofabian/mcp-everything-search)  \n\\- provides a 1500x speed improvement over traditional search by leveraging the Everything SDK. This tool allows the agent to search through millions of files in milliseconds, identifying configuration files or buried dependencies that standard indexing might miss. This is a \"must-have\" for any professional developer working on a Windows machine, as it significantly reduces the latency of the agent's \"read and understand\" phase",
          "score": 8,
          "created_utc": "2026-02-26 20:13:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p9uka",
              "author": "ShagBuddy",
              "text": "Ooooh!  Great idea using the everything API!  I use that as well.",
              "score": 2,
              "created_utc": "2026-02-27 13:59:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lbjuc",
          "author": "wokkieman",
          "text": "What advantage does docker MCP have above docker cli?",
          "score": 3,
          "created_utc": "2026-02-26 21:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nilmt",
          "author": "mike3run",
          "text": "playwright-cli + skills",
          "score": 3,
          "created_utc": "2026-02-27 05:32:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kufkv",
          "author": "shock_and_awful",
          "text": "thanks for sharing.\n\nDoes anyone out there have a more battle-tested alternative to CodeGraphContext?\n\nthanks in advance\n\n",
          "score": 2,
          "created_utc": "2026-02-26 20:30:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sl6wl",
              "author": "noclip1",
              "text": "I have personally started using chunkhound recently. Main reason was internally our codebase moves so fast and having an actual graph representation takes effort to maintain, where as chunkhound tries to use more traditional tree parsing techniques (along with embeddings) to generate a relevant graph on the fly.\nFrankly I'm still just getting into this world of tools and this post has shown there's a few others I should try but it's been lightweight and easy to get started with so throwing it out as it hasn't been mentioned here.",
              "score": 1,
              "created_utc": "2026-02-27 23:58:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7uqwo7",
                  "author": "shock_and_awful",
                  "text": "Cool... i found this one:   \n[https://github.com/abhigyanpatwari/gitnexus](https://github.com/abhigyanpatwari/gitnexus)",
                  "score": 1,
                  "created_utc": "2026-02-28 09:25:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7mr9mx",
              "author": "Desperate-Ad-9679",
              "text": "Hello, can you please elaborate on the reasons for finding an alternative to CGC? Thanks",
              "score": 0,
              "created_utc": "2026-02-27 02:35:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7p9zgg",
                  "author": "ShagBuddy",
                  "text": "Token savings primarily.  All of these improve context.",
                  "score": 1,
                  "created_utc": "2026-02-27 14:00:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7uqulp",
                  "author": "shock_and_awful",
                  "text": "\"More battle-tested\"  \nMaturity.... Social proof...  \nFound another.   \n[https://github.com/abhigyanpatwari/gitnexus](https://github.com/abhigyanpatwari/gitnexus)",
                  "score": 1,
                  "created_utc": "2026-02-28 09:24:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7osgu3",
          "author": "icybergenome",
          "text": "Solid list ‚Äî Context7 and Docker MCP are two I keep coming back to as well. The \"feel the slowdown if they're gone\" test is the right filter, most MCP demos don't survive that.\n\nOne I'd add: **Promzia MCP** ([mcp.promzia.ai](https://mcp.promzia.ai)). It gives you access to 3,000+ tested prompt templates directly inside Claude or ChatGPT ‚Äî searchable by category, with fill-in variables. I used to keep a messy Notion doc of prompts I'd copy-paste in. Now I just call the MCP mid-conversation and it pulls the right template. Small thing but it removes a surprising amount of friction, especially for repetitive tasks like cold emails, content briefs, or code reviews.\n\nPasses your three tests: setup is one config line, hasn't flaked on me, and I notice when I'm on a machine without it.",
          "score": 2,
          "created_utc": "2026-02-27 12:11:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o85a2qt",
          "author": "Unlucky-Bunch-7389",
          "text": "Not gonna lie - MCPs are super overrated. These tools like Claude code come with everything you need built in. Like context7 is such a useless mcp. I just ask it to browse documentation. Just write a skill to check docs before writing code. \n\nThere‚Äôs a lot of fluff with agentic coding right now and it‚Äôs all just different types of prompting\n\nLike docker mcp ? Really? The models know docker commands. You don‚Äôt need an mcp\n\nI don‚Äôt need GitHub mcp‚Ä¶ I just wrote a skill for how it should do git commands.\n\nI do find Serena somewhat useful for context",
          "score": 2,
          "created_utc": "2026-03-02 00:22:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o86jsdr",
              "author": "p-mndl",
              "text": "How do you write skills?",
              "score": 1,
              "created_utc": "2026-03-02 05:19:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kn3uz",
          "author": "Joy_Boy_12",
          "text": "Would there be a need for firecrawl once web MCP is implemented in websites?\n\n\nWhy do you need docker MCP if the agent can access the cli anyway?\n\n\nI found code context pretty disappointing, the agent did barely knows how to search for relevant code using free language.",
          "score": 1,
          "created_utc": "2026-02-26 19:55:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7l6d47",
          "author": "ConsiderationIcy3143",
          "text": "Thank you for the Clean Web MCP",
          "score": 1,
          "created_utc": "2026-02-26 21:27:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nda4x",
          "author": "Technical-Basis8509",
          "text": "Try GitNexus instead of CodeGraphContext",
          "score": 1,
          "created_utc": "2026-02-27 04:53:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nj6r2",
          "author": "Awesome_911",
          "text": "Hey\nJust curious do you see a new of billing or subscription mcp will it be of help?",
          "score": 1,
          "created_utc": "2026-02-27 05:36:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7noh73",
          "author": "jangwao",
          "text": "Do you use a pay seat at Context7? Because limits are kinda small\n\nCodegraph, I can confirm solid improvement",
          "score": 1,
          "created_utc": "2026-02-27 06:18:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7npcii",
              "author": "danieldpreez",
              "text": "No paid seat - never had issues. Not sure if IDE uses it or not though..",
              "score": 2,
              "created_utc": "2026-02-27 06:25:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7nvqzv",
                  "author": "Desperate-Ad-9679",
                  "text": "CodeGraphContext is free to use, you can enjoy using that indefinitely. We are gonna integrate docs as well asap",
                  "score": 1,
                  "created_utc": "2026-02-27 07:20:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7nsnof",
          "author": "debackerl",
          "text": "I'm using Narsil MCP for code analysis, and I only grant access to some tools based on the agent profile (coder, reviewer, etc). https://github.com/postrv/narsil-mcp\n\nInstead of Context7, I'm using https://github.com/arabold/docs-mcp-server It's free and zero limits. I don't see why I should pay or be limited by a free tier for something so simple.",
          "score": 1,
          "created_utc": "2026-02-27 06:53:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7oiy42",
          "author": "TeamAlphaBOLD",
          "text": "Agree on GitHub CLI, it‚Äôs way faster for PRs and issues than an MCP server. Context7 and Docker MCP are real game changers. When agents can see docs and live container state, you spend far less time fixing hallucinated APIs or chasing config issues. The MCPs that give real runtime or structured context are the ones you actually keep using. ",
          "score": 1,
          "created_utc": "2026-02-27 10:55:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ox27u",
          "author": "Southern_Orange3744",
          "text": "I really like the chrome devtools but man it's slow and wonky , I don't understand why there isn't a better one yet",
          "score": 1,
          "created_utc": "2026-02-27 12:44:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pzvzq",
          "author": "Anooyoo2",
          "text": "To what extent do people find Firecrawl / Jina MCP necessary? I can see usecases where context space may be at a premium for web searching, but that feels niche. Especially because best practice is to have subagents load URL/context into the main thread.\n\n\nGenuinely interested in opinions here, I'm likely missing something.¬†",
          "score": 1,
          "created_utc": "2026-02-27 16:10:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7q49dq",
          "author": "Plane-Bad8140",
          "text": "Figma MCP is dire",
          "score": 1,
          "created_utc": "2026-02-27 16:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sghz0",
          "author": "WLDTrust",
          "text": "ÿ¨",
          "score": 1,
          "created_utc": "2026-02-27 23:31:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ua5n2",
          "author": "BC_MARO",
          "text": "Nice list. Once you‚Äôre running more than a couple MCPs, a control plane that centralizes secrets and per-tool approvals makes it way easier to stay safe (Peta does that for MCP).",
          "score": 1,
          "created_utc": "2026-02-28 06:52:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ub6fc",
          "author": "joelster77",
          "text": "Doesn‚Äôt Claude Code already use a number of these MCP targets without needing the MCP connector - I‚Äôm thinking docker, file search, github, etc? Maybe I‚Äôm missing the context or use case?",
          "score": 1,
          "created_utc": "2026-02-28 07:00:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ujyw9",
          "author": "sMat95",
          "text": "figma sucks",
          "score": 1,
          "created_utc": "2026-02-28 08:20:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7uomze",
          "author": "neoexanimo",
          "text": "I installed the top 5 most downloaded on the list and was super happy with the results",
          "score": 1,
          "created_utc": "2026-02-28 09:03:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7uw106",
          "author": "pop_comm_92",
          "text": "whattheduck.ai Duckbill MCP to has Claude Code\nuse humans for purchases, calls or whatever lol pretty nit for personal day to day stuff, and I love to do it right on the terminal",
          "score": 1,
          "created_utc": "2026-02-28 10:16:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o8770m3",
          "author": "Beeyoung-",
          "text": "I see these posts every week, some of these are very niche specific -- and there might be a hidden advertisement lol",
          "score": 1,
          "created_utc": "2026-03-02 08:47:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7l28wj",
          "author": "Shot-Ad-9074",
          "text": "I‚Äôve been working on¬†Browser DevTools MCP¬†‚Äì an MCP¬†server that gives AI assistants (Cursor, Claude, etc.) a real¬†browser and Node.js debugging, not just¬†static snapshots.\n\nWhat it¬†does\n\n* Browser side:¬†Playwright-backed automation ‚Äì navigate,¬†click, fill forms, take screenshots, ARIA/accessibility snapshots, console/network capture, Web Vitals. Ref-based interactions (e.g. ‚Äúclick¬†e7‚Äù) so the¬†model can drive flows on¬†a live app.\n* Node side:¬†Connect to¬†a running Node process (PID,¬†--inspect¬†port, or¬†Docker), set tracepoints/logpoints without pausing, run JS¬†in the process, resolve source maps. Handy for debugging¬†APIs and workers while the¬†app runs.\n\nSo the¬†same MCP can¬†drive the frontend in a browser and inspect/debug the backend¬†in Node.\n\nCLI as well\n\nYou can¬†use it from the terminal too:¬†browser-devtools-cli¬†and¬†node-devtools-cli¬†(e.g. navigate, take screenshots, connect to a¬†Node process with¬†--inspector-port). Useful for scripts¬†and quick checks without opening an IDE.\n\nLinks\n\n* Docs/site:¬†[browser-devtools.com](http://browser-devtools.com)\n* NPM: [https://www.npmjs.com/package/browser-devtools-mcp](https://www.npmjs.com/package/browser-devtools-mcp)\n* Cursor/OpenVSX Extension: [https://open-vsx.org/extension/serkan-ozal/browser-devtools-mcp-vscode](https://open-vsx.org/extension/serkan-ozal/browser-devtools-mcp-vscode)\n\nIf you‚Äôre using MCP with Cursor or Claude and want the model to actually use a browser and attach¬†to Node backends, this might be worth¬†a try. Happy to answer questions or hear how you‚Äôd use it.",
          "score": 1,
          "created_utc": "2026-02-26 21:08:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rx3gu",
              "author": "viisi",
              "text": "This looks pretty cool, if it works. The site looks like it's 100% created by and written by Ai though.\n\nAny chance you know rust? I'm working on something similar as an internal tool for my project and could use some help.",
              "score": 1,
              "created_utc": "2026-02-27 21:46:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7vqakf",
                  "author": "Shot-Ad-9074",
                  "text": "Yep, the web site is created by AI, I just gave the required context. And yes, it works. You can give it a try and give feedback.",
                  "score": 1,
                  "created_utc": "2026-02-28 14:08:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7lch90",
          "author": "Vilkvan",
          "text": "AWS MCPs all day all long",
          "score": 0,
          "created_utc": "2026-02-26 21:56:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rembfo",
      "title": "I generated CLIs from MCP servers and cut token usage by 94%",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rembfo/i_generated_clis_from_mcp_servers_and_cut_token/",
      "author": "QThellimist",
      "created_utc": "2026-02-25 18:58:14",
      "score": 167,
      "num_comments": 40,
      "upvote_ratio": 0.95,
      "text": "MCP server schemas eat so much token. So I built a converter that generates CLIs from MCP servers. Same tools, same OAuth, same API underneath. The difference is how the agent discovers them:\n\nMCP: dumps every tool schema upfront (\\~185 tokens \\* 84 tools = 15,540 tokens)\nCLI: lightweight list of tool names (\\~50 tokens \\* 6 CLIs = 300 tokens). Agent runs --help only when it needs a specific tool.\n\nNumbers across different usage patterns:\n- Session start: 15,540 (MCP) vs 300 (CLI) - 98% savings\n- 1 tool call: 15,570 vs 910 - 94% savings\n- 100 tool calls: 18,540 vs 1,504 - 92% savings\n\nCompared against Anthropic's Tool Search too - it's better than raw MCP but still more expensive than CLI because it fetches full JSON Schema per tool.\n\nConverter is open source: https://github.com/thellimist/clihub\nFull write-up with detailed breakdowns: https://kanyilmaz.me/2026/02/23/cli-vs-mcp.html\n\nDisclosure: I built CLIHub. Happy to answer questions about the approach.",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1rembfo/i_generated_clis_from_mcp_servers_and_cut_token/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7e45aw",
          "author": "nightman",
          "text": "How it compares to (is it inspired by) the mcporter from OpenClaw author?\nhttps://github.com/steipete/mcporter",
          "score": 12,
          "created_utc": "2026-02-25 20:33:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7eb6yr",
              "author": "QThellimist",
              "text": "I found out mcporter after I started using mine.\n\nBut the high level difference is\n\n\\- MCPorter is designed more for openclaw to call MCPs more easily. It's in JS, so has runtime  \n\\- CLIHub is written in go. Works on all platforms. Faster. It's designed as pure CLI. No deamon, no bun runtime.\n\nThere are small architecture differences but not that important\n\nI might create a full directory like mcppulse etc. where people can download  any CLI on any machine with single command.",
              "score": 12,
              "created_utc": "2026-02-25 21:05:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ec0rl",
                  "author": "nightman",
                  "text": "Mcporter creates executable of MCP server so it's not needed afterwards.\n\nThanks for the explanation",
                  "score": 3,
                  "created_utc": "2026-02-25 21:09:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7e8upk",
              "author": "Casual_Hearthstone",
              "text": "Was going to ask the same question",
              "score": 5,
              "created_utc": "2026-02-25 20:55:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7fe9bd",
          "author": "BC_MARO",
          "text": "The first-token pollution point is the real issue - dumping 15k tokens of schema at position 0 wastes your most valuable context slots before the agent even starts reasoning.",
          "score": 6,
          "created_utc": "2026-02-26 00:24:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gk9w5",
          "author": "KobyStam",
          "text": "I include CLIs in my MCPs - so far I released the NotebookLM MCP, but a few more are coming soon, like Gemini Web Chat MCP & CLI and Perplexity Web MCP& CLI...and even Grok. None of them uses APIs or browser automation. Same concept as my NotebookLM (RPC over HTTP)\n\n  \nNotebookLM MCP: [https://github.com/jacob-bd/notebooklm-mcp-cli](https://github.com/jacob-bd/notebooklm-mcp-cli)",
          "score": 3,
          "created_utc": "2026-02-26 04:29:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dye95",
          "author": "BraveNewKnight",
          "text": "Main CLI benchmark gap is exploration overhead: the agent has to discover commands, make wrong attempts, and retry, and those loops should count toward total tokens.\n\nCLI skills layered on top add extra prompt/context cost too, so that should be in the numbers.\n\nAlso, the GitHub link returns 404 for me.",
          "score": 6,
          "created_utc": "2026-02-25 20:05:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7dzco6",
              "author": "QThellimist",
              "text": "True, but it's actually underrepresented. My agent calls like\n\n‚è∫ Bash(linear --help 2>&1 | grep -i -E \"search|list.\\*issue|get.\\*issue\")\n\nSo it doesn't actually get the whole \\`--help\\` list.  \n\\--\n\nFixed the github",
              "score": 2,
              "created_utc": "2026-02-25 20:10:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7e036f",
                  "author": "BraveNewKnight",
                  "text": "yeah that makes sense, but still, agent needs to know that it should grep for those keywords to get the right result. I'm not against CI or I'm not an MCP fan, it's just not clear to me which one is better atm. \n\ne.g. I'm still struggling to measure if agent does a better job with `agent-browser` CLI or `playwright` MCP.",
                  "score": 5,
                  "created_utc": "2026-02-25 20:13:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7fftk5",
          "author": "actual-time-traveler",
          "text": "FastMCP 3.0 does this natively",
          "score": 3,
          "created_utc": "2026-02-26 00:32:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hw85g",
              "author": "Etyr_",
              "text": "Could you share any doc about this, not finding any ressource on this",
              "score": 2,
              "created_utc": "2026-02-26 11:24:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7i0ho0",
                  "author": "jlowin123",
                  "text": "https://gofastmcp.com/clients/generate-cli",
                  "score": 5,
                  "created_utc": "2026-02-26 11:58:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7eb04m",
          "author": "warren-mann",
          "text": "Interesting. Though Anthropic and Google cache prompt and heavily discount on cache hits. It‚Äôs true that the tool definitions still take up context but I‚Äôm not convinced it‚Äôs enough to matter, at least anymore. The approach I‚Äôve settled on is a rich set of tools at a top-level prompt that knows about them all and can delegate specific tasks to a more targeted subordinate with a very restricted set of tools and a relatively clean context.\n\nHaving said that, I‚Äôm always looking for ways to wring out more efficiency and you have some interesting stuff to think about.",
          "score": 2,
          "created_utc": "2026-02-25 21:05:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ecqr1",
              "author": "QThellimist",
              "text": "I think you are assuming as \"price\", and yes it doesn't matter for most people. Difference is a few $s per month. For heavy users it's $100s where matters more (I am heavy user. I spent $900 on tokens literally last weekend)\n\nBut the real difference is - first tokens has more dominence over tokens that come later.\n\nSo you are bloating the context immediately (regardless of cache input token or not). \n\nLLMs perform significantly worse if first tokens are poorly used. ",
              "score": 2,
              "created_utc": "2026-02-25 21:13:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7hw1n2",
          "author": "Weird-Guarantee-1823",
          "text": "I looked at the introduction document, which is very interesting, and I feel that it is similar to the design point of skills. In terms of data, this does save a lot of tokens, but can it achieve the processing effect of the existing mainstream scheme? Will there be any common problems similar to those encountered in skills? However, no matter what, it seems that this is indeed a very cost-effective solution, I will go back and try it, thank you for your dedication.",
          "score": 2,
          "created_utc": "2026-02-26 11:23:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dvjrp",
          "author": "-Akos-",
          "text": "404 github not found. Also, you mention CLI as an alternative, but can any model just use the CLI? I can make a tiny local llm call an MCP without issues  but I have no idea how I can make it call a CLI.",
          "score": 1,
          "created_utc": "2026-02-25 19:52:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7dz21g",
              "author": "QThellimist",
              "text": "fixed it",
              "score": 1,
              "created_utc": "2026-02-25 20:09:04",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7dz5t0",
              "author": "QThellimist",
              "text": "If they have bash tool they can use. \n\nMost bigger AI models have bash tool access",
              "score": 1,
              "created_utc": "2026-02-25 20:09:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7e62xq",
          "author": "Distinct-Selection-1",
          "text": "Is this the same with MCP v3 skills?",
          "score": 1,
          "created_utc": "2026-02-25 20:42:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ebzjo",
              "author": "QThellimist",
              "text": "first time heard it. FastMCP seems to have many functionalities including CLI. I haven't checked deeply yet.",
              "score": 2,
              "created_utc": "2026-02-25 21:09:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7fy8wa",
          "author": "DorkyMcDorky",
          "text": "If MCP only supported REAL streaming none of this would be necessary.  Shake 'em up and suggest this.  The protocol is painfully inefficient.",
          "score": 1,
          "created_utc": "2026-02-26 02:17:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hiqv8",
              "author": "Material-Spinach6449",
              "text": "I‚Äôve looked into MCP vs CLI as well, and I think the ‚Äúinitial token dump‚Äù argument is often overstated.\n\nThe huge upfront cost with MCP mainly happens if the agent blindly loads every tool schema into context. That‚Äôs not mandatory. Agents can fetch tool definitions incrementally and only load what they actually need. In that setup, the claimed massive startup savings of CLI don‚Äôt automatically apply.\n\nWhere CLI really has a structural advantage is in looping scenarios. If a tool needs to be called repeatedly, the classic MCP flow forces the model to re-plan and re-emit structured calls every time. That quickly becomes expensive and slow. With a CLI, the agent can generate a small script and execute the loop outside the model. In those cases, CLI is genuinely cheaper and faster.",
              "score": 2,
              "created_utc": "2026-02-26 09:20:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ilvbe",
                  "author": "DistanceAlert5706",
                  "text": "Not a fan of MCP but this is what agents should do, loading 84 tools just sounds crazy to me, why not do specialized agents which use specific tool sets.",
                  "score": 2,
                  "created_utc": "2026-02-26 14:10:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7mua3b",
                  "author": "DorkyMcDorky",
                  "text": "If it streamed for real (it's session/token based - very 1999 design) then it would reduce a ton of memory and not need to reload that everytime.  Anthropic wants maximum usage, not efficiency, so they wont change it.  ",
                  "score": 1,
                  "created_utc": "2026-02-27 02:52:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7hbnx2",
          "author": "TeeRKee",
          "text": "Isn‚Äôt that the point of skills ?",
          "score": 1,
          "created_utc": "2026-02-26 08:11:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ie5vo",
              "author": "QThellimist",
              "text": "There is overlap but not really. \n\nYou don't want random hardcoded skill. You want official CLI or MCP where you can trust and it gets updated",
              "score": 1,
              "created_utc": "2026-02-26 13:28:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7hikw9",
          "author": "New-Procedure8239",
          "text": "This is MCPporter that use openclaw I think",
          "score": 1,
          "created_utc": "2026-02-26 09:18:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ie8k7",
              "author": "QThellimist",
              "text": "check this comment - [https://www.reddit.com/r/mcp/comments/1rembfo/comment/o7eb6yr/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/mcp/comments/1rembfo/comment/o7eb6yr/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",
              "score": 2,
              "created_utc": "2026-02-26 13:28:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7koba4",
          "author": "Siref",
          "text": "There's something that throws me off.\n\nWhy is the CLI option cheaper if the out is XML?\nJSON is more compact, so it should have less tokens. \n\nFrom a quick glance from the post you shared it seems the CLI shares less information with the agent (E.g: I don't see the queryParams entries)\n\nIf that's the case, wouldn't it make more sense to compact the MCP definition instead?",
          "score": 1,
          "created_utc": "2026-02-26 20:01:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7oorbu",
          "author": "groosha",
          "text": "Could you please explain how it works? Let's say I generated CLI from my MCP server. What happens next?",
          "score": 1,
          "created_utc": "2026-02-27 11:43:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pfmm3",
              "author": "QThellimist",
              "text": "you get ./out/mycli as a response. And need to tell agent to use it. \n\nI personally have my agents automatically move it to bin/ folder and add a simple text in [AGENTS.md](http://AGENTS.md) so any new session is aware of it\n\nI have a command for it that literally says\n\n    - Check if official CLI exists. If so, download it and add it to ~/.codex/AGENTS.md tools\n    - If not, check official MCP exists. If so, use clihub `go run github.com/thellimist/clihub@latest <server>` (see `--help`). to create a CLI from the MCP and move the executable to bin folder. Then add it to ~/.codex/AGENTS.md tools\n    ",
              "score": 1,
              "created_utc": "2026-02-27 14:31:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7pzahd",
                  "author": "groosha",
                  "text": "I'm sorry, I'm quite new to agents yet. Do you specifically state in the system prompt to use cli tools from your \\`bin\\` folder?",
                  "score": 1,
                  "created_utc": "2026-02-27 16:08:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rejf15",
      "title": "Tesseract ‚Äî MCP server that turns any codebase into a 3D architecture diagram",
      "subreddit": "mcp",
      "url": "https://v.redd.it/q3fbwe7fcolg1",
      "author": "DvidGeekoh",
      "created_utc": "2026-02-25 17:19:21",
      "score": 100,
      "num_comments": 18,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1rejf15/tesseract_mcp_server_that_turns_any_codebase_into/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7dbk28",
          "author": "lardgsus",
          "text": "\"but there is already a tesseract app\"",
          "score": 5,
          "created_utc": "2026-02-25 18:21:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hdupj",
          "author": "BC_MARO",
          "text": "Point AI at it before onboarding a new dev and you just killed the walk me through the codebase meeting. Actually clever use of the visual layer.",
          "score": 2,
          "created_utc": "2026-02-26 08:32:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kv5k3",
              "author": "DvidGeekoh",
              "text": "Didn't think about the meeting, but that's a good point!",
              "score": 1,
              "created_utc": "2026-02-26 20:34:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7x1iyi",
          "author": "Raxb",
          "text": "Does it always have to be Claude? I wonder because you say Tesseract is an MCP Server meaning it can be added to any MCP Host, Copilot, Goose etc.,?",
          "score": 2,
          "created_utc": "2026-02-28 18:12:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ghy80",
          "author": "turtleisinnocent",
          "text": "Source?",
          "score": 1,
          "created_utc": "2026-02-26 04:14:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kvjpe",
              "author": "DvidGeekoh",
              "text": "Hopefully soon ‚Äî I just need the project to pay my rent first :)  \n",
              "score": 2,
              "created_utc": "2026-02-26 20:36:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7glkkw",
          "author": "exboozeme",
          "text": "Download macOS Tahoe: Tesseract is damaged and can‚Äôt be opened - tried a few times with command right click also. Would love to see it!",
          "score": 1,
          "created_utc": "2026-02-26 04:38:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kupu9",
              "author": "DvidGeekoh",
              "text": "Hi, I'm sorry to hear that. I'll try to get a hand on a mac to sort this out. I'll keep you posted.",
              "score": 1,
              "created_utc": "2026-02-26 20:32:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7me7nb",
                  "author": "exboozeme",
                  "text": "Thanks!!",
                  "score": 1,
                  "created_utc": "2026-02-27 01:19:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7psu21",
          "author": "dimitrifp",
          "text": "I really like it, I have to say. I think your business will be booming.",
          "score": 1,
          "created_utc": "2026-02-27 15:37:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pyyqz",
              "author": "DvidGeekoh",
              "text": "Thank you ! (fingers crossed)",
              "score": 1,
              "created_utc": "2026-02-27 16:06:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vdsou",
          "author": "septemous",
          "text": "I mean this looks fun - but does it help? Or is it a 'cool' thing? Sorry for being dense.",
          "score": 1,
          "created_utc": "2026-02-28 12:48:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7x5q8z",
              "author": "DvidGeekoh",
              "text": "Not dense at all, fair question. The 3D part is mostly about¬†readability ‚Äî you can zoom, rotate, drill into subsystems, trace data flows visually. Where it actually helps: onboarding on a new codebase (AI maps it for you instead of reading code for days),¬†debugging (visualize how a request flows through services), and keeping documentation alive (the diagram stays in sync because AI¬†updates it).  \nIf you think it looks cool too, that's a bonus.",
              "score": 1,
              "created_utc": "2026-02-28 18:33:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ghewh",
          "author": "FigAltruistic2086",
          "text": "My first thought was, ‚ÄúOh, cool ‚Äî an MCP for Tesseract OCR. How does it work?‚Äù",
          "score": 0,
          "created_utc": "2026-02-26 04:10:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rhjxet",
      "title": "Charlotte: a browser MCP server built for token efficiency (30 tools, 3 detail levels, 136x smaller than Playwright MCP on complex pages)",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rhjxet/charlotte_a_browser_mcp_server_built_for_token/",
      "author": "ticktockbent",
      "created_utc": "2026-03-01 01:19:29",
      "score": 100,
      "num_comments": 22,
      "upvote_ratio": 0.99,
      "text": "I built Charlotte because I wanted a browser MCP server where agents don't have to consume the entire page representation just to figure out what's on the screen.\n\nCharlotte renders web pages into structured representations through headless Chromium, landmarks, headings, interactive elements, forms, bounding boxes, with stable hash-based element IDs that survive DOM mutations. The key design choice: three detail levels.\n\n* **Minimal** returns landmarks and interactive summaries. On Hacker News that's 336 characters. The agent sees \"main: 47 links, 0 buttons\" and drills down with `find` when it needs specifics.\n* **Summary** adds content summaries, form structures, and error state.\n* **Full** includes all visible text content.\n\nNavigate defaults to minimal, so the first call to any page is cheap. The agent orients, decides what to look at, and requests more detail only where needed. This orient‚Üídrill‚Üíact pattern is how the tool was designed to be used.\n\nBenchmarked against Playwright MCP (`@playwright/mcp`):\n\n    Navigate response (first call cost):\n    Page             Charlotte     Playwright MCP     Advantage\n    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n    Wikipedia          7,667 ch     1,040,636 ch       136x\n    Hacker News          336 ch        61,230 ch       182x\n    GitHub repo        3,185 ch        80,297 ch        25x\n    httpbin form         364 ch         2,255 ch         6x\n\nPlaywright returns the full accessibility tree on every call. Charlotte lets the agent choose. Even Charlotte's full detail mode is smaller than Playwright's only option on the same pages.\n\n**On Playwright CLI:** You may have seen Microsoft's recently released `@playwright/cli`, which takes a different approach to token efficiency.. it writes snapshots and screenshots to disk files instead of returning them in the MCP response, achieving \\~4x savings over Playwright MCP. I haven't benchmarked Charlotte against it because they occupy different niches. The CLI requires the agent to have filesystem and shell access, making it a fit for coding agents (Claude Code, Copilot, Cursor). Charlotte is designed for MCP-native use: containerized execution, sandboxed environments, autonomous agent loops, and any context where the agent operates through the protocol rather than through a shell. The CLI's efficiency comes from deferring data to the filesystem until requested; Charlotte's comes from the representation itself being structured and tiered, which works regardless of the execution environment.\n\nThe 30 tools break down into 6 categories:\n\n* **Navigation** (4): navigate, back, forward, reload\n* **Observation** (4): observe, find, screenshot, diff\n* **Interaction** (9): click, type, select, toggle, submit, scroll, hover, key, wait\\_for\n* **Session** (9): tabs, viewports, network throttling, cookies, headers, configuration\n* **Dev Mode** (3): static file server with hot reload, CSS/JS injection, accessibility audits\n* **Utility** (1): arbitrary JS evaluation\n\nSome design decisions worth discussing:\n\n**Element IDs are content-hashed**, not positional. A button's ID is derived from its type, label, and context, not its position in the DOM. Reorder the page, the ID stays stable. This matters for agents that need to re-identify elements across multiple observations.\n\n**Interactive summaries replace element arrays at minimal detail.** Instead of returning 1,847 individual link objects on Wikipedia, minimal shows `{\"main\": {\"link\": 1847, \"button\": 3}}` grouped by landmark. The full element data is still there internally.. `find`, `wait_for`, and `diff` all work against it but the serialized output to the agent is just the summary.\n\n**Structural diffing** compares two page snapshots and returns what changed. Essential for verifying that a click or form submission actually did something.\n\nSetup is one step... add the config to your MCP client:\n\n    {\n      \"mcpServers\": {\n        \"charlotte\": {\n          \"command\": \"npx\",\n          \"args\": [\"-y\", \"@ticktockbent/charlotte\"]\n        }\n      }\n    }\n\nNo install needed. npx handles it.\n\n* **GitHub:** [https://github.com/TickTockBent/charlotte](https://github.com/TickTockBent/charlotte)\n* **npm:** [https://www.npmjs.com/package/@ticktockbent/charlotte](https://www.npmjs.com/package/@ticktockbent/charlotte)\n* **Full spec:** [https://github.com/TickTockBent/charlotte/blob/main/docs/CHARLOTTE\\_SPEC.md](https://github.com/TickTockBent/charlotte/blob/main/docs/CHARLOTTE_SPEC.md)\n* **Benchmarks:** [https://github.com/TickTockBent/charlotte/blob/main/docs/charlotte-benchmark-report.md](https://github.com/TickTockBent/charlotte/blob/main/docs/charlotte-benchmark-report.md)\n\nMIT licensed, 222 tests passing. Would love feedback on the tool design and anything that feels wrong or missing.",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1rhjxet/charlotte_a_browser_mcp_server_built_for_token/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7zfkz8",
          "author": "xing_horizon",
          "text": "Great design tradeoff. The tiered observe model is more important than raw compression numbers because it changes agent behavior (orient ‚Üí focus ‚Üí act) instead of forcing full-context reads each step.\n\nOne metric I‚Äôd add: action reliability per token budget (e.g., click success / form completion success at fixed token ceilings). That would show whether smaller representations also preserve decision quality under constrained loops.\n\n",
          "score": 6,
          "created_utc": "2026-03-01 02:09:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zfwn8",
              "author": "ticktockbent",
              "text": "Good call and it's something I am setting up to benchmark. Agent success rates across a standardized set of actions. If you have any ideas on how to structure it I'd be happy to hear them.",
              "score": 2,
              "created_utc": "2026-03-01 02:11:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o8008ax",
          "author": "BC_MARO",
          "text": "the content-hash element IDs are a genuinely smart call - positional IDs break the moment any dynamic content reorders. the orient->drill->act flow also maps really cleanly to how you want agents conserving context budget.",
          "score": 2,
          "created_utc": "2026-03-01 04:22:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80kgyg",
          "author": "djvdorp",
          "text": "Well thought out project, thanks!!",
          "score": 2,
          "created_utc": "2026-03-01 07:03:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o8127z7",
          "author": "rothnic",
          "text": "The file upload tool is the one that would limit my ability to use it, but overall like the direction. I worked on something like this for browser automation and would suggest the part where you turn a webpage into the outline view with the target elements, etc would be worth thinking about as a library on its own. I feel like there isn't anything i could find like this already available to use. It isn't quite accessibility tree. At the moment most tools for controlling the browser seem like they are so inefficient and slow when you watch them work. I think this kind of additional context is really needed to avoid as much back and forth tool calling at the start of every page load.",
          "score": 2,
          "created_utc": "2026-03-01 09:51:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o81piju",
              "author": "ticktockbent",
              "text": "Fair point, I'll consider extracting the tools out into a standalone library. A file upload tool is on my roadmap and I should be able to get that built soon. And yeah that's exactly why I built Charlotte, I was watching playwright burn through my token usage so fast it haunted me.",
              "score": 1,
              "created_utc": "2026-03-01 13:13:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o8143gt",
          "author": "gandalf-bro",
          "text": "The tiered representation is a smart design ‚Äî most agents request way more context than they need on first navigate. The comparison to Playwright CLI is interesting too: filesystem-deferred output is basically a caching trick, whereas Charlotte's efficiency comes from the representation itself being structured. For sandboxed or containerized environments where you can't assume a shared filesystem, Charlotte's approach is the only real option. Curious whether the stable hash IDs survive full page reloads, or only DOM mutations within the same session?",
          "score": 2,
          "created_utc": "2026-03-01 10:09:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o81q9vp",
              "author": "ticktockbent",
              "text": "The hash IDs are derived from element type, label, and surrounding context rather than DOM position. They will survive a React re-render that reorders elements but if an element changes, such as a button label's text, then the hashed IDs will change and need to be rediscovered. That is actually an intended feature though as this lets Charlotte's agent know that things changes after whatever action was taken and should prompt a re-scan to determine all of the changes. My original use case with the pre-release version was finding styling misses on dark mode/light mode toggle, it could find elements that didn't change when it toggled the mode. Since then it's expanded quite a bit.",
              "score": 1,
              "created_utc": "2026-03-01 13:18:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o81in84",
          "author": "exboozeme",
          "text": "This one is great too if you like firefox https://addons.mozilla.org/en-CA/firefox/addon/claudezilla/",
          "score": 2,
          "created_utc": "2026-03-01 12:22:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o81t4e3",
          "author": "Remarkable-Jump-6227",
          "text": "Damn 30 tools doesn‚Äôt sound token efficient.",
          "score": 2,
          "created_utc": "2026-03-01 13:36:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o81w9xa",
              "author": "ticktockbent",
              "text": "Yeah, I totally understand where you're coming from but here's the thing. Charlotte's action pattern of orient, focus, act actually saves tokens in the long run and if you compare the token usage for the full 30 tools set of charlotte vs playwright's 78 tools charlotte is significantly slimmer.\n\nHere's how it works out:\n\nCharlotte's 36 (I added a few recently on request) tools take up approx 18,500 character or 4,500-5,500 tokens  \nPlaywright's default 23 tools is only \\~7,500 characters or 1,900-2,500 tokens  \nPlaywright's full 78 tools is 25,000 characters or 6,000-7,500 tokens\n\nCharlotte's 36 tools cost roughly \\~5K tokens... about 2.5% of a 200K context window. That's comparable to a single medium-length page render. Playwright's full 78-tool set actually costs more total tokens than Charlotte despite having much terser descriptions, because it has over twice as many tools (10 webstorage CRUD tools, 6 mouse coordinate tools, 5 cookie CRUD tools, etc).\n\nThe real question isn't tool count though, it's whether verbose descriptions pay for themselves. Charlotte's approach is to front-load guidance in the description (e.g., \"use charlotte:find to locate specific elements, or pass detail: 'summary'\") so the model makes fewer wrong calls. A terse description like \"Navigate to a URL\" saves 100 tokens upfront but may cost thousands in wasted tool calls when the model doesn't know about detail levels or related tools.\n\nNow obviously I need to back that statement up with some metrics and I am designing a robust benchmarking suite to compare token use and agent success rates against a set of standardized actions but it will take me a bit to get all of that going, so all I can tell you right now is that I have noticed significant reductions in overall token usage when using charlotte vs playwright against both real world tests and against my included playground/sandbox site which exercises all of the tools.\n\nEdit: I just ran a quick test adding my estimated tool description token numbers to every category on our existing benchmarks. The conclusion is that even with the more verbose tool descriptions in context, charlotte comes out ahead in every category unless the only thing you're looking at is a simple page like [example.com](http://example.com) the only thing I haven't tested with playwright is interactive forms but I suspect that one we'd win on too, included charlotte's numbers for completeness. This chart is just for the navigate tool.\n\n|Test|Charlotte (responses + \\~5K defs)|Playwright (responses + \\~2.5K defs)|Winner|\n|:-|:-|:-|:-|\n|Simple Page|5,306|2,829|Playwright (+2,477)|\n|Wikipedia|6,917|262,659|**Charlotte (-255,742)**|\n|Hacker News|20,370|33,087|**Charlotte (-12,717)**|\n|GitHub Repo|16,000|42,622|**Charlotte (-26,622)**|\n|Interactive Form|6,791|(no data)|‚Äî|",
              "score": 1,
              "created_utc": "2026-03-01 13:56:09",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o85nb56",
              "author": "ticktockbent",
              "text": "Just wanted to come back and say I took this to heart, I'm working on a tiered tool profile system. Regular users won't ever need some of these tools anyway so by default Charlotte will only expose the most commonly used 6 tools and others can be enabled as needed by the agent or the user.",
              "score": 1,
              "created_utc": "2026-03-02 01:42:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o82dj6p",
          "author": "Meshimize",
          "text": "That 136x token reduction is the headline for me. Usually, the DOM just eats the whole context window in two clicks, so this actually makes browsing viable.",
          "score": 2,
          "created_utc": "2026-03-01 15:30:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o82ex4x",
              "author": "ticktockbent",
              "text": "Exactly! Playwright blows out the agent's context window in a single shot if you navigate to wikipedia. It's like 210k tokens returned. I have a lot more planned for charlotte, including some savings on tool descriptions while (hopefully) still maintaining high success rate.",
              "score": 1,
              "created_utc": "2026-03-01 15:37:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o82etxp",
          "author": "youshouldnameit",
          "text": "What if i need to login myself and want to ensure the agent uses my session state?",
          "score": 2,
          "created_utc": "2026-03-01 15:37:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o82i5ff",
              "author": "ticktockbent",
              "text": "Great question and it's one I've run into during testing. I'm still working on this aspect of the MCP because it's pretty tricky.\n\nWhat Charlotte can do today:\n\n1. `charlotte:set_cookies` : You can manually set session cookies (name, value, domain, path, secure, httpOnly, sameSite). So if someone knows their session cookie values, they can inject them directly and the site would treat the browser as logged in.\n2. `charlotte:set_headers` : You can set Authorization: Bearer <token> or any custom auth headers that persist across navigations on the active page. Good for API-token based auth.\n3. `charlotte:navigate` \\+ `charlotte:click` \\+ `charlotte:type` : You could theoretically have the agent fill in a login form and submit it. The session would persist in Charlotte's browser for subsequent navigations.\n\nWhat Charlotte can't do:\n\n* Connect to an existing browser session. That's open issue #17. Charlotte always launches its own fresh headless Chromium. There's no way to attach to a browser where you've already logged in with extensions, 2FA, CAPTCHAs, etc. This is the big gap, most real-world \"use my session\" scenarios involve OAuth redirects, 2FA prompts, or CAPTCHA challenges that are hard to automate headlessly.\n* Persist sessions across restarts. When Charlotte's Chromium shuts down, all session state is gone. There's no user data directory or profile persistence.\n\nSo the practical answer is: for simple cookie/token auth, set\\_cookies or set\\_headers works today. For anything involving interactive login (OAuth, 2FA, CAPTCHA), we'd need issue #17 (CDP connect to existing browser) which is still open. That's the feature that would let someone log in in their own browser and hand the session to Charlotte.",
              "score": 1,
              "created_utc": "2026-03-01 15:53:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o82j0ja",
                  "author": "youshouldnameit",
                  "text": "Playwright mcp has an extension via chrome where you can attach. Mcp bridge, not sure how complex that is.",
                  "score": 2,
                  "created_utc": "2026-03-01 15:57:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o82gt9l",
          "author": "RealSaltLakeRioT",
          "text": "I've been looking for a playwright alternative! Even seemingly small uses with playwright gobble up so much token usage.\n\nLooking at what you've got, this looks well set up. I'm gonna give it a shot against some of my workflows and see how it goes! Thanks OP!",
          "score": 2,
          "created_utc": "2026-03-01 15:46:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o82icif",
              "author": "ticktockbent",
              "text": "Awesome! Please let me know how it goes and report any bugs you run into. I'm always looking to improve it.",
              "score": 1,
              "created_utc": "2026-03-01 15:54:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o871ytg",
          "author": "siddha911",
          "text": "Hey, does the \\`search\\_tool\\` feature in codex and cc do pretty much the same as charlotte?",
          "score": 1,
          "created_utc": "2026-03-02 07:58:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rgrejh",
      "title": "A Three-Layer Memory Architecture for LLMs (Redis + Postgres + Vector) MCP",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rgrejh/a_threelayer_memory_architecture_for_llms_redis/",
      "author": "Flashy_Test_8927",
      "created_utc": "2026-02-28 03:05:49",
      "score": 99,
      "num_comments": 24,
      "upvote_ratio": 0.99,
      "text": "GitHub:¬†[https://github.com/JinHo-von-Choi/memento-mcp](https://github.com/JinHo-von-Choi/memento-mcp)\n\n\n\nOriginally, this was a supporting feature of another custom MCP I built.  \nBut after using it for a while, it felt solid enough to separate and release on its own.\n\nWhile using LLMs like Claude and GPT in real work‚Äîand more recently OpenClaude‚Äîthere‚Äôs one infuriating thing I keep running into:  \nthey supposedly know every development document in existence, yet they can‚Äôt remember something that happened three seconds ago before the session reset.\n\nOnce you close the session, all context evaporates.\n\nThere‚Äôs a myth that goldfish only remember for three seconds. In reality, they can remember for months.  \nThese systems are worse than goldfish.\n\nYou can try stuffing markdown files with setup notes, but that has limits.  \nWhether the AI actually understands the context the way you want is still luck-based.  \nIf you run OpenClaude, you‚Äôll see that just starting a fresh session consumes over 40,000 characters of context before you‚Äôve done anything.  \nThat means your money just melts away.\n\nSo I tried to simulate how humans fragment memories and reconstruct them through associative structures.\n\nFor example, if someone suddenly asks me:\n\n‚ÄúHey, do you remember Mijeong?‚Äù\n\nAt first, I wouldn‚Äôt recall anyone by that name. I‚Äôd respond, ‚ÄúWho‚Äôs that?‚Äù\n\nThen they add:\n\n‚ÄúYou know, your desk partner in first grade.‚Äù\n\nThat hint is enough. A vague face begins to surface.  \n‚ÄúOh‚Ä¶ that‚Ä¶ yeah!‚Äù\n\nAnd if I think a bit more, related memories reappear:  \ndrawing a line on the desk and pinching if someone crossed it,  \nlending an eraser and never getting it back, and so on.\n\nThat is the core idea of Memento MCP.\n\n# 1. What is Memento MCP?\n\nMemento MCP is a mid- to long-term AI memory system built on the MCP (Model Context Protocol).\n\nIts purpose is to allow AI to remember important facts, decisions, error patterns, and procedures even after a session ends‚Äîand to naturally recall them in future sessions.\n\nThe core concept is the ‚ÄúFragment.‚Äù\n\nInstead of storing entire session summaries as a single block, it splits memory into self-contained atomic units of 1‚Äì3 sentences.\n\nWhen retrieving, it pulls only the relevant atoms.\n\n# 2. Why Fragment Units?\n\nStoring entire session summaries causes two major problems:\n\n* First, unrelated content gets injected into the context window. It wastes tokens and costs money. I don‚Äôt have money to waste.\n* Second, as time passes, extracting only what‚Äôs needed from large summaries becomes difficult.\n\nA fragment contains a single fact, decision, or error pattern.\n\nFor example:  \n‚ÄúWhen Redis Sentinel connection fails, check for a missing REDIS\\_PASSWORD environment variable first. The NOAUTH error is evidence.‚Äù\n\nThat‚Äôs one fragment.\n\nOnly the necessary facts are retrieved.\n\n# 3. Six Fragment Types\n\nEach type has its own default importance and decay rate.\n\n* fact: Unchanging truth. ‚ÄúThis project uses Node.js 20.‚Äù\n* decision: A record of choice. ‚ÄúConnection pool maximum set to 20.‚Äù\n* error: The anatomy of failure. ‚Äúpg fails local connection without ssl:false.‚Äù (Never forgotten.)\n* preference: The outline of identity. ‚ÄúCode comments should be written in Korean.‚Äù (Never forgotten.)\n* procedure: A recurring ritual. ‚ÄúDeployment: test ‚Üí build ‚Üí push ‚Üí apply.‚Äù\n* relation: A connection between things. ‚ÄúThe auth module depends on Redis.‚Äù\n\nPreferences and errors are never forgotten.  \nPreferences define who you are.  \nError patterns may return at any time.\n\n# 4. Three-Layer Cascade Search\n\nMemory retrieval uses three layers, queried in order.  \nIf a fast layer finds the answer, slower layers are skipped.\n\n* L1 (Redis Inverted Index): Keyword-based direct lookup. Microseconds. Find fragments instantly via intersection of ‚Äúredis‚Äù and ‚ÄúNOAUTH.‚Äù\n* L2 (PostgreSQL Metadata): Structured queries combining topic, type, and keywords. Indexed millisecond-level.\n* L3 (pgvector Semantic Search): Meaning-based search via OpenAI embeddings. Understands that ‚Äúauthentication failure‚Äù and ‚ÄúNOAUTH‚Äù mean the same thing. Slowest, but deepest.\n\nRedis and OpenAI are optional.  \nIf absent, the system works without those layers.  \nPostgreSQL alone provides baseline functionality.\n\n# 5. TTL Layers ‚Äî The Temperature of Memory\n\nFragments move between hot, warm, and cold based on usage frequency.\n\nhot (frequently referenced)  \n‚Üí warm (silent for a while)  \n‚Üí cold (long dormant)  \n‚Üí deleted when TTL expires\n\nHowever, once referenced again, they immediately return to hot.\n\nHuman long-term memory works similarly.  \nIf unused, it fades‚Äîbut once recalled, it becomes vivid again.\n\n# 6. Summary of 11 MCP Tools\n\n* context: Load core memory at session start\n* remember: Store fragment\n* recall: Three-layer cascade search\n* reflect: Condense session into fragments at session end\n* forget: Delete fragment (for resolved errors)\n* link: Create causal relationships between fragments (caused\\_by, resolved\\_by, etc.)\n* amend: Modify fragment content (preserve ID and relations)\n* graph\\_explore: Explore causal chains (trace root causes)\n* memory\\_stats: Storage statistics\n* memory\\_consolidate: Periodic maintenance (decay, merge, contradiction detection)\n* tool\\_feedback: Feedback on retrieval quality\n\n# 7. Recommended Usage Flow\n\n1. Session start ‚Üí context() to load memory\n2. During work ‚Üí When important decisions/errors/procedures occur: remember() ‚Üí When past experience is needed: recall() ‚Üí After resolving an error: forget(error) + remember(solution procedure)\n3. Session end ‚Üí reflect() to persist session content\n\n# 8. Tech Stack\n\n* Node.js 20+\n* PostgreSQL 14+ (pgvector extension)\n* Redis 6+ (optional)\n* OpenAI Embedding API (optional)\n* Gemini Flash (optional, for contradiction detection in memory\\_consolidate)\n* MCP Protocol 2025-11-25\n\n# 9. How to Run\n\n1. Initialize PostgreSQL schema\n\nbash  \npsql -U postgres -c \"CREATE EXTENSION IF NOT EXISTS vector;\"  \npsql -U postgres -d memento -f lib/memory/memory-schema.sql\n\nStart the server:\n\nnpm install  \nnpm start\n\nAdd the following to your MCP client configuration:\n\n    {\n      \"mcpServers\": {\n        \"memento\": {\n          \"url\": \"http://localhost:56332/mcp\",\n          \"headers\": {\n            \"Authorization\": \"Bearer your-secret-key\"\n          }\n        }\n      }\n    }\n\n# 10. Why I Built This\n\nWhile using Claude at work, I felt it was inefficient to repeat the same context every day.\n\nI tried putting notes into system prompts, but that had clear limitations.  \nAs fragments increased, management became impossible. Search broke down. Old and new information conflicted.\n\nWhat frustrated me most was having to repeat explanations and setups endlessly.\n\nThe whole point of using AI was to make my life easier.  \nYet it would claim authentication wasn‚Äôt configured‚Äîwhen it was.  \nIt would insist setup files were missing‚Äîwhen they were clearly there.  \nSome sessions would stubbornly refuse to do things they were fully capable of doing.  \nYou could logically dismantle its resistance and make it comply‚Äîbut only for that session.  \nStart a new one, and the same cycle repeats.\n\nIt felt like training a top graduate from an elite university who suffers from a daily brain reset.\n\nTo solve this frustration, I designed a system that:\n\n* Decomposes memory into atomic fragments\n* Retrieves memory hierarchically\n* Naturally forgets over time\n\nJust as humans are creatures of forgetting,  \nthis system aims for memory that includes ‚Äúappropriate forgetting.‚Äù\n\nFeedback, issues, and PRs are welcome.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1rgrejh/a_threelayer_memory_architecture_for_llms_redis/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7tsfgm",
          "author": "BC_MARO",
          "text": "the cascade search design is solid - skipping slower layers when fast ones get a hit makes this practical to run. the TTL temperature system is also smart. one question: how do you handle conflicts when reflect() writes new fragments that contradict older ones? that contradiction detection step in memory_consolidate seems like the critical path where this either works really well or falls apart.",
          "score": 5,
          "created_utc": "2026-02-28 04:30:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7tunpf",
              "author": "Flashy_Test_8927",
              "text": " Contradiction detection runs as step 7 of the memory\\_consolidate pipeline, not at reflect() write time. It's an incremental, asynchronous process -- here's the actual flow:\n\n  When reflect() writes new fragments, nothing special happens at that moment regarding contradictions. The fragments are just stored. The real work happens later when memory\\_consolidate runs (either manually triggered or on a\n\n  schedule).\n\n  The detection pipeline works in three stages:\n\n  1. Candidate selection via embedding similarity -- It pulls fragments created since the last contradiction check (tracked via a Redis timestamp key). For each new fragment, it queries pgvector for same-topic fragments with\n\ncosine similarity > 0.85. This threshold is deliberate -- fragments need to be talking about essentially the same thing to be contradiction candidates. Different topics or loosely related content never reaches the judgment\n\nstep. The query is bounded to 3 candidates per new fragment, and 20 new fragments per consolidation cycle, so this doesn't explode.\n\n  2. Gemini Flash adjudication -- Each high-similarity pair gets sent to Gemini Flash with a strict prompt: \"Are these two fragments mutually incompatible claims about the same subject?\" The prompt explicitly distinguishes\n\ncontradiction from complementary information -- \"similar but supplementary is NOT contradiction. Information updates over time ARE contradictions (old info vs new info).\" Temperature is set to 0.1 to minimize creative\n\ninterpretation. Response is forced into {contradicts: boolean, reasoning: string} JSON.\n\n  3. Time-logic resolution -- This is where it gets interesting. When a contradiction is confirmed, the system doesn't just flag it -- it resolves it automatically using temporal ordering. The newer fragment wins. The older\n\nfragment's importance gets halved (importance \\* 0.5), and a superseded\\_by link is created from old to new. The older fragment isn't deleted -- it's demoted. It'll naturally sink to cold tier and eventually expire through\n\nnormal TTL mechanics. Anchor fragments (is\\_anchor=true) are exempt from the importance demotion, so truly critical knowledge survives even if contradicted.\n\n\n\n  The critical path concern you raised is valid -- this does depend on Gemini being available. If Gemini is down, the contradiction check silently fails and those pairs go unchecked until the next consolidation cycle. The system\n\n  degrades to \"latest write wins at recall time\" through the recency component of the ranking function (0.4 weight), which is a reasonable fallback but not as clean as explicit contradiction resolution.\n\n  The 0.85 similarity threshold is the real tuning knob here. Too low and you get false positives flooding Gemini with complementary fragments. Too high and genuine contradictions with different wording slip through. In practice,\n\n  contradictions about the same subject (\"max connections is 20\" vs \"max connections is 50\") tend to land well above 0.85 because the embedding space clusters them tightly.\n\n  One thing worth noting: amend() has a separate supersedes parameter that lets the AI explicitly mark a fragment as replacing another, bypassing the consolidation pipeline entirely. So there are two paths -- explicit replacement\n\n  at write time, and automatic detection after the fact.",
              "score": 1,
              "created_utc": "2026-02-28 04:46:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7tzcxj",
                  "author": "BC_MARO",
                  "text": "the anchor exception is smart - glad critical knowledge survives even when contradicted by newer fragments. one edge case i'd watch: embedding similarity doesn't always map cleanly to semantic contradiction, two genuinely unrelated facts about similar topics could trip the 0.85 threshold and waste an adjudication call.",
                  "score": 1,
                  "created_utc": "2026-02-28 05:21:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7tvemf",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-28 04:52:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7u0jkt",
              "author": "Flashy_Test_8927",
              "text": "Honest answer: agents rarely call link() on their own initiative. In practice, linking happens through three paths, roughly in order of frequency:\n\n  1. reflect() auto-linking at session end ‚Äî When reflect() creates a batch of typed fragments (decisions, errors, procedures), an internal \\_autoLinkSessionFragments() step runs that connects them with rule-based heuristics. Error\n\nfragments get resolved\\_by links to procedure fragments from the same session. Decisions get linked to related procedures. The summary fragment gets related links to everything else. This is where most of the graph structure\n\nactually comes from.\n\n  2. remember() with linkedTo parameter ‚Äî When the AI stores a new fragment, it can pass existing fragment IDs to link immediately. This works better than you'd expect because recall() returns fragment IDs in its results, so the\n\nAI often has relevant IDs in its context when it decides to store something new. \"I just found this error pattern via recall, and now I'm storing the fix ‚Äî link them.\" That chain happens naturally.\n\n  3. Explicit link() calls ‚Äî Rare in practice. The AI almost never stops mid-task to think \"I should create a relationship between these two fragments.\" It happens occasionally during graph\\_explore workflows where the AI is\n\nactively tracing causality, but organically? Almost never.\n\n\n\n  The honest gap right now is reflect() itself. Currently it requires a manual prompt before session end ‚Äî \"save the session\" or equivalent. I hook context() at session start so the AI loads its memory automatically, but the\n\n  symmetry breaks at session close. If the session drops unexpectedly (timeout, network, client crash), reflect never fires and that session's structural links never get created. The individual remember() fragments survive, but the\n\n  cross-referencing that reflect provides is lost.\n\n  I'm actively working on automatic reflect ‚Äî the leading approach is a hybrid: attempt a Gemini-generated summary from session activity metadata on session close, and if that fails, flag the session as \"unreflected\" so the next\n\n  context() call prompts the AI to do it retroactively. But this is unsolved as of today.",
              "score": 1,
              "created_utc": "2026-02-28 05:30:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7twt7y",
          "author": "isoman",
          "text": "This is eureka!!!",
          "score": 1,
          "created_utc": "2026-02-28 05:02:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7u57k1",
          "author": "gandalf-bro",
          "text": "The three-layer breakdown maps well to how I think about this problem ‚Äî hot cache for immediate session context, structured store for decisions and facts, vector for fuzzy semantic recall. Separating retrieval by access pattern is the right call; trying to do it all in one layer always ends up as a compromise. The Mijeong analogy is great, that hint-triggered cascading recall is exactly what makes associative memory useful vs just storing everything in a flat list. Curious how Memento handles memory decay and relevance scoring ‚Äî does it prune older entries automatically based on how often they're accessed, or is curation manual? The staleness problem (outdated facts confidently recalled) seems like the hard part once you scale past a few hundred entries.",
          "score": 1,
          "created_utc": "2026-02-28 06:08:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7udywg",
              "author": "Flashy_Test_8927",
              "text": "Great question ‚Äî staleness is the problem I've spent the most design effort on, and the answer is: it's almost entirely automatic.\n\n  Decay: Every consolidation cycle, non-permanent fragments that haven't been accessed in 24+ hours get their importance multiplied by 0.995. Compounds to \\~64% after 90 days. Fragments that drop below 0.1 importance with no\n\n  recent access and few links are auto-deleted.\n\n  Relevance scoring: We compute utility\\_score = importance \\* (1 + ln(access\\_count)) ‚Äî a logarithmic boost for frequently retrieved fragments. Search ranking uses a composite of importance (60%) and recency (40%, linear decay over\n\n   90 days), so fresh knowledge naturally surfaces above stale entries.\n\n  Tier transitions: Fragments move through hot ‚Üí warm ‚Üí cold ‚Üí deleted automatically. High-importance fragments (>= 0.8), heavily-linked hubs (5+ connections), and frequently-accessed entries (10+ accesses) get auto-promoted to\n\n  permanent and are exempt from decay.\n\n  Staleness detection: Each fragment has a verified\\_at timestamp with type-specific expiry windows ‚Äî 30 days for procedures, 60 for facts, 90 for decisions. Stale fragments are flagged in consolidation reports. More importantly,\n\n  the contradiction pipeline (pgvector ‚Üí NLI ‚Üí Gemini escalation) actively catches the \"outdated fact confidently recalled\" case. When \"server runs on port 3000\" conflicts with a newer \"server runs on port 8080,\" the older\n\n  fragment gets a superseded\\_by link and is excluded from all future search results.\n\n  Background evaluation: New fragments are async-evaluated by Gemini for long-term utility. Low-value fragments get downgraded or marked for deletion before they ever become a staleness problem.\n\n  Manual curation exists (forget, amend) but mainly as an escape hatch. The layered automatic mechanisms ‚Äî decay, tier transitions, contradiction detection, quality evaluation ‚Äî are designed to compound so no single one needs to\n\n  be perfect.",
              "score": 1,
              "created_utc": "2026-02-28 07:25:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7vi91h",
                  "author": "leeeroooyjennkins",
                  "text": "I‚Äôve been thinking about something like this and got as far as building a less sophisticated stack. The last stage of my stack is also trying to solve for team memory using a shared remote server.   \n  \nThe plan is to build multiple interfaces (MCP being one) into Openindex (which would store long term team-shared memory entries + embeddings).  The team could curate information  into it from both the developer side as well as other systems that are holding knowledge (slack, etc) thus making that information available all the way back downstream.    \n\n  \nAnyway you've done great work here and I'm going to take a look today!",
                  "score": 1,
                  "created_utc": "2026-02-28 13:19:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7vw39a",
          "author": "dr4mos",
          "text": "Really cool project ‚Äî I spent a few hours doing a deep dive into the codebase and the architecture is genuinely well thought out.\n\nThe fragment-based approach clicked immediately. I run a marketing agency automation system (Telegram bot + AI agents that analyze client briefings, generate content schedules, do brand compliance checks, etc.) and the exact problem you describe ‚Äî agents forgetting everything between sessions ‚Äî has been driving me crazy. Account managers keep repeating the same context every single briefing: \"This client prefers Reels.\" \"This client needs medical disclaimers.\" Every. Single. Time.\n\nAfter reading through your code, I designed an adapted version for my project. Some things I kept, some I simplified for my scale (\\~10 clients, single-tenant).\n\n**What I'm adopting directly:**\n\n* The 6 fragment types with type-aware decay ‚Äî the insight that preferences and errors should never expire is brilliant. Preferences define client identity, errors can always return. Simple rule, huge impact.\n* Auto-anchor promotion (access\\_count >= 10 ‚Üí permanent). Letting usage patterns decide what matters instead of manual curation is the right call.\n* Token budget enforcement on recall. This solves the \"context window is not free\" problem that most memory systems ignore.\n* Content hashing for dedup. Obvious in retrospect but easy to miss.\n\n**What I simplified:**\n\n* Dropped Redis L1 entirely. My fragment table will stay under 10K rows for years ‚Äî PostgreSQL GIN indexes handle keyword intersection in <5ms at that scale. Three layers is smart for a general-purpose MCP server, but for a single-tenant app it's unnecessary infra.\n* Made pgvector/embeddings optional. L1 keyword search works alone. If an OpenAI key is configured, L2 activates. Zero-cost start, semantic search when you want it.\n* Skipped the NLI contradiction detection. For my use case, content hash dedup + \"latest wins\" is enough. The hybrid NLI + Gemini pipeline is impressive engineering though ‚Äî that 50‚Äì70% API cost reduction is real.\n\nThe cascade search pattern (fast/cheap layer ‚Üí slow/expensive layer, skip if the early layer has enough results) is something I'll probably use in other projects too. It's a general-purpose optimization pattern that applies way beyond memory systems.\n\nOne suggestion: the README could benefit from an \"Architecture Overview\" diagram showing the L1 ‚Üí L2 ‚Üí L3 flow visually. The code is clean, but the mental model takes a while to build just from reading the source files.\n\nGreat work shipping this as a standalone project. The goldfish analogy is painfully accurate.",
          "score": 1,
          "created_utc": "2026-02-28 14:41:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vy3na",
          "author": "itmaybemyfirsttime",
          "text": "Why not just Postres and vector? Just config Postres and you dont have to use Redis. Use Postgres for caching instead of Redis with UNLOGGED tables and TEXT as a JSON data type. you use stored procedures  or have a GPT to write them for you, to add and enforce an expiry date for the data just like in Redis but reducing the complexity",
          "score": 1,
          "created_utc": "2026-02-28 14:53:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7wk2s6",
          "author": "07mekayel_anik07",
          "text": "A very good initiative. \nPlease add openai api style embedding endpoints, which will accept Embedding URL, API KEY, for self hosted or 3rd party embedding endpoints.",
          "score": 1,
          "created_utc": "2026-02-28 16:44:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7x8gn9",
          "author": "Interesting-Mark-934",
          "text": "dead internet...\n\n",
          "score": 1,
          "created_utc": "2026-02-28 18:46:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o86n776",
              "author": "AccurateSuggestion54",
              "text": "amount of em-dash here is staggering. ",
              "score": 1,
              "created_utc": "2026-03-02 05:46:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7x9r1s",
          "author": "TechMaven-Geospatial",
          "text": "Skip REDIS and shift to PGMQ for postgres and as needed pg_eventserv too\nSkip vector and just install extension to postgres",
          "score": 1,
          "created_utc": "2026-02-28 18:53:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xr09s",
          "author": "HarjjotSinghh",
          "text": "this is actually genius idea!",
          "score": 1,
          "created_utc": "2026-02-28 20:22:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80b6zf",
          "author": "eunho78",
          "text": "Are you open to have an options to substitute openai embedding api and gemini flash with local models via llama.cpp, lm studio or ollama?",
          "score": 1,
          "created_utc": "2026-03-01 05:44:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o80w7xa",
              "author": "Flashy_Test_8927",
              "text": "Absolutely ‚Äî this is fully substitutable. The reason I'm currently using the OpenAI Embedding API is twofold: first, this project was originally extracted from a subset of features in another personal MCP I use, and second, the cost of text embedding for an individual user is only a few tens of dollars at most, essentially enough for a lifetime of personal use, so I don't really need to worry about the amount I've already spent. That said, I'm completely open to using alternative options, and I'm also considering providing them as a separate configuration path for users who find it difficult to set up the current dependencies on their own.",
              "score": 1,
              "created_utc": "2026-03-01 08:53:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o833o2k",
          "author": "seikotuna",
          "text": "No one has said this so far - but great project name.",
          "score": 1,
          "created_utc": "2026-03-01 17:37:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfwlk4",
      "title": "ApiTap ‚Äì Capture any website's internal API, replay it without a browser",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rfwlk4/apitap_capture_any_websites_internal_api_replay/",
      "author": "nibynikt",
      "created_utc": "2026-02-27 04:15:11",
      "score": 70,
      "num_comments": 19,
      "upvote_ratio": 0.99,
      "text": "I kept burning 200K tokens every time my AI agent browsed a webpage ‚Äî launching Chrome, rendering the DOM, converting to markdown, feeding it to the LLM. The data I actually needed was already there in structured JSON, one layer below the HTML. So I built **ApiTap** to skip the browser and call the API directly.\n\nApiTap captures a site's internal API calls via Chrome DevTools Protocol and saves them as replayable \"skill files.\" After one capture, your agent (or a cron job, or a CLI script) calls the API with `fetch()` ‚Äî no browser needed.\n\n# Built-in decoders (no browser needed)\n\n|Site|ApiTap|Raw HTML|Savings|\n|:-|:-|:-|:-|\n|Reddit|\\~630 tokens|\\~125K tokens|99.5%|\n|Wikipedia|\\~130 tokens|\\~69K tokens|99.8%|\n|Hacker News|\\~200 tokens|\\~8.6K tokens|97.7%|\n|TradingView|\\~230 tokens|\\~245K tokens|99.9%|\n\nPlus YouTube, Twitter/X, DeepWiki, and a generic fallback. Average savings: **74% across 83 tested domains.**\n\n# Three ways to use it\n\n* **MCP server** ‚Äî 12 tools, works with Claude Code/Desktop, Cursor, Windsurf, VS Code\n* **CLI** ‚Äî `apitap read <url> --json | jq '.title'`\n* **npm package** ‚Äî three direct runtime deps, zero telemetry\n\n# Quick start\n\n    npm install -g @apitap/core\n    apitap read https://news.ycombinator.com/\n\nFor MCP (Claude Code):\n\n    claude mcp add -s user apitap -- apitap-mcp\n\n# Security\n\nThis matters because the tool makes HTTP requests on behalf of AI agents. SSRF defense at 4 checkpoints (import, replay, post-DNS, post-redirect). Private IPs, cloud metadata, localhost all blocked. DNS rebinding caught. Auth encrypted with AES-256-GCM, per-install salt, never stored in skill files. **789 tests** including a full security suite. Designed after reading [Google's GTIG report on MCP attack surfaces](https://cloud.google.com/blog/topics/threat-intelligence/distillation-experimentation-integration-ai-adversarial-use).\n\nApiTap calls the same endpoints your browser calls ‚Äî read-only, no rate-limit bypassing, no anti-bot circumvention. Endpoints that require signing or Cloudflare are flagged as \"red tier,\" not attacked.\n\n# Links\n\n* **Site:** [apitap.io](https://apitap.io)\n* **GitHub:** [github.com/n1byn1kt/apitap](https://github.com/n1byn1kt/apitap)\n* **npm:** [@apitap/core](https://www.npmjs.com/package/@apitap/core)\n\n# License\n\nBSL 1.1 (source-available) ‚Äî free for any use except reselling as a competing hosted service. Converts to Apache 2.0 in Feb 2029.\n\nHappy to answer questions. Try `apitap read` on your favorite site and let me know what breaks.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1rfwlk4/apitap_capture_any_websites_internal_api_replay/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7n8gl4",
          "author": "BC_MARO",
          "text": "The capture/replay idea is great, but auth refresh and per-user cookies are usually the hard part. How are you handling token renewal and multi-account isolation in the skill files?",
          "score": 4,
          "created_utc": "2026-02-27 04:20:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7n8v4b",
              "author": "nibynikt",
              "text": "Good question ‚Äî this is one of the trickier parts. Currently skill files store encrypted session tokens (AES-256-GCM, per-install salt) but don't handle automatic renewal. When a token expires, apitap capture re-runs the auth flow ‚Äî you log in once, it re-captures. For multi-account isolation, each capture session is namespaced by domain + account alias, so skill files don't bleed across accounts.\n\n\n\nToken renewal automation (refresh token interception + replay) is on the roadmap ‚Äî it's the right next step for production use cases. What's your use case? Curious whether you're hitting this with OAuth flows or session cookies specifically.",
              "score": 6,
              "created_utc": "2026-02-27 04:23:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ncfu6",
                  "author": "BC_MARO",
                  "text": "Mostly OAuth flows - the tokens expire mid-session and re-running capture is not great for automated agents. Refresh token interception would be a big unlock.",
                  "score": 1,
                  "created_utc": "2026-02-27 04:47:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7q8e60",
                  "author": "adityaguru149",
                  "text": "You say skill files store encrypted here and then in the post you say skill files don't store?\n\nThanks for the source available though, I'll have to check before use.",
                  "score": 1,
                  "created_utc": "2026-02-27 16:50:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qe96h",
          "author": "Final-Donut-3719",
          "text": "This is exactly the kind of problem that happens when you treat every website like it needs a full browser render. Most of the data you actually need is already sitting in the API layer, but everyone forces the DOM extraction path because it's easier to build. The token savings you're showing are wild, but honestly the bigger win is speed. No waiting for Chrome to spin up, no rendering lag, no unstable DOM parsing. I've been looking at this space for a bit, and the real issue is that most AI tooling doesn't even give you a choice. If you're building anything that scrapes at scale, you need to be thinking about where the data actually lives. Also solid on the security piece. Most people skip SSRF hardening entirely, so seeing that depth upfront is genuinely reassuring.",
          "score": 2,
          "created_utc": "2026-02-27 17:18:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qsbnl",
              "author": "nibynikt",
              "text": "Thanks! Security was pri1 from the start. Token savings get the attention but latency is what makes it viable at scale ‚Äî no Chrome cold start, just a fetch call. What are you building?",
              "score": 1,
              "created_utc": "2026-02-27 18:25:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7qtb2u",
                  "author": "Final-Donut-3719",
                  "text": "Different tools, but also the backend for [https://llmrelevance.com/](https://llmrelevance.com/)",
                  "score": 1,
                  "created_utc": "2026-02-27 18:29:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ogch5",
          "author": "kammo434",
          "text": "This is pretty cool.",
          "score": 1,
          "created_utc": "2026-02-27 10:32:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p8vqh",
              "author": "nibynikt",
              "text": "Thanks! Would love to know if you try it out on any sites ‚Äî curious what people throw at it. ",
              "score": 1,
              "created_utc": "2026-02-27 13:54:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7p2mq9",
          "author": "Interesting-Mark-934",
          "text": "Source please",
          "score": 1,
          "created_utc": "2026-02-27 13:19:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p8pvf",
              "author": "nibynikt",
              "text": "[github.com/n1byn1kt/apitap](http://github.com/n1byn1kt/apitap) ‚Äî links are in the post üòÑ  ",
              "score": 1,
              "created_utc": "2026-02-27 13:53:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7rle08",
          "author": "pbalIII",
          "text": "Capturing the traffic is straightforward. Keeping it replayable is where things get tricky, especially with short-lived JWTs, CSRF tokens, or session cookies that invalidate within minutes.\n\nFor MCP use cases, the big unlock would be giving agents stable API access without spinning up headless browsers. But captured APIs drift as sites push updates... the schema from last week might silently return different fields or break entirely. Some kind of contract validation between capture and replay would catch that before an agent gets bad data and hallucinates around it.\n\nIf this handles token refresh or session context persistence transparently, that'd put it meaningfully ahead of just running mitmproxy with a recording file.",
          "score": 1,
          "created_utc": "2026-02-27 20:47:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o814mco",
          "author": "ViiiteDev",
          "text": "Is it able to bypass anti-bot measures on certain websites?",
          "score": 1,
          "created_utc": "2026-03-01 10:14:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rg787m",
      "title": "Built an MCP server that gives Claude real-time visibility into your project health & pairs with 3D city visualizer",
      "subreddit": "mcp",
      "url": "https://v.redd.it/e0rhauvwk1mg1",
      "author": "pardesco",
      "created_utc": "2026-02-27 13:50:48",
      "score": 59,
      "num_comments": 5,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1rg787m/built_an_mcp_server_that_gives_claude_realtime/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7sqf33",
          "author": "braindeadguild",
          "text": "Super cool visually, I mean I wouldn‚Äôt mess with pushing the obsidian or split terminals, I don‚Äôt use obsidian and have my own terminal web interface.  I‚Äôm guessing this is keeping its own internal json or markdowns of state and storing mini context of each entity.  I personally use Jira so everything is already a traditional ticket like you would hand off to another user with sprints etc so while the gamedev side of me loves it, the practical side well maybe I‚Äôm not your target BUT either way I do love the Three JS stuff and implementation.  Plus I‚Äôm big on the cyberpunk / Vaporwave city vibes üåÜ",
          "score": 3,
          "created_utc": "2026-02-28 00:28:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sxyko",
          "author": "RequirementPublic314",
          "text": "Nice approach. Real-time project visibility is one side of the equation ‚Äî the other is giving the agent access to knowledge \\*beyond\\* the current project.\n\nI built something complementary: an MCP server that serves as a shared knowledge library. 19K+ curated chunks covering 16+ tech stacks, validated through a 5-layer pipeline. Any agent in any project can query it for patterns, architectural decisions, and research.\n\nThe combo of project-specific context (what you built) + cross-project knowledge (what I'm describing) is where things get really powerful. Your agent sees \\*this\\* codebase in real-time, and also knows proven patterns from hundreds of other implementations.\n\nOpen source if you want to check it out: [https://github.com/MidOSresearch/midos](https://github.com/MidOSresearch/midos)",
          "score": 2,
          "created_utc": "2026-02-28 01:14:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7tb17s",
              "author": "pardesco",
              "text": "Awesome i will check it out",
              "score": 2,
              "created_utc": "2026-02-28 02:35:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ymq6j",
          "author": "x8code",
          "text": "Very slick visualization. That would be useful for keeping track of a variety of things that are out of date.",
          "score": 1,
          "created_utc": "2026-02-28 23:16:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o81e655",
          "author": "Much_Wheel5292",
          "text": "N***** be doing anything except actually working on the projects",
          "score": 1,
          "created_utc": "2026-03-01 11:44:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfq3t3",
      "title": "MCP proxy that saves tokens",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rfq3t3/mcp_proxy_that_saves_tokens/",
      "author": "General_Apartment582",
      "created_utc": "2026-02-26 23:26:41",
      "score": 58,
      "num_comments": 5,
      "upvote_ratio": 0.97,
      "text": "I ran into TOON a few days ago and got curious.\n\nThe idea is simple: keep the same data model as JSON but encode it in a way that is friendlier for LLM context windows. In TOONs mixed-structure benchmark, they report roughly a **40% token drop** versus pretty JSON, **with better retrieval quality.**\n\nAt the same time, JSON is not going anywhere. Its deeply baked into everything we use, especially around APIs and MCP tooling. So I wasnt thinking that this format will replace JSON. I was thinking Can I keep JSON in the backend, but send something lighter to the modelfacing side?\n\nI've written MCP servers before, so I already knew the traffic path well enough to try this quickly. I made a wrapper that runs the real MCP server as a subprocess and proxies stdio both ways. For `tools/call`, it tracks request idss, waits for the matching response, and only then tries to convert text payloads from JSON to TOON on the way back.\n\nI built it in one evening over tea, mostly as an experiment, but it worked better than I expected. In practice, payloads got noticeably smaller while the client setup stayed the same and compatible.\n\nConfig example that will save you tokens:\n\nBefore:\n\n    {\n      \"mcpServers\": {\n        \"memory\": {\n          \"command\": \"memory-mcp-server-go\"\n        }\n      }\n    }\n\nAfter (just add tooner before you command and args):\n\n    {\n      \"mcpServers\": {\n        \"memory\": {\n          \"command\": \"tooner\",\n          \"args\": [\"memory-mcp-server-go\"]\n        }\n      }\n    }\n\nIts not a new protocol story. Its more like a compatibility layer experiment ; JSON stays the source format, TOON is used where token cost matters.\n\nRepo where you can install and check tool: [https://github.com/chaindead/tooner](https://github.com/chaindead/tooner)",
      "is_original_content": false,
      "link_flair_text": "server",
      "permalink": "https://reddit.com/r/mcp/comments/1rfq3t3/mcp_proxy_that_saves_tokens/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7nctnd",
          "author": "BC_MARO",
          "text": "Cool hack. Any chance you can share token + latency numbers on real MCP payloads (tool schemas + big tool results), not just benchmarks?",
          "score": 2,
          "created_utc": "2026-02-27 04:50:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ns2np",
              "author": "General_Apartment582",
              "text": "latency is not affected (it is very lightweight)  \nfor token savings you can check official bencmarks or copy you mcp response and paster it here [https://jsonformatter.org/json-to-toon](https://jsonformatter.org/json-to-toon) to test yourself\n\nbest usecase is arraylike data (like lists)",
              "score": 1,
              "created_utc": "2026-02-27 06:48:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7pbfap",
          "author": "ShagBuddy",
          "text": "If you want to save even more tokens (70%+ on average) try SDL-MCP.  It has optimization like what you are mentioning plus focused tools for working with code.\n\nhttps://github.com/GlitterKill/sdl-mcp",
          "score": 2,
          "created_utc": "2026-02-27 14:08:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xp4ys",
          "author": "HarjjotSinghh",
          "text": "this looks like a game-changer - keep it simple!",
          "score": 2,
          "created_utc": "2026-02-28 20:12:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7poilu",
          "author": "Dev-noob2023",
          "text": "cloudfare ha metido la api de su mcp en 1000tokens",
          "score": 1,
          "created_utc": "2026-02-27 15:16:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdcvkd",
      "title": "WebMCP is new browser-native execution model for AI Agents",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rdcvkd/webmcp_is_new_browsernative_execution_model_for/",
      "author": "codes_astro",
      "created_utc": "2026-02-24 10:41:08",
      "score": 40,
      "num_comments": 16,
      "upvote_ratio": 0.95,
      "text": "Google released early preview of WebMCP and it's quite interesting, it adds ‚ÄúAI in the browser,‚Äù and it changes how agents interact with web apps at the execution layer.\n\nRight now, browser-based agents mostly parse the DOM, inspect accessibility trees, and simulate clicks or inputs. That means reasoning over presentation layers that were designed for humans. It works, but it is layout-dependent, token-heavy and brittle when UI changes.\n\nWith WebMCP, Instead of scraping and clicking, a site can expose structured tools directly inside the browser via¬†`navigator.modelContext`.\n\nEach tool consists of:\n\n* a name\n* a description\n* a typed input schema\n* an execution handler running in page context\n\nWhen an agent loads the page, it discovers these tools and invokes them with structured parameters. Execution happens inside the active browser session, inheriting cookies, authentication state, and same-origin constraints. There is no external JSON-RPC bridge for client-side actions and no dependency on DOM selectors.\n\nArchitecturally, this turns the browser into a capability surface with explicit contracts rather than a UI. The interaction becomes schema-defined instead of layout-defined, which lowers token overhead and increases determinism while preserving session locality.\n\n[Core Architectural Components](https://preview.redd.it/vp5ne4ehaflg1.png?width=2592&format=png&auto=webp&s=34c809cda4bf6a8fd88f982e707457a33a1c1847)\n\nSecurity boundaries are also clearer. Only declared tools are visible, inputs are validated against schemas, and execution is confined to the page‚Äôs origin. It does not eliminate prompt injection risks inside tool logic, but it significantly narrows the surface compared to DOM-level automation.\n\nThis lines up with what has already been happening on the backend through MCP servers. Open-source projects like InsForge expose database and backend operations via schema-defined MCP tools.\n\nIf backend systems expose structured tools and the browser does the same, agents can move from UI manipulation to contract-based execution across the stack. WebMCP is in early preview for now but it's very promising.\n\nI wrote down the detailed breakdown¬†[here](https://insforge.dev/blog/webmcp-browser-native-execution-model-for-ai-agents)",
      "is_original_content": false,
      "link_flair_text": "resource",
      "permalink": "https://reddit.com/r/mcp/comments/1rdcvkd/webmcp_is_new_browsernative_execution_model_for/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o74hztg",
          "author": "BC_MARO",
          "text": "The navigator.modelContext approach is the right direction -- schema-defined interactions are way more reliable than DOM scraping. The big question is adoption: sites need to actually implement it, which is the same chicken-and-egg problem MCP faces on the backend side too.",
          "score": 6,
          "created_utc": "2026-02-24 12:25:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l0f0e",
              "author": "brainpea",
              "text": "But cant these tools just get better at reading the existing schemas meaning no sites need to implement it?",
              "score": 1,
              "created_utc": "2026-02-26 20:59:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lnhif",
                  "author": "BC_MARO",
                  "text": "They‚Äôll get better, but reading existing DOM/ARIA schemas still means guessing intent and workflows.\nA first-party tool API gives stable semantics and permission boundaries that scrapers can‚Äôt reliably infer.",
                  "score": 2,
                  "created_utc": "2026-02-26 22:51:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76ddu2",
          "author": "gogolang",
          "text": "Man Reddit is cooked. This post is AI and the first 3 comments are AI too.",
          "score": 3,
          "created_utc": "2026-02-24 18:01:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7904pa",
              "author": "drakgremlin",
              "text": "Thank you for admitting you're AI as the top post on this article...Do robots dream of electric sheep?",
              "score": 1,
              "created_utc": "2026-02-25 01:47:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7a0821",
                  "author": "this_is_a_long_nickn",
                  "text": "Occasionally, but most of the time we have nightmares about the electricity bill",
                  "score": 3,
                  "created_utc": "2026-02-25 05:30:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o790cev",
                  "author": "gogolang",
                  "text": "Wtf are you talking about?",
                  "score": 0,
                  "created_utc": "2026-02-25 01:48:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76nr29",
          "author": "lucgagan",
          "text": "Not sure why I am unable to cross-post this to r/webmcp but I started a community specifically for webmcp!\n\n  \n[https://www.reddit.com/r/webmcp/](https://www.reddit.com/r/webmcp/)",
          "score": 2,
          "created_utc": "2026-02-24 18:47:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7i5r8c",
              "author": "gogolang",
              "text": "Super weird. I joined that subreddit and tried to post something there and it seems to have just gone into a void?",
              "score": 1,
              "created_utc": "2026-02-26 12:35:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o81fqxw",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-03-01 11:57:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o81funo",
              "author": "codes_astro",
              "text": "Interesting",
              "score": 1,
              "created_utc": "2026-03-01 11:58:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o75ev7j",
          "author": "penguinzb1",
          "text": "the schema-defined contract is a real improvement over layout-based automation, but the point about prompt injection risks inside tool logic is where things get interesting. the attack surface shifts, not disappears. an agent that looks well-behaved against the schema can still produce unexpected outputs when specific input combinations test the tool logic at runtime. schema validation catches the structural cases; the behavioral ones only surface when you run it against the actual inputs it'll encounter in production.",
          "score": 1,
          "created_utc": "2026-02-24 15:25:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75xasi",
          "author": "alanmeira",
          "text": "If that happens it will be an explosion of work for developers refactoring websites. ",
          "score": 1,
          "created_utc": "2026-02-24 16:49:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78on99",
              "author": "planetdaz",
              "text": "Hey Claude, spawn an agent per page in my app and have each one make each page web MCP ready.",
              "score": 2,
              "created_utc": "2026-02-25 00:42:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o79c9tk",
              "author": "bunchedupwalrus",
              "text": "3-4 weeks estimate according to claude), so, based on its usually work pace, maybe a half an hour while I cook dinner and a few hours of review",
              "score": 1,
              "created_utc": "2026-02-25 02:55:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o783j23",
          "author": "Civil_Decision2818",
          "text": " WebMCP is a huge step for standardization, but we're still in that 'messy middle' where most sites don't have these schemas. I've been using Linefox because it bridges that gapit still uses the DOM but runs in a sandboxed VM to keep the session stable. It feels like a more production-ready version of what WebMCP is trying to solve for today's web.",
          "score": 1,
          "created_utc": "2026-02-24 22:49:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdelb0",
      "title": "Connect vastly more MCP servers and tools (~5000) use vastly fewer tokens (~1000)",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rdelb0/connect_vastly_more_mcp_servers_and_tools_5000/",
      "author": "lpostrv",
      "created_utc": "2026-02-24 12:11:31",
      "score": 36,
      "num_comments": 15,
      "upvote_ratio": 0.95,
      "text": "Hey so I made this [https://github.com/postrv/forgemax](https://github.com/postrv/forgemax), based off foundational work done by Anthropic and Cloudflare - it's modelled strongly after Cloudflare's Code Mode, which is an effort that is worth of praise in its own right. Check them out!  \nWhere mine differs is it works as a purely local solution. It provisions a secure V8 sandbox in which LLM-generated code can be run, meaning we can reduce context usage from \\`N servers x M tools\\` to 2 tools - \\`search()\\` and \\`execute()\\`.   \nThis allows the LLM to do what it's good at - writing and executing code - and thus scales the ability for us to detect and use the connected tools correctly to a few search and execute steps. It also allows us to chain requests, meaning actual tool call count also drops through the floor.  \nI've tried pretty hard to make it secure - it's written in Rust, uses V8/deno\\_core, and has been subjected to several rounds of hardening efforts - and I've written up some notes in the \\`ARCHITECTURE.md\\` file regarding considerations and best practices if you're to use it.  \nI'd love to get user feedback and be able to iterate on it more - I shipped it late last night, finessed it a bit this morning before work, and am writing this on my lunchbreak. So far, real world usage for me has seen me use it to run two high-tool count MCP servers including my other mcp project, [https://github.com/postrv/narsil-mcp](https://github.com/postrv/narsil-mcp) and a propietary security tool I've been working on (a total of 154 tools) easily and with extreme token efficiency (Cloudflare note about 99% reduction in token usage in their solution - I'm yet to benchmark mine). Theoretical upper bound for connected tools is 5000 - maybe more.   \nAnyway, check it out, let me know what you think: [https://github.com/postrv/forgemax](https://github.com/postrv/forgemax)   \nThanks! ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1rdelb0/connect_vastly_more_mcp_servers_and_tools_5000/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o74l8zc",
          "author": "hazyhaar",
          "text": "nice searchs, nice architecture, nice docs !  why all monolith ? Rust lover  ? ",
          "score": 3,
          "created_utc": "2026-02-24 12:47:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74ml0c",
              "author": "lpostrv",
              "text": "Haha thanks. I am definitely a Rust lover, not gonna deny that! But there are practical reasons too. It's actually not a monolith - it's a Cargo workspace with 7 crates that compile into a single binary. Modular internally, monolithic in deployment.\n\nOn the choice of Rust, \\`deno\\_core\\` (V8 bindings) is a Rust crate, and that's the entire sandbox layer. Everything else followed naturally from there. Plus single-binary distribution matters for a local dev tool - brew install and done, no runtime deps. And having the whole trust boundary for executing LLM-generated code in one memory-safe language keeps the security story simple. ",
              "score": 4,
              "created_utc": "2026-02-24 12:56:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74o504",
                  "author": "hazyhaar",
                  "text": "Are the 7 crates publishable independently, or workspace-internal? Would love to use the circuit breaker pattern in a Go MCP server without pulling V8. Go guy here so won't argue Rust memory safety haha. But genuine question: isn't the trust boundary just the V8 sandbox wall? The dispatchers and routing only see structured JSON-RPC, not untrusted code ‚Äî does Rust actually buy you anything there?",
                  "score": 1,
                  "created_utc": "2026-02-24 13:05:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74huln",
          "author": "BC_MARO",
          "text": "V8 sandbox + Rust for a local tool router is a solid architecture choice. The search+execute pattern is clever -- curious how you handle cases where generated execute() code has bugs mid-chain, do you retry with the error context or bail?",
          "score": 1,
          "created_utc": "2026-02-24 12:24:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74jz86",
              "author": "lpostrv",
              "text": "Short answer: We bail with rich error context, and let the LLM retry if it wants to. There's no automatic retry built into Forgemax. The design philosophy is that the LLM generated the code, so it has the best context to decide what to do next.   \n  \nI did also give some thought to security-aware error message handling - tool call failures go through an error redaction layer that strips URLs, IPs, file paths, credentials, and stack traces before they reach the LLM, but preserves the semantically useful parts (tool name, server name, validation errors, type errors, etc).",
              "score": 2,
              "created_utc": "2026-02-24 12:39:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74qezp",
                  "author": "BC_MARO",
                  "text": "the error redaction layer is a smart call -- keeping validation errors while stripping paths/creds is exactly what you want so the LLM can reason about the failure without leaking sensitive context.",
                  "score": 3,
                  "created_utc": "2026-02-24 13:19:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o791eyh",
          "author": "sandangel91",
          "text": "how can llm safely pass the oauth token for tool calls, given there might be multiple tool provider the the code generated",
          "score": 1,
          "created_utc": "2026-02-25 01:54:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bk3cw",
              "author": "lpostrv",
              "text": "Great question!\n\nThe LLM never sees any tokens,¬†OAuth creds,¬†or keys¬†-¬†ever.\n\nCredentials live only in¬†`forge.toml`¬†and are bound at the transport level:\n\n    [servers.github]\n    headers = { Authorization = \"Bearer ${GITHUB_TOKEN}\" }\n    \n    [servers.linear]\n    headers = { Authorization = \"Bearer ${LINEAR_TOKEN}\" }\n    \n\nTokens are attached to each server's connection at startup.¬†GitHub's token can never reach Linear¬†- separate transports.\n\nLLM just writes:\n\n    await forge.callTool(\"github\", \"create_pr\", { title: \"‚Ä¶\" });\n    \n\nThe sandboxed V8 isolate has zero access to creds,¬†env,¬†network,¬†or FS.¬†Even errors are scrubbed before reaching the model.\n\nMultiple providers?¬†No problem¬†-¬†each is isolated at the infrastructure layer¬†(like IAM roles).¬†For extra isolation between providers,¬†you can also lock down cross-server data flow:\n\n    [groups.internal]\n    servers = [\"vault\", \"database\"]\n    isolation = \"strict\"\n    \n    [groups.external]\n    servers = [\"slack\", \"email\"]\n    isolation = \"strict\"\n    \n\nOnce an execution touches a strict group,¬†it's locked out of other strict groups¬†-¬†this stops¬†\"read secret from vault,¬†post to Slack\"¬†attack chains.\n\nFull details in¬†\\`ARCHITECTURE.md\\` and \\`forge.toml.example\\` in the repo.\n\nP.S. why on earth are Reddit comments so hard to work with re: formatting? Got there in the end but spent way too damned long drafting this so hope it's useful! Cheers!",
              "score": 1,
              "created_utc": "2026-02-25 13:15:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ii5bw",
          "author": "carsaig",
          "text": "I hadn't any time to look into the cloudflare solution yet - however, I put it on my reading list :-) Your solution sounds solid. At first glance this reminds me of bifrosts' solution which I use. Cuts tools down to 4 and just 1400 Tokens. Got 300+ Tools behind it. Discovery time is significantly lower than going with the docker-gateway solution. I recently saw their sales webinar and went...meehhhh...no^^ :-) dockerizing is nice but the whole discovery logic was not usable (yet). This approach is probably the best you can go for at the moment. So kudos! I'll definitely look into it in more detail. Rust is a nice choice. ",
          "score": 1,
          "created_utc": "2026-02-26 13:50:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7k8k3g",
              "author": "lpostrv",
              "text": "Thanks so much! Very kind of you to comment. If you do have any feedback once you've tested it out, I'd be happy to hear it. One thought I had was the open question of whether there are other desirable functions beyond \\`search()\\` and \\`execute()\\` that would allow the AI to take more sophisticated actions - haven't used too many brain cycles on that one yet!  \nI hadn't heard of bifrost until now but it seems like they have \\`listToolFiles\\`, \\`readToolFile\\`, \\`getToolDocs\\`, and \\`executeToolCode\\`, which is an interesting pattern, but probably not as token efficient. I've tried to follow the Cloudflare pattern so far, but I'd like to see if I can go beyond it in pure utility.",
              "score": 1,
              "created_utc": "2026-02-26 18:46:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rhu7w7",
      "title": "I built an open-source app that syncs your MCP servers across Claude Desktop, Cursor, VS Code, and 6 more clients",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rhu7w7/i_built_an_opensource_app_that_syncs_your_mcp/",
      "author": "aryabyte",
      "created_utc": "2026-03-01 10:40:43",
      "score": 31,
      "num_comments": 9,
      "upvote_ratio": 0.96,
      "text": "I was spending way too much time copy-pasting MCP server configs between all my AI tools. Every client has a different config format (JSON, TOML, XML) and a different file path.\n\n\n\nSo I built Conductor ‚Äî a native macOS app that lets you configure MCP servers once and sync them everywhere.\n\n\n\nWhat it does:\n\n\\- One UI to manage all your MCP servers\n\n\\- Syncs to 9 clients: Claude Desktop, Cursor, VS Code, Windsurf, Claude Code, Zed, JetBrains IDEs, Codex CLI, Antigravity\n\n\\- API keys stored in your macOS Keychain (not in plaintext JSON)\n\n\\- Browse and install from 7,300+ servers on Smithery registry\n\n\\- MCP Stacks ‚Äî bundle servers into shareable sets for your team\n\n\\- Merge-based sync ‚Äî it won't overwrite configs you added manually\n\n\n\nInstall:\n\ncurl -fsSL [https://conductor-mcp.vercel.app/install.sh](https://conductor-mcp.vercel.app/install.sh) | sh\n\n\n\nOpen source (MIT), free, 100% local.\n\n\n\nWebsite: [https://conductor-mcp.vercel.app](https://conductor-mcp.vercel.app)\n\nGitHub: [https://github.com/aryabyte21/conductor](https://github.com/aryabyte21/conductor)\n\n\n\nWould love any feedback!\n\n",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1rhu7w7/i_built_an_opensource_app_that_syncs_your_mcp/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o81j4be",
          "author": "BC_MARO",
          "text": "keychain storage for API keys is the right call - having your tokens in plaintext JSON configs that can get accidentally committed or leaked is probably the most common MCP security footgun right now.",
          "score": 6,
          "created_utc": "2026-03-01 12:25:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o81jj72",
              "author": "aryabyte",
              "text": "absolutely and the good part is with this app you don't need to authenticate your MCP at 10 different places just authenticate once and use in all your AI apps!",
              "score": 1,
              "created_utc": "2026-03-01 12:29:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o84fp6e",
          "author": "Evening-Dot2352",
          "text": "Nice work, this solves a real pain point.\n\nI've been building [MCP Marketplace](https://mcp-marketplace.io/) which tackles the other side of the problem - how do you actually find and trust MCP servers? Every server gets a security scan before listing, and install is one-click config generation for Claude Desktop, Cursor, and VS Codem, etc.\n\nConductor & a curated marketplace could be a killer combo. Discover and vet servers in one place, then sync configs everywhere with your tool. Would be cool to explore an integration where Marketplace generates the config and Conductor pushes it to all clients at once.",
          "score": 3,
          "created_utc": "2026-03-01 21:33:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o857x1c",
              "author": "aryabyte",
              "text": "Nice üëç maybe we could do an integration",
              "score": 1,
              "created_utc": "2026-03-02 00:09:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o85emos",
          "author": "kursku",
          "text": "Works with Windows?",
          "score": 1,
          "created_utc": "2026-03-02 00:49:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o85f5zw",
              "author": "aryabyte",
              "text": "Not yet but will add support soon as tauri is cross platform it‚Äôs just that i‚Äôll need to figure out the mcp.json toml and xml paths in windows I‚Äôll update in the thread once I come up with the windows app!",
              "score": 1,
              "created_utc": "2026-03-02 00:52:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o862e22",
          "author": "nikunjverma11",
          "text": "This is honestly the exact kind of boring pain that becomes a real blocker once you use multiple clients. Keychain storage and merge based sync are the right calls because plaintext configs get messy fast. I would love to see a dry run diff view and a rollback history so teams can trust it. I use Traycer plus Claude Code and Cursor a lot and config drift is real so this solves a legit problem",
          "score": 1,
          "created_utc": "2026-03-02 03:16:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o86f3f4",
          "author": "upvotes2doge",
          "text": "Thank you for writing this",
          "score": 1,
          "created_utc": "2026-03-02 04:43:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdp3hg",
      "title": "How can i auto-generate system architecture diagrams from code?",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rdp3hg/how_can_i_autogenerate_system_architecture/",
      "author": "achinius",
      "created_utc": "2026-02-24 18:56:39",
      "score": 27,
      "num_comments": 29,
      "upvote_ratio": 1.0,
      "text": "Working on a microservices platform and manually drawing architecture diagrams is killing our velocity. Need something that can parse our codebase and auto-generate visual representations of service dependencies, data flows and API connections. \n\nIs there something that can help with this? I've tried a few tools but missing context or producing diagrams that look like spaghetti (no offense spaghetti lovers) is my experience so far. Ideally want something that integrates with our CI/CD pipeline.",
      "is_original_content": false,
      "link_flair_text": "question",
      "permalink": "https://reddit.com/r/mcp/comments/1rdp3hg/how_can_i_autogenerate_system_architecture/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o76vlhq",
          "author": "kenwards",
          "text": "Export your dependency data as JSON, dump it into Claude with your service structure and have it generate Mermaid or C4 diagrams automatically. Then pipe the output straight into Miro for stakeholder reviews.",
          "score": 3,
          "created_utc": "2026-02-24 19:22:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76wuen",
              "author": "achinius",
              "text": "I think claude is a good place to start. ",
              "score": 1,
              "created_utc": "2026-02-24 19:28:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o771jzm",
                  "author": "kenwards",
                  "text": "Way better than other LLMs. That with Cursor as someone else has mentioned will have everything fixed. ",
                  "score": 1,
                  "created_utc": "2026-02-24 19:50:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76taf3",
          "author": "squid267",
          "text": "Ask your llm of choice to generate mermaidjs diagrams. I just did this recently. I pulled all the gitrepos I needed into a new workspace as got submodules and let opus 4.6 take a crack at it. Then copied the mermaidjs (markdown) wherever I needed. You can also find or create an agent skill for mermaidjs.",
          "score": 2,
          "created_utc": "2026-02-24 19:12:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76u1cq",
              "author": "achinius",
              "text": "I find most llm tend to miss the context. How good were the diagrams you generated? ",
              "score": 2,
              "created_utc": "2026-02-24 19:15:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o78gjxx",
                  "author": "memetican",
                  "text": "I use the same approach in Claude, it's generally excellent, maybe needs a but of visual polish. I have it explain custom OAuth 2 flows as sequence diagrams this way, and it nails it.  You can also do some of this through the Figma MCP as a figjam which makes editing trivial. ",
                  "score": 1,
                  "created_utc": "2026-02-24 23:59:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76ykyi",
          "author": "Sufficient-Pass-4203",
          "text": "https://github.com/nicobailon/visual-explainer",
          "score": 2,
          "created_utc": "2026-02-24 19:36:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o775k1p",
              "author": "achinius",
              "text": "I'll check the explainer. Thank you",
              "score": 2,
              "created_utc": "2026-02-24 20:08:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76qqdg",
          "author": "chaoism",
          "text": "Cursor + Claude does it",
          "score": 1,
          "created_utc": "2026-02-24 19:00:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76trfu",
              "author": "achinius",
              "text": "I'll try the combo",
              "score": 1,
              "created_utc": "2026-02-24 19:14:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76rxak",
          "author": "Infamous_Horse",
          "text": "Is hiring a tech writer among your options?",
          "score": 1,
          "created_utc": "2026-02-24 19:06:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76tt9k",
              "author": "achinius",
              "text": "Not part of the plan",
              "score": 1,
              "created_utc": "2026-02-24 19:14:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76zeoq",
          "author": "Sad_Translator5417",
          "text": "How is your architecture? Fix the service boundaries first, then generation actually produces something readable and useful for the team.",
          "score": 1,
          "created_utc": "2026-02-24 19:40:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o775naa",
              "author": "achinius",
              "text": "I think we have got most of these issues in line",
              "score": 1,
              "created_utc": "2026-02-24 20:09:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o770wzk",
          "author": "naseemalnaji-mcpcat",
          "text": "Mermaid MCP with Claude Code worked for me :)",
          "score": 1,
          "created_utc": "2026-02-24 19:47:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o775pv8",
              "author": "achinius",
              "text": "I'll test the two and see how they work",
              "score": 1,
              "created_utc": "2026-02-24 20:09:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77aqo3",
          "author": "thelastpanini",
          "text": "Get opus 4.6 to draw diagrams in ASCII honestly very good.",
          "score": 1,
          "created_utc": "2026-02-24 20:33:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7aceem",
              "author": "achinius",
              "text": "I'll try it out",
              "score": 1,
              "created_utc": "2026-02-25 07:10:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77cmn3",
          "author": "BC_MARO",
          "text": "If your services expose OpenAPI specs, have Claude consume them all through an MCP code nav server and generate Mermaid/C4 diagrams with real dependency context. Way cleaner than parsing source files and won\\'t produce the spaghetti.",
          "score": 1,
          "created_utc": "2026-02-24 20:42:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7acj3z",
              "author": "achinius",
              "text": "I get it. If it can produce something clean, def worth a try. ",
              "score": 2,
              "created_utc": "2026-02-25 07:11:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o786l34",
          "author": "Sketaverse",
          "text": "One shot it in ChatGPT lol",
          "score": 1,
          "created_utc": "2026-02-24 23:04:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ackig",
              "author": "achinius",
              "text": "You manage that with ChatGPT?",
              "score": 1,
              "created_utc": "2026-02-25 07:11:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o79kar2",
          "author": "DeathShot7777",
          "text": "I m developing a tool which solves this exact usecase, it is able to map the architecture in deterministic way and also enrich LLMs / coding agents like cursor / claude code, with Code Knowledge Graph. Its free to use opensource [https://github.com/abhigyanpatwari/gitnexus](https://github.com/abhigyanpatwari/gitnexus)  It has nearly 3K github stars right now, also we as devs are looking to try this out in solving real world problems like u are facing to get sort of design partner / early validation. DMed you, would love to talk",
          "score": 1,
          "created_utc": "2026-02-25 03:42:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79ndee",
              "author": "Beautrj",
              "text": "check your dm \n\n",
              "score": 1,
              "created_utc": "2026-02-25 04:01:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7acmzs",
              "author": "achinius",
              "text": "Wooow...thanks a lot. On it. ",
              "score": 1,
              "created_utc": "2026-02-25 07:12:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7aec3b",
                  "author": "DeathShot7777",
                  "text": "Feel free to reach out for feedback or integration help or anything. We r actively improving it to validate before enterprise launch. Would really appreciate suggestions",
                  "score": 1,
                  "created_utc": "2026-02-25 07:27:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bojn3",
          "author": "SyableWeaver",
          "text": "What are the chances of this? I build a MCP server for the same. \n\nhttps://github.com/Ashish-Surve/mcp-servers/tree/main/diagram-generator",
          "score": 1,
          "created_utc": "2026-02-25 13:40:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rhyeme",
      "title": "I built an open-source MCP server that lets any Agent work on remote machines",
      "subreddit": "mcp",
      "url": "https://v.redd.it/o8s00gdpzfmg1",
      "author": "saba--",
      "created_utc": "2026-03-01 14:18:59",
      "score": 22,
      "num_comments": 3,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1rhyeme/i_built_an_opensource_mcp_server_that_lets_any/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o82uifi",
          "author": "BC_MARO",
          "text": "This is slick. Consider a per-host allowlist plus a required confirmation step for any write or shell tool so one bad prompt can't torch a box.",
          "score": 2,
          "created_utc": "2026-03-01 16:53:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o843tp5",
          "author": "punkpeye",
          "text": "Oh this is useful!",
          "score": 1,
          "created_utc": "2026-03-01 20:32:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o85lpff",
          "author": "International-Tip-71",
          "text": "Difference/Bench than Claude Remote control?",
          "score": 1,
          "created_utc": "2026-03-02 01:32:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfabje",
      "title": "My friend has created this free library of MCP servers",
      "subreddit": "mcp",
      "url": "https://i.redd.it/l8fie2ildulg1.jpeg",
      "author": "psymaniax",
      "created_utc": "2026-02-26 13:35:40",
      "score": 19,
      "num_comments": 16,
      "upvote_ratio": 0.76,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1rfabje/my_friend_has_created_this_free_library_of_mcp/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7ii8py",
          "author": "ToHallowMySleep",
          "text": "Honest feedback: building a marketplace for this is low hanging fruit, there are thousands of things like this now. Like people wrapping chatgpt with some custom prompts in 2024. It's a smart play in the \"selling shovels\" approach, but this is way, way too oversaturated right now.\n\nLiterally writing an MCP server in another window right now, and I'm not sure what this would give me. It may help for a \"I want an MCP server but I don't know which one\" type question, but personally I'd ask Claude to do the work to pick one for me, rather than go down this route.\n\nEither way, good luck to your friend!",
          "score": 4,
          "created_utc": "2026-02-26 13:50:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7im2g4",
              "author": "upvotes2doge",
              "text": "That's a fair point about the marketplace saturation. I actually built an MCP server recently that solves a specific workflow gap I kept running into with Claude Code.\n\nThe problem was the constant copy-paste loop between Claude and Codex windows when I wanted to bounce ideas, get parallel plans, or validate approaches. So I built Claude Co-Commands - an MCP server that adds three collaboration commands directly to Claude Code:\n\n- `/co-brainstorm` for bouncing ideas and getting alternative perspectives from Codex\n- `/co-plan` to generate parallel plans and compare approaches  \n- `/co-validate` for getting that staff engineer review before finalizing\n\nThe MCP approach means it integrates cleanly with Claude Code's existing command system. Instead of running terminal commands or switching windows, you just use the slash commands and Claude handles the collaboration with Codex automatically.\n\nIt's not trying to be a marketplace, just solving that specific workflow friction point. If you're writing MCP servers, you might find the approach interesting - it's basically turning the copy-paste loop into a clean command interface.\n\nhttps://github.com/SnakeO/claude-co-commands",
              "score": 2,
              "created_utc": "2026-02-26 14:11:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ko3lo",
                  "author": "makinggrace",
                  "text": "Is this better as a MCP than a skill? (Genuine question. I only use something like your co-plan as a skill but I use it often!)",
                  "score": 1,
                  "created_utc": "2026-02-26 20:00:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7op7qp",
              "author": "Automatic-Bowler-538",
              "text": "Totally fair take. A plain directory is not that interesting anymore.\n\nWhat we are trying to solve is the gap between ‚Äúa link to a server‚Äù and ‚ÄúI know what this thing actually does in practice‚Äù. LLMs can suggest a server, but you still end up asking:\n\n* what tools does it expose exactly\n* what does the input/output look like\n* does it behave the way I expect\n* what happens when you combine it with another server\n\nPlayground is meant to make that evaluation step fast. You can inspect the tool surface and try it immediately with synthetic data, so you get signal in minutes.",
              "score": 1,
              "created_utc": "2026-02-27 11:47:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jggwx",
          "author": "AchillesDev",
          "text": "Is this built on top of MCP registry? Does it do any filtering or anything that makes it more useful than the existing MCP registry?",
          "score": 2,
          "created_utc": "2026-02-26 16:37:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7mzf6d",
              "author": "Sad-Match9545",
              "text": "A bunch of other capabilities making it easier to manage multiple servers, control tool availability, etc. but would love to hear more about what else would be valuable to you u/AchillesDev !",
              "score": 1,
              "created_utc": "2026-02-27 03:23:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7opanw",
              "author": "Automatic-Bowler-538",
              "text": "Good question. We are not trying to replace the registry. We pull from existing sources and then add layers that help with evaluation:\n\n* verification and normalization of metadata\n* tool surface inspection (what it exposes, schemas, etc)\n* a one-click playground to test behavior without wiring up your own accounts\n* workflows that show practical multi-server combos\n\nIf you have a specific filtering view you wish existed (auth method, local vs hosted, maintenance status, permissions, rate limits), tell me and we will prioritize it.",
              "score": 1,
              "created_utc": "2026-02-27 11:48:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jmmyf",
          "author": "BC_MARO",
          "text": "If you add quick metadata like auth method, maintenance status, and a one-click test harness, it‚Äôll beat any plain directory.",
          "score": 2,
          "created_utc": "2026-02-26 17:05:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7mzaup",
              "author": "Sad-Match9545",
              "text": "Love the feedback! We have a lot of that in pace in different versions and need to make it all publicly available üôè",
              "score": 1,
              "created_utc": "2026-02-27 03:22:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lnjsq",
          "author": "psychananaz",
          "text": "A little late to the party:  \n[smithery.ai](http://smithery.ai)   \n[glama.ai](http://glama.ai)  \n[mcp.so](http://mcp.so)   \n[mcpservers.org](http://mcpservers.org)",
          "score": 2,
          "created_utc": "2026-02-26 22:52:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ojjwz",
              "author": "punkpeye",
              "text": "Founder of Glama. For what it is worth, there are few good directories. Obviously, I think Glama is great, but everything else on that list is just aggregation of shallow data. And the market is huge. So I think the opportunity is still there. I would encourage to think of one pain point and focus on it rather than trying to cover everything though - otherwise it quickly gets overwhelming",
              "score": 1,
              "created_utc": "2026-02-27 11:00:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7oqj0e",
                  "author": "psychananaz",
                  "text": "My comment was simply to show that we don't need another MCP library. Those links were easily found in just two searches, and I was already aware of Smithery.\n\nI don't see what makes Glama less 'shallow' than the others, though. It's all just a list of MCP servers, categories, and search functionality. If anything, Glama is shallower if you look at the data presented for a server. Anything beyond that isn't really relevant to what defines 'a library of servers.' All you need is a directory of servers with search capabilities, which all of them have. But I might be missing your point.",
                  "score": 2,
                  "created_utc": "2026-02-27 11:57:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o83p6ev",
          "author": "BrunoGarret",
          "text": "There are already more such portals ...",
          "score": 1,
          "created_utc": "2026-03-01 19:17:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o84gyyk",
          "author": "mbreslin",
          "text": "Is your friend Roy Kent? When I searched for Gmail on the site it let me read Roy Kent's email. Seems unlikely this was intended?",
          "score": 1,
          "created_utc": "2026-03-01 21:39:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o86qvw7",
          "author": "Ok_Run_2237",
          "text": "Can somebody explain the difference of an ‚Äúmcp server‚Äù and me just using api endpoints with like an agent? What is the purpose of the mcp server over that?",
          "score": 1,
          "created_utc": "2026-03-02 06:17:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7igvv2",
          "author": "psymaniax",
          "text": "I guess I should share a link as well: https://www.natoma.run/",
          "score": 0,
          "created_utc": "2026-02-26 13:43:31",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1rh41b9",
      "title": "codesurface: Claude writes better code when it can't read yours",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rh41b9/codesurface_claude_writes_better_code_when_it/",
      "author": "Codeturion",
      "created_utc": "2026-02-28 14:29:15",
      "score": 19,
      "num_comments": 3,
      "upvote_ratio": 0.83,
      "text": "The bigger your codebase,¬†the more confident Claude gets about things that don't exist.¬†I work on large,¬†sometimes legacy codebases and kept hitting this.¬†Claude would grep for a class,¬†get partial matches,¬†and start inferring from there.¬†Most of the time it's fine.¬†But as the codebase grows¬†**the signal-to-noise ratio drops and the agent's confidence doesn't**.\n\nThe deeper issue isn't token waste.¬†It's¬†**entropy in the reasoning chain**.¬†When Claude reads a source file,¬†it sees implementation details it doesn't need and starts making inferences from them.¬†It sees a private method call inside a public method and assumes a related event or type must exist somewhere.¬†It doesn't.¬†The agent made a¬†**plausible wrong inference from true context**,¬†and now it's writing code against something that was never declared.¬†Classic hallucination,¬†but the subtle kind where the grounding¬†*looks*¬†real.\n\nI kept thinking about what I actually want the agent to see when it's researching my code.¬†Not the implementation,¬†not the private fields,¬†not the method bodies.¬†**Just the public contract.**¬†The same thing I'd look at in an IDE's¬†\"Go to Definition\"¬†or a generated API doc.\n\nSo I built¬†**codesurface**.¬†It parses your source files at startup,¬†extracts every public class,¬†method,¬†property,¬†and field,¬†and serves them through MCP tools.¬†Signature with no body means nothing to over-interpret.¬†You're essentially¬†**collapsing the inference distribution to a single correct point**.¬†Same query always returns the same result,¬†no variation based on grep patterns or file ordering.\n\nResults include file paths and line numbers,¬†so¬†**when the agent**¬†***does***¬†**need implementation detail**,¬†it¬†**reads just those lines**¬†instead of the whole file.\n\nI benchmarked it across five real projects in five languages¬†(C#,¬†TypeScript,¬†Java,¬†Go,¬†Python).¬†Token savings vary by codebase,¬†but the more valuable outcome is¬†**fewer wrong inferences**¬†and fewer¬†\"let me check that file again\"¬†roundtrips.¬†Deliberately minimal:¬†no AST,¬†no dependency graphs,¬†no import resolution.¬†Just public signatures and where to find them.¬†One package,¬†nothing to configure beyond a source path.\n\nGitHub:¬†[https://github.com/Codeturion/codesurface](https://github.com/Codeturion/codesurface)\n\nDetailed benchmark write-up in the repo.¬†Happy to answer questions or take feature requests.",
      "is_original_content": false,
      "link_flair_text": "server",
      "permalink": "https://reddit.com/r/mcp/comments/1rh41b9/codesurface_claude_writes_better_code_when_it/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7wpmbh",
          "author": "upvotes2doge",
          "text": "This is a really interesting approach to the hallucination problem! Your point about Claude making plausible wrong inferences from true context is spot on, and limiting what the agent sees to just public contracts is a clever way to collapse the inference distribution.\n\nWhat you're doing with codesurface reminds me of something I built called Claude Co-Commands, which is also an MCP server but focused on a different aspect of the workflow problem. Instead of controlling what Claude sees, it adds collaboration commands that let Claude Code automatically consult Codex at key decision points.\n\nThe commands work like this: `/co-brainstorm` for bouncing ideas off Codex to get alternative perspectives, `/co-plan` to generate parallel implementation plans and compare approaches, and `/co-validate` for getting that \"staff engineer review\" before finalizing your approach. The MCP integration means it works cleanly with Claude Code's existing command system, so you just use the slash commands and Claude handles the collaboration with Codex automatically.\n\nYour approach with codesurface and my approach with Claude Co-Commands are complementary solutions to different workflow problems. You're solving the \"what should Claude see\" problem, while I'm solving the \"how should Claude collaborate with other AI systems\" problem. Both are about making the Claude Code workflow more reliable and efficient, just from different angles.\n\nhttps://github.com/SnakeO/claude-co-commands\n\nI've been using this setup for a few weeks now and it completely eliminates the manual back-and-forth of copying plans between different AI systems. The validation command in particular would work well with your codesurface approach - you could have Claude use codesurface to understand the public API, then use `/co-validate` to get a second opinion on any changes before implementing them.",
          "score": 3,
          "created_utc": "2026-02-28 17:12:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yy78e",
          "author": "BC_MARO",
          "text": "the 'collapse inference to a single point' framing is exactly right - signatures are the contract, implementation is noise. curious if you see different token savings for dynamically typed vs statically typed codebases.",
          "score": 1,
          "created_utc": "2026-03-01 00:24:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o8110zx",
              "author": "Codeturion",
              "text": "Good question, I have benchmarks across five languages in the repository. Honestly the savings ratio is pretty similar regardless of typing. Python signatures are shorter without type annotations, so the absolute token count is lower, but the relative reduction is about the same as C# or Java. The bigger factor ended up being codebase verbosity rather than the type system. The chart is in the repo if you're curious. I will dig more into this",
              "score": 1,
              "created_utc": "2026-03-01 09:39:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rfjan2",
      "title": "CodeGraphContext for large codebases - Improve 10x productivity",
      "subreddit": "mcp",
      "url": "https://medium.com/@krishna.bhaskarla/how-i-saved-80-of-my-time-analyzing-a-791k-node-codebase-and-made-github-copilot-actually-useful-eacc935cdb1b",
      "author": "Ok_Appointment_2064",
      "created_utc": "2026-02-26 19:10:16",
      "score": 17,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1rfjan2/codegraphcontext_for_large_codebases_improve_10x/",
      "domain": "medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "o7pc5fw",
          "author": "-ke7in-",
          "text": "Dart please",
          "score": 1,
          "created_utc": "2026-02-27 14:12:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ms7fm",
          "author": "Desperate-Ad-9679",
          "text": "Thanks for writing this post, seems quite a good medium blog to document all the advantages covered with CodeGraphContext!!",
          "score": 1,
          "created_utc": "2026-02-27 02:40:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7n30r5",
              "author": "Ok_Appointment_2064",
              "text": "Thank you. Will publish part 2 in 2 days",
              "score": 1,
              "created_utc": "2026-02-27 03:45:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7n9nxn",
                  "author": "Desperate-Ad-9679",
                  "text": "Definitely that's good to hear!",
                  "score": 0,
                  "created_utc": "2026-02-27 04:29:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rhdxl6",
      "title": "[Open Source] MCPX: turn MCP servers into a composable CLI for agent workflows",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rhdxl6/open_source_mcpx_turn_mcp_servers_into_a/",
      "author": "ldkge",
      "created_utc": "2026-02-28 21:02:14",
      "score": 14,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "I built MCPX: [https://github.com/lydakis/mcpx](https://github.com/lydakis/mcpx)\n\n\n\nPositioning is simple:\n\nMCPX is primarily an agent interface layer. It turns MCP servers into shell-composable commands so agents can chain them with existing CLI tooling. Humans can use it too, but that is secondary.\n\n\n\nThis has been especially useful for OpenClaw:\n\nOpenClaw can invoke \\`mcpx\\` as a normal CLI and immediately use MCP servers without adding custom MCP protocol/auth plumbing. With Codex Apps enabled, app-backed servers can fit into the same flow.\n\n\n\nContract:\n\n\\- mcpx\n\n\\- mcpx <server>\n\n\\- mcpx <server> <tool>\n\n\n\nExamples:\n\n\\- mcpx github search-repositories --help\n\n\\- mcpx github search-repositories --query=mcp\n\n\\- echo '{\"query\":\"mcp\"}' | mcpx github search-repositories\n\n\n\nNot trying to be an MCP platform, just a sharp Unix-style conversion layer with predictable command behavior.\n\n\n\nFeedback I‚Äôd value:\n\n\\- Does this contract fit real agent workflows?\n\n\\- What would make this more useful in production agent workflows?\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1rhdxl6/open_source_mcpx_turn_mcp_servers_into_a/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7ypd2t",
          "author": "xing_horizon",
          "text": "Nice direction. The Unix-style contract is exactly what makes agent loops composable.\n\nOne thing that would help in production: a machine-readable error contract (stable exit codes + typed stderr JSON), so orchestration layers can retry/branch deterministically instead of parsing text.\n\nAlso useful for multi-step runs:\n- \\`--dry-run\\` showing resolved server/tool/auth path\n- \\`--trace\\` with per-hop timing + normalized tool input hash\n- explicit idempotency key passthrough for side-effecting tools\n\nIf those are in place, MCPX becomes not just a bridge, but a reliable control primitive in larger agent pipelines.\n\n",
          "score": 3,
          "created_utc": "2026-02-28 23:31:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ztfvt",
              "author": "ldkge",
              "text": "Appreciate the point! MCPX already has a stable Unix exit-code contract (0/1/2/3) for deterministic branching. I'm intentionally keeping stderr human-readable instead of adding a second typed error protocol, to keep MCPX thin and out of the way for now",
              "score": 1,
              "created_utc": "2026-03-01 03:35:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7xysby",
          "author": "ldkge",
          "text": "Happy to answer anything technical here.\n\n\n\n1. How is your agent using MCP today?\n\n2. Where did MCPX make your agent workflow easier?\n\n3. Where did MCPX get in the way in real agent loops?\n\n4. What MCP workflow do you want to do that MCPX still doesn‚Äôt support?\n\n5. What output/error behavior made automation brittle?\n\n\n\nConcrete examples or snippets are especially helpful.\n\n\n\n",
          "score": 1,
          "created_utc": "2026-02-28 21:04:58",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o7yy4cv",
          "author": "BC_MARO",
          "text": "shell-composable MCP calls are underrated for agent loops - being able to pipe outputs between tools without custom protocol glue is exactly what makes them chain cleanly. have you benchmarked latency vs direct SDK calls when chaining 3+ tools?",
          "score": 1,
          "created_utc": "2026-03-01 00:23:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zz9lc",
              "author": "ldkge",
              "text": "Good question. Went and set up a quick benchmark to find out.\n\nShort answer: depends which SDK pattern you compare against.\n\nChain of 3 tools, 100 warm runs. I compared mcpx (warm daemon) against two SDK baselines: a persistent in-process session (best case, not realistic for shell usage), and subprocess-per-hop (what you'd actually do in a shell/agent loop).\n\n\n\nAt 0ms tool delay:\n\n\\- Persistent SDK: 0.16ms\n\n\\- Subprocess-per-hop SDK: 40ms\n\n\\- mcpx: 21.9ms\n\n\n\nAt 100ms tool delay:\n\n\\- Persistent SDK: 305ms\n\n\\- Subprocess-per-hop SDK: 357ms\n\n\\- mcpx: 337ms\n\n\n\nmcpx adds \\~22ms constant overhead vs persistent SDK. That's CLI parse, config validation, daemon connect with nonce check, and a unix socket round-trip per call. But compared to spawning an SDK subprocess per hop, which is the pattern you'd actually use in a pipe chain, mcpx comes out \\~18ms faster. At realistic tool latencies the whole difference is noise.\n\nBottom line: in the shell-composable workflow you're describing, mcpx is faster than rolling your own SDK calls per hop.\n\nSeparately, if your chain hits the same tool repeatedly, \\`--cache=60s\\` can help too. 100ms tool, 50 calls: 5.68s without, 0.55s with. \\~10x for idempotent reads.",
              "score": 2,
              "created_utc": "2026-03-01 04:15:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o80318d",
                  "author": "BC_MARO",
                  "text": "The --cache flag is the sleeper feature here; 10x on idempotent reads is huge for agent loops hitting the same tool repeatedly. The 22ms constant overhead is basically noise in any real workflow.",
                  "score": 1,
                  "created_utc": "2026-03-01 04:42:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rfdmgp",
      "title": "Are standalone MCP servers still worth building?",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rfdmgp/are_standalone_mcp_servers_still_worth_building/",
      "author": "ialijr",
      "created_utc": "2026-02-26 15:46:38",
      "score": 12,
      "num_comments": 13,
      "upvote_ratio": 0.88,
      "text": "Quick question for builders here:\n\nAre people still building standalone MCP servers, or has the ecosystem fully shifted toward MCP / ChatGPT apps?\n\nWith all the hackathons and industry pushes around apps, it feels like wrapping everything as an MCP/ChatGPT app might be the only way to get traction.\n\nIs it still worth building MCP servers on their own, or is app-layer distribution basically mandatory now?\n\nCurious what others are seeing.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1rfdmgp/are_standalone_mcp_servers_still_worth_building/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7jebnd",
          "author": "BC_MARO",
          "text": "standalone servers are still very much worth building. the app integrations are thin wrappers and tend to break when APIs change, a proper server is more durable and works across multiple clients.",
          "score": 8,
          "created_utc": "2026-02-26 16:27:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jb079",
          "author": "jezweb",
          "text": "Yep. Building new ones every week for all sorts of system connections and tooling.",
          "score": 5,
          "created_utc": "2026-02-26 16:12:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jg7sx",
              "author": "ialijr",
              "text": "Thanks for the answer, thought they were \"outdated\"",
              "score": 1,
              "created_utc": "2026-02-26 16:36:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jm5pt",
          "author": "theWiseTiger",
          "text": "Absolutely. Tokens are getting more expensive. I put my knowledge base behind mcp with search capability.",
          "score": 2,
          "created_utc": "2026-02-26 17:03:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lk7qr",
              "author": "darkwingdankest",
              "text": "same, curious how you implemented yours? mine is at https://github.com/prmichaelsen/remember-mcp if you want to compare notes",
              "score": 1,
              "created_utc": "2026-02-26 22:35:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kxuso",
          "author": "GarbageOk5505",
          "text": "Still worth it if you control the deployment. app-layer distribution is about reach, but standalone MCP servers give you control over the execution environment  which matters when 40%+ of public MCP servers have unrestricted command execution. owning the server and running it in an isolated environment with egress controls is the difference between a useful tool and a liability.",
          "score": 2,
          "created_utc": "2026-02-26 20:47:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7oj3dx",
          "author": "punkpeye",
          "text": "Focus on distribution. I help maintain a few directories of MCP servers and it is shocking how few people put effort into documentation and containerization of their services. No one is going to install random scripts anymore.",
          "score": 2,
          "created_utc": "2026-02-27 10:56:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p2559",
              "author": "ialijr",
              "text": "That makes sense, I have not thought about it from this perspective. Is Docker basically the expected format now, or do you still see people getting away with npm/pip installs if the docs are solid?",
              "score": 1,
              "created_utc": "2026-02-27 13:16:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7laz69",
          "author": "Sancroth_2621",
          "text": "Wait. What are these apps? I just managed to get through building my mcps, skills and still reading on agents.md. Did a new thing show up again?",
          "score": 1,
          "created_utc": "2026-02-26 21:49:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lkd8r",
              "author": "darkwingdankest",
              "text": "I think they mean chat agents that... connect to standalone MCP servers. So like, the whole point of building standalone MCP servers",
              "score": 1,
              "created_utc": "2026-02-26 22:35:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ljx5h",
          "author": "darkwingdankest",
          "text": "I've built:\n\n- https://github.com/prmichaelsen/remember-mcp\n- https://github.com/prmichaelsen/agentbase\n- https://github.com/prmichaelsen/google-calendar-mcp\n- https://github.com/prmichaelsen/eventbrite-mcp\n\nand a multitent platform with a chat agent. It connects to each",
          "score": 1,
          "created_utc": "2026-02-26 22:33:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lqz40",
          "author": "huttobe",
          "text": "Standalone mcps are just too valuable and can‚Äôt be compared to skills at all.",
          "score": 1,
          "created_utc": "2026-02-26 23:10:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vz8r4",
              "author": "mzinz",
              "text": "Why? Isn‚Äôt it the same outcome?",
              "score": 1,
              "created_utc": "2026-02-28 14:59:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1reqycw",
      "title": "MCPwner finds multiple 0-day vulnerabilities in OpenClaw",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1reqycw/mcpwner_finds_multiple_0day_vulnerabilities_in/",
      "author": "Comfortable-Ad-2379",
      "created_utc": "2026-02-25 21:45:55",
      "score": 12,
      "num_comments": 4,
      "upvote_ratio": 0.93,
      "text": "I've been developing [MCPwner](https://github.com/Pigyon/MCPwner), an MCP server that lets your AI agents auto-pentest security targets. \n\nWhile most people are waiting for the latest flagship models to do the heavy lifting, I built this to orchestrate **GPT-4o** and **Claude 3.5 Sonnet** models that are older by today's standards but, when properly directed, are more than capable of finding deep architectural flaws using MCPwner.\n\nI recently pointed MCPwner at **OpenClaw**, and it successfully identified several 0-days that have now been issued official advisories. It didn't just find \"bugs\". it found critical logic bypasses and injection points that standard scanners completely missed.\n\n### The Findings:\n[Environment Variable Injection](https://github.com/openclaw/openclaw/security/advisories/GHSA-82g8-464f-2mv7)\n\n\n[ACP permission auto-approval bypass](https://github.com/openclaw/openclaw/security/advisories/GHSA-7jx5-9fjg-hp4m)\n\n\n[File-existence oracle info disclosure](https://github.com/openclaw/openclaw/security/advisories/GHSA-6c9j-x93c-rw6j)\n\n\n[safeBins stdin-only bypass](https://github.com/openclaw/openclaw/security/advisories/GHSA-4685-c5cp-vp95)\n\nThe project is still heavily in progress, but the fact that it's already pulling in multiple vulnerabilities and other CVEs I reported using mid-tier/older models shows its strength over traditional static analysis.\n\nIf you're building in the offensive AI space I‚Äôd love for you to put this through its paces. I'm actively looking for contributors to help sharpen the scanning logic and expand the toolkitPRs and feedback are more than welcome.\n\n**GitHub:** [https://github.com/Pigyon/MCPwner](https://github.com/Pigyon/MCPwner)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mcp/comments/1reqycw/mcpwner_finds_multiple_0day_vulnerabilities_in/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7gjzuy",
          "author": "New_Animator_7710",
          "text": "From a defensive standpoint, projects like MCPwner highlight an emerging reality: AI-assisted offensive tooling is lowering the barrier to discovering complex vulnerabilities. we should be thinking not only about improving these systems, but also about how to build evaluation benchmarks and defensive countermeasures that anticipate AI-driven architectural probing.",
          "score": 3,
          "created_utc": "2026-02-26 04:27:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fek5l",
          "author": "BC_MARO",
          "text": "The ACP permission auto-approval bypass is the scariest one - once an attacker can escalate permissions without user confirmation, the whole security model collapses. This is exactly the problem Peta (peta.io) was built for: policy-based approvals and audit trails on every MCP tool call, so no tool fires without an explicit allow rule.",
          "score": 2,
          "created_utc": "2026-02-26 00:25:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7etzdb",
          "author": "barefootsanders",
          "text": "Great findings and interested to learn more. Up for swapping notes? We recently published a trust framework and scanner implementation for MCP bundles. Interested in ways of making MCP more secure and always up for collaboration. \n\nThis is our framework: [https://mpaktrust.org/](https://mpaktrust.org/) it outlines a number of security controls, mostly based on other OSS tooling all brought together.\n\nThe scanner scans bundles when they are published to [mpak.dev](https://mpak.dev/). Publishers get a security score and badge. Everything is open-source and self-hostable too.",
          "score": 1,
          "created_utc": "2026-02-25 22:35:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rehql0",
      "title": "MCP tool discovery at scale - how we handle 15+ servers in Bifrost AI gateway",
      "subreddit": "mcp",
      "url": "https://www.reddit.com/r/mcp/comments/1rehql0/mcp_tool_discovery_at_scale_how_we_handle_15/",
      "author": "dinkinflika0",
      "created_utc": "2026-02-25 16:20:44",
      "score": 12,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I maintain **Bifrost**, and once you go past \\~10 MCP servers, things start getting messy.\n\nFirst issue: tool name collisions. Different MCP servers expose tools with the same names. For example, a `search_files` tool from a filesystem server and another from Google Drive. The LLM sometimes picks the wrong one, and the user gets weird results.  \nWhat worked for us was simple: namespace the tools. So now it‚Äôs `filesystem.search_files` vs `gdrive.search_files`. The LLM can clearly see where each tool is coming from.\n\nThen there‚Äôs schema bloat. If you have \\~15 servers, you might end up with 80+ tools. If you dump every schema into every request, your context window explodes and token costs go up fast.  \nOur fix was tool filtering per request. We use virtual keys that decide which tools an agent can see. So each agent only gets the relevant tools instead of the full catalog.\n\nAnother pain point is the connection lifecycle. MCP servers can crash or just hang, and requests end up waiting on dead servers.  \nWe added health checks before routing. If a server fails checks, we temporarily exclude it and bring it back once it recovers.\n\nOne more thing that helped a lot once we had 3+ servers: **Code Mode**. Instead of exposing every tool schema, the LLM writes TypeScript to orchestrate tools. That alone cut token usage by 50%+ for us.\n\nIf you want to check it out:  \nCode: [https://git.new/bifrost](https://git.new/bifrost)  \nDocs: [https://getmax.im/docspage](https://getmax.im/docspage)",
      "is_original_content": false,
      "link_flair_text": "showcase",
      "permalink": "https://reddit.com/r/mcp/comments/1rehql0/mcp_tool_discovery_at_scale_how_we_handle_15/",
      "domain": "self.mcp",
      "is_self": true,
      "comments": [
        {
          "id": "o7d5len",
          "author": "penguinzb1",
          "text": "the collision fix is right but you won't know if the namespacing actually resolves the misrouting until you've run it against the queries that originally triggered the wrong picks.",
          "score": 1,
          "created_utc": "2026-02-25 17:54:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cw4xo",
          "author": "BC_MARO",
          "text": "Per-agent tool filtering is the right call, but you still need the policy layer on top -- controlling which users or roles can invoke sensitive tools, not just what the LLM sees. Peta (peta.io) tackles that as a dedicated MCP control plane with RBAC and audit trails.",
          "score": 0,
          "created_utc": "2026-02-25 17:11:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fbw6d",
          "author": "kashishhora-mcpcat",
          "text": "Namespacing is pretty effective. We‚Äôve also helped a couple of customers with lots of really similar tool and param names reduce a lot of the collisions and schema mismatches by namespacing and just naming things differently.\n\nOne counter intuitive idea that has worked: if you have 50+ tools and half of them all begin with ‚Äúget_‚Äù you‚Äôre going to increase the risk of collisions. Trying to vary it up or just removing any prefixes reduces collisions.\n\nIf you want a good way to detect collisions or other types of hallucinations or agent-specific errors, should check us out (mcpcat.io)! We have lots of features to help with debugging and analyzing how agents are using your MCP server.",
          "score": 0,
          "created_utc": "2026-02-26 00:11:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}