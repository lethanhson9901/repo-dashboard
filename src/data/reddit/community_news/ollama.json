{
  "metadata": {
    "last_updated": "2026-02-06 17:00:02",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 20,
    "total_comments": 134,
    "file_size_bytes": 155702
  },
  "items": [
    {
      "id": "1qsjn38",
      "title": "Running Ollama fully air-gapped, anyone else?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qsjn38/running_ollama_fully_airgapped_anyone_else/",
      "author": "thefilthybeard",
      "created_utc": "2026-02-01 00:45:31",
      "score": 76,
      "num_comments": 56,
      "upvote_ratio": 0.97,
      "text": "Been building AI tools that run fully air-gapped for classified environments. No internet, no cloud, everything local.\n\nOllama has been solid for this. Running it on hardware that never touches a network. Biggest challenges were model selection (needed stuff that performs well without massive VRAM) and building workflows that don't assume any external API calls.\n\nCurious what others are doing for fully offline deployments. Anyone else running Ollama in secure or disconnected environments? What models are you using and what are you running it on?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qsjn38/running_ollama_fully_airgapped_anyone_else/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o2w2ds9",
          "author": "grudev",
          "text": "Here, \n\n\nThink RAG with classified/sensitive info and some other related workflows. ",
          "score": 19,
          "created_utc": "2026-02-01 00:59:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2w40w6",
              "author": "thefilthybeard",
              "text": "Glad to see there are more than just me working on this!",
              "score": 9,
              "created_utc": "2026-02-01 01:09:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2wv4tm",
                  "author": "PrepperDisk",
                  "text": "Found AnythingLLM much better at RAG out of the box, but live Ollama + WebUI for customization and ease of use in non-RAG.",
                  "score": 4,
                  "created_utc": "2026-02-01 03:54:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2x0rsy",
                  "author": "grudev",
                  "text": "There are a few of us,, but everyone keeps it pretty low key and generic. ",
                  "score": 5,
                  "created_utc": "2026-02-01 04:32:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2yhc85",
              "author": "joey2scoops",
              "text": "Anyone dabbling in federated learning?",
              "score": 1,
              "created_utc": "2026-02-01 12:02:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ym7i8",
                  "author": "grudev",
                  "text": "Not really... our data is very centralized anyways. \n\n\nI want to work on fine tuning a model for RAFT in the near future. ",
                  "score": 1,
                  "created_utc": "2026-02-01 12:41:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2x76rg",
          "author": "DutchOfBurdock",
          "text": "Not specifically Ollama, but I have a single isolated device that naturally lacks WiFi or BT. It does have a camera/mic which are used, so not truly air gapped. It's currently an experimental MCP with a twist: I'm giving function calling models full shell access (inside a container) with the intent on making itself a perfect little home. Spoiler, it has destroyed it's chroot several times.\n\nWhat it can do beautifully at the moment is recognise people and objects seen from the camera (openCV), can hear what I say to it (whisper), think about it and provide a reply (llama) and can even talk back to me (vibevoice). I am even experimenting with a model that can recognise sounds, such as alarms, beeping, smashing etc. (GAMA).",
          "score": 10,
          "created_utc": "2026-02-01 05:17:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2w2vsd",
          "author": "cfipilot715",
          "text": "GPT-OSS is pretty good",
          "score": 6,
          "created_utc": "2026-02-01 01:02:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2w775k",
              "author": "Independent_Solid151",
              "text": "For coding devstral-small-2.",
              "score": 3,
              "created_utc": "2026-02-01 01:28:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2w4t1g",
              "author": "thefilthybeard",
              "text": "I haven't tested that yet!",
              "score": 1,
              "created_utc": "2026-02-01 01:13:38",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2xtsdb",
              "author": "AgreeableIron811",
              "text": "Its pretty slow for the 120b model. And I have a very god server",
              "score": 1,
              "created_utc": "2026-02-01 08:29:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3reyef",
              "author": "generousone",
              "text": "It is remarkably good, the 20b model that is",
              "score": 1,
              "created_utc": "2026-02-05 18:32:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o31v72h",
          "author": "thecoder12322",
          "text": "This is exactly the kind of work that needs more visibility! Building for air-gapped/classified environments is challenging but critical.\n\nFor edge deployment scenarios like yours, have you looked into the **RunAnywhere SDK** (it's open source)? It's designed specifically for running LLMs, STT (Whisper), TTS, and VLMs on edge hardware without cloud dependencies. Supports multiple modalities and can run entirely offline once models are loaded.\n\nCurious what hardware specs you're working with and which models you've found perform best in your air-gapped setup? Happy to chat more about edge AI deployment if you want to DM!",
          "score": 3,
          "created_utc": "2026-02-01 22:22:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32ctda",
              "author": "thefilthybeard",
              "text": "Haven't looked into RunAnywhere SDK yet, but I will now... appreciate the tip. Always looking for tools built with offline-first in mind.\n\nHardware specs vary depending on which model I'm running. Quantized 7B models work fine on systems with 16-32GB RAM and no dedicated GPU. When I need more capability, I scale the hardware accordingly. Mistral and Llama variants have been solid for the compliance and document analysis work I'm focused on.\n\nThe key has been batching....breaking tasks into smaller chunks so the model isn't choking on massive context windows. Keeps things fast and accurate.",
              "score": 2,
              "created_utc": "2026-02-01 23:57:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o35ysnt",
          "author": "Desperate-Pattern-54",
          "text": "This resonates a lot. I’m working on an offline-first, air-gapped RAG system specifically designed for sensitive / disconnected environments — no cloud assumptions, no external APIs, and strong hallucination defenses.\nThe focus has been on governance, confidence-gated retrieval, and human-on-the-loop workflows, not just “getting a model to answer.”\n\nIf useful to others exploring this space, here’s the reference implementation I’ve been building:\n\n https://github.com/drosadocastro-bit/nova_rag_public\n\nWould love to compare notes on model selection and workflows that behave well under strict offline constraints.",
          "score": 3,
          "created_utc": "2026-02-02 15:04:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3u34ej",
              "author": "smcgann",
              "text": "This is extremely interesting.  Thanks for sharing!",
              "score": 2,
              "created_utc": "2026-02-06 02:54:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o310m23",
          "author": "Dark_Passenger_107",
          "text": "Yeah, I build tools for the DIB that must run airgapped. The Granite 4 models have been awesome for this. \n\nRecently had to build a tool that runs on mid-tier hardware and works decently well at document scanning + semantic analysis. Used a pipeline of granite3.3-vision:2b for the document ingestion and then granite4:3b-h for analysis/classification. Both models run surprisingly well on CPU only.\n\nFor the production build, ended up going with the Q4 variant (packaged the gguf + llama.cpp in the app).",
          "score": 2,
          "created_utc": "2026-02-01 19:53:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2wg82i",
          "author": "silencedshotty76",
          "text": "Idk why you'd use ollama the model's ollama leverages are gguf models that's the core component.\n\nIf I were you I'd stick to llama-server to serve the gguf model files. in addition llama-server has the option to enable the --offline flag if you're looking for an offline only chat.\nTo address the airgapping mechanism, you're fine as long as the chat interface is restricted to the terminal since a gui chat interface has the potential to be saved in the memory, disk.",
          "score": 1,
          "created_utc": "2026-02-01 02:22:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wsfnh",
              "author": "thefilthybeard",
              "text": "Good call on llama-server, I'll look into that. The offline flag is exactly the kind of thing I need.\n\nFor my use case, terminal-only is actually preferred. These environments have strict requirements about what gets written to disk and where. GUI adds unnecessary risk.\n\nYou running anything air-gapped yourself or just familiar with the setup?",
              "score": 1,
              "created_utc": "2026-02-01 03:37:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2x17vm",
                  "author": "grudev",
                  "text": "Ollama is fine, especially if you use multiple models. ",
                  "score": 1,
                  "created_utc": "2026-02-01 04:35:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zm8yq",
          "author": "BidWestern1056",
          "text": "i built npcpy and npcsh and incognide purposefully to work well with local models/ollama so that users can build more powerful applications with them and make the most of them through better interface design\n\n[https://github.com/npc-worldwide/npcpy](https://github.com/npc-worldwide/npcpy)\n\n[https://github.com/npc-worldwide/incognide](https://github.com/npc-worldwide/incognide)\n\n[https://github.com/npc-worldwide/npcsh](https://github.com/npc-worldwide/npcsh)",
          "score": 1,
          "created_utc": "2026-02-01 16:02:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2zxpr3",
          "author": "sinan_online",
          "text": "I did the same with HuggingFace, first you cache the model, then run it.\n\nI also created some containerized models with Ollama, you can run them using docker locally, airtight as you say, once you download. They are on docker hub. I am experimenting with llama now that it has a POST streaming  endpoint.",
          "score": 1,
          "created_utc": "2026-02-01 16:55:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31mfjb",
          "author": "searchblox_searchai",
          "text": "We integrate with Ollama for our SearchAI application and run in air gapped environments https://developer.searchblox.com/docs/llm-platform-configuration#ollama-native-api",
          "score": 1,
          "created_utc": "2026-02-01 21:39:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o32yy72",
          "author": "smcgann",
          "text": "Glad to see this brought up!   I’m overseeing (from a compliance perspective) several projects that are just starting and planning my own projects.  Looking to help out software developers with coding models.  PEFT and RAG for various uses.  As well as agents for data analytics.",
          "score": 1,
          "created_utc": "2026-02-02 02:01:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o331kv6",
          "author": "Odd_Butterfly_455",
          "text": "I preprocess all my documents with a agent to qdrant and connect qdrant to the model afterwards... I'm on this since 1 month at my job it's kinda work but need improvement",
          "score": 1,
          "created_utc": "2026-02-02 02:16:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tozey",
          "author": "SufficientHold8688",
          "text": "I'm looking for the same thing, but I haven't had good results. My machine isn't that powerful, and I'd like to know what you found.",
          "score": 1,
          "created_utc": "2026-02-06 01:30:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3utke6",
          "author": "Wonderful6014",
          "text": "Air-gapped deployments are underrated. For classified/secure environments, model selection really matters - you need something that performs well without needing constant fine-tuning or external calls. I've found Mistral-7B-Instruct and Llama-3-8B work well at Q4 quantization for most tasks. The key is pre-loading all your embeddings and context docs before going offline so you're not missing RAG data.",
          "score": 1,
          "created_utc": "2026-02-06 05:53:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o30dz6n",
          "author": "r00tdr1v3",
          "text": "How are you getting the models on the air-gapped hardware?",
          "score": 0,
          "created_utc": "2026-02-01 18:09:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o317ok3",
              "author": "thefilthybeard",
              "text": "Pre-built installation package transferred via approved media. The client's sysadmin handles the actual install on their air-gapped network while I'm on-site to walk through the process and verify everything works.\n\nFor updates, they pull them from a secure client portal - no direct internet connection needed on their end. They download the update package from the portal on an unclassified system, transfer it in through their normal media approval process, and apply it.",
              "score": 2,
              "created_utc": "2026-02-01 20:27:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2wtkek",
          "author": "CooperDK",
          "text": "Just fully firewall it. *sigh*\nThat said, most models cannot access the internet and those who can, do because you tell them to",
          "score": -7,
          "created_utc": "2026-02-01 03:44:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xrm93",
              "author": "tom-mart",
              "text": "I think you misunderstanding the issue. Firewall will not help if you are voluntarily sending your sensitive data to LLM api\n\n\nhttps://www.bbc.co.uk/news/articles/cdrkmk00jy0o\n\n\nOthers are not any more secure.",
              "score": 2,
              "created_utc": "2026-02-01 08:09:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2yl44k",
                  "author": "CooperDK",
                  "text": "No, but you do not do that if you want to protect your information. Then you inference locally.\nI do think the OP mentioned NOT using an API.",
                  "score": -1,
                  "created_utc": "2026-02-01 12:32:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2xbqs6",
              "author": "trolololster",
              "text": "no model can \"access the internet\" without you specifically giving it a tool to do so. wth have you been smoking, lol.",
              "score": 2,
              "created_utc": "2026-02-01 05:52:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ylbcr",
                  "author": "CooperDK",
                  "text": "You do know that we, according to the OP, are talking local LLMs, right?",
                  "score": 1,
                  "created_utc": "2026-02-01 12:34:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2y1c0f",
          "author": "UnbeliebteMeinung",
          "text": "Wtf are you classifying that you need a air gapped computer.",
          "score": -4,
          "created_utc": "2026-02-01 09:40:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y8jvt",
              "author": "tecneeq",
              "text": "Thats classified.",
              "score": 6,
              "created_utc": "2026-02-01 10:46:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2y8rvr",
                  "author": "UnbeliebteMeinung",
                  "text": "Probably some 3 Body Problem stuff where the enemy is an allmyghty alien.",
                  "score": 1,
                  "created_utc": "2026-02-01 10:48:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qu4ka4",
      "title": "Recommendations for a good value machine to run LLMs locally?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qu4ka4/recommendations_for_a_good_value_machine_to_run/",
      "author": "onesemesterchinese",
      "created_utc": "2026-02-02 19:14:51",
      "score": 60,
      "num_comments": 64,
      "upvote_ratio": 0.97,
      "text": "Thinking of purchasing a machine in the few thousand $ range to work on some personal projects. Would like to hear if anyone has any thoughts or positive/negative experiences running inference with some of the bigger open models locally or with finetuning? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qu4ka4/recommendations_for_a_good_value_machine_to_run/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o37ioct",
          "author": "ZeroSkribe",
          "text": "Look at rtx 5060ti 16GB, get two if you can",
          "score": 24,
          "created_utc": "2026-02-02 19:23:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37lets",
              "author": "v01dm4n",
              "text": "Two would also need 2x expensive motherboard. Better to get a gpu with more vram with that budget. Say rtx pro 4000 with 24g vram.",
              "score": -4,
              "created_utc": "2026-02-02 19:35:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3aavni",
                  "author": "Big-Masterpiece-9581",
                  "text": "There is no reasonable priced new gpu with 24gb. New Intel b60 or old ass Nvidia RTX 3090 used for \nmining are both $800-900. For that price can get 32gb with two new 5060ti and for inference speeds will be fine.",
                  "score": 4,
                  "created_utc": "2026-02-03 04:14:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o38xglb",
                  "author": "ZeroSkribe",
                  "text": "no you wouldn't, even the cheapest motherboards come with 2 connectors, what you meant to say was you'll need a bigger power supply..duhhhhhh",
                  "score": 2,
                  "created_utc": "2026-02-02 23:30:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o38q7sl",
                  "author": "windumasta",
                  "text": "\"2x une carte mère chère\" ? si non une carte mère avec 2 pcie 8x ?",
                  "score": 0,
                  "created_utc": "2026-02-02 22:51:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37mqsd",
          "author": "photobydanielr",
          "text": "The Ryzen ai max 395+ 128GB shared memory blah blah named machines come to mind. Good value if the models you want to use fit.",
          "score": 9,
          "created_utc": "2026-02-02 19:42:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37roo3",
              "author": "cjc4096",
              "text": "Rocm 7.2 fixed a lot of stability issues on 395+ too.",
              "score": 3,
              "created_utc": "2026-02-02 20:05:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37s30n",
                  "author": "photobydanielr",
                  "text": "I own the flow z13 128GB and boy has it been a wild ride",
                  "score": 5,
                  "created_utc": "2026-02-02 20:07:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3bsyf3",
              "author": "[deleted]",
              "text": "I have the framework desktop and it was the best purchase for me. 128GB ram can do wonders for huge context, but the prompt processing is slower than models that run on better GPUs. For me because i write code slower than these models, works perfectly for me",
              "score": 2,
              "created_utc": "2026-02-03 11:58:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37o98j",
          "author": "band-of-horses",
          "text": "You can spend thousands of dollars on a machine that can run a model that is still a fraction as capable as any of the cloud hosted models. If your motivation is to save money it's probably not worth it. If it's privacy there are cloud options for open models with no retention. If you just want to run small models locally for fun a cheap machine with 32 of ram or a 16gb c video card will do it.",
          "score": 16,
          "created_utc": "2026-02-02 19:49:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37wvto",
              "author": "mtbMo",
              "text": "That’s right. Your local LLM instance will never ever compete with a trillion params models from cloud inference provider.\nYou can run local models and achieve good results.\n\nI’m running a Dell workstation Xeon v4 256gb ram and three GPUs with 50GB VRAM all second hand\nIt’s up to 37t/s with drawing 500W from the wall",
              "score": 4,
              "created_utc": "2026-02-02 20:30:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37z6ji",
                  "author": "mon_key_house",
                  "text": "What GPUs do you have? I currently have 2x Rtx3060 and thinking about going 4x or getting 2x rtx 3090.",
                  "score": 1,
                  "created_utc": "2026-02-02 20:40:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o383h04",
              "author": "BoostedHemi73",
              "text": "This is so depressing. We’re just supposed to be beholden to cloud companies for this stuff? That’s going to push everyone into like three places.\n\nComputers used to be so fun. This timeline sucks.",
              "score": 3,
              "created_utc": "2026-02-02 21:01:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3868lu",
                  "author": "goodguybane",
                  "text": "Running local LLMs IS fun, you just have to manage your expectations.",
                  "score": 9,
                  "created_utc": "2026-02-02 21:14:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3bxwhj",
                  "author": "SirWinstonSmith",
                  "text": "Plenty of technology has been very computationally expensive until it suddenly wasn't. What has changed is that you now get a sneak peek of the future through cloud services, but that future will eventually be optimized. Most likely there will be an innovation or new architecture that severely reduces the computational expense. And even if there isn't, current open source models are more than good enough to do most simple tasks on a limited context window.",
                  "score": 1,
                  "created_utc": "2026-02-03 12:34:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3cc928",
                  "author": "luncheroo",
                  "text": "Smaller local LLMs are currently are currently at GPT-3.5 or GPT-4, so a couple of years behind frontier. I'm not expecting them to surpass frontier in real time, but over time the trend is that they get better and better at a pretty fast clip ",
                  "score": 1,
                  "created_utc": "2026-02-03 14:00:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3f0cjd",
                  "author": "mpw-linux",
                  "text": "They still are fun, local models give one a great learning experience that surprisingly powerful, give it a try. Most AI developers work on local machines before porting to the big iron.",
                  "score": 1,
                  "created_utc": "2026-02-03 21:33:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ezy6i",
              "author": "mpw-linux",
              "text": "Of course you are correct but for experimentation and getting the code right before using paid cloud services local development is fine. Apples's M chips are great for developing AI applications.",
              "score": 1,
              "created_utc": "2026-02-03 21:32:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37liwz",
          "author": "AmphibianFrog",
          "text": "I'm not convinced there is a good value machine to run LLMs locally! If I was on a budget I would build a pc with a single RTX3090.",
          "score": 5,
          "created_utc": "2026-02-02 19:36:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37kn8l",
          "author": "germanpickles",
          "text": "You can consider getting a Mac to run local LLM’s. Mac’s have something called Unified Memory where the vRAM and system RAM are shared on the same bus. So if you have 64 GB of Unified Memory, you will be able to load a lot of different local models. While Ollama is a great tool, you can also look at LM Studio which has a MLX inference engine which allows you to run MLX LLM’s very fast on a Mac.",
          "score": 16,
          "created_utc": "2026-02-02 19:32:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38rnlo",
              "author": "n2itus",
              "text": "This is really good advice.  I've just started down this journey and was surprised as how well my Macbook Air with 16gb ran local models.  I started looking at mac minis with 64 gb ... as memory is very important. You could not get 64 gb from video cards for as cheap as getting a 64 gb mac mini.",
              "score": 9,
              "created_utc": "2026-02-02 22:58:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o37mc0j",
              "author": "onesemesterchinese",
              "text": "Nice, I was looking at the mac mini -- You think that would work for finetuning some of the smaller models?",
              "score": 2,
              "created_utc": "2026-02-02 19:40:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o37rl6l",
                  "author": "st0ut717",
                  "text": "I am running Gemma 3b.  On my Mac air m4 with 16gb ram.",
                  "score": 6,
                  "created_utc": "2026-02-02 20:04:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3b4a3g",
                  "author": "arqn22",
                  "text": "I believe that fine tuning is especially slow on Mac chips, but you should verify that.  I have a 64gb MacBook pro and it can run a lot of models at decent speeds as long as context window isn't too high, not over 16k-32k (or an advanced attention mechanism is in place like in nemotron-nano-3 which easily handles 128 or probably more).  It's still significantly slower than GPUs with less RAM for models that fit in them though.  And I believe I've read that it's extra slow for training/fine-tuning.  But I don't have a source for that.",
                  "score": 3,
                  "created_utc": "2026-02-03 08:11:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o38c77n",
                  "author": "germanpickles",
                  "text": "Absolutely. Do check out Alex Ziskind on YouTube - https://youtube.com/@azisk?si=Q7cXou-ONnH0IP-P, he has so many videos on running local LLM’s on Mac as well as DGX Spark etc",
                  "score": 2,
                  "created_utc": "2026-02-02 21:42:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o39s626",
              "author": "rorowhat",
              "text": "Lol don't. If anything get a strict halo",
              "score": -1,
              "created_utc": "2026-02-03 02:21:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o38xv46",
              "author": "ZeroSkribe",
              "text": "the mac mini slop is getting old, please research GPU memory speeds vs unified speeds. Its not worth it if every request takes forever.",
              "score": -8,
              "created_utc": "2026-02-02 23:32:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o39fhsr",
                  "author": "germanpickles",
                  "text": "Everyone's experience with speed on a Mac Mini vs GPU will really depend on the specs of the machine, the inference engine, quantization and the model. From my own real world experience, using local AI every day, I'm getting 104 tok/sec. Here are the specs:\n\n* Mac Mini with M4 Pro\n* 14 core CPU/20 core GPU\n* 64GB UMA (Unified Memory Architecture)\n* Llama 3.2 3B 4-bit MLX\n\nRunning the same model (Q4\\_K\\_M) on my RTX 5080, I get 276 tok/sec. Is it faster? Yes. But is the Mac still usable? Of course.\n\nNow of course, if someone is running the entry level Mac Mini and loading a 70 billion parameter model, it will be extremely slow.",
                  "score": 3,
                  "created_utc": "2026-02-03 01:09:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o39q42d",
          "author": "8ballfpv",
          "text": "I just built myself a machine to mess around with:\n\n\\- Chassis - supermicro SYS-7049GP-TRT\n\n\\- CPU - Dual Intel Xeon Gold 6240\n\n\\- Ram - 165GB DDr4\n\n\\- GPU - Dual rtx 3090\n\nplaying around with various models atm.  its nice to have the flexability with the 2 gpus. Means I can run a 70b size or 2 30b sizes ( 1 on each gpu) or many smaller ones depending what I want.  \nGot the hardware piping out to a grafana dashboard so I can keep an eye on the basics realtime, Run openwebui on another machine and Ollaman for quick maintenance of models etc etc. All up cost me about $5k aud.",
          "score": 4,
          "created_utc": "2026-02-03 02:09:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37hoew",
          "author": "DutchGM",
          "text": "I am currently testing the Dell GB10 spark. I see potential but I’m still benchmark testing it.",
          "score": 3,
          "created_utc": "2026-02-02 19:18:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37ws4q",
          "author": "st0ut717",
          "text": "Large models are not always the answer. \nWhat do you want to do?  \n\n\nMac mini if you want a general purpose pc that can also do llm \nOtherwise a dell gb10.  Or the like if you want a dedicated llm work station",
          "score": 3,
          "created_utc": "2026-02-02 20:29:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o382b5w",
          "author": "Possible-Algae4391",
          "text": "An older X99 workstation board, a cheap Xeon CPU, 2x3060 GPU (2x12GB)\n\n\nIt was enough for me, I could run 30b models with reasonable speed (still far from what we're used with cloud models). It cost me about 1000$ with used parts.\n\n\nYou can go for beefier 3090s but then you might want a newer board with PCIe 4.",
          "score": 3,
          "created_utc": "2026-02-02 20:55:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38ybep",
          "author": "dipoots_",
          "text": "MBP Pro M5 at least 24 gb ram or Mac mini equivalent",
          "score": 2,
          "created_utc": "2026-02-02 23:34:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o394rv9",
          "author": "XdtTransform",
          "text": "These are all fine suggestions, but if you are planning to do something serious, you need, at the very least, a ~30b type model.  Something like Gemma3:27b.  This means at least a card with 24GB of VRAM.  \n\nThe other alternatives I've seen suggested like various 3b models fall apart at anything serious or with a larger context window.",
          "score": 2,
          "created_utc": "2026-02-03 00:10:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3a5rdi",
          "author": "recoverygarde",
          "text": "I would recommend get either a m5 14in MBP or wait for the rest of the lineup to get the M5 gen",
          "score": 1,
          "created_utc": "2026-02-03 03:41:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3blcrx",
          "author": "Financial-Source7453",
          "text": "If size & power consumption matter, go for Asus Aspire Gx10 (so far the cheapest clone of Nvidia Spark). It's a palm sized box filled with 128GB unified memory and 20x core ARM CPU. It's also Nvidia box, so you will be able to run almost everything from the local Ai world with acceptable speed. Macs suck at video/image generation, dedicated GPUs are noisy and eat a lot of space and electricity. AMD Strix Halo systems have lower performance and often come with HW issues.",
          "score": 1,
          "created_utc": "2026-02-03 10:55:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rd70w",
              "author": "drivebyposter2020",
              "text": "I would think or hope that the Strix Halo systems have hardware issues that depend upon how cheap a vendor you went with. there must be 50 of them on the market by now. back on Black Friday in the US last year I found I could get an HP maxed out for about $300 more than the mystery meat brands. so worth it. I'm not sure how the relative pricing is now, but watch for sales and HP coupon codes online. I managed to knock 10% off by just stumbling on to the right coupon code on one of those silly coupon code websites.\n\n\nI'm still just scratching the surface because my own first level interests actually lie in things like MCP and orchestration and the middleware of how you tie all this stuff together, but as I get more serious about the AI part, I suspect that this box will continue to do the heavy lifting for me in a way that I find more than acceptable, even though I know it's not exactly one of the big cloud systems.",
              "score": 1,
              "created_utc": "2026-02-05 18:24:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ez0s7",
          "author": "mpw-linux",
          "text": "Get one of those Macbook pro: M1,M2,M3,M4 with at least 32g ram. The Mlx framework is great designed for the Silicon chip. There are many models converted to Mlx on huggingface. I recently bought a Macbook pro 16 M1 with 32g to run lots local models.",
          "score": 1,
          "created_utc": "2026-02-03 21:27:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3a9y18",
          "author": "m-gethen",
          "text": "I just built a really solid budget-constrained PC for a friend to use for a) 3D CAD in Solidworks and Blender and b) so he could start down the local LLM path. He’s been very happy so far.\n\nhttps://preview.redd.it/h0fowxpfe7hg1.jpeg?width=4624&format=pjpg&auto=webp&s=09669791be012602f9e57f633cc67e4d5961a062\n\nKey specs…\n\n* Intel Core Ultra 5 245K\n* Gigabyte Z890M motherboard (automatically bifurcates and runs GPUs at PCIE 8)\n* 32Gb DDR5-6400 memory\n* 2x Intel Arc B580 12Gb (pooled for total 24Gb VRAM).\n\nOllama and LM Studio working smoothly with Vulcan and newest 32B MoE models like Granite 4 H in this set up.",
          "score": 1,
          "created_utc": "2026-02-03 04:07:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39e4p0",
          "author": "Seninut",
          "text": "Follow Jenson around with a pooper scooper, I am sure one will come out.",
          "score": -3,
          "created_utc": "2026-02-03 01:01:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs233a",
      "title": "Best open weight llm model to run with 8gb of vram",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qs233a/best_open_weight_llm_model_to_run_with_8gb_of_vram/",
      "author": "Sweazou",
      "created_utc": "2026-01-31 13:16:18",
      "score": 57,
      "num_comments": 40,
      "upvote_ratio": 0.94,
      "text": "I'd like to get your thought on the best model you can use with 8gb of vram in 2026, with the best performance possible for general purpose and coding, the least censorship possible, i know this won't be as good as state of the art llm but i'd like to try something good i can run locally",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qs233a/best_open_weight_llm_model_to_run_with_8gb_of_vram/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o2snxjt",
          "author": "alhinai_03",
          "text": "Gpt-oss 20b, while offloading experts to ram and keeping routers and kv cache in vram. It would run pretty fast.",
          "score": 20,
          "created_utc": "2026-01-31 14:45:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2tr0u6",
              "author": "Admirable-Choice9727",
              "text": "Yup",
              "score": 1,
              "created_utc": "2026-01-31 17:57:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2tu2ni",
              "author": "Daepilin",
              "text": "do you have a starting point on setting that up? I started some local stuff with Ollama and CrewAI, but so far have not seen a way to do things like offloading the experts, etc",
              "score": 1,
              "created_utc": "2026-01-31 18:11:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2uogh8",
                  "author": "alhinai_03",
                  "text": "As the other person said, you gonna need llama.cpp, download prebuilt binaries or build them yourself.\n\nRun with these flags\n```\n--n-gpu-layers -1\n--cpu-moe\n\nyou also can tweak this further with\n--n-cpu-moe N\n```",
                  "score": 3,
                  "created_utc": "2026-01-31 20:37:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2udi6b",
                  "author": "Zestyclose-Shift710",
                  "text": "You do that with llama.cpp and setting the correct arguments, --cpu-moe specifically \n\n\nMaybe ollama can do that too but idk\n\n\n\nArguably the new router mode, webui and models.ini config style of llama.cpp make ollama obsolete",
                  "score": 2,
                  "created_utc": "2026-01-31 19:43:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2tr6bn",
              "author": "Admirable-Choice9727",
              "text": "But kv cache causes context rot In my experience",
              "score": -2,
              "created_utc": "2026-01-31 17:57:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2uqgw1",
                  "author": "alhinai_03",
                  "text": "You're not making sense, If you meant running quantized kv cache this is not it. Kv cache quantization is done with ```--cache-type-k q8_0``` which you don't need it unless you're really tight on vram.",
                  "score": 5,
                  "created_utc": "2026-01-31 20:46:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ud51y",
                  "author": "Zestyclose-Shift710",
                  "text": "What",
                  "score": 4,
                  "created_utc": "2026-01-31 19:41:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2t1b0a",
          "author": "abalancer",
          "text": "I have found ministral-3:8b to be pretty good all around, it's fast, not very censored (although some queries are blocked), has good image recognition, can read pdfs (ministral-3:14b will read them more thoroughly), and gives some of the best answers for 8 billion and 14 billion parameters. I used to use qwen3:14b for most of my tasks but ministral-3:8b has largely taken that role now.",
          "score": 7,
          "created_utc": "2026-01-31 15:53:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sc4j7",
          "author": "sinan_online",
          "text": "I am running Gemma3 1B on 6GB VRAM. Contender is the equivalent Qwen3, but for now I am settled on Gemma3. (It is also general purpose for me, I actually test some of my flows, it’s integrated to one oy my test harnesses.)\n\nEdit: Gemma3 4B was too large for the VRAM.",
          "score": 7,
          "created_utc": "2026-01-31 13:36:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2udlma",
              "author": "Zestyclose-Shift710",
              "text": "Isn't 4b around 1gb quantized",
              "score": 1,
              "created_utc": "2026-01-31 19:43:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2uuc4h",
                  "author": "sinan_online",
                  "text": "I used Ollama, I didn’t check the precision of the floating points when I pulled the Qwen3 models. They are both the standard on Ollama.\n\nNow that llama has a standard API support, I am making a switch, and that will allow me to be a bit more selective with that, so long as there is a GGUF. I’ll see what fits in, I am starting with Gemma3 270m to get the containerization pipeline going.",
                  "score": 1,
                  "created_utc": "2026-01-31 21:06:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2u1ock",
          "author": "jimmyfoo10",
          "text": "For me I got 8gb vram and 32 gb ram cpu. I find really nice performance/quality in qwen2.5 14b and deepseek 14b\nGTP-oss 20b is nice but quite heavy \nI find sweet spot around 14b parameters \nAnd better performance if I use from other computer, when using desktop and running openwebui in the same computer is a bit laggy…",
          "score": 6,
          "created_utc": "2026-01-31 18:46:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sox5e",
          "author": "seangalie",
          "text": "Chiming in to say that your best bets are some of the new MoE models.... gpt-oss:20b will likely impress with performance even with 8GB VRAM but you may not love the censorship based on your post.  You can always YOLO and use huihui AI's abliterated gpt-oss:20b for the uncensored version.\n\nOther than that - Qwen3's 30b-a3b architecture works surprisingly well in VRAM constraints if you have the system RAM for the spillover (which would include qwen3-coder in the same sizing).  Nemotron 3 Mini is only a little bit larger of a foot print and may fit in the same VRAM constraints.",
          "score": 4,
          "created_utc": "2026-01-31 14:50:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tt5su",
          "author": "ykushch",
          "text": "It is depends what you’re planning to do with it. I tested my extension with qwen2.5-coder:7b - works fine. Tried to use 'qwen3:8b' and it was a lot slower especially on my machine comparing to 'qwen2.5-coder:7b'. Quality-wise 'qwen3:8b' follows instructions a bit better, but making it a default one the experience will look less pleasant. Especially for such extension like this one \n\nhttps://github.com/ykushch/ask",
          "score": 1,
          "created_utc": "2026-01-31 18:07:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2uc2cw",
          "author": "swipegod43",
          "text": "something with 10-12b parameters would be decent",
          "score": 1,
          "created_utc": "2026-01-31 19:36:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2uc3g6",
              "author": "swipegod43",
              "text": "with 4 bit quantization",
              "score": 1,
              "created_utc": "2026-01-31 19:36:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2uecma",
          "author": "Zestyclose-Shift710",
          "text": "Either 7-8b class models in vram, 12-14b splitting them between ram and vram, or 20-30b moes with expert offloading and kv in vram\n\n\nAs for specific models, check out latest qwens (3VL), ministral 3 series, granite 4 for rag, and qwen3 30b a3b, nemotron 3 nano, gpt oss 20b and glm 4.7 flash as for the moe models\n\n\nThose last ones have pruned REAP variants that are a bit smaller, too",
          "score": 1,
          "created_utc": "2026-01-31 19:47:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ug0ho",
          "author": "Dzhmelyk135",
          "text": "Either qwen3 in 4/8b or if you can find it (I haven't) GPT-OSS 20b in like Q3 or even Q2 (yes they make them, I have Qwen3Coder 30b on a 16 GB GPU)",
          "score": 1,
          "created_utc": "2026-01-31 19:55:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2x485c",
          "author": "No_Cartographer7065",
          "text": "Made GPT-OSS 0.6b parameter model on hugging face just make agentic loop iterate through one pass. It comes with built in web search too.",
          "score": 1,
          "created_utc": "2026-02-01 04:56:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yif88",
          "author": "Potential_Code6964",
          "text": "I'm currently running nemotron-3-nano:30b with a 3060TI, Ryzen 7, 64GB.",
          "score": 1,
          "created_utc": "2026-02-01 12:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3157a1",
          "author": "CorrGL",
          "text": "Ministral, it comes in various sizes, see the biggest you can fit",
          "score": 1,
          "created_utc": "2026-02-01 20:15:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ge5ww",
          "author": "gamesta2",
          "text": "Qwen3:4b",
          "score": 1,
          "created_utc": "2026-02-04 01:56:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iga81",
          "author": "BeginningPush9896",
          "text": "Right now i use a Qwen3 8b-vl for OCR and context text generation. Its work very well, of course, if i trying to run more then 2 Client at the same time, it will be slow, but for experiment and testing theory, acceptable. Also using a custom tools, super simple, with SQLite, works pretty good.",
          "score": 1,
          "created_utc": "2026-02-04 11:22:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtine0",
      "title": "Released: VOR — a hallucination-free runtime that forces LLMs to prove answers or abstain",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qtine0/released_vor_a_hallucinationfree_runtime_that/",
      "author": "CulpritChaos",
      "created_utc": "2026-02-02 02:31:03",
      "score": 47,
      "num_comments": 21,
      "upvote_ratio": 0.86,
      "text": "I just open-sourced a project that might interest people here who are tired of hallucinations being treated as “just a prompt issue.”\nVOR (Verified Observation Runtime) is a runtime layer that sits around LLMs and retrieval systems and enforces one rule:\nIf an answer cannot be proven from observed evidence, the system must abstain.\nHighlights:\n0.00% hallucination across demo + adversarial packs\nExplicit CONFLICT detection (not majority voting)\nDeterministic audits (hash-locked, replayable)\nWorks with local models — the verifier doesn’t care which LLM you use\nClean-room witness instructions included\nThis is not another RAG framework.\nIt’s a governor for reasoning: models can propose, but they don’t decide.\nPublic demo includes:\nCLI (neuralogix qa, audit, pack validate)\nTwo packs: a normal demo corpus + a hostile adversarial pack\nFull test suite (legacy tests quarantined)\nRepo: https://github.com/CULPRITCHAOS/VOR\nTag: v0.7.3-public.1\nWitness guide: docs/WITNESS_RUN_MESSAGE.txt\nI’m looking for:\nPeople to run it locally (Windows/Linux/macOS)\nIdeas for harder adversarial packs\nDiscussion on where a runtime like this fits in local stacks (Ollama, LM Studio, etc.)\nHappy to answer questions or take hits. This was built to be challenged.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qtine0/released_vor_a_hallucinationfree_runtime_that/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3353yy",
          "author": "newbietofx",
          "text": "What's the difference between this and doing promoting to force llm to answer i don't have the answers versus expanding the pipeline to allow rag or integrating scraping function to pull from the internet? ",
          "score": 8,
          "created_utc": "2026-02-02 02:36:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o335f0e",
              "author": "CulpritChaos",
              "text": "Short answer:\nThose approaches try to encourage the model not to hallucinate. VOR removes the model’s authority to answer at all unless the answer is provably grounded.\nLonger, concrete difference:\nPrompting “say I don’t know”\nThis is behavioral.\nThe model still decides whether it “knows” something.\nFailure mode: confident-sounding abstention or quiet fabrication.\nYou can’t audit why it answered or abstained — only that it did.\nVOR doesn’t trust the model’s self-assessment. The model can propose an answer, but it cannot finalize it.\nExpanding the pipeline (RAG, scraping, tools)\nRAG increases information availability, not correctness.\nScraping just widens the attack surface.\nThe model still performs the final reasoning step and can:\nMix sources\nMisattribute evidence\nHallucinate joins or relationships\nMost RAG systems fail silently — you don’t know when they’re wrong.\nVOR treats retrieval as observations, not truth.\nIf retrieved evidence is insufficient, conflicting, or ambiguous → ABSTAIN, even if the model “thinks it knows.”\nWhat VOR actually changes\nThe model is demoted to a proposer, not an authority.\nAll answers must:\nBe derivable from observed evidence\nPass conflict checks\nBe replayable and auditable\nOtherwise the system returns:\nABSTAIN or CONFLICT — explicitly\nThis is closer to a runtime governor than a prompting or RAG trick.\nMental model:\nPrompting = asking the driver to be careful\nRAG = giving the driver more maps\nVOR = installing brakes that physically prevent lying\nHappy to go deeper if you want — especially around failure modes RAG can’t detect.",
              "score": 5,
              "created_utc": "2026-02-02 02:38:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o336tq2",
                  "author": "newbietofx",
                  "text": "It sounds good that you have a bull sh*it filter. How do you ensure the filter is collecting dust or stopping air flow altogether? I don't have a metaphor for this analogy. ",
                  "score": 7,
                  "created_utc": "2026-02-02 02:46:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o33jkoi",
                  "author": "Outrageous-Split-646",
                  "text": "Can you answer the question without relying on an LLM?",
                  "score": 4,
                  "created_utc": "2026-02-02 04:02:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33uxux",
          "author": "ExtentOdd",
          "text": "I read through the [README.md](http://README.md) and I think it would be best if you could include the result or video of the demon on the README. Although this is potential, It could not justify cloning this repo and trying to run before I understand what it does.",
          "score": 4,
          "created_utc": "2026-02-02 05:19:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33v80w",
              "author": "CulpritChaos",
              "text": "No problem! Check back tomorrow if you'd like and I'll have it resolved by then. Thank you for your feedback!",
              "score": 2,
              "created_utc": "2026-02-02 05:21:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o33bs69",
          "author": "Intercellar",
          "text": "great work!!",
          "score": 3,
          "created_utc": "2026-02-02 03:14:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33bv3m",
              "author": "CulpritChaos",
              "text": "Thank you!",
              "score": 2,
              "created_utc": "2026-02-02 03:15:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3de5j4",
          "author": "Electronic-Yak-7270",
          "text": "Looks interesting in terms of practical application. Before trying it out,  willl wait for a demo video that shows an end-to-end scenario, including a comparison of results before and after using VOR. It would also be helpful to see a real use case to demonstrate the benefits, especially for non-tech users. A simple example in the GitHub README.md would be great as well.",
          "score": 1,
          "created_utc": "2026-02-03 17:05:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3di4d2",
              "author": "CulpritChaos",
              "text": "Thanks! Great suggestions as I do need to get a video out with better proofs for those curious to see without running. Also a kind Reddit user/Programmer Greg Randall helped me add a great easy explanation to the readme. \n\n# How VOR Fixes AI Mistakes\n\nNeuraLogix stops AI from making errors using a system we call the **Truth Gate**.\n\n# The Problem: AI Guesses\n\nAI tools often make mistakes. They guess which word comes next in a sentence, but they do not check if the words are true. They sound sure of themselves even when they are wrong.\n\n# The Solution: The Truth Gate\n\nVOR acts like a filter. The AI must prove a statement is true before it speaks. It works in three steps:\n\n# 1. The Facts\n\nFirst, we give VOR a list of true things.\n\n* **Fact A:** Alice is Bob's mother.\n* **Fact B:** Bob is Charlie's father.\n\n# 2. The Claim\n\nThe AI wants to say something new based on those facts.\n\n* **Claim:** \"Alice is Charlie's grandmother.\"\n\n# 3. The Check\n\nVOR looks at the facts. It checks if the facts link together to support the claim.\n\n* **VOR asks:** Is there a path from Alice to Bob? Is there a path from Bob to Charlie?\n* **Answer:** Yes.\n* **Result:** The statement is **Verified**. The AI allows the text.\n\n# When the AI is Wrong\n\nWhat happens if the AI tries to say: *\"Alice is Dave's grandmother\"*?\n\n* **VOR asks:** Do facts link Alice to Dave?\n* **Answer:** No.\n* **Result:** The statement is **Rejected**. VOR stops the AI from saying it.",
              "score": 1,
              "created_utc": "2026-02-03 17:24:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qvycjs",
      "title": "Just started using local LLMs, is this level of being wrong normal?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qvycjs/just_started_using_local_llms_is_this_level_of/",
      "author": "Cempres",
      "created_utc": "2026-02-04 19:31:52",
      "score": 46,
      "num_comments": 75,
      "upvote_ratio": 0.83,
      "text": "https://preview.redd.it/4swbpymp4jhg1.png?width=746&format=png&auto=webp&s=16d362a6bd68f24a9b349d8f0f21aec95376aa11\n\nTried using the glm4.7 flash version, and just randomly asking it stuff, and i got his with this trip, is that normal to expect and can someone explain why this even happens? lack of internet connection? Should I expect this from other models as well? \n\nMy regular usage wouldn't be this, but a tool to help me create D&D stuff, such as character ideas, helping me when i get stuck ecetera, any model to recommend for such? \n\nSpecs:  \nwin 11  \n32gb RAM  \nXTX 24gb VRAM",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qvycjs/just_started_using_local_llms_is_this_level_of/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3l3wsb",
          "author": "InfraScaler",
          "text": "Yup. Now online models have tools to fetch data from the web, but yeah hallucination is relatively common. The LLM tries to fill knowledge gaps and it just makes up stuff to accomplish that.",
          "score": 45,
          "created_utc": "2026-02-04 19:35:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l86nc",
              "author": "Cempres",
              "text": "Then i'm missing the point of local LLMs, kinda..",
              "score": 2,
              "created_utc": "2026-02-04 19:56:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3la92q",
                  "author": "Dubious-Decisions",
                  "text": "I think you are missing the point of LLMs in general. They are not definitive sources of fact. They are chat simulators. They are designed to predict the most reasonable, statistically likely series of tokens that should be emitted in response to all of their inputs. They have no ability to self-assess or fact check or even understand the semantics of what you are giving them as input.\n\nSo no, you shouldn't expect them to be accurate. The larger the model, the larger the training set and the more likely it is to match some piece of human-created information it was trained on. But as models shift to training with synthetic data and output from other models, you can imagine what happens to accuracy.\n\nWhat they ARE is reasonably inexpensive, simple to interact with natural language processors (NLPs) that can act as some reasonably reliable glue between users and tools. It's a gross oversimplification to say they can't produce useful results, but it's also incorrect to assume that they will ever be foolproof. There are certain types of interactions that they simply will never be able to perform.\n\nThere are many other types of AI and machine learning (ML) platforms besides LLMs, and the tragedy lately is that no one seems to remember that and instead are trying to build the entire AI house with nothing but a LLM hammer.\n\nOnce you realize that these are just tools for parsing human text inputs and producing some compelling, simulated text outputs, you will be better suited to figure out what they are and are not good for.",
                  "score": 83,
                  "created_utc": "2026-02-04 20:06:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3l8stl",
                  "author": "LePfeiff",
                  "text": "An LLM doesnt need to know alot of facts to be a DM assistant in DnD, but its also just a consequence of having to run smaller quantized models that fit on consumer hardware",
                  "score": 5,
                  "created_utc": "2026-02-04 19:58:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lbupi",
                  "author": "sinan_online",
                  "text": "I think they work best for fairly technical hobbyists. For instance, I am combining repeated LLM calls with some Python code to find a good way to get structured markdown out of PDF books. In other words, it's a specific use case of improving OCR quality.\n\nYou can also use them for classification tasks, etc... I personally do not have any cases that do not involve some programming or technical knowledge.\n\n",
                  "score": 3,
                  "created_utc": "2026-02-04 20:13:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lmray",
                  "author": "zenmatrix83",
                  "text": "if you want proper research you can likely try something like this [https://github.com/langchain-ai/local-deep-researcher](https://github.com/langchain-ai/local-deep-researcher) . a deep research look is doing a websearch, the ai gets a finding from the research report,  and then does a loop looking up answers trying to create a report.",
                  "score": 2,
                  "created_utc": "2026-02-04 21:05:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m8301",
                  "author": "Medium_Ordinary_2727",
                  "text": "To be clear all LLMs will do this to a certain extent. Some are better than others bit they can still hallucinate.\n\nFrame your prompt not as a question that you are asking the LLM, but as a research request, and give it the tools to do that research (such as web access).",
                  "score": 2,
                  "created_utc": "2026-02-04 22:50:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3mk2e4",
                  "author": "Western_Courage_6563",
                  "text": "Think about them as an engine, they need rest of the stuff to work (memory, Internet aces, local database, etc).",
                  "score": 2,
                  "created_utc": "2026-02-04 23:55:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3llgoe",
                  "author": "zenmatrix83",
                  "text": "to keep it simple llm means large language model or its a model of how a language is.  This is a decent video on how it works [https://www.youtube.com/watch?v=wjZofJX0v4M](https://www.youtube.com/watch?v=wjZofJX0v4M) .  Its doesn't know whats its saying, its a statistically correct response, which alot of time is correct, but not always. The lower power the model the more it will be statistically wrong.",
                  "score": 2,
                  "created_utc": "2026-02-04 20:59:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m16qs",
                  "author": "florinandrei",
                  "text": "More complex tasks - use bigger models.\n\nSimpler tasks - smaller models are okay.\n\nUnless you have your own dedicated inference hardware, you're not going to run big models at home.\n\nThe definitions of \"simpler\" and \"more complex\" will continue to evolve over time.",
                  "score": 1,
                  "created_utc": "2026-02-04 22:15:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3mv6sa",
                  "author": "oVerde",
                  "text": "If you use it in an agentic environment, like OpenCode, you then can provide tools to fetch from the internet",
                  "score": 1,
                  "created_utc": "2026-02-05 00:56:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3nzbs1",
                  "author": "fasti-au",
                  "text": "Cintext is a bucket of things it can juggle to find the 8/16/32k it can actually relate to your question that’s what is juggled by 1 shit then think deals with one shot output to decide if it was good. \n\nSo how you fill that first one shot is the key to getting this broken binary system to work.  Ternerary fixes it but it’s not really hardware friendly yet.  Next round of llm with bee new chips if we are transforming but big fusion seems more in line with the in between steps",
                  "score": 1,
                  "created_utc": "2026-02-05 04:59:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3v5w92",
                  "author": "andrew_barratt",
                  "text": "Think of it less like a huge knowledgeable system, more like a software component that can read",
                  "score": 1,
                  "created_utc": "2026-02-06 07:39:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3m49wm",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-02-04 22:30:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3m91wk",
                  "author": "InfraScaler",
                  "text": "If you're offline you can't fetch data. Local does not mean offline. Thing is, if you want it to be online, you need to give it the tools to fetch data.",
                  "score": 2,
                  "created_utc": "2026-02-04 22:55:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3m36jm",
          "author": "themaskbehindtheman",
          "text": "That's why web search and tools are a thing, the tools are a big reason these companies still hire software engineers. \n\nMonitor chats > find commonly asked questions where an incorrect answer is given > write tool > claim sentience /s\n\nIf you whack openweb ui over the top of it, enable web search and ask about the weather it'll get it wrong, you have to write a tool to call a weather API, it can figure out it needs to use the tool and then to parse the JSON response.",
          "score": 10,
          "created_utc": "2026-02-04 22:25:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ladtw",
          "author": "HonestoJago",
          "text": "GLM benefits from a low temp in situations like this, at least in my experience. But yeah, if you’re expecting Claude…..don’t.",
          "score": 3,
          "created_utc": "2026-02-04 20:06:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lw42r",
              "author": "Cempres",
              "text": "was not expecting anything in particular, was just wondering. I'll test it with ideas for D&D, advices on how to connect different story arcs and such and i'll write a comment when i have the chance to test it for such use",
              "score": 1,
              "created_utc": "2026-02-04 21:50:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3qw8wy",
                  "author": "Dubious-Decisions",
                  "text": "Load a bunch of slightly structured (e.g. YAML) files into an IDE like VS Code and hook it up to your local LLM. It can crawl all over that document tree and make notes to itself. When you tell it you are in \"room x\", it can go read through the YAML file for room x at the direction of the IDE and switch its context to use the facts and descriptions you put in the room x file. llama3.2 is actually a competent little LLM for doing this locally. ",
                  "score": 1,
                  "created_utc": "2026-02-05 17:06:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3orp7j",
          "author": "cibernox",
          "text": "Yes. Small locals models are reasonably good at being smart, not at being knowledgeable about every niche. Whey will get most thing that widely known well (year of a mayor war, what is the biggest mammal, and wether it rains more in Seattle or in Los Angeles, but not how many mg of caffeine a drink has).",
          "score": 3,
          "created_utc": "2026-02-05 09:04:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l9bgg",
          "author": "Condomphobic",
          "text": "They generally suck unless they're the full-sized model, which normal consumers cannot afford",
          "score": 4,
          "created_utc": "2026-02-04 20:01:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q37r3",
          "author": "Past-Grapefruit488",
          "text": "You need to enable Web Search so that App can use it to look up information. LLMs (Local or otherwise) are not encyclopedia. ChatGPT or Gemini will run a web search to answer such questions . Example : \n\nhttps://preview.redd.it/wvwdp3xdvohg1.png?width=1091&format=png&auto=webp&s=bb9ad3dbe1437c5f24e3a3e3f37e1bbf985dcf6f\n\n",
          "score": 2,
          "created_utc": "2026-02-05 14:48:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l99z6",
          "author": "Witty_Mycologist_995",
          "text": "That’s kind of strange. I use glm flash. What quantization is that?\nAsk it the capital of france",
          "score": 1,
          "created_utc": "2026-02-04 20:01:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lkd9i",
              "author": "gregusmeus",
              "text": "Berlin, lol. Sorry.",
              "score": 2,
              "created_utc": "2026-02-04 20:54:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ln11g",
                  "author": "mattv8",
                  "text": "🪦",
                  "score": 1,
                  "created_utc": "2026-02-04 21:07:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3pk783",
              "author": "Dzhmelyk135",
              "text": "Not even my Q3 version is that dumb",
              "score": 2,
              "created_utc": "2026-02-05 13:02:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3lvq1m",
              "author": "Cempres",
              "text": "https://preview.redd.it/bz8ghqbltjhg1.png?width=744&format=png&auto=webp&s=6e3477d638ab234ed58ad52dd52d3d6ab0b6c135\n\nI was trying out the glm-4.7-flash:q4\\_K\\_M",
              "score": 1,
              "created_utc": "2026-02-04 21:48:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3lzqy9",
                  "author": "Witty_Mycologist_995",
                  "text": "Send reasoning traces",
                  "score": 2,
                  "created_utc": "2026-02-04 22:07:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3lzww6",
          "author": "Ryanmonroe82",
          "text": "You need to use a smaller model with bf/fp16.  Using larger models that are quantized will always be less accurate especially a thinking/reasoning model that’s a MoE.  With 24gb VRAM look at dense 8b models that you can run in FP16/B16 with headroom for a decent context window.  RNJ-1-8b is excellent so is Gemma 2-7b.  They might not be as large as you want but the precision and accuracy is noticeably better especially on lower temps.",
          "score": 1,
          "created_utc": "2026-02-04 22:08:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mrp7q",
          "author": "superdariom",
          "text": "Isn't glm 4.7  a specialized coding model? Why not try deepseek instead?",
          "score": 1,
          "created_utc": "2026-02-05 00:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n4wj4",
          "author": "mpw-linux",
          "text": "Look at Liquid AI models. I use this one my Mac M1:\n\nmodel, tokenizer = load(\"mlx-community/LFM2.5-1.2B-Thinking-8bit\")\n\n It is quite good. I asked it create a 3 chord modern country song which i did with lyrics and chords. ",
          "score": 1,
          "created_utc": "2026-02-05 01:52:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ocqvk",
          "author": "tom-mart",
          "text": "There is nothing wrong about the example you showed. What is the problem?",
          "score": 1,
          "created_utc": "2026-02-05 06:45:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pc8xi",
              "author": "damhack",
              "text": "It’s low calorie, zero sugar but has caffeine in it.",
              "score": 1,
              "created_utc": "2026-02-05 12:08:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3pd024",
                  "author": "tom-mart",
                  "text": "I asked what's the problem in LLM response? It looks correct.",
                  "score": 1,
                  "created_utc": "2026-02-05 12:13:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pouht",
          "author": "bitmux",
          "text": "I find that most of the single-gpu local LLM's I've played with generate about 60% bs mostly regardless of settings and even including the websearch tool UNLESS they're hyper specialized in something and you use them right down the center of their specialization.  At that point its 40% bs :-P.",
          "score": 1,
          "created_utc": "2026-02-05 13:29:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rfwmp",
          "author": "generousone",
          "text": "With 24gb vram, go down to a smaller model like gpt-oss:20b and give it full 128k context. You'll see a vast improvement on capability and usefulness. GLM flash is 19gb. That's not a lot of room for context with 24gb vram. \n\n\n\nGenerally, it's also good to enable things like web search when asking fact specific questions.  \n\n\nEdit: lol, tried the exact prompt in my gpt-oss:20b and it wouldn't complete the request because it was censored to providing proprietary information. So I omitted \"secret\" and just asked what the ingredient were and it worked. ",
          "score": 1,
          "created_utc": "2026-02-05 18:36:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vamc5",
          "author": "joshiegy",
          "text": "One of the best ways to use a local llm is to combine with an MCP that gives some sort of webacrape ability.\nOr to have one specific to a task, like storytelling or code review in a specific language.\n\nBut it's not a GPT, those are huuuuuge. Chatgpt, a while back at least, requires something like 256gb of vram. Maybe it's even more now.",
          "score": 1,
          "created_utc": "2026-02-06 08:23:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vislx",
          "author": "maxbiz",
          "text": "The problem you have run into is that LLMs are poor at saying, 'I don't know.'  Where the LLM does not know, it treats it as a multiple-choice question; a random pick of 4 possible answers is a 25% chance of being right rather than 0% for 'I do not know'.",
          "score": 1,
          "created_utc": "2026-02-06 09:42:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n17yi",
          "author": "PrepperDisk",
          "text": "Yep!  We had to give up on bringing one to Prepper Disk even with RAG.  They are proofs of concept at best.",
          "score": 1,
          "created_utc": "2026-02-05 01:31:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nyx8l",
          "author": "fasti-au",
          "text": "Models are how they thing not what they think.   What they thing is parameters so the memory base inside a 1 trillion midel is about not looking.  Local models are all about the perfect prompt to avoid think having a different plan to you",
          "score": 0,
          "created_utc": "2026-02-05 04:56:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvj4la",
      "title": "Qwen3-Coder-Next",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qvj4la/qwen3codernext/",
      "author": "stailgot",
      "created_utc": "2026-02-04 08:29:24",
      "score": 36,
      "num_comments": 5,
      "upvote_ratio": 0.97,
      "text": "Qwen3-Coder-Next is a coding-focused language model from Alibaba's Qwen team, optimized for agentic coding workflows and local development. \n\n> ollama run qwen3-coder-next\n\nhttps://ollama.com/library/qwen3-coder-next\n\n> requires Ollama 0.15.5",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qvj4la/qwen3codernext/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3iq21p",
          "author": "WaitformeBumblebee",
          "text": "looking forward to test this and pit it against GLM4.7 flash which after minimal testing seems better than qwen3-A3B-2507",
          "score": 1,
          "created_utc": "2026-02-04 12:35:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ivvfi",
          "author": "Final-Watercress-253",
          "text": "What do you use these models with? I mean, with Claude I have the terminal, but if I want to use one of these models, what terminal software do I have to use?",
          "score": 1,
          "created_utc": "2026-02-04 13:12:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3j120b",
              "author": "JMowery",
              "text": "There are dozens of them nowadays. OpenCode seems to be one for the more popular.",
              "score": 7,
              "created_utc": "2026-02-04 13:41:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3mx7ap",
              "author": "AmphibianFrog",
              "text": "I use the Cline plugin in VS Code and it works well with the previous coder model (haven't tested on this one)",
              "score": 1,
              "created_utc": "2026-02-05 01:08:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3u4pmm",
              "author": "coding9",
              "text": "I use it with Claude code still. Its just a couple env variables using LM Studio",
              "score": 1,
              "created_utc": "2026-02-06 03:03:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qv1yey",
      "title": "Your thoughts on \"thinking\" LLMs?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qv1yey/your_thoughts_on_thinking_llms/",
      "author": "stonecannon",
      "created_utc": "2026-02-03 19:41:24",
      "score": 31,
      "num_comments": 38,
      "upvote_ratio": 0.75,
      "text": "almost all of the ollama-ready models released in recent months have been \"thinking\" or \"chain of thought\" or \"reasoning\" models -- you know, the ones that force you to watch the model's simulated thought process before it generates a final answer. \n\npersonally, i find this trend extremely annoying for a couple reasons:\n\n1). it's fake.  that's not how LLMs work.  it's a performance to make it look like the LLM has more consciousness than it does. \n\n2).  it's annoying.  i really don't want to sit through 18 seconds (actual example) of faux-thinking to get a reply to a prompt that just says \"good morning!\".\n\nThe worst example i've seen so far was with Olmo-3.1, which generated 1932 words of \"thinking\" to reply to \"good morning\" (i saved them if you're curious).\n\nin the Ollama CLI, some thinking models respond to the \"/set nothink\" command to turn off thinking mode, but not all do.  and there is no corresponding way to turn off thinking in the GUI.  same goes for the AnythingLLM, LM Studio, and GPT4All GUIs.\n\nso what do \\_you\\_ think?  do you enjoy seeing the simulated thought process in spite of the delays it causes?  if so, i'd love to know what it is that appeals to you... maybe you can help me understand this trend.\n\ni realize some people say this can actually improve results by forcing checkpoints into the inference process (or something like that), but to me it's still not worth it.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qv1yey/your_thoughts_on_thinking_llms/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3ew6mm",
          "author": "synth_mania",
          "text": "thinking models are created because they work. RL is used during post-training to train it to think in a way that increases performance.",
          "score": 27,
          "created_utc": "2026-02-03 21:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eqm1j",
          "author": "Wrapzii",
          "text": "Objectively for complex tasks it’s much better. It should *if done correctly* ask the question you asked but in different ways resulting in slightly different responses which the model compacts into one response. It does the “what, how, why” to itself to improve responses. Obviously if you tell it “hi” thats a stupid thought process. You shouldn’t really be using thinking models unless you need them.",
          "score": 18,
          "created_utc": "2026-02-03 20:49:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eyywm",
          "author": "iam_maxinne",
          "text": "Look, \"thinking\" is the model generating more tokens to try (and often to succeed) to create more tokens about your prompt, so your desired output is more likely to be result. You exchange some tokens (and the time it takes to generate them) for precision.\n\nIf you are coding on a specific language, that have specific terms, jargoons, file formats, tools, and etc..., so adding them to the session may make the model find an answer more in line to what you need.\n\nI find thinking worse when doing generic work, as there is less data to it to \"think about\", while it improves when the scope is smaller and more specialized.",
          "score": 9,
          "created_utc": "2026-02-03 21:27:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3emb7s",
          "author": "msltoe",
          "text": "The smaller models thinking process does work for certain things, like solving puzzles, but it's generally much flimsier than a SotA model's thoughts.",
          "score": 5,
          "created_utc": "2026-02-03 20:28:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f6u4m",
          "author": "gamesta2",
          "text": "I have a great experience with devstral-small-2:24b even though its a coder, it does a fantastic job at tool use and general tasks. The thing with llm's, its not about paper performance, but your setup, prompt, and integration. For example, nemotron spends 1500 words to say good morning on the same setup and same prompt, but on paper, nemotron is supposed to be better and faster",
          "score": 6,
          "created_utc": "2026-02-03 22:04:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3q2xqn",
              "author": "Go_Fast_1993",
              "text": "I've found nemotron to be better than devstral at parameter extraction for tool calling. I only bring this up because I was specifically evaluating devstral-small against nemotron-3-nano this week. Response time is slower on nemotron, but that tradeoff works for my use case.",
              "score": 2,
              "created_utc": "2026-02-05 14:46:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3r0kxo",
                  "author": "gamesta2",
                  "text": "Yeah it all depends on setup. Another example for me is qwen3:30b model absolutely destroying gpt-oss. Everyone talks good about oss, but it just would not work for me. For now devstral performs the best",
                  "score": 1,
                  "created_utc": "2026-02-05 17:26:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3etmc6",
          "author": "Savantskie1",
          "text": "In some tasks the thinking variants improve the model but the /no_think only works for Qwen models",
          "score": 4,
          "created_utc": "2026-02-03 21:02:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ezco4",
          "author": "funfun151",
          "text": "If you’re saying “good morning” I’m guessing you’re using it as a chatbot? Not really the use case for thinking models tbf, you’d be better off with a high context standard model for that.",
          "score": 3,
          "created_utc": "2026-02-03 21:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gaj2h",
          "author": "eleqtriq",
          "text": "You want my thoughts on thinking?  I’d rather get straight to the answer.",
          "score": 3,
          "created_utc": "2026-02-04 01:36:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f0kuo",
          "author": "sinan_online",
          "text": "It’s not that they are “thinking”, it’s more that they are generating language that is a chin of thought. Since LLMs can generated human language, a straightforward application is to create one that is trained towards a chain-of-thought format.\n\nIt does look like it is wasted GPU hours and energy, I am not sure if measurable outcomes are much different.\n\nI’d have to see something concrete. Claude 4.5 is good at programming, but I am not sure how much of that is related to how it’s trained vs how it is implemented.",
          "score": 1,
          "created_utc": "2026-02-03 21:35:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fckmo",
              "author": "d_the_great",
              "text": "It make the model, well, model a solution to the user's problems better, resulting in a better answer. The only problem is small models don't have the ability to decide when or when not to enter thinking mode, which means they just default to thinking every time even when it's a waste of time and resources.",
              "score": 1,
              "created_utc": "2026-02-03 22:32:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3fcvfh",
          "author": "AmphibianFrog",
          "text": "Regarding point 1, I do agree it's kind of \"fake\" in that the chain of thought is not necessarily representative of what's going on in the network or how it came to a conclusion, but there's more to it than that. Because the chain of thought tokens get fed back in for subsequent token generation, it does provide the model with extra processing and those tokens do influence the final output. \n\nPersonally I normally pick instruct models or use whatever flag (i.e. /nothink) turns the thinking off. I'm not sure the benefits outweigh the extra processing time and energy for most local models. Often I just want it to be faster.\n\nIt can give a bit of extra insight too if you read the chain of thought but you have to take it with a pinch of salt!",
          "score": 1,
          "created_utc": "2026-02-03 22:33:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fdx7q",
          "author": "Frogy_mcfrogyface",
          "text": "It works better for some stuff, but ive noticed that if I ask the model something like \"Take this list and in front of each line add xx with a space before each line\" it will think about it, and it its thought process will actually create the list exactly how I want it, but then overthink it and its result would be something different.",
          "score": 1,
          "created_utc": "2026-02-03 22:38:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fnjus",
          "author": "duplicati83",
          "text": "There’s a huge difference between my qwen3 30b thinking vs non thinking models. It’s the difference between right and wrong answers.",
          "score": 1,
          "created_utc": "2026-02-03 23:29:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fzb5d",
          "author": "JLeonsarmiento",
          "text": "I don’t like “thinking” models either.  Hybrid or those with budget thinking, behaving like “instruct” or “reasoning” based on the complexity of the prompt are better.",
          "score": 1,
          "created_utc": "2026-02-04 00:33:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g0zu2",
          "author": "Medium_Ordinary_2727",
          "text": "Chain-of-thought reasoning definitely improves the results I get on certain types of questions.\n\nExample: until recently most LLMs couldn't correctly answer \"How many R's are in the word strawberry?\" But those same LLMs, prompted with my homemade janky chain-of-thought system instructions, would get the answer correct.\n\nI agree it's annoying to see the \"thinking\" process especially if you are just asking a quick question that doesn't require thought. Some frontends like OpenWebUI do have an [option to disable thinking mode](https://docs.openwebui.com/features/chat-features/reasoning-models/#think-ollama).",
          "score": 1,
          "created_utc": "2026-02-04 00:43:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g1gyl",
          "author": "sultan_papagani",
          "text": "yes its bad. gpt oss 20b creates fake policies while its \"thinking\" and cant even play a number guessing game.",
          "score": 1,
          "created_utc": "2026-02-04 00:45:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g6cl2",
          "author": "triynizzles1",
          "text": "It’s good for long prompts where you’re asking the LLM to complete multiple tasks. The thinking tokens allow the model to work out the intricacies of what you are asking. Simple prompts like “hello” and “summarize this:” isnt where these models excel. If the use cases correct, it’s great!",
          "score": 1,
          "created_utc": "2026-02-04 01:12:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g80tt",
          "author": "sn0n",
          "text": "Ok, so the OP said thinking, so I should call <tool>….",
          "score": 1,
          "created_utc": "2026-02-04 01:22:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gkplk",
          "author": "fasti-au",
          "text": "One shots work in tokens in.  Reasoners work in token in and review which implies it has enough tokens to work with to one shot and the also to reason. \n\nIf one shotting is already there in tool calling then you are not really needing a reasoner but reasoners needed one shots to work to not think about code breaks and to think about it as a concept or design flaw.   Error then  fix error if no error look at results …. Nope 👎 fixed error that’s what think decided was the task and you retry.     Think isn’t your choice of workflow really.  If think is about why it’s broken not how the task is broken in context then your creation token burn on retries",
          "score": 1,
          "created_utc": "2026-02-04 02:34:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hg736",
          "author": "DedsPhil",
          "text": "It's not fake, thinking tokens were created because in a lot of scenarios they help steering the model for a better answer without polluting the contex.\n\nThey were made because people figured that asking the model to \"think\" step by step made the answer better.",
          "score": 1,
          "created_utc": "2026-02-04 05:58:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hh41c",
          "author": "NoxinDev",
          "text": "People still believe that's anything other than the first tier of output for your input tokens, anything more than a performance?",
          "score": 1,
          "created_utc": "2026-02-04 06:06:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hrwb5",
          "author": "Ledeste",
          "text": "1) It's not \"fake,\" but it's clearly branding. But that's how words work. For example, words do not actually work, but when you read \"words work,\" you did not think it was fake.\n\n2) It's a way to improve the result; it's not made to be cool, it's made to make the response better. If it's taking too long, it's mostly because the LLM was not tuned enough and because local tools are basically non-existent (just forwarding user requests to an LLM with minimal wrapping). \n\nYou should learn a bit about how LLMs work; you'll quickly understand why these steps are becoming important.",
          "score": 1,
          "created_utc": "2026-02-04 07:38:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hxr5d",
          "author": "podgorniy",
          "text": "Before thinking was a thing one way to improve quality of LLM reply was to actually ask it to \"make a plan\" or \"think about the problem\" and only then get the final result. Idea of thinking is that this extra step should improve the output of the LLM. At least according to the benchmarks.",
          "score": 1,
          "created_utc": "2026-02-04 08:32:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ik5g4",
          "author": "StaysAwakeAllWeek",
          "text": "Beyond just the intelligence, performance, accuracy, etc benefits, thinking mode MASSIVELY reduces the hallucination rate because the model has ample chance to spot its own hallucinations and correct them\n\nIf you're ever having an LLM output content that cant/won't be fully manually verified it should be with thinking enabled. Always.",
          "score": 1,
          "created_utc": "2026-02-04 11:53:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3isteu",
          "author": "crone66",
          "text": "They're usage should be situational. E.g. in if this than that scenarios it might be able to solve things that are not directly in the training data e.g. simple math tasks (without using tools!).\n\n\nExample if the training data contains\n2+2=4 but not 20+20 it might reason ton calculate 2+2 and simply add a zero. Therefore it could solve it without having it in the trainingdata and without tool use.\n\n\nA non thinking model couldn't do that easily without explicitly prompting for calculation strategies first.\n\n\nThe down side is that think is in many scenarios completely useless and just a waste of time. Therefore, most providers automatically try to select the model based on the complexity of your prompt.\n\n\nTLDR: Use thinking models only when it makes sense because it has negative value for easy tasks/prompts.",
          "score": 1,
          "created_utc": "2026-02-04 12:53:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j3lc0",
          "author": "MrMakerWeTrust",
          "text": "In my Opinion LLMs have now reached a point of larger thinking memory. I don’t think it’s natural thinking I believe they use numbers to determine the quality of the response",
          "score": 1,
          "created_utc": "2026-02-04 13:55:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3r97y3",
          "author": "surfmaths",
          "text": "It is fake, and it wastes tokens/compute, but also... it works?! In the sense that the final answer is correct more often. \n\nI agree it's frustrating but the entirety of AI is made of fake layers that improve slightly and that all combined give something acceptable.",
          "score": 1,
          "created_utc": "2026-02-05 18:06:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rk68j",
          "author": "Jshdgensosnsiwbz",
          "text": "sure , you want to get into this....",
          "score": 1,
          "created_utc": "2026-02-05 18:56:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ufnea",
          "author": "RecalcitrantZak",
          "text": "After the anthropic research on model traces it was fairly obvious that most thinking models are just writing their own narrative fan fiction with nothing to do with how they are generating data. \n\nJust another way to burn tokens.",
          "score": 1,
          "created_utc": "2026-02-06 04:12:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ehf0r",
          "author": "Dubious-Decisions",
          "text": "They are designed to encourage continued human engagement to drive token usage/sales. A \"thinking\" model that is free/open is just annoying because it's just wasting CPU resources and not even driving revenue for its creator. In all cases, they are a waste unless you are in it for the entertainment value alone.",
          "score": 1,
          "created_utc": "2026-02-03 20:05:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3elmts",
          "author": "Copakela",
          "text": "Not a fan. First model I used was one of the qwen family and I didn’t know how to end the conversation after a few turns so I typed “exit”. Cue 30 seconds of unnecessary paranoia from the model wondering what it did wrong.",
          "score": 1,
          "created_utc": "2026-02-03 20:25:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3et9pw",
              "author": "Savantskie1",
              "text": "Didn’t happen, if you type exit in cli it doesn’t get sent to the model",
              "score": 2,
              "created_utc": "2026-02-03 21:01:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3eyaez",
                  "author": "Copakela",
                  "text": "Maybe it was quit or something like that. It was a good while ago. It wasn’t /bye anyway, I only found that out after.",
                  "score": 1,
                  "created_utc": "2026-02-03 21:24:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3w0nz1",
              "author": "TeachNo196",
              "text": "I did something like that once when I first started trying out local llms.  Made me chuckle.",
              "score": 1,
              "created_utc": "2026-02-06 12:14:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3g8k5n",
          "author": "sn0n",
          "text": "Instant gratification entitlement man… I tell ya… back in my day we had to get in the car, goto the library, hunt down the librarian, then hunt down the right book…. And spend hours pouring through pages…. And you bitch about letting your computer do it for you slowly while you carry about other tasks? Get outta here",
          "score": -1,
          "created_utc": "2026-02-04 01:25:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3entor",
          "author": "BidWestern1056",
          "text": "can't really stand them, and build a lot of products that don't really emphasize them:\n\n[https://github.com/npc-worldwide/npcpy](https://github.com/npc-worldwide/npcpy)\n\n[https://github.com/npc-worldwide/incognide](https://github.com/npc-worldwide/incognide)\n\n[https://github.com/npc-worldwide/npcsh](https://github.com/npc-worldwide/npcsh)",
          "score": -1,
          "created_utc": "2026-02-03 20:36:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrkbsr",
      "title": "Run Ollama on your Android!",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qrkbsr/run_ollama_on_your_android/",
      "author": "DutchOfBurdock",
      "created_utc": "2026-01-30 22:32:38",
      "score": 24,
      "num_comments": 11,
      "upvote_ratio": 0.88,
      "text": "Want to put this out here. I have a Samsung S20 and  a Pixel 8 Pro. Both of these devices pack 12GB of RAM, one an octacore arrangement and the other a nonacore. Now, this is pure CPU and even Vulkan (despite hardware support), doesn't work.\n\nFirst, get yourself Termux from F-Droid or GitHub. **Don't use the Play Store version.**\n\nUpon launching Termux, update the package manager and install some things needed..\n\n    pkg up\n    pkg i build-essential git cmake golang\n    git clone https://github.com/ollama/ollama.git\n    cd ollama\n    go generate ./...\n    go build .\n\nIf all went well, you'll end up with an `ollama` executable in the folder.\n\n    ./ollama serve\n\nOpen a new terminal in the gitted ollama folder \n\n    ./ollama pull smollm2\n    ./ollama run smollm2\n\nThis model should be small enough for even 4GB devices and is pretty fast.\n\nEnjoy and start exploring!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qrkbsr/run_ollama_on_your_android/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o2rcrgy",
          "author": "GutenRa",
          "text": "Pocket Pal - good enough for run what you want from HF. Or Google edge gallery with gemma-3n e4b - one of the best universal model for mobile.",
          "score": 8,
          "created_utc": "2026-01-31 08:37:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ulbh9",
              "author": "DutchOfBurdock",
              "text": "Nice find! I do like Ollama, it's literal childs play getting up and running with self hosted A.I. Also, being in a terminal makes it extra nerdy 😂\n\nI've been trying hard as heck to utilize the Vulkan hardware, vkinfo in Termux shows support and a shared upto 2GB VRAM on my Pixel 8 Pro. Just, cant get it to be seen in llama directly or Ollama",
              "score": 1,
              "created_utc": "2026-01-31 20:21:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qau0l",
          "author": "mlt-",
          "text": "What does `ollama ps` show while running? Somehow I'm skeptical there are accelerated backends.\n\nAlso… what is up with termux from play store?",
          "score": 4,
          "created_utc": "2026-01-31 03:35:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2quro4",
              "author": "DutchOfBurdock",
              "text": ">this is pure CPU\n\nBecause of how Termux works, it has to be built targeting an older SDK, else it just wouldn't work in Android 10+ (W\\^X). Because of this, it can't be updated on Play anymore.",
              "score": 4,
              "created_utc": "2026-01-31 05:56:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qut36",
          "author": "AwayLuck7875",
          "text": "Ollama apk ??",
          "score": 3,
          "created_utc": "2026-01-31 05:56:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qx7zd",
              "author": "DutchOfBurdock",
              "text": "Native binary in a Terminal.",
              "score": 2,
              "created_utc": "2026-01-31 06:16:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rwqir",
          "author": "palec911",
          "text": "What is realistically achievable to run as a model and use case? And how hot would the device get, I mean how tasking on the device would it be?",
          "score": 3,
          "created_utc": "2026-01-31 11:44:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2s2to0",
              "author": "DutchOfBurdock",
              "text": "Text summarisation, chat bot function, function calling (MCP). One use case is having it work with Tasker for Android and WhatsApp. Tasker intercepts WhatsApp notification, sends the chat to Ollama's API to provide a reply. Tasker then uses the Reply function of the notification to reply to the chat.\n\nOllama tool calling can run commands in Termux bound to `am` so that intents can be sent to Tasker and have Tasker as its MCP.",
              "score": 3,
              "created_utc": "2026-01-31 12:33:42",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2us34m",
              "author": "DutchOfBurdock",
              "text": "I didn't fully answer your question, regarding the taxing. I'm using two higher end devices packing a lot of RAM and decent SoCs. The model mentioned gets up to 12.3tps on the P8P, and up to 7.2 on the S20.\n\nI can often get 4b models running on them, but we're talking much slower tps rates.",
              "score": 2,
              "created_utc": "2026-01-31 20:55:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2uqfq0",
          "author": "DutchOfBurdock",
          "text": "I will use this comment post to share what I've used this for. Remember, when Ollama is serving, it's API is available on loopback.\n\nI'm a Tasker user, so want Tasker to be my MCP. This can be achieved by calling functions via Termux, which can Intent other apps (`am` utility is available to Termux, as well as Termux:API). I started simple and will share more as these methods become matured (keep posted on r/Tasker)\n\n**Chat to Ollama from Tasker**\n\n_This is one shot without any context of previous chats_ The model is a custom model called `sim`\n\n\n    Task: Ollama\n    \n    A1: Get Voice [\n         Title: What to ask rudeboy?\n         Language Model: Free Form\n         Maximum Results: 1\n         Timeout (Seconds): 40 ]\n    \n    A2: HTTP Request [\n         Method: POST\n         URL: http://localhost:11434/api/generate \n         Body: {\n           \"model\": \"sim\", \n           \"prompt\": \"%gv_heard\",\n           \"stream\": false\n         }\n         Timeout (Seconds): 300\n         Use Cookies: On\n         Structure Output (JSON, etc): On ]\n    \n    A3: Variable Set [\n         Name: %response\n         To: %http_data.response\n         Structure Output (JSON, etc): On ]\n    \n    A4: Say WaveNet [\n         Text/SSML: %response\n         Voice: en-GB-Wavenet-O\n         Stream: 3\n         Pitch: 20\n         Speed: 8\n         Continue Task Immediately: On\n         Respect Audio Focus: On\n         Continue Task After Error:On ]",
          "score": 2,
          "created_utc": "2026-01-31 20:46:48",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtqp9p",
      "title": "175k+ publicly exposed Ollama servers, so I built a tool",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/gallery/1qtqobb",
      "author": "truthfly",
      "created_utc": "2026-02-02 09:41:37",
      "score": 21,
      "num_comments": 8,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qtqp9p/175k_publicly_exposed_ollama_servers_so_i_built_a/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o34ugp2",
          "author": "HyperWinX",
          "text": "\"<literally any statement> so I built\" my ass",
          "score": 6,
          "created_utc": "2026-02-02 10:40:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ibiv7",
              "author": "ScrapEngineer_",
              "text": "It's vibe coded, and it's bad lol",
              "score": 1,
              "created_utc": "2026-02-04 10:41:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ihz3i",
                  "author": "truthfly",
                  "text": "Humm. No... Coded by hand.. readme was generated by LLM yeah but I coded each module, the orchestrator was made by LLM yeah but not the rest. Maybe it's bad, but it works. It's not at a scraper engineer level 🤗",
                  "score": 1,
                  "created_utc": "2026-02-04 11:36:55",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o35crn9",
              "author": "truthfly",
              "text": "Yeah, bad title maybe. I get the skepticism. Maybe the form is not good.. \n\nBut it's not a hype post. just releasing a tool I was already using once the article went public.",
              "score": -2,
              "created_utc": "2026-02-02 13:03:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o36xye5",
          "author": "sinan_online",
          "text": "Is it that interesting? I run stateless servers on the cloud from time to time, they are completely ephemeral.",
          "score": 2,
          "created_utc": "2026-02-02 17:49:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3cwqt1",
              "author": "truthfly",
              "text": "It highly depends on your plan, infrastructure and billing, but it could be a big issue with custom compagny model for example",
              "score": 3,
              "created_utc": "2026-02-03 15:44:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ib9vz",
          "author": "Adventurous-Hunter98",
          "text": "I dont understand, does running ollama expose my ip or something ?",
          "score": 1,
          "created_utc": "2026-02-04 10:39:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ibp9h",
              "author": "HyperWinX",
              "text": "Running ollama - no. Exposing API endpoint to public - yea. Bots will find that endpoint pretty quickly.",
              "score": 3,
              "created_utc": "2026-02-04 10:43:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qv346v",
      "title": "Recommandation for a power and cost efficient local llm system",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qv346v/recommandation_for_a_power_and_cost_efficient/",
      "author": "Grummel78",
      "created_utc": "2026-02-03 20:23:53",
      "score": 18,
      "num_comments": 15,
      "upvote_ratio": 1.0,
      "text": "Hello everybody,\n\ni am looking power and cost efficient local llm system. especially when it is in idle. But i don't want wait minutes for reaction :-) ok ok i know that can not have everything :-)\n\nUse cases are the following:\n\n1. Using AI for Paperless-NGX (setting tags and ocr)\n\n2. Voice Assistant and automation in Home Assistant.\n\n3. Eventual Clawdbot\n\nAt the moment i tried Ai with the following setup:\n\nasrock n100m + RTX 3060 +32 GB Ram.\n\n  \nBut it use about 35 Watts in idle. I live in Germany with high energy cost. And for an 24/7 system it is too much for me. especially it will not be used every day. Paperless eventually every third day.  Voice Assistant and automation in Home Assistant 10-15 times per day.\n\nClawdbot i don't know.\n\nImportant for me is data stays at home (especially Paperless data).\n\nKnow i am thinking about a mac mini m4 base edition (16 gig unified ram and 256 ssd)\n\nHave somebody recommandations or experience with a mac mini and my use cases ?\n\nBest regards\n\nDirk\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qv346v/recommandation_for_a_power_and_cost_efficient/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3fdht4",
          "author": "prene1",
          "text": "Mac has the LLM low power in a chokehold",
          "score": 7,
          "created_utc": "2026-02-03 22:36:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fqo73",
          "author": "overand",
          "text": "In Germany, you probably pay \\~€0.40 per kilowatt hour. 35 watts, 24 hours a day , 30 days is a total of 25 kw/h.\n\nDo not spend €500 on a computer to lower your monthly electricity bill from €10 to €2.50. (Even if it cut your electricity bill to ZERO, it would take over 4 years to make back the cost. Realistically, at \\~8 watts vs \\~35 watts, you're saving, it's closer to 5 1/2 years)\n\nYou're spending less than €125 a *year* in electricity leaving your current setup on 24/7, if it's really 35 watts, and you're really at €0.40/kwh",
          "score": 6,
          "created_utc": "2026-02-03 23:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fvjzk",
          "author": "TheAussieWatchGuy",
          "text": "Wrong time 😀 RAM prices have gone to the moon because of AI.\n\n\nThe most RAM and VRAM you can get is the answer. Mac M4 and Ryzen AI 395 both offer unified memory, so if you get 128gb of ddr5 ram you can allocate 112gb of it to the built in GPU. \n\n\nThat's pretty much the most cost effective way to run bigger models. 96gb in a pinch can be ok. \n\n\nOtherwise you can double down and get 64gb of ddr4 and a couple of 4090s, and a new PSU for your current PC, you'll spend more but Nvidia is still a bit easier to get local working well on.\n\n\nEither way you're spending a few grand. ",
          "score": 3,
          "created_utc": "2026-02-04 00:13:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3gkndk",
              "author": "ggone20",
              "text": "Not all unified memory is equal. \n\nOnly the Mac has true unified memory, all other systems just use it as marketing jargon with values needing to be set ahead of time performance hits for ‘auto’. \n\nSaid another way - Macs truly share the exact same memory, all other systems split a pool of memory.",
              "score": 2,
              "created_utc": "2026-02-04 02:33:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3h3ln9",
                  "author": "TheAussieWatchGuy",
                  "text": "Technically true. Mac is king here but with a price tag to match.\n\n\nFor the OP if budget is a concern the Ryzen AI platform is cost effective and fast enough. ",
                  "score": 1,
                  "created_utc": "2026-02-04 04:27:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3hcb56",
                  "author": "Zyj",
                  "text": "Have you looked at GTT memory on AMD AI Max+? I guess not.",
                  "score": 1,
                  "created_utc": "2026-02-04 05:29:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3gft6u",
          "author": "Royale_AJS",
          "text": "You’re looking for an AMD Strix Halo, a Mac, or a DGX Spark. The Mac is super efficient, my Strix Halo idles at 9 watts and I can’t get it to pull more than 140 out of the wall.",
          "score": 2,
          "created_utc": "2026-02-04 02:06:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eymn9",
          "author": "GlassAd7618",
          "text": "Any specific model you have in mind for Paperless-ngx and voice automation? What runtime are you thinking about (e.g., Ollama or lmstudio)? I could run some tests on my Mac mini. (I'm in Germany too, so it would be interesting to see how smooth selected models run on a 16GB Mac mini to avoid a hefty electricity bill...)",
          "score": 1,
          "created_utc": "2026-02-03 21:26:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gnhi0",
          "author": "ServeAlone7622",
          "text": "LFM 2.5 seems optimized for this use case.",
          "score": 1,
          "created_utc": "2026-02-04 02:49:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hhpcx",
          "author": "gamesta2",
          "text": "4nm. Ryzen. my 9700x is pretty much identical to 13600k in performance, but draws half the power.",
          "score": 1,
          "created_utc": "2026-02-04 06:10:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwwkip",
      "title": "Built a self-hosted execution control layer for local LLM workflows (works with Ollama)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qwwkip/built_a_selfhosted_execution_control_layer_for/",
      "author": "saurabhjain1592",
      "created_utc": "2026-02-05 20:28:01",
      "score": 13,
      "num_comments": 0,
      "upvote_ratio": 0.93,
      "text": "Hey folks. I am building AxonFlow, a self-hosted, source-available execution control layer for local LLM workflows once they move beyond single prompts and touch real systems.\n\nThe hard part was not model quality. It was making execution visible and controllable:\n\n* clear boundaries around what steps are allowed to run\n* logs tied to decisions and actions, not just model outputs\n* the ability to inspect and replay a run when something goes wrong\n\nRetries and partial failures still mattered, but only after we could see and control what happened in a run.\n\nAxonFlow sits inline between your workflow logic and LLM tool calls to make execution explicit. It is not an agent framework or UI platform. It is the runtime layer teams end up building underneath once local workflows get serious.\n\nWorks with Ollama by pointing the client to a local endpoint.  \nGitHub: [https://github.com/getaxonflow/axonflow](https://github.com/getaxonflow/axonflow)\n\nWould love feedback from folks running Ollama in real workflows.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qwwkip/built_a_selfhosted_execution_control_layer_for/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qt7wzm",
      "title": "Reprompt - Simple desktop GUI application to avoid writing the same prompts repeatedly",
      "subreddit": "ollama",
      "url": "https://i.redd.it/u520ax05nxgg1.gif",
      "author": "PuzzleheadedHeat9056",
      "created_utc": "2026-02-01 19:22:11",
      "score": 12,
      "num_comments": 1,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qt7wzm/reprompt_simple_desktop_gui_application_to_avoid/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o32fyud",
          "author": "thecoder12322",
          "text": "This is exactly the kind of tool that makes working with local LLMs so much more practical! Rust + egui is a great choice for a lightweight desktop GUI, and the focus on reusable prompts is spot-on for real-world workflows.\n\nI've been working on similar local-first AI tooling, and if you're looking to expand beyond text-based models, you might want to check out the **open source RunAnywhere SDK** (https://github.com/RunanywhereAI/runanywhere-sdks). It makes it really easy to integrate STT, TTS, VLM, and LLM capabilities into desktop apps while keeping everything running locally – perfect for the same privacy-first philosophy you've built Reprompt around.\n\nLove that you're keeping it simple and focused. Tools like this are what make local AI actually usable for everyday tasks. Happy to chat more about local AI tooling if you're interested – feel free to DM!",
          "score": 2,
          "created_utc": "2026-02-02 00:14:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qt04u5",
      "title": "OpenClaw For data scientist that support Ollama",
      "subreddit": "ollama",
      "url": "https://github.com/JasonHonKL/PardusClawer",
      "author": "jasonhon2013",
      "created_utc": "2026-02-01 14:39:27",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qt04u5/openclaw_for_data_scientist_that_support_ollama/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qvnpp7",
      "title": "Using Ollama for a real-time desktop assistant — latency vs usability tradeoffs?",
      "subreddit": "ollama",
      "url": "https://v.redd.it/n32j33l45hhg1",
      "author": "Ore_waa_luffy",
      "created_utc": "2026-02-04 12:50:21",
      "score": 11,
      "num_comments": 6,
      "upvote_ratio": 0.76,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qvnpp7/using_ollama_for_a_realtime_desktop_assistant/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3lsq4u",
          "author": "TheAndyGeorge",
          "text": "why you promoting [interview cheating](https://www.reddit.com/r/OpenAI/comments/1qvnrht/cheat_interviews_easily/) ?",
          "score": 3,
          "created_utc": "2026-02-04 21:34:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rupbm",
              "author": "Ore_waa_luffy",
              "text": "Im not promoting it , there are literally tools that people pay 30 dollars or so , I'm just making it free",
              "score": 1,
              "created_utc": "2026-02-05 19:45:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3o3enm",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-05 05:28:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3o88j3",
              "author": "Ore_waa_luffy",
              "text": "Oh Lemme try that . What i did was i used whisper small for ui transcription so the user can feel it’s being transcribed and use whisper medium for actual transcription but the latency was like few seconds which i didn’t like personally",
              "score": 1,
              "created_utc": "2026-02-05 06:06:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3khmh8",
          "author": "amgir1",
          "text": "I have 5070 ti, and my olama have been typing realy slow. It produces a lot of text but I don't trust it, too many words, too many time to type answer",
          "score": 0,
          "created_utc": "2026-02-04 17:54:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kln97",
              "author": "Ore_waa_luffy",
              "text": "Yes in my app i have given preference to google stt if anyone cant pay then can use dual pipeline  whisper quick and medium",
              "score": 0,
              "created_utc": "2026-02-04 18:13:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qry7r9",
      "title": "[Ollama Cloud] 29.7% failure rate, 3,500+ errors in one session, support ignoring\ntickets for 2 weeks - Is this normal?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qry7r9/ollama_cloud_297_failure_rate_3500_errors_in_one/",
      "author": "Few-Point-3626",
      "created_utc": "2026-01-31 09:46:10",
      "score": 10,
      "num_comments": 12,
      "upvote_ratio": 0.86,
      "text": "'ve been using Ollama Cloud API for my production workflow (content moderation)\n\nand I'm experiencing catastrophic reliability issues that are making the service\n\nunusable.\n\n\n\n\\## The Numbers (documented with full logs)\n\n\n\n| Metric | Value |\n\n|--------|-------|\n\n| Total requests sent | 4,079 |\n\n| Successful responses | 2,868 |\n\n| \\*\\*Failed requests\\*\\* | \\*\\*1,211\\*\\* |\n\n| \\*\\*Failure rate\\*\\* | \\*\\*29.7%\\*\\* |\n\n\n\n\\## Incident Timeline\n\n\n\n| Date | Error 429 | Error 500 | Success Rate |\n\n|------|-----------|-----------|--------------|\n\n| Dec 10, 2025 | 235 | 0 | 0% |\n\n| Dec 20, 2025 | 0 | 30 | 0% |\n\n| \\*\\*Jan 4, 2026\\*\\* | \\*\\*3,508\\*\\* | 0 | \\*\\*0%\\*\\* |\n\n| Jan 29, 2026 | 0 | 0 | 86.8% |\n\n| Jan 30, 2026 | 0 | 0 | 74.3% |\n\n| \\*\\*Jan 31, 2026\\*\\* | 0 | \\*\\*194\\*\\* | \\*\\*28.8%\\*\\* |\n\n\n\nYes, you read that right: \\*\\*3,508 consecutive 429 errors in 40 minutes\\*\\* on\n\nJanuary 4th.\n\n\n\n\\## The Pattern\n\n\n\nEvery session follows the same pattern:\n\n\\- \\~30 requests succeed normally\n\n\\- Then the server crashes with 500 errors\n\n\\- All subsequent requests fail\n\n\\- I have to restart and hope for the best\n\n\n\n\\## My Configuration\n\n\n\n\\- Model: deepseek-v3.1:671b\n\n\\- Concurrent requests: 3 (using 3 separate API keys)\n\n\\- Workers per key: 1 (minimal load)\n\n\\- Timeout: 25 seconds\n\n\n\nI'm not hammering the API. 3 concurrent requests with 3 different API keys is\n\nextremely conservative.\n\n\n\n\\## Support Response\n\n\n\nI opened a support ticket on \\*\\*January 18th, 2026\\*\\*.\n\n\n\n\\*\\*Response received: NONE.\\*\\*\n\n\n\nIt's been 2 weeks. Radio silence. No acknowledgment, no \"we're looking into it\",\n\nnothing.\n\n\n\n\\## Questions for the Community\n\n\n\n1. Is anyone else experiencing similar issues with deepseek models on Ollama Cloud?\n\n2. Is this level of unreliability normal?\n\n3. Has anyone actually gotten a response from Ollama support (hello@ollama.com)?\n\n4. Are there alternative providers for deepseek-v3 that are more reliable?\n\n\n\n\\## What I'm Asking Ollama\n\n\n\n1. Investigate why your servers are returning 3,500+ 429 errors in a single session\n\n2. Investigate the 500 errors that crash the service after \\~30 requests\n\n3. Respond to support tickets\n\n4. Credit for the failed requests that were still billed\n\n\n\nI have complete logs documenting every single error with timestamps. Happy to\n\nshare with Ollama support if they ever decide to respond.\n\n\n\n\\---\n\n\n\n\\*\\*Edit:\\*\\* I'll update this post if/when I get a response.\n\n\n\n\\*\\*Edit 2:\\*\\* For those asking, my use case is legitimate content moderation for a\n\nFrench platform. \\~200-300 requests per day, nothing excessive.\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qry7r9/ollama_cloud_297_failure_rate_3500_errors_in_one/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o2u79lb",
          "author": "mchiang0610",
          "text": "I'm Michael C from Ollama. This is very unacceptable and I take full accountability for your experience. \n\n  \nI am looking into what could be the cause of this, and why rate limits are being hit to cause the errors. In the meantime, I have gone ahead and refunded 3 months of payment. I get it, it does not mean much if you are not receiving the service you've paid for.  \n\n  \n[michael@ollama.com](mailto:michael@ollama.com)",
          "score": 9,
          "created_utc": "2026-01-31 19:13:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ubxr3",
              "author": "StardockEngineer",
              "text": "I would say this is great customer service, if not for ignoring their pleas for help outside of social media.",
              "score": 5,
              "created_utc": "2026-01-31 19:35:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3rzfse",
              "author": "Few-Point-3626",
              "text": "    Thank you Michael. I can confirm the 3-month refund has been received.\n    \n    I appreciate you taking accountability publicly and acting quickly.\n    Please let me know when the technical issue is resolved on your end.",
              "score": 1,
              "created_utc": "2026-02-05 20:07:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2s1nxe",
          "author": "Few-Point-3626",
          "text": "https://preview.redd.it/mtk5v6i6hogg1.png?width=885&format=png&auto=webp&s=fdd81444265cde40a910a64834c51696a5656ef2\n\nFor those asking, my use case is legitimate content moderation for a\n\nFrench platform. \\~200-300 requests per day, nothing excessive.",
          "score": 2,
          "created_utc": "2026-01-31 12:24:40",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2u7edg",
              "author": "mchiang0610",
              "text": "I agree. This is a totally legitimate use case we want and should be serving.",
              "score": 1,
              "created_utc": "2026-01-31 19:13:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2sewnl",
          "author": "Ryanmonroe82",
          "text": "Have you checked the issues page on their GitHub?",
          "score": 2,
          "created_utc": "2026-01-31 13:53:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lfvqz",
          "author": "stealthagents",
          "text": "That’s pretty rough. Sounds like they're really dropping the ball on reliability, especially for something crucial like content moderation. Hopefully, they find a fix soon because dealing with constant errors isn’t just a hassle, it can seriously mess up your workflow.",
          "score": 1,
          "created_utc": "2026-02-04 20:33:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3utt4f",
          "author": "Wonderful6014",
          "text": "That 29.7% failure rate with 3,500+ consecutive 429 errors is definitely not normal for production infrastructure. Your configuration looks conservative (3 concurrent requests, 25s timeout) which makes this even more concerning. For content moderation workloads that need predictable performance, you might want to explore alternatives that give you more control—running models directly on-device eliminates API failures entirely and gives you guaranteed response times.",
          "score": 1,
          "created_utc": "2026-02-06 05:55:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3v9tld",
              "author": "Few-Point-3626",
              "text": "Thanks, but running locally isn't really an option here. My use case is content \n\nmoderation in French, and smaller models really struggle with the nuances of \n\nthe language - idioms, double meanings, context-dependent expressions. \n\n\n\nFor accurate moderation you need a capable model, and that means cloud APIs \n\nfor now.\n\n",
              "score": 1,
              "created_utc": "2026-02-06 08:16:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dzkum",
          "author": "LeadingFun1849",
          "text": "Hi everyone, I've been working on this project for a while.\nDaveLovable is an open-source, AI-powered web UI/UX development platform, inspired by Lovable, Vercel v0, and Google's Stitch.\nWebsite https://dlovable.daveplanet.com\nCode: https://github.com/davidmonterocrespo24/DaveAgent",
          "score": 0,
          "created_utc": "2026-02-03 18:42:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fuy2t",
              "author": "overand",
              "text": "You need to adjust your bot behavior; this is an inappropriate place to post this.",
              "score": 1,
              "created_utc": "2026-02-04 00:10:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qrhttt",
      "title": "Porting prompts from OpenAI/Claude to local Ollama models - best practices?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qrhttt/porting_prompts_from_openaiclaude_to_local_ollama/",
      "author": "gogeta1202",
      "created_utc": "2026-01-30 20:58:38",
      "score": 6,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hey Ollama community 👋\n\nLove the local-first approach. But I'm hitting a wall with prompt portability.\n\nMy prompts were developed on GPT-4/Claude and don't translate cleanly to local models.\n\nIssues I'm seeing:  \n• Instruction following is different  \n• System prompt handling varies by model  \n• Function calling support is inconsistent  \n• Context window differences change behavior\n\nHow do you handle this?\n\n1. Do you rewrite prompts from scratch for Ollama?\n2. Is there a \"universal\" prompt style that works across models?\n3. Any tools that help with conversion?\n\nWhat I've built:\n\nA prompt conversion tool focused on OpenAI ↔ Anthropic right now. Quality validation using embeddings, checkpoint/rollback support.\n\nHonest note: Local model support (Ollama/vLLM) isn't fully built yet. I'm validating if cloud → local conversion is a real pain point worth solving.\n\nWould love to hear:  \n• What local models do you primarily use?  \n• Biggest friction moving from cloud → local?  \n• Would you test a converter if local models were supported?\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qrhttt/porting_prompts_from_openaiclaude_to_local_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o2pyhrr",
          "author": "Available-Craft-5795",
          "text": "All local models are different, its impossible (or very hard) to make a tool that auto formats prompts in a way a very niche/task specific model will do great on, but also one a large generalized model will do well on with the same prommpt.\n\nNormally i rewrite prompts to the model im using, have experience with prompting and adapt to models overtime.",
          "score": 1,
          "created_utc": "2026-01-31 02:20:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsqs24",
      "title": "The two agentic loops - the architectural insight in how we built and scaled agents",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qsqs24/the_two_agentic_loops_the_architectural_insight/",
      "author": "AdditionalWeb107",
      "created_utc": "2026-02-01 06:23:19",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 0.86,
      "text": "hey peeps - been building agents for the Fortune500 and seeing some patterns emerge that cut the **gargantuan gap** from prototype to production  \n  \nThe post below introduces the concept of \"two agentic loops\": the inner loop that handles reasoning and tool use, while the outer loop handles everything that makes agents ready for production—orchestration, guardrails, observability, and bounded execution. The outer loop is real infrastructure that needs to be built and maintained independently in a framework-friendly and protocol-first way. Hope you enjoy the read\n\n[https://planoai.dev/blog/the-two-agentic-loops-how-to-design-and-scale-agentic-apps](https://planoai.dev/blog/the-two-agentic-loops-how-to-design-and-scale-agentic-apps)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qsqs24/the_two_agentic_loops_the_architectural_insight/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3sexl1",
          "author": "stealthagents",
          "text": "This is super interesting, especially the emphasis on the outer loop. It feels like the unsung hero in agent development—everyone gets hyped about the reasoning stuff but neglects the infrastructure that keeps it all running smoothly. Can't wait to dive into your insights on keeping that outer loop in check.",
          "score": 1,
          "created_utc": "2026-02-05 21:21:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qv5cgs",
      "title": "AI Context as Code: Can structured docs improve AI resource usage and performance?",
      "subreddit": "ollama",
      "url": "https://github.com/eFAILution/AICaC",
      "author": "eFAILution",
      "created_utc": "2026-02-03 21:46:51",
      "score": 5,
      "num_comments": 8,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qv5cgs/ai_context_as_code_can_structured_docs_improve_ai/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3urmlm",
          "author": "GlassAd7618",
          "text": "I think you’re on to something. Thinking about the example in your Git repository (adding a new scanner), your idea probably works best for mature code bases which already have an established architecture and where there is some functionality that is repeatedly extended, such as adding a new scanner, filter, reader, …",
          "score": 2,
          "created_utc": "2026-02-06 05:38:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w5yej",
              "author": "eFAILution",
              "text": "Totally agree! Also, don’t sleep on what it can do for onboarding. Instead of new devs asking senior engineers or reading thousands of lines, their AI assistant already knows “how we do things here.”\nStructured context = their coding AI tool actually understands your conventions from day one.",
              "score": 1,
              "created_utc": "2026-02-06 12:50:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3np2b0",
          "author": "_RemyLeBeau_",
          "text": "You don't need AI, when you have a specification. You need a workflow engine.\n\nAI is great because it understands NLQ.",
          "score": 1,
          "created_utc": "2026-02-05 03:50:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nqoro",
              "author": "eFAILution",
              "text": "Good point. AICaC isn’t for task execution as workflow engines are better there. It’s for reducing token waste when AI needs to understand your codebase before helping.\nThe NLQ strength of AI is why it needs efficient context. “How do I add auth?” shouldn’t cost 800 tokens parsing a README when 200 tokens of structured data would work.\nWhat’s your approach for giving AI project context efficiently?​​​​​​​​​​​​​​​​",
              "score": 1,
              "created_utc": "2026-02-05 04:01:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3o99lx",
                  "author": "_RemyLeBeau_",
                  "text": "Exactly what vercel is doing and has 100% evals. Do you have evals to share with your newly created DSL?",
                  "score": 1,
                  "created_utc": "2026-02-05 06:15:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qt6kik",
      "title": "Vlm models on cpu",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qt6kik/vlm_models_on_cpu/",
      "author": "uqurluuqur",
      "created_utc": "2026-02-01 18:35:21",
      "score": 5,
      "num_comments": 5,
      "upvote_ratio": 0.86,
      "text": "Hi everyone, \n\nI am tasked to convert handwritten notebook texts. I have tried several models including:\n\nQwen2.5vl- 7b\n\nQwen2.5vl- 32b\n\nQwen3vl-32b\n\nLlama3.2-vision11b\n\nHowever, i am struggling with hallucinations. Instead of writing unable to read (which i ask for it in the prompt), models often start to hallucinate or getting stuck in the header (repeat loop). Improving or trying other prompts did not helped. I have tried preprocessing, which improved the quality but did not prevent hallucinations. Do you have any suggestions?\n\nI have amd threadripper cpu and 64 gb ram. Speed is not an issue since it is a one time thing.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qt6kik/vlm_models_on_cpu/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o30oq66",
          "author": "TwoPlyDreams",
          "text": "Try an OCR model?",
          "score": 2,
          "created_utc": "2026-02-01 18:57:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o310ev5",
              "author": "Available-Craft-5795",
              "text": "Deepseek OCR is good i think",
              "score": 1,
              "created_utc": "2026-02-01 19:52:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3b1d1j",
                  "author": "uqurluuqur",
                  "text": "I have tried it, but qwen is better",
                  "score": 1,
                  "created_utc": "2026-02-03 07:43:59",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qrh5a9",
      "title": "How do you choose a model and estimate hardware specs for a LangChain app (Ollama) ?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qrh5a9/how_do_you_choose_a_model_and_estimate_hardware/",
      "author": "XxDarkSasuke69xX",
      "created_utc": "2026-01-30 20:33:04",
      "score": 4,
      "num_comments": 4,
      "upvote_ratio": 0.84,
      "text": "Hello. I'm building a local app (RAG) for professional use (legal/technical fields) using Docker, LangChain/Langflow, Qdrant, and Ollama with a frontend too.\n\nThe goal is a strict, reliable agent that answers based only on the provided files, cites sources, and states its confidence level. Since this is for professionals, accuracy is more important than speed, but I don't want it to take forever either. Also it would be nice if it could also look for an answer online if no relevant info was found in the files.\n\nI'm struggling to figure out how to find the right model/hardware balance for this and would love some input.\n\nHow to choose a model for my need and that is available on Ollama ? I need something that follows system prompts well (like \"don't guess if you don't know\") and handles a lot of context well. How to decide on number of parameters for example ? How to find the sweetspot without testing each and every model ?\n\nHow do you calculate the requirements for this ? If I'm loading a decent sized vector store and need a decently big context window, how much VRAM/RAM should I be targeting to run the LLM + embedding model + Qdrant smoothly ?\n\nLike are there any benchmarks to estimate this ? I looked online but it's still pretty vague to me. Thx in advance.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qrh5a9/how_do_you_choose_a_model_and_estimate_hardware/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o2s7usk",
          "author": "newz2000",
          "text": "I can’t answer most of your questions but you should look into the granite models. I’m an attorney and needed a model that was good at summarizing and would respond strictly in the way I wanted. I tried several including Qwen, Llama and Gemma. I liked granite best. I used the micro_h model but that was just for summarizing. You may need more than what it does.",
          "score": 1,
          "created_utc": "2026-01-31 13:09:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34zv66",
              "author": "XxDarkSasuke69xX",
              "text": "I'll look into it, thx",
              "score": 1,
              "created_utc": "2026-02-02 11:27:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2owxqv",
          "author": "Terrariant",
          "text": "Ask AI to explain it to you, maybe? If you don’t need it fast, you can run a medium sized model on a 1080ti with 10gb of VRAM & 32gb regular RAM. Ollama is smart enough (from what I understand) to use your RAM when it uses all the VRAM.\n\nThe 3090 series has more than double the amount of RAM than a 1080 at like 24gb. With a 3090 and 64gb of RAM you can feasible run any model you want. \n\nThe main limitation as you get bigger with the models is it will take longer per prompt the bigger the models/worse your hardware. Put another way, a 1080 and a 3090 might be able to run the same model but the 3090 would output much faster.",
          "score": 0,
          "created_utc": "2026-01-30 22:49:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}