{
  "metadata": {
    "last_updated": "2026-01-17 16:48:01",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 18,
    "total_comments": 66,
    "file_size_bytes": 88675
  },
  "items": [
    {
      "id": "1qbrx4b",
      "title": "Open Source Enterprise Search Engine (Generative AI Powered)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qbrx4b/open_source_enterprise_search_engine_generative/",
      "author": "Effective-Ad2060",
      "created_utc": "2026-01-13 13:55:10",
      "score": 50,
      "num_comments": 4,
      "upvote_ratio": 0.96,
      "text": "Hey everyone!\n\nIâ€™m excited to share something weâ€™ve been building for the past 6 months, aÂ **fully open-source Enterprise Search Platform**Â designed to bring powerful Enterprise Search to every team, without vendor lock-in. The platform brings all your business data together and makes it searchable. It connects with apps like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, Local file uploads and more. You can deploy it and run it with just one docker compose command.\n\nYou can run the full platform locally. Recently, one of our users triedÂ **qwen3-vl:8b (16 FP)**Â withÂ **Ollama**Â and got very good results.\n\nThe entire system is built on aÂ **fully event-streaming architecture powered by Kafka**, making indexing and retrieval scalable, fault-tolerant, and real-time across large volumes of data.\n\nAt the core, the system uses an **Agentic Graph RAG approach**, where retrieval is guided by an enterprise knowledge graph and reasoning agents. Instead of treating documents as flat text, agents reason over relationships between users, teams, entities, documents, and permissions, allowing more accurate, explainable, and permission-aware answers.\n\n**Key features**\n\n* Deep understanding of documents, user, organization and teams with enterprise knowledge graph\n* Connect to any AI model of your choice including OpenAI, Gemini, Claude, or Ollama\n* Use any provider that supports OpenAI compatible endpoints\n* Choose from 1,000+ embedding models\n* Visual Citations for every answer\n* Vision-Language Models and OCR for visual or scanned docs\n* Login with Google, Microsoft, OAuth, or SSO\n* Rich REST APIs for developers\n* All major file types support including pdfs with images, diagrams and charts\n* Agent Builder - Perform actions like Sending mails, Schedule Meetings, etc along with Search, Deep research, Internet search and more\n* Reasoning Agent that plans before executing tasks\n* 40+ Connectors allowing you to connect to your entire business apps\n\nCheck it out and share your thoughts or feedback. Your feedback is immensely valuable and is much appreciated:  \n[https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)\n\nDemo Video:  \n[https://www.youtube.com/watch?v=xA9m3pwOgz8](https://www.youtube.com/watch?v=xA9m3pwOgz8)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qbrx4b/open_source_enterprise_search_engine_generative/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nzfb76t",
          "author": "Green-Ad-3964",
          "text": "wow, this sounds awesome and I'll surely test it. As a local LLM, would you suggest a VL or a plain LLM? Thanks.",
          "score": 2,
          "created_utc": "2026-01-13 21:27:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhkrq0",
              "author": "Effective-Ad2060",
              "text": "VL",
              "score": 2,
              "created_utc": "2026-01-14 05:00:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzmrd48",
          "author": "stealthagents",
          "text": "If you're aiming for better accuracy and context understanding, Iâ€™d lean towards a VL model. They tend to handle specific queries and nuances in data better, especially with all that enterprise info. Plus, the combination with your knowledge graph should really enhance the search quality.",
          "score": 1,
          "created_utc": "2026-01-14 23:16:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzocyl7",
              "author": "Effective-Ad2060",
              "text": "yes.. we already use VL model",
              "score": 1,
              "created_utc": "2026-01-15 04:54:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qd4snz",
      "title": "Building Opensource client sided Code Intelligence Engine -- Potentially deeper than Deep wiki :-) ( Need suggestions and feedback )",
      "subreddit": "ollama",
      "url": "https://v.redd.it/clkvriftsedg1",
      "author": "DeathShot7777",
      "created_utc": "2026-01-15 00:39:37",
      "score": 42,
      "num_comments": 12,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qd4snz/building_opensource_client_sided_code/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzn9gez",
          "author": "Spaceman_Splff",
          "text": "I donâ€™t even know whatâ€™s happening here but it looks awesome. Good work.",
          "score": 5,
          "created_utc": "2026-01-15 00:54:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nznaawx",
              "author": "DeathShot7777",
              "text": "![gif](giphy|sWBzg2D15WwQjHcxbt)",
              "score": 4,
              "created_utc": "2026-01-15 00:58:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzn91nd",
          "author": "UseHopeful8146",
          "text": "That looks pretty cool actually, any plans to write up docker configs?",
          "score": 2,
          "created_utc": "2026-01-15 00:51:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzn9typ",
              "author": "DeathShot7777",
              "text": "Right now I m trying to integrate ollama, external DB connect ( Neo4j ) and some way to auto track github for updates. If u wanna use it somewhere or got any ideas I would work on creating docker config though",
              "score": 2,
              "created_utc": "2026-01-15 00:56:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzodn5u",
                  "author": "UseHopeful8146",
                  "text": "No pressure, Iâ€™ll pm you or something if I put it together - gotta get the food off the plate I already got first lol",
                  "score": 2,
                  "created_utc": "2026-01-15 04:59:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzwx3ho",
          "author": "koldbringer77",
          "text": "This is most beautifull thing ive dreamt to code, the messy mess that works,  I propouse sth like Engram filtering to enchance human readability",
          "score": 2,
          "created_utc": "2026-01-16 13:10:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx1grm",
              "author": "DeathShot7777",
              "text": "Thanks means a lot. Idk what's Engram filter will check it out.\nIt crossed 300 stars I m so happy ðŸ˜­",
              "score": 1,
              "created_utc": "2026-01-16 13:35:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzt6le9",
          "author": "RGBrayan",
          "text": "I don't know much about it, but it looks very good. What does it consist of?",
          "score": 1,
          "created_utc": "2026-01-15 22:08:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztp7m7",
              "author": "DeathShot7777",
              "text": "That graph you are seeing is the knowledge graph created by parsing all the relations of your codebase. So this gives very deep insight about the codebases to LLMs as well as let's humans visualize it. \n\nI am trying to resolve the fundamental gap of incomplete context of the codebase in coding agents like cursor , claude code, etc. leading to them making breaking changes",
              "score": 2,
              "created_utc": "2026-01-15 23:43:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o008k4w",
          "author": "BallinwithPaint",
          "text": "This sounds super interesting! I've been looking for something that handles code relations better than standard context tools. Iâ€™m going to give it a spin soonâ€”if it clicks, Iâ€™d love to dig into the repo and see if I can contribute some of those ideas you mentioned. Just starred it!",
          "score": 1,
          "created_utc": "2026-01-16 22:29:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o008z2x",
              "author": "DeathShot7777",
              "text": "Thanks..feel free to raise issues or DM me or use the github discussions",
              "score": 1,
              "created_utc": "2026-01-16 22:31:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qb4jaq",
      "title": "Chat With Your Favorite GitHub Repositories via CLI with the new RAGLight Feature",
      "subreddit": "ollama",
      "url": "https://v.redd.it/2lu95uwtyycg1",
      "author": "Labess40",
      "created_utc": "2026-01-12 19:27:02",
      "score": 25,
      "num_comments": 3,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qb4jaq/chat_with_your_favorite_github_repositories_via/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzhts0g",
          "author": "Whyme-__-",
          "text": "Broski itâ€™s admirable to build something of your own. for local setup using ollama itâ€™s great. But if you have multiple GitHub repos and you are trying to extract algorithms. Then just dump the Md file into Gemini and go to town with it. Itâ€™s 1m context length is unmatched to any opensource model",
          "score": 1,
          "created_utc": "2026-01-14 06:09:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhwmg1",
              "author": "Labess40",
              "text": "You're right, but if you are in an industrial context or you're data are sensible, sometimes you don't want or you can't share your data with a remote LLM provider. And RAGLight is more than a CLI tool. You can use it in your codebase to setup easily a RAG or an Agentic RAG with freedom to modify some pieces of it (data readers, models, providers,...).\nBut I agree, for many usecases, using gemini 1m contexte length is better, but for your private or professional usecases, have an alternative is also useful.",
              "score": 1,
              "created_utc": "2026-01-14 06:33:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzhwpwv",
                  "author": "Whyme-__-",
                  "text": "I donâ€™t disagree with your points",
                  "score": 2,
                  "created_utc": "2026-01-14 06:33:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qb06tm",
      "title": "Docker on Linux or Nah?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qb06tm/docker_on_linux_or_nah/",
      "author": "Honest-Cheesecake275",
      "created_utc": "2026-01-12 16:53:05",
      "score": 13,
      "num_comments": 21,
      "upvote_ratio": 0.84,
      "text": "My ADHD impulses got the better of me and I jumped the gun and installed Ollama locally. Then installed the Docker container then saw that there is a Docker container that streamlines setup of WebUI. \n\nWhatâ€™s the most idiot proof way to set this up?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qb06tm/docker_on_linux_or_nah/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nz74q2j",
          "author": "florinandrei",
          "text": "> Whatâ€™s the most idiot proof way to set this up?\n\nDepends on the idiot.\n\nOn Linux, this idiot runs Ollama in a container, and Open WebUI in another container, pointing at Ollama. I even have two machines running Ollama, with very different amounts of RAM, and so different models saved; Open WebUI uses both as backends (and runs on a third machine because that's how my home network rolls).\n\nBut you could definitely do it all on one machine with two containers.\n\nI have shell scripts that I run and just pull the latest images and re-launch the containers. Upgrades are no-brainers this way. This is the main reason why I run Ollama in Docker. If I reboot the machine, the containers start again automatically. This is the Ollama update script (the one for Open WebUI is similar):\n\n    docker stop ollama && echo \"stopped the ollama container\" || echo \"could not stop the ollama container\"\n    docker rm ollama && echo \"removed the ollama container\" || echo \"could not remove the ollama container\"\n    docker pull ollama/ollama:latest\n    \n    # the host part is not really needed for Ollama\n    docker run -d --restart always \\\n    \t--gpus=all \\\n    \t-p 11434:11434 \\\n    \t--add-host=host.docker.internal:host-gateway \\\n    \t-v ollama:/root/.ollama \\\n    \t--name ollama \\\n    \tollama/ollama:latest\n    \n    (\n    \tsleep 1\n    \techo; echo\n    \tdocker logs -f ollama &\n    \tDOCKER_LOGS_PID=$!\n    \tsleep 30\n    \tkill $DOCKER_LOGS_PID\n    ) &\n\nYou need to install the NVIDIA Docker compatibility layer, for containers to access the GPU. On Ubuntu, that's a package called `nvidia-container-toolkit`.\n\nYou can even invoke the `ollama` command from the terminal as usual, even when it runs in a container. But you do have to distinguish between interactive and non-interactive sessions, so I have this in `~/.bash_aliases` :\n\n    # shell functions\n    ollama() {\n        if [ -t 0 ]; then\n    \t# this is a terminal, allocate a pseudo-TTY\n            docker exec -it ollama ollama \"$@\"\n        else\n    \t# this is not a terminal, do not allocate a pseudo-TTY\n            docker exec -i ollama ollama \"$@\"\n        fi\n    }\n\nAnd that allows me to invoke the `ollama` command as if it were running on the host.\n\nCheck the status of your containers with `docker ps -a`.",
          "score": 8,
          "created_utc": "2026-01-12 17:39:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz7aawn",
              "author": "Honest-Cheesecake275",
              "text": "Legend",
              "score": 2,
              "created_utc": "2026-01-12 18:04:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz7bqfb",
                  "author": "florinandrei",
                  "text": "Here's the update script for Open WebUI:\n\n    docker stop open-webui\n    docker rm open-webui\n    docker pull ghcr.io/open-webui/open-webui:main\n    \n    docker run -d -p 3000:8080 \\\n    \t--add-host=host.docker.internal:host-gateway \\\n    \t-v open-webui:/app/backend/data \\\n    \t--name open-webui \\\n    \t--restart always \\\n    \tghcr.io/open-webui/open-webui:main\n    \n    docker logs -f open-webui\n\nThis one does not let go of logs at the end, Open WebUI is a bit more finicky that way, so I prefer to wait until it's stable before I let it go. Keep in mind, this is the Open WebUI version without GPU enabled (GPU access is only enabled for Ollama); you may prefer otherwise.\n\n`host.docker.internal` is the host. You will use this in the Open WebUI settings if Ollama runs on the same machine. Otherwise it's not important.\n\nBoth update scripts simply restart the containers if there's nothing to update. They are also \"installer\" scripts. In other words, idiot-proof. I like it that way.",
                  "score": 2,
                  "created_utc": "2026-01-12 18:11:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzhimxj",
              "author": "p_235615",
              "text": "You dont need update scripts... You can either have a docker management like Arcane, where you can update stuff with a click of a button in a webUI, or even better like I use is a docker container called Watchtower. \nYou can set it to either update all the docker containers automatically, or set that only the ones you add the docker label \"com.centurylinklabs.watchtower.enable=true\" are being automatically updated.\nYou can set the timer for it and even send notifications.\n\nAnd I run both ollama + open-webui in a single docker-compose.yaml file, that way its easier to setup the connection between them.",
              "score": 1,
              "created_utc": "2026-01-14 04:45:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzktqam",
                  "author": "florinandrei",
                  "text": "If only there were one single solution to all the world's problems. /s",
                  "score": 1,
                  "created_utc": "2026-01-14 17:56:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz6vwhi",
          "author": "UnbeliebteMeinung",
          "text": "You should always do all stuff inside docker.",
          "score": 6,
          "created_utc": "2026-01-12 16:58:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz70eh3",
          "author": "nicksterling",
          "text": "The most difficult part of using ollama with docker is to ensure the GPU passthrough is working properly otherwise youâ€™ll only be on CPU inferencing.",
          "score": 1,
          "created_utc": "2026-01-12 17:19:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz75rhm",
              "author": "florinandrei",
              "text": "It's just one package that needs to be installed, and using `--gpus=all` when launching the container. It's not hard.",
              "score": 3,
              "created_utc": "2026-01-12 17:44:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz9tuj5",
                  "author": "nicksterling",
                  "text": "The nvidia container toolkit can be a little finicky to get configured on some distros. Itâ€™s better than it used to be at least.",
                  "score": 2,
                  "created_utc": "2026-01-13 01:32:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzc8bs9",
              "author": "ComedianObjective572",
              "text": "Agree docker config took a while hahaha",
              "score": 1,
              "created_utc": "2026-01-13 12:10:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz73lu0",
              "author": "UseHopeful8146",
              "text": "Iâ€™ve been meaning to look into this since I did a big hardware upgrade - I could absolutely find it myself with enough time but do you know off top where to get info on configuring ollama containers for gpu pass through?",
              "score": 0,
              "created_utc": "2026-01-12 17:34:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz74q6c",
                  "author": "nicksterling",
                  "text": "The best place is to look at the Dockerhub page for Ollama: https://hub.docker.com/r/ollama/ollama",
                  "score": 3,
                  "created_utc": "2026-01-12 17:39:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz7atfu",
          "author": "Bakkario",
          "text": "I am actually using distrobox- I have both ollama and webui installed inside distrobox instead of docker ðŸ˜…",
          "score": 1,
          "created_utc": "2026-01-12 18:06:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7f8vb",
          "author": "microcandella",
          "text": "most that use docker heavily hate it on windows for what it's worth.",
          "score": 1,
          "created_utc": "2026-01-12 18:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz96d0s",
              "author": "florinandrei",
              "text": "Yeah, Docker Desktop is not really meant to run \"services\". I have the same problem on macOS also, and again it boils down to Docker Desktop being, well, a desktop app. You can make it work, eventually, but it's brittle and it's not elegant.\n\nOn Linux, you [install Docker CE](https://docs.docker.com/engine/install/), and persistent containers behave the way you'd expect a service to behave. Solid, reliable, elegant.\n\nAs long as you run containers in one-off mode, Windows and macOS are fine. I build Docker images and run ad-hoc containers all the time on both macOS and Windows, and both are perfectly fine this way, including GPU support. It's persistent containers that are brittle on these platforms.\n\nOn Windows and macOS I just install the Ollama app, and I don't mess with it very much. I have a dual-boot machine, Linux and Windows, I run Ollama on both disks: on Linux via Docker CE, on Windows via the native app. Both are backends for Open WebUI running in Docker on another machine. Both backends are equally reliable.",
              "score": 1,
              "created_utc": "2026-01-12 23:24:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz72c9p",
          "author": "merguel",
          "text": "I did that with an RTX 3060 12GB, and Ollama leaves the RTX's temperature at 90Â°C and higher, even when you stop using it. With KoboldCPP or LM Studio, that doesn't happen; they don't go above 73Â°C. Apparently, they're gentler on the hardware. Ollama pushes it to its limits and leaves it at that temperature.",
          "score": 1,
          "created_utc": "2026-01-12 17:28:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz75idk",
              "author": "florinandrei",
              "text": "Your setup is just broken.",
              "score": 2,
              "created_utc": "2026-01-12 17:43:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz7b0bp",
              "author": "Just-Syllabub-2194",
              "text": "you can fix that issue with following command\n\n**docker update --cpus \"2.0\"  your-ollama-container**\n\njust limit the cpus or gpus usage and the hardware temperature will stay low",
              "score": 1,
              "created_utc": "2026-01-12 18:07:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz86sqf",
                  "author": "florinandrei",
                  "text": "Their problem is bigger than a mere limit. Something is fundamentally wrong with their setup. Ollama should not be doing that.",
                  "score": 1,
                  "created_utc": "2026-01-12 20:33:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q9ff6k",
      "title": "what AI models can I run locally on my PC with Ollama?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q9ff6k/what_ai_models_can_i_run_locally_on_my_pc_with/",
      "author": "Kitchen-Patience8176",
      "created_utc": "2026-01-10 20:57:13",
      "score": 12,
      "num_comments": 28,
      "upvote_ratio": 0.83,
      "text": "Hey everyone,  \nIâ€™m pretty new to local AI and still learning, so sorry if this is a basic question.\n\nI canâ€™t afford a ChatGPT subscription anymore due to financial reasons, so Iâ€™m trying to use **local models** instead. Iâ€™ve installed **Ollama**, and it works, but I donâ€™t really know which models I should be using or what my PC can realistically handle.\n\n**My specs:**\n\n* Ryzen 9 5900X\n* RTX 3080 (10GB VRAM)\n* 32GB RAM\n* 2TB NVMe SSD\n\nIâ€™m mainly curious about:\n\n* Which models run well on this setup\n* What I *canâ€™t* run\n* How close local models can get to ChatGPT\n* If things like web search, fact-checking, or up-to-date info are possible locally (or any workarounds)\n\nAny beginner advice or model recommendations would really help.\n\n  \nThanks ðŸ™",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q9ff6k/what_ai_models_can_i_run_locally_on_my_pc_with/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nyvaqyy",
          "author": "Birdinhandandbush",
          "text": "If you're starting out, use LM Studio as well, and this is for a learning reason. In LM Studio when you look up a model, there's a little graphic beside it that says things like Partial Offload, Full Offload, and an icon of a rocketship that shows your system will run that model like a rocket basically. You might not even use LM Studio, but you can then go to Ollama and install the same model and you'll know what result to expect.",
          "score": 16,
          "created_utc": "2026-01-10 22:49:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzzdlm",
              "author": "tyrants666",
              "text": "Nice Approach will give a try",
              "score": 2,
              "created_utc": "2026-01-11 16:58:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyupfn2",
          "author": "Blueberry_Dependent",
          "text": "I just try. Download the lower one first, if it works good then go for a heavier one",
          "score": 8,
          "created_utc": "2026-01-10 21:03:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyv3f2u",
          "author": "-Akos-",
          "text": "If you want websearch, youâ€™ll need something like MCP. Consider LM Studio, look for websearch MCP (https://github.com/mrkrsl/web-search-mcp works for me), the look for lfm 2.5 which is a small model, but lightning fast and can do toolcalling.",
          "score": 6,
          "created_utc": "2026-01-10 22:12:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyv62y7",
              "author": "TheBrainPolice",
              "text": "The difference in my experience between Ollama and LM Studio is the size of models I am able to run (ollama > lm studio).",
              "score": 1,
              "created_utc": "2026-01-10 22:25:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyvpo8s",
                  "author": "-Akos-",
                  "text": "Thatâ€™s odd, my experience has been the opposite. I have a 4gb 1050 in a 8th gen i7 laptop, and Iâ€™m able to run the same size, but have easier/better tweaking options. I run granite 4 with 50000 tokens, with web search, and own MCP modules I wrote myself with fastmcp. LFM 2.5 I run with 128000 tokens. That one can do 51tps too, on a potato laptop thatâ€™s crazy.",
                  "score": 3,
                  "created_utc": "2026-01-11 00:07:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyutjzc",
          "author": "Own_Attention_3392",
          "text": "\\- GPT-OSS 20B is worth a shot -- it can offload layers to system RAM while still maintaining reasonable performance which will help since you don't have a lot of VRAM to work with. [https://huggingface.co/unsloth/gpt-oss-20b-GGUF](https://huggingface.co/unsloth/gpt-oss-20b-GGUF)\n\nFor dense models, something like Llama 3.1 7B would run pretty well on that setup. You'd want to find a reasonable quantization -- something [https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n\nYou could probably squeeze a dense model up to about 12-13B on there, although I haven't run models that small for a while so I don't have any good recommendations in that size range. \n\n\\- You can't run, well, most things. You're very severely constrained by VRAM when running LLMs. The second you exceed VRAM, it starts to offload to system RAM, and system RAM is much slower. No models you are capable of running will be anywhere near as \"smart\" as ChatGPT, and you won't be able to get anything near that level without spending tens of thousands of dollars on hardware. For dense models like Llama, the maximum size you can run is roughly \"your total VRAM minus \\~1-2 GB\". For MoE models like GPT-OSS, the maximum size you can run is roughly \"your total system RAM minus 1-2 GB\". \n\n\\- Web searching and similar can be accomplished via a web search MCP tool, but the model has to understand how to do tool calling -- GPT-OSS definitely can. MCP basically exposes an external tool to an LLM with instructions on how to use it, and the LLM can choose how and when to use the tool. There are a few web search MCP tools out there, but I haven't used MCP in conjunction with Ollama so I'll leave others to explain how to set up.",
          "score": 3,
          "created_utc": "2026-01-10 21:23:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyuppnx",
          "author": "ghormeh_sabzi",
          "text": "There's a ton you can run with that setup, lots of options for different applications, but for the best immediate experience and getting started gpt-oss:20b and qwen3:30b",
          "score": 4,
          "created_utc": "2026-01-10 21:04:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyv43hv",
              "author": "neil_555",
              "text": "Qwen3-4b-thinking-2507 would be better as it would allow use of a less quantized version (Q8 version should just about fit into memory)\n\nEdited, i thought the BF16 version would fit but not in 32GB (unless the context window was reduced considerably)",
              "score": 1,
              "created_utc": "2026-01-10 22:15:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyxdzen",
          "author": "alhinai_03",
          "text": "You could try MoE models, like gpt-oss 20b, granite -4.0-small..etc. Idk about ollama though, if it supports offloading experts to CPU, if not, try lm studio, jan, or even raw llama.cpp which I use and get a lot faster inference than the rest. With MoE models you can have very big models running very fast without using a lot of vram, saving space for context size.",
          "score": 2,
          "created_utc": "2026-01-11 05:53:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbx50o",
          "author": "shsjsisnejd",
          "text": "I have same setup just different cpu, here is my recommendation:\n\nFor general purpose: gpt oss 20b - q4 ( you can go as far as q6)  \nFor coding specific: qwen 3 coder 30B-A3B q6\n\nBoth are MoE model so it runs pretty great on setup that lacks of vram ( offload to system ram). Also, try to setup  llama.cpp, for my experience it has the best performance ( don't know why but it often 4x 6x faster than lmstudio, both at default setting, no tweaks).\n\nThen you can try other one like glm, deepseek, gemma model that is 12B or less, though I would say gpt oss 20b is the best one in term of general purpose in my personal experience. Gemma comes in second, GLM and deepseek I often experience weird answer, like overthinking stuff on really simple joke or question.",
          "score": 2,
          "created_utc": "2026-01-13 10:37:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyw43j8",
          "author": "Excellent_Spell1677",
          "text": "Try Gemma3n:E4b, Qwen3:14b, granite4:tiny-h, llama2:13b to start.  Keep expectations realistic but they are very good.",
          "score": 1,
          "created_utc": "2026-01-11 01:24:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyxlvce",
          "author": "CooperDK",
          "text": "You could try Gemma-3-7B or Gemma-3N-E4B. They are amazing for their size, AND they have built-in vision. You should be able to run the latter with high context.\nBut seriously, use LM Studio. It is much easier to configure and much faster than ollama. It uses llamacpp as engine.",
          "score": 1,
          "created_utc": "2026-01-11 06:57:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyxmjxp",
          "author": "960be6dde311",
          "text": "Granite4",
          "score": 1,
          "created_utc": "2026-01-11 07:03:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz0zwat",
          "author": "Jayfree138",
          "text": "Take your VRAM minus 3 GB or so and that's the size model you can run normally. Depending on how long the prompt (context window) is. The more RAM you use, the slower the model will run until it's unbearable. Ideally AI is not meant to work with RAM at all. Its meant for VRAM only.\n\nSo 7b or so is for you. But don't expect it to be anything near as good as ChatGPT. This explanation is simplified. It gets complicated.",
          "score": 1,
          "created_utc": "2026-01-11 19:44:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz11rqh",
          "author": "most_crispy_owl",
          "text": "Microsoft's phi4",
          "score": 1,
          "created_utc": "2026-01-11 19:52:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4qb83",
          "author": "XertonOne",
          "text": "LM Studio is better choice because it suggests you what to pick",
          "score": 1,
          "created_utc": "2026-01-12 08:39:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlmnv9",
          "author": "florinandrei",
          "text": "> Which models run well on this setup\n\n> What I canâ€™t run\n\nIt's mostly about the different kinds of RAM.\n\nA model that can fit in the VRAM of your GPU will always run fast. On the model page, look at the Size column. If it's less than the VRAM, it will stay in the VRAM.\n\nhttps://ollama.com/library/gemma3\n\nThere are some tricks you could do with offloading parts of the model to system RAM, but a model will always run more slowly that way, all else being equal.\n\nSo, look at any model family you like, and pick the versions with a Size less than 10 GB. E.g. gemma3:12b will fit in your VRAM.\n\nScript to download and update models:\n\n    ollama pull gemma3:12b\n    ollama pull llama3.1:8b\n    ollama pull ministral-3:14b\n    ollama pull qwen3:14b\n\nIt's worth re-running it once every few weeks, because they sometimes update existing models.\n\n> How close local models can get to ChatGPT\n\nRigorous performance evaluations and benchmarking are notoriously difficult with LLMs. So the best answer is: they're not quite as smart as frontier models, but they can do stuff.\n\n> If things like web search, fact-checking, or up-to-date info are possible locally (or any workarounds)\n\nThere are many ways to skin this cat. Here's one:\n\nAssuming your OS is Windows, you could run Open WebUI in Docker, and enable web search and other tools.\n\n- install Docker Desktop\n- configure owui to run in a container in Docker\n- point it at Ollama\n- play with web search settings, etc.\n\nScript to download, install, and update owui (re-running it once in a while will update the image):\n\n    docker stop open-webui\n    docker rm open-webui\n    docker pull ghcr.io/open-webui/open-webui:main\n\nScript to run owui:\n\n    docker run -it --rm -p 3000:8080 \\\n    \t--add-host=host.docker.internal:host-gateway \\\n    \t-v open-webui:/app/backend/data \\\n    \t--name open-webui \\\n    \tghcr.io/open-webui/open-webui:main\n\nIf that's a .bat script then replace \\ with ^\n\nIf that's a bash script in WSL then it's good to go.\n\nOllama will be available to Open WebUI as `host.docker.internal` on port 11434.\n\nYou will browse to Open WebUI at http://localhost:3000/\n\nI prefer to invoke owui from a script when I need it. Running containers as services in Docker Desktop is kind of janky.",
          "score": 1,
          "created_utc": "2026-01-14 20:05:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxh53u",
          "author": "donotfire",
          "text": "You can get a good deal on cloud subscriptions here: r/discountden7 I got a few things from the guy and heâ€™s super professional",
          "score": 1,
          "created_utc": "2026-01-16 14:55:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyup27q",
          "author": "XxAnomo305",
          "text": "the max would be around a 32-34b model.",
          "score": -1,
          "created_utc": "2026-01-10 21:01:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyuqr8k",
              "author": "Own_Attention_3392",
              "text": "There's no way you're running a dense model bigger than around 12b with that little vram unless you're okay with it being ridiculously slow. 7b would probably be the sweet spot.",
              "score": 3,
              "created_utc": "2026-01-10 21:09:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyxm5fp",
                  "author": "CooperDK",
                  "text": "You can't run a 12b. That requires about 11 GB without the context. With 16384 ctx it is near 13.",
                  "score": 1,
                  "created_utc": "2026-01-11 07:00:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyuqbdd",
              "author": "Kitchen-Patience8176",
              "text": "Would it handle longer chats (like 10â€“15 messages) and still remember the context?",
              "score": 1,
              "created_utc": "2026-01-10 21:07:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyur03z",
                  "author": "XxAnomo305",
                  "text": "it should be able to remember at least a few depending on tokens used,  but if you want it to be stable and not overload your system just go with a smaller model like a 30 or a bit lower will give you buffer room.",
                  "score": 1,
                  "created_utc": "2026-01-10 21:11:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyxj17w",
          "author": "RickLXI",
          "text": "I run ollama and openwebui and similar system as yours. Running an 8 to 12b is OK. 20b gets really slow when doing anything complicated. More vram is what you want.",
          "score": 0,
          "created_utc": "2026-01-11 06:33:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q9svp5",
      "title": "Looking for open source contributers | LocalAgent",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q9svp5/looking_for_open_source_contributers_localagent/",
      "author": "FriendshipCreepy8045",
      "created_utc": "2026-01-11 07:11:25",
      "score": 9,
      "num_comments": 4,
      "upvote_ratio": 0.81,
      "text": "Hi All,  \nHope you're all doing well.\n\nSo little background: I'm a frontend/performance engineer working as an IT consultant for the past year or so.  \nRecently made a goal to learn and code more in python and basically entering the field of AI Applied engineering.  \nI'm still learning concepts but with a little knowledge and claude, I made a researcher assistent that runs entirly on laptop(if you have a descent one using Ollama) or just use the default cloud.\n\nI understand langchain quite a bit and might be worth checking out langraph to somehow migrate it into more controlled research assistent(controlling tools,tokens used etc.).  \nSo I need your help, I would really appretiate if you guys go ahead and check \"[https://github.com/vedas-dixit/LocalAgent](https://github.com/vedas-dixit/LocalAgent)\" and let me know:\n\nYour thoughts | Potential Improvements | Guidance \\*what i did right/wrong\n\nor if i may ask, just some meaningful contribution to the project if you have time ;).\n\nI posted about this like idk a month ago and got 100+ stars in a week so might have some potential but idk.\n\nThanks.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q9svp5/looking_for_open_source_contributers_localagent/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nz0971n",
          "author": "kitanokikori",
          "text": "I saw this project earlier, it's actually quite nice - if you're looking for things to improve, I would consider trying to build a local, less overwhelming/all-encompassing version of https://clawd.bot - some missing pieces:\n\n- general MCP support\n- easy chat connectors to a few platforms (Telegram is the easiest to implement)\n- an importer / sync for local content, so people can sync local Obsidian notes or maybe like Paperless content (i.e. PDFs)\n- A self-contained docker-compose example that contains all of the services into one big blob",
          "score": 3,
          "created_utc": "2026-01-11 17:45:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4wgzs",
              "author": "FriendshipCreepy8045",
              "text": "I'll certainly try this, thank you!",
              "score": 1,
              "created_utc": "2026-01-12 09:39:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzbpgjj",
          "author": "BackUpBiii",
          "text": "Please check out my GitHub I built a fully free local ide from the ground up using only AI as I am no programmer, but hey it worked!\n\nItsmehrawrxd is my GitHub itâ€™s RawrXD aka the IDE and there are tons more from Java to http",
          "score": 1,
          "created_utc": "2026-01-13 09:24:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8x86x",
          "author": "Suitable-Program-181",
          "text": "Do something good and learn rust. Not a language groupie myself, but by facts, A.I needs speed, you cant get speed with GC's and you need to talk to hardware with less latency possible. Learn python to unlearn it fast.",
          "score": 0,
          "created_utc": "2026-01-12 22:37:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbk8l4",
      "title": "Privacy-First Voice Assistant with AI web-enabled search",
      "subreddit": "ollama",
      "url": "https://i.redd.it/gt9geaosa2dg1.jpeg",
      "author": "dsept",
      "created_utc": "2026-01-13 06:37:21",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qbk8l4/privacyfirst_voice_assistant_with_ai_webenabled/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qbwnjf",
      "title": "Seeking Advice: Deploying Local LLMs for a Large-Scale Food & Goods Distributor",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qbwnjf/seeking_advice_deploying_local_llms_for_a/",
      "author": "JPedrroo",
      "created_utc": "2026-01-13 17:04:14",
      "score": 9,
      "num_comments": 4,
      "upvote_ratio": 0.91,
      "text": "Hi everyone! Iâ€™m a Software Analyst and Developer for a major distribution company in the state of Bahia, Brazil. We handle a massive operation ranging from food and beverages to cosmetics and hygiene products, serving basically the entire state in terms of city coverage.\n\nI am currently exploring the possibility of implementing a local AI infrastructure to enhance productivity while maintaining strict privacy over our data. I am not an expert in AI, so I am still figuring out the best way to start. I have tested some local LLMs on my laptop, but I am unfamiliar with the technical nuances involved in a large-scale corporate implementation.\n\nInitially, I thought of developing a system that reads database entries regarding expiry dates and turnover rates in our warehouse. The goal would be to automatically recommend flash promotions or stock transfers to our retail branches before products expire.\n\nI'm seeking any feedback on thisâ€”past experiences, technical advice, additional use case ideas, or anything relevant. Thank you all for your time and for any insights you can share!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qbwnjf/seeking_advice_deploying_local_llms_for_a/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nzmimxb",
          "author": "AdditionalWeb107",
          "text": "I am not sure if I fully understood your use case in terms on when/how would users interact with these agents. Are these long-running tasks or would users chat with the agent too? What's on your checklist in terms of key features you need for the system.",
          "score": 1,
          "created_utc": "2026-01-14 22:31:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzrfqfu",
              "author": "JPedrroo",
              "text": "The model would mostly work on general data analysis, such as suggesting areas for cost reduction, finding patterns and alerting about deviations from those patterns, generating reports...\n\nIt would also be great if users could interact with the model like a chat, since in that sense it would be great for increasing productivity while guaranteeing the privacy of company data.\n\nI'm still developing the idea, I'm in a phase of understanding if it's really viable and where I could apply it here. Honestly, there are so many things that can be done... that's why I want to ask for advice on this, this world of AI in a corporate environment is very new to me.",
              "score": 1,
              "created_utc": "2026-01-15 17:21:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzsh3yl",
                  "author": "AdditionalWeb107",
                  "text": "you are really starting at the very top then - If you want to talk to a local model you have several options. But I would build an instruction prompt, setup ollama, use the openai client to talk to it via v1/chat/completions by updating the base\\_url field and run some workflow scenarios by appending some context and see how far you can get. \n\nThen comes multi-agent orchestration, observability, moderation -- delivery concerns:",
                  "score": 1,
                  "created_utc": "2026-01-15 20:10:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qepvki",
      "title": "Do you actually need prompt engineering to get value from AI?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qepvki/do_you_actually_need_prompt_engineering_to_get/",
      "author": "Xthebuilder",
      "created_utc": "2026-01-16 19:36:53",
      "score": 9,
      "num_comments": 39,
      "upvote_ratio": 0.91,
      "text": "\n\nIâ€™ve been using AI daily for about 6 months while building a local AI inferencing app, and one thing that surprised me is how little prompt engineering mattered compared to other factors.\n\nWhat ended up making the biggest difference for me was:\n\n* giving the model enough **context**\n* iterating on ideas *with* the model before writing real code\n* choosing models that are actually good at the specific task\n\nBecause LLMs have some randomness, I found theyâ€™re most useful early on, when youâ€™re still figuring things out. Iterating with the model helped surface bad assumptions before I committed to an approach. Theyâ€™re especially good at starting broad and narrowing down if you keep the conversation going so context builds up.\n\nWhen I add new features now, I donâ€™t explain my appâ€™s architecture anymore. I just link the relevant GitHub repos so the model can see how things are structured. That alone cut feature dev time from weeks to about a day in one case.\n\nIâ€™m not saying prompt engineering is useless, just that for most practical work, context, iteration, and model choice mattered more for me.\n\nCurious how others here approach this. Has prompt engineering been critical for you, or have you seen similar results?\n\n(I wrote up the full experience here if anyone wants more detail: [https://xthebuilder.github.io](https://xthebuilder.github.io))\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qepvki/do_you_actually_need_prompt_engineering_to_get/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nzzcjbw",
          "author": "Dry_Yam_4597",
          "text": "I dont prompt engineer. I ask an llm to write a prompt for another llm when needed. Context is more important.",
          "score": 13,
          "created_utc": "2026-01-16 19:57:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzpr49",
              "author": "Xthebuilder",
              "text": "I agree I also use that method when I want to use an engineered prompt for a workflow I just have another model make it though my language request",
              "score": 2,
              "created_utc": "2026-01-16 20:59:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o004pqy",
                  "author": "Dry_Yam_4597",
                  "text": "Nice. I actually run a couple of P100s just for that. One to analyse images, one to write a prompt, one to refine stuff and is fine tuned, and then i send the final prompt to say comfyui. Pretty neat. That's also how I work around nano banana's lame ai \"logo\". I just recreate images on local.",
                  "score": 2,
                  "created_utc": "2026-01-16 22:10:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzzozca",
          "author": "Purple-Programmer-7",
          "text": "Yes, itâ€™s critical. And weâ€™re calling it â€œcontext engineeringâ€ now.",
          "score": 6,
          "created_utc": "2026-01-16 20:55:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzq5fo",
              "author": "Xthebuilder",
              "text": "I like that term better than prompt engineering .",
              "score": 2,
              "created_utc": "2026-01-16 21:01:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzzns21",
          "author": "tom-mart",
          "text": "I'd argue that adding context is prompt engineering.",
          "score": 8,
          "created_utc": "2026-01-16 20:50:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzplac",
              "author": "Xthebuilder",
              "text": "Okay thankyou for the clarification",
              "score": 1,
              "created_utc": "2026-01-16 20:58:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzzt1jz",
          "author": "DeepInEvil",
          "text": "Pretty much this, prompt engineering is bs and for people who are new in the field. Afair r/airealist had an article on that.",
          "score": 3,
          "created_utc": "2026-01-16 21:14:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o004od6",
          "author": "Shoddy-Tutor9563",
          "text": "I can't stop laughing seeing how others are sometimes trying to squeeze out a bit of additional performance from a model in agentic tasks by putting different personas \"hat\" on it. E.g. \"you are super senior 10x engineer ...\"",
          "score": 2,
          "created_utc": "2026-01-16 22:10:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00gev7",
              "author": "Xthebuilder",
              "text": "Lmaooo you are so right I have personally never had much  use of personalities for the LLM workflows I have been using",
              "score": 1,
              "created_utc": "2026-01-16 23:09:55",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o00y705",
              "author": "track0x2",
              "text": "Iâ€™ve found that only useful for emulation. â€œYou are Dr. Seuss.â€",
              "score": 1,
              "created_utc": "2026-01-17 00:51:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o011pfa",
              "author": "NoAdministration6906",
              "text": "I know its funny but it actually works. Response are much better",
              "score": 1,
              "created_utc": "2026-01-17 01:12:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o00ahv3",
          "author": "MrSomethingred",
          "text": "From my understanding of the history of it, \"Prompt Engineering\" was more important in the GPT4 era when you had to say things like \"fix this or I will explode 4000 puppies with Napalm\" and there was nuance in the art of getting them to actually follow instructions. E.g. getting them to actually output in JSON consistently was a nightmare\n\n\nModern models are pretty good at rule following these days so there isn't really any secrets to prompting them \"correctly\" these days\n\n\nAs you highlighted, context management is far more important these days, hence the new buzzword is \"context engineering\"Â ",
          "score": 2,
          "created_utc": "2026-01-16 22:39:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00guz2",
              "author": "Xthebuilder",
              "text": "I like that , context engineering feels more like what Iâ€™m doing and you can relate that to say just having a conversation , regular folk can get to understand that suing AI effectively isnâ€™t rocket science",
              "score": 1,
              "created_utc": "2026-01-16 23:12:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00g7ji",
          "author": "HeavyDluxe",
          "text": "Prompt engineering is (IMHO) best understood as one \\_facet\\_ of context engineering.  The model makes its predictions based on the contextual information it's delivered.  In early models, carefully crafting a prompt was really the best you could do to ground the model's output.  We have many, many more tools available to us now to set a meaningful foundation for the model to use in generating output.\n\nIf you give the model tons of \\_really good\\_ information in  the codebase, supporting documents, etc etc etc, the user prompt becomes less and less critical.\n\nThe illustration I use at work is to imagine a random stranger comes up at you with a pile of papers, tells you to summarize the data therein for them, and, if you do a good job, you get $1M.  Imagine how you the quality of your product improves if the data has good labels... or if there's a previous report drawing on similar data that's there as an example. Or if the person also tells you what industry they're in.  Or if they tell you to \"imagine you're a customer service manager\" or whatever.\n\nEach little bit of information available improves your output.  The prompt is vital if that's all you give the model.  But context is \\_everything\\_.",
          "score": 2,
          "created_utc": "2026-01-16 23:08:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02rmqo",
              "author": "Xthebuilder",
              "text": "Basically itâ€™s like data cleaning for your AI output pipeline, itâ€™s really conceptual but it cuts across a lot of LLM interactions as the base of whatâ€™s controlling the models response",
              "score": 1,
              "created_utc": "2026-01-17 08:54:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00gr19",
          "author": "Additional-Actuary-8",
          "text": "I think the key is to ask what exactly you want rather than design tons of prompts. \n\nAs a human I also want my ai to answer shorter and concise.",
          "score": 2,
          "created_utc": "2026-01-16 23:11:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00h1zm",
              "author": "Xthebuilder",
              "text": "I find myself wanting the models to be more concise across the board too , youâ€™re correct about being specific about what you want from the model , sounds more like communciation skills",
              "score": 1,
              "created_utc": "2026-01-16 23:13:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00jt2d",
          "author": "Low-Opening25",
          "text": "no, you need context management",
          "score": 2,
          "created_utc": "2026-01-16 23:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00onyo",
              "author": "Xthebuilder",
              "text": "Agreed agreed",
              "score": 1,
              "created_utc": "2026-01-16 23:56:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00zxfy",
          "author": "DaleCooperHS",
          "text": "I dont see the difference. prompt engineering is not writing a prompt, is the process that ends with writing a prompt. Now that process can be done mentally or trought iteration and research . cna be pages long of iteration with an LLm or one 30 lines prompt. Is still prompt engineering.  \nHowever context is not cheap.. so ideally you want to condense all the necessary context and prune out all taht is not.. leaving with the shorter set of instructions and context necessary to do the job, so that the maximum ammount of still avaible context can be used to actually do the job.",
          "score": 2,
          "created_utc": "2026-01-17 01:01:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0120v8",
          "author": "NoAdministration6906",
          "text": "There is also a fine line in giving context, too much context also make the model hallucinations. Be aware of the context window tokens for a model.",
          "score": 2,
          "created_utc": "2026-01-17 01:14:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01mo1i",
              "author": "Xthebuilder",
              "text": "Good point I havenâ€™t really considered context in token window size too much but maybe that adjustment will lead to further optimization",
              "score": 1,
              "created_utc": "2026-01-17 03:25:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o017umx",
          "author": "cointalkz",
          "text": "Most LLMs, diffusion models or whatever you are using have documentation. Take that documentation into your LLM of choice and train it on how to best write prompts for said model. Profit!",
          "score": 2,
          "created_utc": "2026-01-17 01:52:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01ms94",
              "author": "Xthebuilder",
              "text": "Ooo like full circle training the model on its own developer written documentation",
              "score": 1,
              "created_utc": "2026-01-17 03:26:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o01nxl8",
                  "author": "cointalkz",
                  "text": "Bingo",
                  "score": 1,
                  "created_utc": "2026-01-17 03:34:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o01lqth",
          "author": "fasti-au",
          "text": "Itâ€™s a tipping scale.  \n\n1 million context but your really only pulling lie 16k tokens to process.  So in some cases you get cleaner runs but you need to one shot not correct and now think is breaking things to boiler plating itâ€™s now more important to back track not correct. \n\nDoes prompting matter.   Not until It does.  \n\nIâ€™m bad at words out to human but I can cintext a model better than most and build library systems and I tells ya.   Code the tool teacher model to know how to pull and when.  How you get that code is irrelevant in many ways because itâ€™s all exiting concepts and process with small adjustments so nothing is special beyond the results \n\n\nSo promt engineering is about repetative stability not the day to day in many ways",
          "score": 2,
          "created_utc": "2026-01-17 03:19:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01mjmh",
              "author": "Xthebuilder",
              "text": "I like how you put it , if you can get the same result many times over you can trust the system more overall",
              "score": 1,
              "created_utc": "2026-01-17 03:25:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o02sp52",
          "author": "generate-addict",
          "text": "Nope. And even â€œcontext engineeringâ€. It changes from model to model, generation to generation. People just gaslight themselves.",
          "score": 2,
          "created_utc": "2026-01-17 09:04:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02stv2",
              "author": "Xthebuilder",
              "text": "Iâ€™m really glad I asked the community because I thought I was tripping seeing all the â€œprompt engineering â€œ buzzwords for testing new people about using AI bc I learned though trying to build and use them that fr fr I donâ€™t need none of that shit ðŸ˜‚ðŸ¤£",
              "score": 1,
              "created_utc": "2026-01-17 09:06:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o03ky5r",
          "author": "ZynthCode",
          "text": "promt-crafting is helpful to correct *undesired* behavior.   \n  \n\\*whips LLM on the wrist\\* No comments in code!",
          "score": 1,
          "created_utc": "2026-01-17 13:09:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o01t58f",
          "author": "YouAreTheCornhole",
          "text": "Yes you do, and most people in this thread seem to not know what prompt engineering actually means. Straight up garbage in garbage out",
          "score": 0,
          "created_utc": "2026-01-17 04:09:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02ra7k",
              "author": "Xthebuilder",
              "text": "Good way to add to the discussion brother ðŸ‘ðŸ½",
              "score": 0,
              "created_utc": "2026-01-17 08:51:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o02shgs",
                  "author": "YouAreTheCornhole",
                  "text": "You literally described your way of prompt engineering, acting like you don't need prompt engineering, because you prompt engineer in a certain way.",
                  "score": 1,
                  "created_utc": "2026-01-17 09:02:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qen6w1",
      "title": "The Preprocessing Gap Between RAG and Agentic",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qen6w1/the_preprocessing_gap_between_rag_and_agentic/",
      "author": "OnyxProyectoUno",
      "created_utc": "2026-01-16 17:59:44",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "RAG is the standard way to connect documents to LLMs. Most people building RAGs know the steps by now: parse documents, chunk them, embed, store vectors, retrieve at query time. But something different happens when you're building systems that act rather than answer.\n\n### The RAG mental model\n\nRAG preprocessing optimizes for retrieval. Someone asks a question, you find relevant chunks, you synthesize an answer. The whole pipeline is designed around that interaction pattern.\n\nThe work happens before anyone asks anything. Documents get parsed into text, extracting content from PDFs, Word docs, HTML, whatever format you're working with. Then chunking splits that text into pieces sized for context windows. You choose a strategy based on your content: split on paragraphs, headings, or fixed token counts. Overlap between chunks preserves context across boundaries. Finally, embedding converts each chunk into a vector where similar meanings cluster together. \"The contract expires in December\" ends up near \"Agreement termination date: 12/31/2024\" even though they share few words. That's what makes semantic search work.\n\nRetrieval is similarity search over those vectors. Query comes in, gets embedded, you find the nearest chunks in vector space. For Q&A, this works well. You ask a question, the system finds relevant passages, an LLM synthesizes an answer. The whole architecture assumes a query-response pattern.\n\nThe requirements shift when you're building systems that act instead of answer.\n\n### What agentic actually needs\n\nConsider a contract monitoring system. It tracks obligations across hundreds of agreements: Example Bank owes a quarterly audit report by the 15th, so the system sends a reminder on the 10th, flags it as overdue on the 16th, and escalates to legal on the 20th. The system doesn't just find text about deadlines. It acts on them.\n\nThat requires something different at the data layer. The system needs to understand that Party A owes Party B deliverable X by date Y under condition Z. And it needs to connect those facts across documents. Not just find text about obligations, but actually know what's owed to whom and when.\n\nThe preprocessing has to pull out that structure, not just preserve text for later search. You're not chunking paragraphs. You're turning \"Example Bank shall submit quarterly compliance reports within 15 days of quarter end\" into data you can query: party, obligation type, deadline, conditions. Think rows in a database, not passages in a search index.\n\n### Two parallel paths\n\nThe architecture ends up looking completely different.\n\nRAG has a linear pipeline. Documents go in, chunking happens, embeddings get created, vectors get stored. At query time, search, retrieve, generate.\n\nAgentic systems need two tracks running in parallel. The main one pulls structured data out of documents. An LLM reads each contract, extracts the obligations, parties, dates, and conditions, and writes them to a graph database. Why a graph? Because you're not just storing isolated facts, you're storing how they connect. Example Bank owes a report. That report is due quarterly. The obligation comes from Section 4.2 of Contract #1847. Those connections between entities are what graph databases are built for. This is what powers the actual monitoring.\n\nBut you still need embeddings. Just for different reasons.\n\nThe second track catches what extraction misses. Sometimes \"the Lender\" in paragraph 12 needs to connect to \"Example Bank\" from paragraph 3. Sometimes you don't know what patterns matter until you see them repeated across documents. The vector search helps you find connections that weren't obvious enough to extract upfront.\n\nSo you end up with two databases working together. The graph database stores entities and their relationships: who owes what to whom by when. The vector database helps you find things you didn't know to look for.\n\nI wrote the rest on my [blog](https://nickrichu.me/posts/the-preprocessing-gap-between-rag-and-agentic).",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qen6w1/the_preprocessing_gap_between_rag_and_agentic/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q9jdo8",
      "title": "Is it possible to see AI Request and Response in Realtime on Llama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q9jdo8/is_it_possible_to_see_ai_request_and_response_in/",
      "author": "Professional-Fun7765",
      "created_utc": "2026-01-10 23:36:56",
      "score": 6,
      "num_comments": 7,
      "upvote_ratio": 0.75,
      "text": "Hi everyone\n\nI am new in the world of Ollama so pardon me if this may sound like a stupid question. I am transitioning from GPT4All where when I make a request via API I can see in real time On the desktop app (on the server chat tab) The incoming request, the model thinking and the model generating a response. This was so great in debugging but GPT4All was slow for me so a colleague suggested Ollama and I can see much improvements in speed. I am currently integrating a Laravel App with Ollama and sending various request to the model and I wish I can be able to see the request and response in real time in Ollama just like I did in GPT4All Desktop App, so my question is whether or not this is possible? If it is then how can I go about configuring it?\n\nif it helps I am on Windows and this is for my local development.\n\nThank you in advance, your input will be highly appreciated.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q9jdo8/is_it_possible_to_see_ai_request_and_response_in/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nyw9ir1",
          "author": "Character_Pie_5368",
          "text": "Try ollama with OLLAMA_DEBUG=1 ollama serve\n or use something like wireshark to capture every packet",
          "score": 2,
          "created_utc": "2026-01-11 01:53:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyw9pee",
              "author": "Character_Pie_5368",
              "text": "Or a web proxy such as burp suite.",
              "score": 1,
              "created_utc": "2026-01-11 01:54:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyxk35x",
          "author": "AcanthaceaeNorth6189",
          "text": "Ollama is simply a serviced program that builds model services and solves standard interface problems at the application layer (OpenAI protocol). The part you mentioned is actually the inference process of a CHAT request, which OLLAMA returns directly to the client without processing. As far as I know, OLLAMA, a lightweight framework, does not have the session monitoring function you mentioned, you can check OLLAMA's plugins or open source projects for UI.\n\n\n\n\n\nIn theory, you can also do it yourself, just make a request agent and persist the results of the stream.",
          "score": 2,
          "created_utc": "2026-01-11 06:42:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyw8juk",
          "author": "immediate_a982",
          "text": "I should have said pentest a WEB APP or pentest REST API app",
          "score": 1,
          "created_utc": "2026-01-11 01:48:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzq0wq",
          "author": "stealthagents",
          "text": "You can definitely see what's happening in real time, just like with GPT4All. If you're using the command line, setting \\`OLLAMA\\_DEBUG=1\\` is a solid move for that debug info. Also, if you want to dive deeper, tools like Postman can help visualize requests and responses easier while developing.",
          "score": 1,
          "created_utc": "2026-01-16 21:00:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyvllb6",
          "author": "immediate_a982",
          "text": "Look into pentest Ollama",
          "score": 0,
          "created_utc": "2026-01-10 23:46:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyvrsj8",
              "author": "Professional-Fun7765",
              "text": "Thanks for your response, i tried searching \"pentest Ollama\" but nothing came up that speaks to this, can you please share a link if possible? Thank you",
              "score": 1,
              "created_utc": "2026-01-11 00:19:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qd7dmx",
      "title": "Hey all- I built a self-hosted MCP server to run AI semantic search over your own databases, files, and codebases. Supports Ollama and cloud providers if you want. Thought you all might find a good use for it.",
      "subreddit": "ollama",
      "url": "/r/selfhosted/comments/1qbv3fm/ragtime_a_selfhosted_mcp_server_to_run_ai/",
      "author": "mattv8",
      "created_utc": "2026-01-15 02:33:30",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qd7dmx/hey_all_i_built_a_selfhosted_mcp_server_to_run_ai/",
      "domain": "",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qepzot",
      "title": "Polymcp Integrates Ollama â€“ Local and Cloud Execution Made Simple",
      "subreddit": "ollama",
      "url": "https://github.com/poly-mcp/Polymcp",
      "author": "Just_Vugg_PolyMCP",
      "created_utc": "2026-01-16 19:41:16",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qepzot/polymcp_integrates_ollama_local_and_cloud/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qef38r",
      "title": "Prompt tool I built/use with Ollama daily - render prompt variations without worrying about text files",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qef38r/prompt_tool_i_builtuse_with_ollama_daily_render/",
      "author": "springwasser",
      "created_utc": "2026-01-16 12:49:45",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.8,
      "text": "I posted this to another subreddit, but should have posted it here.. sorry if you've seen it.\n\nThis is a tool I built because I use it in local development. I know there are solutions for these things mixed into other software, but this is standalone and does just one thing really well for me.\n\n\\- create/version/store prompts.. don't have to worry about text files unless I want to  \n\\- runs from command line, can pipe stdout into anything.. eg Ollama, ci, git hooks  \n\\- easily render variations of prompts on the fly, inject {{variables}} or inject files.. e.g. git diffs or documents  \n\\- can store prompts globally or in projects, run anywhere\n\nBasic usage:\n\n    # Create a prompt.. paste in text\n    $ promptg prompt new my-prompt \n    \n    # -or-\n    $ echo \"Create a prompt with pipe\" | promptg prompt save hello\n    \n    # Then.. \n    $ promptg get my-prompt | ollama run deepseek-r1\n\nOr more advanced, render with dynamic variables and insert files..\n\n    # before..\n    cat prompt.txt | sed \"s/{{lang}}/Python/g; s/{{code}}/$(cat myfile.py)/g\" | ollama run mistral\n    \n    # now, replace dynamic {{templateValue}} and insert code/file.\n    promptg get code-review --var lang=Python --var code@myfile.py | ollama run mistral\n\nInstall:\n\n    npm install -g @promptg/cli",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qef38r/prompt_tool_i_builtuse_with_ollama_daily_render/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nzwtsz5",
          "author": "springwasser",
          "text": "Should have posted the Gitub:\n\ngithub dot com/promptg/cli",
          "score": 1,
          "created_utc": "2026-01-16 12:50:33",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qeatiw",
      "title": "New version of Raspberry Pie Generative AI card (HAT+ 2)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qeatiw/new_version_of_raspberry_pie_generative_ai_card/",
      "author": "Unique_Winner_5927",
      "created_utc": "2026-01-16 08:43:42",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Perfect for private assistants, industrial equipment,Â proof of concept, ...\n\n[https://www.raspberrypi.com/news/introducing-the-raspberry-pi-ai-hat-plus-2-generative-ai-on-raspberry-pi-5/](https://www.raspberrypi.com/news/introducing-the-raspberry-pi-ai-hat-plus-2-generative-ai-on-raspberry-pi-5/)\n\n\\#RaspberryPi #DataSovereignty #EmbeddedAI",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qeatiw/new_version_of_raspberry_pie_generative_ai_card/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nzw85q6",
          "author": "-Akos-",
          "text": "Jeffrey Geerling was less impressed: [https://youtu.be/jRQaur0LdLE](https://youtu.be/jRQaur0LdLE)\n\nBesides, all models Iâ€™ve seen run on this board were 1.5B parameters or less, and older models at that.",
          "score": 1,
          "created_utc": "2026-01-16 10:01:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwoo6q",
              "author": "Comfortable_Ad_8117",
              "text": "Was that the original board or the +2 -The new +2 has 8gb of onboard ram and 2x speed",
              "score": 1,
              "created_utc": "2026-01-16 12:16:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzwu1yx",
                  "author": "-Akos-",
                  "text": "It's all in the video. In the end, you're better off buying a second Pi5 than this thing.",
                  "score": 2,
                  "created_utc": "2026-01-16 12:52:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzwp5t7",
                  "author": "danishkirel",
                  "text": "Still slower than the pi 5 cpu and there are models with more system memory too.",
                  "score": 1,
                  "created_utc": "2026-01-16 12:19:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzy284k",
          "author": "Unique_Winner_5927",
          "text": "Or we can directly install Ollama on Pi 5 : [https://monraspberry.com/installer-ollama-raspberry-pi/](https://monraspberry.com/installer-ollama-raspberry-pi/)",
          "score": 1,
          "created_utc": "2026-01-16 16:30:44",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcsn8m",
      "title": "We tried to automate product labeling in one prompt. It failed. 27 steps later, we've processed 10,000+ products.",
      "subreddit": "ollama",
      "url": "/r/LocalLLaMA/comments/1qcsmww/we_tried_to_automate_product_labeling_in_one/",
      "author": "No-Reindeer-9968",
      "created_utc": "2026-01-14 16:58:42",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qcsn8m/we_tried_to_automate_product_labeling_in_one/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "nzonye7",
          "author": "tom-mart",
          "text": "Sounds like a skill issue.",
          "score": 1,
          "created_utc": "2026-01-15 06:18:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qb46gv",
      "title": "How to Evaluate AI Agents? (Part 2)",
      "subreddit": "ollama",
      "url": "/r/AIEval/comments/1qb43wg/how_to_evaluate_ai_agents_part_2/",
      "author": "Ok_Constant_9886",
      "created_utc": "2026-01-12 19:14:13",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qb46gv/how_to_evaluate_ai_agents_part_2/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "nzqi3pk",
          "author": "stealthagents",
          "text": "It really helps to set clear metrics for performance before diving in, like how well they handle specific tasks or adapt to changes. Also, running some A/B tests can reveal a lot about their decision-making in real-world scenarios.",
          "score": 1,
          "created_utc": "2026-01-15 14:47:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q99hsz",
      "title": "Nvidia Quadro P400 2GB GDDR5 card good enough?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q99hsz/nvidia_quadro_p400_2gb_gddr5_card_good_enough/",
      "author": "Curious_Party_4683",
      "created_utc": "2026-01-10 17:09:37",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "qwen3-vl:8b refuses to work on my i7, 7th gen,  windows machine.  \n\nwill this cheap nvidia work? or what's the bare minimum card?\n\nhttps://preview.redd.it/lj263g5g0kcg1.png?width=1639&format=png&auto=webp&s=2ba459647259dba77ccefd933da88fdfc646e3fa\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q99hsz/nvidia_quadro_p400_2gb_gddr5_card_good_enough/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nytf6gq",
          "author": "HyperWinX",
          "text": "It has 2GB VRAM bruh. Its not gonna help you in any way. You need at least 8 gigs to run Qwen3-VL 8b Q4 quant maybe (you can get less quantized one, but it will use more vram)",
          "score": 4,
          "created_utc": "2026-01-10 17:20:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyuxfn2",
          "author": "shadowentityg",
          "text": "Try running a 1b model or less. I wouldn't try to run a vision model like that. Plus that architecture doesn't have any tensor cores. So it can't do inference and drivers are not supported. My opinion go get a RTX 3050 or 3060 off eBay for $100-$150",
          "score": 2,
          "created_utc": "2026-01-10 21:42:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyuiplm",
          "author": "SFsports87",
          "text": "You want enough vram to load the model.  A 4 bit 8b model will be about 5-6 gb, so get at least an 8gb gpu to run it.",
          "score": 1,
          "created_utc": "2026-01-10 20:29:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyx1z13",
          "author": "sinan_online",
          "text": "I got this to work only on a 16GB VRAM instance, on the cloud. I tried a 6GB card for Qwen3vl, didnâ€™t work.",
          "score": 1,
          "created_utc": "2026-01-11 04:33:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyx8d00",
          "author": "calivision",
          "text": "Inkscape might challenge that card ðŸ« ",
          "score": 1,
          "created_utc": "2026-01-11 05:13:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}