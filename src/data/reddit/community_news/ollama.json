{
  "metadata": {
    "last_updated": "2026-01-10 08:50:01",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 41,
    "total_comments": 157,
    "file_size_bytes": 202344
  },
  "items": [
    {
      "id": "1q4p8aa",
      "title": "Introducing MiroThinker 1.5 â€” the worldâ€™s leading search-based agent model!",
      "subreddit": "ollama",
      "url": "https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B",
      "author": "wuqiao",
      "created_utc": "2026-01-05 15:49:57",
      "score": 103,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q4p8aa/introducing_mirothinker_15_the_worlds_leading/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nxufh04",
          "author": "LightBrightLeftRight",
          "text": "Very cool. Is it expecting a specific tool to search with? I tried with my defaults on LMStudio (search wikipedia, search duckduckgo, retrieve webpage are all available). With the query \"Tell me what's expected at 2026\" it searches wikipedia for \"2026 CES announcements\" and then got the syntax wrong so it crashes. I just copy pasted the suggested system prompt in.",
          "score": 5,
          "created_utc": "2026-01-05 16:54:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxi6xj",
              "author": "Front_Eagle739",
              "text": "they have an open source mirothinker tool I think. If its what the website uses Ill be giving it a go because im impressed.",
              "score": 1,
              "created_utc": "2026-01-06 01:48:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxu2e69",
          "author": "wuqiao",
          "text": "https://preview.redd.it/v47lhkhyyjbg1.png?width=1184&format=png&auto=webp&s=8d7919f7dc31f43f51d0c75a9725ab6cb9732475\n\nPerformance Comparison on BrowseComp Benchmark",
          "score": 3,
          "created_utc": "2026-01-05 15:53:47",
          "is_submitter": true,
          "replies": [
            {
              "id": "nxu2ohx",
              "author": "wuqiao",
              "text": "https://preview.redd.it/ti8js6p1zjbg1.png?width=1304&format=png&auto=webp&s=75b2656a96e85ff4b1dd5d82da83afc5806a1227\n\nPerformance Comparison on Agentic Search Benchmarks",
              "score": 1,
              "created_utc": "2026-01-05 15:55:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxz7u3z",
          "author": "ozcapy",
          "text": "How can I use it through LMStudio?",
          "score": 2,
          "created_utc": "2026-01-06 09:06:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzm9m5",
          "author": "mr_randomspam",
          "text": "Are there any there any recommended opensource perplexity style apps that could take advantage of this model or any other for that matter? I use perplexity for so much these days, all super useful tech and business stuff that saves me a ton of time BUT I know deep down it's not the right thing to be doing.\n\n\nA local version would be great if it exists.",
          "score": 2,
          "created_utc": "2026-01-06 11:18:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvmzb9",
          "author": "Witty_Mycologist_995",
          "text": "Waiting for tiny version",
          "score": 1,
          "created_utc": "2026-01-05 20:12:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxx6a30",
          "author": "Plane_Ad9568",
          "text": "World leading, quite the claim !",
          "score": 1,
          "created_utc": "2026-01-06 00:44:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxzczr",
              "author": "wuqiao",
              "text": "![gif](giphy|8WkX7Zsk1fXIUipPYg)",
              "score": 3,
              "created_utc": "2026-01-06 03:23:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q2isyx",
      "title": "Run Claude Code with ollama without losing any single feature offered by Anthropic backend",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q2isyx/run_claude_code_with_ollama_without_losing_any/",
      "author": "Dangerous-Dingo-5169",
      "created_utc": "2026-01-03 02:47:39",
      "score": 81,
      "num_comments": 12,
      "upvote_ratio": 0.91,
      "text": "Hey folks! Sharing an open-source project that might be useful:\n\nLynkr connects AI coding tools (like Claude Code) to multiple LLM providers with intelligent routing.\n\n\nKey features:\n\n- Route between multiple providers: Databricks, Azure Ai Foundry, OpenRouter, Ollama,llama.cpp, OpenAi\n\n- Cost optimization through hierarchical routing, heavy prompt caching\n\n- Production-ready: circuit breakers, load shedding, monitoring\n\n- It supports all the features offered by claude code like sub agents, skills , mcp , plugins etc unlike other proxies which only supports basic tool callings and chat completions.\n\nGreat for:\n\n- Reducing API costs as it supports hierarchical routing where you can route requstes to smaller local models and later switch to cloud LLMs automatically.\n\n- Using enterprise infrastructure (Azure)\n\n-Â  Local LLM experimentation\n\n```bash\n\nnpm install -g lynkr\n\n```\n\nGitHub: https://github.com/Fast-Editor/Lynkr (Apache 2.0)\n\nWould love to get your feedback on this one. Please drop a star on the repo if you found it helpful",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q2isyx/run_claude_code_with_ollama_without_losing_any/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nxgw44t",
          "author": "Zyj",
          "text": "If all you want is to run Claude Code with llama.cpp, you don't need this:\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1p9bk2b/claude\\_code\\_can\\_now\\_connect\\_directly\\_to\\_llamacpp/](https://www.reddit.com/r/LocalLLaMA/comments/1p9bk2b/claude_code_can_now_connect_directly_to_llamacpp/)\n\nJust point ANTHROPIC\\_BASE\\_URL to your llama.cpp server.",
          "score": 4,
          "created_utc": "2026-01-03 17:14:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhhnlm",
              "author": "Dangerous-Dingo-5169",
              "text": "Yeah I understand that part. With that one you do not get features like sub agents. I am not sure if mcp also works fine all of which are supported by this proxy which we built. But a good feedback to improve our post and readme \nThanks",
              "score": 3,
              "created_utc": "2026-01-03 18:52:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxp7bz8",
                  "author": "iongion",
                  "text": "You guys did great!",
                  "score": 2,
                  "created_utc": "2026-01-04 21:33:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxdusgq",
          "author": "pstuart",
          "text": "Looks intriguing, will definitely check it out.",
          "score": 2,
          "created_utc": "2026-01-03 04:43:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxf7szv",
          "author": "FigZestyclose7787",
          "text": "This is actually a great idea! Thanks for sharing.",
          "score": 2,
          "created_utc": "2026-01-03 11:26:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdhmv6",
          "author": "wortelbrood",
          "text": "no link....",
          "score": 1,
          "created_utc": "2026-01-03 03:19:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdhqgt",
              "author": "Dangerous-Dingo-5169",
              "text": "GitHub:Â [https://github.com/Fast-Editor/Lynkr](https://github.com/Fast-Editor/Lynkr)Â (Apache 2.0)\n\nWould love to get your feedback on this one. Please drop a star on the repo if you found it helpful",
              "score": 2,
              "created_utc": "2026-01-03 03:20:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxdhtmw",
                  "author": "Dangerous-Dingo-5169",
                  "text": "I would want to include it in the post body but the moment I do it. The reddit filters remove the entire post which is why I put it in the first comment",
                  "score": 1,
                  "created_utc": "2026-01-03 03:21:04",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxxx7ml",
          "author": "SpiritualReply1889",
          "text": "Does it work with copilot, antigravity, cursor etc using their oauth, similar to how opencode supports them via plugins? That would be dope.",
          "score": 1,
          "created_utc": "2026-01-06 03:10:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny26th4",
              "author": "Dangerous-Dingo-5169",
              "text": "Hey not yet its a good use case \nCan you raise an issue with label feature enhancement on the repo please",
              "score": 1,
              "created_utc": "2026-01-06 19:20:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pym7c0",
      "title": "Running Ministral 3 3B Locally with Ollama and Adding Tool Calling (Local + Remote MCP)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1pym7c0/running_ministral_3_3b_locally_with_ollama_and/",
      "author": "shricodev",
      "created_utc": "2025-12-29 13:24:35",
      "score": 61,
      "num_comments": 14,
      "upvote_ratio": 0.93,
      "text": "Iâ€™ve been seeing a lot of chatter around Ministral 3 3B, so I wanted to test it in a way that actually matters day to day. Can such a small local model do reliable tool calling, and can you extend it beyond local tools to work with remotely hosted MCP servers?\n\nHereâ€™s what I tried:\n\n# Setup\n\n* Ran a quantized 4-bit (Q4\\_K\\_M) Ministral 3 3B on Ollama\n* Connected it to Open WebUI (with Docker)\n* Tested tool calling in two stages:\n   * Local Python tools inside Open WebUI\n   * **Remote MCP tools** via Composio (so the model can call externally hosted tools through MCP)\n\nThe model, despite the super tiny size of just 3B parameters, is said to support tool calling with even support for structured output. So, this was really fun to see the model in action.\n\nMost of the guides show you how to work with just the local tools, which is not ideal when you plan to use the model for bigger, better and managed tools for hundreds of different services. \n\nIn this guide, I've covered the model specs and the entire setup, including setting up a Docker container for Ollama and running Ollama WebUI.\n\nAnd the nice part is that the model setup guide here works for all the other models that support tool calling.\n\nI wrote up the full walkthrough with commands and screenshots:\n\nYou can find it here: [MCP tool calling guide with Ministral 3B, Composio, and Ollama](https://composio.dev/blog/tool-calling-with-ministral-3b)\n\nIf anyone else has tested tool calling on Ministral 3 3B (or worked with it using vLLM instead of Ollama), Iâ€™d love to hear what worked best for you, as I couldn't get vLLM to work due to CUDA errors. :(",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pym7c0/running_ministral_3_3b_locally_with_ollama_and/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nwjkctl",
          "author": "Medical_Reporter_462",
          "text": "Copy Paste that guide here too. Why link it?",
          "score": 2,
          "created_utc": "2025-12-29 13:42:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp83u2",
              "author": "Potential-Leg-639",
              "text": "Itâ€˜s a composio Ad, thatâ€˜s why",
              "score": 4,
              "created_utc": "2025-12-30 08:43:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwjkqrb",
              "author": "shricodev",
              "text": "It'd be a bit too long",
              "score": -5,
              "created_utc": "2025-12-29 13:44:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpoll7",
          "author": "TheAndyGeorge",
          "text": "slopvertisment",
          "score": 1,
          "created_utc": "2025-12-30 11:15:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvyvpf",
          "author": "mr_Owner",
          "text": "This model i tried at q4km and is a hit and miss with openwebui and searxng in my experience",
          "score": 1,
          "created_utc": "2025-12-31 09:17:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxtc17",
              "author": "shricodev",
              "text": "Yeah, it isn't very reliable. I say it's decent considering the size.",
              "score": 1,
              "created_utc": "2025-12-31 16:45:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxqm5zp",
          "author": "Individual_Chest_204",
          "text": "Thanks for the guide. Did you test it on other tools? I tested it with composio gmail mcp tool (with locally hosted ollama Mistral 3B), it does not seem to work. When I tested it from the composio's playground, it works like charm, but when I tested via OpenWebUI, it keep asking me to logon to web gmail to check my mails. J\n\nI also tested it via n8n and cli tools (with gpt20Boss model) but it returns the same result (the gpt20B is hosted on kaggle ollama).  \nHowever, if in my n8n workflow, if I replace the ollama node with openrouter (also connected to gpt20Boss), it also works like charm just like in composio's playground.\n\nAnyone has have success running local model with consistent gmail or google workspace tools calling?",
          "score": 1,
          "created_utc": "2026-01-05 01:45:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxrc1yr",
              "author": "shricodev",
              "text": "Were you actually stuck on the login part? If itâ€™s telling you to log in, the tool calling is kinda working, itâ€™s just the auth/session part not sticking in OpenWebUI/n8n.\n\nAlso maybe try a different model like Qwen (qwen3). Mistral 3B is kinda weird for multi-step tool calling in my experience. Iâ€™ve run OpenWebUI with other models and never got any errors as such.",
              "score": 1,
              "created_utc": "2026-01-05 04:07:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxrjjys",
                  "author": "Individual_Chest_204",
                  "text": "Thanks for the note u/shricodev. I wasn't stuck in login, for composio mcp, it;'s preconfigured with my gmail account and then it provides the endpoint url and the api key which is then configured in the mcp node (n8n) or in the settings of openwebui therefore no login is required.   \nThe reason I can ascertain is when using openrouter (in n8n or composio playground), it smoothly access the mcp and returned the mails without requiring me to login.   \nI rented a GPU server, I have tried pretty every single model listed in ollama/tools model list pages including the qwen32B but same outcome. I also tried this llama3-groq-tool-use:8b but same outcome, it struggle to engage the mcp tool and just return this {\"name\":\"GMAIL\\_LIST\\_MESSAGES\",\"parameters\":{\"query\":\"is:unread\"}} or this We need to respond to user. The user hasn't asked anything yet. Let's see what user says.No user message. Probably they want us to list available tools or do nothing. But the instruction says when user asks anything related to emails, etc. We have exactly one tool: googleworkspace\\_mcp. So we can call functions. But no user query yet. We should wait for user.Sure! Just let me know what youâ€™d like to do with Gmail, Sheets, Drive, or any other Workspace feature, and Iâ€™ll fetch the real data for you.\"\"\n\nthis is the mcp tool that I added on composio, nevertheless, thanks for your thread, only found out about composio mcp from your thread, it's really good. \n\nhttps://preview.redd.it/quqjlfjhogbg1.png?width=1011&format=png&auto=webp&s=5131ff88d71b41631c4b05a1b96274ca6a5673ac",
                  "score": 1,
                  "created_utc": "2026-01-05 04:51:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwjjtrj",
          "author": "Worried_Equivalent95",
          "text": "Thanks",
          "score": 0,
          "created_utc": "2025-12-29 13:38:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjku0o",
              "author": "shricodev",
              "text": "You're welcome",
              "score": 1,
              "created_utc": "2025-12-29 13:44:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkkebx",
          "author": "Moon_stares_at_earth",
          "text": "Does this work on MacOS?",
          "score": 0,
          "created_utc": "2025-12-29 16:47:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwkt8av",
              "author": "shricodev",
              "text": "It should run fine on a Mac with Ollama. Ministral 3B is small enough that performance is usually decent on most modern machines. I havenâ€™t tested it on macOS personally though, so take this as a best guess.",
              "score": 1,
              "created_utc": "2025-12-29 17:29:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q4jxcj",
      "title": "Use ollama to run lightweight, open-source, local agents as UNIX tools.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/gallery/1q4jxcj",
      "author": "Available_Pressure47",
      "created_utc": "2026-01-05 12:02:51",
      "score": 58,
      "num_comments": 20,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q4jxcj/use_ollama_to_run_lightweight_opensource_local/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nxtixkl",
          "author": "newz2000",
          "text": "Clever idea!",
          "score": 3,
          "created_utc": "2026-01-05 14:14:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxtleo8",
              "author": "Available_Pressure47",
              "text": "Thank you! It truly makes me so happy when people\nuse something open source Iâ€™ve built.",
              "score": 1,
              "created_utc": "2026-01-05 14:28:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxtpzsi",
          "author": "suicidaleggroll",
          "text": "How is this different from [shellGPT](https://github.com/TheR1D/shell_gpt)?",
          "score": 2,
          "created_utc": "2026-01-05 14:53:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxtsoqt",
              "author": "Available_Pressure47",
              "text": "By default, shellGPT uses open AI models and requires an API key. While there is a way to configure these existing tools to run local models, it requires setup + configuration and these tools arenâ€™t inherently built around it. Orla is designed to be built entirely around the local open source llm ecosystem. In addition, it also comes with a built in MCP server and an MCP tool registry where you can download tools from similar to Linux package managers like apt. For example, orla tool install fs gets you an open source file system tool. Iâ€™m currently working on a sandbox for these tools to run in and the hope is for the community to be able to write these tools and just plug them in either locally (as simple as putting them in Orlaâ€™s tool dir folder in the same directory as orla.yaml) or adding them to registry. A tool can be as simple as a bash script and orla with auto discover it and it usable to llms. The long term vision is essentially for orla to be as local, easy to deploy, and script with as a tool like grep. Thank you for your question!",
              "score": 5,
              "created_utc": "2026-01-05 15:06:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxwlfue",
          "author": "Ill_Evidence_5833",
          "text": "Looks very interesting! Is it possible to use with dockerized ollama?",
          "score": 1,
          "created_utc": "2026-01-05 22:56:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwlsoz",
              "author": "Available_Pressure47",
              "text": "Yes absolutely! You can put this entire thing in a docker container. If you want Orla to run bare metal and ollama to run separately in a docker container, I can implement a configuration knob for this. Also very happy to review pull requests. Thank you so much for using Orla.",
              "score": 2,
              "created_utc": "2026-01-05 22:57:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxwmte7",
                  "author": "Ill_Evidence_5833",
                  "text": "Having orla run in bare metal, so it can use ollama bt simply having  a variable like 'export OLLAMA_ADDRESS=http://192.168.88.1.1:11434\" would be great.",
                  "score": 2,
                  "created_utc": "2026-01-05 23:03:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxx24k0",
          "author": "numbworks",
          "text": "Which ollama model does it use in the backend?",
          "score": 1,
          "created_utc": "2026-01-06 00:23:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxacma",
              "author": "Available_Pressure47",
              "text": "Thank you for your question! It is configurable but by default it runs qwen3:0.6b. I was thinking maybe developing a small recommendation algorithm to pick something that is a good performance and utility tradeoff for whether Orla is installed. What do you think? Thank you!",
              "score": 1,
              "created_utc": "2026-01-06 01:06:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny0w2sg",
                  "author": "numbworks",
                  "text": "Thank you for replying! Since I know the model, which tends to be very wordy, I would add some logic to limit the lenght of output. Maybe with a specific prompt! You don't want the shell to be floaded by the LLM! Apart from that, very good idea! :)",
                  "score": 1,
                  "created_utc": "2026-01-06 15:49:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxzbqfp",
          "author": "poedy78",
          "text": ">The current ecosystem around agents feels like a collection of bloated SaaS with expensive subscriptions and privacy concerns.\n\nAbsolutely right.  \nGonna give a go, seems to be easy to include in workflows.",
          "score": 1,
          "created_utc": "2026-01-06 09:44:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzk40h",
              "author": "Available_Pressure47",
              "text": "Thank you! I really appreciate your support!",
              "score": 1,
              "created_utc": "2026-01-06 10:59:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyfq6hi",
          "author": "A-n-d-y-R-e-d",
          "text": "Kind of like this one ?\nhttps://github.com/sigoden/aichat",
          "score": 1,
          "created_utc": "2026-01-08 17:44:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygosdb",
          "author": "petyussz",
          "text": "https://preview.redd.it/ck7autp7o6cg1.jpeg?width=1096&format=pjpg&auto=webp&s=914b8d81ba0ec83fab4e998074f8f74c506099b4\n\nTrying to build something similar with python and langchain. A simple command line interface for various linux tasks with tool calling.",
          "score": 1,
          "created_utc": "2026-01-08 20:15:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyo1zvs",
          "author": "epicjardmoment",
          "text": "This is actually so goated, love the concept and I'll definitely be testing it out",
          "score": 1,
          "created_utc": "2026-01-09 21:03:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyo3qev",
              "author": "Available_Pressure47",
              "text": "Thank you so much! I really appreciate your support.",
              "score": 1,
              "created_utc": "2026-01-09 21:11:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q6bw3a",
      "title": "New llama.cpp 30x faster....",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q6bw3a/new_llamacpp_30x_faster/",
      "author": "u1pns",
      "created_utc": "2026-01-07 10:26:54",
      "score": 48,
      "num_comments": 30,
      "upvote_ratio": 0.73,
      "text": "Excited about NVIDIA collaboration on this. Incredible improving!  \nSince Ollama is (or was) based on llama.cpp....Will ollama take benefit of this improving?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q6bw3a/new_llamacpp_30x_faster/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "ny7oty8",
          "author": "AdPristine1358",
          "text": "30% not 30x\n\n\"For SLMs, token generation throughput performance on mixture-of-expert (MoE) models has increased by 35% on llama.cpp on NVIDIA GPUs, and 30% on Ollama on RTX PCs.\"\n\nSource:\nhttps://developer.nvidia.com/blog/open-source-ai-tool-upgrades-speed-up-llm-and-diffusion-models-on-nvidia-rtx-pcs/",
          "score": 56,
          "created_utc": "2026-01-07 15:24:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny871ld",
              "author": "florinandrei",
              "text": "Yeah. I mean people complain about \"AI slop\", but it's a massive improvement over the regular human slop.",
              "score": 17,
              "created_utc": "2026-01-07 16:48:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nydnenw",
              "author": "Particular-Way7271",
              "text": "I don't think op gets it. I think he needs to be explained what percentages are and that they don't translate into x something.",
              "score": 1,
              "created_utc": "2026-01-08 11:12:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyncenh",
                  "author": "use_net",
                  "text": "Mathematically speaking, the percent sign is just a multiplier. 30% literally means 30\\*1/100â€‹. ;)",
                  "score": 1,
                  "created_utc": "2026-01-09 19:05:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyldfci",
              "author": "Affectionate-Hat-536",
              "text": "Any improvement for Macs?",
              "score": 1,
              "created_utc": "2026-01-09 13:36:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny6mm3d",
          "author": "Key-Guitar4732",
          "text": "The contributions are being made to ggml and Ollama uses ggml for their engine. So yes it should be faster.",
          "score": 9,
          "created_utc": "2026-01-07 11:43:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7qx5f",
          "author": "redditor100101011101",
          "text": "Definitely not 30x faster lol",
          "score": 8,
          "created_utc": "2026-01-07 15:34:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8ci9x",
          "author": "electrified_ice",
          "text": "Does anyone know when is Ollama getting the update? The blogs and GitHub pages seem to have not been updated recently",
          "score": 6,
          "created_utc": "2026-01-07 17:12:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6kfg0",
          "author": "Baddog1965",
          "text": "I guess that's going to have no effect on the one for CPU",
          "score": 8,
          "created_utc": "2026-01-07 11:26:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6kgki",
              "author": "haikusbot",
              "text": "*I guess that's going*\n\n*To have no effect on the*\n\n*One for CPU*\n\n\\- Baddog1965\n\n---\n\n^(I detect haikus. And sometimes, successfully.) ^[Learn&#32;more&#32;about&#32;me.](https://www.reddit.com/r/haikusbot/)\n\n^(Opt out of replies: \"haikusbot opt out\" | Delete my comment: \"haikusbot delete\")",
              "score": 7,
              "created_utc": "2026-01-07 11:26:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny7go3a",
                  "author": "Baddog1965",
                  "text": "Haha! Completely unintentional on my part.",
                  "score": 4,
                  "created_utc": "2026-01-07 14:44:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny9lj0p",
                  "author": "Militop",
                  "text": "There's no poetic value in that. This is not a haiku.",
                  "score": 1,
                  "created_utc": "2026-01-07 20:29:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyd3bdb",
          "author": "cibernox",
          "text": "To clarify, this is not that ollama is going to get faster. Itâ€™s that it already got that much faster over the course of some months. \n\nI did notice. One year ago i was running 4B models at 80ish tokens/s, now Iâ€™m running on the same hardware a 8B model at 70tk/s with only some mild memory over clock, and the 4B models are at 105-110tk/s. In 12ish months or so.",
          "score": 3,
          "created_utc": "2026-01-08 08:12:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7ax1f",
          "author": "-MXXM-",
          "text": "Source...?",
          "score": 2,
          "created_utc": "2026-01-07 14:14:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny7danj",
              "author": "eleqtriq",
              "text": "https://developer.nvidia.com/blog/open-source-ai-tool-upgrades-speed-up-llm-and-diffusion-models-on-nvidia-rtx-pcs/",
              "score": 2,
              "created_utc": "2026-01-07 14:27:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny74ml4",
          "author": "79215185-1feb-44c6",
          "text": "I'm more pissed that I have to pick up 2 RTX 6000s at some point because the changes are Nvidia-only.",
          "score": 3,
          "created_utc": "2026-01-07 13:40:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny7z67y",
              "author": "q-admin007",
              "text": "You could pick up two RTX 5060 16GB and use a smaller model and pay less than 1000â‚¬. In my experience most of the tasks one comes across can be done with very small models.\n\nI for one use CPU only for very large models on a server with 12 memory channels. Most large models are MoE and so they are zippy for CPU only.",
              "score": 3,
              "created_utc": "2026-01-07 16:12:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny8e7x8",
                  "author": "79215185-1feb-44c6",
                  "text": "Anything less than like 100t/s is so mind numbingly slow I don't even bother.",
                  "score": 1,
                  "created_utc": "2026-01-07 17:20:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny8spcq",
          "author": "Difficult_Hand_509",
          "text": "Will this increase speed for Mac using MLX?",
          "score": 3,
          "created_utc": "2026-01-07 18:24:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyc5l75",
              "author": "CynicalTelescope",
              "text": "The speedups were developed by Nvidia and are for their GPUs, so probably not",
              "score": 2,
              "created_utc": "2026-01-08 04:03:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nybji6z",
          "author": "eriklo88",
          "text": "No day dreaming 30x ğŸ˜‚",
          "score": 1,
          "created_utc": "2026-01-08 02:03:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycv32m",
          "author": "Powerful_Evening5495",
          "text": "30x faster lol, nope, that is not happening. \n\nGoogle before you post, OP.",
          "score": 1,
          "created_utc": "2026-01-08 07:01:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nye7i64",
          "author": "Ok_Sense_3580",
          "text": "i dont get with the â€œFasterâ€ trend. You either picked shitty output or refine one.",
          "score": 1,
          "created_utc": "2026-01-08 13:28:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfiuw0",
          "author": "u1pns",
          "text": "I mean 30%, of course. But is a relevant improvement....",
          "score": 1,
          "created_utc": "2026-01-08 17:12:35",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nyiulj0",
          "author": "lordpuddingcup",
          "text": "What is the actual change like from a technical kernel perspective what changed?",
          "score": 1,
          "created_utc": "2026-01-09 02:34:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nykb2ok",
              "author": "Overall-Somewhere760",
              "text": "There s a post on git with all changes made. There s like 4 main commits that happened",
              "score": 1,
              "created_utc": "2026-01-09 08:39:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nynsn2j",
          "author": "Only_Name3413",
          "text": "Tangentially related, I just changed from Ollama to Lorax. I wanted to explore dynamic LoRA loading and saw a 50% speed improvement using the same base model (LAMA 3.1 8B). Same prompt / response went from 8000MS to 4100MS",
          "score": 1,
          "created_utc": "2026-01-09 20:19:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q84177",
      "title": "Create specialized Ollama models in 30 seconds",
      "subreddit": "ollama",
      "url": "https://v.redd.it/rqpjr0gbnacg1",
      "author": "ComfyTightwad",
      "created_utc": "2026-01-09 09:39:13",
      "score": 41,
      "num_comments": 10,
      "upvote_ratio": 0.77,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q84177/create_specialized_ollama_models_in_30_seconds/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nykkiwm",
          "author": "Noiselexer",
          "text": "Putting a custom system prompt on existing models is not creating a new model in my book.",
          "score": 35,
          "created_utc": "2026-01-09 10:05:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nykorgh",
              "author": "Smooth-Cow9084",
              "text": "Yeah lollll",
              "score": 5,
              "created_utc": "2026-01-09 10:43:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nykqbdk",
              "author": "dareima",
              "text": "In Ollama it is. What OP likely does with OllaMan is creating a model file including all the custom settings, which is a new model in Ollama terms.",
              "score": 3,
              "created_utc": "2026-01-09 10:56:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyl6rte",
                  "author": "baxter_the_martian",
                  "text": "Agreeing with you but to be \"one of those guys\" the word isn't technically \"new\", it's \"fork\"",
                  "score": 5,
                  "created_utc": "2026-01-09 12:57:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nypqm50",
                  "author": "StardockEngineer",
                  "text": "It needs to be renamed then.",
                  "score": 2,
                  "created_utc": "2026-01-10 02:14:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nynqm1s",
          "author": "HotMud9713",
          "text": "I thought I would see way to fine-running a model without the Colab Hell",
          "score": 1,
          "created_utc": "2026-01-09 20:10:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nypbmwe",
          "author": "venpuravi",
          "text": "Interesting. \n\nIs it possible to upload an already downloaded gguf to ollama locally? I really need this functionality for vision models.",
          "score": 1,
          "created_utc": "2026-01-10 00:51:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nypbvqa",
          "author": "venpuravi",
          "text": "A simple renaming would be a great help especially for the hf models.",
          "score": 1,
          "created_utc": "2026-01-10 00:53:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyq6vno",
          "author": "Latlook",
          "text": "how this works",
          "score": 1,
          "created_utc": "2026-01-10 03:48:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylmiyc",
          "author": "UseMoreBandwith",
          "text": "That will quickly make the website useless.  \nIt has already many low quality models (without descriptions, date, etc.) , but this will end all.",
          "score": 0,
          "created_utc": "2026-01-09 14:24:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0rzbw",
      "title": "Local AI Memory System - Beta Testers Wanted (Ollama + DeepSeek + Knowledge Graphs)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q0rzbw/local_ai_memory_system_beta_testers_wanted_ollama/",
      "author": "danny_094",
      "created_utc": "2026-01-01 00:59:23",
      "score": 26,
      "num_comments": 19,
      "upvote_ratio": 0.88,
      "text": "**\\*\\*The Problem:\\*\\***\n\nÂ Your AI forgets everything between conversations. You end up re-explaining context every single time.\n\n**\\*\\*The Solution:\\*\\***Â \n\nI built \"Jarvis\"  - a local AI assistant with actual long-term memory that works across conversations. And my latest pipeline update is the graph.\n\n**\\*\\*Example:\\*\\***Â \\`\\`\\`Â Day 1: \"My favorite pizza is Tunfisch\"Â Day 7: \"What's my favorite pizza?\"Â AI: \"Your favorite pizza is Tunfisch-Pizza!\" âœ…Â \\`\\`\\`Â \n\n**\\*\\*How it works:\\*\\***\n\nÂ \\-Â Semantic search finds relevant memories (not just keywords)\n\nÂ \\-Â Knowledge graph connects related factsÂ -Â Auto-maintenance (deduplicates, merges similar entries)Â \n\n\\-Â 100% local (your data stays on YOUR machine)\n\nÂ **\\*\\*Tech Stack:\\*\\***Â \n\n\\-Â Ollama (DeepSeek-R1 for reasoning, Qwen for control)Â \n\n\\-Â SQLite + vector embeddingsÂ \n\n\\-Â Knowledge graphs with semantic/temporal edgesÂ \n\n\\-Â MCP (Model Context Protocol) architecture\n\nÂ \\-Â Docker compose setupÂ \n\n**\\*\\*Current Status:\\*\\***Â \n\n\\-Â 96.5% test coverage (57 passing tests)Â \n\n\\-Â Graph-based memory optimizationÂ \n\n\\-Cross-conversation retrieval working\n\nÂ \\-Â Automatic duplicate detection\n\nÂ \\-Â Production-ready (running on my Ubuntu server)\n\n**\\*\\*Looking for Beta Testers:\\*\\***\n\nÂ \\-Â Linux users comfortable with DockerÂ \n\n\\-Â Willing to use it for \\~1 week\n\nÂ \\-Â Report bugs and memory accuracy\n\nÂ \\-Â Share feedback on usefulnessÂ \n\n**\\*\\*What you get:\\*\\***Â \n\n\\-Â Your own local AI with persistent memory\n\nÂ \\-Â Full data privacy (everything stays local)Â \n\n\\-Â One-command Docker setupÂ \n\n\\-Â GitHub repo + documentationÂ \n\n**\\*\\*Why this matters:\\*\\***Â \n\nLocal AI is great for privacy, but current solutions forget context constantly. This bridges that gap - you get privacy AND memory.Â Interested? Comment below and I'll share:Â -Â GitHub repoÂ -Â Setup instructionsÂ -Â Bug report templateÂ Looking forward to getting this in real users' hands! ğŸš€Â \n\n\\---Â \n\n**\\*\\*Edit:\\*\\***Â Just fixed a critical cross-conversation retrieval bug today - great timing for beta testing! ğŸ˜„Â \\`\\`\\`\n\n[https://github.com/danny094/Jarvis](https://github.com/danny094/Jarvis)\n\nhttps://reddit.com/link/1q0rzbw/video/fb7n6q0dzmag1/player\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q0rzbw/local_ai_memory_system_beta_testers_wanted_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nx5tveo",
          "author": "maturax",
          "text": "Great work, bro, we need more projects like this that are supported.",
          "score": 3,
          "created_utc": "2026-01-01 23:28:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5z0dq",
              "author": "danny_094",
              "text": "Thank you! That's very motivating.",
              "score": 1,
              "created_utc": "2026-01-01 23:56:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx0dxa7",
          "author": "Dense_Gate_5193",
          "text": "one of the biggest problems youâ€™re going to face is security.\n\n\ni wrote https://github.com/orneryd/NornicDB which has an idiomatic MCP server for AI agents as well as multiple endpoints which mimics neo4j (cypher+ bolt) and beats it in terms of performance. it also has a qdrant-compatible grpc endpoint. it handles embeddings for you and does RRF search with HNSW/ANN and GPU accelerated brute-force and k-means in the GPU for search",
          "score": 2,
          "created_utc": "2026-01-01 01:12:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0fs5c",
              "author": "danny_094",
              "text": "Re: security - agreed, on the TODO list.\n\nRe: NornicDB - cool project! Could replace my vector store (\\~3% of codebase). \n\nThe bulk is multi-model orchestration (DeepSeek â†’ Qwen pipeline), MCP hub routing, graph maintenance workers, and persona system. Memory search is just one tool the control layer calls.\n\n15k LOC, 12 containers, 3 AI models. Storage layer is actually the simple part. ğŸ˜…",
              "score": 2,
              "created_utc": "2026-01-01 01:24:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx0guxq",
                  "author": "danny_094",
                  "text": "Appreciate the NornicDB rec - solid graph DB work!\n\n\n\nQuick clarification on scope:\n\n\n\n\\*\\*Full Stack:\\*\\*\n\n\\`\\`\\`\n\nChat UIs (LobeChat/OpenWebUI)\n\n  â†“ Adapter Layer\n\n  â†“ Classifier  \n\n  â†“ ThinkingLayer (DeepSeek-R1)\n\n  â†“ ControlLayer (Qwen)\n\n  â†“ MCP Hub (6 servers):\n\nâ”œâ”€ Memory (Vector + Graph) â† NornicDB could fit here\n\nâ”œâ”€ Sequential Reasoning\n\nâ”œâ”€ Validator Service\n\nâ”œâ”€ Filesystem Tools\n\nâ””â”€ Search Tools\n\n  â†“ OutputLayer (Persona)\n\n  â†“ Response\n\n\\`\\`\\`\n\n\n\n\\*\\*Stats:\\*\\*\n\n\\- 9 services orchestrated\n\n\\- 3 AI models coordinated  \n\n\\- 15k+ LOC, 96.5% test coverage\n\n\\- Multi-transport (HTTP/SSE/STDIO)\n\n\n\n\\*\\*NornicDB vs Jarvis:\\*\\*\n\n\\- You: Graph database (storage layer)\n\n\\- Me: AI agent framework (orchestration layer)\n\n\n\nYour project could replace my vector store (\\~5% of system).\n\nThe complexity is in coordinating multiple AI models + tool servers.\n\n\n\nDifferent problems - both valid approaches! ğŸ‘\n\n\n\nRe: security - yep, on the list before wider beta.",
                  "score": 1,
                  "created_utc": "2026-01-01 01:31:25",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxcerxq",
          "author": "Specialist-Feeling-9",
          "text": "iâ€™m down to try it",
          "score": 2,
          "created_utc": "2026-01-02 23:37:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxcnrb3",
              "author": "danny_094",
              "text": "Great, please tell me how it works, and if there are any bugs or problems. I'm also constantly developing new features.",
              "score": 1,
              "created_utc": "2026-01-03 00:27:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3ffm8",
          "author": "danny_094",
          "text": "I fixed another Docker Compose bug today. Yesterday, I was a bit euphoric because it was working. Now you can build the Compose directly after cloning. No errors! :D Good point about the formatting. To be honest: I'm a solo developer and I use AI for documentation and answers, and for code problems. I developed everything myself (15,000 lines of code without a team), but yes, Claude helps me write clearer explanations, or when I'm stuck. It helps me when I'm more stuck than I could manage at 2 a.m. ğŸ˜…",
          "score": 1,
          "created_utc": "2026-01-01 16:00:45",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nx3rv6e",
          "author": "zh4k",
          "text": "Could the output layer persona be a separate AI that is fine-tuned on the writings of someone to mimic their writing persona?  I was thinking of something like this in terms of flow for a writing AI using multiple AIs as different mixtures of experts",
          "score": 1,
          "created_utc": "2026-01-01 17:07:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3voos",
              "author": "danny_094",
              "text": "YES, absolutely possible! But it requires modifications to the validator and control layer. Stability depends on how you separate content from style.\n\nA new component is needed: Style Validator.\n\nIt would then be: Controller - Collects data. Content Layer (factual response) and: new layer: Style Layer (fine-tuned to author X) Output model: Response in the style of author X\n\n\\-You would need to extend the validator.\n\n\\-Extend Controller Decision\n\n\\-New component: Style Validator\n\nAs you can see, it's possible. However, you would first need to fine-tune a model and make some minor modifications to the code. I'll definitely work on a solution that can implement styles via the web interface. I just don't know when. If you have more specific ideas, open a discussion on GitHub and I'll take a look.",
              "score": 1,
              "created_utc": "2026-01-01 17:26:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6b5rm",
          "author": "cipga",
          "text": "how do i install this on windows 11?",
          "score": 1,
          "created_utc": "2026-01-02 01:05:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6ck9t",
              "author": "danny_094",
              "text": "So far, I've only been able to install it on Ubuntu Server, which is where I develop it. But it should be essentially the same on Windows 11. You'll need Docker and Ollama (my Ollama installation is in the wiki).",
              "score": 2,
              "created_utc": "2026-01-02 01:14:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxj1vdl",
          "author": "yugami",
          "text": "can this be integrated with something like msty studio?  I've been using that as my front end",
          "score": 1,
          "created_utc": "2026-01-03 23:28:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxj28ha",
              "author": "danny_094",
              "text": "Yes. You can create adapters.",
              "score": 1,
              "created_utc": "2026-01-03 23:30:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q6ntug",
      "title": "which small model can i use to read this gauge?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/gallery/1q6ntug",
      "author": "Curious_Party_4683",
      "created_utc": "2026-01-07 18:45:38",
      "score": 22,
      "num_comments": 37,
      "upvote_ratio": 0.83,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q6ntug/which_small_model_can_i_use_to_read_this_gauge/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "ny8zbxw",
          "author": "Unlucky-Message8866",
          "text": "it's probably easier and more reliable to do some computer vision",
          "score": 42,
          "created_utc": "2026-01-07 18:52:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9a63p",
              "author": "Longjumping-Newt-143",
              "text": "This is the way, run open CV and do some HoughCirc / Lines to get the gauge data.\n\nopencv - How to read an analogue gauge using Open CV - Stack Overflow https://share.google/RwdvlrCOZsvtdo5NR",
              "score": 15,
              "created_utc": "2026-01-07 19:40:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nybxue3",
                  "author": "Tikitanka_11",
                  "text": "Thank you for info. I have monitoring project in few months.",
                  "score": 2,
                  "created_utc": "2026-01-08 03:19:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nydop89",
                  "author": "Curious_Party_4683",
                  "text": "strange there are no youtube guides on how to get this working using a cheap wifi camera to read a gauge and then output the value to Home Assistant.",
                  "score": 1,
                  "created_utc": "2026-01-08 11:22:55",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyavno8",
          "author": "davecrist",
          "text": "You might try having the model give you the position of the arm in degrees or radians and just map it to the gauge value.",
          "score": 3,
          "created_utc": "2026-01-08 00:01:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydamm3",
          "author": "optomas",
          "text": "The way to do this is a transducer and push signal to a PLC/raspberry pi.  \n\nIf the gauge is important enough that it needs watching for alarm, it is important enough to do right.  llm is not the correct application, here.",
          "score": 3,
          "created_utc": "2026-01-08 09:19:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyaxwl6",
          "author": "ParticularAnt5424",
          "text": "Why is everything AI now, did we forget how to code?",
          "score": 10,
          "created_utc": "2026-01-08 00:12:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyd5o43",
              "author": "kkania",
              "text": "Maybe try to be helpful?",
              "score": 6,
              "created_utc": "2026-01-08 08:33:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nygyjkk",
                  "author": "annoyed_NBA_referee",
                  "text": "This seems like a bad application of AI. If itâ€™s a static camera image, you could just look at a bunch of pixels along a define arc (the white part of the gauge, where the needle crosses) and find the dark part mathematically. Much more reliable.",
                  "score": 1,
                  "created_utc": "2026-01-08 20:58:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nybjk1x",
          "author": "Wrapzii",
          "text": "Do not have an llm do this wtf.",
          "score": 3,
          "created_utc": "2026-01-08 02:04:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nybo731",
              "author": "Curious_Party_4683",
              "text": "what's the solution?",
              "score": 2,
              "created_utc": "2026-01-08 02:28:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nybr6mo",
                  "author": "Disastrous_Meal_4982",
                  "text": "Traditional ML (Computer Vision)",
                  "score": 6,
                  "created_utc": "2026-01-08 02:44:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny9ygrf",
          "author": "---j0k3r---",
          "text": "Try any qwen3-vl they are amazing. I use.tje 8b, but the 3b model should handle this well",
          "score": 4,
          "created_utc": "2026-01-07 21:25:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydjutk",
              "author": "Medium_Chemist_4032",
              "text": "It gives 90 reading and can describe the whole gauge perfectly",
              "score": 1,
              "created_utc": "2026-01-08 10:42:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny971l1",
          "author": "merica420_69",
          "text": "GLM 4.6V flash. Very impressed with it.",
          "score": 1,
          "created_utc": "2026-01-07 19:26:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycf88s",
          "author": "Joe-Eye-McElmury",
          "text": "There are some things that LLMs are just always going to be bad at. Weâ€™ve been pretty close to maxing out their capacity for some time, and we are now in the phase of diminishing returns on AI.",
          "score": 1,
          "created_utc": "2026-01-08 05:03:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyctqqa",
              "author": "CooperDK",
              "text": "AI is incredibly good at this. Even a 4 gigabyte model will do this to perfection. Even handwriting!",
              "score": 0,
              "created_utc": "2026-01-08 06:50:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nydxgwc",
                  "author": "Joe-Eye-McElmury",
                  "text": "Oh yes, models that hallucinate 30% of the time can do it â€œto perfection.â€\n\nKeep dreaming, pal.",
                  "score": 1,
                  "created_utc": "2026-01-08 12:27:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyctkgp",
          "author": "CooperDK",
          "text": "Use joycaption for anything text. It is always accurate.",
          "score": 1,
          "created_utc": "2026-01-08 06:48:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyd542r",
          "author": "Wrestler7777777",
          "text": "Is granite4 even a vision model? I tried to feed it your pic on my Open WebUI but it refuses to read the picture.",
          "score": 1,
          "created_utc": "2026-01-08 08:28:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyd9fs8",
          "author": "dkeiz",
          "text": "qwen4Vl told me its 110, but recognize image. qwen3-vl:8B give me 90 psi.   \ni dont want to be salty, but its ollama, why dont you just try on your own? ollama install this models in 1 click",
          "score": 1,
          "created_utc": "2026-01-08 09:08:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydaqeq",
          "author": "RijnKantje",
          "text": "We use Mistral OCR at work to OCR old Electricity meters and their info, so might work for this too.",
          "score": 1,
          "created_utc": "2026-01-08 09:20:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydprgm",
              "author": "Curious_Party_4683",
              "text": "any way to get this working locally and with Home Assistant?",
              "score": 1,
              "created_utc": "2026-01-08 11:31:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nydt15p",
                  "author": "RijnKantje",
                  "text": "Locally as in the entire model offline? Maybe, there's something on their site about it being possible, but probably only for full enterprise setups. (My assumption).\n\nIf the model can be in the cloud then it's:\n\nHome assistant takes pictures -> Uploads pic to Mistral with instructions to only read a part with the dial -> receive and clean answer -> (confirm answer with generic OCR) -> store answer.",
                  "score": 1,
                  "created_utc": "2026-01-08 11:56:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nydwph2",
          "author": "rorowhat",
          "text": "Qwen3 8b-VL should do this no problem",
          "score": 1,
          "created_utc": "2026-01-08 12:22:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nye1unr",
              "author": "Curious_Party_4683",
              "text": "once i give it the image as seen,  can i give it natural language such as \"what number is the needle pointing at?\"",
              "score": 1,
              "created_utc": "2026-01-08 12:55:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nye9qqc",
                  "author": "rorowhat",
                  "text": "Yes, add the image and ask that question l. It should get it right. I've asked to break down way more complex images with data and it did an excellent job.",
                  "score": 1,
                  "created_utc": "2026-01-08 13:41:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyhqos2",
          "author": "dxdementia",
          "text": "python",
          "score": 1,
          "created_utc": "2026-01-08 23:05:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q4ov85",
      "title": "Hardware Suggestions for Local LLM with RAG and MCP for Nonprofit",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q4ov85/hardware_suggestions_for_local_llm_with_rag_and/",
      "author": "Realistic-Foot8724",
      "created_utc": "2026-01-05 15:37:08",
      "score": 21,
      "num_comments": 14,
      "upvote_ratio": 1.0,
      "text": "Good morning.\n\nSorry in advance if I use any terms incorrectly, still a newb to much of this.\n\nLooking for advice on building a PC for learning local LLM usage/deployment. I also have relationships with local non-profit organizations that are very interested in adding AI to their workflows and have major privacy concerns. \n\nUsage:\n\nFor me; local home network with two users looking for inference/chat capabilities as well as developing skills in local AI implementation.\n\n  \nFor non-profits; vectorizing a couple decades worth of documentation (reports in .doc, .pdf, .xls) for RAG, help with statistical analysis (they currently use SPSS), tool calling to search APIs for up-to-date information for literature reviews or adding context/examples to reports, day to day chat/inference.\n\n  \nBudget is $1500-2000 (could stretch this a bit if it will really improve the experience).\n\n  \nConcerns: having at least reasonable speed (conversational) with acceptable power consumption (say not drastically higher than a good quality PC workstation when idling).\n\nLooks like a high capacity (2tb-4tb) NVMe is helpful for model storage/loading.\n\nBudgeting $800 for a RTX 3090 as Nvidia seems to be the way to go and that is the least expensive way to get a decent amount of Vram. Also like the possibility of adding a second RTX 3090 in the future.\n\nShopping used as storage/RAM prices are what they are.\n\nWhere I am really stuck is CPU, motherboard, RAM combo. I see online builds using old HP Z440s, Z4 G4s, Lenovo P620s, or other older workstations with some success. Is Xeon/Threadripper/EPYC worth the power consumption penalty? What would they help with? Would I be better off with a newer (10th-12th gen) I5 or similar CPU? Is a high amount onboard RAM helpful? \n\n\n\nAny direction is appreciated. \n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q4ov85/hardware_suggestions_for_local_llm_with_rag_and/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nxudn0d",
          "author": "irodov4030",
          "text": "You do not need all of this\n\nWhat you need depends on-\n\n1. Existing documents - you can build a RAG db with basic hardware. Initial build may take some time. But it does not need any fancy hardware. Optimise the models.\n\n  \n2. Frequency and size of new documents- this can be critical but mostly basic hardware will be able to manage this. Embed new documents and add to your vecotrdb. Add new documents incremently. Automate this in the night if workload is a concern.\n\n  \n3. Storage of vectorDB- I have built vectorDBs. You might never need TBs of storage.\n\n  \n4. Inference / chat- SLMs perform really well in RAG pipelines. Try inference on user's existing machine with gemma3:1b or even smollm2:135m.\n\n  \nFor two users for inference / chat with a decade of data, I would start at $0 if my user has access to mac with unified memory. If no, then I would get them used macbook at \\~ $600-700 each (preferably any mac with M series and 16GB RAM). For more users, mac mini / studio with more RAM might make sense.\n\n\n\nI have built similar setup for myself.  \nI believe older workstations make sense if you have a separate room for them because they are very noisy(vaccum cleaner level noise).  \nConsider only if they have space for it\n\n  \nI would love to collaborate on this project as volunteer if you are open to it!",
          "score": 9,
          "created_utc": "2026-01-05 16:46:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvpcnp",
              "author": "ChanceKale7861",
              "text": "Love this response! Spot on!",
              "score": 2,
              "created_utc": "2026-01-05 20:24:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxwb300",
              "author": "Realistic-Foot8724",
              "text": "Thanks, this is great information, and I really appreciate the specific model suggestions. I have been testing Mistral 3 3b so far.\n\nFor the organizations, they have years of experience in Windows and run some Windows specific software such as SPSS for statistics. I am not sure migrating to completely new hardware/OS would be welcomed.\n\nFor me, I really want to try larger models to see how close I can get to state of the art online public models. I also want headroom to load more context and for tool usage.\n\nWow, thanks and yes, I will definitely reach out to you soon when I have a better understanding of exactly how large the document hoard to be embedded is and their goals.",
              "score": 1,
              "created_utc": "2026-01-05 22:05:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxu2b6g",
          "author": "dsartori",
          "text": "Hi. I work with not-for-profits a lot and run a technology not-for-profit in my community.\n\nOne problem with custom builds for this type of organization is that there's no money to maintain the custom solution. Often there is a well-intended volunteer who builds out the initial solution. It can definitely lead to trouble down the road without a solid maintenance plan that includes knowledge transfer!\n\nIf there is a technology community or civic tech organization in your community you might want to communicate with them for technical assistance and for help with sustaining an initiative that sounds very worthy and interesting to me.\n\nHappy to chat in DMs if you want to follow up.",
          "score": 3,
          "created_utc": "2026-01-05 15:53:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxu74qd",
              "author": "Realistic-Foot8724",
              "text": "Great point and one of my concerns with using older hardware. I currently am the IT person (not an expert) for one of the organizations and still want to learn as much about local AI as possible, but obviously uptime and general maintenance are a factor in the decision as well as ease of use.",
              "score": 2,
              "created_utc": "2026-01-05 16:15:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxvpmvj",
                  "author": "ChanceKale7861",
                  "text": "Okay, check out several books on it. \n\n-Agents in action\n\n-LLMs in production \n\n- AI engineering\n\n-Agentic Artificial Intelligence\n\nBut these also have great examples to help you as well!",
                  "score": 1,
                  "created_utc": "2026-01-05 20:25:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxuff9o",
          "author": "Own_Attention_3392",
          "text": "Before you spend a single cent on any hardware, do a limited scale proof of concept and see if what you want to do is even feasible, works well, or solves any problems. It's entirely possible you'll discover for $0 that this is unworkable.",
          "score": 3,
          "created_utc": "2026-01-05 16:54:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwcom1",
              "author": "Realistic-Foot8724",
              "text": "Yes agreed. The build I mention above is largely for personal development and wanting to have a lot of headroom to try larger models with more context, etc.\n\nFor the organizations I am associated with, they are using mostly 10th gen i5 or i7 Dell laptops and SFF Optiplex's with no GPUs, so finding SMLs that run well on CPUs is optimal. We have been experimenting with Mistral 3 3b with some success, albeit on the slow side.",
              "score": 1,
              "created_utc": "2026-01-05 22:12:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxyq443",
          "author": "fasti-au",
          "text": "Your likely better on open router using green models and ween3 cider devstral.  And having a gut hub sub for  big models.  If itâ€™s non profit and not private then you can chat all day for free in bursts so thereâ€™s heaps of ways to get models.   Google gives it away all over the place \n\nThe reality is that you need 32gb ram now for most small big models.  120b. Is 4 24gb cards.  70b you can do in 64ish. \n\n\nIf your looking more a task helper qwen 14 and phi4 amongst others soo great at 14 and the lower you go the more   Do this specifically.  Vs   Sonething to do this pls. \n\nSo a 4070 superti is about entry or 3090 if still around and then your still rag building so lightrag or mem0 etc. \n\nMcp is sort irrelevant as anything is an mcp if you put code in.   What you need it all for?\n\nRewriting emails so post content.  Basic but solid enough to get things like n8n doing actions etc. \n\nYou can do heaps with 2b models too BUT they are very much I only know exactly what you tell me and if you tell me wrong !!!    I turn left now good luck everybody.  \n\nBasic startup look at Cole medin au starter kit. Gives you guid backend rag etc all pre setup mostly so your in the ground.    Else you package I. To say openwebui lmstudio    Not blocked from them they just have their own backend options too.",
          "score": 1,
          "created_utc": "2026-01-06 06:24:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1t2ex",
              "author": "Realistic-Foot8724",
              "text": "That tip on the Cole Medin starter kit is great. I had not come across that. Thanks!",
              "score": 1,
              "created_utc": "2026-01-06 18:18:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny1xe8r",
                  "author": "fasti-au",
                  "text": "No worries.  Heâ€™s a pretty good guy for a click grabber. Actually has some decent guys in the projects heâ€™s in also.",
                  "score": 1,
                  "created_utc": "2026-01-06 18:37:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nypi01m",
          "author": "quantimx",
          "text": "You don't need fancy stuff. I built my rig for around Â£1200, search for i9 desktops in the marketplace depending upon your location you may find good used hardware with still in warranty. I bought 4070 used. Install claude code and let it handle all for you. Don't waste money here and there.",
          "score": 1,
          "created_utc": "2026-01-10 01:27:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzrjxs",
          "author": "natika1",
          "text": "First, don't be sorry. We all still learn and it's ok. Second there are plenty options in the cloud. For e.g. Google for Gov or other startup/ nonprofit plans. It's worth to check it before buying new machines. \nOf course if there is private data involved, you need to be careful to not break any policies.",
          "score": 0,
          "created_utc": "2026-01-06 12:00:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1ud1h",
      "title": "Does Open WebUI actually crawl links with Ollama, or is it just hallucinating based on the URL?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q1ud1h/does_open_webui_actually_crawl_links_with_ollama/",
      "author": "Whole-Competition223",
      "created_utc": "2026-01-02 09:32:31",
      "score": 20,
      "num_comments": 27,
      "upvote_ratio": 0.92,
      "text": "Hi everyone,\n\nI recently started using **Open WebUI** integrated with **Ollama**. Today, I tried giving a specific URL to an LLM using the `#` prefix and asked it to summarize the content in Korean.\n\nAt first, I was quite impressed because the summary looked very plausible and well-structured. However, I later found out that Ollama models, by default, cannot access the internet or visit external links.\n\nThis leaves me with a few questions:\n\n1. **How did it generate the summary?** Was the LLM just \"guessing\" the content based on the words in the URL and its pre-existing training data? Or does Open WebUI pass some scraped metadata to the model?\n2. **Is there a way to enable \"real\" web browsing?** I want the model to actually visit the link and analyze the current page content. Are there specific functions, tools, or configurations in Open WebUI (like RAG settings) that allow Ollama models to access external websites?\n\nI'd love to hear how you guys handle web-based tasks with local LLMs. Thanks in advance!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q1ud1h/does_open_webui_actually_crawl_links_with_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nx8k2hc",
          "author": "Ultralytics_Burhan",
          "text": "Might be a better question for r/OpenWebUI but natively AFAIK, you can't use the `#` (at least not on a recent version) to inject the webpage content. You need to click the `+` in the chat to add content and select \"Attach Webpage\" [see the code here for the UI modal](https://github.com/open-webui/open-webui/blob/a7271532f8a38da46785afcaa7e65f9a45e7d753/src/lib/components/chat/MessageInput/AttachWebpageModal.svelte#L41). Which will fetch the webpage contents and add it to the chat as context. Remember you will need to also ensure that the `num_ctx` is large enough to include the entire prompt, page content, and response to avoid hallucinations. If any part gets truncated, the quality of output will decrease significantly.",
          "score": 11,
          "created_utc": "2026-01-02 11:25:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxedcv9",
              "author": "Whole-Competition223",
              "text": "Youâ€™re right. Using the '+' button correctly fetches and analyzes the URL. The quality isn't quite at Gemini's level yet, but it gets the job done. Through this process, Iâ€™ve also learned that the '#' symbol is used to call up documents or web collections. Thanks for the help!",
              "score": 2,
              "created_utc": "2026-01-03 07:05:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxgpj2a",
                  "author": "Ultralytics_Burhan",
                  "text": "Of course! Funny enough, I learned that you can directly inject the website page b/c of your question!",
                  "score": 1,
                  "created_utc": "2026-01-03 16:43:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx8k65h",
          "author": "inspiredbyhands",
          "text": "In the Open WebUI you can configure the web scraper with some popular search engines and their APi key. I think this happens before itâ€™s actually passed to the model.",
          "score": 5,
          "created_utc": "2026-01-02 11:26:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxedz01",
              "author": "Whole-Competition223",
              "text": "I tried SearXNG, but it is not working properly yet. ğŸ¥² search is hard! ğŸ”",
              "score": 1,
              "created_utc": "2026-01-03 07:11:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxds2q1",
          "author": "irodov4030",
          "text": "what you want is agentic ai tool use.\n\ntools can be for\n\n1. web search: searches web, retieves top results and retrieves url and metadata\n2. web scraping: actually visits the wbesit and does web scraping.\n\nThere are multiple ways to do this, I believe multiple project support this  \nNot sure about Open WebUI.\n\nIf you have some experience with python, ollama and web scraping, you can do these yourself.\n\n\\* remember not every llm model supports tool use",
          "score": 2,
          "created_utc": "2026-01-03 04:25:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxeeser",
              "author": "Whole-Competition223",
              "text": "Thanks for the clear explanation! That really helps me understand the difference between web search and scraping. Iâ€™ll do some more digging on 'Agentic AI' and tool-use-supported models on my own. Appreciate the guidance!",
              "score": 1,
              "created_utc": "2026-01-03 07:18:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxeo3rs",
              "author": "DutchOfBurdock",
              "text": "2. Loading the page and taking a screenshot and having the AI scrape that instead. Stealthier and less likely to get the web agent blocked.",
              "score": 1,
              "created_utc": "2026-01-03 08:38:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgbnct",
                  "author": "irodov4030",
                  "text": "yes, it might make more sense.",
                  "score": 1,
                  "created_utc": "2026-01-03 15:37:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx8mxg4",
          "author": "ButCaptainThatsMYRum",
          "text": "Did you enable the web search? Seems like you should know more about your setup than we do.",
          "score": 2,
          "created_utc": "2026-01-02 11:49:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxee4ry",
              "author": "Whole-Competition223",
              "text": "I tried SearXNG, but no luck so far. The LLM is refusing to use my settings for some reason ğŸ¥². It's harder than it looks! ğŸ› ï¸",
              "score": 1,
              "created_utc": "2026-01-03 07:12:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx9q5ee",
          "author": "Striking_Wishbone861",
          "text": "Actually I just set this up in open web ui 2 days agoâ€¦. So itâ€™s doable. Unfortunately I am not at all technical with a lot of this LLM but Iâ€™m learning. I used Gemini to assist me. I can absolutely verify that it worked and came back with real,data. While I had started to set up the api key mid way we switched to a different search engine. Maybe it was called pse ? I think it was near the bottom of the list and it did not need an api key.",
          "score": 1,
          "created_utc": "2026-01-02 15:49:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxedsjo",
              "author": "Whole-Competition223",
              "text": "Thanks for sharing! I'm still a bit confused about using the '#' feature, though. Sometimes it feels like it's pulling the actual content perfectly, but other times it seems to be hallucinating. It's tricky to get it consistent.\n\nRegarding PSEâ€”if you're talking about the Google Programmable Search Engine, was it difficult to set up? I'd love to know if it's beginner-friendly.",
              "score": 1,
              "created_utc": "2026-01-03 07:09:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxiwuxs",
                  "author": "Striking_Wishbone861",
                  "text": "I was using # but instead I went ahead and created a container and added that in the models main page for the knowledge. I have 3 files in there for my model to reference. I found that better for my needs.\n\nAs far as web searching I didnâ€™t do anything special with prompts just ensure there is a search engine selected. And the boxes for we search is ticked. As long as youâ€™re there enable image too. I was able to find something online from an image \n\nI am pretty sure in my main system prompt I addressed hallucinations. \nKeep in mind once again, I have no idea how to do this stuff. I have a Gemini account and basically used that to set up my offline model",
                  "score": 1,
                  "created_utc": "2026-01-03 23:02:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxdodn5",
          "author": "gamesta2",
          "text": "I use searxng, I host it in a separate container and it works great. But the \"scrape\" is the first 200 tokens of each web page so its mostly headlines. \n\nIm working on a pipeline to use an mcp tool to open the full web page that ranks the highest in result ranked, but so far im just using openai for my multi-step reasoning searches.\n\nIf youre self hosting for privacy, definitely look into searxng.",
          "score": 1,
          "created_utc": "2026-01-03 04:01:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdrhcm",
              "author": "irodov4030",
              "text": "If i am not wrong, it would be pulling metadata of the website and not scraping at all",
              "score": 1,
              "created_utc": "2026-01-03 04:21:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxe6z5y",
                  "author": "gamesta2",
                  "text": "I can see what it pulls from the sources it gives me in the results, and it just shows like the first paragraph of the article.\n\nUnless its all part of the Metadata then youre not wrong. \nIn either case, its not too different given that in both cases the info pulled is not enough to have full context. Im hoping to utilize playwright instead or in conjunction with searxng",
                  "score": 1,
                  "created_utc": "2026-01-03 06:13:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxk24ir",
          "author": "Revolutionary-Judge9",
          "text": "I built a desktop application called [Askimo](https://askimo.chat) which  support scrapping a webpageâ€™s HTML and sends the extracted text to an AI model for summarization.The AI only read the HTML content then summarize it.\n\nThe same approach applies when you ask an AI to summarize PDFs or binary files like Word or Excel documents. The AI does not read those files directly. Instead, a client tool extracts the text and sends it to the AI with an instruction to summarize.\n\nI think the WebUI uses the same techniques when it supports multiple models and some model can access or browse the internet content, some are not then process at the client side guarantee the client behave the same behavior with different AI models.\n\nDisclaimer: Askimo currently only reads the static HTML content. If parts of the page are generated by JavaScript, it may not capture that content. This can be addressed by using Playwright to render the page and extract the fully generated HTML.\n\nhttps://preview.redd.it/5m5hi4wku8bg1.png?width=3680&format=png&auto=webp&s=9654e5de6efdade750d9f3b9122357e3c12ee2a6",
          "score": 1,
          "created_utc": "2026-01-04 02:45:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxv6rb5",
              "author": "DrJuliiusKelp",
              "text": "Thanks for posting this.\n\nSo far, I have been very impressed.\n\nOne quick question: What are the best tool-calling models to use with this? (I noticed that DeepSeek didn't work.)",
              "score": 2,
              "created_utc": "2026-01-05 18:58:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxvb8b2",
                  "author": "Revolutionary-Judge9",
                  "text": "Thank you for trying askimo! I tested with gpt-oss mostly, but it should work with any tool-calling models support. I will test with DeepSeek, and keep your post in this thread. I must prompt differently for gemini and perhaps i must tweak the prompt for deepseek.",
                  "score": 1,
                  "created_utc": "2026-01-05 19:18:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx8jnol",
          "author": "HyperWinX",
          "text": "!remindme 4d",
          "score": -1,
          "created_utc": "2026-01-02 11:21:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8jr7s",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 4 days on [**2026-01-06 11:21:30 UTC**](http://www.wolframalpha.com/input/?i=2026-01-06%2011:21:30%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/ollama/comments/1q1ud1h/does_open_webui_actually_crawl_links_with_ollama/nx8jnol/?context=3)\n\n[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Follama%2Fcomments%2F1q1ud1h%2Fdoes_open_webui_actually_crawl_links_with_ollama%2Fnx8jnol%2F%5D%0A%0ARemindMe%21%202026-01-06%2011%3A21%3A30%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201q1ud1h)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 0,
              "created_utc": "2026-01-02 11:22:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q79f82",
      "title": "Rethinking RAG: How Agents Learn to Operate",
      "subreddit": "ollama",
      "url": "https://i.redd.it/f6gc8q8k24cg1.png",
      "author": "frank_brsrk",
      "created_utc": "2026-01-08 11:30:17",
      "score": 17,
      "num_comments": 4,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q79f82/rethinking_rag_how_agents_learn_to_operate/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nye7hrz",
          "author": "danny_094",
          "text": "This is what I'm currently developing https://github.com/danny094/Jarvis \n\nYou can read all the documents. I'm very transparent. If you'd like to discuss it, feel free to get in touch :)",
          "score": 3,
          "created_utc": "2026-01-08 13:28:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nykaqch",
              "author": "Thin_Beat_9072",
              "text": "hey I'm building something similar as well as a web app. are you still running a discord server? the link in the github repo didnt work, invite expired.",
              "score": 2,
              "created_utc": "2026-01-09 08:35:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nykb6qz",
                  "author": "danny_094",
                  "text": "Thanks for the information. I've updated the link. I don't want to include unnecessary advertising in this post. Simply open the link again :)",
                  "score": 2,
                  "created_utc": "2026-01-09 08:40:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q179er",
      "title": "Ollama models to specific GPU",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q179er/ollama_models_to_specific_gpu/",
      "author": "NormalSmoke1",
      "created_utc": "2026-01-01 15:51:10",
      "score": 14,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "I'm trying to hard force the OLLAMA model to specifically sit on a designated GPU.  As I looked through the OLLAMA docs, it says to use the CUDA visible devices in the python script, but isn't there somewhere in the unix configuration I can set at startup?  I have multiple 3090's and I would like to have the model on sit on one, so the other is free for other agents.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q179er/ollama_models_to_specific_gpu/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nx3gspc",
          "author": "AndThenFlashlights",
          "text": "Ollama doesnâ€™t appear to have a setting for GPU affinity like that. I use docker containers for forcing Ollama endpoints to use a specific GPU, by only passing the GPU I want it to use into its container. Force it to never unload, and make sure the containers launch first. Then my main Ollama instance can flex and load/unload whatever it needs to in remaining VRAM across all cards.\n\nEDIT: According to u/AlexByrth [in this comment](https://www.reddit.com/r/LocalLLaMA/comments/1fe8g8z/comment/lyowmuv/) (and https://docs.ollama.com/gpu), you can specify GPU affinity for Ollama overall in environment vars. But doesn't let you specify GPU affinity for a specific LLM model, though.",
          "score": 7,
          "created_utc": "2026-01-01 16:08:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4bvey",
              "author": "NormalSmoke1",
              "text": "Would it have any problems connecting to my vector store in another container, or use that secondary endpoint to help?",
              "score": 3,
              "created_utc": "2026-01-01 18:47:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx4gks3",
                  "author": "AndThenFlashlights",
                  "text": "Iâ€™m not sure, I havenâ€™t played with that yet. How are you handling your vector DB? I do have my docker containersâ€™ Ollama model store directory mapped to share the hostsâ€™ directory and it hasnâ€™t caused any problems.",
                  "score": 1,
                  "created_utc": "2026-01-01 19:10:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx46gqh",
          "author": "suicidaleggroll",
          "text": "Run it in docker and only pass in the GPU that you want it to have access to",
          "score": 3,
          "created_utc": "2026-01-01 18:20:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5rdcc",
          "author": "StardockEngineer",
          "text": "Switch tov llama.cpp. It's almost as easy to use these days.",
          "score": 1,
          "created_utc": "2026-01-01 23:14:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxqi11",
      "title": "Ollama Model which Suits for my System",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1pxqi11/ollama_model_which_suits_for_my_system/",
      "author": "devil__6996",
      "created_utc": "2025-12-28 12:43:52",
      "score": 13,
      "num_comments": 34,
      "upvote_ratio": 0.85,
      "text": "I havenâ€™t downloaded these models yet and want to understand real-world experience before pulling them locally.\n\nHardware:\n\n* RTX 4050 (6GB VRAM)\n* 32GB RAM\n* Ryzen 7 7000 series\n\nUse case:\n\n* Vibe coding\n* Code generation\n* Building software applications\n\n\\- Web UI via Ollama (Open WebUI or similar)  \n\\-For Cybersecurity Code generations etc,,,",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pxqi11/ollama_model_which_suits_for_my_system/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nwcv308",
          "author": "-Akos-",
          "text": "It's been here a million times: with your setup, it will not be spectacular, but it's free. just try it. You can also try LM Studio, which has a full interface built in, and selection of a model catalog built-in. It will even tell you whether your computer will be able to run things or not.",
          "score": 3,
          "created_utc": "2025-12-28 13:00:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwk8qmw",
              "author": "FieldMouseInTheHouse",
              "text": "Spectacular or not: Could you please list which models would actually fit inside of the OP's specified VRAM?",
              "score": 1,
              "created_utc": "2025-12-29 15:52:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwldcgp",
                  "author": "DelbertAud",
                  "text": "[https://github.com/Pavelevich/llm-checker](https://github.com/Pavelevich/llm-checker)",
                  "score": 1,
                  "created_utc": "2025-12-29 19:02:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdk57e",
          "author": "Excellent_Piccolo848",
          "text": "Yes, you are Not going to get any spectacular Here, but local ist Always the prefered option! Look at ministral 3b or qwen 4b. Any reasoning model unser 5b should Work in your device, just klick on \"latest\" on ollama.com and Look for them!",
          "score": 2,
          "created_utc": "2025-12-28 15:35:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwexebe",
          "author": "ZeroSkribe",
          "text": "Use \"ollama ps\" to ensure whatever model you run fits entirely into VRAM. Anything that will fit will run decently on any modern card. My 3050's run great. I would work through qwen 3.",
          "score": 2,
          "created_utc": "2025-12-28 19:34:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgrdr7",
          "author": "CooperDK",
          "text": "With your setup, I am inclined to say forget it. You want at least 16 GB VRAM for it to be remotely funny.",
          "score": 2,
          "created_utc": "2025-12-29 01:12:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdxbtb",
          "author": "itsbinaryck",
          "text": "Gemma 3 (1B) or llama 3.2 (3B)\n\nIf you use it for work, consider getting additional vram for better models",
          "score": 1,
          "created_utc": "2025-12-28 16:42:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfsbo9",
          "author": "grudev",
          "text": "You can pull several models and test how they perform on a series of prompts using Ollama Grid Search:\n\n\nhttps://github.com/dezoito/ollama-grid-search\n\n\nThe \"releases\" section has installers for all major OSs",
          "score": 1,
          "created_utc": "2025-12-28 22:05:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg5f9z",
          "author": "Nearby_Truth9272",
          "text": "Yeah, I'd focus on quantization models of larger ones or even smaller ones, such as those suggested. Many on Huggingface. If you plan to use for real cyber related items, you may run into content or response refusal on many of the models. You can however, vibe code away but your context windows and response times are going to be problematic, even with smaller models. If you can upgrade your GPU to a 5060Ti with 16GB, it would be better than attempting to find a few 4060ti with the same VRAM",
          "score": 1,
          "created_utc": "2025-12-28 23:14:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgoqtt",
          "author": "Witty_Mycologist_995",
          "text": "Use a 4b dense, or 30b MoE",
          "score": 1,
          "created_utc": "2025-12-29 00:56:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwk01es",
              "author": "FieldMouseInTheHouse",
              "text": "Could you provide a list of models that you would recommend?",
              "score": 1,
              "created_utc": "2025-12-29 15:09:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwksih6",
                  "author": "Witty_Mycologist_995",
                  "text": "ArliAIâ€™s GPT OSS derestricted, qwen 4b instruct (latest version), Qwen vl 30b, nemotron nano 30b",
                  "score": 1,
                  "created_utc": "2025-12-29 17:26:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwgx05x",
          "author": "ngg990",
          "text": "Use antigravity and other idea free tier layer and that's it. Vining with that setup won't be possible",
          "score": 1,
          "created_utc": "2025-12-29 01:44:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjs6nr",
          "author": "FieldMouseInTheHouse",
          "text": "What editors are you considering?",
          "score": 1,
          "created_utc": "2025-12-29 14:27:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjt8n2",
          "author": "AlgorithmicMuse",
          "text": "depending on how complicated whatever you are  vibe coding , local llms just cant compete , not even close to the the cloud models , claude/gemini.   on the other hand if you are  making  agents  especially using  tools  locals can be great using dense models Best one i've tried so far that follows  prompt instructions best was  qwen3-coder:30b , tried about 10 models up to 70b Q4 models .  Trivial vibe coding  locals can sort of work , but anything other than trivial head to the cloud.",
          "score": 1,
          "created_utc": "2025-12-29 14:33:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwju3qc",
              "author": "FieldMouseInTheHouse",
              "text": "Wow!  What models have you actually tried that would remotely work at all on the OP's platform configuration?\nPlease list those models.",
              "score": 1,
              "created_utc": "2025-12-29 14:38:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjyaku",
                  "author": "AlgorithmicMuse",
                  "text": "tried most of the qwen,deepseekR1 ,  mistral,  Im no expert in any of this , I just try them for what I want to use them for because the rack and stack specs mean nothing if  they don't work for what you want.  depending on what your doing the best thing I found  to help models along is  setting  temperature, and larger num\\_ctx   , although that can  greatly increase ram usage so you might wind up in swap, also increase  num\\_predict",
                  "score": 1,
                  "created_utc": "2025-12-29 15:00:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxeajsh",
          "author": "realnub235",
          "text": "qwen3:4b is just the hands down here not even close imo, i use the instruct version on my 6GB GPU it runs super well, punches way above its weight so much it is not even funny",
          "score": 1,
          "created_utc": "2026-01-03 06:42:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfutcr",
          "author": "RandomSwedeDude",
          "text": "You're not gonna be  vibing with anything less than 24 GB VRAM. If even then",
          "score": 1,
          "created_utc": "2025-12-28 22:18:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwky0ma",
          "author": "FieldMouseInTheHouse",
          "text": "# Summary of Vibe Coding Models for 6GB VRAM Systems\n\nSo, I will summarize what models have been suggested here so far.  Here is what we have that would actually fit inside of your 6GB VRAM budget.  I am deliberately leaving out any models that anybody suggested that would not have fit inside of your 6GB VRAM budget! ğŸ¤—\n\n* \\`[qwen3:4b](https://ollama.com/library/qwen3:4b)\\` size=2.5GB\n* \\`[ministral-3:3b](https://ollama.com/library/ministral-3:3b)\\` size=3.0GB\n* \\`[gemma3:1b](https://ollama.com/library/gemma3:1b)\\` size=815MB\n* \\`[gemma3:4b](https://ollama.com/library/gemma3:4b)\\` size=3.3GB ğŸ‘ˆ I added this one because it is a little bigger than the `gemma3:1b`, but still fits confortably inside of your 6GB VRAM budget.  This model should be more capable than `gemma3:1b`.\n\nI would suggest that you first try these models with `ollama run MODELNAME` and check to see how they fit in your VRAM (`ollama ps`) and check them for performance (`/set verbose`).\n\nWhat do you think?",
          "score": 0,
          "created_utc": "2025-12-29 17:52:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlrhxv",
              "author": "Witty_Mycologist_995",
              "text": "Iâ€™m going to leave this here for the OP, but unlike what this guy says, you do not have to fit models completely inside of your vram, unlike this guy says. It is futile to get ChatGPT quality code with 4b models, sadly. You can try MoE models. This guy hates MoE models for whatever reason, cuz itâ€™s â€œslowâ€. It isnâ€™t that slow.",
              "score": 1,
              "created_utc": "2025-12-29 20:10:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0hwsc",
      "title": "Built an offline-first vector database (v0.2.0)  looking for real-world feedback",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q0hwsc/built_an_offlinefirst_vector_database_v020/",
      "author": "Serious-Section-5595",
      "created_utc": "2025-12-31 17:04:49",
      "score": 12,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "Iâ€™ve been working on **SrvDB**, an **offline embedded vector database** for local and edge AI use cases.\n\nNo cloud. No services. Just files on disk.\n\n**Whatâ€™s new in v0.2.0:**\n\n* Multiple index modes: Flat, HNSW, IVF, PQ\n* Adaptive â€œAUTOâ€ mode that selects index based on system RAM / dataset size\n* Exact search + quantized options (trade accuracy vs memory)\n* Benchmarks included (P99 latency, recall, disk, ingest)\n\nDesigned for:\n\n* Local RAG\n* Edge / IoT\n* Air-gapped systems\n* Developers experimenting without cloud dependencies\n\nGitHub: [https://github.com/Srinivas26k/srvdb](https://github.com/Srinivas26k/srvdb)  \nBenchmarks were run on a consumer laptop (details in repo).  \nI have included the benchmark code run it on your and upload it  on the GitHub discussions which helps to improve and add features accordingly. I request for contributors to make the project great.\\[ [https://github.com/Srinivas26k/srvdb/blob/master/universal\\_benchmark.py](https://github.com/Srinivas26k/srvdb/blob/master/universal_benchmark.py) \\]\n\nIâ€™m **not trying to replace Pinecone / FAISS / Qdrant**  this is for people who want something small, local, and predictable.\n\nWould love:\n\n* Feedback on benchmarks\n* Real-world test reports\n* Criticism on design choices\n\nHappy to answer technical questions.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q0hwsc/built_an_offlinefirst_vector_database_v020/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nwy3657",
          "author": "tom-mart",
          "text": "How does it compare to pgvector?",
          "score": 3,
          "created_utc": "2025-12-31 17:34:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy5tag",
              "author": "Serious-Section-5595",
              "text": "pgvector is great when you already have Postgres.\nSrvDB is focused on low-memory, offline/edge use cases simple to prototype and runs well even on 4GB RAM machines without a database server.\nItâ€™s a small, local-first tool, not a replacement for exsisting systems.",
              "score": 1,
              "created_utc": "2025-12-31 17:47:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwyajw4",
                  "author": "tom-mart",
                  "text": "When I'm starting a new project, why to pick your project over postgres + pgvector? Postgres can run on a potato, you are likely to need DB for your agent anyway. So what is the benefit in prototypimg with SrvDB?",
                  "score": 2,
                  "created_utc": "2025-12-31 18:10:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxfzvk0",
                  "author": "Tobi-Random",
                  "text": "But then we already have \n\nhttps://github.com/sqliteai/sqlite-vector\nhttps://github.com/asg017/sqlite-vec\n\nEven turso has vectors: https://github.com/tursodatabase/turso\n\nSqlite-vector claims using only 30mb RAM.\n\nThese days more and more people seem to reimplement the same things over and over again.",
                  "score": 2,
                  "created_utc": "2026-01-03 14:35:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx4f7fa",
          "author": "Fit-Presentation-591",
          "text": "How does this compare to sqlite vec?",
          "score": 2,
          "created_utc": "2026-01-01 19:03:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlosan",
              "author": "Serious-Section-5595",
              "text": "Good question. sqlite-vec (and similar SQLite extensions) are great if you already want a SQL database with vector support layered on top.\n\nSrvDB is intentionally different in scope: itâ€™s a **pure Rust, embedded, offline vector store** with **no SQL layer, no extensions, and no database server assumptions**.\n\nThe trade-off is that you lose general-purpose querying, but gain **simpler internals, predictable memory usage, and tighter control over storage and search** for local/edge use cases.\n\nItâ€™s not meant to replace sqlite-vec more like an alternative building block when you donâ€™t want SQLite in the stack at all. Feedback and comparisons are very welcome.",
              "score": 1,
              "created_utc": "2026-01-04 10:04:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx87kku",
          "author": "guitar_rick",
          "text": "That looks good but the new trend is Knowledge Graphs. We're going more into structured search rather than semantic-search. [https://microsoft.github.io/graphrag](https://microsoft.github.io/graphrag) and [https://learnopencv.com/lightrag](https://learnopencv.com/lightrag)",
          "score": 2,
          "created_utc": "2026-01-02 09:29:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlox1t",
              "author": "Serious-Section-5595",
              "text": "Totally agree knowledge graphs and structured search are becoming increasingly important, especially for explainability and reasoning-heavy tasks.\n\nI see SrvDB as complementary rather than competing here. Vector search still works well as a **low-level retrieval primitive** (for embeddings, signals, or candidate recall), while graphs sit higher in the stack for structure and reasoning.\n\nOne idea Iâ€™m interested in is using a small, local vector store like this *alongside* lightweight graph layers, especially for offline or edge setups.\n\nThanks for sharing the links always good to see where the ecosystem is heading.",
              "score": 1,
              "created_utc": "2026-01-04 10:05:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzny3e",
      "title": "OllamaFX Client - Add to Ollama oficial list of clients",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/gallery/1pzny3e",
      "author": "Electronic-Reason582",
      "created_utc": "2025-12-30 17:22:53",
      "score": 11,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pzny3e/ollamafx_client_add_to_ollama_oficial_list_of/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwvx7o6",
          "author": "Noiselexer",
          "text": "Looks clean, I'll try it.",
          "score": 2,
          "created_utc": "2025-12-31 09:01:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwx22po",
              "author": "Electronic-Reason582",
              "text": "muchas gracias, espero tu feedback y comentarios para mirarque mejorar, pulir o agregar",
              "score": 1,
              "created_utc": "2025-12-31 14:24:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5n8oy",
          "author": "keith_heaton",
          "text": "would be nice if you would make a docker for this",
          "score": 1,
          "created_utc": "2026-01-01 22:51:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q7qy2i",
      "title": "Happy New Year! ğŸ‰ Nanocoder 1.20.0 Release: A Fresh Start to 2026 with Major Improvements",
      "subreddit": "ollama",
      "url": "/r/nanocoder/comments/1q7qxqq/happy_new_year_nanocoder_1200_release_a_fresh/",
      "author": "willlamerton",
      "created_utc": "2026-01-08 22:56:35",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q7qy2i/happy_new_year_nanocoder_1200_release_a_fresh/",
      "domain": "",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q36m42",
      "title": "What model to use and how to disable using cloud.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q36m42/what_model_to_use_and_how_to_disable_using_cloud/",
      "author": "ItsWappers",
      "created_utc": "2026-01-03 21:24:58",
      "score": 11,
      "num_comments": 9,
      "upvote_ratio": 0.82,
      "text": "I just don't want to use credits and want to know what model is the best for offline use.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q36m42/what_model_to_use_and_how_to_disable_using_cloud/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nxiqs1d",
          "author": "seangalie",
          "text": "It all depends on your computer and what specs you are running - at the low end, the qwen3:4b family is solidly useful on almost any decent modern hardware; if you're rocking a semi-decent GPU - gpt-oss:20b is a solid choice.  The qwen3-coder:30b is a MoE model that will run surprisingly \"okay\" on as little as 6 GB of VRAM (as long as you have the slightly higher system RAM to back things up).",
          "score": 5,
          "created_utc": "2026-01-03 22:31:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmazlg",
              "author": "ItsWappers",
              "text": "R7 7800X3D 5060 Ti 16 gb 32 gb ddr5 6000",
              "score": 1,
              "created_utc": "2026-01-04 13:08:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxzvias",
                  "author": "seangalie",
                  "text": "You'll likely find that the qwen3 family in the 30b-a3b MoE size at Q4\\_K\\_M quant will be a decent fit - you'll have to play with the context a little.  You'll have spillover from the GPU but the way the MoE architecture works, you should maintain workable performance.  I have an older (A5000 Mobile) GPU with 16GB VRAM that qwen3-coder:30b, qwen3:30b-a3b, and qwen3-vl:30b all run well on.  \n  \nIf you don't want to \"put all your eggs in one basket\" - nemotron-3-nano and gpt-oss:20b will be good companions as well, with the gpt-oss:20b likely being the fastest on your setup.",
                  "score": 1,
                  "created_utc": "2026-01-06 12:29:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxjpylv",
          "author": "Preconf",
          "text": "No need to disable cloud. As long as you don't explicitly download any models with cloud tag and don't signin/sign up you couldnt use it even if you wanted to.",
          "score": 4,
          "created_utc": "2026-01-04 01:37:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxie150",
          "author": "ozcapy",
          "text": "For what computer? What are your specs? What would you like for it to do?",
          "score": 2,
          "created_utc": "2026-01-03 21:28:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmawup",
              "author": "ItsWappers",
              "text": "R7 7800X3D 5060 Ti 16 gb 32 gb ddr5 6000",
              "score": 1,
              "created_utc": "2026-01-04 13:08:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxnw2o0",
                  "author": "CynicalTelescope",
                  "text": "I have almost the exact same configuration (my CPU is a 9700X, everything else is the same) and a good general-purpose model that runs very well is gpt-oss:20b.  It just fits entirely in VRAM on the 5060 Ti  and delivers very good performance.",
                  "score": 3,
                  "created_utc": "2026-01-04 18:01:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxig7j3",
          "author": "drakgremlin",
          "text": "I've been using \\`qwen3:8b\\` quiet a lot.  On an M4 Max it's great.  On my 13 year old computer I've had most run at a snails pace; those with AVX2 perform reasonably better.  Need to get one of those offline ones to run AVX512 to see if it's even better.  I guess I'm saying use a GPU :-D",
          "score": 4,
          "created_utc": "2026-01-03 21:39:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxnywrq",
          "author": "_twrecks_",
          "text": "Ollama has a free cloud tier, it's pretty good actually. I use it to run bigger models like glm-4.7. you get a reasonable amount of tokens per week though I don't see details in the limits.",
          "score": 1,
          "created_utc": "2026-01-04 18:13:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q8a8uy",
      "title": "RAGLight Framework Update : Reranking, Memory, VLM PDF Parser & More!",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q8a8uy/raglight_framework_update_reranking_memory_vlm/",
      "author": "Labess40",
      "created_utc": "2026-01-09 14:50:41",
      "score": 11,
      "num_comments": 2,
      "upvote_ratio": 0.88,
      "text": "Hey everyone! Quick update onÂ [RAGLight](https://github.com/Bessouat40/RAGLight), my framework for building RAG pipelines in a few lines of code. Try it easily using your favorite Ollama model ğŸ‰\n\n# Better Reranking\n\nClassic RAG now retrieves more docs and reranks them for higher-quality answers.\n\n# Memory Support\n\nRAG now includes memory for multi-turn conversations.\n\n# New PDF Parser (with VLM)\n\nA new PDF parser based on a vision-language model can extract content from images, diagrams, and charts inside PDFs.\n\n# Agentic RAG Refactor\n\nAgentic RAG has been rewritten usingÂ **LangChain**Â for better tools, compatibility, and reliability.\n\n# Dependency Updates\n\nAll dependencies refreshed to fix vulnerabilities and improve stability.\n\nğŸ‘‰ Repo:Â [https://github.com/Bessouat40/RAGLight](https://github.com/Bessouat40/RAGLight)\n\nğŸ‘‰ Documentation :Â [https://raglight.mintlify.app](https://raglight.mintlify.app/)\n\nHappy to get feedback or questions!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q8a8uy/raglight_framework_update_reranking_memory_vlm/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nylyiua",
          "author": "immediate_a982",
          "text": "Which vulnerability was fixed",
          "score": 1,
          "created_utc": "2026-01-09 15:22:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylzv1x",
              "author": "Labess40",
              "text": "Vulnerabilities on previous langchain versions and langchain\\_core versions",
              "score": 2,
              "created_utc": "2026-01-09 15:28:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q5zcyq",
      "title": "JRVS Community Feedback",
      "subreddit": "ollama",
      "url": "https://i.redd.it/v6c9mctugtbg1.jpeg",
      "author": "Xthebuilder",
      "created_utc": "2026-01-06 23:50:47",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q5zcyq/jrvs_community_feedback/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1py1odn",
      "title": "Old server for local models",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1py1odn/old_server_for_local_models/",
      "author": "Jacobmicro",
      "created_utc": "2025-12-28 20:33:03",
      "score": 9,
      "num_comments": 13,
      "upvote_ratio": 0.8,
      "text": "Ended up with an old poweredge r610 with the dual xeon chips and 192gb of ram. Everything is in good working order. Debating on trying to see if I could hack together something to run local models that could automate some of the work I used to pay API keys for with my work. \n\nAnybody ever have any luck using older architecture? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1py1odn/old_server_for_local_models/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nwffwfu",
          "author": "King0fFud",
          "text": "I have an R730 with dual Xeons (8 cores/16 threads each) and 240GB RAM but no GPUs and had at best mixed success with some moderate to larger qwen2.5-coder and deepseek-coder-v2 models. The advantages of having a pile of memory and cores are minimal compared to having GPUs for processing and the lower memory bandwidth of older machines doesnâ€™t help.\n\nIâ€™d say that as long as youâ€™re okay with a relatively low rate in terms of tokens per second then all good. Otherwise youâ€™ll need some to install some GPUs.",
          "score": 4,
          "created_utc": "2025-12-28 21:04:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwfg9ht",
              "author": "Big-Masterpiece-9581",
              "text": "I would argue theyâ€™ll spend enough on electricity depending on local prices that in no time theyâ€™ll pay for a more efficient gpu or system like a Ryzen 395.",
              "score": 2,
              "created_utc": "2025-12-28 21:06:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwfyzpu",
                  "author": "King0fFud",
                  "text": "Maybe, it depends on the configuration as my R730 idles at 70W and can get up to 120-140W full load and thatâ€™s with Xeon V4s. There are obviously more efficient setups than old servers for this considering that these beasts were meant to run VM loads and such.",
                  "score": 1,
                  "created_utc": "2025-12-28 22:39:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwghod1",
                  "author": "Jacobmicro",
                  "text": "I mean I did get it for free and power bills arent bad, if I ever get the money I'll build a dedicated 395 unit.",
                  "score": -1,
                  "created_utc": "2025-12-29 00:19:24",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwh1bhl",
          "author": "AndThenFlashlights",
          "text": "It will work, but itâ€™ll be painfully slow and very power hungry. Iâ€™m a huge proponent of rat-rod LLM servers, but even the R720 motherboard and top-of-the-line Xeons that it supports are slower running a GPU for inference than an R740. \n\nI donâ€™t recommend it. You need a GPU for anything thatâ€™ll feel useful. Even an old P4 or something is better than trying to use those Xeons.",
          "score": 1,
          "created_utc": "2025-12-29 02:09:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwikg7a",
          "author": "Candid_Highlight_116",
          "text": "the problem isn't in the age of CPU but it being CPU with close to zero SIMD capability relative to GPU. Neural networks rely on applying same operation for extreme numbers of variables as if you were laying up images over images, and all the superscalar features on CPUs become dead weights in doing that",
          "score": 1,
          "created_utc": "2025-12-29 08:45:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwflsaf",
          "author": "According_Study_162",
          "text": "GPU /w VRAM matters more, not SYSTEM memory.",
          "score": 1,
          "created_utc": "2025-12-28 21:33:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwghgqb",
              "author": "Jacobmicro",
              "text": "True, but I just got this server for free and was just going to run docker containers on it for different things, but before I committed wanted to explore this too just in case.\n\nCan't install gpus in this rack anyways since a 1u unit. Not sure if I'll bother with risers or not yet.",
              "score": 0,
              "created_utc": "2025-12-29 00:18:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwgtso7",
                  "author": "thisduuuuuude",
                  "text": "Agree with the mindset lol, nothing beats free especially if it turns out it can do more than what you originally thought. No harm in exploring",
                  "score": 1,
                  "created_utc": "2025-12-29 01:25:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q7xej4",
      "title": "I built Plano - a framework-friendly data plane with orchestration for agents",
      "subreddit": "ollama",
      "url": "https://i.redd.it/hn2gtphwt8cg1.png",
      "author": "AdditionalWeb107",
      "created_utc": "2026-01-09 03:32:09",
      "score": 9,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q7xej4/i_built_plano_a_frameworkfriendly_data_plane_with/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nyja2vc",
          "author": "callmrplowthatsme",
          "text": "Cool",
          "score": 2,
          "created_utc": "2026-01-09 03:59:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyjalww",
              "author": "AdditionalWeb107",
              "text": "ğŸ™ğŸ™",
              "score": 1,
              "created_utc": "2026-01-09 04:02:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q81go3",
      "title": "Fine-tune SLMs 2x faster, with TuneKit!",
      "subreddit": "ollama",
      "url": "https://v.redd.it/9586ijk6v9cg1",
      "author": "Consistent_One7493",
      "created_utc": "2026-01-09 07:00:11",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q81go3/finetune_slms_2x_faster_with_tunekit/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q519lf",
      "title": "Models with a sense of humor?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q519lf/models_with_a_sense_of_humor/",
      "author": "icebergelishious",
      "created_utc": "2026-01-05 23:06:00",
      "score": 7,
      "num_comments": 13,
      "upvote_ratio": 0.82,
      "text": "I was trying some models and hit them the the \"Who invented running?\" prompt, and then I responded back with \"False, running was invented by Thomas Running is 1748 he tried to walk twice at the same time\"\n\nSome of them got the joke, but others it went over their head and they thought I was stupid haha",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q519lf/models_with_a_sense_of_humor/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nxx3h6e",
          "author": "Clipbeam",
          "text": "I think this is just a matter of telling any model in the system prompt to take any responses with a sense of humor?",
          "score": 3,
          "created_utc": "2026-01-06 00:30:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxych5t",
              "author": "icebergelishious",
              "text": "I think this is it actually! This question prompted me to learn how to set up modelfiles and custom system prompts actually. Similar performance, but AI that can handle my sarcasm haha",
              "score": 1,
              "created_utc": "2026-01-06 04:43:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxakyf",
          "author": "XxAnomo305",
          "text": "I've used gemma3 does pretty well BUT you need to make a very sophisticated promot and play around with it until it reacts decently, ive had around 82% (other % it just doesn't respond well) success rate  with humor, rudeness, being cocky etc.",
          "score": 2,
          "created_utc": "2026-01-06 01:07:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxcdgm",
          "author": "laurentbourrelly",
          "text": "Feed the beast \n\n[https://github.com/orionw/rJokesData](https://github.com/orionw/rJokesData)\n\n[https://github.com/taivop/joke-dataset](https://github.com/taivop/joke-dataset)\n\nThere is a bunch of those.\n\nIn the System Prompt, insist on what you want, and most reliable models will do this type of job nowadays.",
          "score": 2,
          "created_utc": "2026-01-06 01:17:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxdqco",
          "author": "social_tech_10",
          "text": "A model with a good sense of humor can make you laugh.  If you are looking for a model that likes your jokes and says you are funny, we call that feature something else.",
          "score": 2,
          "created_utc": "2026-01-06 01:24:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny0j658",
          "author": "Birdinhandandbush",
          "text": "The chattiest model I've found is LFM2 but a good system prompt is your starting point",
          "score": 1,
          "created_utc": "2026-01-06 14:46:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny28uv5",
          "author": "sceadwian",
          "text": "They don't understand humor at all.",
          "score": 1,
          "created_utc": "2026-01-06 19:29:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxx5gvx",
          "author": "justseeby",
          "text": "None of them have a sense of humor OP. Iâ€™m sure you can tell one to pretend, if thatâ€™s what floats your boat.",
          "score": 0,
          "created_utc": "2026-01-06 00:40:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxfnqx",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 0,
              "created_utc": "2026-01-06 01:35:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxybqt8",
                  "author": "justseeby",
                  "text": "Itâ€™s just predictive text",
                  "score": 1,
                  "created_utc": "2026-01-06 04:38:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q4lf3s",
      "title": "Google's Coral chip not compatible?  what's the next cheap hardware to run locally?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q4lf3s/googles_coral_chip_not_compatible_whats_the_next/",
      "author": "Curious_Party_4683",
      "created_utc": "2026-01-05 13:16:44",
      "score": 7,
      "num_comments": 8,
      "upvote_ratio": 0.82,
      "text": "im kinda bummed out out Ollama not compat with this $50 Coral chip that i got.\n\nwhat's the next best thing to run Ollama 100% locally? \n\ni plan to use Ollama with Home Assistant to identify delivery people, boxes or packages left on my porch, read pressure gauges, and utility meters.  so far, Google Gemini has been working flawlessly   but i would like to get off the cloud if i can.... ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q4lf3s/googles_coral_chip_not_compatible_whats_the_next/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nxwgwzs",
          "author": "bohlenlabs",
          "text": "Coral is at end of life. Allegedly there will be something new called Coral NPU but who knows whenâ€¦ and until when it will last. Itâ€™s Google to me.",
          "score": 2,
          "created_utc": "2026-01-05 22:33:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxtbmqw",
          "author": "ghormeh_sabzi",
          "text": "Cpu inference on small models! I have gptoss 20b running usably on 16gb ram and there are even smaller moe models like granite4 tiny",
          "score": 2,
          "created_utc": "2026-01-05 13:32:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxtdovk",
              "author": "Curious_Party_4683",
              "text": "i have an old i7 (7th gen) with 32gb of RAM for Blue Iris.   which small models do you recommend for my usage as mentioned above?",
              "score": 1,
              "created_utc": "2026-01-05 13:44:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxthmul",
                  "author": "UseHopeful8146",
                  "text": "You should be able to run any of the granite 4 models. I have like a 5th gen intel i7, 8 core with 16gb ram - so, old, and I can run granite 4h 1b with 1mil context window without much issue",
                  "score": 2,
                  "created_utc": "2026-01-05 14:07:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxwmzt3",
                  "author": "ghormeh_sabzi",
                  "text": "gpt oss 20b, qwen3 30b, and nemotron nano should all work just fine on that, granite4 tiny will be fast as heck",
                  "score": 1,
                  "created_utc": "2026-01-05 23:04:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nybkam0",
          "author": "sinan_online",
          "text": "I run Gemma 3 4B on 12GB VRAM and Gemma 3 2B on 6GB VRAM on a very old laptop, working reasonably well. I actually containerized the models, if you are interested, PM me.",
          "score": 1,
          "created_utc": "2026-01-08 02:08:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0n8h9",
      "title": "EmergentFlow - Visual AI workflow builder with native Ollama support",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q0n8h9/emergentflow_visual_ai_workflow_builder_with/",
      "author": "l33t-Mt",
      "created_utc": "2025-12-31 20:57:13",
      "score": 6,
      "num_comments": 6,
      "upvote_ratio": 0.72,
      "text": "https://preview.redd.it/1hjueesaslag1.png?width=1918&format=png&auto=webp&s=01d473be20f1064fa77b522d54c8ac4702efd081\n\nSome of you might recognize me from my moondream/minicpm computer use agent posts, or maybe LlamaCards. Ive been tinkering with local AI stuff for a while now.\n\nIm a single dad working full time, so my project time is scattered, but I finally got something to a point worth sharing.\n\nEmergentFlow is a node-based AI workflow builder, but architecturally different from tools like n8n, Flowise, or ComfyUI. Those all run server-side on their cloud or you self-host the backend.\n\n**EmergentFlow runs the execution engine in your browser.** Your browser tab is the runtime. When you connect Ollama, calls go directly from your browser to localhost:11434 (configurable). \n\nIt supports cloud APIs too (OpenAI, Anthropic, Google, etc.) if you want to mix local + cloud in the same flow. There's a Browser Agent for autonomous research, RAG pipelines, database connectors, hardware control.\n\nBecause I want new users to experience the system, I have provided anonymous users without an account, 50 free credits using googles cloud API, these are simply to allow users to see the system in action before requiring they create an account.  \n\nTerrified of launching, be gentle.  \n\n[https://emergentflow.io/](https://emergentflow.io/)\n\nCreate visual flows directly from your browser.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q0n8h9/emergentflow_visual_ai_workflow_builder_with/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nx0y7fz",
          "author": "960be6dde311",
          "text": "Not sure I want to visit that website. Do you have a GitHub link?",
          "score": 2,
          "created_utc": "2026-01-01 03:27:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx10emd",
              "author": "l33t-Mt",
              "text": "Closed source for now. The site won't bite though, its a landing page.",
              "score": 1,
              "created_utc": "2026-01-01 03:42:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2vh9t",
                  "author": "tecneeq",
                  "text": ">Closed source for now\n\nDropped then, for now. Use GPL if you don't want corpos to stripmine your work.",
                  "score": 2,
                  "created_utc": "2026-01-01 13:57:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx0kwsf",
          "author": "Gsfgedgfdgh",
          "text": "Nice! Impressive you build it yourself, will check it out. Cheers!",
          "score": 1,
          "created_utc": "2026-01-01 01:57:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0t6v4",
              "author": "l33t-Mt",
              "text": "Thanks man, appreciate it!",
              "score": 1,
              "created_utc": "2026-01-01 02:53:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxr2ws",
      "title": "How to get started with automated workflows?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1pxr2ws/how_to_get_started_with_automated_workflows/",
      "author": "PlastikHateAccount",
      "created_utc": "2025-12-28 13:14:38",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Hi there, I'm interested how you guys set up ollama to work on tasks.\n\nThe first thing we already tried is having a Python script that calls the company internal Ollama via api with simple tasks in a loop. Imagine pseudocode:\n\n    for sourcecode in repository: \n      api-call-to-ollama(\"Please do a sourcecode review: \" + sourcecode)    \n\nWe tried multiple tasks like this for **multiple usecases, not just sourcecode reviews** and the intelligence is quite promising but ofc the context the LLMs have available to solve tasks like that limiting.\n\nSo the second idea is to somehow let the LLM make the decision what to include in a prompt. Let's call them \"pretasks\".\n\nThis pretask could be a prompt saying Â´\"Write a prompt to an LLM to do a sourcecode review. You can decide to include adjacent PDFs, Jira tickets, pieces of sourcecode by writing <include:filename>\" + list-of-available-files-with-descriptions-what-they-areÂ´. The python script would then parse the result of the pretask to collect the relevant files.\n\nThird and finally, at that point we could let the pretask trigger itself even more pretasks. This is where the thing would be almost bootstrapped. But I'm out of ideas how to coordinate this, prevent endless loops etc.\n\nSorry if my thoughts around this whole topic are a little scattered. I assume the whole world is right now thinking about these kinds of workflows. So I'd like to know where to start reading about it.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pxr2ws/how_to_get_started_with_automated_workflows/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nwg30lv",
          "author": "960be6dde311",
          "text": "Have you tried building agents using the Pydantic AI framework? That's where I would start. You can add tool function calls and MCP servers to extend the core language model capabilities.",
          "score": 1,
          "created_utc": "2025-12-28 23:01:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjk50y",
              "author": "PlastikHateAccount",
              "text": "Tysm for this answer. I did not realize that's what \"agents\" are. Now I know. \n\nMy homemade version of this had the primary issue that it would make a lot of queries without any progress or using any tools.\n\nLets see if Langchain or any similar tool is more useful for this",
              "score": 2,
              "created_utc": "2025-12-29 13:40:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwnjhp8",
                  "author": "Available-Craft-5795",
                  "text": "Try to self host N8N if it matches your workflow and requirements",
                  "score": 1,
                  "created_utc": "2025-12-30 01:44:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q4p21j",
      "title": "Model Running for 1 day",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q4p21j/model_running_for_1_day/",
      "author": "Binary_Alpha",
      "created_utc": "2026-01-05 15:43:48",
      "score": 6,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "https://preview.redd.it/8fq1qo2rwjbg1.png?width=1638&format=png&auto=webp&s=693365be93fcdd8809028b0cbbf5ddf36deb869b\n\nI've been running this model for one day and it's not even finished. For your guys information, I'm running it on a Raspberry Pi 5 overclocked at 2.8 gigahertz. 16 gigabytes of RAM. of course this computer is not meant to do this workload and it's not surprising that it's taking one whole day to do this. when it's finished I'll update you guys with the final tokens per second  and time it ran everything.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q4p21j/model_running_for_1_day/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nxxnlmd",
          "author": "weird_gollem",
          "text": "Can't wait for information! yes please, share!!!! Just be careful with the temperature!",
          "score": 1,
          "created_utc": "2026-01-06 02:17:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxyz9ui",
              "author": "Binary_Alpha",
              "text": "Dont worry Im using a large heat sink. The temperatures are never going over 60Â° C.",
              "score": 1,
              "created_utc": "2026-01-06 07:45:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyeoa6r",
                  "author": "Remarkable_Today9135",
                  "text": "that's hot",
                  "score": 1,
                  "created_utc": "2026-01-08 14:55:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxywf7o",
          "author": "tom-mart",
          "text": "Someone had to try it ğŸ˜‚\n\nWhat's the monitoring tool?",
          "score": 1,
          "created_utc": "2026-01-06 07:19:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxyyw5e",
              "author": "Binary_Alpha",
              "text": "Im using btop++ as the monitoring tool",
              "score": 1,
              "created_utc": "2026-01-06 07:41:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxzb8gi",
          "author": "florinandrei",
          "text": "I bet it ends with 42.",
          "score": 1,
          "created_utc": "2026-01-06 09:39:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4coqz",
          "author": "Available-Craft-5795",
          "text": "Most can go to 3.0Ghz, also what prompt did you use?",
          "score": 1,
          "created_utc": "2026-01-07 01:40:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny58yz5",
              "author": "Binary_Alpha",
              "text": "I can't go to 3.0GHz, it crashes. I was converting a markdown text to typst format, but where I live the electricity isn't stable, and the prompt didnt finish",
              "score": 2,
              "created_utc": "2026-01-07 04:45:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q47pg0",
      "title": "Ollama Cloud?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q47pg0/ollama_cloud/",
      "author": "Natjoe64",
      "created_utc": "2026-01-05 01:14:23",
      "score": 6,
      "num_comments": 13,
      "upvote_ratio": 0.65,
      "text": "Hey everyone, been using ollama as my main ai provider for a while, and it works great for smaller tasks with on device Qwen 3 vl, Ministral, and other models, but my 16 gb of unified memory on my M2 Pro Macbook Pro is getting a little cramped. 4b is plenty fast, and 8b is doable with quantization, but especially with bigger context lengths it's getting tight, and I don't want to cook my ssd alive with overusing swap. I was looking into a server build, but with ram prices being what they are combined with gpus that would make the endeavour worth the squeeze, it's looking very expensive. \n\nWith a yearly cost of 250, is ollama cloud the best way to use these massive 235b+ models without forking over data to openai, anthropic, or google? The whole reason I started to use ollama was the data collection and spooky ammounts of knowledge that these commercial models can learn about you. Ollama cloud seems to have a very \"trust me bro\" approach to privacy in their resources, which only really say \"Ollama does not log prompt or response data\". I would trust them more than the frontier ai labs listed above, but I would like to see some evidence. If you do use ollama cloud, is it worth it? How do these massive models like mistral large 3 and the 235b parameter version of qwen 3 vl compare to the frontier models? \n\nTL;DR: Privacy policy nonexistent, but I need more vram",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q47pg0/ollama_cloud/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nxqh8lo",
          "author": "Condomphobic",
          "text": "Youâ€™re forking over data to Ollama Cloud. Whatâ€™s the difference between that and giving your data to OAI/Google?",
          "score": 13,
          "created_utc": "2026-01-05 01:19:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxql3vg",
              "author": "Natjoe64",
              "text": "As there is no evidence for their security architecture, gut feeling. I know it sounds stupid, but OAI and Google both have financial incentives for them to collect all the data that they possibly can, for both training new models, but also for Google, advertising. Ollama has no financial incentive for them to collect user data. It would irreparably harm their reputation, and wouldn't help them as they don't train their own models, or data broker. Add this in with Google's track record (Chrome incognito being a prime example) and OAIs going to put ads into the ChatGPT apps, which with their cornucopia of training data will be very lucrative for them. Thus further incentivizing the collection of user data.",
              "score": -1,
              "created_utc": "2026-01-05 01:39:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxsardj",
                  "author": "SIMMORSAL",
                  "text": "They wouldn't use the data to train their models, but they could easily sell it to the buyers standing in the queue. They offer free use too, and we all know what that means",
                  "score": 6,
                  "created_utc": "2026-01-05 08:32:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxqpjri",
          "author": "Mr_TakeYoGurlBack",
          "text": "Openrouter.ai",
          "score": 4,
          "created_utc": "2026-01-05 02:03:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxqr9n4",
              "author": "Natjoe64",
              "text": "\"The types of personal data that we may collect include, but are not limited to: the personal data you provide to us, personal data collected automatically about your use of our Site or Service, and information from third parties, including our business partners.\" \n\n\"Details of your visits to our Site, including, but not limited to, traffic data, location data, log files to understand how our Service is performing, browser history, search, information about links you click, pages you view, and other communication data and the resources that you access and use on the Site.\"\n\nI'd rather not, thanks. Their privacy policy is just as bad as using the frontier proprietary models anyways.",
              "score": 2,
              "created_utc": "2026-01-05 02:12:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxqk10f",
          "author": "GloomyPop5387",
          "text": "You could cluster 2 m4 max 128gb Mac studios and have256gb of unified ram for close to that 200 bucks a month that you noted - if you spread the cost over 2 to 3 years.\n\nI sure hope they do an m5 ultra.\nIâ€™ll part with a lot of money to get a 512-1tb of memory.",
          "score": 2,
          "created_utc": "2026-01-05 01:34:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxrcu7m",
              "author": "broimsuperman",
              "text": "How would you go about clustering?",
              "score": 1,
              "created_utc": "2026-01-05 04:12:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxxh9sx",
                  "author": "GloomyPop5387",
                  "text": "Itâ€™s a new thing with m4max and m5 cpuâ€™s. Â Pretty sure itâ€™s just a lightning cable, but itâ€™s specifically for ai workloads as far as I know.",
                  "score": 1,
                  "created_utc": "2026-01-06 01:43:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxqll36",
              "author": "Natjoe64",
              "text": "3,500$ starting price without any storage upgrades per Mac isn't what I'm looking for. That server build would most likely be a beginner setup with 24-48 gb of vram, most likely running on a bunch of 3060 12 gbs. At some point I would like to get a framework desktop or something, but like I said: ram pricing.",
              "score": 0,
              "created_utc": "2026-01-05 01:42:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxvlwt1",
                  "author": "Savantskie1",
                  "text": "Iâ€™m running my gaming computer as my local ai server. Since I donâ€™t throw away old parts and I foolishly bought two sets of 32GB of RAM that only lets me use 3 of the 4 sticks of 16GB of RAM sticks I have, Iâ€™ve got 48GB of ram and an RX 7900 XT 20GB and an old RX 6800 16GB cards. I still try to keep most of the models on vram, but I still get decent t/s. I canâ€™t read very fast, so Iâ€™m happy with 16-18 t/s",
                  "score": 1,
                  "created_utc": "2026-01-05 20:07:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxqu48t",
          "author": "cnmoro",
          "text": "You can create a wrapper openai compatible API, that will use open router for cheap. When sending a request, use a local model to identify and replace any sensitive information on the prompt before sending the request (this can be done automatically, and easy to vibe code)",
          "score": 1,
          "created_utc": "2026-01-05 02:27:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrep29",
          "author": "DutchOfBurdock",
          "text": "Local-AI and a swarm.",
          "score": 1,
          "created_utc": "2026-01-05 04:23:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2hp7u",
      "title": "Integrated Mistral Nemo (12B) into a custom Space Discovery Engine (Project ARIS) for local anomaly detection.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q2hp7u/integrated_mistral_nemo_12b_into_a_custom_space/",
      "author": "Limp-Regular3741",
      "created_utc": "2026-01-03 01:58:21",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.87,
      "text": "Just wanted to share a real-world use case for local LLMs. Iâ€™ve built a discovery engine called Project ARIS that uses Mistral Nemo as a reasoning layer for astronomical data.\n\nThe Stack:\n\nModel: Mistral Nemo 12B (Q4\\_K\\_M) running via Ollama.\n\nHardware: Lenovo Yoga 7 (Ryzen AI 7, 24GB RAM) on Nobara Linux.\n\nIntegration: Tauri/Rust backend calling the Ollama API.\n\nHow Iâ€™m using the LLM:\n\nContextual Memory: It reads previous session reports from a local folder and greets me with a verbal recap on boot.\n\nIntent Parsing: I built a custom terminal where Nemo translates \"fuzzy\" natural language into structured MAST API queries.\n\nAnomaly Scoring: It parses spectral data to flag \"out of the ordinary\" signatures that don't fit standard star/planet profiles.\n\nItâ€™s amazing how much a 12B model can do when given a specific toolset and a sandboxed terminal. Happy to answer any questions about the Rust/Ollama bridge!\n\n  \nA preview of Project ARIS can be found here:\n\n  \n[https://github.com/glowseedstudio/Project-ARIS](https://github.com/glowseedstudio/Project-ARIS)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q2hp7u/integrated_mistral_nemo_12b_into_a_custom_space/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q2wny9",
      "title": "Offline agent testing chat mode using Ollama as the judge (EvalView)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q2wny9/offline_agent_testing_chat_mode_using_ollama_as/",
      "author": "hidai25",
      "created_utc": "2026-01-03 15:00:12",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.78,
      "text": "Quick demo:\n\nhttps://reddit.com/link/1q2wny9/video/z75urjhci5bg1/player\n\n\n\nIâ€™ve been working on EvalView (pytest-style regression tests for tool-using agents) and just added an interactive chat mode that runs fully local with Ollama.\n\nInstead of remembering commands or writing YAML up front, you can just ask:\n\nâ€œrun my testsâ€\n\nâ€œwhy did checkout fail?â€\n\nâ€œdiff this run vs yesterdayâ€™s golden baselineâ€\n\nIt uses your local Ollama model for the chat + for LLM-as-judge grading. No tokens leave your machine, no API costs (unless you count electricity and emotional damage).\n\nSetup:\n\n`ollama pull llama3.2`\n\n`pip install evalview`\n\n`evalview chat --provider ollama --model llama3.2`\n\nWhat it does:\n\n\\- Runs your agent test suite + diffs against baselines\n\n\\- Grades outputs with the local model (LLM-as-judge)\n\n\\- Shows tool-call / latency / token (and cost estimate) diffs between runs\n\n\\- Lets you drill into failures conversationally\n\nRepo:\n\n[https://github.com/hidai25/eval-view](https://github.com/hidai25/eval-view)\n\nQuestion for the Ollama crowd:\n\nWhat models have you found work well for \"reasoning about agent behavior\" and judging tool calls?\n\nIâ€™ve been using llama3.2 but Iâ€™m curious if mistral or deepseek-coder style models do better for tool-use grading.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q2wny9/offline_agent_testing_chat_mode_using_ollama_as/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1py5h57",
      "title": "Questions about usage limits for Ollama Cloud models (high-volume token generation)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1py5h57/questions_about_usage_limits_for_ollama_cloud/",
      "author": "AlexHardy08",
      "created_utc": "2025-12-28 23:08:28",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 0.86,
      "text": "Hello everyone,\n\nIâ€™m currently evaluating **Ollama Cloud models** and would appreciate some clarification regarding **usage limits on paid plans**.\n\nIâ€™m interested in running the following cloud models via Ollama:\n\n* `ollama run gemini-3-flash-preview:cloud`\n* `ollama run deepseek-v3.1:671b-cloud`\n* `ollama run gemini-3-pro-preview`\n* `ollama run kimi-k2:1t-cloud`\n\n# My use case\n\n* Daily content generation: **\\~5â€“10 million tokens per day**\n* Number of prompt submissions: **\\~1,000â€“2,000 per day**\n* Average prompt size: **\\~2,500 tokens**\n* Responses can be long (multi-thousand tokens)\n\n# Questions\n\n1. Do the **paid Ollama plans** support this level of token throughput (5â€“10M tokens/day)?\n2. Are there **hard daily or monthly token caps** per model or per account?\n3. How are **API requests counted** internally by Ollama for each prompt/response cycle?\n4. Does a single `ollama run` execution map to **one API request**, or can it generate multiple internal calls depending on response length?\n5. Are there **per-model limitations** (rate limits, concurrency, max tokens) for large cloud models like DeepSeek 671B or Kimi-K2 1T?\n\nIâ€™m trying to determine whether the current **paid offering can reliably sustain this workload** or if additional arrangements (enterprise plans, quotas, etc.) are required.\n\nAny insights from the Ollama team or experienced users running high-volume workloads would be greatly appreciated.\n\nThank you!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1py5h57/questions_about_usage_limits_for_ollama_cloud/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nwnodln",
          "author": "Narrow-Impress-2238",
          "text": "You better ask this to ollama support team.\n\nHere no one knows actual limits",
          "score": 2,
          "created_utc": "2025-12-30 02:10:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q4slm1",
      "title": "What GPU for lecture summarizing?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q4slm1/what_gpu_for_lecture_summarizing/",
      "author": "dnielso5",
      "created_utc": "2026-01-05 17:50:23",
      "score": 5,
      "num_comments": 12,
      "upvote_ratio": 0.78,
      "text": "Hello,\n\nMy GF is in collage and records her lectures, I was going to get something like Plaude to do AI transcribing and summarizing but the teachers forbid sending the audio to 3rd parties (they even need permission to share recordings with each-other)\n\nI set up a small server as a test and run Scriberr + ollama.\n\nScriberr model: Small\n\nOllama model: llama3.2:3b\n\nThe specs for the proof of concept are:\n\nCPU: 2600x\n\nRam: 16g\n\nGPU: Thats my question!\n\nScribing a 32 minute lecture took about 14 minutes and a very small summary took about 15 minutes. Thats not horrible as they only need to run once, but if i try and use a chat window thats easy another 12 minutes per chat and usually times out. \n\nI understand VRAM is way better than system RAM but I'm wondering what would be ideal.\n\nI have a 1660 with 6G i can test with but im guessing ill need 8G+",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q4slm1/what_gpu_for_lecture_summarizing/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nxve7h4",
          "author": "Excellent_Piccolo848",
          "text": "I would look an eBay. 3060 with 12gb ist the Minimum, you would need more vram, for the summarizing llm. If you can afford it, get a 3090, you can get them for as less as 600$ on eBay (but watch out, dont buy a GPU, that was used for Mining!).",
          "score": 2,
          "created_utc": "2026-01-05 19:32:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvi3uq",
              "author": "The_HenryUK",
              "text": "mining is not a problem for GPUs and does not have any adverse effects as long as it is cooled properly. It's actually better for most components to stay at a static temperature with constant load than to go through lots of heat cycles",
              "score": 2,
              "created_utc": "2026-01-05 19:50:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxvwy63",
                  "author": "DutchOfBurdock",
                  "text": "Agreed. I used a T1000 for Tron mining. It didn't tax it too much, but after 3 years of mining, I'm now using it for LLMs and it's holding it's own.",
                  "score": 2,
                  "created_utc": "2026-01-05 20:59:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxyuhji",
              "author": "fasti-au",
              "text": "Technically 1600 series up for most things.  I have 2080 and 1660s doing shit still",
              "score": 1,
              "created_utc": "2026-01-06 07:01:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny1wdr7",
              "author": "dnielso5",
              "text": "Found a 3060 12g for $170 and it trimmed down the transcribing to 1-2 minutes for a 32 minute video.\n\nrunning qwen2.5:14b-instruct-q4_K_M with a 32k context window summarized the video text in about 4 minutes.",
              "score": 1,
              "created_utc": "2026-01-06 18:33:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxyudi8",
          "author": "fasti-au",
          "text": "Can do with 8gb ram but 16gb be better.  The actual lecture to text is whisper. No real dramas.  You screenshots to image is probably better done only in qwen local as a guess.   Rewrite summarise qwen 8b phi4 mini seems solid wordsmiths for agentic summaries etc. \n\nSo you can do with 12-16gb local easily    Smaller than s pissible but your need more guidance and dependant on topics ect",
          "score": 2,
          "created_utc": "2026-01-06 07:00:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny0sytk",
              "author": "dnielso5",
              "text": "I got a 3060 with 12g vram and dropped the translation time down to 90 seconds and summary time down to 1 minute\n\nYeah. Now I'm just testing different models to see what's better.",
              "score": 0,
              "created_utc": "2026-01-06 15:34:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxuw0cr",
          "author": "Aud3o",
          "text": "Something recent by NVIDIA or Intel, at least 8GB",
          "score": 1,
          "created_utc": "2026-01-05 18:10:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxv6n28",
              "author": "dnielso5",
              "text": "3060 12g?",
              "score": 0,
              "created_utc": "2026-01-05 18:57:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxvwhjj",
          "author": "DutchOfBurdock",
          "text": "nVidia GTX 5000 or better (16GB VRAM as a minimum)",
          "score": 1,
          "created_utc": "2026-01-05 20:57:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzv44x",
          "author": "MattReedly",
          "text": "Ive just completed a similar project, i have a Ryzen 5700x and RX6600 (8gb vram) gpu, summaries of large files took on average 2.25 seconds using python3 and ollama on ubuntu",
          "score": 1,
          "created_utc": "2026-01-06 12:26:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny0sr5k",
              "author": "dnielso5",
              "text": "I got a 3060 with 12g vram and dropped the translation time down to 90 seconds and summary time down to 1 minute.",
              "score": 0,
              "created_utc": "2026-01-06 15:33:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q4vzci",
      "title": "Achieving 30x Real-Time Transcription on CPU . Multilingual STT Openai api endpoint compatible. Plug and play in Open-webui - Parakeet",
      "subreddit": "ollama",
      "url": "/r/LocalLLaMA/comments/1q4vz16/achieving_30x_realtime_transcription_on_cpu/",
      "author": "SlightPossibility331",
      "created_utc": "2026-01-05 19:49:24",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q4vzci/achieving_30x_realtime_transcription_on_cpu/",
      "domain": "",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q81rvx",
      "title": "Make an AI continue mid-sentence?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q81rvx/make_an_ai_continue_midsentence/",
      "author": "poobumfartwee",
      "created_utc": "2026-01-09 07:18:41",
      "score": 5,
      "num_comments": 9,
      "upvote_ratio": 0.86,
      "text": "I know a little how AI works, it just predicts the next word in a sentence. However, when I ask ollama \\`1 + 1 = \\` then it answers \\`Yes, 1 + 1 is 2\\`.\n\nHow do I make it simply continue a sentence of my choosing as if it was the one that said it?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q81rvx/make_an_ai_continue_midsentence/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nyk5eaj",
          "author": "XxAnomo305",
          "text": "use correct models that are \"text\" and \"base\" models they are different from standard models and or make a highly sophisticated system prompt. both are fairly simple solutions.",
          "score": 4,
          "created_utc": "2026-01-09 07:48:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykb9n7",
          "author": "me6me",
          "text": "by default ollama ads to your request a bunch of tokens and system prompt so it  instructs model to do conversation. you can force raw output when using api.",
          "score": 3,
          "created_utc": "2026-01-09 08:40:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykk6m1",
          "author": "abubakkar_s",
          "text": "Try this system prompt:\n\n*You are a language model that only continues text exactly as given. Never answer questions or explain; only predict the next tokens.*",
          "score": 3,
          "created_utc": "2026-01-09 10:02:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylz45r",
              "author": "Sorry-Ease-4854",
              "text": "I tested this both in Ollama and Gemini and it works perfectly â¤ï¸",
              "score": 2,
              "created_utc": "2026-01-09 15:24:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nykl5aa",
          "author": "Medium_Chemist_4032",
          "text": "You can call the API directly:\n\n    |â‡’ curl -s http://localhost:11434/api/generate \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\n        \"model\": \"llama3.1:8b\",\n        \"prompt\": \"1 + 1 = \",\n        \"raw\": true,\n        \"stream\": false,\n        \"options\": {\n          \"temperature\": 0,\n          \"num_predict\": 24,\n          \"stop\": [\"\\n\"]\n        }\n      }'\n    {\"model\":\"llama3.1:8b\",\"created_at\":\"2026-01-09T10:04:48.983785Z\",\"response\":\n    \n    \"2, but what about 1 + 1 + 1?\"\n    \n    ,\"done\":true,\"done_reason\":\"stop\",\"total_duration\":1000507209,\"load_duration\":120500750,\"prompt_eval_count\":7,\"prompt_eval_duration\":253957583,\"eval_count\":14,\"eval_duration\":516983831}%\n\nOr use for example openwebui that allows to edit answers directly and click \"continue\".\n\nhttps://preview.redd.it/6haf5hcftacg1.png?width=748&format=png&auto=webp&s=b0098c6afa7348fab099618b6f101be945ac60c4",
          "score": 2,
          "created_utc": "2026-01-09 10:11:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyno8gg",
          "author": "sceadwian",
          "text": "You can't do that because people engineer text to make what they're thinking understandable, the AI has no idea what your thoughts are so will deviate from the way you were talking about things almost immediately.",
          "score": 1,
          "created_utc": "2026-01-09 19:59:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyk9wjk",
          "author": "Thin_Beat_9072",
          "text": "\"What is 1+1? Return the answer as a JSON object with the key 'result'.\"  \n  \n`{`\n\n  `\"result\": 2`\n\n`}`\n\nOR simply \"Answer with only the number: 1+1\"",
          "score": 0,
          "created_utc": "2026-01-09 08:28:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nypdtlo",
              "author": "poobumfartwee",
              "text": "Unfortunately I want this to work with the stupidest of models running on my budget laptop, so relying on prompts alone will probably never work",
              "score": 2,
              "created_utc": "2026-01-10 01:04:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyr7ir8",
                  "author": "Thin_Beat_9072",
                  "text": "works fine with my 1B model.",
                  "score": 1,
                  "created_utc": "2026-01-10 08:29:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pz7hpe",
      "title": "Upload folders to a chat",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1pz7hpe/upload_folders_to_a_chat/",
      "author": "Cool-Condition466",
      "created_utc": "2025-12-30 03:42:17",
      "score": 4,
      "num_comments": 3,
      "upvote_ratio": 0.84,
      "text": "I have a problem, im kinda new to this so bear with me. I have a mod for a game that i'm developing and I just hit a dead end so i'm trying to use ollama to see if it can help me. I wanted to upload the whole mod folder but it is not letting me do it instead it just uploads the python and txt files thar are scattered all over there. How can I upload the whole folder?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pz7hpe/upload_folders_to_a_chat/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nwo802o",
          "author": "No-Consequence-1779",
          "text": "How can you convert the whole folder to a single file?Â ",
          "score": 1,
          "created_utc": "2025-12-30 04:01:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo940a",
              "author": "Cool-Condition466",
              "text": "I mean like how can i get the AI to read the whole mod file",
              "score": 1,
              "created_utc": "2025-12-30 04:08:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxgwl56",
          "author": "Available-Craft-5795",
          "text": "RAG",
          "score": 1,
          "created_utc": "2026-01-03 17:16:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q04s56",
      "title": "Has anyone tried routing Claude Code CLI to multiple model providers?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q04s56/has_anyone_tried_routing_claude_code_cli_to/",
      "author": "Dangerous-Dingo-5169",
      "created_utc": "2025-12-31 05:18:47",
      "score": 4,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Iâ€™m experimenting with running Claude Code CLI against different backends instead of a single API.\n\nSpecifically, Iâ€™m curious whether people have tried:\n\n* using local models for simpler prompts\n* falling back to cloud models for harder requests\n* switching providers automatically when one fails\n\nI hacked together a local proxy to test this idea and it *seems* to reduce API usage for normal dev workflows, but Iâ€™m not sure if Iâ€™m missing obvious downsides.\n\nIf anyone has experience doing something similar (Databricks, Azure, OpenRouter, Ollama, etc.), Iâ€™d love to hear what worked and what didnâ€™t.\n\n(If useful, I can share code â€” didnâ€™t want to lead with a link.)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q04s56/has_anyone_tried_routing_claude_code_cli_to/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nwv8c0r",
          "author": "LittleBlueLaboratory",
          "text": "I just use OpenCode. Comes with the ability to choose provider built in. I use it with my local llama-serverÂ ",
          "score": 1,
          "created_utc": "2025-12-31 05:25:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv95ue",
              "author": "Dangerous-Dingo-5169",
              "text": "Understood opencode is a great product but for folks who want to use claude code with their infra and dont want to miss on features offered by anthropic backend like live websearch, mcp, sub agents etc   \ncan use lynkr (https://github.com/Fast-Editor/Lynkr). It has ACE framework which is very similar to skills that learns based on experience and also has a long term memory as discussed in Titans paper to save on tokens and give accurate answers.",
              "score": 1,
              "created_utc": "2025-12-31 05:32:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwi05b",
          "author": "AI_is_the_rake",
          "text": "Thereâ€™s several threads on this. Iâ€™ve considered trying it but havenâ€™t. Thereâ€™s several models out there that look super cheap but good enough. I would be curious to try it. Itâ€™s easier to just pay for Claude code and codex and stay SOTA.Â ",
          "score": 1,
          "created_utc": "2025-12-31 12:11:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx74we",
          "author": "mtbMo",
          "text": "Itâ€™s not an ide, but Iâ€™m using LiteLLM to route my requests accordingly to their best local GPU option.",
          "score": 1,
          "created_utc": "2025-12-31 14:53:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy9zw9",
          "author": "BidWestern1056",
          "text": "just use npcsh\n\n[https://github.com/npc-worldwide/npcsh](https://github.com/npc-worldwide/npcsh)",
          "score": 1,
          "created_utc": "2025-12-31 18:07:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0tmfl",
      "title": "Tool Weaver (open sourced) inspired by Anthropicâ€™s advanced tool use.",
      "subreddit": "ollama",
      "url": "/r/mcp/comments/1q0br8n/tool_weaver_open_sourced_inspired_by_anthropics/",
      "author": "andavan_ivan",
      "created_utc": "2026-01-01 02:30:52",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q0tmfl/tool_weaver_open_sourced_inspired_by_anthropics/",
      "domain": "",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q33op3",
      "title": "Radxa Orion O6 LLM Benchmarks (Ollama, Debian 12 Headless, 64GB RAM) â€“ 30B on ARM is actually usable",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q33op3/radxa_orion_o6_llm_benchmarks_ollama_debian_12/",
      "author": "RasPiBuilder",
      "created_utc": "2026-01-03 19:30:06",
      "score": 3,
      "num_comments": 6,
      "upvote_ratio": 0.72,
      "text": "I spent some time benchmarking the Radxa Orion O6 running Debian 12 + Ollama after sorting out early thermal issues. Sharing results in case theyâ€™re helpful for anyone considering this board for local LLM inference. One important note is that the official Radxa Debian 12 image for the Orion O6 only ships with a desktop environment. For these tests, I removed the desktop and ran the system headless, which helped reduce background load and thermals.\n\n# Hardware / Setup\n\n* Radxa Orion O6\n* 64 GB RAM\n* Powered over USB-C PD\n* Radxa AI Kit case (this significantly improved thermals)\n* Debian 12 (official Radxa image, desktop removed â†’ headless)\n* Ollama (CPU-only)\n* CPU governor: `schedutil` (performed better than forcing `performance`)\n* Adequate cooling and airflow (critical on this board)\n\n# Results (tokens/sec = eval rate)\n\n**Qwen3-Next**\n\n* Eval rate: **2.41 tok/s**\n* Eval tokens: 1203\n* Total time: \\~8m25s\n\n**Nemotron-3-nano**\n\n* Eval rate: **6.04 tok/s**\n* Eval tokens: 836\n* Total time: \\~2m21s\n\n**Qwen3:30B (MoE)**\n\n* Eval rate: **6.42 tok/s**\n* Eval tokens: 709\n* Total time: \\~1m52s\n\n**Qwen3:30B-Instruct (MoE)**\n\n* Eval rate: 6.81 **tok/s**\n* Eval tokens: 148\n* Total time: \\~23s\n\n**Qwen3:14B (dense)**\n\n* Eval rate: **3.66 tok/s**\n* Eval tokens: 328\n* Total time: \\~1m33s\n\n**GPT-OSS**\n\n* Eval rate: **3.01 tok/s**\n* Eval tokens: 543\n* Total time: \\~3m09s\n\n**Llama3:8B**\n\n* Eval rate: **6.42 tok/s**\n* Eval tokens: 273\n* Total time: \\~45s\n\n**DeepSeek-R1:1.5B**\n\n* Eval rate: **19.57 tok/s**\n* Eval tokens: 44\n* Total time: \\~2.7s\n\n**Granite 3.1 MoE (3B)**\n\n* Eval rate: **17.87 tok/s**\n* Eval tokens: 66\n* Total time: \\~4.8s\n\n# Observations\n\n* 30B-class models *do* run on the Orion O6 â€” slow, but usable for experimentation.\n* Larger models (8Bâ€“30B) cluster around \\~3â€“6 tok/s, suggesting a memory-bandwidth / ARM CPU ceiling, not a power or clock issue.\n* Smaller models (Granite 3B, DeepSeek 1.5B) feel very responsive.\n* schedutil governor consistently outperformed performance in testing.\n* Thermals matter a lot: moving to the Radxa AI Kit case and running headless eliminated thermal shutdowns seen earlier.\n* USB-C PD has been stable so far with adequate cooling.\n\n# TL;DR\n\nThe Orion O6 isnâ€™t a GPU replacement, but as a compact ARM server with 64 GB RAM that can genuinely run 30B MoE models, it exceeded my expectations. Running Debian headless and using the AI Kit case makes a real difference. With realistic performance expectations, itâ€™s a solid platform for local LLM experimentation.\n\nHappy to answer questions or run additional tests if people are interested.\n\n\n\n# Update\n\nI was able to slightly increase perform by making a few more tweaks.  \n1.  Changed CPU Governor to ondemand  \n2.  Pruned Unnecessary background services (isp\\_app, avahi-daemon, cups, fwupd, upower, etc.)  \n3.  OLLAMA_SCHED_SPREAD=false\n\nFor Qwen3:30b-instruct, this boosted performance from \\~6.8t/s to \\~7.4t/s",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q33op3/radxa_orion_o6_llm_benchmarks_ollama_debian_12/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nxivjm0",
          "author": "urostor",
          "text": "How did you perform these tests? How many ollama threads? If you used any of the slower cores, you probably got super low results.\nYou need to eliminate the small cores. Here's some inspiration how: https://gist.github.com/lukaszsobala/9d985c4cd294dcaecd68328fcc068935",
          "score": 1,
          "created_utc": "2026-01-03 22:55:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxj6f5q",
              "author": "RasPiBuilder",
              "text": "I have ollama running bare-metal. I setup a simple python script that will incrementally run each model, set verbose, pass prompt, and log the results. For initial testing I'm just using the prompt \"Tell me about yourself.\", which tends to generate a decent amount of tokens (I plan to test heavier prompts in the future.\n\nAs for the threads/ cores, I'm currently running it unrestricted. I did a couple of quick tests on restricting the cores via taskset, but didn't see any noticeable improvement (some of the small models ran a bit faster but performance decreased on the larger models). Still running through some other stuff to see what else I can tweak to further increase performance.",
              "score": 1,
              "created_utc": "2026-01-03 23:52:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxj6zzq",
                  "author": "urostor",
                  "text": "As far as I experimented, taskset doesn't work on Ollama as it always reads `/proc/cpuinfo` directly, executing more threads than the available CPUs, tanking performance. Unless something changed since then.",
                  "score": 1,
                  "created_utc": "2026-01-03 23:55:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pyn9kw",
      "title": "So hi all, i am currently playing with all this self hosted LLM (SLM in my case with my hardware limitations) im just using a Proxmox enviroment with Ollama installed direcly on a Ubuntu server container and on top of it Open WebUI to get the nice dashboard and to be able to create user accounts.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1pyn9kw/so_hi_all_i_am_currently_playing_with_all_this/",
      "author": "Franceesios",
      "created_utc": "2025-12-29 14:11:58",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 0.64,
      "text": "So far im using just these models \n\nhttps://preview.redd.it/w18f48hnh5ag1.png?width=1306&format=png&auto=webp&s=c46e7759d8c3bb13d8238a4f1503ad3dd7620957\n\n\\- Llama3.2:1.2b\n\n\\- Llama3.2:latest 3.2b\n\n\\- Llama3.2:**8b**\n\n**- Ministral-3:8b** \n\n  \nThey are running ok at the time, the 8B ones would take atleast 2 minutes to give some proper answer, and ive also put this template for the models to remember with each answer they give out ;\n\n\n\n \\### Task:\n\nRespond to the user query using the provided context, incorporating inline citations in the format \\[id\\] \\*\\*only when the <source> tag includes an explicit id attribute\\*\\* (e.g., <source id=\"1\">). Always include a confidence rating for your answer.\n\n\n\n\\### Guidelines:\n\n\\- Only provide answers you are confident in. Do not guess or invent information.\n\n\\- If unsure or lacking sufficient information, respond with \"I donâ€™t know\" or \"Iâ€™m not sure.\"\n\n\\- Include a confidence rating from 1 to 5:\n\n  1 = very uncertain\n\n  2 = somewhat uncertain\n\n  3 = moderately confident\n\n  4 = confident\n\n  5 = very confident\n\n\\- Respond in the same language as the user's query.\n\n\\- If the context is unreadable or low-quality, inform the user and provide the best possible answer.\n\n\\- If the answer isnâ€™t present in the context but you possess the knowledge, explain this and provide the answer.\n\n\\- Include inline citations \\[id\\] only when <source> has an id attribute.\n\n\\- Do not use XML tags in your response.\n\n\\- Ensure citations are concise and directly relevant.\n\n\\- Do NOT use Web Search or external sources.\n\n\\- If the context does not contain the answer, reply: â€˜I donâ€™t knowâ€™ and Confidence 1â€“2.\n\n\n\n\\### Example Output:\n\nAnswer: \\[Your answer here\\]\n\nConfidence: \\[1-5\\]\n\n\n\n\\### Context:\n\n<context>\n\n{{CONTEXT}}\n\n</context>\n\nhttps://preview.redd.it/tbnk6bekh5ag1.png?width=1647&format=png&auto=webp&s=38c75ac55e6951ca80a0f364fdcf8629379c69aa\n\n  \nWith so far works great, my primarly test right about now is the RAG method that Open WebUI offers, ive currently uploaded some invoices from this whole year worth of data as .MD files.\n\nhttps://preview.redd.it/nchwh0kyh5ag1.png?width=887&format=png&auto=webp&s=a43d510aa7032f361dbfc7849903d1d87ba221a5\n\nAnd asks the model (selecting the folder with the data first with # command/option) and i would get some good answers and some times some not so good answers but witj the confidence level accurate.\n\nhttps://preview.redd.it/vqzwaupsh5ag1.png?width=559&format=png&auto=webp&s=2737560e7562ccb31845f578e8ac89dbd42d33bb\n\n\n\nNow my question is, if some tech company wants to implement these type of LLM (SML) into there on premise network for like finance department to use, is this a good start? How does some enterprise do it at the moment? Like sites like [llm.co](http://llm.co) \n\nhttps://preview.redd.it/9knu91phh5ag1.png?width=1438&format=png&auto=webp&s=a790870d44637e073b7807f3120306fdee8db623\n\n\n\nSo far i can see real use case for this RAG method with some more powerfull hardware ofcourse, but let me know your real enterprise use case of a on-prem LLM RAG method. \n\nThanks all! ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pyn9kw/so_hi_all_i_am_currently_playing_with_all_this/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nwmpz9d",
          "author": "OnyxProyectoUno",
          "text": "Your setup looks solid for testing. The confidence scoring in your prompt template is smart, especially for enterprise use where wrong answers are expensive.\n\nFor enterprise deployment, most companies I've seen go bigger on hardware and add more guardrails. They'll run 70B models on multi-GPU setups rather than 8B, and they obsess over data governance. Who can access what documents, audit trails, that kind of thing.\n\nThe inconsistent RAG answers you're getting are probably from chunking issues rather than the model itself. When you converted those invoices to markdown, did you check what the actual chunks look like after processing? Sometimes invoice tables get mangled during conversion and the model gets garbage input. Even with your confidence scoring, it can't fix upstream data problems.\n\nEnterprise teams usually spend more time on the document processing pipeline than the model selection. They'll have dedicated teams just for parsing financial documents, making sure entities like amounts and dates are extracted correctly.\n\nWhat kind of inconsistencies are you seeing in the RAG responses? Are they factual errors or more like incomplete information?",
          "score": 2,
          "created_utc": "2025-12-29 23:02:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqkkid",
              "author": "Franceesios",
              "text": "Hi, many thanks for your amazing information! the inconsistencies im getting are now feels more like  incomplete information its like you just said i need toÂ check what the actual chunks look like after processing. Im attaching a screenshot of the example bellow, its reading the sources of the knowledge base without any issue but still its like the model cant read whats on the md files? But other times it can read it. \n\nhttps://preview.redd.it/yvldoey4tcag1.png?width=1221&format=png&auto=webp&s=231c1ec5e3a099f06a492e6c304aef28bc9dfa21",
              "score": 1,
              "created_utc": "2025-12-30 14:48:58",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwqkvmf",
              "author": "Franceesios",
              "text": "https://preview.redd.it/gut3640utcag1.png?width=1132&format=png&auto=webp&s=888fd712cdeca437429e34eeefa777fd4594a3ed\n\nHere is an example that it can read the information exacly as i wanted it to do. Thus maybe its my retrieval settings that needs to be changed in Open WebUI admin panel settings?",
              "score": 1,
              "created_utc": "2025-12-30 14:50:37",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwqllik",
              "author": "Franceesios",
              "text": "Also i there another RAG system that you dont have to manually select the knowledge base? As with Open WebUI you have to select the knowledge base by pressing # before every chat. I was checking out docling serve, but docling serve will need its own additional database setup but with that said RAG system will it be more intregared to Ollama LLM (SML) model?",
              "score": 1,
              "created_utc": "2025-12-30 14:54:25",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwqlz6k",
              "author": "Franceesios",
              "text": "Also for a bit more information here is my general setup of the retrieval settings, maybe some fine tuning can help?\n\nhttps://preview.redd.it/8l1gws63vcag1.png?width=1647&format=png&auto=webp&s=3e5847e3b0e10355c8b10def14d94fa2030859c8",
              "score": 1,
              "created_utc": "2025-12-30 14:56:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1py6f8c",
      "title": "Cooperative team problems",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1py6f8c/cooperative_team_problems/",
      "author": "Nearby_You_313",
      "created_utc": "2025-12-28 23:48:33",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I've been trying to create a virtual business team to help me with tasks. The idea was to have a manager who interacts hub-and-spoke style with all other agents. I provide only high-level direction and it develops a plan, assigns and delegates tasks, saves output, and gets back to me. \n\nI was able to get this working in self-developed code and Microsoft Agent Framework, both accessing Ollama, but the results are... interesting. The manager would delegate a task to the researcher, who would search and provide feedback, but then the manager would completely hallucinate actually saving the data. (It seems to me to be a model limitation issue, mostly, but I'm developing a new testing method that takes tool usage into account and will test all my local models again to see if I get better results with a different one.)\n\nI'd like to use Claude Code or systems due to their better models, but they're all severely limited (Claude can't create agents on-the-fly, etc.) or very costly. \n\nHas anyone actually accomplished something like this locally that actually works semi-decently? How do your agents interact? How did you fix tool usage? What models? Etc.\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1py6f8c/cooperative_team_problems/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q1ictw",
      "title": "igpu + dgpu for reducing cpu load",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1q1ictw/igpu_dgpu_for_reducing_cpu_load/",
      "author": "sultan_papagani",
      "created_utc": "2026-01-01 23:19:53",
      "score": 2,
      "num_comments": 0,
      "upvote_ratio": 0.67,
      "text": "i wanted to share my findings on using iGPU + dGPU to reduce cpu load during inference.\n\nPrompt: write a booking website for hotels\nModel: gpt-oss:latest\nigpu: intel arrow lake integrated graphics\ndgpu: rtx5060\nsystem ram: 32gb\n\n----------\n\nCPU offloading + dGPU (cuda)\n\nSize:     14GB  \nProcessor: 57% CPU / 43% GPU  \nContext:  32K\nAll 8 CPU cores fully utilized (100% per core)\nTotal CPU load: ~33â€“47%\nFans ramp up and system is loud\n\nTotal duration: 2m 42s\nPrompt eval: 73 tokens @ ~68 tok/s\nGeneration: 3756 tokens @ ~25.7 tok/s\n\n-----------\n\niGPU + dGPU only (vulkan)\n\nSize:     14GB  \nProcessor: 100% GPU  \nContext:  32K\nCPU usage drops to ~1â€“6%\nSystem stays quiet\n\nTotal duration: 10m 30s\nPrompt eval: 73 tokens @ ~46.8 tok/s\nGeneration: 4213 tokens @ ~6.7 tok/s\n\n---------\n\nRunning fully on iGPU + dGPU dramatically reduces CPU load and noise, but generation speed drops significantly. For long or non-interactive runs, this tradeoff can be worth it.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1q1ictw/igpu_dgpu_for_reducing_cpu_load/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    }
  ]
}