{
  "metadata": {
    "last_updated": "2026-01-24 16:49:57",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 20,
    "total_comments": 113,
    "file_size_bytes": 103039
  },
  "items": [
    {
      "id": "1qh5z0j",
      "title": "I built a voice-first AI mirror that runs fully on Ollama.",
      "subreddit": "ollama",
      "url": "https://v.redd.it/bjeyts2qibeg1",
      "author": "DirectorChance4012",
      "created_utc": "2026-01-19 14:43:25",
      "score": 362,
      "num_comments": 47,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qh5z0j/i_built_a_voicefirst_ai_mirror_that_runs_fully_on/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0hf0l3",
          "author": "ThomasNowProductions",
          "text": "How far we have come ;-) It is real nice tho",
          "score": 25,
          "created_utc": "2026-01-19 14:47:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hfop5",
              "author": "DirectorChance4012",
              "text": "Thanks!",
              "score": 5,
              "created_utc": "2026-01-19 14:51:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0hr4is",
                  "author": "mlt-",
                  "text": "Did you aim for that Pal thing look from Netflix cartoon The Mitchells vs. the Machines?",
                  "score": 7,
                  "created_utc": "2026-01-19 15:46:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0lqmr5",
                  "author": "redditissocoolyoyo",
                  "text": "Start a Kickstarter! Make it a real product. People will buy it!",
                  "score": 4,
                  "created_utc": "2026-01-20 03:31:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0i5cwe",
          "author": "croninsiglos",
          "text": "Need one with a vision model that both gives compliments and body shames me when appropriate. Must support tool use and connect with my scale.\n\nYou know, it also needs a more humanoid avatar that can  answer a simple question about who is the fairest of them all. The answer should be obvious, but I just want to be sure.",
          "score": 10,
          "created_utc": "2026-01-19 16:49:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tfvmu",
              "author": "Fuzzy_Independent241",
              "text": "WARNING: might have serious consequences if left to control prescription magical drugs for others in your kingdom!",
              "score": 3,
              "created_utc": "2026-01-21 07:28:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0j0wgi",
              "author": "ScoreUnique",
              "text": "SmolVlm",
              "score": 2,
              "created_utc": "2026-01-19 19:10:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0hgxfn",
          "author": "Money-Frame7664",
          "text": "That is really fun ! Congratulations ðŸ‘ \nDo you have connectors with any home automation system? (Or MCP capacity?)",
          "score": 4,
          "created_utc": "2026-01-19 14:57:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ho7ss",
          "author": "Travelosaur",
          "text": "Mirror mirror on the wall... You made it a reality bro. Nice job!",
          "score": 4,
          "created_utc": "2026-01-19 15:32:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jhgq8",
          "author": "ServeAlone7622",
          "text": "This is the future",
          "score": 3,
          "created_utc": "2026-01-19 20:27:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jtr8r",
          "author": "klei10",
          "text": "Where did you get the mirror ?",
          "score": 3,
          "created_utc": "2026-01-19 21:25:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0l4fx0",
              "author": "DirectorChance4012",
              "text": "I covered the details here: [https://noted.lol/mirrormate/](https://noted.lol/mirrormate/).  \nThe mirror was purchased from [https://www.e-kagami.com/](https://www.e-kagami.com/).",
              "score": 1,
              "created_utc": "2026-01-20 01:29:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0lamvu",
          "author": "KernelFlux",
          "text": "Thatâ€™s very cool and I want to build one!",
          "score": 3,
          "created_utc": "2026-01-20 02:03:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mjfif",
          "author": "Easy_Cable6224",
          "text": "so cool, this is something I wouldn't imagine back when I was a kid, we are IN the future now!",
          "score": 3,
          "created_utc": "2026-01-20 06:49:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hki0u",
          "author": "kiyyik",
          "text": "Thank you for sharing this! Looking forward to doing something similar.",
          "score": 2,
          "created_utc": "2026-01-19 15:15:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hs0z2",
          "author": "stoopwafflestomper",
          "text": "Very cool!",
          "score": 2,
          "created_utc": "2026-01-19 15:50:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hss7j",
              "author": "DirectorChance4012",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-19 15:53:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ht3w8",
          "author": "kkiran",
          "text": "Awesome, kudos! Mac Studio hosted AI mirror sounds cool. I wanted to justify my home lab spending and this fits part of the bill. Thank you!",
          "score": 2,
          "created_utc": "2026-01-19 15:54:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0htbp3",
              "author": "DirectorChance4012",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-19 15:55:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0hyitp",
          "author": "irodov4030",
          "text": "super cool!",
          "score": 2,
          "created_utc": "2026-01-19 16:19:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hzg6o",
              "author": "DirectorChance4012",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-19 16:23:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0i36ve",
          "author": "Reasonable_Brief578",
          "text": "Beautiful",
          "score": 2,
          "created_utc": "2026-01-19 16:39:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jk7b9",
          "author": "roshan231",
          "text": "Thatâ€™s cool",
          "score": 2,
          "created_utc": "2026-01-19 20:40:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ju6s9",
          "author": "After_Construction72",
          "text": "Was only looking into similar over the weekend",
          "score": 2,
          "created_utc": "2026-01-19 21:27:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jyutj",
          "author": "No_Thing8294",
          "text": "Thanks for sharing! Very nice project! I love the face of the agent!",
          "score": 2,
          "created_utc": "2026-01-19 21:51:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kjwd0",
          "author": "thunder-wear",
          "text": "This is really awesome. I love it!",
          "score": 2,
          "created_utc": "2026-01-19 23:38:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ksxo6",
          "author": "Relevant_Middle_4779",
          "text": "Wow!!!",
          "score": 2,
          "created_utc": "2026-01-20 00:26:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rlzzf",
          "author": "zaschmaen",
          "text": "Damn nice! Thats what i wanna make but with an comic character or smth like thisðŸ‘Œ",
          "score": 2,
          "created_utc": "2026-01-21 00:18:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0syczv",
          "author": "wittlewayne",
          "text": "shit.... beat me to it !",
          "score": 2,
          "created_utc": "2026-01-21 05:06:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0t9gdr",
          "author": "beast_modus",
          "text": "Good Jobâ€¦Greatâ€¦",
          "score": 2,
          "created_utc": "2026-01-21 06:32:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tg0t5",
          "author": "Fuzzy_Independent241",
          "text": "That looks cool! Did you embed the hardware or is it calling Ollama over your lan / VPN ?",
          "score": 2,
          "created_utc": "2026-01-21 07:29:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tg6nd",
              "author": "Fuzzy_Independent241",
              "text": "PS - noticed your GH link on my phone. Will check the repo. Thx!",
              "score": 2,
              "created_utc": "2026-01-21 07:31:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0iv6rl",
          "author": "BringOutYaThrowaway",
          "text": "I need to know where you got the glass?",
          "score": 1,
          "created_utc": "2026-01-19 18:45:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0l4h3z",
              "author": "DirectorChance4012",
              "text": "I covered the details here: [https://noted.lol/mirrormate/](https://noted.lol/mirrormate/).  \nThe mirror was purchased from [https://www.e-kagami.com/](https://www.e-kagami.com/).",
              "score": 1,
              "created_utc": "2026-01-20 01:29:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0l6nc9",
          "author": "MeatballStroganoff",
          "text": "How long is it from prompt to response? It seems like you cut the video right before it starts speaking.",
          "score": 1,
          "created_utc": "2026-01-20 01:41:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0l8eh0",
              "author": "DirectorChance4012",
              "text": "I takes roughly 5-7 second.",
              "score": 2,
              "created_utc": "2026-01-20 01:51:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0l92mv",
                  "author": "MeatballStroganoff",
                  "text": " Not bad, thanks!",
                  "score": 1,
                  "created_utc": "2026-01-20 01:55:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0n7ehn",
          "author": "JacketHistorical2321",
          "text": "How does the vision plugin work? I don't see any documentation besides referencing it exists",
          "score": 1,
          "created_utc": "2026-01-20 10:28:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0plcut",
          "author": "Affectionate_Bus_884",
          "text": "![gif](giphy|ZRICsU0zv4tJS)",
          "score": 1,
          "created_utc": "2026-01-20 18:27:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0skjvs",
          "author": "Ordinary_Marketing36",
          "text": "Having a Ktop of 5 means that you will always recover 5 of the best memories?",
          "score": 1,
          "created_utc": "2026-01-21 03:35:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10mpbe",
          "author": "Pleasant_Designer_14",
          "text": "funny,following",
          "score": 1,
          "created_utc": "2026-01-22 09:10:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17m9wl",
          "author": "realaneesani",
          "text": "Mate, what voice package you used?",
          "score": 1,
          "created_utc": "2026-01-23 09:43:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e2fye",
          "author": "Routine-Opening-90",
          "text": "This has been my long term dream for the longest time! Thanks for the guide! Looks very cool! :)",
          "score": 1,
          "created_utc": "2026-01-24 07:39:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nw31u",
          "author": "NickMcGurkThe3rd",
          "text": "the problem with those videos is that they are always cut to cache/hide the fact that it takes the like 10 seconds for that thing to respond and make it seem seemless",
          "score": 1,
          "created_utc": "2026-01-20 13:30:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nwd8p",
              "author": "DirectorChance4012",
              "text": "yeah, it actually takes 5-7second.\nhttps://www.reddit.com/r/ollama/s/8b6TeRsRFx",
              "score": 3,
              "created_utc": "2026-01-20 13:32:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qiug46",
      "title": "Fine-tuned Qwen3 0.6B for Text2SQL using a claude skill. The result tiny model matches a Deepseek 3.1 and runs locally on CPU.",
      "subreddit": "ollama",
      "url": "https://i.redd.it/fy13421qjoeg1.png",
      "author": "party-horse",
      "created_utc": "2026-01-21 10:30:24",
      "score": 177,
      "num_comments": 7,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qiug46/finetuned_qwen3_06b_for_text2sql_using_a_claude/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0up0ey",
          "author": "cirejr",
          "text": "This is great, I've been trying to make this text2sql happen for couple of weeks now using lightweight models. And I have to say without fine tuning them it's really something ðŸ˜…. I tried couple of ways, giving functionGemma bunch of tools. Using some 3b models and giving and creating a Neon mcp client but yeah I guess fine tuning is all that's left.",
          "score": 9,
          "created_utc": "2026-01-21 13:37:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0utcy8",
              "author": "party-horse",
              "text": "Awsome, feel free to use the claude skill to train a model for your specific domain/dialect!",
              "score": 1,
              "created_utc": "2026-01-21 14:00:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0wnup2",
          "author": "jlugao",
          "text": "How did you come up with the datasets for training and evaluating? I am thinking of doing a similar project for evaluating execution plans and coming up with recommendations",
          "score": 3,
          "created_utc": "2026-01-21 19:06:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0x6gmz",
              "author": "party-horse",
              "text": "I chatted with a few LLMs to get example conversations. Fortunately you only need approx 20 to get started so its pretty easy",
              "score": 3,
              "created_utc": "2026-01-21 20:30:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zezv0",
          "author": "Puzzled_Fisherman_94",
          "text": "Thx for the tutorial",
          "score": 1,
          "created_utc": "2026-01-22 03:30:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14ig60",
          "author": "_RemyLeBeau_",
          "text": "\"all local\"\n\nIf this were true, you could just share the skill and not have toÂ distil login",
          "score": 1,
          "created_utc": "2026-01-22 21:46:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1aakp1",
          "author": "Odd-Photojournalist8",
          "text": "Would be cool to do one that could integrate `ctibutler` and a few KEV reputable sources. Then a bigger model ask detailed queries asking small fine tuned model(cheap) to extract correlated set of data. Cyber security basics using AI",
          "score": 1,
          "created_utc": "2026-01-23 18:41:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj3b01",
      "title": "[Open Sourse] I built a tool that forces 5 AIs to debate and cross-check facts before answering you",
      "subreddit": "ollama",
      "url": "https://i.redd.it/ja8et3degqeg1.jpeg",
      "author": "S_Anv",
      "created_utc": "2026-01-21 16:55:27",
      "score": 54,
      "num_comments": 15,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qj3b01/open_sourse_i_built_a_tool_that_forces_5_ais_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0wv1t2",
          "author": "atika",
          "text": "Wasnâ€™t enough to â€œmakeâ€ them debate, you had to â€œforceâ€ them against their wishes?",
          "score": 8,
          "created_utc": "2026-01-21 19:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0wvpic",
              "author": "Nerdinat0r",
              "text": "Not to mention the overhead. How much more RAM and electricity and GPU Time this usesâ€¦",
              "score": 1,
              "created_utc": "2026-01-21 19:41:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0x5eoh",
          "author": "Basic_Young538",
          "text": "This could be really funny...",
          "score": 6,
          "created_utc": "2026-01-21 20:25:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14nk2z",
              "author": "EXPATasap",
              "text": "It can be that and you can get them to be SO DARK and twisted to, think Dark Eldar maxed out lol! Iâ€™ve had a pyqt app I built six months ago that does this with local/external models, I use Jinja templates which gives me all the control Iâ€™d ever want and need with per turn reinforcement system messages, custom to each turn/model. lol. Itâ€™s crazy effective.",
              "score": 1,
              "created_utc": "2026-01-22 22:11:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o11ec2n",
          "author": "butterninja",
          "text": "You gave me an idea. I will waterboard the crap out of 5 AIs to debate and cross-check facts before answering you. Give me a bit of time.",
          "score": 2,
          "created_utc": "2026-01-22 12:55:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0x81b9",
          "author": "Faisal_Biyari",
          "text": "If I use this between the same model, does that mean it is effectively converted into a \"thinking\" model?",
          "score": 1,
          "created_utc": "2026-01-21 20:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0y1t4a",
          "author": "ServeAlone7622",
          "text": "This is the way!\n\nI love that you built an ensemble setup too!\nI could never get this to work for me in a real sense. Accuracy and a lack of ability or possibly desire to critique one another leading into a giant circle jerk of AI handshaking.\n\nHowâ€™s your accuracy? Do they argue back and forth or do they mostly just circle jerk each other?",
          "score": 1,
          "created_utc": "2026-01-21 22:57:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o106gle",
          "author": "upboat_allgoals",
          "text": "multi turn zero shot ensembling? \n\nnice tool. worth running it through some benchmarks.",
          "score": 1,
          "created_utc": "2026-01-22 06:44:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17o1qa",
          "author": "Fabulous-Speech6593",
          "text": "This is good because Chat-GPT is lying its ass off too often, it should be standard to regulate these lying ass Ai confuse models ðŸ˜!",
          "score": 1,
          "created_utc": "2026-01-23 09:59:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17rmoo",
          "author": "usercantollie",
          "text": "You're a genius, OP! This is exactly what the AI world needs right now, forcing multiple models to debate and cross-check facts is the only way to actually maximize productivity without drowning in hallucinations.\n\nMost people are just blindly trusting single-model outputs, but you're actually solving the real problem.",
          "score": 1,
          "created_utc": "2026-01-23 10:31:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17sweo",
          "author": "milli_xoxxy",
          "text": "Honestly, this is brilliant idea! Making five Als argue with each other to find the truth is a game-changer. Also running it locally with Ollama is the real win. This is the future of reliable Al, not just another API wrapper.",
          "score": 1,
          "created_utc": "2026-01-23 10:43:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17xw2s",
          "author": "Eugene_sh",
          "text": "I used \"chorus\" app just for that but they smh abandoned the project it seems",
          "score": 1,
          "created_utc": "2026-01-23 11:25:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15f7m5",
          "author": "NoxinDev",
          "text": "I love it, waste 5 times as much money from the AI speculation companies - way to help burst this bubble, just fantastic.",
          "score": 1,
          "created_utc": "2026-01-23 00:37:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10v8di",
          "author": "390adv",
          "text": "Awesome. Post the results of the Holocaust question",
          "score": 0,
          "created_utc": "2026-01-22 10:30:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1b981e",
          "author": "Low-Coconut5857",
          "text": "â€5 person that debates and cross-checks facts before answering you.â€ Sounds like the weekly meeting I have with my teamâ€¦  except the part about providing an answer.\nI think I skip.",
          "score": 0,
          "created_utc": "2026-01-23 21:23:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj3qkv",
      "title": "Hi folks, Iâ€™ve built an openâ€‘source project that could be useful to some of you",
      "subreddit": "ollama",
      "url": "https://i.redd.it/4plhjok3jqeg1.png",
      "author": "panos_s_",
      "created_utc": "2026-01-21 17:10:42",
      "score": 46,
      "num_comments": 10,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qj3qkv/hi_folks_ive_built_an_opensource_project_that/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0x1sha",
          "author": "jovn1234567890",
          "text": "Very useful thank you",
          "score": 2,
          "created_utc": "2026-01-21 20:09:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0xt8z7",
              "author": "panos_s_",
              "text": "thanks mate!",
              "score": 1,
              "created_utc": "2026-01-21 22:14:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0xgfv7",
          "author": "mofa1",
          "text": "I just want to say that I like the project and have been using it for some time in my Unraid system!",
          "score": 2,
          "created_utc": "2026-01-21 21:15:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0xsofs",
              "author": "panos_s_",
              "text": "thanks a lot :)",
              "score": 1,
              "created_utc": "2026-01-21 22:12:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ybfg1",
          "author": "selfdestroyer",
          "text": "I also have been running this for a month or so and itâ€™s been great to monitor while using OpenwebUI and ConfyUI.",
          "score": 1,
          "created_utc": "2026-01-21 23:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15c5xk",
              "author": "panos_s_",
              "text": "thanks a lot :)",
              "score": 2,
              "created_utc": "2026-01-23 00:21:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o10jxgg",
          "author": "UseHopeful8146",
          "text": "Stumbling blind into the world of GPU compute and am very appreciative",
          "score": 1,
          "created_utc": "2026-01-22 08:44:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15e99z",
          "author": "IllustriousMessage79",
          "text": "I love you",
          "score": 1,
          "created_utc": "2026-01-23 00:32:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15ejne",
              "author": "panos_s_",
              "text": ":)",
              "score": 2,
              "created_utc": "2026-01-23 00:34:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o15yotd",
          "author": "960be6dde311",
          "text": "Nice design. Cool little project",
          "score": 1,
          "created_utc": "2026-01-23 02:26:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkokhv",
      "title": "Ollama Image Generator",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qkokhv/ollama_image_generator/",
      "author": "nickinnov",
      "created_utc": "2026-01-23 11:40:26",
      "score": 42,
      "num_comments": 22,
      "upvote_ratio": 0.96,
      "text": "Hey fellow Ollama fans, I'm delighted that image generation is available so I have written a web app you can run on your own computer (alongside Ollama) to make it easier to generate, save and delete images.\n\nOK it ain't no ComfyUI but makes things tidier and, unlike using the terminal Ollama CLI, images don't clutter up your home folder!  \nRepo at [https://github.com/nicklansley/OllamaImageGenerator](https://github.com/nicklansley/OllamaImageGenerator)\n\n... but all you really need to download is:\n\n* [**server.py**](https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/server.py) \\- small proxying server which you can run without extra packages on your machine as long as you have python 3.9 or higher. No need for a venv as I've just used built-in packages like *http.server* \\- run from terminal as ***python3*** [***server.py***](https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/server.py)\n* [index.html](https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/index.html) \\- does all the grunt work in your web browser on port 8080. Generated images are saved to local storage along with generation settings, and can be deleted individually. Once [server.py](https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/server.py) is up and running, take your web browser to [http://localhost:8080](http://localhost:8080)\n\n[README.md](https://github.com/nicklansley/OllamaImageGenerator/blob/main/README.md) gives more info but quick instructions:\n\n* Download an image generation model in terminal - currently '***ollama pull x/z-image-turbo:bf16***' and '***ollama pull x/flux2-klein:latest***' are supported.\n* Type a prompt, set image width and height, choose a seed and the number of steps, then click 'Generate Image'.\n* During image progression, a 'step N of X' message appears to denote progress.\n* Images are saved to a side panel (actually they are in localStorage so they survive from one session to the next).\n* Save an image onto your machine with right-click 'Save Image As..' or drag if out of the main window and into a folder.\n* Double click a saved image to move it to the main window along with the settings that created it (the image can be recreated as long as seed 0 (Ollama internal random seed) was not used.\n* Single click an image then click 'x' to delete it. The image is removed from the image list saved in localStorage.\n\nWhen more image models become available:\n\n* Download a model with 'image pull image\\_gen\\_model\\_name:tag'\n* Update image list variable IMAGE\\_GEN\\_MODEL\\_LIST with the same model name and tag at the top of index.html\n\n[Screenshot of Ollama Image Generator](https://preview.redd.it/n8kap3a823fg1.png?width=1369&format=png&auto=webp&s=accb2258e20e62c356823ad8b85439aab8424f4a)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qkokhv/ollama_image_generator/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1867zs",
          "author": "vini_stoffel",
          "text": "Only in Mac?",
          "score": 3,
          "created_utc": "2026-01-23 12:26:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o187csu",
              "author": "simplir",
              "text": "Image generation on Ollama is still only Mac at the moment",
              "score": 3,
              "created_utc": "2026-01-23 12:34:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1cna9u",
                  "author": "FlyByPC",
                  "text": "Are there plans to add this to the Windows client anytime soon?",
                  "score": 1,
                  "created_utc": "2026-01-24 01:46:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o18aaup",
          "author": "planetearth80",
          "text": "Doesnâ€™t the Ollama GUI do the same thing?",
          "score": 3,
          "created_utc": "2026-01-23 12:53:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18y01i",
              "author": "nickinnov",
              "text": "Not (yet) - it only understands /api/chat",
              "score": 5,
              "created_utc": "2026-01-23 15:00:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o182vmk",
          "author": "Narrow-Impress-2238",
          "text": "Is that work for windows too already?",
          "score": 1,
          "created_utc": "2026-01-23 12:03:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18yd0f",
              "author": "nickinnov",
              "text": "BY all means give it a go -  [server.py](http://server.py) will work fine as long as you have Python 3.9 or later, but you may need to check the Ollama app for Windows. At its heart [server.py](http://server.py) simply proxies the Ollama API (avoids CORS) plus has its own history saving of images capability.",
              "score": 1,
              "created_utc": "2026-01-23 15:01:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o18atuv",
          "author": "Birdinhandandbush",
          "text": "So why no quantized Z-Image models? that one is like 32gb. At least Klien will work for me",
          "score": 1,
          "created_utc": "2026-01-23 12:56:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18ykmq",
              "author": "nickinnov",
              "text": "Out of my control I'm afraid! Klein is very fast and works well (I am using Apple Metal GPU).",
              "score": 2,
              "created_utc": "2026-01-23 15:03:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o18zacd",
          "author": "nickinnov",
          "text": "UPDATE: Yes so soon used up local storage! Now saves to a history folder that [server.py](http://server.py) creates in its own folder instead. Who knew that web browser local storage can only save about 10MB per web site. Not me it seemed... ðŸ˜‘",
          "score": 1,
          "created_utc": "2026-01-23 15:06:31",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o190nlk",
          "author": "shokuninstudio",
          "text": "Ollama's implementation of the Flux2 model is weaker than the Comfy workflows recommended by Comfy. The output is much rougher and doesn't use Owen's text encoder for text generation.",
          "score": 1,
          "created_utc": "2026-01-23 15:13:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o198s10",
              "author": "nickinnov",
              "text": "I agree - like much of Ollama it is simplified to work cleanly without any complexity. Comfy is absolutely at the opposite spectrum. However, I think it's useful to have a simple tool that can be used with little setup. And I think the images are pretty good for all that lack of ComfyUI fine tuning.",
              "score": 1,
              "created_utc": "2026-01-23 15:50:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o19g4tv",
                  "author": "shokuninstudio",
                  "text": "It should be possible with the command line to select another text encoder and some more options so that it can come closer to Comfy's output.",
                  "score": 1,
                  "created_utc": "2026-01-23 16:23:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o19dd35",
          "author": "Total-Context64",
          "text": "You should take a look at [ALICE](https://github.com/SyntheticAutonomicMind/ALICE), it's pretty advanced and can already do this without Ollama.  I'm curious though, what does Ollama add over just using the diffusion models directly?",
          "score": 1,
          "created_utc": "2026-01-23 16:11:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a8y11",
              "author": "nickinnov",
              "text": "For me it's about Ollama's simplicity (with this and all its models). It abstracts away model loading and inference for local 'offline' use. If anyone wants to be more serious about image generation then really Comfy-UI is really the best way to go.",
              "score": 1,
              "created_utc": "2026-01-23 18:34:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1abbw7",
                  "author": "Total-Context64",
                  "text": "Ahh, fair enough.  With ALICE you do need to set up the backend, ROCm or whatever but that's mostly automated.  I created it to interface with SAM so I can use AI assistants to generate images using old hardware that I just had sitting around.  It works very well, and it integrates with HuggingFace and CivitAI directly so you can download models right from the web interface.  It even has LoRA support.",
                  "score": 1,
                  "created_utc": "2026-01-23 18:44:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o19z2wl",
          "author": "Euphoric-Tank-6791",
          "text": "what OS? it does not seem to run on w11 wsl2 as of last night",
          "score": 1,
          "created_utc": "2026-01-23 17:50:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a9brw",
              "author": "nickinnov",
              "text": "MacOS only at present for image generation - a limitation imposed by Ollama itself. I'm sure they are working on Windows and Linux though. I suspect they are developing Ollama on Macs ðŸ˜",
              "score": 1,
              "created_utc": "2026-01-23 18:35:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1bti1g",
          "author": "CoDMplayer_",
          "text": "Any image+prompt->image capability?",
          "score": 1,
          "created_utc": "2026-01-23 23:02:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e4utp",
              "author": "nickinnov",
              "text": "Yes I'd like that too - plus text encoders + loras! I suspect their curren work will be to make image gen work on Windows and Linux (Mac only right now). But hey they read this r/ollama stuff so hopefully they are creating a backlog that includes image+prompt->image too (please!).",
              "score": 1,
              "created_utc": "2026-01-24 08:01:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qfufg3",
      "title": "Claude Code with Anthropic API compatibility",
      "subreddit": "ollama",
      "url": "https://ollama.com/blog/claude",
      "author": "GhettoFob",
      "created_utc": "2026-01-18 01:01:12",
      "score": 34,
      "num_comments": 10,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qfufg3/claude_code_with_anthropic_api_compatibility/",
      "domain": "ollama.com",
      "is_self": false,
      "comments": [
        {
          "id": "o07x7s2",
          "author": "iron_coffin",
          "text": "The questions are whether anthropic is onboard with it and if they can block it.",
          "score": 3,
          "created_utc": "2026-01-18 02:19:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o088i85",
              "author": "xxdesmus",
              "text": "The seem more worried with Claude models being used by 3rd party clients and less concerned with this inverse scenario. Hopefully that doesnâ€™t change.",
              "score": 5,
              "created_utc": "2026-01-18 03:22:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o09svlh",
              "author": "StardockEngineer",
              "text": "There is nothing they can do to stop it as long as Claude Code can work with an API",
              "score": 2,
              "created_utc": "2026-01-18 10:53:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0b7flp",
                  "author": "iron_coffin",
                  "text": "Nothing, when their control both the closed source app and api? X to doubt. They could at least make it inconvenient",
                  "score": 1,
                  "created_utc": "2026-01-18 16:14:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0atpiq",
              "author": "kiwibonga",
              "text": "They are currently preventing anyone from doing the first time user set up without a paid api key.",
              "score": 2,
              "created_utc": "2026-01-18 15:07:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0b46k7",
                  "author": "iron_coffin",
                  "text": "It sounds like codex is a better bet because the system prompt is shorter, also.",
                  "score": 1,
                  "created_utc": "2026-01-18 15:58:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mvn76",
          "author": "uwemaurer",
          "text": "Did anyone get this to work with a local model?\n\nI tried with gpt-oss:20b but I get the error:\n\n     harmony parser: no reverse mapping found for function name \"harmonyFunctionName=assistant<|channel|>AskUserQuestion\"\n    \n\nFor gwen3-coder, I get \n\n    model does not support thinking, relaxing thinking to nil",
          "score": 1,
          "created_utc": "2026-01-20 08:38:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0poqww",
              "author": "GhettoFob",
              "text": "Did you try bumping up the context length? My Ollama was defaulting to 4k but it started working after I set it to 64k.",
              "score": 2,
              "created_utc": "2026-01-20 18:42:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0qpsph",
                  "author": "uwemaurer",
                  "text": "Thank you! this was the problem. It worked for me with `gpt-oss:20b` and 32k context. \n\n    OLLAMA_CONTEXT_LENGTH=32768 ollama serve",
                  "score": 1,
                  "created_utc": "2026-01-20 21:32:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qjqcqv",
      "title": "Built an open-source, self-hosted AI agent automation platform â€” feedback welcome",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qjqcqv/built_an_opensource_selfhosted_ai_agent/",
      "author": "Feathered-Beast",
      "created_utc": "2026-01-22 09:44:30",
      "score": 32,
      "num_comments": 16,
      "upvote_ratio": 0.96,
      "text": "Hey folks ðŸ‘‹\n\nIâ€™ve been building an open-source, self-hosted AI agent automation platform that runs locally and keeps all data under your control. Itâ€™s focused on agent workflows, scheduling, execution logs, and document chat (RAG) without relying on hosted SaaS tools.\n\nI recently put together a small website with docs and a project overview.\n\nLinks to the website and GitHub are in the comments.\n\nWould really appreciate feedback from people building or experimenting with open-source AI systems ðŸ™Œ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qjqcqv/built_an_opensource_selfhosted_ai_agent/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o10q88s",
          "author": "Feathered-Beast",
          "text": "Github:- https://github.com/vmDeshpande/ai-agent-automation\n\nWebsite:- https://vmdeshpande.github.io/ai-automation-platform-website/",
          "score": 9,
          "created_utc": "2026-01-22 09:44:43",
          "is_submitter": true,
          "replies": [
            {
              "id": "o11tpn1",
              "author": "cirejr",
              "text": "Curious to know, did you build the website and docs with Gemini-3 ?",
              "score": 2,
              "created_utc": "2026-01-22 14:20:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o11uotu",
                  "author": "Feathered-Beast",
                  "text": "Nope. I built the website myself and wrote the docs manually.\nI did my own research, took feedback from other developers, and documented each feature step by step.\nThis is my first open-source project, so I wanted to do it properly.",
                  "score": 2,
                  "created_utc": "2026-01-22 14:25:17",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o10uvij",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 2,
          "created_utc": "2026-01-22 10:27:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10wmtx",
              "author": "Feathered-Beast",
              "text": "Yes i do have screenshot\n\nhttps://preview.redd.it/ds7l369wqveg1.jpeg?width=831&format=pjpg&auto=webp&s=19ed72d8ced71008629fab414d5a61d9fcdf859f",
              "score": 2,
              "created_utc": "2026-01-22 10:43:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1112bx",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-01-22 11:20:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o127gra",
          "author": "JakeMascaOfficial",
          "text": "This looks amazing",
          "score": 1,
          "created_utc": "2026-01-22 15:28:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o129bd8",
              "author": "Feathered-Beast",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-01-22 15:37:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o14hprs",
          "author": "Known-Maintenance-83",
          "text": "Will test it tomorrow can we contribute?",
          "score": 1,
          "created_utc": "2026-01-22 21:42:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16cqra",
              "author": "Feathered-Beast",
              "text": "Yes, contributions are welcome!",
              "score": 1,
              "created_utc": "2026-01-23 03:45:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o15a6wc",
          "author": "looktwise",
          "text": "it builts a RAG locally?? would it be possible to switch RAGs if I want to separate distinguish content? ",
          "score": 1,
          "created_utc": "2026-01-23 00:11:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16d10g",
              "author": "Feathered-Beast",
              "text": "Yes, the RAG runs fully locally.\nRight now it uses a single vector store, but switching between multiple RAGs (or separate knowledge bases per domain) is definitely possible and planned.\n\nThe architecture already supports isolating content; itâ€™s mostly a config / routing layer on top.",
              "score": 2,
              "created_utc": "2026-01-23 03:47:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o17dox7",
                  "author": "looktwise",
                  "text": "Yeah, separate knowledge domains could be a term for that. I would be very interested if the chosen option of a 'RAG-file' would allow to chat with content 1 in RAG-file 1 and then or in a parallel session would allow a separated chat in RAG-file 2. (think in terms of 2 different schools of thought in philosophy for example, RAG-files filled by several book-PDFs, OCR ready).",
                  "score": 1,
                  "created_utc": "2026-01-23 08:22:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ai5o7",
          "author": "dephraiiim",
          "text": "That's awesome you're building this! For scheduling and coordinating agent workflows, you might want to check out [weekday.so](http://weekday.so) ;  it's an open-source calendar alternative with AI capabilities that keeps everything self-hosted and under your control. Could be a solid complement to your platform.",
          "score": 1,
          "created_utc": "2026-01-23 19:15:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkrpnx",
      "title": "I gave my local LLM pipeline  a brain - now it thinks before it speaks",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qkrpnx/i_gave_my_local_llm_pipeline_a_brain_now_it/",
      "author": "danny_094",
      "created_utc": "2026-01-23 14:06:41",
      "score": 31,
      "num_comments": 4,
      "upvote_ratio": 0.84,
      "text": "Jarvis/TRION has received a major update after weeks of implementation. Jarvis (soon to be TRION) has now been provided with a self-developed SEQUENTIAL THINKING MCP.\n\nI would love to explain everything it can do in this Reddit post. But I don't have the space, and neither do you have the patience. u/frank_brsrk Provided a self-developed CIM framework That's hard twisted with Sequential Thinking. So Claude help for the answer:\n\nðŸ§  Gave my local Ollama setup \"extended thinking\" - like Claude, but 100% local\n\nTL;DR: Built a Sequential Thinking system that lets DeepSeek-R1\n\n\"think out loud\" step-by-step before answering. All local, all Ollama.\n\nWhat it does:\n\n\\- Complex questions â†’ AI breaks them into steps\n\n\\- You SEE the reasoning live (not just the answer)\n\n\\- Reduces hallucinations significantly\n\nThe cool part: The AI decides WHEN to use deep thinking.\n\nSimple questions â†’ instant answer.\n\nComplex questions â†’ step-by-step reasoning first.\n\nBuilt with: Ollama + DeepSeek-R1 + custom MCP servers\n\nShoutout to u/frank_brsrk for the CIM framework that makes\n\nthe reasoning actually make sense.\n\nGitHub: [https://github.com/danny094/Jarvis/tree/main](https://github.com/danny094/Jarvis/tree/main)\n\nHappy to answer questions! This took weeks to build ðŸ˜…\n\nOther known issues:\n\n\\- excessively long texts, skipping the control layer - Solution in progress\n\n\\- The side panel is still being edited and will be integrated as a canvas with MCP support.\n\n  \n\n\nhttps://reddit.com/link/1qkrpnx/video/zb6z5muax3fg1/player\n\nhttps://preview.redd.it/el6uhfy6q4fg1.png?width=1147&format=png&auto=webp&s=a16a9525fc50ba59b710f6932cdb3626c2562074\n\nhttps://preview.redd.it/j1ol6fy6q4fg1.png?width=863&format=png&auto=webp&s=f7726aee3e5079419dc665959fc0b779b6d37571\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qkrpnx/i_gave_my_local_llm_pipeline_a_brain_now_it/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o18q7ov",
          "author": "frank_brsrk",
          "text": "Here above is the architecture of the causal intelligence module that decouples thought and augments reasoning on demand. Thanks u/danny094 for the trust. \" Everyday is the day that the project should be finished\" :D\n\nhttps://preview.redd.it/cr7y0qimy3fg1.png?width=2800&format=png&auto=webp&s=3fa05b50978d7f2ea14c2ae7854c682db888331d\n\ncompliments and from the bottom of my heart's void all the best!!! :",
          "score": 4,
          "created_utc": "2026-01-23 14:20:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eoboz",
              "author": "Batinium",
              "text": "Pic Quality too low can't read. On what tool did you prepare them?",
              "score": 1,
              "created_utc": "2026-01-24 10:59:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1eurxj",
                  "author": "frank_brsrk",
                  "text": "On draw.io, I will pubblish soon the open source with polished version for n8n template. Otherwise send dm ur email and i will send u the diagram in higher quality",
                  "score": 1,
                  "created_utc": "2026-01-24 11:56:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1evb3j",
                  "author": "Awkward--Panda",
                  "text": "I guess it's readable here: https://www.reddit.com/r/LocalLLM/s/Hf6UO4Laeo\n\n(via Google picture search. I hope it reflects the actual content)",
                  "score": 1,
                  "created_utc": "2026-01-24 12:00:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qiv7v8",
      "title": "I built a CLI tool using Ollama (nomic-embed-text) to replace grep with Semantic Code Search",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qiv7v8/i_built_a_cli_tool_using_ollama_nomicembedtext_to/",
      "author": "Technical_Meeting_81",
      "created_utc": "2026-01-21 11:16:14",
      "score": 24,
      "num_comments": 2,
      "upvote_ratio": 0.94,
      "text": "Hi r/ollama,\n\nI've been working on an open-source tool called **GrepAI**, and I wanted to share it here because it relies heavily on **Ollama** to function.\n\n**What is it?** GrepAI is a CLI tool (written in Go) designed to help AI agents (like Claude Code, Cursor, or local agents) understand your codebase better.\n\nInstead of using standard regex `grep` to find codeâ€”which often misses the contextâ€”GrepAI uses **Ollama** to generate local embeddings of your code. This allows you to perform **semantic searches** directly from the terminal.\n\n**The Stack:**\n\n* **Core:** Written in Go.\n* **Embeddings:** Connects to your local Ollama instance (defaults to `nomic-embed-text`).\n* **Vector Store:** In-memory / Local (fast and private).\n\n**Why use Ollama for this?** I wanted a solution that respects privacy and doesn't cost a fortune in API credits just to index a repo. By using Ollama locally, GrepAI builds an index of your project (respecting `.gitignore`) without your code leaving your machine.\n\n**Real-world Impact (Benchmark)** I tested this setup by using GrepAI as a filter for Claude Code (instead of the default grep). The idea was to let Ollama decide what files were relevant before sending them to the cloud. The results were huge:\n\n* **-97% Input Tokens** sent to the LLM (because Ollama filtered the noise).\n* **-27.5% Cost reduction** on the task.\n\nEven if you don't use Claude, this demonstrates how effective local embeddings (via Ollama) are at retrieving the right context for RAG applications.\n\nðŸ‘‰ **Benchmark details:**[https://yoanbernabeu.github.io/grepai/blog/benchmark-grepai-vs-grep-claude-code/](https://yoanbernabeu.github.io/grepai/blog/benchmark-grepai-vs-grep-claude-code/)\n\n**Links:**\n\n* ðŸ“¦ **GitHub:**[https://github.com/yoanbernabeu/grepai](https://github.com/yoanbernabeu/grepai)\n* ðŸ“š **Docs:**[https://yoanbernabeu.github.io/grepai/](https://yoanbernabeu.github.io/grepai/)\n\nI'd love to know what other embedding models you guys are running with Ollama. Currently, `nomic-embed-text` gives me the best results for code, but I'm open to suggestions!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qiv7v8/i_built_a_cli_tool_using_ollama_nomicembedtext_to/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0vbgvm",
          "author": "Ok-District-1756",
          "text": "I'm going to try Qwen3-Embedding-4B-GGUF:Q5\\_K\\_M. Honestly, I have no idea how it will perform in real-world conditions, but I'll test it for a week and report back if anyone is interested",
          "score": 5,
          "created_utc": "2026-01-21 15:30:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ybqej",
          "author": "yesbee-yesbee",
          "text": "Will it work for opencode?Â ",
          "score": 1,
          "created_utc": "2026-01-21 23:50:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qh10xr",
      "title": "Demo: On-device browser agent (Qwen) running locally in Chrome",
      "subreddit": "ollama",
      "url": "https://v.redd.it/ljp6zwzfcaeg1",
      "author": "thecoder12322",
      "created_utc": "2026-01-19 10:48:47",
      "score": 24,
      "num_comments": 3,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qh10xr/demo_ondevice_browser_agent_qwen_running_locally/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0k28rw",
          "author": "TigerOk6003",
          "text": "Would be very curious to see how data centers will be rendered useless if small models get better and hardware for edge inference gets better too",
          "score": 3,
          "created_utc": "2026-01-19 22:07:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0j7059",
          "author": "New_Inflation_6927",
          "text": "Next challenge for the team could be trying out VLM within it?",
          "score": 1,
          "created_utc": "2026-01-19 19:38:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0n4kha",
              "author": "thecoder12322",
              "text": "We tried and weâ€™re fixing a few bugs for that, itâ€™ll make it even more accurate! Open for any contributions, please check our runanywhere-sdks as well here: https://github.com/RunanywhereAI/runanywhere-sdks\n\nWeâ€™ll be adding web-gpu and vlm support as well which",
              "score": 1,
              "created_utc": "2026-01-20 10:02:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qgxulm",
      "title": "Would Anthropic Block Ollama?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qgxulm/would_anthropic_block_ollama/",
      "author": "Lopsided_Dot_4557",
      "created_utc": "2026-01-19 07:36:07",
      "score": 23,
      "num_comments": 17,
      "upvote_ratio": 0.85,
      "text": "Few hours ago, Ollama announced following:\n\nOllama now has Anthropic API compatibility. This enables tools like Claude Code to be used with open-source models.\n\nOllama Blog:Â [Claude Code with Anthropic API compatibility Â· Ollama Blog](https://ollama.com/blog/claude)\n\nHands-on Guide:Â [https://youtu.be/Pbsn-6JEE2s?si=7pdAv5LU9GiBx7aN](https://youtu.be/Pbsn-6JEE2s?si=7pdAv5LU9GiBx7aN)\n\nFor now it's working but for how long?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qgxulm/would_anthropic_block_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0fwcc2",
          "author": "Kholtien",
          "text": "I donâ€™t think they could. You can use Claude code (the application, not the service Claude) with just about any LLM these days. Itâ€™s all local on your computer.",
          "score": 14,
          "created_utc": "2026-01-19 07:49:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0i42ty",
              "author": "sinan_online",
              "text": "Hey, so I got a question to ask. If you are doing this, what are you doing, exactly, and whatâ€™s the VRAM? \n\nMy understanding is that the Claude endpoint does quite a bit of orchestration, not just coding, when used under GitHub CoPilot. Also the model they are using seems to require more VRAM than I have locally (12GB) to respond in a reasonable amount of time.",
              "score": 1,
              "created_utc": "2026-01-19 16:43:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0ktgnp",
              "author": "Big-Masterpiece-9581",
              "text": "It works but only with hacks and youâ€™re at Claudeâ€™s mercy anytime they want to throw a curveball and break everything else. I would just use opencode.",
              "score": 1,
              "created_utc": "2026-01-20 00:29:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0l87yb",
                  "author": "StardockEngineer",
                  "text": "They're not hacks.  It's just pointing the base url to another server.  It's a documented feature in Claude Code.",
                  "score": 4,
                  "created_utc": "2026-01-20 01:50:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0jhwpr",
          "author": "ShadoWolf",
          "text": "The Claude Code situation isnâ€™t about Anthropicâ€™s standard API.  \nWhat was happening is that people were signing up for Claude Code, then using tools like OpenCode to redirect it and reuse the Claude Code credential.  \nThat credential wasnâ€™t a normal Anthropic API key, it was tied to the Claude Code plan itself, which is priced as a product SKU, not as metered API access. Anthropic will happy take your money for API usage through their normal API plans. They just donâ€™t want the Claude Code plan being used as a cheap substitute for the standard API pricing",
          "score": 3,
          "created_utc": "2026-01-19 20:29:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g5sld",
          "author": "gabrielxdesign",
          "text": "Why would they block it? I've been using Open WebUI for years, which uses Ollama for Open Source LLM, and I use it together with paid APIs. I don't see the problem with using a platform that can handle local and remote.",
          "score": 5,
          "created_utc": "2026-01-19 09:16:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0gsxig",
          "author": "UnbeliebteMeinung",
          "text": "They blocked the other way because of their costs. Why would they block it when they have no costs?",
          "score": 2,
          "created_utc": "2026-01-19 12:38:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0it8hw",
          "author": "croninsiglos",
          "text": "They are pretty confident in their modelâ€™s ability over open source models and the CEO seems to think youâ€™ll never be hosting the more performant open models locally. \n\nSo does it matter?\n\nIâ€™m sure theyâ€™d love people to standardize on their APO vs openaiâ€™s API.",
          "score": 1,
          "created_utc": "2026-01-19 18:36:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0osgxo",
              "author": "Practical-Plan-2560",
              "text": "Under that logic, they should open source Claude Code. If the model is the magic, why keep tools to use the model so closed?",
              "score": 1,
              "created_utc": "2026-01-20 16:14:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0izx4z",
          "author": "roger_ducky",
          "text": "Itâ€™s the same as the OpenAI API compatibility.\n\nAs long as anthropic doesnâ€™t suddenly make major breaking changes, itâ€™d work.\n\nThis isnâ€™t the same thing anthropic banned recentlyâ€” thatâ€™s piggybacking on the actual Claude using a personâ€™s monthly paid subscription, rather than APIs.\n\nThis is using Claude code the CLI with your local models.",
          "score": 1,
          "created_utc": "2026-01-19 19:06:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ju0js",
          "author": "Clay_Ferguson",
          "text": "Anthropic realized that if OpenCode and/or LangChain OpenWork open source solutions are what people start using for local LLMs, then it gets people out of the Claude Code way of doing things. So there was no disadvantage to letting Claude Code be used for local inference, but there was an advantage, which is to make Claude Code be what people want to use regardless of whether they're using local or cloud-based LLMs.",
          "score": 1,
          "created_utc": "2026-01-19 21:26:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nhtd1",
          "author": "Ok_Hospital_5265",
          "text": "And just as I installed Crushâ€¦ ðŸ¤¦ðŸ»â€â™‚ï¸",
          "score": 1,
          "created_utc": "2026-01-20 11:56:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0p7oio",
              "author": "seangalie",
              "text": "Sometimes it's useful to have Opencode, Crush, Codex, and Claude all co-existing.  Good way to jump between models and such without having to play around with configs everytime.  I use Ollama's cloud models in Opencode, and local models in Crush (since Crush supports a big task/little task setup).",
              "score": 1,
              "created_utc": "2026-01-20 17:25:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0m8y9k",
          "author": "stocky789",
          "text": "Would anyone want to? \nClaude code isn't really anything to write home about",
          "score": 0,
          "created_utc": "2026-01-20 05:26:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qijhth",
      "title": "Weekend Project: An Open-Source Claude Cowork That Can Handle Skills",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qijhth/weekend_project_an_opensource_claude_cowork_that/",
      "author": "Frequent_Cash2598",
      "created_utc": "2026-01-21 00:59:50",
      "score": 19,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "I spent last weekend building something I had been thinking about for a while. Claude Cowork is great, but I wanted an open-source, lightweight version that could run with any model, so I created Open Cowork.\n\nIt's written entirely in Rust, which I had never used before. Starting from scratch meant no heavy dependencies, no Python bloat, and no reliance on existing agent SDKs. Just a tiny, fast binary that works anywhere.\n\nSecurity was a big concern since the agents can execute code. Open Cowork handles this by running tasks inside temporary Docker containers. Everything stays isolated, but you can still experiment freely.\n\nYou can plug in any model you want. OpenAI, Anthropic, or even fully offline LLMs through Ollama are all supported. You keep full control over your API keys and your data.\n\nIt already comes with built-in skills for handling documents like PDFs and Excel files. I was surprised by how useful it became right away.\n\nThe development experience was wild. An AI agent helped me build a secure, open-source version of itself, and I learned Rust along the way. It was one of those projects where everything just clicked together in a weekend.\n\nThe code is live on GitHub: [https://github.com/kuse-ai/kuse\\_cowork](https://github.com/kuse-ai/kuse_cowork) . It's still early, but I'd love to hear feedback from anyone who wants to try it out or contribute.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qijhth/weekend_project_an_opensource_claude_cowork_that/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0rvzc8",
          "author": "Available-Craft-5795",
          "text": "Im pretty sure this is AI.   \nLoads of emojis in readme  \nLoads of commments in code that no sane dev would add  \nDone in a weekend? How?",
          "score": 3,
          "created_utc": "2026-01-21 01:13:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ss3zn",
              "author": "Frequent_Cash2598",
              "text": "Yes, it is Claude Code making a Rust alternative of itself. \n\nSo you are right. :)",
              "score": 4,
              "created_utc": "2026-01-21 04:23:15",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0s2p5t",
              "author": "gingeropolous",
              "text": "Sounds like they used Claude code.",
              "score": 4,
              "created_utc": "2026-01-21 01:51:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0sepez",
          "author": "Efficient_Click_2689",
          "text": "cool project bro, would love to try out!",
          "score": 1,
          "created_utc": "2026-01-21 03:00:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0w48w9",
          "author": "wombweed",
          "text": "OS compatibility? If it runs on Linux can you include a Nix flake?",
          "score": 1,
          "created_utc": "2026-01-21 17:40:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjtnqm",
      "title": "How to implement a RAG (Retrieval Augmented Generation) on your laptop",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qjtnqm/how_to_implement_a_rag_retrieval_augmented/",
      "author": "Unique_Winner_5927",
      "created_utc": "2026-01-22 12:48:36",
      "score": 17,
      "num_comments": 0,
      "upvote_ratio": 0.85,
      "text": "This guide explains how to implement a RAG (Retrieval Augmented Generation) on your laptop.\n\nhttps://preview.redd.it/ftsddeqtcweg1.png?width=2184&format=png&auto=webp&s=640e3013e9113c3c7780a88b39d6992cd34b8d6f\n\nWith n8n, Ollama and Qdrant (with Docker).\n\n[https://github.com/ThomasPlantain/n8n](https://github.com/ThomasPlantain/n8n)\n\nI put a lot of screenshots to explain how to configure each component.\n\n\\#Ollama #n8n #Qdrant #dataSovereignty #embeddedAI",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qjtnqm/how_to_implement_a_rag_retrieval_augmented/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qj02gw",
      "title": "New Rules for ollama cloud",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qj02gw/new_rules_for_ollama_cloud/",
      "author": "killing_daisy",
      "created_utc": "2026-01-21 14:57:44",
      "score": 16,
      "num_comments": 5,
      "upvote_ratio": 0.9,
      "text": "so i've just seen this:\n\nPro:  \nEverything in Free, plus:\n\n* Run 3 cloud models at a time\n* Faster responses from cloud hardware\n* Larger models for challenging tasks\n* 3 private models\n* 3 collaborators per model\n\nits been a lot slower for usage within zed for me the last hours - does anyone have more information whats happening to the pro subscription? it seems like the changes in the subscription are random and without any notice to users? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qj02gw/new_rules_for_ollama_cloud/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0vg4mk",
          "author": "jmorganca",
          "text": "Hi there. I work on Ollama. No new restrictions on the cloud model usage with this change. We actually increased usage amounts on each plan on Monday and will share more about that this week. Our goal is to make this a the best subscription for using open models with your favorite tools as we add more model support, better performance and reliability.\n\nHappy to answer any questions and if you hit any issues or limits with the plans let me know (email is jeff at ollama.com)",
          "score": 13,
          "created_utc": "2026-01-21 15:52:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vraab",
              "author": "jmorganca",
              "text": "Also, OP, let me know if you're still seeing any slowdown (and for which models). We've been working on improving performance and capacity a lot in the last few weeks and will keep doing so. (Feel free to DM/email me)",
              "score": 8,
              "created_utc": "2026-01-21 16:42:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0w2o4h",
              "author": "killing_daisy",
              "text": "hi, it a bit worrying, that the contracts are sortof changing without notice to active users, i've only seen this because i was investigating the slow down - i'll have a try today at home, maybe it was only our network at the company.  \nedit: forgot to thank you for a response :) - so thanks :D",
              "score": 6,
              "created_utc": "2026-01-21 17:33:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o10on2k",
                  "author": "Ryanmonroe82",
                  "text": "This isn't the first time",
                  "score": 1,
                  "created_utc": "2026-01-22 09:29:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o10ol2i",
              "author": "Ryanmonroe82",
              "text": "Why does the APi no longer work when using synthetic data gen tools? Kiln AI and Easy Dataset both return errors now using cloud API and it was not like this when I first subscribed. All cloud models worked for this now none do.",
              "score": 1,
              "created_utc": "2026-01-22 09:28:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qkcg6a",
      "title": "Boom, the greatest repo yet from Lucas Valbuena ( x1xhlol)",
      "subreddit": "ollama",
      "url": "https://i.redd.it/dspr44juxzeg1.jpeg",
      "author": "OriginalZebraPoo",
      "created_utc": "2026-01-23 00:57:26",
      "score": 13,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qkcg6a/boom_the_greatest_repo_yet_from_lucas_valbuena/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qhr198",
      "title": "GLM 4.7 is apparently almost ready on Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/",
      "author": "Savantskie1",
      "created_utc": "2026-01-20 04:18:14",
      "score": 11,
      "num_comments": 19,
      "upvote_ratio": 0.87,
      "text": "It's listed, just not downloadable yet. Trying in WebOllama, and in CLI gives weird excuses\n\nhttps://preview.redd.it/96ly2bgckfeg1.png?width=1723&format=png&auto=webp&s=d8bfc2386dd789ef4f28ff0516de4893bf5c6772",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0u11zy",
          "author": "Resonant_Jones",
          "text": "Iâ€™m running it on apple silicon m4 32gb\n\nGLM-4.7-flash:q4_K_M\n\nItâ€™s an incredible model. Iâ€™ve been blown away so far.",
          "score": 2,
          "created_utc": "2026-01-21 10:47:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17clmx",
              "author": "Forbidden-era",
              "text": "how you running it exactly?",
              "score": 1,
              "created_utc": "2026-01-23 08:13:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ojnqc",
          "author": "beefgroin",
          "text": "thanks, running glm-4.7-flash:q8\\_0, so far so good",
          "score": 2,
          "created_utc": "2026-01-20 15:33:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0pkcjg",
              "author": "thexdroid",
              "text": "What is your GPU?",
              "score": 1,
              "created_utc": "2026-01-20 18:22:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0plxu3",
                  "author": "beefgroin",
                  "text": "I have 4 5060 16gb, I moved to q4k\\_m because the model consumes a lot of vram when the context is increasing, with 32k context it's taking 50gb of vram.",
                  "score": 1,
                  "created_utc": "2026-01-20 18:30:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0pm8wh",
              "author": "beefgroin",
              "text": "I liked it, the thinking process is very structured and very different from other thinking models. I guess something is still buggy cause every long conversation ends up in an infinite thinking loop",
              "score": 1,
              "created_utc": "2026-01-20 18:31:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0oe8av",
          "author": "WaitformeBumblebee",
          "text": "this one? updated 19 hours ago:\n\nhttps://ollama.com/library/glm-4.7-flash\n\n\nHmm, glm-4.7-flash:latest and glm-4.7-flash:q4_K_M are the same size, are they the same?\n\n\nCan't pull the new model, it requires Ollama update, even right after updating...\n\nollama --version\nollama version is 0.14.2\n\n\nedit: download latest release from github\nhttps://github.com/ollama/ollama/releases\n\nunpack, sudo cp lib files to /usr/local/lib/ollama/ and bin file to /usr/local/bin/   \n\npulling glm4.7 now\n\nedit2: quite fast on laptop 3060 6GB VRAM + 32GB RAM\n\nNAME                    ID              SIZE     PROCESSOR          CONTEXT              \nglm-4.7-flash:latest    ff14144f31df    23 GB    77%/23% CPU/GPU    4096",
          "score": 1,
          "created_utc": "2026-01-20 15:07:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0x6083",
          "author": "Zyj",
          "text": "I just pulled an Ollama container update, now I can download GLM 4.7 flash q8\\_0 with it. It will run on 2x 3090.",
          "score": 1,
          "created_utc": "2026-01-21 20:28:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0y8f7h",
          "author": "thexdroid",
          "text": "Odd, my system ram is 32GB and 16GB for GPU (4070ti). However I can't run any GLM model due memory... The glm-4.7-flash:q4\\_K\\_M model is actually 18GB but it requires 128GiB, I have downloaded and ran models like qwen3:30b-a3b-instruct-2507-q4\\_K\\_M (same 18GB) and even more. Do I would need more RAM? With current prices? lol, thanks.",
          "score": 1,
          "created_utc": "2026-01-21 23:32:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zjcs1",
              "author": "Savantskie1",
              "text": "I can run this exact model as of today with an RX 6800 16GB and a 7900 XT 20GB and 48GB of ram perfectly fine with room to spare. Itâ€™s not even using all the vram yet. WTF are you on about? 4.7 flash is a 30b moe model.",
              "score": 1,
              "created_utc": "2026-01-22 03:57:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o10pq8q",
                  "author": "thexdroid",
                  "text": "I am saying the file size is 18GB and 30B parameters, for comparison, so qwen and the GLM are same in file sizes and parameters, qwen loads and but with glm Ollama fails due insufficient memory error. Maybe I should download it again, not sure.",
                  "score": 1,
                  "created_utc": "2026-01-22 09:40:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0zwrta",
              "author": "_twrecks_",
              "text": "I see something similar, with just 8K context the glm-4.7-flash:q4\\_K\\_M model pulled today from Ollama overlfows my 3090 by like 30%. It does work though. Its like it turned of kv cache and flash attention? I do see the model file has \"RENDERER glm-4.7\" and \"PARSER glm-4.7\" in it.\n\nThe 2 GGUF versions on HF I tried uses way less memory, but also starts babbling to itself w/o end. I tried the recommend settings.\n\nNot seeing this model as even useful ATM.",
              "score": 1,
              "created_utc": "2026-01-22 05:27:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19zgy8",
                  "author": "thexdroid",
                  "text": "Someone in Ollama's Discord told me that it won't fit right now in the 16GB due current context size and they're working in another version",
                  "score": 1,
                  "created_utc": "2026-01-23 17:52:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0m0ppd",
          "author": "iansltx_",
          "text": "You can't use the GGUF. You \\*can\\* use the ones from the website if you have the latest ollama build (on GitHub, see that link).\n\nOne caveat: it runs slow on a unified-memory Mac for some reason, and splits 80/20 CPU/GPU. Not sure what's going on there. My Intel Mac runs faster.",
          "score": 1,
          "created_utc": "2026-01-20 04:30:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zjrel",
              "author": "Savantskie1",
              "text": "No shit ollama canâ€™t use the gguf version. It never was going to be able to. LM studio will though.",
              "score": 1,
              "created_utc": "2026-01-22 03:59:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qkl6rr",
      "title": "GitHub - FlorinAndrei/pipe-llama: Put an LLM in your shell scripts and command-line pipelines. Dead simple.",
      "subreddit": "ollama",
      "url": "https://github.com/FlorinAndrei/pipe-llama",
      "author": "florinandrei",
      "created_utc": "2026-01-23 08:14:30",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 0.83,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qkl6rr/github_florinandreipipellama_put_an_llm_in_your/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o196rdm",
          "author": "smith288",
          "text": "Pretty slick. Could you throw in an ascii progress while it makes the request?",
          "score": 3,
          "created_utc": "2026-01-23 15:41:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dob7e",
              "author": "florinandrei",
              "text": "It's meant to use small, fast models, doing relatively simple jobs with short or very short prompts. It's not meant for inference jobs that take a long time. Thinking is disabled by default. It has a warm-up mode. It has a one-line mode. I did everything I could think of to make it go faster.\n\nYou can always put `pv` in front of it in the pipeline, if you need a progress bar for a large file where many inference requests are issued to the endpoint for that one file. I'll add it to the examples.",
              "score": 2,
              "created_utc": "2026-01-24 05:40:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1eyx9t",
                  "author": "smith288",
                  "text": "All fair. But it still has delay especially when my ollama server is on another computer. Just spitting out an idea.",
                  "score": 1,
                  "created_utc": "2026-01-24 12:29:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1bphjy",
              "author": "1800not4you",
              "text": "+1 to this",
              "score": 1,
              "created_utc": "2026-01-23 22:41:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qidbdr",
      "title": "Local LLM (16GBRAM + 8VRAM) for gamedev",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qidbdr/local_llm_16gbram_8vram_for_gamedev/",
      "author": "RagingBass2020",
      "created_utc": "2026-01-20 20:58:38",
      "score": 9,
      "num_comments": 11,
      "upvote_ratio": 0.92,
      "text": "I am a developer that has been doing gamedev for 2 years but I used to be a backend developer for almost 10 years and a CS researcher before that.\n\nI use mostly Unity and Jetbrains Rider. \n\nAlthough I have a computer with more RAM at home, I need something that runs on a 16+8 GB laptop.\n\nI don't want to use it to develop full systems. I want something that is decent enough to create boilerplate code and help with some scripts and maybe some stuff I'm less used to (getting ready for the global game jam).\n\nIt needs to run offline with no access to the internet. I'm using ollama but I also have ComfyUI for some uni classes I was taking last semester.\n\nIf anyone could give me recommendations, I'd appreciate it.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qidbdr/local_llm_16gbram_8vram_for_gamedev/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0qovo3",
          "author": "zenmatrix83",
          "text": "the problem is context lenth, depending on what you want to do you might not be able to generate anything big enough. With the 24gb vram and 64gb ram I can only get decent speed with 64k context length. The problem I've seen with unity is most models don't know enough and you need to input examples or give the llm we search and wastes alot of context space\n\nI'm not saying don't try just have realistic expectations, you can get code but its really slow and usually not that good with smaller models and its usually only very popular languages that the model has alot in its training data. Agentic coding with cline or something similar is even tougher, the system prompt in roo was 20k or so. With it generating on small sections of code at a time it was often wrong since it didn't understand the entire file enough.",
          "score": 3,
          "created_utc": "2026-01-20 21:28:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1estm9",
              "author": "maxrd_",
              "text": "Could you tell what are your goto models for coding?\nThank you!",
              "score": 1,
              "created_utc": "2026-01-24 11:39:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1eu3pu",
                  "author": "zenmatrix83",
                  "text": "deepseek has been the best really, but I'd check here for ones to try, I mainly just pay for coding models. I have my mostly use my local llm for research projects\n\n[https://lmarena.ai/leaderboard/code](https://lmarena.ai/leaderboard/code)",
                  "score": 1,
                  "created_utc": "2026-01-24 11:50:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0syvsz",
          "author": "Ryanmonroe82",
          "text": "I have a Lenovo Legion with 8gb VRAM and 32gb RAM. If I use an 8b model in BF16 it around 16gb by itself. I usually set the context length to 8092 and I can get about 14-16 tokens per second.  Time to first token can be a little slow but itâ€™s usable for sure. Iâ€™m using DeepSeekR1 Distill 8b",
          "score": 2,
          "created_utc": "2026-01-21 05:09:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qj99g",
          "author": "XxCotHGxX",
          "text": "look for a model that is around 5GB. that way you can have it loaded ino your VRAM for faster inference. I like to use Qwen3 Coder 8B, but i am a ML engineer.",
          "score": 2,
          "created_utc": "2026-01-20 21:02:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0s5jdm",
          "author": "newbietofx",
          "text": "Tinyllama or smoll but I don't have success their temperature is not normal.Â ",
          "score": 1,
          "created_utc": "2026-01-21 02:08:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tqkt5",
          "author": "WaitformeBumblebee",
          "text": "I don't have experience with those programs but in general LLM seem to excel at Python and then have trouble with other languages and even some python modules especially ones that have many versions with deprecated stuff. Ideally to run locally we would need fine tuned versions of LLM for a certain language and version.",
          "score": 1,
          "created_utc": "2026-01-21 09:09:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0y54iz",
          "author": "StardockEngineer",
          "text": "No man. Not enough horsepower.   Youâ€™ll make a lot of garbage.",
          "score": 1,
          "created_utc": "2026-01-21 23:14:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11zeef",
          "author": "Seninut",
          "text": "You are not going to be happy with 8GB of Vram, It will be a constant war of resources. Even a 12GB GPU would make a world of difference.",
          "score": 1,
          "created_utc": "2026-01-22 14:49:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o120bz1",
              "author": "RagingBass2020",
              "text": "It's what I have on the laptop...",
              "score": 1,
              "created_utc": "2026-01-22 14:53:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1eu5tz",
          "author": "UnbeliebteMeinung",
          "text": "Just wait and use som api product in the mean time",
          "score": 0,
          "created_utc": "2026-01-24 11:51:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qi8ouc",
      "title": "Plano 0.4.3 â­ï¸ Filter Chains via MCP and OpenRouter Integration",
      "subreddit": "ollama",
      "url": "https://i.redd.it/o590ks78pjeg1.png",
      "author": "AdditionalWeb107",
      "created_utc": "2026-01-20 18:12:47",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qi8ouc/plano_043_filter_chains_via_mcp_and_openrouter/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qjvhs6",
      "title": "I can't get qwen2.5-coder:7b working with claude code",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qjvhs6/i_cant_get_qwen25coder7b_working_with_claude_code/",
      "author": "acidiceyes",
      "created_utc": "2026-01-22 14:09:18",
      "score": 5,
      "num_comments": 13,
      "upvote_ratio": 0.74,
      "text": "Hey, I just read that we can use ollama with claude code now, but I have been trying to get qwen2.5-coder:7b working with claude code, but tool calling just doesn't work.  \nWhat am i doing wrong?\n\nhttps://preview.redd.it/mc5u9eoorweg1.png?width=1376&format=png&auto=webp&s=403d76d563760d11c890855a3b03e6a62bbc27fd\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qjvhs6/i_cant_get_qwen25coder7b_working_with_claude_code/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o14eb11",
          "author": "Outrageous_Rub_6527",
          "text": "Hi OP! I'm one of the engineers on the Ollama team. qwen2.5-coder doens't have the greatest RL for tool calling. I'd recommend qwen3, qwen3-coder, gpt-oss:20b instead.",
          "score": 3,
          "created_utc": "2026-01-22 21:26:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11zqgz",
          "author": "BidWestern1056",
          "text": "use npcsh\n\n[https://github.com/npc-worldwide/npcsh](https://github.com/npc-worldwide/npcsh)\n\nqwen2.5 coder doesnt have tool calling",
          "score": 2,
          "created_utc": "2026-01-22 14:50:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12wpi9",
              "author": "acidiceyes",
              "text": "I'll check this out, thanks",
              "score": 1,
              "created_utc": "2026-01-22 17:22:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o11tq20",
          "author": "AcanthaceaeNorth6189",
          "text": "OLLAMA is just an inference framework, and the 7b coder model should have no problem writing simple code, but I don't think the function calling ability is up to it. Maybe you need to change to a larger model to support this. Also, have the working interfaces of LLM and Claude clearly input the fuction protocol?",
          "score": 1,
          "created_utc": "2026-01-22 14:20:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12wo7k",
              "author": "acidiceyes",
              "text": "I don't understand your question exactly",
              "score": 1,
              "created_utc": "2026-01-22 17:22:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o16bctn",
                  "author": "AcanthaceaeNorth6189",
                  "text": "First of all, function calling ability requires the model to have strong instruction following ability, that is, the model needs to have the ability to call functions given in context. The scale of 7b is obviously insufficient. Secondly, when claude code and LLM are used together, what tools or middleware are used to collaborate with each other?",
                  "score": 1,
                  "created_utc": "2026-01-23 03:37:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o11xnou",
          "author": "maciek_glowka",
          "text": "Context size increase seems to help with tool calling. However indeed the qwen2.5-coder:7b might not be best with tools (from my experience). I've had a bit better tool results in the same memory footprint area with \\`qwen3:4b-q8\\_0\\` or \\`granite4:tiny-h\\` (with this one I could even bump the context to 32k) - but I am not sure how those model handle coding tasks...",
          "score": 1,
          "created_utc": "2026-01-22 14:40:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12cbsu",
              "author": "zenmatrix83",
              "text": "I haven't tried this with claude code yet, but for roo in vscode I needed at least 24k just for the system prompt realistically I wouldn't try local without 64k. You still need room for the agent to work.",
              "score": 1,
              "created_utc": "2026-01-22 15:50:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o11xt14",
          "author": "seangalie",
          "text": "Are you running the default context window?  Because you'll have to max out to 32K context within your environment variables to really get Claude Code working - the CC prompt alone could be chewing up the context window that Ollama is providing.",
          "score": 1,
          "created_utc": "2026-01-22 14:41:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12wtmn",
              "author": "acidiceyes",
              "text": "I had increased the context window to 32k",
              "score": 1,
              "created_utc": "2026-01-22 17:23:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o12we5o",
          "author": "shikima",
          "text": "I find in HF another qwen2.5 coder with tool calling and works but in opencode as frontend, in medium there is an article that make qwen works in lmstudio too.",
          "score": 1,
          "created_utc": "2026-01-22 17:21:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18hino",
          "author": "Ok_Hospital_5265",
          "text": "Tried the same w Crush/Ollama before I learned I could use CC with Ollama + local models (which I hasnâ€™t actually tried yet). M1 with 24gigs and ran into the same tool calling issues with qwen and 4 other models, presumably due to limited context, possible issues with streaming vs raw mode (though that could have been CC troubleshooting halluciBS). Would love to hear how you get a 7B or similar sized model running locally w CC if you ever sort it out. Good luck!",
          "score": 1,
          "created_utc": "2026-01-23 13:35:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18q7hn",
          "author": "StardockEngineer",
          "text": "That model is not good enough.  Plain and simple.",
          "score": 1,
          "created_utc": "2026-01-23 14:20:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}