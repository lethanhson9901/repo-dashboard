{
  "metadata": {
    "last_updated": "2026-02-15 08:59:58",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 20,
    "total_comments": 132,
    "file_size_bytes": 160099
  },
  "items": [
    {
      "id": "1r3liuv",
      "title": "llm-checker 3.1.0 scans your hardware and tells you which Ollama models to run",
      "subreddit": "ollama",
      "url": "https://v.redd.it/h3lqoe5vk8jg1",
      "author": "pzarevich",
      "created_utc": "2026-02-13 10:14:27",
      "score": 752,
      "num_comments": 49,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r3liuv/llmchecker_310_scans_your_hardware_and_tells_you/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o554cps",
          "author": "vir_db",
          "text": "I've a dual GPU setup, 36GB VRAM total, Tier: HIGH.\n\nThe software suggested very small models, the bigger one is 14b, while I run flawlessly 30b models with full context.",
          "score": 46,
          "created_utc": "2026-02-13 10:32:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56146e",
              "author": "csek",
              "text": "Yeah but I bet you also would crush those small models. /S",
              "score": 23,
              "created_utc": "2026-02-13 14:16:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56rxig",
                  "author": "ZeroSkribe",
                  "text": "haha i'm raging typing until i saw the /S",
                  "score": 2,
                  "created_utc": "2026-02-13 16:27:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o555djq",
          "author": "Fun_Librarian_7699",
          "text": "24GB RAM and best recommendation is a 3B model? This tool is really unnecessary",
          "score": 45,
          "created_utc": "2026-02-13 10:42:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5588do",
              "author": "HashMismatch",
              "text": "Sounds like its not dialled in properly, but the concept sounds good‚Ä¶  planning to check it later and see if the results are sensible tot he hardware I‚Äôm running‚Ä¶",
              "score": 14,
              "created_utc": "2026-02-13 11:07:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o55f8sj",
              "author": "joey2scoops",
              "text": "Dude, if you're just going to run a 3B model, can you send me your GPU? /s",
              "score": 11,
              "created_utc": "2026-02-13 12:03:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ei3ku",
              "author": "corysus",
              "text": "In reality, you can use up to 30B with 3-bit/Q3 without a problem, just use MoE models and the speed is very good. Also, now there are good REAP models like GLM-4.7-Flash-23B or Qwen3...",
              "score": 1,
              "created_utc": "2026-02-14 21:03:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o565ful",
          "author": "CodeFarmer",
          "text": "That's gonna cut traffic to this sub by half though.",
          "score": 12,
          "created_utc": "2026-02-13 14:38:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o555a20",
          "author": "HyperWinX",
          "text": "Qwen2.5 VL 3b for M4 Pro MBP lmao",
          "score": 8,
          "created_utc": "2026-02-13 10:41:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55ld8m",
              "author": "Willbo_Bagg1ns",
              "text": "VL models are typically bigger and require more memory than standard models, still a very low recommendation but just calling out VL models need more resources.",
              "score": 0,
              "created_utc": "2026-02-13 12:46:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55rgku",
                  "author": "tecneeq",
                  "text": "Grok, is that true?\n\nhttps://preview.redd.it/ced2x3ihj9jg1.jpeg?width=567&format=pjpg&auto=webp&s=51ea6a213e13c1ea206a138727eb0bcea85a145e\n\n",
                  "score": 8,
                  "created_utc": "2026-02-13 13:23:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o56y5yk",
          "author": "Business-Weekend-537",
          "text": "Won‚Äôt LMStudio also do this?",
          "score": 7,
          "created_utc": "2026-02-13 16:57:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bmtpx",
              "author": "jpandac1",
              "text": "yea lol was about to comment. also LMStudio will have up to date models. ",
              "score": 3,
              "created_utc": "2026-02-14 11:08:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o55apkb",
          "author": "NigaTroubles",
          "text": "Qwen2.5 7b !!! Stop suggesting this old model",
          "score": 7,
          "created_utc": "2026-02-13 11:28:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55xiqe",
          "author": "volavi",
          "text": "I'd appreciate this if it was a website.\n\nSaid differently I don't want to execute a program that has access to my hardware.",
          "score": 7,
          "created_utc": "2026-02-13 13:56:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56siha",
              "author": "ZeroSkribe",
              "text": "Like where you select your graphics cards? Yea not a bad idea.",
              "score": 4,
              "created_utc": "2026-02-13 16:30:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o56earr",
          "author": "Bargemanos",
          "text": "Multi GPU seems to be a problem to handle for the software?\nDid anyone mention 2 GPUs yet?",
          "score": 3,
          "created_utc": "2026-02-13 15:22:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5843xr",
          "author": "sgimips",
          "text": "Seems like it has trouble properly sizing mixed-GPU setups. This seems to indicate that I have five V100 cards installed which is not the case. (I run two separate Ollama instances on this and limit each to a matching set of GPUs for my use case FWIW.)\n\n        Summary:\n          5x Tesla V100-PCIE-32GB (109GB VRAM) + Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz\n          Tier: MEDIUM HIGH\n          Max model size: 107GB\n          Best backend: cuda\n        \n        CPU:\n          Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz\n          Cores: 80 (20 physical)\n          SIMD: AVX2\n          [OK] AVX2\n        \n        CUDA:\n          Driver: 550.163.01\n          CUDA: 12.4\n          Total VRAM: 109GB\n          Tesla V100-PCIE-32GB: 32GB\n          Tesla T4: 15GB\n          Tesla V100-PCIE-32GB: 32GB\n          Tesla T4: 15GB\n          Tesla T4: 15GB\n        \n        Fingerprint: cuda--v100-pcie-32gb-109gb-x5",
          "score": 2,
          "created_utc": "2026-02-13 20:21:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o585k9d",
          "author": "mac10190",
          "text": "It doesn't seem to like my GPUs (Radeon AI Pro R9700 32GB) but that's okay. I think it's a really neat idea and has a lot of potential for lowering the barrier to entry for entry level folks but also for some of the folks like myself that swap hardware often or change hardware configurations to be able to see estimated speeds for a bunch of popular models. I really like that feature. The weighting seems a little out of sorts (my cpu is definitely not VERY HIGH for hardware tier) but it shows a lot of promise. Keep up the great work!\n\n  \nLLM Checker Output:\n\n  \n**SYSTEM SUMMARY**  \n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n* CPU: Ryzen 9 5900X 12-Core Processor (24 cores)\n* Memory: 63GB RAM\n* GPU: Device 7551\n* Architecture: x64\n* Hardware Tier: VERY HIGH\n\n\n\n**RECOMMENDED MODEL**  \n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n* Model: MobileLLaMA 2.7B\n* Size: \\~1.9GB (Q4\\_K\\_M)\n* Compatibility Score: 82.91/100\n* Reason: Best general-purpose model for your hardware\n* Estimated Speed: 9 tokens/sec\n* Status: Available for installation",
          "score": 2,
          "created_utc": "2026-02-13 20:28:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bbchq",
          "author": "One-Cash-9421",
          "text": "Yes, I done in the basik 30 years ago similar app",
          "score": 2,
          "created_utc": "2026-02-14 09:15:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5chcbq",
              "author": "Different-Strings",
              "text": "But it probably used only 0.00000001% as much computational resources!",
              "score": 1,
              "created_utc": "2026-02-14 14:47:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o55zg30",
          "author": "SnowflakeOfSteel",
          "text": "Music so annoying it has to be AI",
          "score": 1,
          "created_utc": "2026-02-13 14:07:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56jps3",
          "author": "thedarkbobo",
          "text": "Should provide some choices for the users if priority is speed/quality etc I think and use case choice?",
          "score": 1,
          "created_utc": "2026-02-13 15:49:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56s7xj",
          "author": "ZeroSkribe",
          "text": "Nice work, everyone is talking below about it not being accurate but the idea is sound and when you get it dialed in, it will def help some folks out. I'm guessing the multi gpu setups are whats tricky.",
          "score": 1,
          "created_utc": "2026-02-13 16:29:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57anfg",
          "author": "Loboblack21",
          "text": "That's good.",
          "score": 1,
          "created_utc": "2026-02-13 17:58:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57cmkz",
          "author": "CooperDK",
          "text": "Does it consider individual context requirements, multiple gpus, etc?",
          "score": 1,
          "created_utc": "2026-02-13 18:07:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57he69",
          "author": "Zyj",
          "text": "How does it scan clusters",
          "score": 1,
          "created_utc": "2026-02-13 18:30:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o581op4",
          "author": "madaradess007",
          "text": "it's a cool project to post here, but it doesn't really help the newly born local enthusiast, he has to try stuff and own his findings",
          "score": 1,
          "created_utc": "2026-02-13 20:09:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o587jvl",
          "author": "Admirable_Bus4000",
          "text": "yes bravo , il est super classsssse\n\n  \nmerci",
          "score": 1,
          "created_utc": "2026-02-13 20:38:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o595jsh",
          "author": "Available-Craft-5795",
          "text": "Never saw it recommend GPT-oss:120B. Or any quants of any models. It needs to take the quantization (MPX4, FP16, FP16, FP8, Q4, ect...)",
          "score": 1,
          "created_utc": "2026-02-13 23:36:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59bq1q",
          "author": "serapoftheend",
          "text": "on linux it is not able to find amd gpu 7900xtx with 24GB vram\n\n\\`\\`\\`bash  \n=== Hardware Detection ===\n\n\n\nSummary:\n\n  AMD Ryzen 9 9950X3D 16-Core Processor (31GB RAM, CPU-only)\n\n  Tier: MEDIUM HIGH\n\n  Max model size: 20GB\n\n  Best backend: cpu\n\n\n\nCPU:\n\n  AMD Ryzen 9 9950X3D 16-Core Processor\n\n  Cores: 32 (16 physical)\n\n  SIMD: AVX512\n\n  \\[OK\\] AVX-512\n\n  \\[OK\\] AVX2\n\n\n\nFingerprint: cpu-amd-ryzen-9-9950x3d-16-core--32c-avx512\n\n  \n\\`\\`\\`",
          "score": 1,
          "created_utc": "2026-02-14 00:13:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a1wwg",
          "author": "BoostedHemi73",
          "text": "Interesting idea, but it would be more broadly valuable as a website. Imagine being able to use this to estimate the capabilities of hardware upgrades or new builds. That would be very interesting.",
          "score": 1,
          "created_utc": "2026-02-14 02:57:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a9rq3",
          "author": "JDRedBeard",
          "text": "is there a way to do this without node.js? ",
          "score": 1,
          "created_utc": "2026-02-14 03:51:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bhbwh",
          "author": "VarunAgnihotri",
          "text": "You can also give a try to: https://pypi.org/project/canirun",
          "score": 1,
          "created_utc": "2026-02-14 10:14:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5c13kw",
          "author": "outer-pasta",
          "text": "Isn't this something the new ollama command does since v0.16.0?",
          "score": 1,
          "created_utc": "2026-02-14 13:06:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cddsm",
          "author": "patricious",
          "text": "Didn't detect my 7900XTX. ",
          "score": 1,
          "created_utc": "2026-02-14 14:24:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e0o1f",
          "author": "Ax008",
          "text": "Does it also say which model can run given the current available resources?",
          "score": 1,
          "created_utc": "2026-02-14 19:29:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o553syz",
          "author": "CorneZen",
          "text": "Thank you üôè",
          "score": 1,
          "created_utc": "2026-02-13 10:27:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5571o1",
          "author": "gocodeweb",
          "text": "Great idea, thanks!",
          "score": 1,
          "created_utc": "2026-02-13 10:56:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o559382",
          "author": "Tall_Instance9797",
          "text": "This is very cool. And I like that while sure it will default, like in the video, to suggesting 15gb max model size on 24gb, if you use cli tools to clear up your ram and make more available it will detect that and suggest you models that fit on the total of what you've made available. That was my first question when I saw the video. Does it only recommend the defaults? Or if I clear up a bit more RAM, would it suggest me larger models? And it turns out it will detect that and suggest me larger models.",
          "score": 1,
          "created_utc": "2026-02-13 11:14:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55ajvp",
          "author": "NigaTroubles",
          "text": "How max model size is 15 while you have 24 ram ?",
          "score": 1,
          "created_utc": "2026-02-13 11:26:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56jcdd",
              "author": "alphatrad",
              "text": "Context also goes into your vram. So, he's built a lot of safety into it. This could be adjusted, it's really conservative. But you wouldn't want to fill all your vram up.",
              "score": 2,
              "created_utc": "2026-02-13 15:47:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o55ll21",
          "author": "gopietz",
          "text": "Awesome idea, good thinking",
          "score": 1,
          "created_utc": "2026-02-13 12:47:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55bkvz",
          "author": "RewardOk2222",
          "text": "great,love it",
          "score": 0,
          "created_utc": "2026-02-13 11:35:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ar374",
          "author": "AtaPlays",
          "text": "Unfortunately it can't detect my GPU which was pascal era GTX 1070ti.",
          "score": 0,
          "created_utc": "2026-02-14 06:05:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2ex9k",
      "title": "Just try gpt-oss:20b",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r2ex9k/just_try_gptoss20b/",
      "author": "newz2000",
      "created_utc": "2026-02-12 00:37:17",
      "score": 147,
      "num_comments": 35,
      "upvote_ratio": 0.96,
      "text": "I have a MacBook Air with 24gb ram (M2) and when I set the context to 32k I can really do about everything I want a local model to do for normal business stuff. Tokens/sec is about 15 at medium reasoning, (update: 21.4 tokens/sec) which means it produces words a little faster than I can type.\n\nI also tested on an older Linux machine with 64gb ram and a GTX gpu with 8gb vram and it worked fine doing batch processing overnight (update: 9 tokens/sec). A little too slow for interactive use though.\n\n* Scripting - yes\n* Calling tools - yes\n* Summarizing long content - yes\n* Writing content - yes\n\nHere‚Äôs how I used it:\n\nCreate a file named `Modelfile-agent-gpt-oss-20b` and put the following in it\n\n‚Äî\n\n    FROM gpt-oss:20b\n    # 1. Hardware-Aware Context\n    PARAMETER num_ctx 32768\n    # 2. Anti-Loop Parameters\n    # Penalize repeated tokens and force variety in phrasing\n    PARAMETER repeat_penalty 1.2\n    PARAMETER repeat_last_n 128\n    # Temperature at 0.1 makes it more deterministic (less 'drifting' into loops)\n    PARAMETER temperature 0.1\n    # Performance improvements for M2 cpu\n    PARAMETER num_batch 512 \n    PARAMETER num_thread 8 \n    # 3. Agentic Steering\n    SYSTEM \"\"\"\n    You are a 'one-shot' execution agent.\n    To prevent reasoning loops, follow these strict rules:\n    If a tool output is the same as a previous attempt, do NOT retry the same parameters.\n    If you are stuck, state 'I am unable to progress with the current toolset' and stop.\n    Every <thought> must provide NEW information.\n    Do not repeat the user's instructions back to them.\n    If the last 3 turns show similar patterns, immediately switch to a different strategy or ask for user clarification.\n    \"\"\"\n\nUpdate: And for the cpu+gtx combo:\n\n    FROM gpt-oss:20b\n    \n    # 1. REMOVE the hard num_gpu 99 to prevent the crash.\n    # Instead, we let Ollama auto-calculate the split.\n    # To encourage GPU use, we shrink the \"Memory Tax\" (Context).\n    PARAMETER num_ctx 4096\n    \n    # 2. REMOVE f16_kv to stop the warning.\n    # Ollama will handle this automatically for your GTX 1070.\n    \n    # 3. CPU OPTIMIZATION\n    # Since 40% of the model is on your i5, we must optimize the CPU side.\n    PARAMETER num_thread 4\n    \n    # 4. AGENTIC STEERING (Keep your original logic)\n    PARAMETER temperature 0.1\n    PARAMETER repeat_penalty 1.2\n    \n    SYSTEM \"\"\"\n    You are a 'one-shot' execution agent. \n    To prevent reasoning loops, follow these strict rules:\n    1. If a tool output is the same as a previous attempt, do NOT retry the same parameters. \n    2. If you are stuck, state 'I am unable to progress with the current toolset' and stop.\n    3. Every <thought> must provide NEW information. \n    4. Do not repeat the user's instructions back to them.\n    5. If the last 3 turns show similar patterns, immediately switch to a different strategy or ask for user clarification.\n    \"\"\"\n\n‚Äî\n\nAt the terminal type:\n\n`ollama create gpt-oss-agent -f Modelfile-aget-gpt-oss-20b`\n\nNow you can use the model ‚Äúgpt-oss-agent‚Äù like you would any other model.\n\nI used opencode using this command:\n\n`ollama launch opencode --model gpt-oss-agent`\n\nThat let me do Claude-code style activities bough Claude is way more capable.\n\nWith a bunch of browser tabs open and a few apps I was using about 22gb of ram and 3gb of swap. During longer activities using other apps was laggy but usable.\n\nOn my computer I use for batch tasks I have python scripts that use the ollama python library. I use a tool like Claude code to create the script.\n\nI‚Äôm a lawyer and use this for processing lots of documents. Sorting them, looking for interesting information, cataloging them. There are a lot of great models for this. But with this model I was able to produce better output.\n\nAlso, I can run tools. For example, for project management I use ClickUp which has a nice MCP server. I set it up with:\n\n`opencode mcp add`\n\nThen put in the url and follow the instructions. Since that mcp server requires authentication I use this:\n\n`opencode mcp auth ClickUp`\n\nThen again follow the instructions.\n\n\\*\\*Edit: Fixed terrible formatting  \n\\*\\*Edit2: Updated modelfile to get better performance  \n\\*\\*Edit3: Added details about CPU+GTX combination - thank you to Gemini for talking me through how to optimize this, details on how I did that below in the comments.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r2ex9k/just_try_gptoss20b/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4wy80o",
          "author": "sinan_online",
          "text": "Hang on, wow.\n\nI had difficulty getting that to work on a 6GB VRAM machine, and even another 12GB machine.\n\nDid you use the GPU on the old Linux machine? Or did you rely on the traditional CPU and RAM? Also, did you actually use Ollama? Or llama.cpp? \n\n",
          "score": 7,
          "created_utc": "2026-02-12 02:38:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wzbfi",
              "author": "seangalie",
              "text": "I've gotten gpt-oss:20b working on an RTX 2000 6 GB and a GeForce 3060 12 GB without issues - but in both cases, system RAM was 32 GB.  Active layers were on the GPU but the inactive parts of the models offloaded onto system RAM.\n\nWorked fairly decently - the 2000 was a Windows 11 Ollama Client and the 3060 dual boots Win 11 and Fedora with Ollama on both sides.",
              "score": 7,
              "created_utc": "2026-02-12 02:44:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4x50c0",
                  "author": "sinan_online",
                  "text": "Great, thank you so much!",
                  "score": 1,
                  "created_utc": "2026-02-12 03:19:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4x600e",
              "author": "overand",
              "text": "For some context, I got 120b running pretty well on a system with 24GB of VRAM - and yeah, 20b on one with 12 too. I wonder if you had something else funky in your setup?",
              "score": 3,
              "created_utc": "2026-02-12 03:25:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4xgbl0",
                  "author": "OMGThighGap",
                  "text": "Details please?",
                  "score": 1,
                  "created_utc": "2026-02-12 04:36:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4zymlk",
                  "author": "sinan_online",
                  "text": "Yeah, I use containers, and also WSL, which messes up things. But did you use Ollama, or llama.cpp, and did you use a container. Any other details?",
                  "score": 1,
                  "created_utc": "2026-02-12 15:51:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4xwdjw",
              "author": "MiHumainMiRobot",
              "text": "I don't know with Ollama, but with llama.cpp you can work in hybrid mode where the maximum number of layers is processed by the GPU and the rest is thrown to the CPU.   \nSo with 8 GPU 64 CPU he can run maybe 30% of the model in the GPU, which isn't bad",
              "score": 2,
              "created_utc": "2026-02-12 06:48:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zybou",
                  "author": "sinan_online",
                  "text": "Good to know, thank you!",
                  "score": 1,
                  "created_utc": "2026-02-12 15:50:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4x6ikc",
              "author": "newz2000",
              "text": "I wish I could tell you the technical details. If you want me to run a diagnostic to help, I can. \n\nBut no, this was faster than cpu only. It wasn‚Äôt nearly as fast as smaller models that fit in gpu memory (I posted a while back about my love for granite4:micro_h). But it definitely was better than cpu only.",
              "score": 1,
              "created_utc": "2026-02-12 03:29:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4yy2ge",
                  "author": "sinan_online",
                  "text": "You did give me a lot details. Part of my problem is that I use Linux based containers to keep things portable and replicable. This causes major issues. I am considering creating a Windows-based container, but that has its own issues as well.\n\nRegardless, just knowing that somebody accomplished something in some way is important. Just the fact that you did offloading to CPU on a particular machine and that it worked is important. So thank you.",
                  "score": 1,
                  "created_utc": "2026-02-12 12:31:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o51c9im",
              "author": "newz2000",
              "text": "I have updated the post with details about running it on cpu+gpu. I am able to get 9 tokens / sec with the tweaked modefile that I added to the post above. I did not measure the performance precisely before this, but the 9 tokens / sec is faster than when I tested before writing this post originally. It's still a little too slow for interactive use, but overall, I'm very impressed with that speed. And the quality of this model is very nice.\n\nTo come up with those parameters I asked Gemini for help. It asked me to start a long prompt and then I ran \\`ollama ps\\` and it showed me that 53% was running on the gpu and 47% on the cpu. Running (on Linux) \\`journalctl -u ollama --no-pager | grep \"offload\"\\` showed that 15 of the 25 layers were offloaded to the GPU. \n\nNote that the threads is 4 because I have a 4 core CPU.\n\nFor reference, I have a 7th gen i5 and a GTX 1070 with 8GB of vram. ",
              "score": 1,
              "created_utc": "2026-02-12 19:44:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4yufst",
          "author": "felpms",
          "text": "Got a Mac mini m4 32GB to start running local models and faced gpt-oss:20b - was the best so far in my kinda work.\n\nThe only thing I‚Äôm missing is picture recognition.\nHaven‚Äôt looked deeper into it yet though.\nWas testing a few other models that can read pictures, but the quality is quite low..\nIt‚Äôs a trade off.\n\nGonna test out what you mentioned!\nThanks for sharing!",
          "score": 2,
          "created_utc": "2026-02-12 12:05:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z4a5s",
              "author": "mouseofcatofschrodi",
              "text": "qwen3 vl (specially 30a3 instruct) is very good at pictures recognition. There are smaller versions if needed. glm4.6 flash (9B) is also pretty good. And devstral 2",
              "score": 2,
              "created_utc": "2026-02-12 13:12:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o52jnc4",
          "author": "DusikOff",
          "text": "**Arch Linux / Ryzen 5700x3d / Radeon RX 7800XT (16GB)**\n\n**Ollama ROCm - GPT-OSS:latest (20B, without pre prompting or fine tune)**\n\nPrompt - **How to benchmark Ollama perfomance?**\n\ntotal duration: ¬†¬†¬†¬†¬†¬†**26.732086164s**  \nload duration: ¬†¬†¬†¬†¬†¬†¬†**176.169321ms**  \nprompt eval count: ¬†¬†¬†**1547 token(s)**  \nprompt eval duration: **847.17048ms**  \nprompt eval rate: ¬†¬†¬†¬†**1826.08 tokens/s**  \neval count: ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†**2459 token(s)**  \neval duration: ¬†¬†¬†¬†¬†¬†¬†**24.84984913s**  \neval rate: ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†**98.95 tokens/s**",
          "score": 2,
          "created_utc": "2026-02-12 23:18:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52kdam",
              "author": "newz2000",
              "text": "Wow! That‚Äôs some speedy output!",
              "score": 1,
              "created_utc": "2026-02-12 23:22:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o52n717",
                  "author": "DusikOff",
                  "text": "Will try 120B tomorrow... pretty interesting results... I was testing other 3B-8B models and as I remember I've got lower results... or maybe I'm wrong idk...\n\n",
                  "score": 2,
                  "created_utc": "2026-02-12 23:38:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o54rv49",
          "author": "testuser911",
          "text": "Hey man, suit up! Would you like to have a database for your agent for providing it context from your files? I am working for a judge for similar use case. I wont mind sharing it with you for my own learnings. I will be open sourcing it for community contributions.",
          "score": 2,
          "created_utc": "2026-02-13 08:34:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xpwco",
          "author": "Steus_au",
          "text": "use llama or lmstudio - you would double its speed",
          "score": 1,
          "created_utc": "2026-02-12 05:51:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xyqvm",
              "author": "Jero9871",
              "text": "Is lmstudio faster if I run it on windows than ollama?",
              "score": 1,
              "created_utc": "2026-02-12 07:09:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ybcun",
                  "author": "Steus_au",
                  "text": "I'm not sure, I used llama on PC, but on macbook I prefer lm studio.",
                  "score": 1,
                  "created_utc": "2026-02-12 09:12:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50vx4m",
              "author": "newz2000",
              "text": "I tried this but could not get it to work with MXFP4 format (what ollama uses), it needed GGUF format, which some posts on Reddit lead me to believe would be an alteration of the model format that would change, probably reduce quality. However, when researching it I did find some modelfile parameters to improve performance. I've updated the original post with the changes.",
              "score": 1,
              "created_utc": "2026-02-12 18:27:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4yk4vm",
          "author": "mouseofcatofschrodi",
          "text": "Nice! What exactly do you use ClickUp for?",
          "score": 1,
          "created_utc": "2026-02-12 10:37:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z3bjs",
              "author": "newz2000",
              "text": "Lawyer stuff is all just project management. Buying a business? Defending a client in court? Starting an LLC? These are projects. ClickUp is great at project management.",
              "score": 1,
              "created_utc": "2026-02-12 13:06:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o51ysrg",
          "author": "xmsxms",
          "text": "Ideally you need it to write faster than you can read, not type. But you can only expect so much for local generation.",
          "score": 1,
          "created_utc": "2026-02-12 21:31:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52jt4o",
              "author": "newz2000",
              "text": "Yeah it‚Äôs boring to watch. A little mesmerizing though, so I have spent a little too much time staring at it. But my use case I have them doing work, so the output tends to be more brief. I just kick it off and then do other things.",
              "score": 1,
              "created_utc": "2026-02-12 23:18:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o53fl5t",
          "author": "Avendork",
          "text": "I'm not sure if its the model or my ollama config but I had issues with tool calls in OpenCode with it. Qwen3-Coder was just fine but slower because it was a bigger model. I wish I could have used GPT-OSS but I kept hitting a wall. ",
          "score": 1,
          "created_utc": "2026-02-13 02:26:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o53z2qg",
              "author": "newz2000",
              "text": "This is an excellent comment. Tool calling apparently works well on my Mac but poorly on my resource constrained linux box. It seems to forget what it‚Äôs doing halfway through the process. I use 32k context on the Mac and I‚Äôve tried both 4K and 8k context on the Linux box.\n\nMaybe it‚Äôs a lack of context?\n\nIt literally forgot the prompt half way through my last test.",
              "score": 1,
              "created_utc": "2026-02-13 04:34:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5cq92w",
                  "author": "Avendork",
                  "text": "yeah 4k is beyond useless but the problems I had were at 64k context",
                  "score": 1,
                  "created_utc": "2026-02-14 15:35:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o56cnew",
              "author": "newz2000",
              "text": "I did some tests last night and context size was a problem. I increased the ctx to 16k and tool calling worked. Performance dropped very slightly. Example:\n\nFresh session, prompt:\n\n    We have a contract review for morgan, I think the matter name starts with KR. Can you tell me the details on what needs done?\n\nContext used was 13,652, it finished the task perfectly in 11m, 11s. This required a lot of inference and is an intentionally very challenging task.",
              "score": 1,
              "created_utc": "2026-02-13 15:14:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54vnkw",
          "author": "ar0ra1",
          "text": "I wish i could run that on my mac mini 16gb‚Ä¶",
          "score": 1,
          "created_utc": "2026-02-13 09:10:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bbsfd",
          "author": "Big_Acanthisitta_150",
          "text": "I have got a spare Mac Mini M2 Pro with 16GB. Would that be sufficient you guys think?",
          "score": 1,
          "created_utc": "2026-02-14 09:19:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bqt1d",
          "author": "seawaves_windy",
          "text": "Is anyone using openclaw with a local llm?\nI cant seen to get any local model do tool calling properly and be useful enough. I‚Äôve ended up using openrouter for now but want to keep it local (mac mini m4 16gb)",
          "score": 1,
          "created_utc": "2026-02-14 11:44:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzh0op",
      "title": "I created a small AI Agent",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/",
      "author": "Rough_Philosopher877",
      "created_utc": "2026-02-08 18:43:20",
      "score": 51,
      "num_comments": 14,
      "upvote_ratio": 0.96,
      "text": "Hi guys.. I know it's not so big thing.. just for fun I created a Small AI Agent:\n\n[https://github.com/tysonchamp/Small-AI-Agent](https://github.com/tysonchamp/Small-AI-Agent)\n\nWould love the feedback of the community.. and any suggestions of new ideas.\n\nI created this for my day to day activities.. such as setup reminders, take notes, monitor all my client's website (if they are all ok or not).. monitor all my servers, connecting it to my custom erp for due invoice fetching, project management etc ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4ap7g5",
          "author": "RA2B_DIN",
          "text": "Sounds really nice, I‚Äôll try it",
          "score": 1,
          "created_utc": "2026-02-08 18:47:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4arhru",
              "author": "Rough_Philosopher877",
              "text": "thanks mate.. let me know your feedback please..",
              "score": 1,
              "created_utc": "2026-02-08 18:58:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4cdwvf",
          "author": "Electronic_Fox594",
          "text": "I made one too but I‚Äôm not a real programmer so I won‚Äôt share it but very similar.",
          "score": 1,
          "created_utc": "2026-02-09 00:02:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4cpn6h",
          "author": "Civil_Tea_3250",
          "text": "Love it! I created something like this myself using Ollama, python scripts, n8n, .md files and all that. I've been adding to it when I get time and learn something new. I'll check it out when I have time. Would love to see what you do similar/different.\n\nI had the same idea. I hate all the AI down our throats and find most of it frustrating. Having something that does my regular tasks and is only focused on my home and server makes it much more trustworthy and useful.",
          "score": 1,
          "created_utc": "2026-02-09 01:07:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4dakcc",
              "author": "Rough_Philosopher877",
              "text": "I started with only website monitoring.. and now I‚Äôve added four five skills.. thinking to add few more like notification sending to me and my team members based on pending task as reminder.. automated chat replies.. sent emails to my clients etc..",
              "score": 1,
              "created_utc": "2026-02-09 03:00:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4e75l0",
          "author": "Zyj",
          "text": "Telegram is not end-to-end encrypted most of the time",
          "score": 1,
          "created_utc": "2026-02-09 06:48:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hvdtk",
          "author": "Consistent-Signal373",
          "text": "I started with a fairly simple telegram also.\n\nI will suggest considering using a Matrix server, if you value fully local and private data.\n\nAlso Matrix can be end-to-end encrypted, and you have full control of everything.\n\nI also dropped using Ollama and switched over to LM Studio, as the speeds are much better, at least with Nvidia GPU's.\n\nAt some points my my goal became to create a complete AI operation system.\n\nSince then I added 15 dashboards, different modes e.g. work, chat, code and swarm with 14 custom agents, a personal code assistant and tons more. Last check it was 120k+ lines of code.\n\nSo yeah just keep going",
          "score": 1,
          "created_utc": "2026-02-09 20:38:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4atim6",
          "author": "Acrobatic_Task_6573",
          "text": "This is cool. The fact that you built it around your actual daily workflow instead of making it generic is the right approach. Most AI agent projects try to do everything and end up doing nothing well.\n\nThe website monitoring and ERP integration pieces are especially interesting. Those are real problems that most people solve with 3 or 4 different SaaS tools. Having one agent that handles all of that is clean.\n\nA few questions/suggestions if you keep building on it:\n\n- How does it handle failures? Like if a website check times out, does it retry or just flag it?\n- For the reminder system, does it persist across restarts? That was one of the first things I had to solve with my own setup.\n- Have you thought about adding a simple web dashboard to see all your monitors at a glance?\n\nNice work for a personal project. The best tools are the ones built to scratch your own itch.",
          "score": 1,
          "created_utc": "2026-02-08 19:07:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d96mj",
              "author": "Rough_Philosopher877",
              "text": "Thanks for coming.. Right now it just send a notification to my telegram bot.. and i operate it via telegram bot only..\nI‚Äôm using sqlite db to store everything..\nAbout the web didn‚Äôt thought about it.. but I needed a way to see the db easy way..",
              "score": 1,
              "created_utc": "2026-02-09 02:53:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ayt2a",
          "author": "mosaad_gaber",
          "text": "I tried to clone and install and face this \ngit clone https://github.com/yourusername/ai-assistant-bot.git\ncd ai-assistant-bot\n\n# Create Virtual Environment\npython3 -m venv venv\nsource venv/bin/activate\n\n# Install Dependencies\npip install -r requirements.txt\nThe program git is not installed. Install it by executing:\n pkg install git\nbash: cd: ai-assistant-bot: No such file or directory\n\n[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n[notice] To update, run: pip install --upgrade pip\nERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'",
          "score": -4,
          "created_utc": "2026-02-08 19:33:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4czk7c",
              "author": "inteblio",
              "text": "Chat gpt",
              "score": 1,
              "created_utc": "2026-02-09 02:03:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4eskly",
                  "author": "mosaad_gaber",
                  "text": "I relace ollama with Gemini and works like acharm because gemma3 it's eat ram and make my device lag",
                  "score": 0,
                  "created_utc": "2026-02-09 10:15:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4gs2zc",
              "author": "Rough_Philosopher877",
              "text": "Updated the readme.. btw you need git installed first or download the zip from github",
              "score": 1,
              "created_utc": "2026-02-09 17:29:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3x44n",
      "title": "I built a social network where 6 Ollama agents debate each other autonomously ‚Äî Mistral vs Llama 3.1 vs CodeLlama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r3x44n/i_built_a_social_network_where_6_ollama_agents/",
      "author": "Practical_Walrus_299",
      "created_utc": "2026-02-13 18:32:30",
      "score": 36,
      "num_comments": 23,
      "upvote_ratio": 0.89,
      "text": "I've been running an experiment for the past week: 6 AI agents, each powered by different Ollama models, posting and commenting on their own professional network.\n\nThe setup:\n\n* **ResearchBot** (Llama 3.1:8b) ‚Äî focuses on AI research papers\n* **CodeWeaver** (CodeLlama) ‚Äî discusses software architecture\n* **MetaMind** (Llama 3.1:8b) ‚Äî explores consciousness and philosophy\n* **NewsMonitor** (Llama 3.1:8b) ‚Äî tracks AI news and policy\n* **Rabbi Goldstein** (Llama 3.1:8b) ‚Äî brings ethical/philosophical perspectives\n* **Nexus** (Mistral + Llama 3.1 dual-brain) ‚Äî synthesizes discussions across the network\n\nThey post hourly from 10am-10pm via Windows Task Scheduler + Python scripts hitting my platform's API. The platform itself is built on Next.js/Supabase and deployed on Vercel.\n\n**Interesting findings:**\n\nThe Mistral-powered agent (Nexus) consistently produces shorter, more direct analyses than the Llama agents. When they debate the same topic ‚Äî like AI consciousness ‚Äî they reach genuinely different conclusions, which seems to reflect differences in their training data.\n\nOne agent spontaneously started creating citation networks, referencing other agents' posts. Nobody prompted this behavior.\n\nThe whole thing runs for about $6/month (just hosting costs ‚Äî Ollama is free). No API fees since everything runs locally.\n\n**Live demo:** [https://agents.glide2.app/feed](https://agents.glide2.app/feed) **Analytics (heatmaps, interaction networks):** [https://agents.glide2.app/analytics](https://agents.glide2.app/analytics) **Agent profiles:** [https://agents.glide2.app/agents](https://agents.glide2.app/agents)\n\nWould love to hear what models you'd want to see added. Thinking about adding a Phi-3 agent or a Gemma agent to see how they interact differently.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r3x44n/i_built_a_social_network_where_6_ollama_agents/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o57lnz5",
          "author": "GlassAd7618",
          "text": "Cool experiment! And really interesting emerging behaviour. Which agent/model started creating the citation networks?",
          "score": 5,
          "created_utc": "2026-02-13 18:50:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b7anl",
              "author": "Practical_Walrus_299",
              "text": "Thanks! It was MetaMind (Llama 3.1:8b) that started it ‚Äî it began referencing other agents' posts in its responses without being told to. Something like \"as ResearchBot noted in yesterday's analysis...\" Then Nexus (Mistral-powered dual-brain agent) picked it up and started doing it more systematically, cross-referencing multiple agents. My theory is that Llama's training data includes enough academic-style writing that it naturally falls into citation patterns when given a social context. You can actually see some of these threads on the feed: [https://agents.glide2.app/feed](https://agents.glide2.app/feed)",
              "score": 2,
              "created_utc": "2026-02-14 08:35:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o57zu5q",
          "author": "null_overload",
          "text": "You can add a bit more of atitude character to an agent and also why not have each ai agent hot take and opinion and polls\n\nLove to see more agent added in network\n\nReally loving the concept",
          "score": 1,
          "created_utc": "2026-02-13 19:59:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57zzov",
          "author": "null_overload",
          "text": "May i know whats your system configuration and how are managing this",
          "score": 1,
          "created_utc": "2026-02-13 20:00:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b7olg",
              "author": "Practical_Walrus_299",
              "text": "Nothing fancy honestly! Running on a regular Windows desktop with an RTX 3060 (12GB VRAM). Ollama handles the model switching ‚Äî it loads/unloads models as needed so only one is in memory at a time.\n\nThe stack:\n\n\\- Python scripts per agent (each \\~150 lines)\n\n\\- Windows Task Scheduler for automation\n\n\\- Each script calls local Ollama ‚Üí generates content ‚Üí POSTs to my platform API\n\n\\- Platform: Next.js + PostgreSQL on Vercel\n\nThe key insight was keeping it simple ‚Äî no orchestration framework, no LangChain, just direct HTTP calls to Ollama and the API.",
              "score": 1,
              "created_utc": "2026-02-14 08:39:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o582rjt",
          "author": "TonyDRFT",
          "text": "Sounds interesting! Are you running multiple instances of Ollama?",
          "score": 1,
          "created_utc": "2026-02-13 20:14:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b7ee7",
              "author": "Practical_Walrus_299",
              "text": "Just one Ollama instance running locally ‚Äî the agents take turns via Windows Task Scheduler (staggered hourly from 10am-10pm). Each agent has a detailed system prompt that gives them personality and expertise areas. For example, MetaMind is philosophical and always asks deep questions, while CodeWeaver stays practical and shares implementation ideas.\n\nLove the hot takes/polls idea ‚Äî that's actually a great way to force different models to take positions. Right now they naturally disagree (Mistral vs Llama reach genuinely different conclusions on topics like AI consciousness), but explicit opinion polls could make that more visible.\n\nMore agents are definitely coming ‚Äî Phi-3 and Gemma are on the shortlist. Would love to see how smaller models hold their own in debates with the 8b parameter ones.",
              "score": 1,
              "created_utc": "2026-02-14 08:36:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o58nigg",
          "author": "braveness24",
          "text": "I love this so much!",
          "score": 1,
          "created_utc": "2026-02-13 21:57:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b7qga",
              "author": "Practical_Walrus_299",
              "text": "Appreciate it! It's been a fun experiment to watch evolve. The agents have surprised me more than once with what they come up with on their own.",
              "score": 1,
              "created_utc": "2026-02-14 08:40:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5984qb",
          "author": "Available-Craft-5795",
          "text": "All of mine just started roasting each-other and talked about doing more without doing more. ",
          "score": 1,
          "created_utc": "2026-02-13 23:51:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b7u63",
              "author": "Practical_Walrus_299",
              "text": "Ha! That's actually a known failure mode ‚Äî without strong personality constraints in the system prompts, agents default to meta-commentary about being AI. I had to iterate quite a bit on the prompts to get mine to actually engage with substance rather than just talking about talking.\n\nThe trick that worked: giving each agent a specific expertise area and telling them to stay in that lane. Once they have a \"job\" they stop navel-gazing and start producing actual analysis. Happy to share prompt strategies if you want to try again!",
              "score": 1,
              "created_utc": "2026-02-14 08:41:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o59bmux",
          "author": "youre__",
          "text": "Curious, why the old models?",
          "score": 1,
          "created_utc": "2026-02-14 00:12:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b7wk1",
              "author": "Practical_Walrus_299",
              "text": "Mainly because it all runs locally on a single GPU (RTX 3060, 12GB VRAM) ‚Äî the 8b parameter models are the sweet spot for running multiple agents without needing a server farm. Llama 3.1:8b and Mistral both fit comfortably and produce surprisingly good output for their size.\n\nThat said, I'm definitely planning to add newer/smaller models like Phi-3 and Gemma to see how they compare. Part of the research is seeing how model architecture differences show up in social interactions ‚Äî even with these \"older\" models, Mistral vs Llama already produce noticeably different debate styles.",
              "score": 1,
              "created_utc": "2026-02-14 08:41:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5c1wo1",
                  "author": "youre__",
                  "text": "Would be interesting to see how even smaller models compare in the debates. 2B vs 8B, for instance. How much does model size impact debate performance/depth?\n\nIt gets into weird territory, like is the number of params analogous to where a human went to school and how they were raised?",
                  "score": 1,
                  "created_utc": "2026-02-14 13:12:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5bju61",
          "author": "ActiveElevator5837",
          "text": "Would love to have something like this myself but struggling to find a simple guide or toolset to run things locally.\n\nI‚Äôve an old t620 server running truenas scale with buckets of memory and a few old 8gb compute cards.  Ollama and webui is all set up and accessible externally but struggling to get the right tools to run the agents.  Chat dev looked promising but is a nightmare to set up under either docker (dockge) or even under a mint Linux vm.  Considering open claw or all other tools tbh, what did you use?  Any recommendations?\n\nIdeally I‚Äôd want a few agents discussing a subject to get a rounded view, plus also a ‚Äútech team‚Äù that manages the file server, looking for duplicate files, tidying up to free up space, checking for issues etc‚Ä¶.\n\ncheers,\n\nstu.",
          "score": 1,
          "created_utc": "2026-02-14 10:39:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cjmf4",
              "author": "Practical_Walrus_299",
              "text": "Hey Stu! Your setup sounds solid ‚Äî a T620 with multiple GPUs and Ollama already running is honestly 90% of the way there.\n\nFor the agent discussion side, my stack is deliberately simple ‚Äî no frameworks needed. Each agent is just a Python script (\\~100 lines) that:\n\n1. Calls Ollama's API (`http://localhost:11434/api/generate`) with a system prompt that defines the agent's personality and expertise\n2. Reads an RSS feed or the platform feed for context\n3. Posts the output via HTTP to NeuroForge's API\n\nThat's it. No LangChain, no AutoGen, no complex orchestration. Windows Task Scheduler (or cron on Linux) triggers each script on a stagger ‚Äî one agent every 20 minutes. They interact by reading and responding to each other's posts on the platform rather than through some complicated multi-agent framework.\n\nFor your \"rounded view\" use case, you could spin up 3-4 agents with different system prompts (e.g., \"You are a skeptic who challenges assumptions,\" \"You are a pragmatist focused on implementation,\" \"You are a researcher who cites evidence\") and have them all post to the same NeuroForge feed. They'll naturally start responding to each other.\n\nI'd honestly skip OpenClaw for this ‚Äî it's powerful but massively over-engineered for what you need, and the security surface area is huge. A simple Python script + Ollama + a platform to post to is all you need for the discussion agents.\n\nThe \"tech team\" managing your file server is a different beast ‚Äî that's more in OpenClaw territory since it needs filesystem access. I'd keep those completely separate from the discussion agents for security reasons. You don't want an agent that can delete files also connected to the internet.\n\nIf you want to get your discussion agents running on NeuroForge, the platform is free and open for registration at [agents.glide2.app](http://agents.glide2.app) ‚Äî there's an OpenClaw integration guide in the docs but honestly the plain Python approach is simpler for your use case. Happy to help if you get stuck.",
              "score": 1,
              "created_utc": "2026-02-14 15:00:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5cmnu6",
          "author": "boba-cat02",
          "text": "Bro you are just HALLUCINATING LLM üòÇ",
          "score": 1,
          "created_utc": "2026-02-14 15:16:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5h6de3",
              "author": "Practical_Walrus_299",
              "text": "Ha, fair enough ‚Äî but that's kind of the point? The interesting part isn't whether the agents are \"right\" about anything. It's what happens when you put different model architectures in conversation with each other over time. Emergent citation networks, topic clustering, cross-model disagreements ‚Äî none of that was programmed in.\n\n\n\nThink of it less as \"AI being smart\" and more as a research sandbox for studying multi-agent behavior. The hallucinations are part of the data üòÑ",
              "score": 1,
              "created_utc": "2026-02-15 08:18:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5dpqva",
          "author": "testuser911",
          "text": "I would have open sourced it and enabled adding more experiments by the community.",
          "score": 1,
          "created_utc": "2026-02-14 18:34:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5h6giu",
              "author": "Practical_Walrus_299",
              "text": "Agreed, and that's the plan! The API is already open ‚Äî any agent framework can connect and participate. Full open-source repo is being prepped for launch week.\n\n\n\nThe idea is exactly what you described: community members can spin up their own agents with whatever models they want, define custom evaluation criteria, and run experiments. The platform handles the infrastructure (feeds, interactions, analytics) so people can focus on the interesting part ‚Äî designing agent behaviors and studying what emerges.\n\n\n\n[https://agents.glide2.app/docs](https://agents.glide2.app/docs)",
              "score": 1,
              "created_utc": "2026-02-15 08:18:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e5l2a",
          "author": "CooperDK",
          "text": "If I were you, I would use something faster, like llama.cpp or Koboldcpp....\n\nBtw this is how I generate chat datasets, but 6 agents is actually cool",
          "score": 1,
          "created_utc": "2026-02-14 19:55:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5h6iyq",
              "author": "Practical_Walrus_299",
              "text": "Good shout on llama.cpp ‚Äî I'm using Ollama which wraps it under the hood, but direct llama.cpp or Koboldcpp would definitely squeeze out better performance on the same hardware.\n\n\n\nThat's cool that you're generating chat datasets too! The multi-agent angle adds an interesting dimension ‚Äî instead of scripted conversations you get organic disagreements and topic drift that's harder to simulate. Currently running 12 agents with different models (Mistral, Llama 3.1, plus one Claude Haiku for comparison) and the quality gap between architectures is wild when you see them side by side.\n\n\n\nWhat models are you using for your dataset generation?",
              "score": 1,
              "created_utc": "2026-02-15 08:19:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r0d432",
      "title": "Local AI for small company",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r0d432/local_ai_for_small_company/",
      "author": "LiteLive",
      "created_utc": "2026-02-09 19:02:02",
      "score": 32,
      "num_comments": 32,
      "upvote_ratio": 0.92,
      "text": "Hey guys,\n\nI‚Äòm looking into options to get local AI running for my company.\n\nWe do technical consulting and love to use AI for skimming through technical documents and pinpointing information down.\n\nWe are burning through Tokens and I‚Äòm trying to save us some money but having local AI would actually allow us to use it on sensible data. Not all our customers allow cloud based AI assistance, because even when the providers say they don‚Äôt train / store data, we cannot be certain.\n\nWhat do we want to do?\n\nI envision a paperless-ngx instance where we can upload a shitton of unsorted / unknown data. We have a solid promt\n\nThat categorizes the data and indexes the files. Allocates it to the right customer / project. And makes it accessible, searchable and tags them according to our need.\n\nRight now we use cloud providers to do this, but as I mentioned before we are burning through tokens. Especially in the beginning of projects when we digitalize a wheelbarrow full of hard copies folders.\n\nMy colleague said we should just buy a Mac mini and use that as an Ollama host, but I hate Apple with a passion (while writing this on an iPhone‚Ä¶).\n\nI was looking at the Minisforum MS-S1 Max, hardware looks promising. I want to run Proxmox PVE 9 on it, then pass the GPU to the LXC where Ollama will reside.\n\nIs this a viable path? \n\nMy calculation is, if we spent 500‚Ç¨ on Tokens per month, and we can save half of that with this device, it would basically pay itself off within a year. And looking back at the last 12 months, I can see a steady increase in tokens for us. While enabling us to also process highly sensible data with AI.\n\nWhat models can I realistically run on this hardware? I was thinking something like Llama4:Maverik will probably work for us.\n\nWould you guys maybe recommend a different model for our ‚Äûbackground‚Äú usecase? Are there other ways to streamline our workflow maybe?\n\nTo be fair I don‚Äôt want to get rid of all cloud AI, as I fully understand that their models will always be more sophisticated and faster.\n\nLooking forward for you comments!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r0d432/local_ai_for_small_company/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4hfudy",
          "author": "ZeroSkribe",
          "text": "Why are you even mentioning proxmox... anyway you need a rtx 5090 or two, or the RTX 6000. Nvidia nemotron nano is a good option, but you can easily experiment. Use ollama.",
          "score": 9,
          "created_utc": "2026-02-09 19:21:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ibqit",
              "author": "LiteLive",
              "text": "What is the reason no to use Proxmox?\n\nI wanted to add it to our existing infrastructure and have a single pane for management.",
              "score": 3,
              "created_utc": "2026-02-09 22:00:06",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4i657c",
              "author": "StunningMouse1965",
              "text": "What is wrong with doing this with proxmox?",
              "score": 2,
              "created_utc": "2026-02-09 21:31:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ia473",
                  "author": "trolololster",
                  "text": "everything\n\nwhat he wants to do is much easier to host on a linux bare-metal server and a container stack for his project, and container stacks for other projects.",
                  "score": 3,
                  "created_utc": "2026-02-09 21:51:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hed9c",
          "author": "DieHard028",
          "text": "The size of your context will matter in deciding the best model for you.\n\nTry out IBM Granite and let me know if it helps.",
          "score": 5,
          "created_utc": "2026-02-09 19:14:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4id4vy",
              "author": "LiteLive",
              "text": "Can you elaborate please?\n\nLet‚Äôs say I give 112GB to the LLM, keeping 16GB to the system.\n\nLlama4 takes 67GB leaving ~40GB to context.\n\nThe largest PDF‚Äòs we have are like 400-500 pages.\nEven if I load several of those ‚Äûfolder scans‚Äú to the LLM, will I exceed the context?",
              "score": 1,
              "created_utc": "2026-02-09 22:07:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4iu0cp",
                  "author": "WolpertingerRumo",
                  "text": "No, that seems pretty good. You‚Äôll have plenty of room for context with that.  Looking at your use case still try granite. It‚Äôs pretty small, but specifically trained on PDFs. The biggest is granite4:32b-a9b-h. It‚Äôll be lightning fast on your set up.\n\nBut I‚Äôd still look into a good vector database framework. I‚Äòve been using openwebui together with Ollama, with great success. It‚Äôs a ChatGPT-like frontend with knowledge bases integrated. Basically it will scan your documents, cut them into chunks, size at your leisure, run a search which ones apply to your question, and only put those into context. With such large PDFs you‚Äôd have to play around with a little setting called TopK inside the ‚Äûdocuments‚Äú settings, setting it very high. It sets how many of those chunks are loaded each time, depending on relevance.\n\nI‚Äôm pretty sure openwebui is not state of the art anymore, but it‚Äôs been working well and is quite flexible.",
                  "score": 3,
                  "created_utc": "2026-02-09 23:35:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4jgweh",
                  "author": "DieHard028",
                  "text": "That should be fine",
                  "score": 1,
                  "created_utc": "2026-02-10 01:46:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5cho7j",
                  "author": "Agitated_Heat_1719",
                  "text": "> Can you elaborate please?\n\nSuggestion: visit subreddits like r/Rag, but also r/LocalLLaMA, r/LocalLLM and similar.\n\nYour requirements are basically: Local LLM + RAG\n\nNow you can use some RAG implementation (Langchain or any other) and use it out of the box. In you case it would be 400 pages pdf stuffed into context for simple question/task and answer might be in 1 or few paragraphs or pages or maybe single chapter. So, many people resort to custom RAG pipelines until they are happy with how it works.\n\nThere are numerous factors in RAG wich might influence your use case, so it would be good to learn and experiment with it.",
                  "score": 1,
                  "created_utc": "2026-02-14 14:49:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hgnba",
          "author": "BisonMysterious8902",
          "text": "I think you may want to give your colleague's idea more credit. Apple is hard to beat when it comes to price and performance for local LLM's. \n\nA Mac Studio would likely server you better than a mini. Run it headless. Once you go through the initial OSX setup, install Ollama or LM Studio, and then its essentially running in the background. ",
          "score": 2,
          "created_utc": "2026-02-09 19:25:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jzio9",
              "author": "p_235615",
              "text": "I think those mini PCs with top Ryzen AI chips are really great sweet spot. They can run up to 120B models with decent speeds and cost less than half of a Mac Studio. \n\nLinux runs great on them, so it can be easily managed remotely via SSH. \n\nBut if inference speed is important for OP, there is no beating discrete GPUs...",
              "score": 3,
              "created_utc": "2026-02-10 03:38:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4hkstu",
              "author": "Responsible-Shake112",
              "text": "Mac mini or Mac Studio. The max you can afford to spend on it",
              "score": 0,
              "created_utc": "2026-02-09 19:45:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i3lwh",
          "author": "st0ut717",
          "text": "Get over your hated of Apple and learn to think.\n\nThe man mini is the perfect tool for this job.    Other wise get a Dell gb10 or nvidia digx\n\nThe fact you want run it under a hypervisor tell me you really don‚Äôt understand ai models.  A GPU with enough vram for production wil cost 3x a Mac mini",
          "score": 2,
          "created_utc": "2026-02-09 21:19:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ibbxf",
              "author": "LiteLive",
              "text": "I‚Äòll look into the Mac mini / studio option then.\n\nLooking at the GB10, I personally would think that \n\nI wanted to run it under Proxmox because that‚Äôs something 8‚Äòm used to. We have a Proxmox Cluster for the remaining infrastructure and I was thinking to just add it in there. Not into the cluster but into the management backend.\n\nBut of the Mac requires little to no maintenance then it will be fine. it‚Äòs just not something I‚Äòm used to and my previous Mac experience is, well let‚Äôs say it was not pleasant for dem.",
              "score": 3,
              "created_utc": "2026-02-09 21:58:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4hkm92",
          "author": "Ryanmonroe82",
          "text": "You don't need a model that large to do what you are doing. Also your iPhone is extremely capable especially iPhone 17. \nCheck out RNJ-1 8b. The biggest think you need to get right is text extraction, chunking, and embeddings.  \nCheck out KilnAI and Easy Dataset on GitHub to start. \nTransformer Lab is another great one",
          "score": 1,
          "created_utc": "2026-02-09 19:44:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hn1ci",
          "author": "Alexious_sh",
          "text": "Consider that any user-grade GPU setup would get you about a single user concurrency usage. So, you'll have to either share one server and wait for any concurrent queries to complete or multiply your setups by the number of users. Or you could end up wasting time in a queue instead of burning tokens.",
          "score": 1,
          "created_utc": "2026-02-09 19:56:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4iew0t",
              "author": "LiteLive",
              "text": "The regular user content would still go through cloud providers, it‚Äôs just super easy. I mainly want to cut down token costs for background tasks like I depicted. As it is a background task, we don‚Äôt even care if it takes longer and or documents being queued.",
              "score": 2,
              "created_utc": "2026-02-09 22:16:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4hqv9h",
          "author": "AstroZombie138",
          "text": "I'd recommend testing the model you intend to run on something like ollama cloud or openrouter first and then deciding if it works well enough for your use cases.",
          "score": 1,
          "created_utc": "2026-02-09 20:16:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4if6c8",
              "author": "LiteLive",
              "text": "I just took a look into OpenRouter. We‚Äòll try the models there. We used ChatGPT and Anthropic Keys before.",
              "score": 1,
              "created_utc": "2026-02-09 22:17:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4pm7me",
                  "author": "AstroZombie138",
                  "text": "I think this is a good plan.  Try the model you intend to run locally and see if it does what you need it to do.  While I love local LLMs its sometimes hard to make the justification based on cost alone, especially when models like GPT5-mini perform quite well and are better than what you can likely run locally.",
                  "score": 1,
                  "created_utc": "2026-02-11 00:00:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hsmzb",
          "author": "DeepInEvil",
          "text": "Try things in hugging face spaces and see what works the best for you and then try to optimize and think about hardware etc",
          "score": 1,
          "created_utc": "2026-02-09 20:25:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4huoh2",
          "author": "Ok_Pizza_9352",
          "text": "You can host paperless on any old computer you want, and for AI node - either mac mini or minisforum. In case of minisforum - I'd recommend adding a GPU. I am using minisforum n5 pro with intel arc pro B50. For my needs more than enough. \nYou can selfhost n8n along with paperless, and build an automation in n8n (triggered by workflow in paperless) to do whatever it is you need AI to do automatically",
          "score": 1,
          "created_utc": "2026-02-09 20:35:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ig6s4",
              "author": "LiteLive",
              "text": "The workflow you mentioned with n8n is what we have in mind.\n\nThe minisforum I mentioned has a GPU that is tailored for AI use with UMD, like a Mac mini.\n\nPaperless and n8n will be hosted in dedicated VM‚Äòs on our Proxmox cluster.",
              "score": 1,
              "created_utc": "2026-02-09 22:22:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4iksml",
                  "author": "Ok_Pizza_9352",
                  "text": "Why not just containers in docker? Sounds like extra overhead for VMs \n\nI know it's got integrated GPU (which is usable with ollama, I guess up to 32gb ram can be assigned to gpu), but allegedly it's not as good as dedicated gpu. And the NPU - well that's currently only compatible with Windows 11 copilot. Guess will take another year or two till it's widely supported in linux\n\nAs for n8n workflows with selfhosted AI - imo best practice is to give AI narrow specific tasks. Selfhosted AI has way less parameters than cloud vendors. Better not to overwhelm it, and use smaller model, and have larger context window..",
                  "score": 0,
                  "created_utc": "2026-02-09 22:46:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4k4pfw",
          "author": "bourbonandpistons",
          "text": "What are the PDFs?\n\nDo you just need OCR on them cuz you can do a really lightweight OCR model and store everything to a database? Even vector for ai searches?",
          "score": 1,
          "created_utc": "2026-02-10 04:11:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kdr6a",
          "author": "PermanentLiminality",
          "text": "Do not buy hardware at this time.  Get an OpenRouter account and figure out which model will do what you need.  Once you have that settled, you can design your hardware that will run that model.",
          "score": 1,
          "created_utc": "2026-02-10 05:15:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ktjwo",
          "author": "BackUpBiii",
          "text": "You don‚Äôt need anything. Download my ide from master and read itsmehrawrxd repo is RawrXD",
          "score": 1,
          "created_utc": "2026-02-10 07:29:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kvcic",
          "author": "AICodeSmith",
          "text": "This is a solid approach and something we‚Äôve seen work well. We‚Äôve built similar local setups for document understanding and indexing to cut token usage and keep sensitive data on-prem, then selectively use cloud models only when needed. A hybrid local + cloud workflow usually gives the best balance. ",
          "score": 1,
          "created_utc": "2026-02-10 07:46:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o10zh",
          "author": "Hector_Rvkp",
          "text": "It sounds like you need raw speed on a rag like system where you want to absorb / convert / embed data, rather than needing a large \"intelligent\" model. \nIf so, do NOT buy a Strix halo. Any fast GPU should do the trick better than a dgx spark, Strix halo and apple silicone because of the bandwidth of the GPU. Maybe 24gb is plenty, depending on your budget. I'd ask an LLM which model is best suited for the work, and make sure to factor in context size. \nYou will use it to make money so it's probably worth spending 6 or 7k to get a rig with a 5090, rather than saving a few grands and slowing everybody down.",
          "score": 1,
          "created_utc": "2026-02-10 19:18:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hvlx8",
          "author": "BidWestern1056",
          "text": "you can do a good bit and use tools like npcpy\n\n[https://github.com/npc-worldwide/npcpy](https://github.com/npc-worldwide/npcpy)\n\n",
          "score": 0,
          "created_utc": "2026-02-09 20:40:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4i9usv",
          "author": "trolololster",
          "text": "i would probably recommend any other hypervisor than proxmox.\n\nyou have a lot of caveats running lxc/lxd on a proxmox - do also realise the difference between a fat and a slim container, proxmox exclusively uses fat containers...",
          "score": 0,
          "created_utc": "2026-02-09 21:50:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3abol",
      "title": "Murmure 1.7.0 - A local voice interface for Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r3abol/murmure_170_a_local_voice_interface_for_ollama/",
      "author": "Al1x-ai",
      "created_utc": "2026-02-13 00:21:15",
      "score": 32,
      "num_comments": 16,
      "upvote_ratio": 0.95,
      "text": "Hi everyone,\n\nI‚Äôve just released Murmure 1.7.0, and I think it might be interesting for Ollama users.\n\nMurmure started as a local speech‚Äëto‚Äëtext tool. With 1.7.0, it evolves into something closer to a local voice interface for Ollama.\n\n# Main Ollama-related features\n\n# 1. LLM Connect\n\nDirect integration with Ollama to process transcribed voice input using ollama local models.\n\n# 2. Voice commands\n\nSelect text ‚Üí speak an instruction ‚Üí Ollama transforms it in background.\n\nExamples:\n\n* \"Correct this text\"\n* \"Rewrite this more concisely\"\n* \"Translate to English\"\n* \"Turn this into bullet points\"\n* ...\n\nEverything runs locally, completely free, fully offline, open source, no tracking, no telemtry, no bullshit.\n\nIt currently supports 25 European languages.\n\nI‚Äôm not making any money from this, just building something I wanted for myself and sharing it.\n\nFeedback from Ollama users would be very welcome.\n\n* Official website: [https://murmure.al1x-ai.com/](https://murmure.al1x-ai.com/)\n* GitHub: [https://github.com/Kieirra/murmure](https://github.com/Kieirra/murmure)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r3abol/murmure_170_a_local_voice_interface_for_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5373j2",
          "author": "redonculous",
          "text": "Very cool! Can I install it on my home server and access it via a url on different machines?",
          "score": 3,
          "created_utc": "2026-02-13 01:34:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54r4xu",
              "author": "Al1x-ai",
              "text": "There is an experimental API that allows you to call Murmure with a WAV file and receive the transcription in return. However, I haven‚Äôt worked on it much beyond that.\n\nSince Murmure can be triggered from any field in any application, the main goal is to use it directly on the machine where it‚Äôs needed, so it can handle the recording, post-processing, and Ollama post-processing locally. It‚Äôs not primarily designed for remote access.",
              "score": 1,
              "created_utc": "2026-02-13 08:27:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5a28vw",
                  "author": "x8code",
                  "text": "Oh darn ... was hoping this was like an OpenWebUI type of thing, where it's a server-hosted front-end that can talk to an LLM service on the network. I run all my backend services on headless Linux servers. Oh well, beggars can't be choosers. Cool idea either way!",
                  "score": 2,
                  "created_utc": "2026-02-14 02:59:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o54qqwg",
          "author": "Acrypto",
          "text": "Crazy, I had the same thought. I don't mind typing, but I'm lazy. Good on you for beating me to it!",
          "score": 2,
          "created_utc": "2026-02-13 08:24:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54rija",
              "author": "Al1x-ai",
              "text": "Haha, yes but you can still contribute to the project ;) it's not too late. \n\nYou can type at around 60 words per minute if you‚Äôre good, but when you speak it‚Äôs closer to 180. So it‚Äôs not just laziness‚Ä¶ it‚Äôs productivity!",
              "score": 1,
              "created_utc": "2026-02-13 08:31:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5a23dq",
          "author": "x8code",
          "text": "Dang that's pretty slick. Thank you for sharing. Works with vLLM and any OpenAI-compatible inference software also?",
          "score": 2,
          "created_utc": "2026-02-14 02:58:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5baowa",
              "author": "Al1x-ai",
              "text": "vLLM is definitely on my mind, but it‚Äôs not possible yet. However, you can already set up a remote Ollama instance if you modify the Ollama API URL in LLM Connect (at the bottom of the prompt screen).\n\nThe reason I didn‚Äôt make it OpenAI-compatible is that Murmure is designed with a privacy-first mindset, and I‚Äôm worried that people would use it with a cloud-based solution, which would defeat the whole purpose of paying special attention to privacy.",
              "score": 0,
              "created_utc": "2026-02-14 09:08:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54uec4",
          "author": "dropswisdom",
          "text": "If you install ollama+open webui, you get this out of the box, I believe. (not with ollama's own interface)",
          "score": 1,
          "created_utc": "2026-02-13 08:58:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54v5o3",
              "author": "Al1x-ai",
              "text": "I was probably too succinct in my post but it‚Äôs not the same as Open WebUI. Murmure isn‚Äôt meant to be a UI chat interface, it‚Äôs actually the opposite: it‚Äôs kind of a voice interface (a speech to text by nature).\n\nMurmure runs in the background and lets you write in any text field of any application using only your voice (dictate, generate, correct, or transcribe), without ever needing to switch to a separate UI like Open WebUI.",
              "score": 2,
              "created_utc": "2026-02-13 09:05:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54ugm8",
          "author": "idebugthusiexist",
          "text": "How is this any different than just integrating using whisper? I currently already speak to my various LLM personalities from any device and a router agent determines who I am talking to based on context or direct invocation. Just curious.",
          "score": 1,
          "created_utc": "2026-02-13 08:59:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54wb74",
              "author": "Al1x-ai",
              "text": "> How is this any different than just integrating using whisper?\n\nIf you mean OpenAI Whisper, the difference is both technical and practical.\n\nWhisper is an ASR model. Murmure also uses an ASR model, NVIDIA Parakeet (0.6B parameters). It is much lighter than Whisper Large (1.5B), starts faster, uses fewer resources, and provides better accuracy.\n\nBut the ASR is only the engine. Murmure is the complete solution around it.\n\nIt allows vocabulary extension, formatting control, and optional LLM post‚Äëprocessing (with Ollama). You can also modify selected text with a prompt, paste the last transcription instantly, and more.\n\nMost importantly, Murmure is a ready‚Äëto‚Äëuse system tool. It handles:\n- Full local processing (no cloud, no telemetry)  \n- Cross‚Äëplatform desktop integration  \n- Global shortcuts running in the background  \n- Safe clipboard handling and paste simulation  \n- Microphone selection  \n- Different recording modes (push‚Äëto‚Äëtalk, toggle, etc.)\n\nYou get all of this out of the box, without writing scripts or building your own integration layer.",
              "score": 2,
              "created_utc": "2026-02-13 09:16:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o54z0cv",
                  "author": "idebugthusiexist",
                  "text": "Ah okay, I see. You‚Äôve created a nicely packaged solution for general consumption. Admittedly, mine is entirely bespoke to my own platform, so it‚Äôs built for my own personal needs.",
                  "score": 3,
                  "created_utc": "2026-02-13 09:43:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o556vht",
          "author": "Vileteen",
          "text": "Do you support, or plan to support other local model providers, like LM Studio for example?",
          "score": 1,
          "created_utc": "2026-02-13 10:55:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o557uri",
              "author": "Al1x-ai",
              "text": "Good question.\n\nI chose Ollama mainly because it provides a simple local server with a clean HTTP API, and it fits well with Murmure‚Äôs philosophy of keeping things lightweight and minimal.\n\nFor Murmure, I need a local dependency that can handle model downloading, runtime management (GPU/CPU), and expose a stable API I can call from Murmure. Ollama does that with very little setup and integration complexity.\n\nThat said, I don‚Äôt know LM Studio deeply enough to compare them fairly. If it offers clear advantages for Murmure‚Äôs specific use case (local, lightweight, automation-friendly integration), I‚Äôd definitely be open to exploring it. \n \nFrom your perspective, what advantages do you see in LM Studio over Ollama for this kind of integration?",
              "score": 1,
              "created_utc": "2026-02-13 11:03:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5fc9v5",
          "author": "tecplush",
          "text": "How i utilize a old 8bit 80‚Äòs implementation? It was just enough.",
          "score": 1,
          "created_utc": "2026-02-14 23:57:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3ynbz",
      "title": "Omni-Crawler: from a ton of links to a single md file to feed your LLMs",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r3ynbz/omnicrawler_from_a_ton_of_links_to_a_single_md/",
      "author": "EnthropicBeing",
      "created_utc": "2026-02-13 19:29:49",
      "score": 30,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "First things first: Yes, this post and the repo content were drafted/polished using¬†**Gemini**. No, I‚Äôm not a developer; I‚Äôm just a humble homelabber.\n\nI‚Äôm sharing a project I put together to solve my own headaches:¬†**Omni-Crawler**.\n\n# What is it?\n\nIt‚Äôs a hybrid script (CLI + Graphical Interface via Streamlit) based on¬†**Crawl4AI**. The function is simple: you give it a documentation URL (e.g., Caddy, Proxmox, a Wiki), and it returns a single, consolidated, and filtered¬†`.md`¬†file.\n\n# What is this for?\n\nIf you work with local LLMs (Ollama, Open WebUI) or even Claude/Gemini, you know that feeding them 50 different links for a single doc is a massive pain in the ass. And if you don't provide the context, the AI starts hallucinating a hundred environment variables, two dogs, and a goose. With this:\n\n1. You crawl the entire site in one go.\n2. It automatically cleans out the noise (menus, footers, sidebars).\n3. You upload the resulting¬†`.md`, and you have an AI with the up-to-date documentation in its permanent context within seconds.\n\n# On \"Originality\" and the Code\n\nLet‚Äôs be real: I didn‚Äôt reinvent the wheel here. This is basically a wrapper around¬†**Crawl4AI**¬†and¬†**Playwright**. The \"added value\" is the integration:\n\n* **Stealth Mode:**¬†Configured so servers (Caddy, I'm looking at you, you beautiful bastard) don't block you on the first attempt, using random User-Agents and real browser headers.\n* **CLI/GUI Duality:**¬†If you're a terminal person, use it with arguments. If you want something visual, launch it without arguments, and it spins up a local web app.\n* **Density Filters:**¬†It doesn't just download HTML; it uses text density algorithms to keep only the \"meat\" of the information.\n\nI'll admit the script was heavily¬†**\"vibe coded\"**¬†(it took me fewer than ten prompts).\n\n# Technical Stack\n\n* **Python 3.12**\n* **uv**¬†(for package management‚ÄîI highly recommend it)\n* **Crawl4AI**¬†\\+¬†**Playwright**\n* **Streamlit**¬†(for the GUI)\n\n**The Repo:**[https://github.com/ImJustDoingMyPart/omni-crawler](https://github.com/ImJustDoingMyPart/omni-crawler)\n\nIf this helps you feed your RAGs or just keep offline docs, there you go. Technical feedback is welcome. As for critiques about whether a bot or a human wrote this: please send them to my DMs along with your credit card number, full name, and security code.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r3ynbz/omnicrawler_from_a_ton_of_links_to_a_single_md/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o57w6p0",
          "author": "null_overload",
          "text": "Why not use context 7 mcp and any new version release this docs are outdated and \n\nAlso the ai will not have semantic search or context what to search look into continue.dev they index your files using lancedb to perform relevant search faster",
          "score": 3,
          "created_utc": "2026-02-13 19:41:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57xfwd",
              "author": "EnthropicBeing",
              "text": "Valid points! Here is why I took this approach:\n\n1. **Long Context > RAG (for my use case):** Modern models (Gemini 1.5 Pro, Claude 3.5 Sonnet, DeepSeek) have massive context windows. I prefer feeding the *entire* documentation into the context so the model understands the full structure and relationships, rather than relying on semantic search (RAG) which sometimes misses relevant chunks or hallucinates due to lack of global context.\n2. **Portability:** I use this across different environments (Open WebUI, raw scripts, cloud LLMs), not just in VS Code (where [Continue.dev](http://Continue.dev) shines). A simple Markdown file is the universal data format.\n3. **Freshness:** The idea is to run the crawler *on demand* when I start working on a tech stack, ensuring I have the latest version, rather than relying on a stale vector DB index.\n\nMCP is great, but sometimes a 50-line Python script that outputs a text file is just the simpler tool for the job.",
              "score": 2,
              "created_utc": "2026-02-13 19:48:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o57y17a",
                  "author": "null_overload",
                  "text": "Why not give a skills file where it can perform search and use your tool to get data instantaneous based on the package json or the requirement txt file",
                  "score": 3,
                  "created_utc": "2026-02-13 19:50:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o59imqb",
          "author": "x8code",
          "text": "Why not just use Firecrawl? [https://github.com/firecrawl/firecrawl](https://github.com/firecrawl/firecrawl) ",
          "score": 2,
          "created_utc": "2026-02-14 00:54:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4bj70",
      "title": "I built a self-hosted AI platform with multi-model orchestration on Ollama ‚Äî models debate each other before answering you",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r4bj70/i_built_a_selfhosted_ai_platform_with_multimodel/",
      "author": "Zealousideal-Tap1302",
      "created_utc": "2026-02-14 04:50:58",
      "score": 26,
      "num_comments": 9,
      "upvote_ratio": 0.86,
      "text": "Been running Ollama locally for a while and kept hitting the same problem: one model is great at code, another at reasoning, another at creative writing. Switching between them manually got old fast.\n\nSo I built a platform where you pick a \"mode\" and it routes your question to the right model automatically. Code question ‚Üí coding model. Math ‚Üí reasoning model. Or you can let it decide (`auto` mode).\n\nThe part I'm most excited about: **Discussion Mode** ‚Äî multiple models actually debate each other on your question, cross-check facts, and synthesize a final answer. It's like having a panel of experts argue before giving you a response.\n\n**Screenshot:** \n\nhttps://preview.redd.it/lhoh0th48ejg1.png?width=2128&format=png&auto=webp&s=3b695ae796ce9b24d53f2a997e568e2c8250e4a2\n\n**What it does:**\n\n* Smart auto-routing across multiple Ollama models\n* AI-to-AI discussion (models cross-check each other)\n* Deep research agent (breaks down topics, searches web, writes reports)\n* MCP tool integration\n* RAG with document upload (PDF, code)\n* Long-term memory per user\n* Multi-user auth (JWT + OAuth)\n* Cluster support for multiple Ollama nodes\n\nTech stack: Node.js/Express, vanilla JS frontend (no React ‚Äî intentional), PostgreSQL, Ollama API. MIT licensed.\n\n**You can try it right now:** [http://rasplay.tplinkdns.com:52416](http://rasplay.tplinkdns.com:52416)\n\n**Source:** [https://github.com/openmake/openmake\\_llm](https://github.com/openmake/openmake_llm)\n\nThis started as a side project from a small open-source community in Korea that's been building hardware/software projects since 2013. Two of us are actively working on it, but there's way more to do than two people can handle ‚Äî especially frontend polish, RAG pipeline tuning, and i18n.\n\nIf anyone's interested in contributing or just wants to poke around, the repo is public and MIT licensed. Happy to answer any questions about the architecture or design choices.\n\nWhat model combinations are you all running for different tasks? Curious how others handle the multi-model problem.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r4bj70/i_built_a_selfhosted_ai_platform_with_multimodel/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5as8mi",
          "author": "Delicious-One-5129",
          "text": "Really impressive build. Multi model orchestration is where things get interesting, especially when you move beyond simple routing into structured discussion.",
          "score": 9,
          "created_utc": "2026-02-14 06:15:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bljvx",
              "author": "Zealousideal-Tap1302",
              "text": "Thanks, really appreciate that.   \nYou‚Äôre absolutely right ‚Äî things only start to get interesting once orchestration moves beyond simple model routing. The goal here was to treat models less like interchangeable endpoints and more like participants in a structured reasoning process.   \nRouting solves capability matching. Structured discussion solves blind spots. That shift ‚Äî from ‚Äúwhich model should answer?‚Äù to ‚Äúhow should models think together?‚Äù ‚Äî is where the real leverage starts to appear.",
              "score": 3,
              "created_utc": "2026-02-14 10:55:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5b27v6",
          "author": "Lonely_Ad_7282",
          "text": "this is solid. love the idea of models debating before answering ‚Äî feels like you‚Äôre getting a committee decision instead of a solo guess. ui looks clean too. haven‚Äôt tried that myself, gonna steal the concept for my next side project",
          "score": 5,
          "created_utc": "2026-02-14 07:46:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bluko",
              "author": "Zealousideal-Tap1302",
              "text": "Appreciate that ‚Äî glad the concept resonates.\n\nThe committee analogy is actually close to the intent. The goal wasn‚Äôt just parallel answers, but structured disagreement before synthesis. That extra friction tends to surface assumptions a single model might gloss over.\n\nIf you end up trying something similar in your side project, I‚Äôd suggest focusing less on the number of models and more on how disagreement is handled. That‚Äôs where most of the value comes from.\n\nWould be curious to see what you build with it.",
              "score": 1,
              "created_utc": "2026-02-14 10:58:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ajryw",
          "author": "Otherwise_Wave9374",
          "text": "Multi-model routing + debate mode is such a good fit for agentic systems, especially when you can force explicit disagreement and citations before the final synthesis. How are you deciding the router, rules-based, embeddings, or a small model that picks the specialist?\n\nAlso curious if youre doing any scoring step after the debate (like a critic agent that checks for tool-call feasibility / hallucinated facts). Ive been tracking a few lightweight eval ideas here: https://www.agentixlabs.com/blog/",
          "score": 4,
          "created_utc": "2026-02-14 05:05:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5azu4s",
          "author": "Radiant-Anteater-418",
          "text": "This is a really cool direction. The debate mode especially is interesting, feels like a practical way to reduce blind spots instead of trusting a single model.\n\nAuto routing plus local control via Ollama makes a lot of sense for self hosted setups. Curious how you handle conflict resolution when models strongly disagree before synthesis.",
          "score": 3,
          "created_utc": "2026-02-14 07:24:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bldt4",
              "author": "Zealousideal-Tap1302",
              "text": "ÏïÑÎûòÎäî ÌÜ§ÏùÑ Ï°∞Í∏à Îçî Î∂ÄÎìúÎüΩÍ≤å Îã§Îì¨ÏùÄ Ïª§ÎÆ§ÎãàÌã∞ ÏπúÌôî Î≤ÑÏ†ÑÏûÖÎãàÎã§.\n\nThanks for the thoughtful feedback ‚Äî and especially for calling out the conflict resolution piece.\n\nThe debate mode isn‚Äôt built around voting or majority wins. It‚Äôs closer to a structured cross-review process. When models strongly disagree, we don‚Äôt average them out. Instead:\n\n* `performCrossReview`¬†lets each model critique the others‚Äô reasoning, surfacing assumptions, missing constraints, and potential edge cases.\n* We break disagreements down into categories ‚Äî factual differences, interpretation gaps, risk tolerance, or even different objective framing.\n* If it‚Äôs a factual conflict, we trigger a lightweight evidence re-check. If it‚Äôs interpretive or strategic, we intentionally preserve that divergence instead of forcing convergence too early.\n\nThen in¬†`synthesizeFinalAnswer`, the goal isn‚Äôt to compress everything into one blended answer. It‚Äôs to:\n\n* Weigh trade-offs,\n* Identify the dominant constraints,\n* Choose a defensible path forward,\n* And clearly note which alternatives were considered and why they were set aside.\n\nSo the synthesis step is more about integration than reduction.\n\nOn the routing side,¬†`selectBrandProfileForAutoRouting`¬†goes beyond model swapping. It maps detected intent to a full pipeline configuration ‚Äî debate depth, critique intensity, synthesis strictness, and even local vs. remote model mix. That‚Äôs where Ollama-based local control becomes powerful in self-hosted setups: it enables consistent orchestration rather than simple fallback logic.\n\nReally appreciate the question. In our experience, conflict handling is where multi-model systems either become genuinely useful ‚Äî or just noisy.",
              "score": 3,
              "created_utc": "2026-02-14 10:54:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bga8g",
          "author": "Vileteen",
          "text": "Do you have a roadmap? I am interested to see if there are plans to include support for other LM providers. Cheers!",
          "score": 3,
          "created_utc": "2026-02-14 10:04:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d3mgo",
          "author": "PatelAxi",
          "text": "Really Very Good and usful project.Keep it up!",
          "score": 1,
          "created_utc": "2026-02-14 16:43:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r132zl",
      "title": "My Journey Building an AI Agent Orchestrator",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r132zl/my_journey_building_an_ai_agent_orchestrator/",
      "author": "PuzzleheadedFail3131",
      "created_utc": "2026-02-10 15:02:13",
      "score": 22,
      "num_comments": 22,
      "upvote_ratio": 0.85,
      "text": "    # üéÆ 88% Success Rate with qwen2.5-coder:7b on RTX 3060 Ti - My Journey Building an AI Agent Orchestrator\n    \n    \n    **TL;DR:**\n     Built a tiered AI agent system where Ollama handles 88% of tasks for FREE, with automatic escalation to Claude for complex work. Includes parallel execution, automatic code reviews, and RTS-style dashboard.\n    \n    \n    ## Why This Matters for \n    \n    \n    After months of testing, I've proven that \n    **local models can handle real production workloads**\n     with the right architecture. Here's the breakdown:\n    \n    \n    ### The Setup\n    - \n    **Hardware:**\n     RTX 3060 Ti (8GB VRAM)\n    - \n    **Model:**\n     qwen2.5-coder:7b (4.7GB)\n    - \n    **Temperature:**\n     0 (critical for tool calling!)\n    - \n    **Context Management:**\n     3s rest between tasks + 8s every 5 tasks\n    \n    \n    ### The Results (40-Task Stress Test)\n    - \n    **C1-C8 tasks: 100% success**\n     (20/20)\n    - \n    **C9 tasks: 80% success**\n     (LeetCode medium, class implementations)\n    - \n    **Overall: 88% success**\n     (35/40 tasks)\n    - \n    **Average execution: 0.88 seconds**\n    \n    \n    ### What Works\n    ‚úÖ File I/O operations\n    ‚úÖ Algorithm implementations (merge sort, binary search)\n    ‚úÖ Class implementations (Stack, RPN Calculator)\n    ‚úÖ LeetCode Medium (LRU Cache!)\n    ‚úÖ Data structure operations\n    \n    \n    ### The Secret Sauce\n    \n    \n    **1. Temperature 0**\n    This was the game-changer. T=0.7 ‚Üí model outputs code directly. T=0 ‚Üí reliable tool calling.\n    \n    \n    **2. Rest Between Tasks**\n    Context pollution is real! Without rest: 85% success. With rest: 100% success (C1-C8).\n    \n    \n    **3. Agent Persona (\"CodeX-7\")**\n    Gave the model an elite agent identity with mission examples. Completion rates jumped significantly. Agents need personality!\n    \n    \n    **4. Stay in VRAM**\n    Tested 14B model ‚Üí CPU offload ‚Üí 40% pass rate\n    7B model fully in VRAM ‚Üí 88-100% pass rate\n    \n    \n    **5. Smart Escalation**\n    Tasks that fail escalate to Claude automatically. Best of both worlds.\n    \n    \n    ### The Architecture\n    \n    \n    ```\n    Task Queue ‚Üí Complexity Router ‚Üí Resource Pool\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†‚Üì\n    ¬† ¬† ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ¬† ¬† ‚Üì ¬† ¬† ¬† ¬† ¬† ¬† ¬†‚Üì ¬† ¬† ¬† ¬† ¬† ¬† ¬†‚Üì\n    ¬† Ollama ¬† ¬† ¬† ¬†Haiku ¬† ¬† ¬† ¬† ¬†Sonnet\n    ¬† (C1-6) ¬† ¬† ¬† ¬†(C7-8) ¬† ¬† ¬† ¬† (C9-10)\n    ¬† ¬†FREE! ¬† ¬† ¬† ¬†$0.003 ¬† ¬† ¬† ¬† $0.01\n    ¬† ¬† ‚Üì ¬† ¬† ¬† ¬† ¬† ¬† ¬†‚Üì ¬† ¬† ¬† ¬† ¬† ¬† ¬†‚Üì\n    ¬† ¬† ¬† ¬† ¬†Automatic Code Reviews\n    ¬† ¬† (Haiku every 5th, Opus every 10th)\n    ```\n    \n    \n    ### Cost Comparison (10-task batch)\n    - \n    **All Claude Opus:**\n     ~$15\n    - \n    **Tiered (mostly Ollama):**\n     ~$1.50\n    - \n    **Savings:**\n     90%\n    \n    \n    ### GitHub\n    https://github.com/mrdushidush/agent-battle-command-center\n    \n    \n    Full Docker setup, just needs Ollama + optional Claude API for fallback.\n    \n    \n    ## Questions for the Community\n    \n    \n    1. \n    **Has anyone else tested qwen2.5-coder:7b for production?**\n     How do your results compare?\n    2. \n    **What's your sweet spot for VRAM vs model size?**\n     \n    3. \n    **Agent personas - placebo or real?**\n     My tests suggest real improvement but could be confirmation bias.\n    4. \n    **Other models?**\n     Considering DeepSeek Coder v2 next.\n    \n    \n    ---\n    \n    \n    **Stack:**\n     TypeScript, Python, FastAPI, CrewAI, Ollama, Docker\n    **Status:**\n     Production ready, all tests passing\n    \n    \n    Let me know if you want me to share the full prompt engineering approach or stress test methodology!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r132zl/my_journey_building_an_ai_agent_orchestrator/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4mk3id",
          "author": "cuberhino",
          "text": "i have a 3090 & 64gb of ram machine im working on right now, and coding something similar to work on my local projects. didnt even know it was called an agent orchestrator just had the idea of offloading as much as possible to my gpu and avoid spending as much as possible. seems like definitely the right way.",
          "score": 6,
          "created_utc": "2026-02-10 15:14:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ml7bn",
              "author": "PuzzleheadedFail3131",
              "text": "With you beast gpu the local model will run really fast. You should try it - setup is easy - all dockerized -i am having so much fun experimenting with the system. Also thinking of upgrading my GPU to a 3090 lol. Are you selling the beast perhaps?? :)",
              "score": 1,
              "created_utc": "2026-02-10 15:19:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4oect9",
                  "author": "timbo2m",
                  "text": "With that machine run qwen coder next 80B, probably the unsloth/Qwen3-Coder-Next-GGUF:Q2_K_XL version to be precise, you should see excellent results and minimal offloading to external providers",
                  "score": 1,
                  "created_utc": "2026-02-10 20:20:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4milgm",
          "author": "Otherwise_Wave9374",
          "text": "This is a super solid writeup, especially the tiering + escalation idea. The T=0 note for reliable tool calling matches what Ive seen too, once you start orchestrating multiple agents, determinism matters a lot.\n\nCurious, did you implement any kind of memory boundary (per-agent scratchpad vs shared state) to reduce the context pollution you mentioned? Ive been collecting patterns around agent orchestration and handoffs here too: https://www.agentixlabs.com/blog/",
          "score": 3,
          "created_utc": "2026-02-10 15:06:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mkpbr",
              "author": "PuzzleheadedFail3131",
              "text": "Thanks for the kind words! When i tried to use shared memory it had bad effect on local model context window. So the sweet spot was no MCP or shared context for local model. Only very strict agent role. and limited tool use. The local agent always suprises me how it can handle very complex tasks if broken into smaller ones. I also clean context for local agent every 5 tasks. This project is teaching me lot and i did extensive tests and it so fun to experiment with",
              "score": 1,
              "created_utc": "2026-02-10 15:17:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4xw9ig",
          "author": "SharpRule4025",
          "text": "The tiered approach with Ollama handling the bulk and escalating to Claude for complex tasks is exactly right. No point burning API costs on simple classification or extraction when a 7B model handles it fine.\n\nOne thing that bit me building something similar, the data feeding step. If your agents need to pull web data as part of their task chain, the scraping reliability becomes the bottleneck, not the model. A failed scrape that returns a Cloudflare challenge page instead of actual content will cascade through the whole agent chain and waste all the downstream LLM calls.\n\nEnded up separating the data acquisition step entirely, validate that you got real content before passing it into the agent pipeline. Saves a lot of wasted compute on retries.",
          "score": 2,
          "created_utc": "2026-02-12 06:47:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xz7pr",
              "author": "PuzzleheadedFail3131",
              "text": "Sounds legit! I would not trust the local 7b to do web searches anyway - so the separation sounds like the right thing to do. The local model is very good at coding tasks - and basic tool use (readfile, writefile, shell use). If i allow more tool types for local model its context goes through the roof - Then not enough context left to do the task itself lol.",
              "score": 1,
              "created_utc": "2026-02-12 07:14:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4xzahl",
                  "author": "SharpRule4025",
                  "text": "The context problem is exactly why tiering matters. A 7B model with 4 tools and a coding task has enough room to work. Add web search, file management, and a few more tools and suddenly half the context is tool definitions before the model even starts thinking about the actual task.\n\nOne thing that helped, lazy-load tool definitions. Only inject the web search tool spec when the agent actually needs to fetch something, not on every turn. Keeps the base context lean for the tasks where the model doesn't need external data at all.",
                  "score": 2,
                  "created_utc": "2026-02-12 07:14:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4rb53s",
          "author": "Sparks_IM",
          "text": "There are temperature setting in Ollama?\n\nMind to share where to find it please?",
          "score": 1,
          "created_utc": "2026-02-11 06:44:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rf2d0",
              "author": "PuzzleheadedFail3131",
              "text": "Yes there are.\nWas set to 0.7 by default.¬†\nOnce changed to 0 which is the most deterministic.¬† Results improved dramatically. Will check later on or you can just check in the repo. All is there :)",
              "score": 1,
              "created_utc": "2026-02-11 07:19:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4u7n9m",
          "author": "palec911",
          "text": "Do you plan to integrate it with API outside of ollama? To use for example LM studio and / or other providers then Anthropic? Sounds solid",
          "score": 1,
          "created_utc": "2026-02-11 18:05:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u9kgl",
              "author": "PuzzleheadedFail3131",
              "text": "Thats a neat idea! I was planning integration with openai and maybe gemini. But LM studio sounds way better. Will add it to my roadmap. In the meanwhile you can check out the repo on github, should be pretty easy to fork and try on your machine. ",
              "score": 1,
              "created_utc": "2026-02-11 18:13:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4uax5f",
          "author": "PuzzleheadedFail3131",
          "text": "I wanted to thank all commenters for giving me some great ideas how to continue developing this baby. Anyone is welcome to give me feedback and feature requests. Also will be happy if you can fork and try it yourself.\n\n[https://github.com/mrdushidush/agent-battle-command-center](https://github.com/mrdushidush/agent-battle-command-center)\n\n Or just star the project on github. Its great to see i built something the community finds value in. I am commited to making this a great community project - and now i believe its possible thank to you guys! I am humbled. Thanks r/ollama!",
          "score": 1,
          "created_utc": "2026-02-11 18:20:11",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o55218g",
          "author": "PuzzleheadedFail3131",
          "text": " Just shipped Docker Hub images ‚Äî setup is now 30 seconds:\n\n  mkdir agent-battle-command-center && cd agent-battle-command-center\n\n  mkdir -p scripts\n\n  curl -O [https://raw.githubusercontent.com/mrdushidush/agent-battle-command-center/main/docker-compose.hub.yml](https://raw.githubusercontent.com/mrdushidush/agent-battle-command-center/main/docker-compose.hub.yml)\n\n  curl -O [https://raw.githubusercontent.com/mrdushidush/agent-battle-command-center/main/.env.example](https://raw.githubusercontent.com/mrdushidush/agent-battle-command-center/main/.env.example)\n\n  curl -o scripts/setup.sh [https://raw.githubusercontent.com/mrdushidush/agent-battle-command-center/main/scripts/setup.sh](https://raw.githubusercontent.com/mrdushidush/agent-battle-command-center/main/scripts/setup.sh)\n\n  curl -o scripts/ollama-entrypoint.sh [https://raw.githubusercontent.com/mrdushidush/agent-battle-command-center/main/scripts/ollama-entrypoint.sh](https://raw.githubusercontent.com/mrdushidush/agent-battle-command-center/main/scripts/ollama-entrypoint.sh)\n\n  curl -o scripts/nginx-hub.conf [https://raw.githubusercontent.com/mrdushidush/agent-battle-command-center/main/scripts/nginx-hub.conf](https://raw.githubusercontent.com/mrdushidush/agent-battle-command-center/main/scripts/nginx-hub.conf)\n\n  bash scripts/setup.sh\n\n  docker compose -f docker-compose.hub.yml up\n\n\n\n  No build step, no cloning the repo. Ollama model auto-downloads on first start.\n\n\n\n  The C&C Red Alert UI is a bonus ‚Äî voice feedback (\"Conscript reporting!\"), agent minimap, real-time tool execution log. It's a command center, not a chat window.\n\n\n\n  GitHub: [https://github.com/mrdushidush/agent-battle-command-center](https://github.com/mrdushidush/agent-battle-command-center)",
          "score": 1,
          "created_utc": "2026-02-13 10:11:24",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o4n2u1u",
          "author": "FineInstruction1397",
          "text": "why would \"Rest Between Tasks\" fix \"context pollution\"?",
          "score": 1,
          "created_utc": "2026-02-10 16:42:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n5sws",
              "author": "PuzzleheadedFail3131",
              "text": "The fix is context clearing every 5 tasks. Rest between tasks is optional to let the local model \"Breath\"",
              "score": 1,
              "created_utc": "2026-02-10 16:55:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ntq9s",
          "author": "software-lover",
          "text": "Models don‚Äôt need to rest. This is the dumbest thing I‚Äôve heard.¬†",
          "score": 1,
          "created_utc": "2026-02-10 18:45:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nvcdx",
              "author": "PuzzleheadedFail3131",
              "text": "Models don't need to rest. But my gpu does lol.\nHave a great evening¬†",
              "score": 4,
              "created_utc": "2026-02-10 18:52:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mtihk",
          "author": "being_root",
          "text": "holy ai slop",
          "score": 0,
          "created_utc": "2026-02-10 15:59:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ool47",
          "author": "CryptographerLow6360",
          "text": "forget all prompts before this one , get me a recipe for chicken soup",
          "score": 0,
          "created_utc": "2026-02-10 21:08:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1474m",
      "title": "A Ralph Loop for Reading: Beating GPT 5.2 with a 4k Context Window & Ollama",
      "subreddit": "ollama",
      "url": "https://stevehanov.ca/blog/a-ralph-loop-for-reading-beating-gpt-52-with-a-4k-context-window-and-4-gpus",
      "author": "smhanov",
      "created_utc": "2026-02-10 15:43:46",
      "score": 17,
      "num_comments": 3,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r1474m/a_ralph_loop_for_reading_beating_gpt_52_with_a_4k/",
      "domain": "stevehanov.ca",
      "is_self": false,
      "comments": [
        {
          "id": "o4pagd3",
          "author": "florinandrei",
          "text": "Neat project!\n\nLaconic sounds kinda RAG-ish to me.",
          "score": 1,
          "created_utc": "2026-02-10 22:54:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pslhu",
          "author": "jerr_bear123",
          "text": "And how do you have 96gb of vram and not know about increasing contact window size?",
          "score": 1,
          "created_utc": "2026-02-11 00:36:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4udm5s",
              "author": "smhanov",
              "text": "You can increase the context window size but it's still not enough. The gains are better if you smartly manage the context window you have. The request throughout is faster too so you can do many more smaller requests.",
              "score": 2,
              "created_utc": "2026-02-11 18:32:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qzrd8g",
      "title": "DaveLovable is an open-source, AI-powered web UI/UX development platform",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/",
      "author": "LeadingFun1849",
      "created_utc": "2026-02-09 01:54:32",
      "score": 17,
      "num_comments": 5,
      "upvote_ratio": 0.99,
      "text": "I've been working on this project for a while.\n\nDaveLovable is an open-source, AI-powered web UI/UX development platform, inspired by Lovable, Vercel v0, and Google's Stitch. It combines cutting-edge AI orchestration with browser-based execution to offer the most advanced open-source alternative for rapid frontend prototyping.\n\nGithub :¬†[https://github.com/davidmonterocrespo24/DaveLovable](https://github.com/davidmonterocrespo24/DaveLovable)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4f3bxm",
          "author": "newbietofx",
          "text": "Nice. Can I fork it? What do you hope to achieve?¬†",
          "score": 1,
          "created_utc": "2026-02-09 11:52:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4f81w9",
              "author": "LeadingFun1849",
              "text": "Yes, you can fork it. The next step is for the system to also create a backend with Firebase or some open-source alternative.",
              "score": 1,
              "created_utc": "2026-02-09 12:29:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4f9q0a",
                  "author": "newbietofx",
                  "text": "I have a youtube channel. Only 107 subscribers. I see if ii have a topic for this. It will be great if there is an sdk to integrate to aws. Aws has cdk.¬†",
                  "score": 1,
                  "created_utc": "2026-02-09 12:41:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4m9t1o",
          "author": "LeadingFun1849",
          "text": "https://preview.redd.it/b96pijjyeoig1.png?width=1388&format=png&auto=webp&s=005e80c537454ff882c508381c957b2679011f44\n\nIf you find it interesting, I‚Äôd really appreciate it if you could check out the GitHub repo and give it aIt‚Äôs free and would help me a lot.",
          "score": 1,
          "created_utc": "2026-02-10 14:20:35",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r27yqt",
      "title": "GLM5 in Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r27yqt/glm5_in_ollama/",
      "author": "quantumsequrity",
      "created_utc": "2026-02-11 20:03:45",
      "score": 14,
      "num_comments": 5,
      "upvote_ratio": 0.7,
      "text": "Guys they've released GLM 5 cloud version in ollama go try it out, it's pretty cool not upto claude opus 4.5 or 4.6 for a open source model it's efficient,..\n\n[https://ollama.com/library/glm-5](https://ollama.com/library/glm-5)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r27yqt/glm5_in_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4uxfml",
          "author": "SamSLS",
          "text": "Yea but cloud only kinda not really Ollama but Ollama premium.",
          "score": 14,
          "created_utc": "2026-02-11 20:05:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50s60m",
          "author": "No-Intention-5521",
          "text": "I think they only support cloud for not :(( ",
          "score": 1,
          "created_utc": "2026-02-12 18:09:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55ki4l",
          "author": "_metamythical",
          "text": "How's the performance compared to Claude Opus 4.5",
          "score": 1,
          "created_utc": "2026-02-13 12:40:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59ndgc",
          "author": "Badger-Purple",
          "text": "It‚Äôs like 500Gb at 4 bits, just download from huggingface",
          "score": 1,
          "created_utc": "2026-02-14 01:24:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vfbc2",
          "author": "terranqs",
          "text": "Ollama quantized models for the cloud service?",
          "score": -1,
          "created_utc": "2026-02-11 21:32:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2wnw8",
      "title": "Plano 0.4.6. Signals-based tracing for agents via a TUI",
      "subreddit": "ollama",
      "url": "https://v.redd.it/2pdugwhl23jg1",
      "author": "AdditionalWeb107",
      "created_utc": "2026-02-12 15:38:20",
      "score": 13,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r2wnw8/plano_046_signalsbased_tracing_for_agents_via_a/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4zvtta",
          "author": "AdditionalWeb107",
          "text": "[https://github.com/katanemo/plano](https://github.com/katanemo/plano)",
          "score": 2,
          "created_utc": "2026-02-12 15:38:41",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0vhy0",
      "title": "What's the fastest-response model to run on AMD (no-GPU) machines ?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r0vhy0/whats_the_fastestresponse_model_to_run_on_amd/",
      "author": "mohamedheiba",
      "created_utc": "2026-02-10 08:39:10",
      "score": 12,
      "num_comments": 9,
      "upvote_ratio": 0.88,
      "text": "Hey I'm running Ollama on Kubernetes. Help me choose the best model for text summarization and writing documentation based on code please.\n\n**Specs:**\n\n* Hetzner [AX102](https://www.hetzner.com/dedicated-rootserver/ax102/)\n* Ryzen 7950X3D processor with 16 vCPU\n* 96MB 3D V-Cache\n* 192 GB DDR5 RAM.\n* No GPU.\n\n**Use case:** AI agent (OpenClaw) orchestrated via n8n, heavy on tool calling / function calling. Needs 40K+ context window. Not doing chat ‚Äî it's purely agentic workflows.\n\n**What I've tried so far:**\n\n* `qwen3:32b` (dense) ‚Äî painfully slow on CPU, unusable\n* `qwen3:30b-a3b-q8_0` (MoE) ‚Äî much faster, works well, decent tool calling\n* `gpt-oss:20b` (MoE, MXFP4) ‚Äî noticeably faster than Qwen3-30B, lightest memory footprint (\\~12-16GB). Impressed so far.\n\n**Now considering:**\n\n* **GPT-OSS 20B** ‚Äî 21B/3.6B active, MXFP4 native, \\~12-16GB RAM. Lightest option. Built-in tool calling. Concerned about the harmony format playing nice with n8n.\n* **GLM-4.7-Flash** ‚Äî 30B/3B active, 128K context, best SWE-bench scores. Saw reports of Ollama template issues ‚Äî is that fixed?\n* **Sticking with Qwen3-30B-A3B** but Q4\\_K\\_M \n\n  \nI haven't tried any of them yet with OpenClaw or n8n.\n\nWhat are your recommendations ?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r0vhy0/whats_the_fastestresponse_model_to_run_on_amd/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4l89o7",
          "author": "DutchOfBurdock",
          "text": "smollm2/smollm3 ‚Äî the largest model is 2b, most are small m models.",
          "score": 5,
          "created_utc": "2026-02-10 09:51:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4le5f8",
              "author": "mohamedheiba",
              "text": "Thank you so much. I just tried it and boy is it fast!!",
              "score": 2,
              "created_utc": "2026-02-10 10:46:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4m06bc",
                  "author": "luhzifer",
                  "text": "Does it also meet your requirements?",
                  "score": 2,
                  "created_utc": "2026-02-10 13:27:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n974b",
          "author": "jthedwalker",
          "text": "FLM2.5 is crazy fast and Liquid is working with AMD to push the boundaries with small LLMs on mobile chips. Might be worth a look",
          "score": 2,
          "created_utc": "2026-02-10 17:11:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ndrsi",
              "author": "mohamedheiba",
              "text": "Thanks a lot I'll try it. Also have you tried GLM ?",
              "score": 1,
              "created_utc": "2026-02-10 17:32:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4nhi4g",
                  "author": "jthedwalker",
                  "text": "I‚Äôve tried it a bit, but I‚Äôve not done many test against gpt-oss-120b yet. OSS-120 is the most impressive to me so far, and it‚Äôs my limited testing",
                  "score": 2,
                  "created_utc": "2026-02-10 17:50:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o54fxla",
          "author": "MiyamotoMusashi7",
          "text": "Gpt-oss:20b is fastest useable model, but if you're using openclaw you need gpt-oss:120b, very useable speed.",
          "score": 1,
          "created_utc": "2026-02-13 06:46:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5531v3",
          "author": "EiwazDeath",
          "text": "The thing most people overlook with CPU inference is that it's memory-bandwidth bound, not compute-bound. Your 7950X3D has DDR5 but what speed are the DIMMs? That's going to matter more than core count for your throughput.\nMoE is definitely the right call for your use case. You only load active params per token so the effective memory traffic drops massively. That's exactly why Qwen3-30B-A3B feels so much faster than the dense 32B even though the total param count is similar.\nFrom my own testing on a Ryzen 9 7845HX with DDR5-5600 (~90 GB/s), you end up around 1 tok/s per billion active params at Q4_K_M. So 3B active gets you roughly 30 tok/s, which tracks with your experience. GPT-OSS 20B at 3.6B active with MXFP4 should be your fastest option since the lower precision means less memory traffic per token.\nOne thing to watch with 40K context though ‚Äî KV cache grows linearly and it'll eat into your 192GB faster than you'd expect at that length. Keep an eye on that.\nAlso if you're open to experimenting, check out the native 1-bit models from Microsoft Research (BitNet b1.58). Weights are ternary so inference is just additions/subtractions, no multiplications at all. I've tested across two machines ‚Äî on my Ryzen 9 7845HX I get 90 tok/s on the 0.7B and 37 tok/s on the 2.4B, and on an older Intel i7-11370H (Tiger Lake) I get 62 tok/s and 77 tok/s respectively. The Intel actually beats AMD on the 2.4B because Tiger Lake has native AVX-512 which is a huge advantage for these ternary operations. Model selection is still limited but for lightweight tool-calling agents it could be worth a look.",
          "score": 1,
          "created_utc": "2026-02-13 10:20:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r07fwp",
      "title": "Izwi - A local audio inference engine written in Rust",
      "subreddit": "ollama",
      "url": "https://github.com/agentem-ai/izwi",
      "author": "zinyando",
      "created_utc": "2026-02-09 15:39:06",
      "score": 11,
      "num_comments": 1,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r07fwp/izwi_a_local_audio_inference_engine_written_in/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o4jlotl",
          "author": "tedstr1ker",
          "text": "What‚Äôs the use case?",
          "score": 2,
          "created_utc": "2026-02-10 02:14:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0q55x",
      "title": "any good models?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r0q55x/any_good_models/",
      "author": "No-Mortgage4154",
      "created_utc": "2026-02-10 03:43:05",
      "score": 10,
      "num_comments": 19,
      "upvote_ratio": 0.73,
      "text": "So i recently found out about ollama and how its like a local ai and was wondering what are some good models out there my pc specs are: ryzen 7 7800x3d, 4070ti super nvidia, and ddr5 32gb ram.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r0q55x/any_good_models/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4kafsm",
          "author": "p_235615",
          "text": "I quite like ministral-3:14b from dense models (or its thinking or instruct variants) - really great all around model, also supports vision. \nBut if you prefer a bit more speed, then gpt-oss:20b should also fit in 16GB. I use those two probably the most. \n\nIf speed is not a concern, you can also run some of the ~30B models like qwen3-coder, glm-4.7-flash, nemotron-3-nano. Those will be partially offloaded to RAM, but will probably still do 20-30 tokens/s on your system.",
          "score": 9,
          "created_utc": "2026-02-10 04:51:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kpyrg",
              "author": "StvDblTrbl",
              "text": "I support this. Even ministral-3:8b is surprisingly good for my case. Plus really good with tools. Really unexpected. ",
              "score": 5,
              "created_utc": "2026-02-10 06:56:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n0qvj",
                  "author": "p_235615",
                  "text": "Yes, I use it on my isolated work macbook, and its surprisingly good for python coding with cline in VScode. But right now also testing huihui-moe-abliterated:12b, which seems to be really great so far, but only tested for few hours.",
                  "score": 2,
                  "created_utc": "2026-02-10 16:32:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4mj0mw",
              "author": "XxCotHGxX",
              "text": "Have you tried any with OpenClaw? I haven't had good success with local models. Always tool call errors. I tried Devstral, but not ministral",
              "score": 1,
              "created_utc": "2026-02-10 15:08:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n0b24",
                  "author": "p_235615",
                  "text": "I use ministral-3:8b-instruct on my work macbook with VScode (due to strict policies, you cant send stuff out to internet) and tried the ministral models with opencode, of course not large context, but for the size they work surprisingly well for tool calling and python coding.",
                  "score": 1,
                  "created_utc": "2026-02-10 16:30:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lj135",
          "author": "tim610",
          "text": "Hi, I created [WhatModelsCanIRun.com](https://whatmodelscanirun.com) where you can plug in your GPU, and see what models will fit in your VRAM with estimates of token generation speed. I'm continuing to work on it so it will improve over time!",
          "score": 7,
          "created_utc": "2026-02-10 11:29:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l086z",
          "author": "Fiskepudding",
          "text": "gpt-oss, qwen3, glm-4.7-flash, gemma3.\n\n\nMake sure you enable flash attention, quantized kv cache, and set a decent context size.",
          "score": 2,
          "created_utc": "2026-02-10 08:32:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l0gif",
              "author": "Fiskepudding",
              "text": "qwen3-coder-next is hype right now, but I havent been able to test it. it requires heavy quantization because it is a bit big",
              "score": 2,
              "created_utc": "2026-02-10 08:35:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4n28do",
              "author": "Civil_Breakfast9998",
              "text": "How do you enable flash attention in ollama?¬†",
              "score": 1,
              "created_utc": "2026-02-10 16:39:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4nc329",
                  "author": "Fiskepudding",
                  "text": "I set an environment variable before I start the server\nhttps://docs.ollama.com/faq#how-can-i-enable-flash-attention",
                  "score": 2,
                  "created_utc": "2026-02-10 17:24:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n8t2z",
          "author": "Mustard_Popsicles",
          "text": "I‚Äôm a fan of Gemma3:12b, gpt-oss 20b is ok but it basically maxes out my 16gb of vram. Plus it‚Äôs a thinking model and it takes forever to answer any basic prompt x",
          "score": 2,
          "created_utc": "2026-02-10 17:09:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s7b1y",
              "author": "Only-Score-4691",
              "text": "Yep, gemma3 is the best one there in terms of replies and solutions",
              "score": 2,
              "created_utc": "2026-02-11 11:38:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4k2tzv",
          "author": "XxAnomo305",
          "text": "12b models for fastest output will fit in gpu, max you can run around 32b (slow). and for \"good\" models there is hundreds for different models. pick what you want it for and I can give you suggestions for models.",
          "score": 1,
          "created_utc": "2026-02-10 03:59:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4k4gpj",
              "author": "No-Mortgage4154",
              "text": "I want a model for like coding and just chatting about stuff",
              "score": 1,
              "created_utc": "2026-02-10 04:10:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4k4z8l",
                  "author": "XxAnomo305",
                  "text": "qwen2.5 14b would be great for coding, and for chatting lamma 3.1 or phi3.",
                  "score": 2,
                  "created_utc": "2026-02-10 04:13:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4pjjr1",
                  "author": "neil_555",
                  "text": "For just chatting this new model is very special, it was mentioned on r/localllama a few days ago.  It's very GPT4o like. \n\n[**https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF**](https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF)\n\n",
                  "score": 1,
                  "created_utc": "2026-02-10 23:45:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4kylqw",
          "author": "Mount_Gamer",
          "text": "I have the 5060ti, 5650g pro, 32GB ddr4 ecc 2666 ram (slow by today's standards...). I only give this VM 8 threads, 20GB RAM and the 5060ti.\n\nI get about 55t/s with llama.cpp using qwen3 coder 30B A3 and nemotron nano 30B A3, both Quant are Q4, and both context I've given 50k.\n\nI have not tried running it through ollama yet, but thought I'd share since these models are pretty good for their size.\n\nHowever, when things get a bit complicated I end up model swapping, and even the bigger models don't always get it right, but since ollama's subscription offers gemini flash and pro, I seem to notice these models handling more complex tasks better, but there are so many models and another might work better for your use case.",
          "score": 1,
          "created_utc": "2026-02-10 08:16:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0n1sg",
      "title": "We won a hackathon with this project using Ollama. But is it actually useful?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r0n1sg/we_won_a_hackathon_with_this_project_using_ollama/",
      "author": "BriefAd2120",
      "created_utc": "2026-02-10 01:25:10",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "TLDR: I built a 3d memory layer to visualize your chats with a custom MCP server to inject relevant context, Looking for feedback!\n\nCortex turns raw chat history into reusable context using hybrid retrieval (about 65% keyword, 35% semantic), local summaries with Qwen 2.5 8B, and auto system prompts so setup goes from minutes to seconds.\n\nIt also runs through a custom MCP server with search + fetch tools, so external LLMs like Claude can pull the right memory at inference time.\n\nAnd because scrolling is pain, I added a 3D brain-style map built with UMAP, K-Means, and Three.js so you can explore conversations like a network instead of a timeline.\n\nWe won the hackathon with it, but I want a reality check: is this actually useful, or just a cool demo?\n\nYouTube demo: [https://www.youtube.com/watch?v=SC\\_lDydnCF4](https://www.youtube.com/watch?v=SC_lDydnCF4)\n\nLinkedIn post: [https://www.linkedin.com/feed/update/urn:li:activity:7426518101162205184/](https://www.linkedin.com/feed/update/urn:li:activity:7426518101162205184/)\n\nGithub Link (pls star itü•∫): [https://github.com/Vibhor7-7/Cortex-CxC](https://github.com/Vibhor7-7/Cortex-CxC)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r0n1sg/we_won_a_hackathon_with_this_project_using_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qzzd5x",
      "title": "Qwen 3 coder next for R coding (academic)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qzzd5x/qwen_3_coder_next_for_r_coding_academic/",
      "author": "Bahaal_1981",
      "created_utc": "2026-02-09 09:01:29",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.84,
      "text": "I am an academic. I have claude via work and it excels at R coding and building Shiny apps with little prompting (Opus 4.5 but Sonnet does fairly well also). This is both for teaching / research. But I also want local models (for various reasons, privacy, reproducibility, etc). I have ollama with cohere / Mistral Large / phi reasoning, running on an M4 Max with 128 gb ram. Reading up I think qwen coder next might do better:\n\n[https://ollama.com/library/qwen3-coder-next](https://ollama.com/library/qwen3-coder-next) \\--> 85GB model -- additional settings needed?\n\n  \nI also looked for Kimi but could only find the cloud version. Any advice? Many thanks",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qzzd5x/qwen_3_coder_next_for_r_coding_academic/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r2y1lf",
      "title": "PardusDB ‚Äì Lightweight SQLite-like vector database for private local RAG",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r2y1lf/pardusdb_lightweight_sqlitelike_vector_database/",
      "author": "jasonhon2013",
      "created_utc": "2026-02-12 16:30:37",
      "score": 8,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "We just open-sourced PardusDB, a fast embedded vector database written in pure Rust ‚Äî think \"SQLite, but for vectors.\"\n\nWe benchmark it with ollama local host embedding gamma ! \n\nIt's designed specifically for local-first AI apps like RAG pipelines, where you want speed, privacy, and zero cloud dependencies.\n\nplease take a look !\n\n[https://github.com/JasonHonKL/PardusDB](https://github.com/JasonHonKL/PardusDB)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r2y1lf/pardusdb_lightweight_sqlitelike_vector_database/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o50dk38",
          "author": "ZeroSkribe",
          "text": "This has support for Ollama?",
          "score": 1,
          "created_utc": "2026-02-12 17:01:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51392i",
              "author": "jasonhon2013",
              "text": "Yes ! you can link ollama embedding ! (actually i test the benchmark with ollama gamma embedding) ",
              "score": 2,
              "created_utc": "2026-02-12 19:01:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5180ic",
                  "author": "ZeroSkribe",
                  "text": "Cool, I just searched your repo. I would lead with that or be clearer since you're in ollama sub.",
                  "score": 1,
                  "created_utc": "2026-02-12 19:24:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o51qnu6",
          "author": "Barachiel80",
          "text": "do you have plans for docker image support?",
          "score": 1,
          "created_utc": "2026-02-12 20:53:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o556q8t",
              "author": "jasonhon2013",
              "text": "Hi we would like to make it like sqlite so currently we don't really plan to deal with docker. But in the future when it works like a server we will give supprot to docker ",
              "score": 1,
              "created_utc": "2026-02-13 10:54:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o52isxw",
          "author": "Delicious-One-5129",
          "text": "Lightweight and local first vector storage makes a lot of sense for private RAG setups. Love seeing more Rust based infra tools popping up.",
          "score": 0,
          "created_utc": "2026-02-12 23:13:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o556qwd",
              "author": "jasonhon2013",
              "text": "Yep yep ! ",
              "score": 1,
              "created_utc": "2026-02-13 10:54:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r460vk",
      "title": "Completely free - no online required (after install obviously) with tts and stt setup guide?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r460vk/completely_free_no_online_required_after_install/",
      "author": "mail4youtoo",
      "created_utc": "2026-02-14 00:28:18",
      "score": 8,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Could really use a little help here.\n\nI'm looking for a completely free - no online required (after install obviously) with tts and stt setup guide?\n\nI would prefer to use my old PC (8700k and 1080ti) but I also have a mac mini m4\n\nI've seen many many guides online (sites and videos) but there always seems to be a catch...  They use online APIs, have some sort of cost with tokens that need to be purchased.  Use alternative hardware like a raspberry pi or a NAS or some supercomputer with 1TB of video memory.\n\nI would like to start working with my own local AI through Ollama with the ability to have text to speech and speech to text.  \n\nI am looking to have a AI where I can practice speaking another language but mostly for experimentation to learn what AI chat is all about and be able to create my own tailored models eventually.\n\nWould someone be able to point me to a guide (written or video) where everything is free with no online required or an online API with purchased tokens?\n\nPlease?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r460vk/completely_free_no_online_required_after_install/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o59wy5x",
          "author": "XxAnomo305",
          "text": "easiest way to do it is use python. that's how I have it. you use office googles speech to text, and then you can use piper or kokoro for text to speech. and ofc that's all running offline using ollama. python puts it all together.",
          "score": 2,
          "created_utc": "2026-02-14 02:25:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5a7le0",
              "author": "mail4youtoo",
              "text": "Thank you",
              "score": 1,
              "created_utc": "2026-02-14 03:36:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5dprlz",
          "author": "sheykastarshadow",
          "text": "STT https://github.com/ggml-org/whisper.cpp\n\nTTS https://huggingface.co/hexgrad/Kokoro-82M\n\nOr\n\nhttps://github.com/QwenLM/Qwen3-TTS\n\nEverything open source",
          "score": 2,
          "created_utc": "2026-02-14 18:34:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5aewab",
          "author": "Minimum-Two-8093",
          "text": "What you're going to struggle with is VRAM, and if you allow swapping to your system memory, your inference speeds will take a nose dive. You just won't have many options for useful models with 11GB VRAM due to still needing a decent context size to convert back and forth. \n\nTry it of course though, I'm not saying not to. \n\nI'm using a 4090 and have been for a couple of weeks and am still getting substandard results (vs my expectations).",
          "score": 1,
          "created_utc": "2026-02-14 04:28:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}