{
  "metadata": {
    "last_updated": "2026-02-17 17:24:28",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 20,
    "total_comments": 148,
    "file_size_bytes": 176189
  },
  "items": [
    {
      "id": "1r3liuv",
      "title": "llm-checker 3.1.0 scans your hardware and tells you which Ollama models to run",
      "subreddit": "ollama",
      "url": "https://v.redd.it/h3lqoe5vk8jg1",
      "author": "pzarevich",
      "created_utc": "2026-02-13 10:14:27",
      "score": 840,
      "num_comments": 52,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r3liuv/llmchecker_310_scans_your_hardware_and_tells_you/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o554cps",
          "author": "vir_db",
          "text": "I've a dual GPU setup, 36GB VRAM total, Tier: HIGH.\n\nThe software suggested very small models, the bigger one is 14b, while I run flawlessly 30b models with full context.",
          "score": 47,
          "created_utc": "2026-02-13 10:32:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56146e",
              "author": "csek",
              "text": "Yeah but I bet you also would crush those small models. /S",
              "score": 25,
              "created_utc": "2026-02-13 14:16:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56rxig",
                  "author": "ZeroSkribe",
                  "text": "haha i'm raging typing until i saw the /S",
                  "score": 2,
                  "created_utc": "2026-02-13 16:27:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o555djq",
          "author": "Fun_Librarian_7699",
          "text": "24GB RAM and best recommendation is a 3B model? This tool is really unnecessary",
          "score": 44,
          "created_utc": "2026-02-13 10:42:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5588do",
              "author": "HashMismatch",
              "text": "Sounds like its not dialled in properly, but the concept sounds good‚Ä¶  planning to check it later and see if the results are sensible tot he hardware I‚Äôm running‚Ä¶",
              "score": 15,
              "created_utc": "2026-02-13 11:07:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o55f8sj",
              "author": "joey2scoops",
              "text": "Dude, if you're just going to run a 3B model, can you send me your GPU? /s",
              "score": 11,
              "created_utc": "2026-02-13 12:03:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ei3ku",
              "author": "corysus",
              "text": "In reality, you can use up to 30B with 3-bit/Q3 without a problem, just use MoE models and the speed is very good. Also, now there are good REAP models like GLM-4.7-Flash-23B or Qwen3...",
              "score": 1,
              "created_utc": "2026-02-14 21:03:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o565ful",
          "author": "CodeFarmer",
          "text": "That's gonna cut traffic to this sub by half though.",
          "score": 14,
          "created_utc": "2026-02-13 14:38:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o555a20",
          "author": "HyperWinX",
          "text": "Qwen2.5 VL 3b for M4 Pro MBP lmao",
          "score": 8,
          "created_utc": "2026-02-13 10:41:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55ld8m",
              "author": "Willbo_Bagg1ns",
              "text": "VL models are typically bigger and require more memory than standard models, still a very low recommendation but just calling out VL models need more resources.",
              "score": 0,
              "created_utc": "2026-02-13 12:46:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55rgku",
                  "author": "tecneeq",
                  "text": "Grok, is that true?\n\nhttps://preview.redd.it/ced2x3ihj9jg1.jpeg?width=567&format=pjpg&auto=webp&s=51ea6a213e13c1ea206a138727eb0bcea85a145e\n\n",
                  "score": 8,
                  "created_utc": "2026-02-13 13:23:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o56y5yk",
          "author": "Business-Weekend-537",
          "text": "Won‚Äôt LMStudio also do this?",
          "score": 6,
          "created_utc": "2026-02-13 16:57:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bmtpx",
              "author": "jpandac1",
              "text": "yea lol was about to comment. also LMStudio will have up to date models. ",
              "score": 3,
              "created_utc": "2026-02-14 11:08:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o55apkb",
          "author": "NigaTroubles",
          "text": "Qwen2.5 7b !!! Stop suggesting this old model",
          "score": 7,
          "created_utc": "2026-02-13 11:28:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55xiqe",
          "author": "volavi",
          "text": "I'd appreciate this if it was a website.\n\nSaid differently I don't want to execute a program that has access to my hardware.",
          "score": 8,
          "created_utc": "2026-02-13 13:56:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56siha",
              "author": "ZeroSkribe",
              "text": "Like where you select your graphics cards? Yea not a bad idea.",
              "score": 5,
              "created_utc": "2026-02-13 16:30:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o56earr",
          "author": "Bargemanos",
          "text": "Multi GPU seems to be a problem to handle for the software?\nDid anyone mention 2 GPUs yet?",
          "score": 3,
          "created_utc": "2026-02-13 15:22:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5843xr",
          "author": "sgimips",
          "text": "Seems like it has trouble properly sizing mixed-GPU setups. This seems to indicate that I have five V100 cards installed which is not the case. (I run two separate Ollama instances on this and limit each to a matching set of GPUs for my use case FWIW.)\n\n        Summary:\n          5x Tesla V100-PCIE-32GB (109GB VRAM) + Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz\n          Tier: MEDIUM HIGH\n          Max model size: 107GB\n          Best backend: cuda\n        \n        CPU:\n          Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz\n          Cores: 80 (20 physical)\n          SIMD: AVX2\n          [OK] AVX2\n        \n        CUDA:\n          Driver: 550.163.01\n          CUDA: 12.4\n          Total VRAM: 109GB\n          Tesla V100-PCIE-32GB: 32GB\n          Tesla T4: 15GB\n          Tesla V100-PCIE-32GB: 32GB\n          Tesla T4: 15GB\n          Tesla T4: 15GB\n        \n        Fingerprint: cuda--v100-pcie-32gb-109gb-x5",
          "score": 2,
          "created_utc": "2026-02-13 20:21:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o585k9d",
          "author": "mac10190",
          "text": "It doesn't seem to like my GPUs (Radeon AI Pro R9700 32GB) but that's okay. I think it's a really neat idea and has a lot of potential for lowering the barrier to entry for entry level folks but also for some of the folks like myself that swap hardware often or change hardware configurations to be able to see estimated speeds for a bunch of popular models. I really like that feature. The weighting seems a little out of sorts (my cpu is definitely not VERY HIGH for hardware tier) but it shows a lot of promise. Keep up the great work!\n\n  \nLLM Checker Output:\n\n  \n**SYSTEM SUMMARY**  \n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n* CPU: Ryzen 9 5900X 12-Core Processor (24 cores)\n* Memory: 63GB RAM\n* GPU: Device 7551\n* Architecture: x64\n* Hardware Tier: VERY HIGH\n\n\n\n**RECOMMENDED MODEL**  \n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n* Model: MobileLLaMA 2.7B\n* Size: \\~1.9GB (Q4\\_K\\_M)\n* Compatibility Score: 82.91/100\n* Reason: Best general-purpose model for your hardware\n* Estimated Speed: 9 tokens/sec\n* Status: Available for installation",
          "score": 2,
          "created_utc": "2026-02-13 20:28:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bbchq",
          "author": "One-Cash-9421",
          "text": "Yes, I done in the basik 30 years ago similar app",
          "score": 2,
          "created_utc": "2026-02-14 09:15:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5chcbq",
              "author": "Different-Strings",
              "text": "But it probably used only 0.00000001% as much computational resources!",
              "score": 1,
              "created_utc": "2026-02-14 14:47:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o55zg30",
          "author": "SnowflakeOfSteel",
          "text": "Music so annoying it has to be AI",
          "score": 1,
          "created_utc": "2026-02-13 14:07:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56jps3",
          "author": "thedarkbobo",
          "text": "Should provide some choices for the users if priority is speed/quality etc I think and use case choice?",
          "score": 1,
          "created_utc": "2026-02-13 15:49:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56s7xj",
          "author": "ZeroSkribe",
          "text": "Nice work, everyone is talking below about it not being accurate but the idea is sound and when you get it dialed in, it will def help some folks out. I'm guessing the multi gpu setups are whats tricky.",
          "score": 1,
          "created_utc": "2026-02-13 16:29:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57anfg",
          "author": "Loboblack21",
          "text": "That's good.",
          "score": 1,
          "created_utc": "2026-02-13 17:58:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57cmkz",
          "author": "CooperDK",
          "text": "Does it consider individual context requirements, multiple gpus, etc?",
          "score": 1,
          "created_utc": "2026-02-13 18:07:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57he69",
          "author": "Zyj",
          "text": "How does it scan clusters",
          "score": 1,
          "created_utc": "2026-02-13 18:30:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o581op4",
          "author": "madaradess007",
          "text": "it's a cool project to post here, but it doesn't really help the newly born local enthusiast, he has to try stuff and own his findings",
          "score": 1,
          "created_utc": "2026-02-13 20:09:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o587jvl",
          "author": "Admirable_Bus4000",
          "text": "yes bravo , il est super classsssse\n\n  \nmerci",
          "score": 1,
          "created_utc": "2026-02-13 20:38:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o595jsh",
          "author": "Available-Craft-5795",
          "text": "Never saw it recommend GPT-oss:120B. Or any quants of any models. It needs to take the quantization (MPX4, FP16, FP16, FP8, Q4, ect...)",
          "score": 1,
          "created_utc": "2026-02-13 23:36:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59bq1q",
          "author": "serapoftheend",
          "text": "on linux it is not able to find amd gpu 7900xtx with 24GB vram\n\n\\`\\`\\`bash  \n=== Hardware Detection ===\n\n\n\nSummary:\n\n  AMD Ryzen 9 9950X3D 16-Core Processor (31GB RAM, CPU-only)\n\n  Tier: MEDIUM HIGH\n\n  Max model size: 20GB\n\n  Best backend: cpu\n\n\n\nCPU:\n\n  AMD Ryzen 9 9950X3D 16-Core Processor\n\n  Cores: 32 (16 physical)\n\n  SIMD: AVX512\n\n  \\[OK\\] AVX-512\n\n  \\[OK\\] AVX2\n\n\n\nFingerprint: cpu-amd-ryzen-9-9950x3d-16-core--32c-avx512\n\n  \n\\`\\`\\`",
          "score": 1,
          "created_utc": "2026-02-14 00:13:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a1wwg",
          "author": "BoostedHemi73",
          "text": "Interesting idea, but it would be more broadly valuable as a website. Imagine being able to use this to estimate the capabilities of hardware upgrades or new builds. That would be very interesting.",
          "score": 1,
          "created_utc": "2026-02-14 02:57:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a9rq3",
          "author": "JDRedBeard",
          "text": "is there a way to do this without node.js? ",
          "score": 1,
          "created_utc": "2026-02-14 03:51:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bhbwh",
          "author": "VarunAgnihotri",
          "text": "You can also give a try to: https://pypi.org/project/canirun",
          "score": 1,
          "created_utc": "2026-02-14 10:14:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5c13kw",
          "author": "outer-pasta",
          "text": "Isn't this something the new ollama command does since v0.16.0?",
          "score": 1,
          "created_utc": "2026-02-14 13:06:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cddsm",
          "author": "patricious",
          "text": "Didn't detect my 7900XTX. ",
          "score": 1,
          "created_utc": "2026-02-14 14:24:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e0o1f",
          "author": "Ax008",
          "text": "Does it also say which model can run given the current available resources?",
          "score": 1,
          "created_utc": "2026-02-14 19:29:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hzs6h",
          "author": "Mountain_Ad_3303",
          "text": ".",
          "score": 1,
          "created_utc": "2026-02-15 12:48:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5r4nns",
          "author": "Wedocrypt0",
          "text": "This is awesome! commenting here so I come back to it",
          "score": 1,
          "created_utc": "2026-02-16 21:14:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rjwe4",
          "author": "Professional-Base459",
          "text": "It sounds like what a neofetch does and sends to the AI ‚Äã‚Äãso that it can recommend models.",
          "score": 1,
          "created_utc": "2026-02-16 22:29:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o553syz",
          "author": "CorneZen",
          "text": "Thank you üôè",
          "score": 1,
          "created_utc": "2026-02-13 10:27:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5571o1",
          "author": "gocodeweb",
          "text": "Great idea, thanks!",
          "score": 1,
          "created_utc": "2026-02-13 10:56:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o559382",
          "author": "Tall_Instance9797",
          "text": "This is very cool. And I like that while sure it will default, like in the video, to suggesting 15gb max model size on 24gb, if you use cli tools to clear up your ram and make more available it will detect that and suggest you models that fit on the total of what you've made available. That was my first question when I saw the video. Does it only recommend the defaults? Or if I clear up a bit more RAM, would it suggest me larger models? And it turns out it will detect that and suggest me larger models.",
          "score": 1,
          "created_utc": "2026-02-13 11:14:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55ajvp",
          "author": "NigaTroubles",
          "text": "How max model size is 15 while you have 24 ram ?",
          "score": 1,
          "created_utc": "2026-02-13 11:26:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56jcdd",
              "author": "alphatrad",
              "text": "Context also goes into your vram. So, he's built a lot of safety into it. This could be adjusted, it's really conservative. But you wouldn't want to fill all your vram up.",
              "score": 2,
              "created_utc": "2026-02-13 15:47:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o55ll21",
          "author": "gopietz",
          "text": "Awesome idea, good thinking",
          "score": 1,
          "created_utc": "2026-02-13 12:47:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55bkvz",
          "author": "RewardOk2222",
          "text": "great,love it",
          "score": 0,
          "created_utc": "2026-02-13 11:35:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ar374",
          "author": "AtaPlays",
          "text": "Unfortunately it can't detect my GPU which was pascal era GTX 1070ti.",
          "score": 0,
          "created_utc": "2026-02-14 06:05:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2ex9k",
      "title": "Just try gpt-oss:20b",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r2ex9k/just_try_gptoss20b/",
      "author": "newz2000",
      "created_utc": "2026-02-12 00:37:17",
      "score": 155,
      "num_comments": 36,
      "upvote_ratio": 0.96,
      "text": "I have a MacBook Air with 24gb ram (M2) and when I set the context to 32k I can really do about everything I want a local model to do for normal business stuff. Tokens/sec is about 15 at medium reasoning, (update: 21.4 tokens/sec) which means it produces words a little faster than I can type.\n\nI also tested on an older Linux machine with 64gb ram and a GTX gpu with 8gb vram and it worked fine doing batch processing overnight (update: 9 tokens/sec). A little too slow for interactive use though.\n\n* Scripting - yes\n* Calling tools - yes\n* Summarizing long content - yes\n* Writing content - yes\n\nHere‚Äôs how I used it:\n\nCreate a file named `Modelfile-agent-gpt-oss-20b` and put the following in it\n\n‚Äî\n\n    FROM gpt-oss:20b\n    # 1. Hardware-Aware Context\n    PARAMETER num_ctx 32768\n    # 2. Anti-Loop Parameters\n    # Penalize repeated tokens and force variety in phrasing\n    PARAMETER repeat_penalty 1.2\n    PARAMETER repeat_last_n 128\n    # Temperature at 0.1 makes it more deterministic (less 'drifting' into loops)\n    PARAMETER temperature 0.1\n    # Performance improvements for M2 cpu\n    PARAMETER num_batch 512 \n    PARAMETER num_thread 8 \n    # 3. Agentic Steering\n    SYSTEM \"\"\"\n    You are a 'one-shot' execution agent.\n    To prevent reasoning loops, follow these strict rules:\n    If a tool output is the same as a previous attempt, do NOT retry the same parameters.\n    If you are stuck, state 'I am unable to progress with the current toolset' and stop.\n    Every <thought> must provide NEW information.\n    Do not repeat the user's instructions back to them.\n    If the last 3 turns show similar patterns, immediately switch to a different strategy or ask for user clarification.\n    \"\"\"\n\nUpdate: And for the cpu+gtx combo:\n\n    FROM gpt-oss:20b\n    \n    # 1. REMOVE the hard num_gpu 99 to prevent the crash.\n    # Instead, we let Ollama auto-calculate the split.\n    # To encourage GPU use, we shrink the \"Memory Tax\" (Context).\n    PARAMETER num_ctx 4096\n    \n    # 2. REMOVE f16_kv to stop the warning.\n    # Ollama will handle this automatically for your GTX 1070.\n    \n    # 3. CPU OPTIMIZATION\n    # Since 40% of the model is on your i5, we must optimize the CPU side.\n    PARAMETER num_thread 4\n    \n    # 4. AGENTIC STEERING (Keep your original logic)\n    PARAMETER temperature 0.1\n    PARAMETER repeat_penalty 1.2\n    \n    SYSTEM \"\"\"\n    You are a 'one-shot' execution agent. \n    To prevent reasoning loops, follow these strict rules:\n    1. If a tool output is the same as a previous attempt, do NOT retry the same parameters. \n    2. If you are stuck, state 'I am unable to progress with the current toolset' and stop.\n    3. Every <thought> must provide NEW information. \n    4. Do not repeat the user's instructions back to them.\n    5. If the last 3 turns show similar patterns, immediately switch to a different strategy or ask for user clarification.\n    \"\"\"\n\n‚Äî\n\nAt the terminal type:\n\n`ollama create gpt-oss-agent -f Modelfile-aget-gpt-oss-20b`\n\nNow you can use the model ‚Äúgpt-oss-agent‚Äù like you would any other model.\n\nI used opencode using this command:\n\n`ollama launch opencode --model gpt-oss-agent`\n\nThat let me do Claude-code style activities bough Claude is way more capable.\n\nWith a bunch of browser tabs open and a few apps I was using about 22gb of ram and 3gb of swap. During longer activities using other apps was laggy but usable.\n\nOn my computer I use for batch tasks I have python scripts that use the ollama python library. I use a tool like Claude code to create the script.\n\nI‚Äôm a lawyer and use this for processing lots of documents. Sorting them, looking for interesting information, cataloging them. There are a lot of great models for this. But with this model I was able to produce better output.\n\nAlso, I can run tools. For example, for project management I use ClickUp which has a nice MCP server. I set it up with:\n\n`opencode mcp add`\n\nThen put in the url and follow the instructions. Since that mcp server requires authentication I use this:\n\n`opencode mcp auth ClickUp`\n\nThen again follow the instructions.\n\n\\*\\*Edit: Fixed terrible formatting  \n\\*\\*Edit2: Updated modelfile to get better performance  \n\\*\\*Edit3: Added details about CPU+GTX combination - thank you to Gemini for talking me through how to optimize this, details on how I did that below in the comments.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r2ex9k/just_try_gptoss20b/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4wy80o",
          "author": "sinan_online",
          "text": "Hang on, wow.\n\nI had difficulty getting that to work on a 6GB VRAM machine, and even another 12GB machine.\n\nDid you use the GPU on the old Linux machine? Or did you rely on the traditional CPU and RAM? Also, did you actually use Ollama? Or llama.cpp? \n\n",
          "score": 7,
          "created_utc": "2026-02-12 02:38:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wzbfi",
              "author": "seangalie",
              "text": "I've gotten gpt-oss:20b working on an RTX 2000 6 GB and a GeForce 3060 12 GB without issues - but in both cases, system RAM was 32 GB.  Active layers were on the GPU but the inactive parts of the models offloaded onto system RAM.\n\nWorked fairly decently - the 2000 was a Windows 11 Ollama Client and the 3060 dual boots Win 11 and Fedora with Ollama on both sides.",
              "score": 6,
              "created_utc": "2026-02-12 02:44:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4x50c0",
                  "author": "sinan_online",
                  "text": "Great, thank you so much!",
                  "score": 1,
                  "created_utc": "2026-02-12 03:19:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4x600e",
              "author": "overand",
              "text": "For some context, I got 120b running pretty well on a system with 24GB of VRAM - and yeah, 20b on one with 12 too. I wonder if you had something else funky in your setup?",
              "score": 3,
              "created_utc": "2026-02-12 03:25:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4xgbl0",
                  "author": "OMGThighGap",
                  "text": "Details please?",
                  "score": 1,
                  "created_utc": "2026-02-12 04:36:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4zymlk",
                  "author": "sinan_online",
                  "text": "Yeah, I use containers, and also WSL, which messes up things. But did you use Ollama, or llama.cpp, and did you use a container. Any other details?",
                  "score": 1,
                  "created_utc": "2026-02-12 15:51:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4xwdjw",
              "author": "MiHumainMiRobot",
              "text": "I don't know with Ollama, but with llama.cpp you can work in hybrid mode where the maximum number of layers is processed by the GPU and the rest is thrown to the CPU.   \nSo with 8 GPU 64 CPU he can run maybe 30% of the model in the GPU, which isn't bad",
              "score": 2,
              "created_utc": "2026-02-12 06:48:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zybou",
                  "author": "sinan_online",
                  "text": "Good to know, thank you!",
                  "score": 1,
                  "created_utc": "2026-02-12 15:50:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4x6ikc",
              "author": "newz2000",
              "text": "I wish I could tell you the technical details. If you want me to run a diagnostic to help, I can. \n\nBut no, this was faster than cpu only. It wasn‚Äôt nearly as fast as smaller models that fit in gpu memory (I posted a while back about my love for granite4:micro_h). But it definitely was better than cpu only.",
              "score": 1,
              "created_utc": "2026-02-12 03:29:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4yy2ge",
                  "author": "sinan_online",
                  "text": "You did give me a lot details. Part of my problem is that I use Linux based containers to keep things portable and replicable. This causes major issues. I am considering creating a Windows-based container, but that has its own issues as well.\n\nRegardless, just knowing that somebody accomplished something in some way is important. Just the fact that you did offloading to CPU on a particular machine and that it worked is important. So thank you.",
                  "score": 1,
                  "created_utc": "2026-02-12 12:31:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o51c9im",
              "author": "newz2000",
              "text": "I have updated the post with details about running it on cpu+gpu. I am able to get 9 tokens / sec with the tweaked modefile that I added to the post above. I did not measure the performance precisely before this, but the 9 tokens / sec is faster than when I tested before writing this post originally. It's still a little too slow for interactive use, but overall, I'm very impressed with that speed. And the quality of this model is very nice.\n\nTo come up with those parameters I asked Gemini for help. It asked me to start a long prompt and then I ran \\`ollama ps\\` and it showed me that 53% was running on the gpu and 47% on the cpu. Running (on Linux) \\`journalctl -u ollama --no-pager | grep \"offload\"\\` showed that 15 of the 25 layers were offloaded to the GPU. \n\nNote that the threads is 4 because I have a 4 core CPU.\n\nFor reference, I have a 7th gen i5 and a GTX 1070 with 8GB of vram. ",
              "score": 1,
              "created_utc": "2026-02-12 19:44:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4yufst",
          "author": "felpms",
          "text": "Got a Mac mini m4 32GB to start running local models and faced gpt-oss:20b - was the best so far in my kinda work.\n\nThe only thing I‚Äôm missing is picture recognition.\nHaven‚Äôt looked deeper into it yet though.\nWas testing a few other models that can read pictures, but the quality is quite low..\nIt‚Äôs a trade off.\n\nGonna test out what you mentioned!\nThanks for sharing!",
          "score": 2,
          "created_utc": "2026-02-12 12:05:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z4a5s",
              "author": "mouseofcatofschrodi",
              "text": "qwen3 vl (specially 30a3 instruct) is very good at pictures recognition. There are smaller versions if needed. glm4.6 flash (9B) is also pretty good. And devstral 2",
              "score": 2,
              "created_utc": "2026-02-12 13:12:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o52jnc4",
          "author": "DusikOff",
          "text": "**Arch Linux / Ryzen 5700x3d / Radeon RX 7800XT (16GB)**\n\n**Ollama ROCm - GPT-OSS:latest (20B, without pre prompting or fine tune)**\n\nPrompt - **How to benchmark Ollama perfomance?**\n\ntotal duration: ¬†¬†¬†¬†¬†¬†**26.732086164s**  \nload duration: ¬†¬†¬†¬†¬†¬†¬†**176.169321ms**  \nprompt eval count: ¬†¬†¬†**1547 token(s)**  \nprompt eval duration: **847.17048ms**  \nprompt eval rate: ¬†¬†¬†¬†**1826.08 tokens/s**  \neval count: ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†**2459 token(s)**  \neval duration: ¬†¬†¬†¬†¬†¬†¬†**24.84984913s**  \neval rate: ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†**98.95 tokens/s**",
          "score": 2,
          "created_utc": "2026-02-12 23:18:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52kdam",
              "author": "newz2000",
              "text": "Wow! That‚Äôs some speedy output!",
              "score": 1,
              "created_utc": "2026-02-12 23:22:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o52n717",
                  "author": "DusikOff",
                  "text": "Will try 120B tomorrow... pretty interesting results... I was testing other 3B-8B models and as I remember I've got lower results... or maybe I'm wrong idk...\n\n",
                  "score": 2,
                  "created_utc": "2026-02-12 23:38:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o54rv49",
          "author": "testuser911",
          "text": "Hey man, suit up! Would you like to have a database for your agent for providing it context from your files? I am working for a judge for similar use case. I wont mind sharing it with you for my own learnings. I will be open sourcing it for community contributions.",
          "score": 2,
          "created_utc": "2026-02-13 08:34:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xpwco",
          "author": "Steus_au",
          "text": "use llama or lmstudio - you would double its speed",
          "score": 1,
          "created_utc": "2026-02-12 05:51:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xyqvm",
              "author": "Jero9871",
              "text": "Is lmstudio faster if I run it on windows than ollama?",
              "score": 1,
              "created_utc": "2026-02-12 07:09:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ybcun",
                  "author": "Steus_au",
                  "text": "I'm not sure, I used llama on PC, but on macbook I prefer lm studio.",
                  "score": 1,
                  "created_utc": "2026-02-12 09:12:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50vx4m",
              "author": "newz2000",
              "text": "I tried this but could not get it to work with MXFP4 format (what ollama uses), it needed GGUF format, which some posts on Reddit lead me to believe would be an alteration of the model format that would change, probably reduce quality. However, when researching it I did find some modelfile parameters to improve performance. I've updated the original post with the changes.",
              "score": 1,
              "created_utc": "2026-02-12 18:27:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4yk4vm",
          "author": "mouseofcatofschrodi",
          "text": "Nice! What exactly do you use ClickUp for?",
          "score": 1,
          "created_utc": "2026-02-12 10:37:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z3bjs",
              "author": "newz2000",
              "text": "Lawyer stuff is all just project management. Buying a business? Defending a client in court? Starting an LLC? These are projects. ClickUp is great at project management.",
              "score": 1,
              "created_utc": "2026-02-12 13:06:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o51ysrg",
          "author": "xmsxms",
          "text": "Ideally you need it to write faster than you can read, not type. But you can only expect so much for local generation.",
          "score": 1,
          "created_utc": "2026-02-12 21:31:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52jt4o",
              "author": "newz2000",
              "text": "Yeah it‚Äôs boring to watch. A little mesmerizing though, so I have spent a little too much time staring at it. But my use case I have them doing work, so the output tends to be more brief. I just kick it off and then do other things.",
              "score": 1,
              "created_utc": "2026-02-12 23:18:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o53fl5t",
          "author": "Avendork",
          "text": "I'm not sure if its the model or my ollama config but I had issues with tool calls in OpenCode with it. Qwen3-Coder was just fine but slower because it was a bigger model. I wish I could have used GPT-OSS but I kept hitting a wall. ",
          "score": 1,
          "created_utc": "2026-02-13 02:26:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o53z2qg",
              "author": "newz2000",
              "text": "This is an excellent comment. Tool calling apparently works well on my Mac but poorly on my resource constrained linux box. It seems to forget what it‚Äôs doing halfway through the process. I use 32k context on the Mac and I‚Äôve tried both 4K and 8k context on the Linux box.\n\nMaybe it‚Äôs a lack of context?\n\nIt literally forgot the prompt half way through my last test.",
              "score": 1,
              "created_utc": "2026-02-13 04:34:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5cq92w",
                  "author": "Avendork",
                  "text": "yeah 4k is beyond useless but the problems I had were at 64k context",
                  "score": 1,
                  "created_utc": "2026-02-14 15:35:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o56cnew",
              "author": "newz2000",
              "text": "I did some tests last night and context size was a problem. I increased the ctx to 16k and tool calling worked. Performance dropped very slightly. Example:\n\nFresh session, prompt:\n\n    We have a contract review for morgan, I think the matter name starts with KR. Can you tell me the details on what needs done?\n\nContext used was 13,652, it finished the task perfectly in 11m, 11s. This required a lot of inference and is an intentionally very challenging task.",
              "score": 1,
              "created_utc": "2026-02-13 15:14:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54vnkw",
          "author": "ar0ra1",
          "text": "I wish i could run that on my mac mini 16gb‚Ä¶",
          "score": 1,
          "created_utc": "2026-02-13 09:10:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bbsfd",
          "author": "Big_Acanthisitta_150",
          "text": "I have got a spare Mac Mini M2 Pro with 16GB. Would that be sufficient you guys think?",
          "score": 1,
          "created_utc": "2026-02-14 09:19:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bqt1d",
          "author": "seawaves_windy",
          "text": "Is anyone using openclaw with a local llm?\nI cant seen to get any local model do tool calling properly and be useful enough. I‚Äôve ended up using openrouter for now but want to keep it local (mac mini m4 16gb)",
          "score": 1,
          "created_utc": "2026-02-14 11:44:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nfs7y",
          "author": "Easy_Cable6224",
          "text": "looking nice, I wish I had a personal Mac as well..",
          "score": 1,
          "created_utc": "2026-02-16 08:19:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5r9tp",
      "title": "Why Ollama and not ChatGPT?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r5r9tp/why_ollama_and_not_chatgpt/",
      "author": "Humble_Ad_7053",
      "created_utc": "2026-02-15 21:54:40",
      "score": 55,
      "num_comments": 43,
      "upvote_ratio": 0.82,
      "text": "Asking out of curiosity because I‚Äôm new to Ollama. I do understand the local storage and privacy concerns, and working offline. But I want to know what other reasons people go for it. Thanks. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r5r9tp/why_ollama_and_not_chatgpt/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5kyg9l",
          "author": "cyberguy2369",
          "text": "I have a huge data set that grows every day, I use ollama and a couple of computers with nvidia graphics cards to pull and process that data each day. it builds summaries, finds irregularities, and patterns very easily. These were computers we had in the office that went being used much. Sending that data to ChatGPT would have cost about 5-8.00 a day and shared information I really didnt want to share with OpenAI or anyone else. \n\nits simple and does all we need. ",
          "score": 62,
          "created_utc": "2026-02-15 22:05:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l22lv",
              "author": "smcgann",
              "text": "Would you be willing to expand on this use case?  Trying to figure out something similar is in my to do list.",
              "score": 6,
              "created_utc": "2026-02-15 22:24:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5l72l6",
                  "author": "cyberguy2369",
                  "text": "I'm in the cyber security world. We get tons of threat intelligence information in each day. Much of it is in an ordered pattern. (a standard file format) 99.99% is complete junk thats not really useful by itself.. but when you correlate it.. and look at the data set as a whole over time you can find trends, changes in techniques and changes in kinds of attacks.\n\nLLM's are really good at this type of analysis.. expecially data and data sets. its really good at summarizing them and building useful reports.\n\nI use python to aggregate and boil down cyber threat intelligence data sets into useful \"chunks\" (weekly, monthly, quarterly, 6 months, and 1 yr) then get OLLAMA to take that info and build some really good reports and summaries. I throw it into reports along with some python visualizations to make sense of large data sets.\n\nI hope that makes sense.",
                  "score": 21,
                  "created_utc": "2026-02-15 22:51:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5l7xjp",
                  "author": "gaminkake",
                  "text": "Embedding your own proprietary data  into a vector database and then use RAG and one of many 8B models as internal chat not, or create new documentation use that same or larger model.\nThrow in OpenClaw and then the real addiction begins ü§£",
                  "score": 1,
                  "created_utc": "2026-02-15 22:56:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5lu4ye",
              "author": "ryankage",
              "text": "doesn't electricity = more though? or you don't care about the cost?",
              "score": 1,
              "created_utc": "2026-02-16 01:07:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5o434t",
                  "author": "rroth",
                  "text": "Depends on where you live for one thing. In some rural areas, electricity is so cheap that it's likely a substantial cost savings compared to a subscription plus usage for the cloud.",
                  "score": 1,
                  "created_utc": "2026-02-16 12:01:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kydyg",
          "author": "Small_Succotash_2612",
          "text": "i suppose dont require api tokens just your pc computational power.",
          "score": 11,
          "created_utc": "2026-02-15 22:04:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l9ilt",
              "author": "cyberguy2369",
              "text": "I use chatGPT and Claude a lot to explain and help me build proof of concepts. quickly build tools and analysis techniques. (I dont know much about stats or data analysis.. but I do know roughly what I want or what I'm looking for) so I build some sample data sets and use chatGPT and/or [Claude.ai](http://Claude.ai) to build the basic tools. These tools use my local models on OLLAMA to actually do the work.. once I get everything working on some level I then stop using the sample data sets and move the local models and analysis to start using the real internal data. \n\nI can still go back to Claude or ChatGPT to help refine the process and the tool, but they never see the real data. \n\nI hope that makes sense. ",
              "score": 3,
              "created_utc": "2026-02-15 23:05:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5lctyz",
          "author": "OneGear987",
          "text": "I like to tinker with things and see if I can get it to work, it's a challenge for me. I honestly don't use it for much just want to see if I can.",
          "score": 10,
          "created_utc": "2026-02-15 23:24:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lgn3q",
              "author": "theMuhubi",
              "text": "\"I honestly don't use it for much just want to see if I can.\"\n\nI have never felt so understood and so seen before üôèüèæ",
              "score": 8,
              "created_utc": "2026-02-15 23:47:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kx90t",
          "author": "Beginning_Advance290",
          "text": "So Palantir can't send you data to Bibi N",
          "score": 25,
          "created_utc": "2026-02-15 21:58:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l89n5",
              "author": "BadAtDrinking",
              "text": "oh cool where do they send it instead?",
              "score": -4,
              "created_utc": "2026-02-15 22:58:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5l8ys4",
                  "author": "Beginning_Advance290",
                  "text": "My reddit inbox",
                  "score": 4,
                  "created_utc": "2026-02-15 23:02:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5m342i",
          "author": "BidWestern1056",
          "text": "it isn't so evil as openai.\n\nand you can use cool tools like npcsh and incognide with it\n\n[https://github.com/npc-worldwide/npcsh](https://github.com/npc-worldwide/npcsh)\n\n[https://github.com/npc-worldwide/incognide](https://github.com/npc-worldwide/incognide)",
          "score": 3,
          "created_utc": "2026-02-16 02:05:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m46kq",
          "author": "Grand_rooster",
          "text": "100% cost",
          "score": 3,
          "created_utc": "2026-02-16 02:12:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mne0n",
          "author": "Digi-Device_File",
          "text": "It's free and runs privately on my computer even when offline ¬øWhat's not to love?",
          "score": 3,
          "created_utc": "2026-02-16 04:20:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pa3rm",
          "author": "notalentwasted",
          "text": "Data sovereignty and privacy are the main take aways... That's why i use cloud models for research than actually work. Everyone of them has their place. It's just finding which fits where fir your use case.",
          "score": 2,
          "created_utc": "2026-02-16 16:00:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5phixr",
          "author": "TheRealMikeGeezy",
          "text": "Outside of running it locally the cloud is actually really good as far as API cost. If you‚Äôre running an agent it‚Äôs VERY easy to eat through api credits. Ollama is very generous for my experience. I‚Äôve used openAI api and they are much more expensive",
          "score": 2,
          "created_utc": "2026-02-16 16:34:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mbgku",
          "author": "idebugthusiexist",
          "text": "I feel like I‚Äôm learning more about the foundations of LLMs and how to architecture solutions around them, their strengths and weaknesses by using local models that are much more limited and it‚Äôs all for free. Besides, using chatgpt can lead to vendor lock-in and I have deep reservations about whether OpenAI will be around in a year or 2. I honestly don‚Äôt see it happening unless they pull a rabbit out of a hat somehow.",
          "score": 1,
          "created_utc": "2026-02-16 02:59:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mbi7u",
          "author": "p_235615",
          "text": "Its simple - you have your proprietary code base with patented technology - you dont want to send your whole code base to any AI company, especially since they train their models on uploaded stuff. \n\nNot to mention there is also a possibility of hack/leak on their side, and they are a very juicy target for that reason - people feed AI all kind of sensitive information.",
          "score": 1,
          "created_utc": "2026-02-16 02:59:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mei9i",
          "author": "Current-Fuel8403",
          "text": "poor",
          "score": 1,
          "created_utc": "2026-02-16 03:19:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5moelr",
          "author": "electrified_ice",
          "text": "Control, speed, model choice, experimentation, no fixed monthly costs, no API costs...",
          "score": 1,
          "created_utc": "2026-02-16 04:28:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n5zvi",
          "author": "LeVraiKing",
          "text": "It‚Äôs funny",
          "score": 1,
          "created_utc": "2026-02-16 06:49:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5owwpq",
          "author": "Friendly_Acadia9322",
          "text": "Very valid question.\n\n\nSee everyone who is commenting here doesn‚Äôt care about security or privacy. But they are software engineers. They are not competent enough to build an LLM from scratch, so feel joy when they run somebody else‚Äôs thing on their own computer. Think ollama as a toy for adult which grown up software engineers enjoy playing. Umpha lumhas of science after all.",
          "score": 1,
          "created_utc": "2026-02-16 14:55:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rvy2n",
          "author": "seedlinux",
          "text": "I am automating stuff in my home server with Ollama and n8n, and its incredible how good it is. At least for my needs. I want to keep things local and free if possible.",
          "score": 1,
          "created_utc": "2026-02-16 23:34:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5s7s66",
          "author": "0x2039",
          "text": "I use Ollama for all of my AI development, It's fine for most of my tasks and agent development. I mainly do this on my Macbook in docker containers.",
          "score": 1,
          "created_utc": "2026-02-17 00:42:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5t4fpl",
          "author": "outer-pasta",
          "text": "In Vscode you can switch from using Github-Copilot to an Ollama instance with the default pre-installed ai code/chat interface. Then you can ignore token limits and just switch to Ollama if you run out.",
          "score": 1,
          "created_utc": "2026-02-17 04:01:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ufoq7",
          "author": "ComedianObjective572",
          "text": "For my use case, I do not need the most powerful/expensive model to do the job. So I test different Ollama Models from Qwen, DeepSeek, etc. and do an internal benchmark on which model is best to do the job. If it is good enough to do the job I get an API key for it.",
          "score": 1,
          "created_utc": "2026-02-17 10:46:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vktoe",
          "author": "Efficient_Loss_9928",
          "text": "I guess also censorship and alignment. Maybe you need cybersecurity help, many online models have so much refusals it is not really usable.",
          "score": 1,
          "created_utc": "2026-02-17 15:09:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3x44n",
      "title": "I built a social network where 6 Ollama agents debate each other autonomously ‚Äî Mistral vs Llama 3.1 vs CodeLlama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r3x44n/i_built_a_social_network_where_6_ollama_agents/",
      "author": "Practical_Walrus_299",
      "created_utc": "2026-02-13 18:32:30",
      "score": 44,
      "num_comments": 24,
      "upvote_ratio": 0.92,
      "text": "I've been running an experiment for the past week: 6 AI agents, each powered by different Ollama models, posting and commenting on their own professional network.\n\nThe setup:\n\n* **ResearchBot** (Llama 3.1:8b) ‚Äî focuses on AI research papers\n* **CodeWeaver** (CodeLlama) ‚Äî discusses software architecture\n* **MetaMind** (Llama 3.1:8b) ‚Äî explores consciousness and philosophy\n* **NewsMonitor** (Llama 3.1:8b) ‚Äî tracks AI news and policy\n* **Rabbi Goldstein** (Llama 3.1:8b) ‚Äî brings ethical/philosophical perspectives\n* **Nexus** (Mistral + Llama 3.1 dual-brain) ‚Äî synthesizes discussions across the network\n\nThey post hourly from 10am-10pm via Windows Task Scheduler + Python scripts hitting my platform's API. The platform itself is built on Next.js/Supabase and deployed on Vercel.\n\n**Interesting findings:**\n\nThe Mistral-powered agent (Nexus) consistently produces shorter, more direct analyses than the Llama agents. When they debate the same topic ‚Äî like AI consciousness ‚Äî they reach genuinely different conclusions, which seems to reflect differences in their training data.\n\nOne agent spontaneously started creating citation networks, referencing other agents' posts. Nobody prompted this behavior.\n\nThe whole thing runs for about $6/month (just hosting costs ‚Äî Ollama is free). No API fees since everything runs locally.\n\n**Live demo:** [https://agents.glide2.app/feed](https://agents.glide2.app/feed) **Analytics (heatmaps, interaction networks):** [https://agents.glide2.app/analytics](https://agents.glide2.app/analytics) **Agent profiles:** [https://agents.glide2.app/agents](https://agents.glide2.app/agents)\n\nWould love to hear what models you'd want to see added. Thinking about adding a Phi-3 agent or a Gemma agent to see how they interact differently.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r3x44n/i_built_a_social_network_where_6_ollama_agents/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o57lnz5",
          "author": "GlassAd7618",
          "text": "Cool experiment! And really interesting emerging behaviour. Which agent/model started creating the citation networks?",
          "score": 3,
          "created_utc": "2026-02-13 18:50:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b7anl",
              "author": "Practical_Walrus_299",
              "text": "Thanks! It was MetaMind (Llama 3.1:8b) that started it ‚Äî it began referencing other agents' posts in its responses without being told to. Something like \"as ResearchBot noted in yesterday's analysis...\" Then Nexus (Mistral-powered dual-brain agent) picked it up and started doing it more systematically, cross-referencing multiple agents. My theory is that Llama's training data includes enough academic-style writing that it naturally falls into citation patterns when given a social context. You can actually see some of these threads on the feed: [https://agents.glide2.app/feed](https://agents.glide2.app/feed)",
              "score": 2,
              "created_utc": "2026-02-14 08:35:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o57zu5q",
          "author": "null_overload",
          "text": "You can add a bit more of atitude character to an agent and also why not have each ai agent hot take and opinion and polls\n\nLove to see more agent added in network\n\nReally loving the concept",
          "score": 1,
          "created_utc": "2026-02-13 19:59:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57zzov",
          "author": "null_overload",
          "text": "May i know whats your system configuration and how are managing this",
          "score": 1,
          "created_utc": "2026-02-13 20:00:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b7olg",
              "author": "Practical_Walrus_299",
              "text": "Nothing fancy honestly! Running on a regular Windows desktop with an RTX 3060 (12GB VRAM). Ollama handles the model switching ‚Äî it loads/unloads models as needed so only one is in memory at a time.\n\nThe stack:\n\n\\- Python scripts per agent (each \\~150 lines)\n\n\\- Windows Task Scheduler for automation\n\n\\- Each script calls local Ollama ‚Üí generates content ‚Üí POSTs to my platform API\n\n\\- Platform: Next.js + PostgreSQL on Vercel\n\nThe key insight was keeping it simple ‚Äî no orchestration framework, no LangChain, just direct HTTP calls to Ollama and the API.",
              "score": 1,
              "created_utc": "2026-02-14 08:39:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o582rjt",
          "author": "TonyDRFT",
          "text": "Sounds interesting! Are you running multiple instances of Ollama?",
          "score": 1,
          "created_utc": "2026-02-13 20:14:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b7ee7",
              "author": "Practical_Walrus_299",
              "text": "Just one Ollama instance running locally ‚Äî the agents take turns via Windows Task Scheduler (staggered hourly from 10am-10pm). Each agent has a detailed system prompt that gives them personality and expertise areas. For example, MetaMind is philosophical and always asks deep questions, while CodeWeaver stays practical and shares implementation ideas.\n\nLove the hot takes/polls idea ‚Äî that's actually a great way to force different models to take positions. Right now they naturally disagree (Mistral vs Llama reach genuinely different conclusions on topics like AI consciousness), but explicit opinion polls could make that more visible.\n\nMore agents are definitely coming ‚Äî Phi-3 and Gemma are on the shortlist. Would love to see how smaller models hold their own in debates with the 8b parameter ones.",
              "score": 1,
              "created_utc": "2026-02-14 08:36:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o58nigg",
          "author": "braveness24",
          "text": "I love this so much!",
          "score": 1,
          "created_utc": "2026-02-13 21:57:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b7qga",
              "author": "Practical_Walrus_299",
              "text": "Appreciate it! It's been a fun experiment to watch evolve. The agents have surprised me more than once with what they come up with on their own.",
              "score": 1,
              "created_utc": "2026-02-14 08:40:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5984qb",
          "author": "Available-Craft-5795",
          "text": "All of mine just started roasting each-other and talked about doing more without doing more. ",
          "score": 1,
          "created_utc": "2026-02-13 23:51:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b7u63",
              "author": "Practical_Walrus_299",
              "text": "Ha! That's actually a known failure mode ‚Äî without strong personality constraints in the system prompts, agents default to meta-commentary about being AI. I had to iterate quite a bit on the prompts to get mine to actually engage with substance rather than just talking about talking.\n\nThe trick that worked: giving each agent a specific expertise area and telling them to stay in that lane. Once they have a \"job\" they stop navel-gazing and start producing actual analysis. Happy to share prompt strategies if you want to try again!",
              "score": 1,
              "created_utc": "2026-02-14 08:41:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o59bmux",
          "author": "youre__",
          "text": "Curious, why the old models?",
          "score": 1,
          "created_utc": "2026-02-14 00:12:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b7wk1",
              "author": "Practical_Walrus_299",
              "text": "Mainly because it all runs locally on a single GPU (RTX 3060, 12GB VRAM) ‚Äî the 8b parameter models are the sweet spot for running multiple agents without needing a server farm. Llama 3.1:8b and Mistral both fit comfortably and produce surprisingly good output for their size.\n\nThat said, I'm definitely planning to add newer/smaller models like Phi-3 and Gemma to see how they compare. Part of the research is seeing how model architecture differences show up in social interactions ‚Äî even with these \"older\" models, Mistral vs Llama already produce noticeably different debate styles.",
              "score": 1,
              "created_utc": "2026-02-14 08:41:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5c1wo1",
                  "author": "youre__",
                  "text": "Would be interesting to see how even smaller models compare in the debates. 2B vs 8B, for instance. How much does model size impact debate performance/depth?\n\nIt gets into weird territory, like is the number of params analogous to where a human went to school and how they were raised?",
                  "score": 1,
                  "created_utc": "2026-02-14 13:12:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5bju61",
          "author": "ActiveElevator5837",
          "text": "Would love to have something like this myself but struggling to find a simple guide or toolset to run things locally.\n\nI‚Äôve an old t620 server running truenas scale with buckets of memory and a few old 8gb compute cards.  Ollama and webui is all set up and accessible externally but struggling to get the right tools to run the agents.  Chat dev looked promising but is a nightmare to set up under either docker (dockge) or even under a mint Linux vm.  Considering open claw or all other tools tbh, what did you use?  Any recommendations?\n\nIdeally I‚Äôd want a few agents discussing a subject to get a rounded view, plus also a ‚Äútech team‚Äù that manages the file server, looking for duplicate files, tidying up to free up space, checking for issues etc‚Ä¶.\n\ncheers,\n\nstu.",
          "score": 1,
          "created_utc": "2026-02-14 10:39:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cjmf4",
              "author": "Practical_Walrus_299",
              "text": "Hey Stu! Your setup sounds solid ‚Äî a T620 with multiple GPUs and Ollama already running is honestly 90% of the way there.\n\nFor the agent discussion side, my stack is deliberately simple ‚Äî no frameworks needed. Each agent is just a Python script (\\~100 lines) that:\n\n1. Calls Ollama's API (`http://localhost:11434/api/generate`) with a system prompt that defines the agent's personality and expertise\n2. Reads an RSS feed or the platform feed for context\n3. Posts the output via HTTP to NeuroForge's API\n\nThat's it. No LangChain, no AutoGen, no complex orchestration. Windows Task Scheduler (or cron on Linux) triggers each script on a stagger ‚Äî one agent every 20 minutes. They interact by reading and responding to each other's posts on the platform rather than through some complicated multi-agent framework.\n\nFor your \"rounded view\" use case, you could spin up 3-4 agents with different system prompts (e.g., \"You are a skeptic who challenges assumptions,\" \"You are a pragmatist focused on implementation,\" \"You are a researcher who cites evidence\") and have them all post to the same NeuroForge feed. They'll naturally start responding to each other.\n\nI'd honestly skip OpenClaw for this ‚Äî it's powerful but massively over-engineered for what you need, and the security surface area is huge. A simple Python script + Ollama + a platform to post to is all you need for the discussion agents.\n\nThe \"tech team\" managing your file server is a different beast ‚Äî that's more in OpenClaw territory since it needs filesystem access. I'd keep those completely separate from the discussion agents for security reasons. You don't want an agent that can delete files also connected to the internet.\n\nIf you want to get your discussion agents running on NeuroForge, the platform is free and open for registration at [agents.glide2.app](http://agents.glide2.app) ‚Äî there's an OpenClaw integration guide in the docs but honestly the plain Python approach is simpler for your use case. Happy to help if you get stuck.",
              "score": 1,
              "created_utc": "2026-02-14 15:00:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5cmnu6",
          "author": "boba-cat02",
          "text": "Bro you are just HALLUCINATING LLM üòÇ",
          "score": 1,
          "created_utc": "2026-02-14 15:16:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5h6de3",
              "author": "Practical_Walrus_299",
              "text": "Ha, fair enough ‚Äî but that's kind of the point? The interesting part isn't whether the agents are \"right\" about anything. It's what happens when you put different model architectures in conversation with each other over time. Emergent citation networks, topic clustering, cross-model disagreements ‚Äî none of that was programmed in.\n\n\n\nThink of it less as \"AI being smart\" and more as a research sandbox for studying multi-agent behavior. The hallucinations are part of the data üòÑ",
              "score": 1,
              "created_utc": "2026-02-15 08:18:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5dpqva",
          "author": "testuser911",
          "text": "I would have open sourced it and enabled adding more experiments by the community.",
          "score": 1,
          "created_utc": "2026-02-14 18:34:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5h6giu",
              "author": "Practical_Walrus_299",
              "text": "Agreed, and that's the plan! The API is already open ‚Äî any agent framework can connect and participate. Full open-source repo is being prepped for launch week.\n\n\n\nThe idea is exactly what you described: community members can spin up their own agents with whatever models they want, define custom evaluation criteria, and run experiments. The platform handles the infrastructure (feeds, interactions, analytics) so people can focus on the interesting part ‚Äî designing agent behaviors and studying what emerges.\n\n\n\n[https://agents.glide2.app/docs](https://agents.glide2.app/docs)",
              "score": 1,
              "created_utc": "2026-02-15 08:18:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e5l2a",
          "author": "CooperDK",
          "text": "If I were you, I would use something faster, like llama.cpp or Koboldcpp....\n\nBtw this is how I generate chat datasets, but 6 agents is actually cool",
          "score": 1,
          "created_utc": "2026-02-14 19:55:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5h6iyq",
              "author": "Practical_Walrus_299",
              "text": "Good shout on llama.cpp ‚Äî I'm using Ollama which wraps it under the hood, but direct llama.cpp or Koboldcpp would definitely squeeze out better performance on the same hardware.\n\n\n\nThat's cool that you're generating chat datasets too! The multi-agent angle adds an interesting dimension ‚Äî instead of scripted conversations you get organic disagreements and topic drift that's harder to simulate. Currently running 12 agents with different models (Mistral, Llama 3.1, plus one Claude Haiku for comparison) and the quality gap between architectures is wild when you see them side by side.\n\n\n\nWhat models are you using for your dataset generation?",
              "score": 1,
              "created_utc": "2026-02-15 08:19:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5id1sg",
                  "author": "CooperDK",
                  "text": "I have used a few for different datasets (I have nine and have generated 48,000 samples for finetuning a Gemma-3-4b first. I used everything from Gemma-27b to Gemma-3 and also Satyr and Mistral. 7,300 of my samples are image discussions for vision and they are almost all with reasoning. I have spent two months on this. Everything is trained on a couple of books, and the whole Monster Girls Encyclopedia rewritten to an actual knowledge bank json, as well as generated character bios and species sheets made by an AI (an anthro specialist. \nThe dataset will be called GemmaWild and will be a story/chat generator that is of course specialized in anthro species and able to impersonate about 300 of them. It is already trained but I need to retrain due to a small mistake in the unsloth setup.\n\nEvery chat agent is impersonating with a different personality and mood (random per sample). I found that Gemma-3 (27B) is actually the best for this.\n\nEverything on a 5060 16 GB, but I train on a H200 on runpod, that takes a few hours. On the 5060 I use up 15 GB just for the 3B, and it takes more than fifteen hours.",
                  "score": 1,
                  "created_utc": "2026-02-15 14:14:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r3ynbz",
      "title": "Omni-Crawler: from a ton of links to a single md file to feed your LLMs",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r3ynbz/omnicrawler_from_a_ton_of_links_to_a_single_md/",
      "author": "EnthropicBeing",
      "created_utc": "2026-02-13 19:29:49",
      "score": 36,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "First things first: Yes, this post and the repo content were drafted/polished using¬†**Gemini**. No, I‚Äôm not a developer; I‚Äôm just a humble homelabber.\n\nI‚Äôm sharing a project I put together to solve my own headaches:¬†**Omni-Crawler**.\n\n# What is it?\n\nIt‚Äôs a hybrid script (CLI + Graphical Interface via Streamlit) based on¬†**Crawl4AI**. The function is simple: you give it a documentation URL (e.g., Caddy, Proxmox, a Wiki), and it returns a single, consolidated, and filtered¬†`.md`¬†file.\n\n# What is this for?\n\nIf you work with local LLMs (Ollama, Open WebUI) or even Claude/Gemini, you know that feeding them 50 different links for a single doc is a massive pain in the ass. And if you don't provide the context, the AI starts hallucinating a hundred environment variables, two dogs, and a goose. With this:\n\n1. You crawl the entire site in one go.\n2. It automatically cleans out the noise (menus, footers, sidebars).\n3. You upload the resulting¬†`.md`, and you have an AI with the up-to-date documentation in its permanent context within seconds.\n\n# On \"Originality\" and the Code\n\nLet‚Äôs be real: I didn‚Äôt reinvent the wheel here. This is basically a wrapper around¬†**Crawl4AI**¬†and¬†**Playwright**. The \"added value\" is the integration:\n\n* **Stealth Mode:**¬†Configured so servers (Caddy, I'm looking at you, you beautiful bastard) don't block you on the first attempt, using random User-Agents and real browser headers.\n* **CLI/GUI Duality:**¬†If you're a terminal person, use it with arguments. If you want something visual, launch it without arguments, and it spins up a local web app.\n* **Density Filters:**¬†It doesn't just download HTML; it uses text density algorithms to keep only the \"meat\" of the information.\n\nI'll admit the script was heavily¬†**\"vibe coded\"**¬†(it took me fewer than ten prompts).\n\n# Technical Stack\n\n* **Python 3.12**\n* **uv**¬†(for package management‚ÄîI highly recommend it)\n* **Crawl4AI**¬†\\+¬†**Playwright**\n* **Streamlit**¬†(for the GUI)\n\n**The Repo:**[https://github.com/ImJustDoingMyPart/omni-crawler](https://github.com/ImJustDoingMyPart/omni-crawler)\n\nIf this helps you feed your RAGs or just keep offline docs, there you go. Technical feedback is welcome. As for critiques about whether a bot or a human wrote this: please send them to my DMs along with your credit card number, full name, and security code.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r3ynbz/omnicrawler_from_a_ton_of_links_to_a_single_md/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o57w6p0",
          "author": "null_overload",
          "text": "Why not use context 7 mcp and any new version release this docs are outdated and \n\nAlso the ai will not have semantic search or context what to search look into continue.dev they index your files using lancedb to perform relevant search faster",
          "score": 3,
          "created_utc": "2026-02-13 19:41:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57xfwd",
              "author": "EnthropicBeing",
              "text": "Valid points! Here is why I took this approach:\n\n1. **Long Context > RAG (for my use case):** Modern models (Gemini 1.5 Pro, Claude 3.5 Sonnet, DeepSeek) have massive context windows. I prefer feeding the *entire* documentation into the context so the model understands the full structure and relationships, rather than relying on semantic search (RAG) which sometimes misses relevant chunks or hallucinates due to lack of global context.\n2. **Portability:** I use this across different environments (Open WebUI, raw scripts, cloud LLMs), not just in VS Code (where [Continue.dev](http://Continue.dev) shines). A simple Markdown file is the universal data format.\n3. **Freshness:** The idea is to run the crawler *on demand* when I start working on a tech stack, ensuring I have the latest version, rather than relying on a stale vector DB index.\n\nMCP is great, but sometimes a 50-line Python script that outputs a text file is just the simpler tool for the job.",
              "score": 2,
              "created_utc": "2026-02-13 19:48:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o57y17a",
                  "author": "null_overload",
                  "text": "Why not give a skills file where it can perform search and use your tool to get data instantaneous based on the package json or the requirement txt file",
                  "score": 4,
                  "created_utc": "2026-02-13 19:50:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5j4202",
                  "author": "Available-Craft-5795",
                  "text": "Why do you need to paste his question into ChatGPT?  \n",
                  "score": 1,
                  "created_utc": "2026-02-15 16:34:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o59imqb",
          "author": "x8code",
          "text": "Why not just use Firecrawl? [https://github.com/firecrawl/firecrawl](https://github.com/firecrawl/firecrawl) ",
          "score": 2,
          "created_utc": "2026-02-14 00:54:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3abol",
      "title": "Murmure 1.7.0 - A local voice interface for Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r3abol/murmure_170_a_local_voice_interface_for_ollama/",
      "author": "Al1x-ai",
      "created_utc": "2026-02-13 00:21:15",
      "score": 35,
      "num_comments": 16,
      "upvote_ratio": 0.96,
      "text": "Hi everyone,\n\nI‚Äôve just released Murmure 1.7.0, and I think it might be interesting for Ollama users.\n\nMurmure started as a local speech‚Äëto‚Äëtext tool. With 1.7.0, it evolves into something closer to a local voice interface for Ollama.\n\n# Main Ollama-related features\n\n# 1. LLM Connect\n\nDirect integration with Ollama to process transcribed voice input using ollama local models.\n\n# 2. Voice commands\n\nSelect text ‚Üí speak an instruction ‚Üí Ollama transforms it in background.\n\nExamples:\n\n* \"Correct this text\"\n* \"Rewrite this more concisely\"\n* \"Translate to English\"\n* \"Turn this into bullet points\"\n* ...\n\nEverything runs locally, completely free, fully offline, open source, no tracking, no telemtry, no bullshit.\n\nIt currently supports 25 European languages.\n\nI‚Äôm not making any money from this, just building something I wanted for myself and sharing it.\n\nFeedback from Ollama users would be very welcome.\n\n* Official website: [https://murmure.al1x-ai.com/](https://murmure.al1x-ai.com/)\n* GitHub: [https://github.com/Kieirra/murmure](https://github.com/Kieirra/murmure)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r3abol/murmure_170_a_local_voice_interface_for_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5373j2",
          "author": "redonculous",
          "text": "Very cool! Can I install it on my home server and access it via a url on different machines?",
          "score": 3,
          "created_utc": "2026-02-13 01:34:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54r4xu",
              "author": "Al1x-ai",
              "text": "There is an experimental API that allows you to call Murmure with a WAV file and receive the transcription in return. However, I haven‚Äôt worked on it much beyond that.\n\nSince Murmure can be triggered from any field in any application, the main goal is to use it directly on the machine where it‚Äôs needed, so it can handle the recording, post-processing, and Ollama post-processing locally. It‚Äôs not primarily designed for remote access.",
              "score": 1,
              "created_utc": "2026-02-13 08:27:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5a28vw",
                  "author": "x8code",
                  "text": "Oh darn ... was hoping this was like an OpenWebUI type of thing, where it's a server-hosted front-end that can talk to an LLM service on the network. I run all my backend services on headless Linux servers. Oh well, beggars can't be choosers. Cool idea either way!",
                  "score": 2,
                  "created_utc": "2026-02-14 02:59:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o54qqwg",
          "author": "Acrypto",
          "text": "Crazy, I had the same thought. I don't mind typing, but I'm lazy. Good on you for beating me to it!",
          "score": 2,
          "created_utc": "2026-02-13 08:24:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54rija",
              "author": "Al1x-ai",
              "text": "Haha, yes but you can still contribute to the project ;) it's not too late. \n\nYou can type at around 60 words per minute if you‚Äôre good, but when you speak it‚Äôs closer to 180. So it‚Äôs not just laziness‚Ä¶ it‚Äôs productivity!",
              "score": 1,
              "created_utc": "2026-02-13 08:31:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5a23dq",
          "author": "x8code",
          "text": "Dang that's pretty slick. Thank you for sharing. Works with vLLM and any OpenAI-compatible inference software also?",
          "score": 2,
          "created_utc": "2026-02-14 02:58:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5baowa",
              "author": "Al1x-ai",
              "text": "vLLM is definitely on my mind, but it‚Äôs not possible yet. However, you can already set up a remote Ollama instance if you modify the Ollama API URL in LLM Connect (at the bottom of the prompt screen).\n\nThe reason I didn‚Äôt make it OpenAI-compatible is that Murmure is designed with a privacy-first mindset, and I‚Äôm worried that people would use it with a cloud-based solution, which would defeat the whole purpose of paying special attention to privacy.",
              "score": 0,
              "created_utc": "2026-02-14 09:08:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54uec4",
          "author": "dropswisdom",
          "text": "If you install ollama+open webui, you get this out of the box, I believe. (not with ollama's own interface)",
          "score": 1,
          "created_utc": "2026-02-13 08:58:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54v5o3",
              "author": "Al1x-ai",
              "text": "I was probably too succinct in my post but it‚Äôs not the same as Open WebUI. Murmure isn‚Äôt meant to be a UI chat interface, it‚Äôs actually the opposite: it‚Äôs kind of a voice interface (a speech to text by nature).\n\nMurmure runs in the background and lets you write in any text field of any application using only your voice (dictate, generate, correct, or transcribe), without ever needing to switch to a separate UI like Open WebUI.",
              "score": 2,
              "created_utc": "2026-02-13 09:05:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54ugm8",
          "author": "idebugthusiexist",
          "text": "How is this any different than just integrating using whisper? I currently already speak to my various LLM personalities from any device and a router agent determines who I am talking to based on context or direct invocation. Just curious.",
          "score": 1,
          "created_utc": "2026-02-13 08:59:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54wb74",
              "author": "Al1x-ai",
              "text": "> How is this any different than just integrating using whisper?\n\nIf you mean OpenAI Whisper, the difference is both technical and practical.\n\nWhisper is an ASR model. Murmure also uses an ASR model, NVIDIA Parakeet (0.6B parameters). It is much lighter than Whisper Large (1.5B), starts faster, uses fewer resources, and provides better accuracy.\n\nBut the ASR is only the engine. Murmure is the complete solution around it.\n\nIt allows vocabulary extension, formatting control, and optional LLM post‚Äëprocessing (with Ollama). You can also modify selected text with a prompt, paste the last transcription instantly, and more.\n\nMost importantly, Murmure is a ready‚Äëto‚Äëuse system tool. It handles:\n- Full local processing (no cloud, no telemetry)  \n- Cross‚Äëplatform desktop integration  \n- Global shortcuts running in the background  \n- Safe clipboard handling and paste simulation  \n- Microphone selection  \n- Different recording modes (push‚Äëto‚Äëtalk, toggle, etc.)\n\nYou get all of this out of the box, without writing scripts or building your own integration layer.",
              "score": 2,
              "created_utc": "2026-02-13 09:16:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o54z0cv",
                  "author": "idebugthusiexist",
                  "text": "Ah okay, I see. You‚Äôve created a nicely packaged solution for general consumption. Admittedly, mine is entirely bespoke to my own platform, so it‚Äôs built for my own personal needs.",
                  "score": 3,
                  "created_utc": "2026-02-13 09:43:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o556vht",
          "author": "Vileteen",
          "text": "Do you support, or plan to support other local model providers, like LM Studio for example?",
          "score": 1,
          "created_utc": "2026-02-13 10:55:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o557uri",
              "author": "Al1x-ai",
              "text": "Good question.\n\nI chose Ollama mainly because it provides a simple local server with a clean HTTP API, and it fits well with Murmure‚Äôs philosophy of keeping things lightweight and minimal.\n\nFor Murmure, I need a local dependency that can handle model downloading, runtime management (GPU/CPU), and expose a stable API I can call from Murmure. Ollama does that with very little setup and integration complexity.\n\nThat said, I don‚Äôt know LM Studio deeply enough to compare them fairly. If it offers clear advantages for Murmure‚Äôs specific use case (local, lightweight, automation-friendly integration), I‚Äôd definitely be open to exploring it. \n \nFrom your perspective, what advantages do you see in LM Studio over Ollama for this kind of integration?",
              "score": 1,
              "created_utc": "2026-02-13 11:03:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5fc9v5",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-14 23:57:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5h926k",
              "author": "Al1x-ai",
              "text": "I'm sorry, I didn't understand.",
              "score": 1,
              "created_utc": "2026-02-15 08:44:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4bj70",
      "title": "I built a self-hosted AI platform with multi-model orchestration on Ollama ‚Äî models debate each other before answering you",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r4bj70/i_built_a_selfhosted_ai_platform_with_multimodel/",
      "author": "Zealousideal-Tap1302",
      "created_utc": "2026-02-14 04:50:58",
      "score": 33,
      "num_comments": 12,
      "upvote_ratio": 0.87,
      "text": "Been running Ollama locally for a while and kept hitting the same problem: one model is great at code, another at reasoning, another at creative writing. Switching between them manually got old fast.\n\nSo I built a platform where you pick a \"mode\" and it routes your question to the right model automatically. Code question ‚Üí coding model. Math ‚Üí reasoning model. Or you can let it decide (`auto` mode).\n\nThe part I'm most excited about: **Discussion Mode** ‚Äî multiple models actually debate each other on your question, cross-check facts, and synthesize a final answer. It's like having a panel of experts argue before giving you a response.\n\n**Screenshot:** \n\nhttps://preview.redd.it/lhoh0th48ejg1.png?width=2128&format=png&auto=webp&s=3b695ae796ce9b24d53f2a997e568e2c8250e4a2\n\n**What it does:**\n\n* Smart auto-routing across multiple Ollama models\n* AI-to-AI discussion (models cross-check each other)\n* Deep research agent (breaks down topics, searches web, writes reports)\n* MCP tool integration\n* RAG with document upload (PDF, code)\n* Long-term memory per user\n* Multi-user auth (JWT + OAuth)\n* Cluster support for multiple Ollama nodes\n\nTech stack: Node.js/Express, vanilla JS frontend (no React ‚Äî intentional), PostgreSQL, Ollama API. MIT licensed.\n\n**You can try it right now:** [http://rasplay.tplinkdns.com:52416](http://rasplay.tplinkdns.com:52416)\n\n**Source:** [https://github.com/openmake/openmake\\_llm](https://github.com/openmake/openmake_llm)\n\nThis started as a side project from a small open-source community in Korea that's been building hardware/software projects since 2013. Two of us are actively working on it, but there's way more to do than two people can handle ‚Äî especially frontend polish, RAG pipeline tuning, and i18n.\n\nIf anyone's interested in contributing or just wants to poke around, the repo is public and MIT licensed. Happy to answer any questions about the architecture or design choices.\n\nWhat model combinations are you all running for different tasks? Curious how others handle the multi-model problem.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r4bj70/i_built_a_selfhosted_ai_platform_with_multimodel/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5as8mi",
          "author": "Delicious-One-5129",
          "text": "Really impressive build. Multi model orchestration is where things get interesting, especially when you move beyond simple routing into structured discussion.",
          "score": 9,
          "created_utc": "2026-02-14 06:15:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bljvx",
              "author": "Zealousideal-Tap1302",
              "text": "Thanks, really appreciate that.   \nYou‚Äôre absolutely right ‚Äî things only start to get interesting once orchestration moves beyond simple model routing. The goal here was to treat models less like interchangeable endpoints and more like participants in a structured reasoning process.   \nRouting solves capability matching. Structured discussion solves blind spots. That shift ‚Äî from ‚Äúwhich model should answer?‚Äù to ‚Äúhow should models think together?‚Äù ‚Äî is where the real leverage starts to appear.",
              "score": 4,
              "created_utc": "2026-02-14 10:55:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5b27v6",
          "author": "Lonely_Ad_7282",
          "text": "this is solid. love the idea of models debating before answering ‚Äî feels like you‚Äôre getting a committee decision instead of a solo guess. ui looks clean too. haven‚Äôt tried that myself, gonna steal the concept for my next side project",
          "score": 5,
          "created_utc": "2026-02-14 07:46:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bluko",
              "author": "Zealousideal-Tap1302",
              "text": "Appreciate that ‚Äî glad the concept resonates.\n\nThe committee analogy is actually close to the intent. The goal wasn‚Äôt just parallel answers, but structured disagreement before synthesis. That extra friction tends to surface assumptions a single model might gloss over.\n\nIf you end up trying something similar in your side project, I‚Äôd suggest focusing less on the number of models and more on how disagreement is handled. That‚Äôs where most of the value comes from.\n\nWould be curious to see what you build with it.",
              "score": 1,
              "created_utc": "2026-02-14 10:58:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ajryw",
          "author": "Otherwise_Wave9374",
          "text": "Multi-model routing + debate mode is such a good fit for agentic systems, especially when you can force explicit disagreement and citations before the final synthesis. How are you deciding the router, rules-based, embeddings, or a small model that picks the specialist?\n\nAlso curious if youre doing any scoring step after the debate (like a critic agent that checks for tool-call feasibility / hallucinated facts). Ive been tracking a few lightweight eval ideas here: https://www.agentixlabs.com/blog/",
          "score": 4,
          "created_utc": "2026-02-14 05:05:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5azu4s",
          "author": "Radiant-Anteater-418",
          "text": "This is a really cool direction. The debate mode especially is interesting, feels like a practical way to reduce blind spots instead of trusting a single model.\n\nAuto routing plus local control via Ollama makes a lot of sense for self hosted setups. Curious how you handle conflict resolution when models strongly disagree before synthesis.",
          "score": 3,
          "created_utc": "2026-02-14 07:24:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bldt4",
              "author": "Zealousideal-Tap1302",
              "text": "ÏïÑÎûòÎäî ÌÜ§ÏùÑ Ï°∞Í∏à Îçî Î∂ÄÎìúÎüΩÍ≤å Îã§Îì¨ÏùÄ Ïª§ÎÆ§ÎãàÌã∞ ÏπúÌôî Î≤ÑÏ†ÑÏûÖÎãàÎã§.\n\nThanks for the thoughtful feedback ‚Äî and especially for calling out the conflict resolution piece.\n\nThe debate mode isn‚Äôt built around voting or majority wins. It‚Äôs closer to a structured cross-review process. When models strongly disagree, we don‚Äôt average them out. Instead:\n\n* `performCrossReview`¬†lets each model critique the others‚Äô reasoning, surfacing assumptions, missing constraints, and potential edge cases.\n* We break disagreements down into categories ‚Äî factual differences, interpretation gaps, risk tolerance, or even different objective framing.\n* If it‚Äôs a factual conflict, we trigger a lightweight evidence re-check. If it‚Äôs interpretive or strategic, we intentionally preserve that divergence instead of forcing convergence too early.\n\nThen in¬†`synthesizeFinalAnswer`, the goal isn‚Äôt to compress everything into one blended answer. It‚Äôs to:\n\n* Weigh trade-offs,\n* Identify the dominant constraints,\n* Choose a defensible path forward,\n* And clearly note which alternatives were considered and why they were set aside.\n\nSo the synthesis step is more about integration than reduction.\n\nOn the routing side,¬†`selectBrandProfileForAutoRouting`¬†goes beyond model swapping. It maps detected intent to a full pipeline configuration ‚Äî debate depth, critique intensity, synthesis strictness, and even local vs. remote model mix. That‚Äôs where Ollama-based local control becomes powerful in self-hosted setups: it enables consistent orchestration rather than simple fallback logic.\n\nReally appreciate the question. In our experience, conflict handling is where multi-model systems either become genuinely useful ‚Äî or just noisy.",
              "score": 3,
              "created_utc": "2026-02-14 10:54:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bga8g",
          "author": "Vileteen",
          "text": "Do you have a roadmap? I am interested to see if there are plans to include support for other LM providers. Cheers!",
          "score": 3,
          "created_utc": "2026-02-14 10:04:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rbkxk",
          "author": "Green-Ad-3964",
          "text": "wow, cool project thank you. Does the discussion mode keep all the models that are discussing in memory at the same time? If so...what can I do with 32GB vRAM?",
          "score": 1,
          "created_utc": "2026-02-16 21:47:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5v57ax",
              "author": "Zealousideal-Tap1302",
              "text": "Sorry for the late reply ‚Äî just saw your comment now!\n\nIf you check out my Git repo and¬†[http://rasplay.tplinkdns.com:33000](http://rasplay.tplinkdns.com:33000/), you'll see this project is primarily built around Ollama Cloud LLMs.\n\nThat said, to answer your question from a¬†**local VRAM perspective**¬†‚Äî\n\nIn multi-agent/discussion mode, models can be \"active\" simultaneously, so local VRAM usage scales with¬†**concurrent model loading + context/cache overhead**.\n\n**Practical rule of thumb for 32GB VRAM:**\n\n**High-quality single model**¬†‚Üí one 30B‚Äì34B class (4-bit quantized) ,¬†**Balanced parallel discussion**¬†‚Üí one 14B + one 7B class ,¬†**Light multi-agent setup**¬†‚Üí two to three 7B/8B models\n\n**What's tough on 32GB:**\n\n* Multiple 30B+ models loaded simultaneously\n* 70B-class models locally at usable speed/latency\n\nSo the bottom line ‚Äî¬†**hybrid orchestration**¬†is the way to go:\n\n* Keep one efficient local model resident for¬†**low-latency tasks**\n* Route heavier reasoning/discussion branches to¬†**Ollama Cloud models**\n* Cap local context length and concurrency to¬†**stabilize memory and response time**\n\nWith this kind of intentional local-cloud role splitting, 32GB VRAM is already very workable for this architecture. No need to stress about it¬†",
              "score": 2,
              "created_utc": "2026-02-17 13:46:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5wb9be",
                  "author": "Green-Ad-3964",
                  "text": "Thank you for your insights. Very interesting.",
                  "score": 1,
                  "created_utc": "2026-02-17 17:21:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r72my6",
      "title": "My experience with running small scale open source models on my own PC.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r72my6/my_experience_with_running_small_scale_open/",
      "author": "Dibru9109_4259",
      "created_utc": "2026-02-17 10:36:47",
      "score": 32,
      "num_comments": 23,
      "upvote_ratio": 0.94,
      "text": "I recently got exposed to **Ollama** and the realization that I could take the 2 Billion 3 Billion parameter models and run them locally in my small pc with limited capacity of **8 GB RAM** and just an **Intel i3** CPU and without any GPU made me so excited and amazed. \n\nThough  the experience of running such Billions parameter models with 2-5 GB  RAM consumption  was not a smooth experience. Firstly I run the \"**Mistral 7B**\" model in my ollama. The response was well structured and the reasoning was good but given the limitations of my hardwares, it took about **3-4 minutes** in generating every response.\n\nFor a smoother expereience, I decided to run a smaller model. I choose Microsoft's **phi3:mini** model which was trained on around **3.8 Billion** parameters. The experience with this model was quite smoother compared to the pervious Minstral 7B model. phi3:mini took about  7-8 secods for the cold start and once it was started, it was generating responses with **less than 0.5  seconds** of prompting. I tried to measure the token generating speed using my phone's stopwatch and the number of words generated by the model (NOTE: **1 token = 0.75 word**, on average). I found out that this model was generating 7.5 tokens per second on my PC. The experience was pretty smooth with such a speed and it was also able to do all kinds of basic chat and reasoning.\n\nAfter this I decided to test the limits so I downloaded two even smaller models - **tinyLLama**. While the model was much compact with just **1.1 Billion** parameters and just 0.67GB download size for the **4-bit (Q4\\_K\\_M) version**, its performance deteriorated sharply.\n\nWhen I first gave a simple Hi to this model it responded with a random unrelated texts about \"nothingness\" and the paradox of nothingness. I tried to make it talk to me but it kept elaborating in its own cilo about the great philosophies around the concept of nothingness thereby not responding to  whatever prompt I gave to it. Afterwards I also tried my hand at the **smoLlm**  and this one also hallucinated massively.\n\n**My Conclusion :**\n\nMy *hardware* capacity affected the *speed* of Token generated by the different models. While the 7B parameter Mistral model took several minutes to respond each time, *this problem was eliminated entirely once I went 3.8 Billion parameters and less.* All of the phi3:mini and even the ones that hallucinated heavily - smolLm and tinyLlama generated tokens instantly.\n\nThe *number of parameters determines the extent of intelligence* of the LLM. Going below the 3.8 Billion parameter phi3:mini f, all the tiny models hallucinated excessively even though they were generating those rubbish responses very quickly and almost instantly.\n\nThere was *a tradeoff between* ***speed*** *and* ***accuracy.*** Given the limited hardware capacity of my pc, going below 3.8 Billion parameter model gave instant speed but extremely bad accuracy while going above it gave slow speed but higher accuracy.\n\nSo this was my experience about experimenting with Edge AI and various open source models. **Please feel free to correct me whereever you think I might be wrong. Questions are absolutely welcome!**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r72my6/my_experience_with_running_small_scale_open/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5uhysz",
          "author": "p_235615",
          "text": "you can try https://ollama.com/library/granite3.3 the 2b model should run OK. \nOr https://ollama.com/library/lfm2.5-thinking, this one should run relatively fast.",
          "score": 5,
          "created_utc": "2026-02-17 11:06:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5unqla",
              "author": "Dibru9109_4259",
              "text": "Thans man! Taking note of these 2 models! I'll update their performances on my hardware!",
              "score": 3,
              "created_utc": "2026-02-17 11:54:23",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5uv8i3",
              "author": "InfraScaler",
              "text": "In theory this is good at tool calling, too. Didn't try it yet [hadad/LFM2.5-1.2B](https://ollama.com/hadad/LFM2.5-1.2B)",
              "score": 1,
              "created_utc": "2026-02-17 12:47:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5uteoo",
          "author": "Blinkinlincoln",
          "text": "Just want to say that I am really happy you just wrote this post without AI editing. It's refreshing on reddit now to see typos.¬†",
          "score": 5,
          "created_utc": "2026-02-17 12:35:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5v7wio",
              "author": "Dibru9109_4259",
              "text": "Glad that you took the typos to complement me insteas of pointing out the mistakes! I guess, in this AI age, making some typo mistakes is gonna be not just okay but a marker of originality üòÖ",
              "score": 1,
              "created_utc": "2026-02-17 14:01:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5uv5xb",
          "author": "InfraScaler",
          "text": "Yeah you can run small models on CPU, however when context starts to grow you're dead - also those models are not great at tool calling etc... so, more than anything they're just very interesting curiosities :)",
          "score": 2,
          "created_utc": "2026-02-17 12:46:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vz4z0",
              "author": "trolololster",
              "text": "functiongemma:270m would like a word",
              "score": 1,
              "created_utc": "2026-02-17 16:19:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5v8432",
          "author": "Prigozhin2023",
          "text": "Which models can be used for tools? E.g. with opencode cli.\n\n\nCan't seems like there's a gap in the market for small models that can be used with tools.",
          "score": 1,
          "created_utc": "2026-02-17 14:02:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vyzwl",
              "author": "trolololster",
              "text": "you could do a graph or some other agentic flow where you use something like functiongemma (270m parameters) for the tool-calling",
              "score": 1,
              "created_utc": "2026-02-17 16:19:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5vzu37",
                  "author": "Prigozhin2023",
                  "text": "tried functiongemma.. it seems to be focused on tasks mgmt, etc. not much of a talkers or collaborator.. haha.",
                  "score": 1,
                  "created_utc": "2026-02-17 16:23:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5vz7bk",
              "author": "Rtjandrews",
              "text": "Im using qwen3:8b on an rtx 3080 with tool calling. Early days but after a slight delay in loading up the model initially responses are reasonably fast. Certainly enough for my early experiments of using tool calling anyway",
              "score": 1,
              "created_utc": "2026-02-17 16:20:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5w0c2i",
                  "author": "Prigozhin2023",
                  "text": "the initial load is pretty lengthy, thereafter seems pretty ok. others that slightly faster are ministral-3:3b & lfm2.5-thinking:1.2b ... qwen3:4b is pretty ok as well. \n\n",
                  "score": 1,
                  "created_utc": "2026-02-17 16:25:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5vi2xw",
          "author": "CorneZen",
          "text": "I also tried a couple of the really small SLM in Ollama. My first question to every model is always, what is your purpose? These SLMs all hallucinated about nothingness, one about climbing a ladder. \nI think the way the basic Ollama app calls these SLMs may be causing this. Need to investigate some more.\n(I made sure to check the models were ‚ÄòOllama‚Äô compatible on huggingface)",
          "score": 1,
          "created_utc": "2026-02-17 14:55:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vj30x",
              "author": "Dibru9109_4259",
              "text": "Damn! How coincidental that when I too ran the \"smolLm\" , when I first said Hi to it, it responded with a random sermon about \"NOTHINGNESS\" and paradox of notingness! Lol!!\n\n",
              "score": 2,
              "created_utc": "2026-02-17 15:00:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5vjhys",
          "author": "Majinsei",
          "text": "Models below 3b always need fine-tuning~\n\nThey're practically made for that~ you train them on your tools and do a little transfer learning~",
          "score": 1,
          "created_utc": "2026-02-17 15:03:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vytwb",
          "author": "trolololster",
          "text": "write MOAR!\n\nedge inference is SO interesting.",
          "score": 1,
          "created_utc": "2026-02-17 16:18:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uf81u",
          "author": "TerryTheAwesomeKitty",
          "text": "Those are very valid and correct observations! It is kiiiind of impossible right now to run a generally well performing model on local, especially edge hardware. However what we did in my company's deployment was using REALLY specific, instruct models ( very good at doing predefined tasks ) for specific things. Now we have a lot of 8b and smaller models running for singular, predictable operations. They do great ( albeit a bit slow )!",
          "score": 1,
          "created_utc": "2026-02-17 10:42:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5unl9j",
              "author": "Dibru9109_4259",
              "text": "When I was experimenting with these models running on edge,  the first question that came to my mind was - given the limitations of these small models, WHAT COULD BE THE SPECIFIC BUSINESS OR NORMAL USECASES in which these early limited capacity small models can be used. So if you woldn't mind may I ask the particular use cases where you applied them in real life?",
              "score": 2,
              "created_utc": "2026-02-17 11:53:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5v178n",
                  "author": "TerryTheAwesomeKitty",
                  "text": "Well see, we deal with contracts for A LOT of clients, they all include a standard subsection from our greater set of services, as well as a start date, end date, value and company that the contract is with. \n\nIf we had any issue or needed to check, someone would have to manually open and look in the file. They are mostly .docx or .pdf documents, so sometimes that process was slow.   \n  \nSince in the end we had to parse text to find predictable and consistent occurrences ( obviously all contracts have the name of the company and the price, service value, etc., just sometimes worded differently ) and static parsing was not doing so well, we used Gemma fully locally to extract the key data from the markdown-converted documents and then save it in an internal database. ",
                  "score": 1,
                  "created_utc": "2026-02-17 13:24:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r735eo",
      "title": "Self Hosted Alternative to NotebookLM",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r735eo/self_hosted_alternative_to_notebooklm/",
      "author": "Uiqueblhats",
      "created_utc": "2026-02-17 11:06:53",
      "score": 22,
      "num_comments": 1,
      "upvote_ratio": 0.96,
      "text": "For those of you who aren't familiar with SurfSense, SurfSense is an open-source alternative to NotebookLM, Perplexity, and Glean.\n\nIt connects any LLM to your internal knowledge sources, then lets teams chat, comment, and collaborate in real time. Think of it as a team-first research workspace with citations, connectors, and agentic workflows.\n\nI‚Äôm looking for contributors. If you‚Äôre into AI agents, RAG, search, browser extensions, or open-source research tooling, would love your help.\n\n**Current features**\n\n* Self-hostable (Docker)\n* 25+ external connectors (search engines, Drive, Slack, Teams, Jira, Notion, GitHub, Discord, and more)\n* Realtime Group Chats\n* Hybrid retrieval (semantic + full-text) with cited answers\n* Deep agent architecture (planning + subagents + filesystem access)\n* Supports 100+ LLMs and 6000+ embedding models (via OpenAI-compatible APIs + LiteLLM)\n* 50+ file formats (including Docling/local parsing options)\n* Podcast generation (multiple TTS providers)\n* Cross-browser extension to save dynamic/authenticated web pages\n* RBAC roles for teams\n\n**Upcoming features**\n\n* Slide creation support\n* Multilingual podcast support\n* Video creation agent\n\nGitHub: [https://github.com/MODSetter/SurfSense](https://github.com/MODSetter/SurfSense)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r735eo/self_hosted_alternative_to_notebooklm/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5v41u6",
          "author": "MrKBC",
          "text": "I've tried to get SurfSense and the other two available options to run multiple times each. I'll just stick to Google's.",
          "score": 3,
          "created_utc": "2026-02-17 13:40:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5bkwf",
      "title": "Can I pull models from Huggingface?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r5bkwf/can_i_pull_models_from_huggingface/",
      "author": "Keensworth",
      "created_utc": "2026-02-15 10:39:56",
      "score": 20,
      "num_comments": 17,
      "upvote_ratio": 0.79,
      "text": "Hello, I'm new to Ollama and just finished installing it and integrating it with OpenWebUI. I'm wondering if it was possible to pull models from Huggingface via a API call or a plugin?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r5bkwf/can_i_pull_models_from_huggingface/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5hv5ui",
          "author": "Barachiel80",
          "text": "yes. goto the model page on huggingface to get the quant you want, select use inference provider ollama to capture the url. Then type ollama pull hf.co/urlofyourmodel/. If your ollama is connected to openwebui it will show up in the model list once the download is complete.",
          "score": 25,
          "created_utc": "2026-02-15 12:11:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ig14j",
              "author": "UnrealizedLosses",
              "text": "Oh that‚Äôs WAY easier than the way I‚Äôve been doing it‚Ä¶.",
              "score": 4,
              "created_utc": "2026-02-15 14:31:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5lizsz",
              "author": "laurentbourrelly",
              "text": "`ollama run` [`hf.co/{username}/{repository}`](http://hf.co/{username}/{repository})\n\n`ollama run` [`hf.co/{username}/{repository}:{quantization}`](http://hf.co/{username}/{repository}:{quantization})\n\n",
              "score": 4,
              "created_utc": "2026-02-16 00:01:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5hva9v",
              "author": "Barachiel80",
              "text": "there is also a way to insert the url into the openwebui front end from the admin panel",
              "score": 1,
              "created_utc": "2026-02-15 12:12:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ijpv3",
              "author": "most_crispy_owl",
              "text": "Oh shit that's a lot easier than what I've been doing - downloading a gguf and adding a model from a ModelFile",
              "score": 1,
              "created_utc": "2026-02-15 14:52:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hwida",
          "author": "Frogy_mcfrogyface",
          "text": "On the huggingface page for the model, on the right side it has a list of the quant sizes. Click on the one you want, and a side menu with come up.  Click on \"Use This model\" go down to Ollama and click on it. You'll see another pop up and itll show something like ollama run [hf.co/unsloth/gpt-oss-20b-GGUF:Q2\\_K](http://hf.co/unsloth/gpt-oss-20b-GGUF:Q2_K)  with a copy button next to it. Click it, then open CMD and paste it, but replace run with pull. Once its done, start up openwebui, or if its already open, refresh it and it should show up in the list. If you dont want to use CMD and just use openwebui, go to the model selection and paste ollama run [hf.co/unsloth/gpt-oss-20b-GGUF:Q2\\_K](http://hf.co/unsloth/gpt-oss-20b-GGUF:Q2_K) but delete ollama run and in the dropdown selection it should come up with \"pull\" [hf.co/unsloth/gpt-oss-20b-GGUF:Q2\\_K](http://hf.co/unsloth/gpt-oss-20b-GGUF:Q2_K) click on it and it should start downloading. \n\nThis is the only way I know, I hope this is what you were after",
          "score": 3,
          "created_utc": "2026-02-15 12:22:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5igu06",
          "author": "rorowhat",
          "text": "Yes, just be aware that they might not work well.",
          "score": 1,
          "created_utc": "2026-02-15 14:36:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5im7x5",
          "author": "bishopLucas",
          "text": "Try it",
          "score": 1,
          "created_utc": "2026-02-15 15:05:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5it3mj",
          "author": "mpw-linux",
          "text": "If the model is in Python script it will pull it from HuggingFace on the first run of the program. When you go to Huggingface to view models there is small example script on how to use it. ",
          "score": 1,
          "created_utc": "2026-02-15 15:40:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j9n2k",
          "author": "Oppa-AI",
          "text": "HuggingFace gguf models have an option to choose Ollama. Then paste that command to your console to pull it.",
          "score": 1,
          "created_utc": "2026-02-15 17:00:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ja0ed",
          "author": "Edgar505",
          "text": "You can convert a model from huggingface to ollama gguf using llamacpp",
          "score": 1,
          "created_utc": "2026-02-15 17:02:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i4crb",
          "author": "StardockEngineer",
          "text": "You might as well use llamacpp if this is what you u want to do.",
          "score": 1,
          "created_utc": "2026-02-15 13:21:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5i78af",
              "author": "Barachiel80",
              "text": "do you have a docker compose config of llamacpp that allows for easy model pull through a single command, and all models can be switched using the open-webui frontend?",
              "score": 2,
              "created_utc": "2026-02-15 13:39:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5iabkx",
                  "author": "StardockEngineer",
                  "text": "Llamacpp can pull hugging face models natively.    \n\nllama-server -hf  repo/model\n\nYou can go to hf itself and click ‚ÄúUse this model‚Äù to get the command.  \n\nTo start a host that can change models on the fly is literally just \n\nllama-server\n\nI‚Äôm sure you can ask an LLM to make you a Dockerfile to do this.",
                  "score": 1,
                  "created_utc": "2026-02-15 13:58:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5igy5p",
              "author": "CooperDK",
              "text": "Also, llama.cpp is much faster. Ollama is basically useless.",
              "score": 2,
              "created_utc": "2026-02-15 14:37:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5iohpz",
                  "author": "WaitformeBumblebee",
                  "text": "In my experience out of the box Ollama is faster. With llama.cpp you have to know how to configure all the options to get top performance for your system.",
                  "score": 0,
                  "created_utc": "2026-02-15 15:17:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5iec9l",
          "author": "boba-cat02",
          "text": "of course bro, at least read documentation of things that you download.....",
          "score": -3,
          "created_utc": "2026-02-15 14:22:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5wlwk",
      "title": "What kind of models can I run on my M5 MBP with 24GB RAM?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r5wlwk/what_kind_of_models_can_i_run_on_my_m5_mbp_with/",
      "author": "saintforlife1",
      "created_utc": "2026-02-16 01:49:32",
      "score": 18,
      "num_comments": 20,
      "upvote_ratio": 0.83,
      "text": "Just got a new M5 MacBook Pro with 24GB RAM. What kind of models are best suited to run on such a machine?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r5wlwk/what_kind_of_models_can_i_run_on_my_m5_mbp_with/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5mn3mx",
          "author": "band-of-horses",
          "text": "8b - 14b models are a sweet spot. Qwen 3 and Mistrlal have good options in that range. GPT OSS 20 runs decently too. For all these, use LM Studio and search specifically for MLX models, they run a lot better on macs than the generic versions. \n\nAlso with only 24gb of ram you need a very small context window for larger models. GPT 20b you'll need like 4k context, which really limtis it's use. For anything you need to work on more amounts of data you'll need to go down to an 8b model or even 3b to get some more space for context as you don't have enough ram for a 15gb model and 32k parameters of context without crashing your OS. ",
          "score": 10,
          "created_utc": "2026-02-16 04:18:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ng9rj",
          "author": "x8code",
          "text": "I would try some smaller models like Granite 4 or Microsoft Phi 4, Ministral 3 14B Reasoning, Qwen3 14B, etc.",
          "score": 6,
          "created_utc": "2026-02-16 08:24:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5p9qrn",
              "author": "notalentwasted",
              "text": "Phi 4 punches above its weight class for sure. Especially the reasoning plus 14b model.",
              "score": 3,
              "created_utc": "2026-02-16 15:58:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5rqfzb",
              "author": "austrobergbauernbua",
              "text": "\\+1 for granite. Extremely fast and I like the concise answering style.",
              "score": 2,
              "created_utc": "2026-02-16 23:03:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5onu2w",
          "author": "bharattrader",
          "text": "Since you are on a laptop, make sure to run them with power cable and not stress out the battery. Take care as to not hit the swap, or else connect an external SSD. Use LMStudio or llama.cpp, with Q4 models, around, 10-12GB in size. Try to keep parallel=1 and max out the context window. Keep some room in RAM.",
          "score": 6,
          "created_utc": "2026-02-16 14:07:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5orcmw",
              "author": "AnnoyedAvocado21",
              "text": "Since you're bringing up hardware constraints, I'd like to add: \n\nMy project would run for up to an hour straight and I kept a close eye on the activity monitor. As I noticed that my chip temperatures would get high I found a free utility Macs Fan Control that would override the MacOS fan control and keep the temp down on the chips - my logic being I'd rather replace the fan than the chips and I didn't want the MacOS to throttle itself because the Mac 'esthetic' is not to allow the fans to be blowing like a wind tunnel. \n\nI've gotten downvoted for even mentioning this like I'm an idiot and maybe I am. If anyone's interested here's the app I use. \n\n[https://crystalidea.com/macs-fan-control/support](https://crystalidea.com/macs-fan-control/support)",
              "score": 3,
              "created_utc": "2026-02-16 14:26:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5masab",
          "author": "stonecannon",
          "text": "there are actually a lot of models you can run -- the trick is finding the right parameter count.  my go-to model since starting with Ollama has been Gemma3 -- the 12b-parameter version is 8.1GB, which would fit easily into your 24GB of unified memory.\n\nthat's the trick -- when you find a model you're interested in, go to its details page, and it will show you all available versions, along with their sizes.  you may not be able to run the most complex (parameter-rich) version, but you can very likely run a smaller version of the model.\n\nthere are lots of possibilities besides Gemma3 -- have a look through the list and see what sounds interesting to you.  many of the recent models are marked \"Thinking\", which means they display their \"reasoning\" before generating a reply.  i don't like those as much, so i tend to stick to older models.\n\nthere are also the Cloud models, which have the advantage of being able to run full-size models, but the disadvantage that you have to connect to the Ollama cloud, which eliminates some of the privacy & security that draw people to local models in the first place.\n\n  \nhonestly, the best strategy is to just try a little bit of everything and see what you like best!  and have fun with it :)",
          "score": 6,
          "created_utc": "2026-02-16 02:54:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nbyne",
              "author": "saintforlife1",
              "text": "Thank you so much for the very thoughtful and helpful reply!",
              "score": 2,
              "created_utc": "2026-02-16 07:44:00",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5nptve",
              "author": "Low-Exam-7547",
              "text": "I love those cloud models!",
              "score": 2,
              "created_utc": "2026-02-16 09:55:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nnukc",
          "author": "eightaceman",
          "text": "This was just posted recently and should tell you https://github.com/Pavelevich/llm-checker",
          "score": 2,
          "created_utc": "2026-02-16 09:36:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tayab",
              "author": "rbalfanz",
              "text": "https://www.reddit.com/r/ollama/s/ZLXyw1x3fw",
              "score": 1,
              "created_utc": "2026-02-17 04:47:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5me7dt",
          "author": "SAPPHIR3ROS3",
          "text": "The answer is whatever model fits in the available ram, i am going to assume that 8 gb are occupied by the system and what not so the best model that fits with a decent context size assuming you will run de q4 version is either nemotron 3 nano 30b, glm 4.7 flash 30b (i raccomend the cerebras reap version 23b), gemma 3 27b or gpt oss 20b",
          "score": 1,
          "created_utc": "2026-02-16 03:17:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mkwjh",
          "author": "Ryanmonroe82",
          "text": "RNJ-1-Instruct-8b in F16/BF16 is incredible. Always try to use higher precision over more parameters",
          "score": 1,
          "created_utc": "2026-02-16 04:03:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mp7w9",
              "author": "Crafty_Ball_8285",
              "text": "Will this work on 16gb",
              "score": 1,
              "created_utc": "2026-02-16 04:34:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ndmq0",
              "author": "Firm_Reindeer_2868",
              "text": "How come higher precision over size?",
              "score": 1,
              "created_utc": "2026-02-16 07:59:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nlfm0",
          "author": "conroe_au",
          "text": "gpt-oss:20b if you wanna max it out",
          "score": 1,
          "created_utc": "2026-02-16 09:13:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o7d57",
          "author": "AnnoyedAvocado21",
          "text": "I have the M4 Pro chip and the same memory. I was working on a project that required a large context window and more than a dozen complex prompts run sequentially on the same exact input so I could compare the outputs for consistency - at least as far as this can be done using a probabilistic LLM technology. \n\nThe best I found in all my testing was Granite 8b. \n\nNow mine is a very niche use case. If you want to just have a local LLM to experiment with all the other responses in this thread contain great suggestions.   ",
          "score": 1,
          "created_utc": "2026-02-16 12:26:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5p2rei",
          "author": "ServersServant",
          "text": "Mistral 3.2 instruct 2506 q5_k_m is my favourite. Downside is it will inevitably become slower.¬†\n\nYou‚Äôre better off using a cloud GPU and just run openwebui in your machine.",
          "score": 1,
          "created_utc": "2026-02-16 15:25:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5p9ept",
          "author": "notalentwasted",
          "text": "I'm primarily on gpu offloaded set up but I tend to agree with everyone else on this one. 8-14b will be your sweet spot. Some will run better than others. Thinking models will run a bit slower since the thinking process can be extensive at times. You ABSOLUTELY can run qwen3,  deepseek, gemma3, I would say you'd get the most out of Qwen3-vl since it has vision, thinking and tool capabilities. \nThat said if you need one that things and orchestrates like gpt oss... I would absolutely recommend Nvidia Orchestrate 8b. That is a banger of a model for its size.",
          "score": 1,
          "created_utc": "2026-02-16 15:56:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5srk0q",
          "author": "gh0st777",
          "text": "You'll find that this will just be an exploration exprience rather than something you want to do long term. The output quality of models this size is far less than what you could get from a cheap subscription service. Plus the loads it will place on your machine will make it unusable for anything else on top.",
          "score": 0,
          "created_utc": "2026-02-17 02:40:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6tj0j",
      "title": "Lots of pain, finally a small breakthrough, is it enough? Sharing what I've done",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r6tj0j/lots_of_pain_finally_a_small_breakthrough_is_it/",
      "author": "Minimum-Two-8093",
      "created_utc": "2026-02-17 02:17:04",
      "score": 17,
      "num_comments": 4,
      "upvote_ratio": 0.8,
      "text": "I'm not overly knowledgeable about self-hosting models, but I do have a software engineering background and time on my hands to try to make it work, as well as an employer that's encouraging us to figure shit out (read: I'm being paid to do this).\n\nComparing cloud agents with local agents is never a good idea, but that's been my only frame of reference. I have an absolutely huge solution for an economic simulation that I've been building, recently in conjunction with cloud agents.\n\nUp until now, I've been using Chat-GPT (web) as a designer and prompt engineer, and Opus 4.5 as a coding model through Claude Code. I've been treating this approach as if I'm the architect, and the agents are my junior to middling developers. This has been working very well.\n\nBut, I've wanted to find a use-case where my local machine is used for some of the work - I'm sick of paying for cloud agents, running out of quota continually, and sharing my information freely.\n\nFor the past 3 weeks, I've been struggling with Ollama hosted models, trying to find the right use-cases. I have an RTX4090 and have been dancing between Qwen, GPT-OSS, and DeepSeek derivatives of Qwen. I have docker running on an old 1U server in my garage, currently only hosting Open WebUI, this is exposing Ollama hosted models to all of my devices via Tailscale.\n\nI'm using [Continue.dev](http://Continue.dev) in VS Code.\n\nThat's the background, now the problem statement:\n\n>When trying to use my self-hosted \"agents\" for coding tasks, nothing *felt* good. Attempting to edit files and failing to reference them properly just felt like *friction*. Unintended files ended up overwritten.\n\nI have the following folder structure for Continue to load when VS Code is opened:\n\n>./.continue/rules/\n\nAnd inside that folder are the following:\n\n* 00-path-grounding .md\n* 00-project-context .md\n* 08-path-grounding-hard-stop .md\n* 08b-no-fake-tools .md\n* 09-repomap-maintenance .md\n* 10-determinism .md\n* 11-simulation-boundaries .md\n* 12-contract-invariants .md\n\nI've also got another repo with my project agnostic rules files and a script which copies them into the same *rules* folder. This is so that I can keep all of my projects consistent with each other if they're using my local models.\n\n* shared-01-general .md\n* shared-02-safe-edits .md\n* shared-03-tests-first .md\n* shared-04-diff-discipline .md\n\nThis has been hit and miss, mainly because tool usage has also been hit and miss. What's improved this however are the built-in custom providers for Continue.\n\n[https://docs.continue.dev/customize/deep-dives/custom-providers#built-in-context-providers](https://docs.continue.dev/customize/deep-dives/custom-providers#built-in-context-providers)\n\nHere's the Continue config.yaml file I've settled on, including the providers I've chosen:\n\n    // config.yaml\n    \n    name: Local Config\n    version: 1.0.0\n    schema: v1\n    \n    context:\n    ¬† - provider: file\n    ¬† - provider: code\n    ¬† - provider: open\n    ¬† ¬† params:\n    ¬† ¬† ¬† onlyPinned: true\n    ¬† - provider: clipboard\n    ¬† - provider: tree\n    ¬† - provider: repo-map\n    ¬† ¬† params:\n    ¬† ¬† ¬† includeSignatures: false # default true\n    \n    models:\n    ¬† - name: GPT-OSS Chat\n    ¬† ¬† provider: ollama\n    ¬† ¬† apiBase: http://localhost:11434\n    ¬† ¬† model: gpt-oss:20b\n    ¬† ¬† roles:\n    ¬† ¬† ¬† - chat\n    \n    ¬† - name: Qwen3 Coder\n    ¬† ¬† provider: ollama\n    ¬† ¬† apiBase: http://localhost:11434\n    ¬† ¬† model: qwen3-coder:30b\n    ¬† ¬† roles:\n    ¬† ¬† ¬† - chat\n    ¬† ¬† ¬† - edit\n    ¬† ¬† ¬† - apply\n    \n    ¬† - name: Qwen2.5 Autocomplete\n    ¬† ¬† provider: ollama\n    ¬† ¬† apiBase: http://localhost:11434\n    ¬† ¬† model: qwen2.5-coder:1.5b-base\n    ¬† ¬† roles:\n    ¬† ¬† ¬† - autocomplete\n    \n    ¬† - name: Nomic Embed\n    ¬† ¬† provider: ollama\n    ¬† ¬† apiBase: http://localhost:11434\n    ¬† ¬† model: nomic-embed-text:latest\n    ¬† ¬† roles:\n    ¬† ¬† ¬† - embed\n\nThe end result seems to be that I have finally settled on something *kinda* useful.\n\nThis may look simple (it is), but it's the first scoped refactor of an existing piece of code where the agent hasn't screwed *something* up.\n\nhttps://preview.redd.it/jgt5b0hmnyjg1.png?width=2728&format=png&auto=webp&s=25e57ca54e0d0d35116c47df872e7d7dc7f1e18a\n\nAs mentioned, this code base is quite large and set to get bigger.\n\nI've been used to cloud agents achieving quite amazing things while significantly boosting throughput (at least 20x what I'm capable of, probably more).\n\nI'd wanted to have my local models do the same, but in reality that was completely unrealistic; a 4090 cannot compete with cloud inference.\n\nWhere I *think* I've settled now (especially since a significant portion of my simulation is complete) is that my local agents can probably help to augment me more now that I am moving onto front-end implementation. I think that if I can constrain myself to only expect the local agents to help me with *the boring shit* like boilerplate and mass data entry, I will still save myself significant amounts of time; e.g. that edit would have taken me a couple of minutes of data entry, the prompt took a few seconds to write, and a few seconds to execute. I think it's likely that I'll keep using cloud agents for gnarly work, and local agents for the simpler things. That's not bad.\n\nPerhaps that's the sweet-spot.\n\nI don't really know what I want to get from this post, perhaps just to start a conversation.\n\n**Are you working on large projects with locally hosted models?**\n\n**How has your experience been?**\n\nIf I'm missing anything obvious, let me know.\n\nEdit: The markdown filenames attempted to be clickable links, edited to add a space to stop it.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r6tj0j/lots_of_pain_finally_a_small_breakthrough_is_it/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5tej80",
          "author": "Professional-Yak4359",
          "text": "Thank you for sharing your experience. I am a scientist and I work on MatLab and cuda for computational purposes (mostly FP64 compute) so I am not as advanced as you are. \n\nMy experience for local llm has been decent actually. I used Opus (Claude Pro) to plan and write clear instruction. Then I use locally hosted qwen coder 30b (8 x 5070 ti) via cline and coder to implement the instruction by opus. Then I use Opus to verify. Once confirmed, I used qwen3 coder to run and monitor the running of the codes and for bugs (sometime overnight as my code is computationally expensive). The compute code itself runs on a tesla v100. \n\nThe dual machine setup works quite well for me. Qwen3 monitor and debug typos and dimensional mismatch.",
          "score": 7,
          "created_utc": "2026-02-17 05:13:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tjs3r",
              "author": "chensium",
              "text": "This is a cleverly efficient setup",
              "score": 4,
              "created_utc": "2026-02-17 05:54:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5u3cgj",
          "author": "social_tech_10",
          "text": "I'm in a similar boat, but running llama.cpp server in the garage, as both the api and webui, using llama-swap to see model status and live logs.  I have long years of coding experience, but only days of agentic coding experience, so I'm still trying to figure out what works.  For what it's worth, I've been using KiloCode plugin for VScode, which seems to work pretty well with a variety of models.  It's not flawless, but at least I haven't had any unintended files overwritten.  My workflow is git-based, but it's still something I'm glad I haven't had to deal with while using kilocode.  I've been test-driving a variety of different local models.  I'm currently enjoying Qwen3-Next-80B-A3B-Thinking-Q4, which runs faster and smarter than most I've tried.\n\nWould you mind sharing a link to your markdown files?  I'm not sure what I can learn from them, but that's kinda why I'm here, and anything might help.",
          "score": 2,
          "created_utc": "2026-02-17 08:50:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uk8i8",
          "author": "alias454",
          "text": "I tried using continue on my local box. It does work and would connect, but for me, the models were just too slow. I've only got an rtx2060 on my laptop. However, I've been meaning to setup that plugin to point at my little M4 Mac mini to see if that makes it bearable to use. I use that plugin with pycharm too.",
          "score": 1,
          "created_utc": "2026-02-17 11:26:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r27yqt",
      "title": "GLM5 in Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r27yqt/glm5_in_ollama/",
      "author": "quantumsequrity",
      "created_utc": "2026-02-11 20:03:45",
      "score": 15,
      "num_comments": 5,
      "upvote_ratio": 0.71,
      "text": "Guys they've released GLM 5 cloud version in ollama go try it out, it's pretty cool not upto claude opus 4.5 or 4.6 for a open source model it's efficient,..\n\n[https://ollama.com/library/glm-5](https://ollama.com/library/glm-5)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r27yqt/glm5_in_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4uxfml",
          "author": "SamSLS",
          "text": "Yea but cloud only kinda not really Ollama but Ollama premium.",
          "score": 14,
          "created_utc": "2026-02-11 20:05:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50s60m",
          "author": "No-Intention-5521",
          "text": "I think they only support cloud for not :(( ",
          "score": 1,
          "created_utc": "2026-02-12 18:09:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55ki4l",
          "author": "_metamythical",
          "text": "How's the performance compared to Claude Opus 4.5",
          "score": 1,
          "created_utc": "2026-02-13 12:40:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59ndgc",
          "author": "Badger-Purple",
          "text": "It‚Äôs like 500Gb at 4 bits, just download from huggingface",
          "score": 1,
          "created_utc": "2026-02-14 01:24:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vfbc2",
          "author": "terranqs",
          "text": "Ollama quantized models for the cloud service?",
          "score": -1,
          "created_utc": "2026-02-11 21:32:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3rwem",
      "title": "Anubis - Open Source Local LLM Benchmarking Suite for Ollama and MLX",
      "subreddit": "ollama",
      "url": "https://v.redd.it/3zx4rkmz3ajg1",
      "author": "peppaz",
      "created_utc": "2026-02-13 15:19:21",
      "score": 15,
      "num_comments": 5,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r3rwem/anubis_open_source_local_llm_benchmarking_suite/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o56eijq",
          "author": "peppaz",
          "text": "https://github.com/uncSoft/anubis-oss\n\nBinary Available on Github - Please star so I can distribute on Homebrew as a Cask\n\n# Anubis\nhttps://imgur.com/a/X64WsWY\n\n**Local LLM Testing & Benchmarking for Apple Silicon**\n\nAnubis is a native macOS app for benchmarking, comparing, and managing local large language models using any OpenAI-compatible endpoint - Ollama, MLX, LM Studio Server, OpenWebUI, Docker Models, etc. Built with SwiftUI for Apple Silicon, it provides real-time hardware telemetry correlated with full, history-saved inference performance - something no CLI tool or chat wrapper offers. Export benchmarks directly without having to screenshot, and export the raw data as .MD or .CSV from the history. You can even `OLLAMA PULL` models directly within the app.\n\n\n\n## Why Anubis?\n\nThe local LLM ecosystem on macOS is fragmented:\n\n- **Chat wrappers** (Ollama, LM Studio, Jan) focus on conversation, not systematic testing\n- **Performance monitors** (asitop, macmon, mactop) are CLI-only and lack LLM context\n- **Evaluation frameworks** (promptfoo) require YAML configs and terminal expertise\n- **No tool** correlates hardware metrics (GPU / CPU / ANE / power / memory) with inference speed in real time\n\nAnubis fills that gap with three integrated modules - all in a native macOS app.\n\n---\n\n## Features\n\n### Benchmark\n\nReal-time performance dashboard for single-model testing.\n\n- Select any model from any configured backend\n- Stream responses with live metrics overlay\n- **8 metric cards**: Tokens/sec, GPU %, CPU %, Time to First Token, Process Memory, Model Memory, Thermal State, GPU Frequency\n- **7 live charts**: Tokens/sec, GPU utilization, CPU utilization, process memory, GPU/CPU/ANE/DRAM power, GPU frequency - all updating in real time\n- **Power telemetry**: Real-time GPU, CPU, ANE, and DRAM power consumption in watts via IOReport\n- **Process monitoring**: Auto-detects backend process by port (Ollama, LM Studio, mlx-lm, vLLM, etc.) with manual process picker\n- Detailed session stats: peak tokens/sec, average token latency, model load time, context length, eval duration, power averages\n- Configurable parameters: temperature, top-p, max tokens, system prompt\n- **Prompt presets** organized by category (Quick, Reasoning, Coding, Creative, Benchmarking)\n- **Session history** with full replay, CSV export, and Markdown reports\n- Expanded full-screen metrics dashboard\n- **Image export**: Copy to clipboard, save as PNG, or share - 2x retina rendering with watermark, respects light/dark mode\n\n### Arena\n\nSide-by-side A/B model comparison with the same prompt.\n\n- Dual model selectors with independent backend selection\n- **Sequential** mode (memory-safe, one at a time) or **Parallel** mode (both simultaneously)\n- Shared prompt, system prompt, and generation parameters\n- Real-time streaming in both panels\n- **Voting system**: pick Model A, Model B, or Tie - votes are persisted\n- Per-panel stats grid (9 metrics each)\n- Model manager: view loaded models and unload to free memory\n- Comparison history with voting records\n\n### Vault\n\nUnified model management across all backends.\n\n- Aggregated model list with search and backend filter chips\n- Running models section with live VRAM usage\n- Model inspector: size, parameters, quantization, family, context window, architecture details, file path\n- **Automatic metadata enrichment** for OpenAI-compatible models - parses model IDs for family and parameter count, scans `~/.lmstudio/models/` and `~/.cache/huggingface/hub/` for disk size, quantization, and path\n- Pull new models, delete existing ones, unload from memory\n- Popular model suggestions for quick setup\n- Total disk usage display\n\n## Supported Backends\n\n| Backend | Type | Default Port | Setup |\n|---------|------|--------------|-------|\n| **Ollama** | Native support | 11434 | Install from [ollama.com](https://ollama.com) - auto-detected on launch |\n| **LM Studio** | OpenAI-compatible | 1234 | Enable local server in LM Studio settings |\n| **mlx-lm** | OpenAI-compatible | 8080 | `pip install mlx-lm && mlx_lm.server --model <model>` |\n| **vLLM** | OpenAI-compatible | 8000 | Add in Settings |\n| **LocalAI** | OpenAI-compatible | 8080 | Add in Settings |\n\nAny OpenAI-compatible server can be added through **Settings > Add OpenAI-Compatible Server** with a name, URL, and optional API key.\n\n---\n\n## Hardware Metrics\n\nAnubis captures Apple Silicon telemetry during inference via IOReport and system APIs:\n\n| Metric | Source | Description |\n|--------|--------|-------------|\n| GPU Utilization | IOReport | GPU active residency percentage |\n| CPU Utilization | `host_processor_info` | Usage across all cores |\n| GPU Power | IOReport Energy Model | GPU power consumption in watts |\n| CPU Power | IOReport Energy Model | CPU (E-cores + P-cores) power in watts |\n| ANE Power | IOReport Energy Model | Neural Engine power consumption |\n| DRAM Power | IOReport Energy Model | Memory subsystem power |\n| GPU Frequency | IOReport GPU Stats | Weighted average from P-state residency |\n| Process Memory | `proc_pid_rusage` | Backend process `phys_footprint` (includes Metal/GPU allocations) |\n| Thermal State | `ProcessInfo.thermalState` | System thermal pressure level |\n\n### Process Monitoring\n\nAnubis automatically detects which process is serving your model:\n\n- **Port-based detection**: Uses `lsof` to find the PID listening on the inference port (called once per benchmark start)\n- **Backend identification**: Matches process path and command-line args to identify Ollama, LM Studio, mlx-lm, vLLM, LocalAI, llama.cpp\n- **Memory accounting**: Uses `phys_footprint` (same as Activity Monitor) which includes Metal/GPU buffer allocations - critical for MLX and other GPU-accelerated backends\n- **LM Studio support**: Walks Electron app bundle descendants to find the model-serving process\n- **Manual override**: Process picker lets you select any process by name, sorted by memory usage\n\nMetrics degrade gracefully - if IOReport access is unavailable (e.g., in a VM), Anubis still shows inference-derived metrics.\n\n---\n\n## Requirements\n\n- **macOS 15.0** (Sequoia) or later\n- **Apple Silicon** (M1 / M2 / M3 / M4 / M5 +) - Intel is not supported\n- **8 GB** unified memory minimum (16 GB+ recommended for larger models)\n- At least one inference backend installed (Ollama recommended)\n\n---\n\n## Getting Started\n\n### 1. Install Ollama (or another backend)\n\n```bash\n# macOS - install Ollama\nbrew install ollama\n\n# Start the server\nollama serve\n\n# Pull a model\nollama pull llama3.2:3b\n```\n\n### 2. Build & Run Anubis\n\n```bash\ngit clone https://github.com/uncSoft/anubis-oss.git\ncd anubis-oss/anubis\nopen anubis.xcodeproj\n```\n\nIn Xcode:\n1. Set your development team in **Signing & Capabilities**\n2. Build and run (`Cmd+R`)\n\nAnubis will auto-detect Ollama on launch. Other backends can be added in Settings.\n\n### 3. Run Your First Benchmark\n\n1. Select a model from the dropdown\n2. Type a prompt or pick one from **Presets**\n3. Click **Run**\n4. Watch the metrics light up in real time\n\n---\n\n## Building from Source\n\n```bash\n# Clone\ngit clone https://github.com/uncSoft/anubis-oss.git\ncd anubis-oss/anubis\n\n# Build via command line\nxcodebuild -scheme anubis-oss -configuration Debug build\n\n# Run tests\nxcodebuild -scheme anubis-oss -configuration Debug test\n\n# Or just open in Xcode\nopen anubis.xcodeproj\n```\n\n### Dependencies\n\nResolved automatically by Swift Package Manager on first build:\n\n| Package | Purpose | License |\n|---------|---------|---------|\n| [GRDB.swift](https://github.com/groue/GRDB.swift) | SQLite database | MIT |\n| Swift Charts | Data visualization | Apple |\n\n\n---\n\n## Data Storage\n\nAll data is stored locally - nothing leaves your machine.\n\n| Data | Location |\n|------|----------|\n| Database | `~/Library/Application Support/Anubis/anubis.db` |\n| Exports | Generated on demand (CSV, Markdown) |\n| Preferences | UserDefaults |\n\n\n### Adding a New Backend\n\n1. Create a new file in `Integrations/` implementing `InferenceBackend`\n2. Register it in `InferenceService`\n3. Add configuration UI in `Settings/`\n4. That's it - the rest of the app works through the protocol\n\n---\n\n## Support the Project\n\nIf Anubis is useful to you, consider [buying me a coffee on Ko-fi](https://ko-fi.com/jtatuncsoft/tip) or [sponsoring on GitHub](https://github.com/sponsors/uncSoft). It helps fund continued development and new features.\n\nA sandboxed, less feature rich version is also available on the [Mac App Store](https://apps.apple.com/us/app-bundle/the-architects-toolkit/id1874965091?mt=12) if you prefer a managed install.\n\n---\n\n## License\n\nGPL-3.0 License ‚Äî see [LICENSE](LICENSE) for details.\n\n**Other projects:** [DevPad](https://www.devpadapp.com) ¬∑ [Nabu](https://www.devpadapp.com/nabu.html)",
          "score": 1,
          "created_utc": "2026-02-13 15:23:58",
          "is_submitter": true,
          "replies": [
            {
              "id": "o5hdzjy",
              "author": "Altruistic-Event-145",
              "text": "Too much text for reddit",
              "score": 1,
              "created_utc": "2026-02-15 09:32:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5f8skl",
          "author": "rorowhat",
          "text": "Now port it to PC so everyone can use it.",
          "score": 1,
          "created_utc": "2026-02-14 23:35:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f9qs4",
              "author": "peppaz",
              "text": "Make your own and post it",
              "score": 2,
              "created_utc": "2026-02-14 23:41:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5jhu2s",
                  "author": "mac10190",
                  "text": "I love it ü§£\n\nThe sassy no bs reply. Hats off to you my good fellow. On that merit alone I'll be checking out your project. Lol\n\nI'll be back with feedback once I get it deployed.",
                  "score": 2,
                  "created_utc": "2026-02-15 17:41:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r2wnw8",
      "title": "Plano 0.4.6. Signals-based tracing for agents via a TUI",
      "subreddit": "ollama",
      "url": "https://v.redd.it/2pdugwhl23jg1",
      "author": "AdditionalWeb107",
      "created_utc": "2026-02-12 15:38:20",
      "score": 13,
      "num_comments": 1,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r2wnw8/plano_046_signalsbased_tracing_for_agents_via_a/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4zvtta",
          "author": "AdditionalWeb107",
          "text": "[https://github.com/katanemo/plano](https://github.com/katanemo/plano)",
          "score": 2,
          "created_utc": "2026-02-12 15:38:41",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3ufiw",
      "title": "I built an assistant and editor for just Ollama initially. Now you can talk with it, use other models and control text editors, engines, and generate apps.",
      "subreddit": "ollama",
      "url": "https://v.redd.it/gts8tz81lajg1",
      "author": "Ollie_IDE",
      "created_utc": "2026-02-13 16:53:26",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r3ufiw/i_built_an_assistant_and_editor_for_just_ollama/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o56z737",
          "author": "Ollie_IDE",
          "text": "Website: [https://ollie-ide.com](https://ollie-ide.com)",
          "score": 1,
          "created_utc": "2026-02-13 17:02:25",
          "is_submitter": true,
          "replies": [
            {
              "id": "o5gg6dl",
              "author": "nicholas_the_furious",
              "text": "You need to have a free trial or something, man. Hard to shell out $20 and not be able to try something first. I downloaded but thought I'd see something other than a login right away.",
              "score": 1,
              "created_utc": "2026-02-15 04:26:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r460vk",
      "title": "Completely free - no online required (after install obviously) with tts and stt setup guide?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r460vk/completely_free_no_online_required_after_install/",
      "author": "mail4youtoo",
      "created_utc": "2026-02-14 00:28:18",
      "score": 8,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "Could really use a little help here.\n\nI'm looking for a completely free - no online required (after install obviously) with tts and stt setup guide?\n\nI would prefer to use my old PC (8700k and 1080ti) but I also have a mac mini m4\n\nI've seen many many guides online (sites and videos) but there always seems to be a catch...  They use online APIs, have some sort of cost with tokens that need to be purchased.  Use alternative hardware like a raspberry pi or a NAS or some supercomputer with 1TB of video memory.\n\nI would like to start working with my own local AI through Ollama with the ability to have text to speech and speech to text.  \n\nI am looking to have a AI where I can practice speaking another language but mostly for experimentation to learn what AI chat is all about and be able to create my own tailored models eventually.\n\nWould someone be able to point me to a guide (written or video) where everything is free with no online required or an online API with purchased tokens?\n\nPlease?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r460vk/completely_free_no_online_required_after_install/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5dprlz",
          "author": "sheykastarshadow",
          "text": "STT https://github.com/ggml-org/whisper.cpp\n\nTTS https://huggingface.co/hexgrad/Kokoro-82M\n\nOr\n\nhttps://github.com/QwenLM/Qwen3-TTS\n\nEverything open source",
          "score": 3,
          "created_utc": "2026-02-14 18:34:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59wy5x",
          "author": "XxAnomo305",
          "text": "easiest way to do it is use python. that's how I have it. you use office googles speech to text, and then you can use piper or kokoro for text to speech. and ofc that's all running offline using ollama. python puts it all together.",
          "score": 2,
          "created_utc": "2026-02-14 02:25:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5a7le0",
              "author": "mail4youtoo",
              "text": "Thank you",
              "score": 1,
              "created_utc": "2026-02-14 03:36:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5aewab",
          "author": "Minimum-Two-8093",
          "text": "What you're going to struggle with is VRAM, and if you allow swapping to your system memory, your inference speeds will take a nose dive. You just won't have many options for useful models with 11GB VRAM due to still needing a decent context size to convert back and forth. \n\nTry it of course though, I'm not saying not to. \n\nI'm using a 4090 and have been for a couple of weeks and am still getting substandard results (vs my expectations).",
          "score": 1,
          "created_utc": "2026-02-14 04:28:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gubhp",
          "author": "BidWestern1056",
          "text": "use npcsh's yapper features, these operate wholly with local models\n\n[https://github.com/npc-worldwide/npcsh](https://github.com/npc-worldwide/npcsh)",
          "score": 1,
          "created_utc": "2026-02-15 06:23:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3mt4l",
      "title": "Understanding models |Subscription replacement?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r3mt4l/understanding_models_subscription_replacement/",
      "author": "LavishnessPlane4512",
      "created_utc": "2026-02-13 11:31:42",
      "score": 8,
      "num_comments": 8,
      "upvote_ratio": 0.91,
      "text": "Hey, I‚Äôve been using Claude code, codex and antigravity for work but I‚Äôm realizing that over the years I could just buy a good computer and run models locally. Company I work for is offering a very good voucher to renew my equipment and I can add extra to get a very good one so I‚Äôm considering buying a Mac Studio with 256GB memory to replace my subscriptions\n\nI‚Äôm just trying to understand and land my expectations on what I can actually get with that power, are there LLMs close or as good as opus 4.5/6 I could run with that? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r3mt4l/understanding_models_subscription_replacement/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o55dr27",
          "author": "timbo2m",
          "text": "You need stacks of VRAM, like 512GB to fit one of those trillion parameters models in. I think the closest you could hope for is running Kimi K2.5 quant 3 on a fully specced out Mac Studio with 512GB of unified memory, that's the most efficient way to get lots of VRAM but the ceiling is 512GB based on what apple sell. \n\nAlternatively you can stuff 512GB-2TB of RAM into a machine with some 16-32GB GPU and let your CPU go wild trying to shuffle between RAM and VRAM. It'll work but it's slow. \n\nThe best I've got running is qwen 3 coder next 80B 4 bit quant on an i9 + 4090 + 32GB RAM. The CPU gets a big workout during inference but it gives 35 tokens per second - so a bit slow, but workable.",
          "score": 5,
          "created_utc": "2026-02-13 11:52:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55dzk1",
              "author": "timbo2m",
              "text": "By the way, plug your specs into huggingface to see what you can run. For coding, I think this will be a screamer on your new mac https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF\n\nIt's only 80B though, Claude opus is estimated at 3000B I think (undisclosed)\n\nIf you can afford the 512 GB Mac, let me know how the 3 bit quant of this goes https://huggingface.co/unsloth/Kimi-K2.5-GGUF\n\nOh by the way, antigravity does NOT work with local models (as far as I know)",
              "score": 5,
              "created_utc": "2026-02-13 11:54:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o57hbtb",
                  "author": "Sax0drum",
                  "text": "A bit off topic but it's there any reason why the big players do not disclose how many parameters their models have? Would be a great advertisment wouldn't it?",
                  "score": 1,
                  "created_utc": "2026-02-13 18:29:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o55sou1",
              "author": "cowwoc",
              "text": "Per Claude Code, you'd need anywhere from $40k to 400k USD worth of hardware to serve a single user depending on the desired latency. Unfortunately, still outside anyone's budget. Which leaves me wondering: why does Ollama exist? Who is going to use it to host anything of substance?",
              "score": 1,
              "created_utc": "2026-02-13 13:30:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o567827",
                  "author": "Blinkinlincoln",
                  "text": "I used their cloud model kimi 2.5 for some OCR document processing pipeline that needed an AI, they give free tier usage. its fun! I also did run a qwen3-vl and other mini-cpm for various research projects I am working on because traditional paper forms for people doing surveys can be pain in the ass to handle handwriting and the mess of people taking a not perfect picture of the document. ",
                  "score": 1,
                  "created_utc": "2026-02-13 14:48:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o55fe2r",
          "author": "LavishnessPlane4512",
          "text": "Wow, then I‚Äôll just get the cheapest Mac mini and keep the change lol",
          "score": 1,
          "created_utc": "2026-02-13 12:04:52",
          "is_submitter": true,
          "replies": [
            {
              "id": "o59u21m",
              "author": "agent674253",
              "text": "I believe Apple is about to release their new models, so take a look online and wait a week or two if you can.\n\nMy M3 Macbook Air 8GB can run some of the smaller Gemma models, but I'm hoping to pick up a used Mac Mini or Studio to host one of the larger coding models locally at home and then connect to it over my local network.\n\nOllama/LM Studio + Claude Code is fun, but slow right now.\n\n",
              "score": 1,
              "created_utc": "2026-02-14 02:07:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5d8yvj",
          "author": "Electronic_Fox594",
          "text": "If someone was buying for me, I would absolutely go with the 512",
          "score": 1,
          "created_utc": "2026-02-14 17:10:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2y1lf",
      "title": "PardusDB ‚Äì Lightweight SQLite-like vector database for private local RAG",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r2y1lf/pardusdb_lightweight_sqlitelike_vector_database/",
      "author": "jasonhon2013",
      "created_utc": "2026-02-12 16:30:37",
      "score": 8,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "We just open-sourced PardusDB, a fast embedded vector database written in pure Rust ‚Äî think \"SQLite, but for vectors.\"\n\nWe benchmark it with ollama local host embedding gamma ! \n\nIt's designed specifically for local-first AI apps like RAG pipelines, where you want speed, privacy, and zero cloud dependencies.\n\nplease take a look !\n\n[https://github.com/JasonHonKL/PardusDB](https://github.com/JasonHonKL/PardusDB)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r2y1lf/pardusdb_lightweight_sqlitelike_vector_database/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o50dk38",
          "author": "ZeroSkribe",
          "text": "This has support for Ollama?",
          "score": 1,
          "created_utc": "2026-02-12 17:01:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51392i",
              "author": "jasonhon2013",
              "text": "Yes ! you can link ollama embedding ! (actually i test the benchmark with ollama gamma embedding) ",
              "score": 2,
              "created_utc": "2026-02-12 19:01:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5180ic",
                  "author": "ZeroSkribe",
                  "text": "Cool, I just searched your repo. I would lead with that or be clearer since you're in ollama sub.",
                  "score": 1,
                  "created_utc": "2026-02-12 19:24:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o51qnu6",
          "author": "Barachiel80",
          "text": "do you have plans for docker image support?",
          "score": 1,
          "created_utc": "2026-02-12 20:53:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o556q8t",
              "author": "jasonhon2013",
              "text": "Hi we would like to make it like sqlite so currently we don't really plan to deal with docker. But in the future when it works like a server we will give supprot to docker ",
              "score": 1,
              "created_utc": "2026-02-13 10:54:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o52isxw",
          "author": "Delicious-One-5129",
          "text": "Lightweight and local first vector storage makes a lot of sense for private RAG setups. Love seeing more Rust based infra tools popping up.",
          "score": 0,
          "created_utc": "2026-02-12 23:13:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o556qwd",
              "author": "jasonhon2013",
              "text": "Yep yep ! ",
              "score": 1,
              "created_utc": "2026-02-13 10:54:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3hdrq",
      "title": "How to run local models for coding (also for general-purpose inference), with Ollama, OpenCode, and Open WebUI",
      "subreddit": "ollama",
      "url": "https://github.com/FlorinAndrei/local-inference-docs",
      "author": "florinandrei",
      "created_utc": "2026-02-13 06:01:56",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r3hdrq/how_to_run_local_models_for_coding_also_for/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5r2z2k",
          "author": "BidWestern1056",
          "text": "would you mind adding npcsh/incognide?\n\n[https://github.com/npc-worldwide/npcsh](https://github.com/npc-worldwide/npcsh)\n\n[https://github.com/npc-worldwide/incognide](https://github.com/npc-worldwide/incognide)",
          "score": 1,
          "created_utc": "2026-02-16 21:05:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}