{
  "metadata": {
    "last_updated": "2026-01-22 02:32:42",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 20,
    "total_comments": 91,
    "file_size_bytes": 94123
  },
  "items": [
    {
      "id": "1qh5z0j",
      "title": "I built a voice-first AI mirror that runs fully on Ollama.",
      "subreddit": "ollama",
      "url": "https://v.redd.it/bjeyts2qibeg1",
      "author": "DirectorChance4012",
      "created_utc": "2026-01-19 14:43:25",
      "score": 340,
      "num_comments": 44,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qh5z0j/i_built_a_voicefirst_ai_mirror_that_runs_fully_on/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0hf0l3",
          "author": "ThomasNowProductions",
          "text": "How far we have come ;-) It is real nice tho",
          "score": 25,
          "created_utc": "2026-01-19 14:47:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hfop5",
              "author": "DirectorChance4012",
              "text": "Thanks!",
              "score": 4,
              "created_utc": "2026-01-19 14:51:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0hr4is",
                  "author": "mlt-",
                  "text": "Did you aim for that Pal thing look from Netflix cartoon The Mitchells vs. the Machines?",
                  "score": 6,
                  "created_utc": "2026-01-19 15:46:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0lqmr5",
                  "author": "redditissocoolyoyo",
                  "text": "Start a Kickstarter! Make it a real product. People will buy it!",
                  "score": 3,
                  "created_utc": "2026-01-20 03:31:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0i5cwe",
          "author": "croninsiglos",
          "text": "Need one with a vision model that both gives compliments and body shames me when appropriate. Must support tool use and connect with my scale.\n\nYou know, it also needs a more humanoid avatar that can  answer a simple question about who is the fairest of them all. The answer should be obvious, but I just want to be sure.",
          "score": 10,
          "created_utc": "2026-01-19 16:49:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tfvmu",
              "author": "Fuzzy_Independent241",
              "text": "WARNING: might have serious consequences if left to control prescription magical drugs for others in your kingdom!",
              "score": 3,
              "created_utc": "2026-01-21 07:28:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0j0wgi",
              "author": "ScoreUnique",
              "text": "SmolVlm",
              "score": 2,
              "created_utc": "2026-01-19 19:10:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0hgxfn",
          "author": "Money-Frame7664",
          "text": "That is really fun ! Congratulations üëè \nDo you have connectors with any home automation system? (Or MCP capacity?)",
          "score": 5,
          "created_utc": "2026-01-19 14:57:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ho7ss",
          "author": "Travelosaur",
          "text": "Mirror mirror on the wall... You made it a reality bro. Nice job!",
          "score": 4,
          "created_utc": "2026-01-19 15:32:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jhgq8",
          "author": "ServeAlone7622",
          "text": "This is the future",
          "score": 3,
          "created_utc": "2026-01-19 20:27:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jtr8r",
          "author": "klei10",
          "text": "Where did you get the mirror ?",
          "score": 3,
          "created_utc": "2026-01-19 21:25:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0l4fx0",
              "author": "DirectorChance4012",
              "text": "I covered the details here: [https://noted.lol/mirrormate/](https://noted.lol/mirrormate/).  \nThe mirror was purchased from [https://www.e-kagami.com/](https://www.e-kagami.com/).",
              "score": 1,
              "created_utc": "2026-01-20 01:29:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0lamvu",
          "author": "KernelFlux",
          "text": "That‚Äôs very cool and I want to build one!",
          "score": 3,
          "created_utc": "2026-01-20 02:03:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mjfif",
          "author": "Easy_Cable6224",
          "text": "so cool, this is something I wouldn't imagine back when I was a kid, we are IN the future now!",
          "score": 3,
          "created_utc": "2026-01-20 06:49:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hki0u",
          "author": "kiyyik",
          "text": "Thank you for sharing this! Looking forward to doing something similar.",
          "score": 2,
          "created_utc": "2026-01-19 15:15:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hs0z2",
          "author": "stoopwafflestomper",
          "text": "Very cool!",
          "score": 2,
          "created_utc": "2026-01-19 15:50:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hss7j",
              "author": "DirectorChance4012",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-19 15:53:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ht3w8",
          "author": "kkiran",
          "text": "Awesome, kudos! Mac Studio hosted AI mirror sounds cool. I wanted to justify my home lab spending and this fits part of the bill. Thank you!",
          "score": 2,
          "created_utc": "2026-01-19 15:54:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0htbp3",
              "author": "DirectorChance4012",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-19 15:55:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0hyitp",
          "author": "irodov4030",
          "text": "super cool!",
          "score": 2,
          "created_utc": "2026-01-19 16:19:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hzg6o",
              "author": "DirectorChance4012",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-19 16:23:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0i36ve",
          "author": "Reasonable_Brief578",
          "text": "Beautiful",
          "score": 2,
          "created_utc": "2026-01-19 16:39:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jk7b9",
          "author": "roshan231",
          "text": "That‚Äôs cool",
          "score": 2,
          "created_utc": "2026-01-19 20:40:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ju6s9",
          "author": "After_Construction72",
          "text": "Was only looking into similar over the weekend",
          "score": 2,
          "created_utc": "2026-01-19 21:27:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jyutj",
          "author": "No_Thing8294",
          "text": "Thanks for sharing! Very nice project! I love the face of the agent!",
          "score": 2,
          "created_utc": "2026-01-19 21:51:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kjwd0",
          "author": "thunder-wear",
          "text": "This is really awesome. I love it!",
          "score": 2,
          "created_utc": "2026-01-19 23:38:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ksxo6",
          "author": "Relevant_Middle_4779",
          "text": "Wow!!!",
          "score": 2,
          "created_utc": "2026-01-20 00:26:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rlzzf",
          "author": "zaschmaen",
          "text": "Damn nice! Thats what i wanna make but with an comic character or smth like thisüëå",
          "score": 2,
          "created_utc": "2026-01-21 00:18:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0syczv",
          "author": "wittlewayne",
          "text": "shit.... beat me to it !",
          "score": 2,
          "created_utc": "2026-01-21 05:06:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0t9gdr",
          "author": "beast_modus",
          "text": "Good Job‚Ä¶Great‚Ä¶",
          "score": 2,
          "created_utc": "2026-01-21 06:32:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tg0t5",
          "author": "Fuzzy_Independent241",
          "text": "That looks cool! Did you embed the hardware or is it calling Ollama over your lan / VPN ?",
          "score": 2,
          "created_utc": "2026-01-21 07:29:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tg6nd",
              "author": "Fuzzy_Independent241",
              "text": "PS - noticed your GH link on my phone. Will check the repo. Thx!",
              "score": 2,
              "created_utc": "2026-01-21 07:31:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0iv6rl",
          "author": "BringOutYaThrowaway",
          "text": "I need to know where you got the glass?",
          "score": 1,
          "created_utc": "2026-01-19 18:45:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0l4h3z",
              "author": "DirectorChance4012",
              "text": "I covered the details here: [https://noted.lol/mirrormate/](https://noted.lol/mirrormate/).  \nThe mirror was purchased from [https://www.e-kagami.com/](https://www.e-kagami.com/).",
              "score": 1,
              "created_utc": "2026-01-20 01:29:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0l6nc9",
          "author": "MeatballStroganoff",
          "text": "How long is it from prompt to response? It seems like you cut the video right before it starts speaking.",
          "score": 1,
          "created_utc": "2026-01-20 01:41:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0l8eh0",
              "author": "DirectorChance4012",
              "text": "I takes roughly 5-7 second.",
              "score": 2,
              "created_utc": "2026-01-20 01:51:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0l92mv",
                  "author": "MeatballStroganoff",
                  "text": " Not bad, thanks!",
                  "score": 1,
                  "created_utc": "2026-01-20 01:55:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0n7ehn",
          "author": "JacketHistorical2321",
          "text": "How does the vision plugin work? I don't see any documentation besides referencing it exists",
          "score": 1,
          "created_utc": "2026-01-20 10:28:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0plcut",
          "author": "Affectionate_Bus_884",
          "text": "![gif](giphy|ZRICsU0zv4tJS)",
          "score": 1,
          "created_utc": "2026-01-20 18:27:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0skjvs",
          "author": "Ordinary_Marketing36",
          "text": "Having a Ktop of 5 means that you will always recover 5 of the best memories?",
          "score": 1,
          "created_utc": "2026-01-21 03:35:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nw31u",
          "author": "NickMcGurkThe3rd",
          "text": "the problem with those videos is that they are always cut to cache/hide the fact that it takes the like 10 seconds for that thing to respond and make it seem seemless",
          "score": 1,
          "created_utc": "2026-01-20 13:30:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nwd8p",
              "author": "DirectorChance4012",
              "text": "yeah, it actually takes 5-7second.\nhttps://www.reddit.com/r/ollama/s/8b6TeRsRFx",
              "score": 3,
              "created_utc": "2026-01-20 13:32:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qiug46",
      "title": "Fine-tuned Qwen3 0.6B for Text2SQL using a claude skill. The result tiny model matches a Deepseek 3.1 and runs locally on CPU.",
      "subreddit": "ollama",
      "url": "https://i.redd.it/fy13421qjoeg1.png",
      "author": "party-horse",
      "created_utc": "2026-01-21 10:30:24",
      "score": 100,
      "num_comments": 4,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qiug46/finetuned_qwen3_06b_for_text2sql_using_a_claude/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0up0ey",
          "author": "cirejr",
          "text": "This is great, I've been trying to make this text2sql happen for couple of weeks now using lightweight models. And I have to say without fine tuning them it's really something üòÖ. I tried couple of ways, giving functionGemma bunch of tools. Using some 3b models and giving and creating a Neon mcp client but yeah I guess fine tuning is all that's left.",
          "score": 4,
          "created_utc": "2026-01-21 13:37:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0utcy8",
              "author": "party-horse",
              "text": "Awsome, feel free to use the claude skill to train a model for your specific domain/dialect!",
              "score": 1,
              "created_utc": "2026-01-21 14:00:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0wnup2",
          "author": "jlugao",
          "text": "How did you come up with the datasets for training and evaluating? I am thinking of doing a similar project for evaluating execution plans and coming up with recommendations",
          "score": 1,
          "created_utc": "2026-01-21 19:06:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0x6gmz",
              "author": "party-horse",
              "text": "I chatted with a few LLMs to get example conversations. Fortunately you only need approx 20 to get started so its pretty easy",
              "score": 1,
              "created_utc": "2026-01-21 20:30:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qfufg3",
      "title": "Claude Code with Anthropic API compatibility",
      "subreddit": "ollama",
      "url": "https://ollama.com/blog/claude",
      "author": "GhettoFob",
      "created_utc": "2026-01-18 01:01:12",
      "score": 32,
      "num_comments": 10,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qfufg3/claude_code_with_anthropic_api_compatibility/",
      "domain": "ollama.com",
      "is_self": false,
      "comments": [
        {
          "id": "o07x7s2",
          "author": "iron_coffin",
          "text": "The questions are whether anthropic is onboard with it and if they can block it.",
          "score": 3,
          "created_utc": "2026-01-18 02:19:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o088i85",
              "author": "xxdesmus",
              "text": "The seem more worried with Claude models being used by 3rd party clients and less concerned with this inverse scenario. Hopefully that doesn‚Äôt change.",
              "score": 5,
              "created_utc": "2026-01-18 03:22:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o09svlh",
              "author": "StardockEngineer",
              "text": "There is nothing they can do to stop it as long as Claude Code can work with an API",
              "score": 2,
              "created_utc": "2026-01-18 10:53:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0b7flp",
                  "author": "iron_coffin",
                  "text": "Nothing, when their control both the closed source app and api? X to doubt. They could at least make it inconvenient",
                  "score": 1,
                  "created_utc": "2026-01-18 16:14:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0atpiq",
              "author": "kiwibonga",
              "text": "They are currently preventing anyone from doing the first time user set up without a paid api key.",
              "score": 2,
              "created_utc": "2026-01-18 15:07:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0b46k7",
                  "author": "iron_coffin",
                  "text": "It sounds like codex is a better bet because the system prompt is shorter, also.",
                  "score": 1,
                  "created_utc": "2026-01-18 15:58:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mvn76",
          "author": "uwemaurer",
          "text": "Did anyone get this to work with a local model?\n\nI tried with gpt-oss:20b but I get the error:\n\n     harmony parser: no reverse mapping found for function name \"harmonyFunctionName=assistant<|channel|>AskUserQuestion\"\n    \n\nFor gwen3-coder, I get \n\n    model does not support thinking, relaxing thinking to nil",
          "score": 1,
          "created_utc": "2026-01-20 08:38:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0poqww",
              "author": "GhettoFob",
              "text": "Did you try bumping up the context length? My Ollama was defaulting to 4k but it started working after I set it to 64k.",
              "score": 2,
              "created_utc": "2026-01-20 18:42:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0qpsph",
                  "author": "uwemaurer",
                  "text": "Thank you! this was the problem. It worked for me with `gpt-oss:20b` and 32k context. \n\n    OLLAMA_CONTEXT_LENGTH=32768 ollama serve",
                  "score": 1,
                  "created_utc": "2026-01-20 21:32:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qj3b01",
      "title": "[Open Sourse] I built a tool that forces 5 AIs to debate and cross-check facts before answering you",
      "subreddit": "ollama",
      "url": "https://i.redd.it/ja8et3degqeg1.jpeg",
      "author": "S_Anv",
      "created_utc": "2026-01-21 16:55:27",
      "score": 30,
      "num_comments": 5,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qj3b01/open_sourse_i_built_a_tool_that_forces_5_ais_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0wv1t2",
          "author": "atika",
          "text": "Wasn‚Äôt enough to ‚Äúmake‚Äù them debate, you had to ‚Äúforce‚Äù them against their wishes?",
          "score": 3,
          "created_utc": "2026-01-21 19:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0wvpic",
              "author": "Nerdinat0r",
              "text": "Not to mention the overhead. How much more RAM and electricity and GPU Time this uses‚Ä¶",
              "score": 1,
              "created_utc": "2026-01-21 19:41:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0x5eoh",
          "author": "Basic_Young538",
          "text": "This could be really funny...",
          "score": 2,
          "created_utc": "2026-01-21 20:25:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0x81b9",
          "author": "Faisal_Biyari",
          "text": "If I use this between the same model, does that mean it is effectively converted into a \"thinking\" model?",
          "score": 1,
          "created_utc": "2026-01-21 20:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0y1t4a",
          "author": "ServeAlone7622",
          "text": "This is the way!\n\nI love that you built an ensemble setup too!\nI could never get this to work for me in a real sense. Accuracy and a lack of ability or possibly desire to critique one another leading into a giant circle jerk of AI handshaking.\n\nHow‚Äôs your accuracy? Do they argue back and forth or do they mostly just circle jerk each other?",
          "score": 1,
          "created_utc": "2026-01-21 22:57:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qh10xr",
      "title": "Demo: On-device browser agent (Qwen) running locally in Chrome",
      "subreddit": "ollama",
      "url": "https://v.redd.it/ljp6zwzfcaeg1",
      "author": "thecoder12322",
      "created_utc": "2026-01-19 10:48:47",
      "score": 22,
      "num_comments": 3,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qh10xr/demo_ondevice_browser_agent_qwen_running_locally/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0k28rw",
          "author": "TigerOk6003",
          "text": "Would be very curious to see how data centers will be rendered useless if small models get better and hardware for edge inference gets better too",
          "score": 3,
          "created_utc": "2026-01-19 22:07:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0j7059",
          "author": "New_Inflation_6927",
          "text": "Next challenge for the team could be trying out VLM within it?",
          "score": 1,
          "created_utc": "2026-01-19 19:38:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0n4kha",
              "author": "thecoder12322",
              "text": "We tried and we‚Äôre fixing a few bugs for that, it‚Äôll make it even more accurate! Open for any contributions, please check our runanywhere-sdks as well here: https://github.com/RunanywhereAI/runanywhere-sdks\n\nWe‚Äôll be adding web-gpu and vlm support as well which",
              "score": 1,
              "created_utc": "2026-01-20 10:02:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qgxulm",
      "title": "Would Anthropic Block Ollama?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qgxulm/would_anthropic_block_ollama/",
      "author": "Lopsided_Dot_4557",
      "created_utc": "2026-01-19 07:36:07",
      "score": 21,
      "num_comments": 17,
      "upvote_ratio": 0.82,
      "text": "Few hours ago, Ollama announced following:\n\nOllama now has Anthropic API compatibility. This enables tools like Claude Code to be used with open-source models.\n\nOllama Blog:¬†[Claude Code with Anthropic API compatibility ¬∑ Ollama Blog](https://ollama.com/blog/claude)\n\nHands-on Guide:¬†[https://youtu.be/Pbsn-6JEE2s?si=7pdAv5LU9GiBx7aN](https://youtu.be/Pbsn-6JEE2s?si=7pdAv5LU9GiBx7aN)\n\nFor now it's working but for how long?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qgxulm/would_anthropic_block_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0fwcc2",
          "author": "Kholtien",
          "text": "I don‚Äôt think they could. You can use Claude code (the application, not the service Claude) with just about any LLM these days. It‚Äôs all local on your computer.",
          "score": 13,
          "created_utc": "2026-01-19 07:49:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0i42ty",
              "author": "sinan_online",
              "text": "Hey, so I got a question to ask. If you are doing this, what are you doing, exactly, and what‚Äôs the VRAM? \n\nMy understanding is that the Claude endpoint does quite a bit of orchestration, not just coding, when used under GitHub CoPilot. Also the model they are using seems to require more VRAM than I have locally (12GB) to respond in a reasonable amount of time.",
              "score": 1,
              "created_utc": "2026-01-19 16:43:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0ktgnp",
              "author": "Big-Masterpiece-9581",
              "text": "It works but only with hacks and you‚Äôre at Claude‚Äôs mercy anytime they want to throw a curveball and break everything else. I would just use opencode.",
              "score": 1,
              "created_utc": "2026-01-20 00:29:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0l87yb",
                  "author": "StardockEngineer",
                  "text": "They're not hacks.  It's just pointing the base url to another server.  It's a documented feature in Claude Code.",
                  "score": 4,
                  "created_utc": "2026-01-20 01:50:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0jhwpr",
          "author": "ShadoWolf",
          "text": "The Claude Code situation isn‚Äôt about Anthropic‚Äôs standard API.  \nWhat was happening is that people were signing up for Claude Code, then using tools like OpenCode to redirect it and reuse the Claude Code credential.  \nThat credential wasn‚Äôt a normal Anthropic API key, it was tied to the Claude Code plan itself, which is priced as a product SKU, not as metered API access. Anthropic will happy take your money for API usage through their normal API plans. They just don‚Äôt want the Claude Code plan being used as a cheap substitute for the standard API pricing",
          "score": 3,
          "created_utc": "2026-01-19 20:29:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g5sld",
          "author": "gabrielxdesign",
          "text": "Why would they block it? I've been using Open WebUI for years, which uses Ollama for Open Source LLM, and I use it together with paid APIs. I don't see the problem with using a platform that can handle local and remote.",
          "score": 6,
          "created_utc": "2026-01-19 09:16:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0gsxig",
          "author": "UnbeliebteMeinung",
          "text": "They blocked the other way because of their costs. Why would they block it when they have no costs?",
          "score": 2,
          "created_utc": "2026-01-19 12:38:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0it8hw",
          "author": "croninsiglos",
          "text": "They are pretty confident in their model‚Äôs ability over open source models and the CEO seems to think you‚Äôll never be hosting the more performant open models locally. \n\nSo does it matter?\n\nI‚Äôm sure they‚Äôd love people to standardize on their APO vs openai‚Äôs API.",
          "score": 1,
          "created_utc": "2026-01-19 18:36:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0osgxo",
              "author": "Practical-Plan-2560",
              "text": "Under that logic, they should open source Claude Code. If the model is the magic, why keep tools to use the model so closed?",
              "score": 1,
              "created_utc": "2026-01-20 16:14:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0izx4z",
          "author": "roger_ducky",
          "text": "It‚Äôs the same as the OpenAI API compatibility.\n\nAs long as anthropic doesn‚Äôt suddenly make major breaking changes, it‚Äôd work.\n\nThis isn‚Äôt the same thing anthropic banned recently‚Äî that‚Äôs piggybacking on the actual Claude using a person‚Äôs monthly paid subscription, rather than APIs.\n\nThis is using Claude code the CLI with your local models.",
          "score": 1,
          "created_utc": "2026-01-19 19:06:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ju0js",
          "author": "Clay_Ferguson",
          "text": "Anthropic realized that if OpenCode and/or LangChain OpenWork open source solutions are what people start using for local LLMs, then it gets people out of the Claude Code way of doing things. So there was no disadvantage to letting Claude Code be used for local inference, but there was an advantage, which is to make Claude Code be what people want to use regardless of whether they're using local or cloud-based LLMs.",
          "score": 1,
          "created_utc": "2026-01-19 21:26:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nhtd1",
          "author": "Ok_Hospital_5265",
          "text": "And just as I installed Crush‚Ä¶ ü§¶üèª‚Äç‚ôÇÔ∏è",
          "score": 1,
          "created_utc": "2026-01-20 11:56:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0p7oio",
              "author": "seangalie",
              "text": "Sometimes it's useful to have Opencode, Crush, Codex, and Claude all co-existing.  Good way to jump between models and such without having to play around with configs everytime.  I use Ollama's cloud models in Opencode, and local models in Crush (since Crush supports a big task/little task setup).",
              "score": 1,
              "created_utc": "2026-01-20 17:25:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0m8y9k",
          "author": "stocky789",
          "text": "Would anyone want to? \nClaude code isn't really anything to write home about",
          "score": 0,
          "created_utc": "2026-01-20 05:26:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj3qkv",
      "title": "Hi folks, I‚Äôve built an open‚Äësource project that could be useful to some of you",
      "subreddit": "ollama",
      "url": "https://i.redd.it/4plhjok3jqeg1.png",
      "author": "panos_s_",
      "created_utc": "2026-01-21 17:10:42",
      "score": 19,
      "num_comments": 5,
      "upvote_ratio": 0.79,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qj3qkv/hi_folks_ive_built_an_opensource_project_that/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0x1sha",
          "author": "jovn1234567890",
          "text": "Very useful thank you",
          "score": 1,
          "created_utc": "2026-01-21 20:09:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0xt8z7",
              "author": "panos_s_",
              "text": "thanks mate!",
              "score": 1,
              "created_utc": "2026-01-21 22:14:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0xgfv7",
          "author": "mofa1",
          "text": "I just want to say that I like the project and have been using it for some time in my Unraid system!",
          "score": 1,
          "created_utc": "2026-01-21 21:15:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0xsofs",
              "author": "panos_s_",
              "text": "thanks a lot :)",
              "score": 1,
              "created_utc": "2026-01-21 22:12:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ybfg1",
          "author": "selfdestroyer",
          "text": "I also have been running this for a month or so and it‚Äôs been great to monitor while using OpenwebUI and ConfyUI.",
          "score": 1,
          "created_utc": "2026-01-21 23:48:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qiv7v8",
      "title": "I built a CLI tool using Ollama (nomic-embed-text) to replace grep with Semantic Code Search",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qiv7v8/i_built_a_cli_tool_using_ollama_nomicembedtext_to/",
      "author": "Technical_Meeting_81",
      "created_utc": "2026-01-21 11:16:14",
      "score": 18,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "Hi r/ollama,\n\nI've been working on an open-source tool called **GrepAI**, and I wanted to share it here because it relies heavily on **Ollama** to function.\n\n**What is it?** GrepAI is a CLI tool (written in Go) designed to help AI agents (like Claude Code, Cursor, or local agents) understand your codebase better.\n\nInstead of using standard regex `grep` to find code‚Äîwhich often misses the context‚ÄîGrepAI uses **Ollama** to generate local embeddings of your code. This allows you to perform **semantic searches** directly from the terminal.\n\n**The Stack:**\n\n* **Core:** Written in Go.\n* **Embeddings:** Connects to your local Ollama instance (defaults to `nomic-embed-text`).\n* **Vector Store:** In-memory / Local (fast and private).\n\n**Why use Ollama for this?** I wanted a solution that respects privacy and doesn't cost a fortune in API credits just to index a repo. By using Ollama locally, GrepAI builds an index of your project (respecting `.gitignore`) without your code leaving your machine.\n\n**Real-world Impact (Benchmark)** I tested this setup by using GrepAI as a filter for Claude Code (instead of the default grep). The idea was to let Ollama decide what files were relevant before sending them to the cloud. The results were huge:\n\n* **-97% Input Tokens** sent to the LLM (because Ollama filtered the noise).\n* **-27.5% Cost reduction** on the task.\n\nEven if you don't use Claude, this demonstrates how effective local embeddings (via Ollama) are at retrieving the right context for RAG applications.\n\nüëâ **Benchmark details:**[https://yoanbernabeu.github.io/grepai/blog/benchmark-grepai-vs-grep-claude-code/](https://yoanbernabeu.github.io/grepai/blog/benchmark-grepai-vs-grep-claude-code/)\n\n**Links:**\n\n* üì¶ **GitHub:**[https://github.com/yoanbernabeu/grepai](https://github.com/yoanbernabeu/grepai)\n* üìö **Docs:**[https://yoanbernabeu.github.io/grepai/](https://yoanbernabeu.github.io/grepai/)\n\nI'd love to know what other embedding models you guys are running with Ollama. Currently, `nomic-embed-text` gives me the best results for code, but I'm open to suggestions!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qiv7v8/i_built_a_cli_tool_using_ollama_nomicembedtext_to/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0vbgvm",
          "author": "Ok-District-1756",
          "text": "I'm going to try Qwen3-Embedding-4B-GGUF:Q5\\_K\\_M. Honestly, I have no idea how it will perform in real-world conditions, but I'll test it for a week and report back if anyone is interested",
          "score": 3,
          "created_utc": "2026-01-21 15:30:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ybqej",
          "author": "yesbee-yesbee",
          "text": "Will it work for opencode?¬†",
          "score": 1,
          "created_utc": "2026-01-21 23:50:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qijhth",
      "title": "Weekend Project: An Open-Source Claude Cowork That Can Handle Skills",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qijhth/weekend_project_an_opensource_claude_cowork_that/",
      "author": "Frequent_Cash2598",
      "created_utc": "2026-01-21 00:59:50",
      "score": 17,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "I spent last weekend building something I had been thinking about for a while. Claude Cowork is great, but I wanted an open-source, lightweight version that could run with any model, so I created Open Cowork.\n\nIt's written entirely in Rust, which I had never used before. Starting from scratch meant no heavy dependencies, no Python bloat, and no reliance on existing agent SDKs. Just a tiny, fast binary that works anywhere.\n\nSecurity was a big concern since the agents can execute code. Open Cowork handles this by running tasks inside temporary Docker containers. Everything stays isolated, but you can still experiment freely.\n\nYou can plug in any model you want. OpenAI, Anthropic, or even fully offline LLMs through Ollama are all supported. You keep full control over your API keys and your data.\n\nIt already comes with built-in skills for handling documents like PDFs and Excel files. I was surprised by how useful it became right away.\n\nThe development experience was wild. An AI agent helped me build a secure, open-source version of itself, and I learned Rust along the way. It was one of those projects where everything just clicked together in a weekend.\n\nThe code is live on GitHub: [https://github.com/kuse-ai/kuse\\_cowork](https://github.com/kuse-ai/kuse_cowork) . It's still early, but I'd love to hear feedback from anyone who wants to try it out or contribute.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qijhth/weekend_project_an_opensource_claude_cowork_that/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0rvzc8",
          "author": "Available-Craft-5795",
          "text": "Im pretty sure this is AI.   \nLoads of emojis in readme  \nLoads of commments in code that no sane dev would add  \nDone in a weekend? How?",
          "score": 4,
          "created_utc": "2026-01-21 01:13:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0s2p5t",
              "author": "gingeropolous",
              "text": "Sounds like they used Claude code.",
              "score": 4,
              "created_utc": "2026-01-21 01:51:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0ss3zn",
              "author": "Frequent_Cash2598",
              "text": "Yes, it is Claude Code making a Rust alternative of itself. \n\nSo you are right. :)",
              "score": 4,
              "created_utc": "2026-01-21 04:23:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0sepez",
          "author": "Efficient_Click_2689",
          "text": "cool project bro, would love to try out!",
          "score": 1,
          "created_utc": "2026-01-21 03:00:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0w48w9",
          "author": "wombweed",
          "text": "OS compatibility? If it runs on Linux can you include a Nix flake?",
          "score": 1,
          "created_utc": "2026-01-21 17:40:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qepvki",
      "title": "Do you actually need prompt engineering to get value from AI?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qepvki/do_you_actually_need_prompt_engineering_to_get/",
      "author": "Xthebuilder",
      "created_utc": "2026-01-16 19:36:53",
      "score": 12,
      "num_comments": 46,
      "upvote_ratio": 0.93,
      "text": "\n\nI‚Äôve been using AI daily for about 6 months while building a local AI inferencing app, and one thing that surprised me is how little prompt engineering mattered compared to other factors.\n\nWhat ended up making the biggest difference for me was:\n\n* giving the model enough **context**\n* iterating on ideas *with* the model before writing real code\n* choosing models that are actually good at the specific task\n\nBecause LLMs have some randomness, I found they‚Äôre most useful early on, when you‚Äôre still figuring things out. Iterating with the model helped surface bad assumptions before I committed to an approach. They‚Äôre especially good at starting broad and narrowing down if you keep the conversation going so context builds up.\n\nWhen I add new features now, I don‚Äôt explain my app‚Äôs architecture anymore. I just link the relevant GitHub repos so the model can see how things are structured. That alone cut feature dev time from weeks to about a day in one case.\n\nI‚Äôm not saying prompt engineering is useless, just that for most practical work, context, iteration, and model choice mattered more for me.\n\nCurious how others here approach this. Has prompt engineering been critical for you, or have you seen similar results?\n\n(I wrote up the full experience here if anyone wants more detail: [https://xthebuilder.github.io](https://xthebuilder.github.io))\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qepvki/do_you_actually_need_prompt_engineering_to_get/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nzzcjbw",
          "author": "Dry_Yam_4597",
          "text": "I dont prompt engineer. I ask an llm to write a prompt for another llm when needed. Context is more important.",
          "score": 13,
          "created_utc": "2026-01-16 19:57:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzpr49",
              "author": "Xthebuilder",
              "text": "I agree I also use that method when I want to use an engineered prompt for a workflow I just have another model make it though my language request",
              "score": 3,
              "created_utc": "2026-01-16 20:59:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o004pqy",
                  "author": "Dry_Yam_4597",
                  "text": "Nice. I actually run a couple of P100s just for that. One to analyse images, one to write a prompt, one to refine stuff and is fine tuned, and then i send the final prompt to say comfyui. Pretty neat. That's also how I work around nano banana's lame ai \"logo\". I just recreate images on local.",
                  "score": 2,
                  "created_utc": "2026-01-16 22:10:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzzozca",
          "author": "Purple-Programmer-7",
          "text": "Yes, it‚Äôs critical. And we‚Äôre calling it ‚Äúcontext engineering‚Äù now.",
          "score": 8,
          "created_utc": "2026-01-16 20:55:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzq5fo",
              "author": "Xthebuilder",
              "text": "I like that term better than prompt engineering .",
              "score": 1,
              "created_utc": "2026-01-16 21:01:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzzns21",
          "author": "tom-mart",
          "text": "I'd argue that adding context is prompt engineering.",
          "score": 10,
          "created_utc": "2026-01-16 20:50:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzplac",
              "author": "Xthebuilder",
              "text": "Okay thankyou for the clarification",
              "score": 1,
              "created_utc": "2026-01-16 20:58:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzzt1jz",
          "author": "DeepInEvil",
          "text": "Pretty much this, prompt engineering is bs and for people who are new in the field. Afair r/airealist had an article on that.",
          "score": 6,
          "created_utc": "2026-01-16 21:14:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o004od6",
          "author": "Shoddy-Tutor9563",
          "text": "I can't stop laughing seeing how others are sometimes trying to squeeze out a bit of additional performance from a model in agentic tasks by putting different personas \"hat\" on it. E.g. \"you are super senior 10x engineer ...\"",
          "score": 2,
          "created_utc": "2026-01-16 22:10:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00gev7",
              "author": "Xthebuilder",
              "text": "Lmaooo you are so right I have personally never had much  use of personalities for the LLM workflows I have been using",
              "score": 1,
              "created_utc": "2026-01-16 23:09:55",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o00y705",
              "author": "track0x2",
              "text": "I‚Äôve found that only useful for emulation. ‚ÄúYou are Dr. Seuss.‚Äù",
              "score": 1,
              "created_utc": "2026-01-17 00:51:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o011pfa",
              "author": "NoAdministration6906",
              "text": "I know its funny but it actually works. Response are much better",
              "score": 1,
              "created_utc": "2026-01-17 01:12:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o00ahv3",
          "author": "MrSomethingred",
          "text": "From my understanding of the history of it, \"Prompt Engineering\" was more important in the GPT4 era when you had to say things like \"fix this or I will explode 4000 puppies with Napalm\" and there was nuance in the art of getting them to actually follow instructions. E.g. getting them to actually output in JSON consistently was a nightmare\n\n\nModern models are pretty good at rule following these days so there isn't really any secrets to prompting them \"correctly\" these days\n\n\nAs you highlighted, context management is far more important these days, hence the new buzzword is \"context engineering\"¬†",
          "score": 2,
          "created_utc": "2026-01-16 22:39:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00guz2",
              "author": "Xthebuilder",
              "text": "I like that , context engineering feels more like what I‚Äôm doing and you can relate that to say just having a conversation , regular folk can get to understand that suing AI effectively isn‚Äôt rocket science",
              "score": 1,
              "created_utc": "2026-01-16 23:12:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00g7ji",
          "author": "HeavyDluxe",
          "text": "Prompt engineering is (IMHO) best understood as one \\_facet\\_ of context engineering.  The model makes its predictions based on the contextual information it's delivered.  In early models, carefully crafting a prompt was really the best you could do to ground the model's output.  We have many, many more tools available to us now to set a meaningful foundation for the model to use in generating output.\n\nIf you give the model tons of \\_really good\\_ information in  the codebase, supporting documents, etc etc etc, the user prompt becomes less and less critical.\n\nThe illustration I use at work is to imagine a random stranger comes up at you with a pile of papers, tells you to summarize the data therein for them, and, if you do a good job, you get $1M.  Imagine how you the quality of your product improves if the data has good labels... or if there's a previous report drawing on similar data that's there as an example. Or if the person also tells you what industry they're in.  Or if they tell you to \"imagine you're a customer service manager\" or whatever.\n\nEach little bit of information available improves your output.  The prompt is vital if that's all you give the model.  But context is \\_everything\\_.",
          "score": 2,
          "created_utc": "2026-01-16 23:08:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02rmqo",
              "author": "Xthebuilder",
              "text": "Basically it‚Äôs like data cleaning for your AI output pipeline, it‚Äôs really conceptual but it cuts across a lot of LLM interactions as the base of what‚Äôs controlling the models response",
              "score": 1,
              "created_utc": "2026-01-17 08:54:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00gr19",
          "author": "Additional-Actuary-8",
          "text": "I think the key is to ask what exactly you want rather than design tons of prompts. \n\nAs a human I also want my ai to answer shorter and concise.",
          "score": 2,
          "created_utc": "2026-01-16 23:11:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00h1zm",
              "author": "Xthebuilder",
              "text": "I find myself wanting the models to be more concise across the board too , you‚Äôre correct about being specific about what you want from the model , sounds more like communciation skills",
              "score": 1,
              "created_utc": "2026-01-16 23:13:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00jt2d",
          "author": "Low-Opening25",
          "text": "no, you need context management",
          "score": 2,
          "created_utc": "2026-01-16 23:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00onyo",
              "author": "Xthebuilder",
              "text": "Agreed agreed",
              "score": 1,
              "created_utc": "2026-01-16 23:56:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00zxfy",
          "author": "DaleCooperHS",
          "text": "I dont see the difference. prompt engineering is not writing a prompt, is the process that ends with writing a prompt. Now that process can be done mentally or trought iteration and research . cna be pages long of iteration with an LLm or one 30 lines prompt. Is still prompt engineering.  \nHowever context is not cheap.. so ideally you want to condense all the necessary context and prune out all taht is not.. leaving with the shorter set of instructions and context necessary to do the job, so that the maximum ammount of still avaible context can be used to actually do the job.",
          "score": 2,
          "created_utc": "2026-01-17 01:01:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0120v8",
          "author": "NoAdministration6906",
          "text": "There is also a fine line in giving context, too much context also make the model hallucinations. Be aware of the context window tokens for a model.",
          "score": 2,
          "created_utc": "2026-01-17 01:14:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01mo1i",
              "author": "Xthebuilder",
              "text": "Good point I haven‚Äôt really considered context in token window size too much but maybe that adjustment will lead to further optimization",
              "score": 1,
              "created_utc": "2026-01-17 03:25:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o017umx",
          "author": "cointalkz",
          "text": "Most LLMs, diffusion models or whatever you are using have documentation. Take that documentation into your LLM of choice and train it on how to best write prompts for said model. Profit!",
          "score": 2,
          "created_utc": "2026-01-17 01:52:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01ms94",
              "author": "Xthebuilder",
              "text": "Ooo like full circle training the model on its own developer written documentation",
              "score": 1,
              "created_utc": "2026-01-17 03:26:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o01nxl8",
                  "author": "cointalkz",
                  "text": "Bingo",
                  "score": 1,
                  "created_utc": "2026-01-17 03:34:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o01lqth",
          "author": "fasti-au",
          "text": "It‚Äôs a tipping scale.  \n\n1 million context but your really only pulling lie 16k tokens to process.  So in some cases you get cleaner runs but you need to one shot not correct and now think is breaking things to boiler plating it‚Äôs now more important to back track not correct. \n\nDoes prompting matter.   Not until It does.  \n\nI‚Äôm bad at words out to human but I can cintext a model better than most and build library systems and I tells ya.   Code the tool teacher model to know how to pull and when.  How you get that code is irrelevant in many ways because it‚Äôs all exiting concepts and process with small adjustments so nothing is special beyond the results \n\n\nSo promt engineering is about repetative stability not the day to day in many ways",
          "score": 2,
          "created_utc": "2026-01-17 03:19:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01mjmh",
              "author": "Xthebuilder",
              "text": "I like how you put it , if you can get the same result many times over you can trust the system more overall",
              "score": 1,
              "created_utc": "2026-01-17 03:25:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o02sp52",
          "author": "generate-addict",
          "text": "Nope. And even ‚Äúcontext engineering‚Äù. It changes from model to model, generation to generation. People just gaslight themselves.",
          "score": 2,
          "created_utc": "2026-01-17 09:04:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02stv2",
              "author": "Xthebuilder",
              "text": "I‚Äôm really glad I asked the community because I thought I was tripping seeing all the ‚Äúprompt engineering ‚Äú buzzwords for testing new people about using AI bc I learned though trying to build and use them that fr fr I don‚Äôt need none of that shit üòÇü§£",
              "score": 1,
              "created_utc": "2026-01-17 09:06:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o03ky5r",
          "author": "ZynthCode",
          "text": "promt-crafting is helpful to correct *undesired* behavior.   \n  \n\\*whips LLM on the wrist\\* No comments in code!",
          "score": 2,
          "created_utc": "2026-01-17 13:09:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05suao",
              "author": "Xthebuilder",
              "text": "ü§£ü§£üòÇ good way to put it , it‚Äôs at the edge of the computation , won‚Äôt change too much but can tweak stuff for sure",
              "score": 1,
              "created_utc": "2026-01-17 19:45:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0gk5gp",
          "author": "d_28ight",
          "text": "Just came across this thread,\n\nI've been researching how to develop LLM responses, beyond just clarifying my intention. I think roleplaying has assisted, but personally, it depends heavily on whether you have a foundational understanding of that role/industry to begin with.   \n\n\nCurrently, I have been scrapping prompts I come across and writing my own renditions specific to my requests, but now I came across JSON writers. I am thinking of still maintaining my context in my written notes, then using another LLM to conduct a meta-analysis of this as a collaborator to get further understanding and refine it, but it depends on my ability to articulate my intent, then using another LLM to convert this into JSON and paste that to my original LLMs  \n\n\nPersonally, I think the ability to articulate your thoughts, both at a direct and subconscious level, is more important than overloading it with context at every pivot.",
          "score": 2,
          "created_utc": "2026-01-19 11:29:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ir150",
              "author": "Xthebuilder",
              "text": "And I think what has been revealed though these discussions is that since you‚Äôre talking us f natural language , interaction with llms follows more principals adjacent to communication than computer science",
              "score": 1,
              "created_utc": "2026-01-19 18:26:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0ir8a9",
              "author": "Xthebuilder",
              "text": "Because I think you‚Äôre right I got better at communicating to the agents and just telling them exactly what I needed , a lot of the time I found the limitation was myself in the loop",
              "score": 1,
              "created_utc": "2026-01-19 18:27:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o09yxvd",
          "author": "stocky789",
          "text": "I never bother with prompt engineering \nJust stay specific and concise when communicating with it",
          "score": 1,
          "created_utc": "2026-01-18 11:47:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0bfh8y",
              "author": "Xthebuilder",
              "text": "right right ,more like simple communication.",
              "score": 1,
              "created_utc": "2026-01-18 16:52:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o01t58f",
          "author": "YouAreTheCornhole",
          "text": "Yes you do, and most people in this thread seem to not know what prompt engineering actually means. Straight up garbage in garbage out",
          "score": 0,
          "created_utc": "2026-01-17 04:09:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02ra7k",
              "author": "Xthebuilder",
              "text": "Good way to add to the discussion brother üëçüèΩ",
              "score": 0,
              "created_utc": "2026-01-17 08:51:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o02shgs",
                  "author": "YouAreTheCornhole",
                  "text": "You literally described your way of prompt engineering, acting like you don't need prompt engineering, because you prompt engineer in a certain way.",
                  "score": 1,
                  "created_utc": "2026-01-17 09:02:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qen6w1",
      "title": "The Preprocessing Gap Between RAG and Agentic",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qen6w1/the_preprocessing_gap_between_rag_and_agentic/",
      "author": "OnyxProyectoUno",
      "created_utc": "2026-01-16 17:59:44",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "RAG is the standard way to connect documents to LLMs. Most people building RAGs know the steps by now: parse documents, chunk them, embed, store vectors, retrieve at query time. But something different happens when you're building systems that act rather than answer.\n\n### The RAG mental model\n\nRAG preprocessing optimizes for retrieval. Someone asks a question, you find relevant chunks, you synthesize an answer. The whole pipeline is designed around that interaction pattern.\n\nThe work happens before anyone asks anything. Documents get parsed into text, extracting content from PDFs, Word docs, HTML, whatever format you're working with. Then chunking splits that text into pieces sized for context windows. You choose a strategy based on your content: split on paragraphs, headings, or fixed token counts. Overlap between chunks preserves context across boundaries. Finally, embedding converts each chunk into a vector where similar meanings cluster together. \"The contract expires in December\" ends up near \"Agreement termination date: 12/31/2024\" even though they share few words. That's what makes semantic search work.\n\nRetrieval is similarity search over those vectors. Query comes in, gets embedded, you find the nearest chunks in vector space. For Q&A, this works well. You ask a question, the system finds relevant passages, an LLM synthesizes an answer. The whole architecture assumes a query-response pattern.\n\nThe requirements shift when you're building systems that act instead of answer.\n\n### What agentic actually needs\n\nConsider a contract monitoring system. It tracks obligations across hundreds of agreements: Example Bank owes a quarterly audit report by the 15th, so the system sends a reminder on the 10th, flags it as overdue on the 16th, and escalates to legal on the 20th. The system doesn't just find text about deadlines. It acts on them.\n\nThat requires something different at the data layer. The system needs to understand that Party A owes Party B deliverable X by date Y under condition Z. And it needs to connect those facts across documents. Not just find text about obligations, but actually know what's owed to whom and when.\n\nThe preprocessing has to pull out that structure, not just preserve text for later search. You're not chunking paragraphs. You're turning \"Example Bank shall submit quarterly compliance reports within 15 days of quarter end\" into data you can query: party, obligation type, deadline, conditions. Think rows in a database, not passages in a search index.\n\n### Two parallel paths\n\nThe architecture ends up looking completely different.\n\nRAG has a linear pipeline. Documents go in, chunking happens, embeddings get created, vectors get stored. At query time, search, retrieve, generate.\n\nAgentic systems need two tracks running in parallel. The main one pulls structured data out of documents. An LLM reads each contract, extracts the obligations, parties, dates, and conditions, and writes them to a graph database. Why a graph? Because you're not just storing isolated facts, you're storing how they connect. Example Bank owes a report. That report is due quarterly. The obligation comes from Section 4.2 of Contract #1847. Those connections between entities are what graph databases are built for. This is what powers the actual monitoring.\n\nBut you still need embeddings. Just for different reasons.\n\nThe second track catches what extraction misses. Sometimes \"the Lender\" in paragraph 12 needs to connect to \"Example Bank\" from paragraph 3. Sometimes you don't know what patterns matter until you see them repeated across documents. The vector search helps you find connections that weren't obvious enough to extract upfront.\n\nSo you end up with two databases working together. The graph database stores entities and their relationships: who owes what to whom by when. The vector database helps you find things you didn't know to look for.\n\nI wrote the rest on my [blog](https://nickrichu.me/posts/the-preprocessing-gap-between-rag-and-agentic).",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qen6w1/the_preprocessing_gap_between_rag_and_agentic/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qepzot",
      "title": "Polymcp Integrates Ollama ‚Äì Local and Cloud Execution Made Simple",
      "subreddit": "ollama",
      "url": "https://github.com/poly-mcp/Polymcp",
      "author": "Just_Vugg_PolyMCP",
      "created_utc": "2026-01-16 19:41:16",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qepzot/polymcp_integrates_ollama_local_and_cloud/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qhr198",
      "title": "GLM 4.7 is apparently almost ready on Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/",
      "author": "Savantskie1",
      "created_utc": "2026-01-20 04:18:14",
      "score": 10,
      "num_comments": 10,
      "upvote_ratio": 0.82,
      "text": "It's listed, just not downloadable yet. Trying in WebOllama, and in CLI gives weird excuses\n\nhttps://preview.redd.it/96ly2bgckfeg1.png?width=1723&format=png&auto=webp&s=d8bfc2386dd789ef4f28ff0516de4893bf5c6772",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0u11zy",
          "author": "Resonant_Jones",
          "text": "I‚Äôm running it on apple silicon m4 32gb\n\nGLM-4.7-flash:q4_K_M\n\nIt‚Äôs an incredible model. I‚Äôve been blown away so far.",
          "score": 2,
          "created_utc": "2026-01-21 10:47:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ojnqc",
          "author": "beefgroin",
          "text": "thanks, running glm-4.7-flash:q8\\_0, so far so good",
          "score": 2,
          "created_utc": "2026-01-20 15:33:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0pkcjg",
              "author": "thexdroid",
              "text": "What is your GPU?",
              "score": 1,
              "created_utc": "2026-01-20 18:22:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0plxu3",
                  "author": "beefgroin",
                  "text": "I have 4 5060 16gb, I moved to q4k\\_m because the model consumes a lot of vram when the context is increasing, with 32k context it's taking 50gb of vram.",
                  "score": 1,
                  "created_utc": "2026-01-20 18:30:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0pm8wh",
              "author": "beefgroin",
              "text": "I liked it, the thinking process is very structured and very different from other thinking models. I guess something is still buggy cause every long conversation ends up in an infinite thinking loop",
              "score": 1,
              "created_utc": "2026-01-20 18:31:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0oe8av",
          "author": "WaitformeBumblebee",
          "text": "this one? updated 19 hours ago:\n\nhttps://ollama.com/library/glm-4.7-flash\n\n\nHmm, glm-4.7-flash:latest and glm-4.7-flash:q4_K_M are the same size, are they the same?\n\n\nCan't pull the new model, it requires Ollama update, even right after updating...\n\nollama --version\nollama version is 0.14.2\n\n\nedit: download latest release from github\nhttps://github.com/ollama/ollama/releases\n\nunpack, sudo cp lib files to /usr/local/lib/ollama/ and bin file to /usr/local/bin/   \n\npulling glm4.7 now\n\nedit2: quite fast on laptop 3060 6GB VRAM + 32GB RAM\n\nNAME                    ID              SIZE     PROCESSOR          CONTEXT              \nglm-4.7-flash:latest    ff14144f31df    23 GB    77%/23% CPU/GPU    4096",
          "score": 1,
          "created_utc": "2026-01-20 15:07:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0x6083",
          "author": "Zyj",
          "text": "I just pulled an Ollama container update, now I can download GLM 4.7 flash q8\\_0 with it. It will run on 2x 3090.",
          "score": 1,
          "created_utc": "2026-01-21 20:28:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0y8f7h",
          "author": "thexdroid",
          "text": "Odd, my system ram is 32GB and 16GB for GPU (4070ti). However I can't run any GLM model due memory... The glm-4.7-flash:q4\\_K\\_M model is actually 18GB but it requires 128GiB, I have downloaded and ran models like qwen3:30b-a3b-instruct-2507-q4\\_K\\_M (same 18GB) and even more. Do I would need more RAM? With current prices? lol, thanks.",
          "score": 1,
          "created_utc": "2026-01-21 23:32:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0m0ppd",
          "author": "iansltx_",
          "text": "You can't use the GGUF. You \\*can\\* use the ones from the website if you have the latest ollama build (on GitHub, see that link).\n\nOne caveat: it runs slow on a unified-memory Mac for some reason, and splits 80/20 CPU/GPU. Not sure what's going on there. My Intel Mac runs faster.",
          "score": 1,
          "created_utc": "2026-01-20 04:30:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qi8ouc",
      "title": "Plano 0.4.3 ‚≠êÔ∏è Filter Chains via MCP and OpenRouter Integration",
      "subreddit": "ollama",
      "url": "https://i.redd.it/o590ks78pjeg1.png",
      "author": "AdditionalWeb107",
      "created_utc": "2026-01-20 18:12:47",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qi8ouc/plano_043_filter_chains_via_mcp_and_openrouter/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qj02gw",
      "title": "New Rules for ollama cloud",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qj02gw/new_rules_for_ollama_cloud/",
      "author": "killing_daisy",
      "created_utc": "2026-01-21 14:57:44",
      "score": 9,
      "num_comments": 3,
      "upvote_ratio": 0.85,
      "text": "so i've just seen this:\n\nPro:  \nEverything in Free, plus:\n\n* Run 3 cloud models at a time\n* Faster responses from cloud hardware\n* Larger models for challenging tasks\n* 3 private models\n* 3 collaborators per model\n\nits been a lot slower for usage within zed for me the last hours - does anyone have more information whats happening to the pro subscription? it seems like the changes in the subscription are random and without any notice to users? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qj02gw/new_rules_for_ollama_cloud/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0vg4mk",
          "author": "jmorganca",
          "text": "Hi there. I work on Ollama. No new restrictions on the cloud model usage with this change. We actually increased usage amounts on each plan on Monday and will share more about that this week. Our goal is to make this a the best subscription for using open models with your favorite tools as we add more model support, better performance and reliability.\n\nHappy to answer any questions and if you hit any issues or limits with the plans let me know (email is jeff at ollama.com)",
          "score": 8,
          "created_utc": "2026-01-21 15:52:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vraab",
              "author": "jmorganca",
              "text": "Also, OP, let me know if you're still seeing any slowdown (and for which models). We've been working on improving performance and capacity a lot in the last few weeks and will keep doing so. (Feel free to DM/email me)",
              "score": 6,
              "created_utc": "2026-01-21 16:42:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0w2o4h",
              "author": "killing_daisy",
              "text": "hi, it a bit worrying, that the contracts are sortof changing without notice to active users, i've only seen this because i was investigating the slow down - i'll have a try today at home, maybe it was only our network at the company.  \nedit: forgot to thank you for a response :) - so thanks :D",
              "score": 2,
              "created_utc": "2026-01-21 17:33:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qidbdr",
      "title": "Local LLM (16GBRAM + 8VRAM) for gamedev",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qidbdr/local_llm_16gbram_8vram_for_gamedev/",
      "author": "RagingBass2020",
      "created_utc": "2026-01-20 20:58:38",
      "score": 7,
      "num_comments": 6,
      "upvote_ratio": 0.9,
      "text": "I am a developer that has been doing gamedev for 2 years but I used to be a backend developer for almost 10 years and a CS researcher before that.\n\nI use mostly Unity and Jetbrains Rider. \n\nAlthough I have a computer with more RAM at home, I need something that runs on a 16+8 GB laptop.\n\nI don't want to use it to develop full systems. I want something that is decent enough to create boilerplate code and help with some scripts and maybe some stuff I'm less used to (getting ready for the global game jam).\n\nIt needs to run offline with no access to the internet. I'm using ollama but I also have ComfyUI for some uni classes I was taking last semester.\n\nIf anyone could give me recommendations, I'd appreciate it.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qidbdr/local_llm_16gbram_8vram_for_gamedev/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0qovo3",
          "author": "zenmatrix83",
          "text": "the problem is context lenth, depending on what you want to do you might not be able to generate anything big enough. With the 24gb vram and 64gb ram I can only get decent speed with 64k context length. The problem I've seen with unity is most models don't know enough and you need to input examples or give the llm we search and wastes alot of context space\n\nI'm not saying don't try just have realistic expectations, you can get code but its really slow and usually not that good with smaller models and its usually only very popular languages that the model has alot in its training data. Agentic coding with cline or something similar is even tougher, the system prompt in roo was 20k or so. With it generating on small sections of code at a time it was often wrong since it didn't understand the entire file enough.",
          "score": 3,
          "created_utc": "2026-01-20 21:28:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0syvsz",
          "author": "Ryanmonroe82",
          "text": "I have a Lenovo Legion with 8gb VRAM and 32gb RAM. If I use an 8b model in BF16 it around 16gb by itself. I usually set the context length to 8092 and I can get about 14-16 tokens per second.  Time to first token can be a little slow but it‚Äôs usable for sure. I‚Äôm using DeepSeekR1 Distill 8b",
          "score": 2,
          "created_utc": "2026-01-21 05:09:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qj99g",
          "author": "XxCotHGxX",
          "text": "look for a model that is around 5GB. that way you can have it loaded ino your VRAM for faster inference. I like to use Qwen3 Coder 8B, but i am a ML engineer.",
          "score": 2,
          "created_utc": "2026-01-20 21:02:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0s5jdm",
          "author": "newbietofx",
          "text": "Tinyllama or smoll but I don't have success their temperature is not normal.¬†",
          "score": 1,
          "created_utc": "2026-01-21 02:08:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tqkt5",
          "author": "WaitformeBumblebee",
          "text": "I don't have experience with those programs but in general LLM seem to excel at Python and then have trouble with other languages and even some python modules especially ones that have many versions with deprecated stuff. Ideally to run locally we would need fine tuned versions of LLM for a certain language and version.",
          "score": 1,
          "created_utc": "2026-01-21 09:09:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0y54iz",
          "author": "StardockEngineer",
          "text": "No man. Not enough horsepower.   You‚Äôll make a lot of garbage.",
          "score": 1,
          "created_utc": "2026-01-21 23:14:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfauq9",
      "title": "Help a noob figure out how to achieve something in a game engine with Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qfauq9/help_a_noob_figure_out_how_to_achieve_something/",
      "author": "MountainPlantation",
      "created_utc": "2026-01-17 11:26:10",
      "score": 6,
      "num_comments": 10,
      "upvote_ratio": 0.88,
      "text": "Hey!\n\nI want to use Ollama to integrate it with a game engine. It's already in the engine and working, but I have some questions on what model I should use, and any tips in general for the experiments I want to do. I understand most LLMs running locally will take a while to think and generate a response, but for now let's ignore that.\n\n1. NPC Chat with commands: I know most people have tried doing NPC chatbots in engines, but I was thinking I could try to spice that up by integrating commands on it. Like the LLM would have a list of commands, given by me, that it could use contextually, like /laugh /cry /givePlayer(item), things like that. And I can make a system that parses the string and extracts/executes the commands. I attempted this one time, not in engine, just by using regular chat GPT and it would eventually come up with its own commands that were not stipulated by me. How to avoid that? Is there a model I should use for that?\n2. NPC consistency in character. I also tried one time to keep chat GPT in character, a peasant from the medieval ages, but I would ask about modern events like COVID and it would eventually break and talk about it as if he knew what it was.\n3. NPC Memory. What if I wanted to have NPCs remember things they have witnessed? I imagine I should make a log system that keeps every action done to that npc (NPC was hit by Player. NPC killed bandit. NPC found 1 gold etc) and then adding it to the beggining of the prompt as a little memory. Is that enough?\n4. Can I reliably limit the response length or is it finicky? Like, setting a limit of how many words per response \n5. Is there a way to guarantee responses are always in character? Because sometimes some of the LLMs will say \"I cannot answer to things related to that\" and that would be a big immersion breaker \n\nAnd another general question, is there a way to train certain models to get them used to a certani context? like i said, using commands I create in my game, or training them to act like a specific type of character etc.\n\nAgain, other than my experiments with just the chat GPT window, I am pretty new to this. If you have advice on what models to use or best practices, I'm listening.\n\nThank you!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qfauq9/help_a_noob_figure_out_how_to_achieve_something/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o040jq8",
          "author": "CooperDK",
          "text": "Just a comment, don't use ollama. Provide koboldcpp in a variant specific to the individual user's hardware. Ollama is too slow. In most cases, kobold is much faster.",
          "score": 1,
          "created_utc": "2026-01-17 14:39:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05cp8s",
              "author": "MountainPlantation",
              "text": "Can it do those things you mentioned?\n\n\nUpon testing something like deepseek takes roughly 16 seconds to respond¬†\n\n\nSomething like tinyllama which is supposed to be fast takes 3 which is still long for most applications\n\n\nIs koboldcpp JUST for character? Because I wanted to make it do other things like game direction and level generation¬†",
              "score": 1,
              "created_utc": "2026-01-17 18:29:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0hjsvy",
                  "author": "CooperDK",
                  "text": "It sure can. Koboldcpp is a CLI engine but can run silent without a console. It can do whatever you instruct the model to do. I am currently using it for AI dataset generation for an anthro AI model.",
                  "score": 1,
                  "created_utc": "2026-01-19 15:11:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o09219a",
                  "author": "alt-weeb",
                  "text": "respectfully nobody's gonna play your fucking game if the entire thing is ai generated, hoping that's not your intention bc if so you're wasting your time",
                  "score": -2,
                  "created_utc": "2026-01-18 06:48:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qeatiw",
      "title": "New version of Raspberry Pie Generative AI card (HAT+ 2)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qeatiw/new_version_of_raspberry_pie_generative_ai_card/",
      "author": "Unique_Winner_5927",
      "created_utc": "2026-01-16 08:43:42",
      "score": 5,
      "num_comments": 6,
      "upvote_ratio": 0.86,
      "text": "Perfect for private assistants, industrial equipment,¬†proof of concept, ...\n\n[https://www.raspberrypi.com/news/introducing-the-raspberry-pi-ai-hat-plus-2-generative-ai-on-raspberry-pi-5/](https://www.raspberrypi.com/news/introducing-the-raspberry-pi-ai-hat-plus-2-generative-ai-on-raspberry-pi-5/)\n\n\\#RaspberryPi #DataSovereignty #EmbeddedAI",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qeatiw/new_version_of_raspberry_pie_generative_ai_card/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nzw85q6",
          "author": "-Akos-",
          "text": "Jeffrey Geerling was less impressed: [https://youtu.be/jRQaur0LdLE](https://youtu.be/jRQaur0LdLE)\n\nBesides, all models I‚Äôve seen run on this board were 1.5B parameters or less, and older models at that.",
          "score": 2,
          "created_utc": "2026-01-16 10:01:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwoo6q",
              "author": "Comfortable_Ad_8117",
              "text": "Was that the original board or the +2 -The new +2 has 8gb of onboard ram and 2x speed",
              "score": 1,
              "created_utc": "2026-01-16 12:16:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzwu1yx",
                  "author": "-Akos-",
                  "text": "It's all in the video. In the end, you're better off buying a second Pi5 than this thing.",
                  "score": 3,
                  "created_utc": "2026-01-16 12:52:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzwp5t7",
                  "author": "danishkirel",
                  "text": "Still slower than the pi 5 cpu and there are models with more system memory too.",
                  "score": 1,
                  "created_utc": "2026-01-16 12:19:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzy284k",
          "author": "Unique_Winner_5927",
          "text": "Or we can directly install Ollama on Pi 5 : [https://monraspberry.com/installer-ollama-raspberry-pi/](https://monraspberry.com/installer-ollama-raspberry-pi/)",
          "score": 1,
          "created_utc": "2026-01-16 16:30:44",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o069278",
          "author": "tecneeq",
          "text": "The problem is the slow access to ram. You can fit up to 9b models i think, but they are all dense models, so everything is extremely slow.\n\nHowever, it has no load on the CPU, so that is great. It even works with a 1GB PI5.",
          "score": 1,
          "created_utc": "2026-01-17 21:07:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qgbl6g",
      "title": "(linux) i'm interested in historical roleplay (1600s)/early modern period), what would be your setup ?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qgbl6g/linux_im_interested_in_historical_roleplay/",
      "author": "Mid-Pri6170",
      "created_utc": "2026-01-18 15:33:57",
      "score": 4,
      "num_comments": 8,
      "upvote_ratio": 0.76,
      "text": "my longer term goal is to use gemini or other ai to make a little isometric world in Godot i can explore.\n\nyesterday gemini had me instal olama and lama3 on my pc. \n\ni only ran it in the terminal, but i am interested in what other things to consider to make it emersive.... considering cgpt etc are nerf'd\n\nGemini suggest Dolphin, Qwen and Nemo models too. however i was wondering if these models have a lot obscure trivia, knowledge of the period, language etc in them like the big llms do, otherwise they will quickly sound stale.\n\ni was thinking there might be a specially trained model on period language/literature?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qgbl6g/linux_im_interested_in_historical_roleplay/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0dhq2e",
          "author": "sinan_online",
          "text": "I have been testing this kind of stuff with self-hosted Ollama models as well, I the Eberron campaign setting. I also tested quite a bit of map generations as images.\n\nFor storytelling and role play, in the end, it has much more to do with RAG, or what you put in front of the model, in its context. At least, that has been my impression.\n\nI containerized much of what I did, PM me and I‚Äôll point you to the containers, if you are interested in using them.",
          "score": 1,
          "created_utc": "2026-01-18 22:52:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0dqek1",
              "author": "Mid-Pri6170",
              "text": "cause im using reddit in the UK i cant dm people as i refuse to verify my profile with a credit card! doh!",
              "score": 3,
              "created_utc": "2026-01-18 23:36:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0dqh2z",
                  "author": "Mid-Pri6170",
                  "text": "> Eberron campaign setting\n\nwhats that?",
                  "score": 2,
                  "created_utc": "2026-01-18 23:36:31",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qd7dmx",
      "title": "Hey all- I built a self-hosted MCP server to run AI semantic search over your own databases, files, and codebases. Supports Ollama and cloud providers if you want. Thought you all might find a good use for it.",
      "subreddit": "ollama",
      "url": "/r/selfhosted/comments/1qbv3fm/ragtime_a_selfhosted_mcp_server_to_run_ai/",
      "author": "mattv8",
      "created_utc": "2026-01-15 02:33:30",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qd7dmx/hey_all_i_built_a_selfhosted_mcp_server_to_run_ai/",
      "domain": "",
      "is_self": false,
      "comments": []
    }
  ]
}