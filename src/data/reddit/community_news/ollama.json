{
  "metadata": {
    "last_updated": "2026-02-26 03:06:08",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 20,
    "total_comments": 148,
    "file_size_bytes": 154945
  },
  "items": [
    {
      "id": "1rajqj6",
      "title": "15,000+ tok/s on ChatJimmy: Is the \"Model-on-Silicon\" era finally starting?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/gallery/1rajqj6",
      "author": "Significant-Topic433",
      "created_utc": "2026-02-21 06:19:08",
      "score": 570,
      "num_comments": 134,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6khfgk",
          "author": "Pantoffel86",
          "text": "Guess we'll start using kilotokens then. \n\n15ktok/s",
          "score": 93,
          "created_utc": "2026-02-21 07:53:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ko51t",
              "author": "JohnnyLovesData",
              "text": "Maybe even Trillion Kilo tokens. A TiKtok, if you will.",
              "score": 88,
              "created_utc": "2026-02-21 08:59:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6kreiv",
                  "author": "insanemal",
                  "text": "![gif](giphy|ac7MA7r5IMYda)",
                  "score": 53,
                  "created_utc": "2026-02-21 09:32:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6lnsh4",
                  "author": "KaMaFour",
                  "text": "So glad SI solved this problem before it could appearÂ ",
                  "score": 8,
                  "created_utc": "2026-02-21 14:01:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6kpybt",
                  "author": "Pantoffel86",
                  "text": "I like your thinking, but wouldn't a trillion kilo tokens be a petatok?",
                  "score": 6,
                  "created_utc": "2026-02-21 09:17:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6lt76n",
              "author": "Nzkx",
              "text": "Or approximately 60 kilobytes per seconds (assuming a token is on average 4 byte).",
              "score": 1,
              "created_utc": "2026-02-21 14:33:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6qupub",
              "author": "sfscsdsf",
              "text": "or ktps",
              "score": 1,
              "created_utc": "2026-02-22 09:20:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o74zi5q",
              "author": "XCSme",
              "text": "tiktoks",
              "score": 1,
              "created_utc": "2026-02-24 14:09:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6p15qd",
              "author": "OilTechnical3488",
              "text": "I created https://github.com/0xMH/chatjimmy-api",
              "score": 0,
              "created_utc": "2026-02-22 00:52:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6qosd1",
                  "author": "Significant-Topic433",
                  "text": "From chatjimmy team about api\n\nhttps://preview.redd.it/pyw62d59a0lg1.jpeg?width=720&format=pjpg&auto=webp&s=975ea42613688bc0770863e21ae4eea2b5bd0084",
                  "score": 3,
                  "created_utc": "2026-02-22 08:23:28",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6kjpcj",
          "author": "MrPhatBob",
          "text": "I think that an ASIC will need a pretty well defined model that you're happy provides you with sensible responses, with the speed that models are being released at the moment you may find that you have a reel of outdated models fused on the silicon within a month or so.\n\nYou could do something with an FPGA accelerator at Â£15k + development time, but you'd need to be up on the tools and technology. There are a few start-ups in my area who are looking to get AI on silicon, so the era is showing signs of life.\n\nhttps://www.mouser.co.uk/ProductDetail/BittWare/IA-860m-0037?qs=%252BICfH0Hx1eTAUbkQBMisxA%3D%3D",
          "score": 41,
          "created_utc": "2026-02-21 08:15:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lcwmt",
              "author": "Sixstringsickness",
              "text": "FPGA was exactly where my mind went as well.Â  Hardcoding a model into silicon, while an amazing proof of concept seems unfeasible for the rate these models are evolving.Â  By the time you are able to scale the number required for concurrency you would be very far behind.Â Â ",
              "score": 18,
              "created_utc": "2026-02-21 12:48:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lobeq",
                  "author": "KaMaFour",
                  "text": "I don't see why silicon version of a model released now and delivered in 2 months (if their claims are true) wouldn't be useful for the next year or maybe longer if it can be served for virtually free to almost anyone. It could realistically replace \"fast\" stupider model tiers",
                  "score": 7,
                  "created_utc": "2026-02-21 14:04:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6lzq9l",
                  "author": "New-Employer-2539",
                  "text": "This would have a place in the expert fields where knowledge base doesn't change fast. Like law knowledge of specific country. Or maybe even healthcare knowledge.",
                  "score": 3,
                  "created_utc": "2026-02-21 15:10:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6wywno",
                  "author": "az226",
                  "text": "I think the idea is to bring the time horizon down from concept to printout. \n\nAnd the idea is that the concept can be done during pre-training, because they know what they will target. So by working with big labs, the ASICs can be ready and deployed waiting for the model to go through full training, post training, and testing before customer access. \n\nI think it can work.",
                  "score": 1,
                  "created_utc": "2026-02-23 07:25:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6xoroh",
                  "author": "Destroyer-128",
                  "text": "Think about robots in future which has a memory modules and core execution model catridge.",
                  "score": 1,
                  "created_utc": "2026-02-23 11:33:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ze48q",
              "author": "orionparrott",
              "text": "[openfpga.ai](http://openfpga.ai) we have the LLama 3.3 70B deployed on the IA-860M, available through our new API marktplace. Contact on website for access. ",
              "score": 2,
              "created_utc": "2026-02-23 17:16:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6lg2um",
              "author": "xRmg",
              "text": "Well combine a few or these asics with tor conditional or role chaining, feed it into a GPU powered model if it needs it and you can get rid of a lot of slow expensive computing. \n\nHaving an ASIC a pre or post filter can offload quite a lot of stuff. Routing becomes fast and cheap.",
              "score": 1,
              "created_utc": "2026-02-21 13:11:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6p1a0o",
              "author": "xmsxms",
              "text": "For consumers maybe. But for the big guys running these hot 24/7 they can probably treat them as disposable and make their money back on electricity and reduced scale savings within a month. Their 'pro' tier models trickle down to the free tier, so the models and the units will get twice the life out of it. With the heavy usage they probably die before being outdated.\n\nI'm no expert, but I would imagine you could use something like this as a first line of routing to the correct model. i.e \"is the prompt for code, image, search or medical?\" and route to the appropriate model.",
              "score": 1,
              "created_utc": "2026-02-22 00:53:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6q1rmt",
              "author": "Lyuseefur",
              "text": "So - yâ€™all know whatâ€™s going to really blow your minds?\n\nMany of the models have similar guts. Kinda like how x64 architecture internals are similar.\n\nSoooâ€¦yeahâ€¦ itâ€™s going to get wild.\n\nOh and RAM wonâ€™t mean as much anymore",
              "score": 1,
              "created_utc": "2026-02-22 04:59:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6qv69p",
              "author": "doppleron",
              "text": "My guess is they are actually burning ROM to dramatically reduce time required to access weights. This is much, much simpler and cheaper than an FPGA and could be quickly customized. Remember when your boot code was burned into a ROM chip that your processor used to get going? Like that. Much faster and more durable than RAM, or God help us, a hard drive. (Yeah, I'm old.)",
              "score": 1,
              "created_utc": "2026-02-22 09:24:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6toz22",
                  "author": "RoutineNet4283",
                  "text": "Man this is so cool, How can I build some POC like this myself.",
                  "score": 2,
                  "created_utc": "2026-02-22 19:23:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6tgolb",
              "author": "Huge_Freedom3076",
              "text": "The thing is this we just need reasoning in this ASIC. Nothing more. All constants, raw data can called from rags. So I'm not a professional in the field but in the end of the days we will have a good model etched on a good ASIC and this is our new CPU .",
              "score": 1,
              "created_utc": "2026-02-22 18:44:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ldbn6",
          "author": "8B1_t",
          "text": "Its so fucking fast also this is the website if anyone wants to try it   \n[https://chatjimmy.ai/](https://chatjimmy.ai/)",
          "score": 24,
          "created_utc": "2026-02-21 12:51:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n2h2e",
              "author": "NVC541",
              "text": "Holy shit that is absurdly fast",
              "score": 10,
              "created_utc": "2026-02-21 18:25:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6qnswi",
              "author": "hurdurdur7",
              "text": "I gave it a bunch of prompts. Yes this speed is the killer. All basic jobs will be taken by this. Yes they need to implement a bigger model - but still, this beats nvidia's approach or anyone else's approach by a light year. \n\nEven if the hardware for an instance will cost 100k , it will pay for itself in a correct implementation in no time what so ever. Time to learn growing potatoes or building power plants. ",
              "score": 5,
              "created_utc": "2026-02-22 08:14:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73hl2m",
                  "author": "mastercoder123",
                  "text": "No it wont. The only reason openai, anthropic and all the other ones arent using asics is because once it's made its made, there is no changing it and if you need to change the model for x bug hallucination you need a new asic with a new model. These companies are making their models better and better as models always get better. Putting them into asics, while making them (excuse my french) fucking stupid ass fast, also makes them unchangeable",
                  "score": 1,
                  "created_utc": "2026-02-24 07:02:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6s4iut",
                  "author": "jerrygreenest1",
                  "text": ">it will pay for itself in a correct implementation in no time what so ever\n\nLike OpenAI? Can you remind us, is OpenAI profitable yet? Oh wait, it never wasâ€¦",
                  "score": 1,
                  "created_utc": "2026-02-22 15:03:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6qmqhy",
              "author": "smallfried",
              "text": "That is super quick. It can basically create webpages on user demand. With those speeds (and J/t energy efficiency) you could have it parse a bunch of stuff and react to it quickly.",
              "score": 2,
              "created_utc": "2026-02-22 08:04:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s0r4g",
                  "author": "floppypancakes4u",
                  "text": "If it were smart enough, sure, but this is a very old model that does not yet have the intelligence for it.\n\n",
                  "score": 1,
                  "created_utc": "2026-02-22 14:44:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6w4ee4",
              "author": "Cultured_Alien",
              "text": "What model?",
              "score": 1,
              "created_utc": "2026-02-23 03:27:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6wzbvb",
                  "author": "az226",
                  "text": "Llama 3.1 8B",
                  "score": 1,
                  "created_utc": "2026-02-23 07:29:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6x6kmu",
              "author": "Bidalos",
              "text": "Holy fuck ! It's so fast",
              "score": 1,
              "created_utc": "2026-02-23 08:39:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o771h5n",
              "author": "Educational-Body4205",
              "text": "WOW !  lol,   This is great.   From what I can tell, it gives very similar results to Gemini thinking. ",
              "score": 1,
              "created_utc": "2026-02-24 19:49:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o773r5e",
              "author": "misha1350",
              "text": "The only problem is that the model is quite dumb",
              "score": 1,
              "created_utc": "2026-02-24 20:00:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7flyrq",
                  "author": "Firipu",
                  "text": "That's putting it very mildly. It falls for every single \"gatcha\" that made the rounds in the past 2 years (e.g. how many r's in strawberry etc)",
                  "score": 1,
                  "created_utc": "2026-02-26 01:06:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6kkzm3",
          "author": "thedarkbobo",
          "text": "Its fast. But the quality hmm...for my purpose it seems low.",
          "score": 16,
          "created_utc": "2026-02-21 08:28:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lusgm",
              "author": "LumbarJam",
              "text": "Itâ€™s basically a PoC to get a feel for speedâ€”almost an alpha. This first version is running with 3-bit INT quantization. According to their site, the next version (mid-year) will be bigger, faster, and use FP4 quantization.",
              "score": 17,
              "created_utc": "2026-02-21 14:43:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6km3j1",
              "author": "SinnersDE",
              "text": "Its POC just 8B Model. Scale it up to 120B and think about it again. \nNVIDIA Short (they will buy it afterwards)",
              "score": 8,
              "created_utc": "2026-02-21 08:39:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6koq1l",
                  "author": "ghoarder",
                  "text": "Can it scale to a 120B model though? If the model is etched direct into the silicon and that 8B chip looks quite big, a 120B would need to be about 4 times the physical size in each axis, or 15 times the area.  That would be a pretty large bit of silicon.",
                  "score": 6,
                  "created_utc": "2026-02-21 09:05:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6kr82z",
                  "author": "thedarkbobo",
                  "text": "That would be good.",
                  "score": 1,
                  "created_utc": "2026-02-21 09:30:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ng9tl",
          "author": "p0u1",
          "text": "Everyone is missing the fact that this could be used as a speculative decoding model and speed up any model built on the same instructions",
          "score": 7,
          "created_utc": "2026-02-21 19:33:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6poi1y",
              "author": "austrobergbauernbua",
              "text": "Could you please elaborate?",
              "score": 3,
              "created_utc": "2026-02-22 03:24:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6qir0z",
                  "author": "p0u1",
                  "text": "So you use a smaller model in general to draft the output then the larger model runs on top to fill in the bits that canâ€™t be drafted.\n\nThatâ€™s a very basic explanation.\n\nhttps://youtu.be/qmAbco38pXA?si=dZrQqCLhAeujApAJ\n\nQuite a good video showing it in use.",
                  "score": 3,
                  "created_utc": "2026-02-22 07:26:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72zdt9",
              "author": "Ashamed-Duck7334",
              "text": "I think communication overhead would kill you. Spec decode is working at the token level (and I think, even that's too coarse), this system is good at \"blasting\", I think it's got lower TTFT, but does it have good enough TTFT to be in the hot path for spec decode while absorbing a network hop, I think probably naaaaaaaaaahhhhh (just the network hop outside of nvlink is nahh)",
              "score": 2,
              "created_utc": "2026-02-24 04:37:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73gn75",
                  "author": "p0u1",
                  "text": "The network overhead would be huge and the massive speed would be wasted but itâ€™s just a thought at this point.\n\nItâ€™s a very interesting product but canâ€™t see how it wouldnâ€™t become outdated extremely quick used as a llm itâ€™s self.",
                  "score": 1,
                  "created_utc": "2026-02-24 06:54:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6kj8i6",
          "author": "frank_brsrk",
          "text": "It is indeed super fast, yesterday was trying it. It explicitly says that it collects your data",
          "score": 5,
          "created_utc": "2026-02-21 08:10:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qo8u6",
              "author": "smallfried",
              "text": "I assume all online LLM services are.",
              "score": 7,
              "created_utc": "2026-02-22 08:18:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nrfky",
          "author": "synth_jarvis",
          "text": "Honestly, etching model weights directly into silicon sounds wild! I get why folks might be worried about being stuck with one model, especially since AI is evolving at breakneck speed. But on the flip side, the idea of super fast and cheap inference is pretty appealing, especially for use cases where you just need reliable performance and aren't as concerned about having the absolute latest model. It reminds me of how GPUs transformed gaming performance. This could really shake things up for certain applications. What do you guys thinkâ€”worth the risk or too limiting?",
          "score": 3,
          "created_utc": "2026-02-21 20:32:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6l8rb7",
          "author": "thatguy122",
          "text": "Seeing the post for Taalas yesterday made me feel as uneasy as ChatGPT when it was first revealed. This development hits different than the model progress we've been seeing to-date.Â \n\n\nThe implications of this...assuming it checks out...seems absolutely staggering. Not to be dramatic, but, perhaps the final warning shot for action.Â ",
          "score": 10,
          "created_utc": "2026-02-21 12:15:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6m1z22",
              "author": "ductiletoaster",
              "text": "Iâ€™m not sure Iâ€™m following how this is a warning shot? Taalas is essentially just taking an existing model and creating specialized hardware for it.\n\nThe results will be incredibly efficient (power and speed) at the cost of being immutable (e.g. requiring hardware swapping to change models).\n\nI guarantee google is already doing this behind closed doors. This is just economics at scale. \n\nWhat is it youâ€™re sounding an alarm on?",
              "score": 10,
              "created_utc": "2026-02-21 15:22:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rk6k5",
                  "author": "thatguy122",
                  "text": "When I say warning shots, I mean those who have been ignorant to the progress and development of models in the past 2.5 years need to open their eyes. That includes devs, policymakers, etc.\n\n\nTake as you will but I don't think it's being dramatic for those unaware to become aware.Â \n\n\n\n",
                  "score": 1,
                  "created_utc": "2026-02-22 13:06:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6qncl5",
              "author": "smallfried",
              "text": "Without saying why you're talking about warning shots, you're definitely being dramatic.",
              "score": 1,
              "created_utc": "2026-02-22 08:09:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rk800",
                  "author": "thatguy122",
                  "text": "When I say warning shots, I mean those who have been ignorant to the progress and development of models in the past 2.5 years need to open their eyes. That includes devs, policymakers, etc.\n\nTake as you will but I don't think it's being dramatic for those unaware to become aware.Â \n\n",
                  "score": 1,
                  "created_utc": "2026-02-22 13:06:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6lc0oc",
          "author": "Mice_With_Rice",
          "text": "Is this kind of hardware able to support LoRA? Otherwise your stuck in a single base model forever.",
          "score": 3,
          "created_utc": "2026-02-21 12:41:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6s4a24",
              "author": "TaiMaiShu-71",
              "text": "They claim it supports lora.",
              "score": 2,
              "created_utc": "2026-02-22 15:02:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6mb687",
          "author": "ba2sYd",
          "text": "It is cool but every few months new llm models comes out so I wonder if it would be that efficient...",
          "score": 3,
          "created_utc": "2026-02-21 16:09:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mqye1",
              "author": "momono75",
              "text": "I guess we will not be able to tell the differences between new models in the near future due to LLMs surpassing humans. At that point, this approach can be a good choice.",
              "score": 2,
              "created_utc": "2026-02-21 17:28:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6mxsmd",
                  "author": "Former-Ad-5757",
                  "text": "It can be a good choice already before that, this kind of speedup can change a whole lot of games, just because you can bruteforce through the errors/hallucinations. Wanna have better performance for a problem, just ask it 3x and take the average.",
                  "score": 3,
                  "created_utc": "2026-02-21 18:02:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6lbud4",
          "author": "evilbarron2",
          "text": "Wait - you didnâ€™t think things were gonna remain unchanging in the world of AI, the fastest-evolving tech weâ€™ve seen in decades, did you?Â ",
          "score": 2,
          "created_utc": "2026-02-21 12:40:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6leglp",
          "author": "Simple_Library_2700",
          "text": "Cool development, but it being an asic means you cannot change the model being used. With the speed things are moving that is not ideal. \n\nNot to mention the size issue if you wanted to scale past 8B the size of the asic would scale linearly with the model size and you would end up with a gargantuan chip that would be impossible to produce. But maybe you could leverage chiplets or something to do with MoE.",
          "score": 2,
          "created_utc": "2026-02-21 13:00:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ri6s3",
              "author": "Imp_erk",
              "text": "That's the challenge for sure. Any chip that runs a SotA models without absurd quantization will be beyond current capabilities to produce. Even small models will be incredibly costly. Nevermind LoRA and other techniques not being possible on these.",
              "score": 1,
              "created_utc": "2026-02-22 12:51:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6syoi4",
              "author": "Specter_Origin",
              "text": "May be its like a dvd with a game, you can't change the game that came on dvd. As in chip becomes cheap enough to become a package for weights. ",
              "score": 1,
              "created_utc": "2026-02-22 17:21:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6xgt0e",
              "author": "adamphetamine",
              "text": "Cerberas already use an entire 300mm wafer",
              "score": 1,
              "created_utc": "2026-02-23 10:19:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ntiyf",
          "author": "mockingtruth",
          "text": "Having a reasonably good model but in raspberry pi sized form and for less than 100 would be good.  Also plugging openclaw into this might be interesting",
          "score": 2,
          "created_utc": "2026-02-21 20:43:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rk5b4",
          "author": "Silver_Jaguar_24",
          "text": "Wow, it's blazingly fast. But the quality of the LLM used, **Llama 3.1 8B**, is obviously sub-par. But it proves a point, that you don't need a supercomputer to have mind-blowingly fast LLM responses.\n\n**ChatJimmy** isn't just a fast chatbot; it's a proof of concept that the future of AI might not be giant, power-hungry data centers, but specialized \"AI bricks\" that are cheap enough to put in everything from your toaster to your glasses.\n\nIf running \"locally\", you might have to upgrade the chip once a year maybe, to get the current/latest model LLM of choice. Looks like this might be the future for local-LLMs. Try it here - [https://chatjimmy.ai/](https://chatjimmy.ai/)",
          "score": 2,
          "created_utc": "2026-02-22 13:05:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k8te2",
          "author": "pamidur",
          "text": "It was inevitable, look at Bitcoin",
          "score": 1,
          "created_utc": "2026-02-21 06:32:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6l36eb",
          "author": "Suitable-Program-181",
          "text": "If its cheaper than a mac M chip I will like it if not, they can have the sillicon but they still lack the kernels, just like M chips. Rather fight asahi and a solid beast like m3 - m4 than another APU. \n\nAMD APU's are actually dope if you can build around them so the sillicon being the model is the sauce no cap!",
          "score": 1,
          "created_utc": "2026-02-21 11:26:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lfiv6",
              "author": "Mice_With_Rice",
              "text": "Even if it were that cheap which it wont be, the physical dimensions of the chip if it were made for a more usfull model size with its current node process would be larger than the MAC itself. Have to return to ATX towers.",
              "score": 1,
              "created_utc": "2026-02-21 13:07:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70qm2v",
                  "author": "New-Week-1426",
                  "text": "I mean, even if it isn't a local chip, I could totally see a future where companies would self host such chips in their datacenters for local inferrence and upgrade the models by upgrading the chips. Only makes sense given a certain scale, but why not? ",
                  "score": 1,
                  "created_utc": "2026-02-23 21:02:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6oh89p",
          "author": "electricleisure",
          "text": "When a new version of the model comes out, could they come out with something like an expansion board rather than needing a whole new board? I imagine this would require some special training to account for the fact that the first X layers arenâ€™t changing.",
          "score": 1,
          "created_utc": "2026-02-21 22:51:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pce9g",
          "author": "ailee43",
          "text": "Failed the R's in strawberry test, but after telling it 3 times, it got it",
          "score": 1,
          "created_utc": "2026-02-22 02:04:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qdod2",
          "author": "joemacross",
          "text": "trying out chat jimmy and it's blowing my mind on how quick it is",
          "score": 1,
          "created_utc": "2026-02-22 06:39:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qol3i",
          "author": "smallfried",
          "text": "They could scale it down a bit. Add STT and TTS into it. Get it to a couple of Watt for generation and stick it in toys. Could run some chatty LLM on batteries like Teddy from the AI movie.",
          "score": 1,
          "created_utc": "2026-02-22 08:21:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qu4rq",
          "author": "doppleron",
          "text": "I think most of this discussion is missing a big point: The vast majority of applications don't need a huge context window (Think agents). They need a compact and streamlined \"brain\" to make many small, fast, consistent decisions. These chips, with supplemental data, could easily run your car, house, a small business, combat drone,  etc. Or a ~~Starlink~~ Skynet bot...",
          "score": 1,
          "created_utc": "2026-02-22 09:14:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r6yth",
          "author": "Ambadeblu",
          "text": "This would positively impact consumer hardware prices right?",
          "score": 1,
          "created_utc": "2026-02-22 11:17:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s8k1o",
          "author": "Auto_17",
          "text": "Exactly what i was thinking we need to figure out a way to make our current tech more efficient to use ai models not just adding more computers together. Our brain doesnt go get another brain to get better at a task it simply hardrewires our current neurons(aka practice).",
          "score": 1,
          "created_utc": "2026-02-22 15:23:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6si6nw",
          "author": "DateOk9511",
          "text": "wtf!!! it is soooo fast! where do we buy the hardware?\n\n",
          "score": 1,
          "created_utc": "2026-02-22 16:07:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t510i",
          "author": "Fair-Cookie9962",
          "text": "You can assume things will change quickly - and integrate that into your plans, including 10x performance in 12 months.",
          "score": 1,
          "created_utc": "2026-02-22 17:51:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tray1",
          "author": "Secret-Collar-1941",
          "text": "I asked it to count to 15,000. it failed to respond :(",
          "score": 1,
          "created_utc": "2026-02-22 19:35:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tsqtk",
          "author": "PaneerEater101",
          "text": "Can someone explain this to me? I don't understand. How are they getting such fast speeds? I tried it out on chat jimmy but it couldn't really explain how it was so fast",
          "score": 1,
          "created_utc": "2026-02-22 19:42:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ysmov",
              "author": "dheison0",
              "text": "It's fast because they have a specific hardware just for running jimmy, it's not a general porpuse hardware like GPUs. This way they can put the model to run on bare metal like we do with our operational systems(windows/linux), when running with GPU they needed to simulate some instrutions or use many of then to do only one thing, with ASIC they can make a single instruction that do it.",
              "score": 2,
              "created_utc": "2026-02-23 15:35:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6uix3i",
          "author": "egil87",
          "text": "will probably go the way of bitcoin/blockchain. general purpose until asic makes it impossible to compete.",
          "score": 1,
          "created_utc": "2026-02-22 21:54:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v91j9",
          "author": "TheNotSoEvilEngineer",
          "text": "Hope so, then they can focus on their ASIC designs instead of ruining the GPU market.",
          "score": 1,
          "created_utc": "2026-02-23 00:18:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vghy5",
          "author": "sooham",
          "text": "Expect the downside of being stuck with the same model architecture throughout the life time of the machine",
          "score": 1,
          "created_utc": "2026-02-23 01:00:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wmbrw",
          "author": "OldWitchOfCuba",
          "text": "Generated inÂ 0.001sÂ â€¢Â 17,267 tok/s",
          "score": 1,
          "created_utc": "2026-02-23 05:35:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x4bfn",
          "author": "Puzzled-Swimmer-4789",
          "text": "At that speed could that impact the way LLMs are built? For example we could have models that are optimized to think a lot before answering",
          "score": 1,
          "created_utc": "2026-02-23 08:17:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z3keb",
          "author": "Western-Source710",
          "text": "Your mistake was buying the Spark over the Ryzen 395+ AI Max units. Other than that, this is pretty interesting ðŸ¤”ðŸ§",
          "score": 1,
          "created_utc": "2026-02-23 16:26:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ze2bg",
          "author": "orionparrott",
          "text": "[openfpga.ai](http://openfpga.ai) we have the LLama 3.3 70B deployed on the IA-860M, available through our new API marktplace. Contact on website for access. ",
          "score": 1,
          "created_utc": "2026-02-23 17:15:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70l20o",
          "author": "Weird-Field6128",
          "text": "i need GLM-5 on 15,000 tokens per second. one can only dream of",
          "score": 1,
          "created_utc": "2026-02-23 20:35:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70osz2",
          "author": "NekoHikari",
          "text": "Probably the future will be sideloading a residual cuda/rocm based network alongside model on chip... ",
          "score": 1,
          "created_utc": "2026-02-23 20:53:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74g3cp",
          "author": "ominotomi",
          "text": "ah yes can't wait to pick what LLM card to buy instead of picking what model to download",
          "score": 1,
          "created_utc": "2026-02-24 12:12:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7a7y3z",
          "author": "gillesdami",
          "text": "Everyone is saying models evolve too fast for this. Yet older model are perfectly fine for the average person question on chat gpt, to do model routing or basic language processing tasks. I think this as serious application and coud cut the cost by 100 in those cases.",
          "score": 1,
          "created_utc": "2026-02-25 06:31:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7affpj",
          "author": "AnonymZ_",
          "text": "How tf is this so fast",
          "score": 1,
          "created_utc": "2026-02-25 07:37:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6l33yc",
          "author": "jerrygreenest1",
          "text": "I donâ€™t quite get the logic behind this idea because transistor does not equal neuron, a transistor can be either 0 or 1, while a neuron can be any state between 0 and 1, â€“ how could you Â«printÂ» neurons to a board?Â ",
          "score": -4,
          "created_utc": "2026-02-21 11:25:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lbrwo",
              "author": "Mice_With_Rice",
              "text": "Easy, it uses more than a single bit, just like every other computer does.",
              "score": 3,
              "created_utc": "2026-02-21 12:40:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lkb62",
                  "author": "jerrygreenest1",
                  "text": "Then I donâ€™t quite see the theoretical benefit behind such a board, you donâ€™t save on anything. Please donâ€™t just say Â«but they did it canâ€™t you see it worksÂ», I am talking theory here",
                  "score": 0,
                  "created_utc": "2026-02-21 13:39:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ltpbi",
              "author": "Nzkx",
              "text": "You can model 0-1 range in electronic and the proof is easy to demonstrate.\n\nSuppose that wouldn't be the case and we can not model 0-1 range in electronics.\n\nIf this is true, how your computer can store a floatting point number like 0.0023233 ? It's made of the same electronic components, and we know a computer can operate on floatting point numbers. So it must be false, hence it's possible, otherwise your computer wouldn't be able to express the 0-1 range.",
              "score": 3,
              "created_utc": "2026-02-21 14:36:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6m21hz",
                  "author": "jerrygreenest1",
                  "text": ">You can model 0-1 range in electronic\n\nOf course you can, what are you talking about. It's not the question. Question is â€“ how then it's different to the current approach? Where do we get benefit from?",
                  "score": 1,
                  "created_utc": "2026-02-21 15:22:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6m2jmf",
              "author": "snowgirl9",
              "text": "https://en.wikipedia.org/wiki/IEEE_754",
              "score": 2,
              "created_utc": "2026-02-21 15:25:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6m4hws",
              "author": "ApprehensiveDelay238",
              "text": "AI doesn't use neurons. ",
              "score": 2,
              "created_utc": "2026-02-21 15:35:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6my5zo",
                  "author": "jerrygreenest1",
                  "text": "And what do you know about AI? I have learned about this in an university, and you? Probably know nothing? AI also known as Â«neural networksÂ» â€“ are neural structures that consists of you guess... NEURONS. Even though not biological ones and instead mimicked with bytes that eventually are a bunch of transistors...",
                  "score": 2,
                  "created_utc": "2026-02-21 18:04:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rc3srb",
      "title": "I ran ClawBot with Ollama locally on my Mac â€” setup, gotchas, and honest review",
      "subreddit": "ollama",
      "url": "https://v.redd.it/1ee663elb5lg1",
      "author": "Spirited-Wind6803",
      "created_utc": "2026-02-23 01:19:58",
      "score": 191,
      "num_comments": 15,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6wslzf",
          "author": "Tyme4Trouble",
          "text": "Please donâ€™t do this on your personal system and certainly donâ€™t give OpenClaw access to your personal accounts. Spin up new ones and if must run it in an isolated environment, ideally not on your LAN. The number of security vulnerabilities associated with this software is astounding.",
          "score": 47,
          "created_utc": "2026-02-23 06:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z47gr",
              "author": "Crafty_Ball_8285",
              "text": "Natural selection",
              "score": 8,
              "created_utc": "2026-02-23 16:29:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7138p3",
                  "author": "Small_Succotash_2612",
                  "text": "as jesus said we sin and fail often",
                  "score": 1,
                  "created_utc": "2026-02-23 22:04:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o71fthy",
              "author": "I_hate_redditf",
              "text": "it's absolutely insane! it's like 700+ and so many CRITICAL ones",
              "score": 3,
              "created_utc": "2026-02-23 23:09:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o71tm51",
              "author": "commandermd",
              "text": "Isolated VM with firewall rules and no lan access. And donâ€™t give it unnecessary access to things like your email. If you absolutely must give it access, setup accounts (ex. GitHub) separate from your personal accounts.",
              "score": 1,
              "created_utc": "2026-02-24 00:26:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6wdwu0",
          "author": "no_rules_to_life",
          "text": "What all skills did you add?   \nWhat was your device configuration?   \nDid you make it write code for you? if so how?",
          "score": 8,
          "created_utc": "2026-02-23 04:32:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wfsyg",
          "author": "Fluffy-Psychology-18",
          "text": "Why do you recommend keeping the context window low? 32GB  is plenty for 8B model, especially Q4 quants.  My findings with Openclaw and Ollama is exactly the opposite. Increase the context window size and the \"bot\" will get much better.",
          "score": 6,
          "created_utc": "2026-02-23 04:45:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70htwv",
              "author": "d4mations",
              "text": "Yeah, Ihave to agree with this. Iâ€™m running ministral3-14b  on a mac mini 16gb as the daily task master and script writer and Iâ€™m very happy with it. I have reached 35k context without it even getting warm. Iâ€™m also running bge-m3 embedded for memory. When I need heavy lifting my agent spins up a sub agent on codex. So far so good but Iâ€™m just a noob trying to figure things out",
              "score": 2,
              "created_utc": "2026-02-23 20:20:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6w40es",
          "author": "Zenclobber",
          "text": "Did it build or do anything though?",
          "score": 4,
          "created_utc": "2026-02-23 03:24:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wnnzm",
              "author": "outofband",
              "text": "It run",
              "score": 7,
              "created_utc": "2026-02-23 05:46:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y6kgs",
          "author": "tecneeq",
          "text": "I would run llama.cpp instead to get a few more T/s, or possibly vLLM.\n\nReducing context speeds things up, but it makes the models unfit for longer tasks.\n\nI have made a bunch of perplexity benchmarks on a 6000 Blackwell of some smaller models and found that quantizing the context and caches from the default f16 to q8 has almost no measurable negative impact. So double your context, but quantize it down to 8. Also use flash attention.\n\nAt home i have a 5090 with 32GB and a 5070 Ti with 16GB, i'm trying to get them into one PC for possibly 48GB VRAM.",
          "score": 2,
          "created_utc": "2026-02-23 13:38:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74nxqs",
          "author": "Limp-Local2538",
          "text": "I believe it can be useful in working set, but google and x are banning the accounts operated by agents... ",
          "score": 1,
          "created_utc": "2026-02-24 13:04:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wmras",
          "author": "A_Dragon",
          "text": "If you completely maxed out a Mac mini whatâ€™s the best local model it could run and would it be sufficient for most uses of openclaw?",
          "score": 1,
          "created_utc": "2026-02-23 05:39:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wg1v1",
          "author": "joezinsf",
          "text": "Thank you",
          "score": -1,
          "created_utc": "2026-02-23 04:47:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zm6fz",
          "author": "Competitive-Push-949",
          "text": "Many Thanks",
          "score": 0,
          "created_utc": "2026-02-23 17:53:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdyosq",
      "title": "I built a locally-hosted AI agent that runs entirely on your own hardware no cloud, no subscriptions",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rdyosq/i_built_a_locallyhosted_ai_agent_that_runs/",
      "author": "Janglerjoe",
      "created_utc": "2026-02-25 00:56:51",
      "score": 111,
      "num_comments": 30,
      "upvote_ratio": 0.91,
      "text": "I built LMAgent a pure Python AI agent that connects to any OpenAI-compatible LLM (LM Studio, Ollama, etc.) and actually does things on your computer.\n\nNo cloud. No API fees. No subscriptions. Runs 100% on your own hardware.\n\n**What it can do autonomously:**\n\n* Read and write files\n* Run shell commands (bash / PowerShell)\n* Manage git (status, diff, commit, branch)\n* Track todos and multi-step plans\n* Spawn sub-agents to delegate tasks\n* Connect to external tools via MCP servers (web search, browsers, databases)\n* Schedule itself to wake up at a future time and resume work\n\n**Three ways to run it:**\n\n* Terminal REPL â€” conversational loop with a live background scheduler\n* One-shot CLI â€” give it a task, get a result, exit\n* Web UI â€” streaming tokens, inline tool calls, session browser, mobile-friendly\n\n**Setup is dead simple:**\n\n1. `pip install requests flask colorama`\n2. Point it at your local LLM server\n3. Set a workspace directory in a `.env` file\n4. Run `python agent_main.py`\n\nWorks on Windows, macOS, and Linux. MIT licensed.\n\nWould love feedback especially from anyone running it with larger models or unconventional LLM backends.\n\nGitHub: [https://github.com/janglerjoe-commits/LMAgent](https://github.com/janglerjoe-commits/LMAgent)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rdyosq/i_built_a_locallyhosted_ai_agent_that_runs/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o79er0q",
          "author": "jerrygreenest1",
          "text": "So what, is it like OpenClaw clone?",
          "score": 13,
          "created_utc": "2026-02-25 03:09:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79kiz1",
              "author": "IAmANobodyAMA",
              "text": "They should call it something similar but different enough. How about â€œClaudeâ€?",
              "score": 14,
              "created_utc": "2026-02-25 03:43:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7a72s6",
                  "author": "Garfieldealswarlock",
                  "text": "How about Clu?",
                  "score": 2,
                  "created_utc": "2026-02-25 06:24:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7c0qfl",
                  "author": "Flimsy_Leadership_81",
                  "text": "Clawbit\n\n",
                  "score": 1,
                  "created_utc": "2026-02-25 14:45:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7a4qcb",
          "author": "rjyo",
          "text": "The scheduling feature is a really nice touch. Being able to tell an agent to wake up and resume work later is something most agent frameworks skip over entirely. How does it handle context when it resumes? Does it serialize the full conversation state or just the task/plan?\n\n  \nAlso curious about the sub-agent delegation. Are those separate LLM sessions or do they share the same context window? With local models the context limit is usually the biggest bottleneck so that tradeoff matters a lot.",
          "score": 4,
          "created_utc": "2026-02-25 06:05:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7abq8e",
          "author": "Fair-Cookie9962",
          "text": "Everybody should do that as part of learning LLMs.",
          "score": 5,
          "created_utc": "2026-02-25 07:04:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7avv8q",
          "author": "woejise",
          "text": "Could it fit on a raspberry pi 5?",
          "score": 2,
          "created_utc": "2026-02-25 10:10:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bfe3l",
              "author": "Janglerjoe",
              "text": "I havenâ€™t tested it on a Pi 5 yet, but the agent itself is lightweight the main constraint would be the LLM runtime and available RAM. If youâ€™re running a small quantized model, it should work fine since execution happens in Docker with configurable resource caps.",
              "score": 3,
              "created_utc": "2026-02-25 12:46:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7en8wg",
                  "author": "woejise",
                  "text": "If I end up running it ill let you know how it does. Ive got the 16gb pi with a 26 tops ai hat",
                  "score": 1,
                  "created_utc": "2026-02-25 22:01:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7cm8qh",
          "author": "yoshkoHS85",
          "text": "How it works with smaller models like 14b q4?",
          "score": 2,
          "created_utc": "2026-02-25 16:26:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ecg0y",
              "author": "Janglerjoe",
              "text": "Iâ€™ve been running it with Qwen3 8B (VL) in 6-bit and it handles most coding and file-task workflows fine. I havenâ€™t stress-tested extremely long, multi-hour tasks yet, but for typical planning + execution loops it performs reliably.",
              "score": 2,
              "created_utc": "2026-02-25 21:11:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ahbls",
          "author": "redonculous",
          "text": "Can it run in portainer?",
          "score": 1,
          "created_utc": "2026-02-25 07:54:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c121f",
          "author": "Flimsy_Leadership_81",
          "text": "follow",
          "score": 1,
          "created_utc": "2026-02-25 14:47:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f2p52",
          "author": "TinyDesigner9155",
          "text": "Commenting so I don't forget",
          "score": 1,
          "created_utc": "2026-02-25 23:20:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ai3n4",
          "author": "emmettvance",
          "text": "giving an agent raw shell access is always a bit terrifying but mcp support makes this actually viable for real workflows..",
          "score": 1,
          "created_utc": "2026-02-25 08:01:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7b742y",
              "author": "Janglerjoe",
              "text": "Totally. Thatâ€™s why itâ€™s sandboxed in Docker by default the LLM never gets direct host access",
              "score": 3,
              "created_utc": "2026-02-25 11:47:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7fh7xu",
                  "author": "TeachNo196",
                  "text": "Unless if you hack the kernel. Only bad thing about docker.",
                  "score": 1,
                  "created_utc": "2026-02-26 00:40:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o79zfos",
          "author": "smwaqas89",
          "text": "sounds interesting! i wonder how it handles different hardware setups though. like, does it optimize for performance based on the specs of your machine? that could really change how effective it is for different users.",
          "score": 0,
          "created_utc": "2026-02-25 05:24:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7alqrj",
          "author": "FloppyWhiteOne",
          "text": "Love this. Iâ€™ve made my own version too but in rust and fully windows based (will make Linux compatible after)\n\nMine has customisations well past most in terms of token squashing, memory and context loading for local llms. Iâ€™ve even made a sweet hot swap utility for lm studio so I can swap models at will to save vram and never gos over system vram. Iâ€™m still heavily in dev with it atm.\n\nMy aim is to make dumb models act smart on consumer hardware. Results so far are looking great.\n\nThanks for sharing I will as soon as itâ€™s in a ready state.\n\nI love when my app runs it only uses 4.5mb â€¦ gotta love some rust efficiency (pic of web ui) \n\nhttps://preview.redd.it/jthckkc9ullg1.png?width=1987&format=png&auto=webp&s=dabe5e06b23b187e66c5820bef7fddbf9e56bef9",
          "score": -2,
          "created_utc": "2026-02-25 08:36:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bxc5h",
              "author": "aptonline",
              "text": "Is this public (GitHub)?",
              "score": 1,
              "created_utc": "2026-02-25 14:28:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bxtkg",
                  "author": "FloppyWhiteOne",
                  "text": "When Iâ€™ve perfected the hot loading for agents with lm studio then yes. Iâ€™m fighting having models loaded, with full context. Then at times they fall back to 4096 (small window) but yeah my aim is to make public to get more help with it",
                  "score": 3,
                  "created_utc": "2026-02-25 14:30:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rc8xvy",
      "title": "spend more time downloading models than actually using them",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rc8xvy/spend_more_time_downloading_models_than_actually/",
      "author": "Sharp-Mouse9049",
      "created_utc": "2026-02-23 05:29:41",
      "score": 76,
      "num_comments": 18,
      "upvote_ratio": 0.92,
      "text": "qwen 2.5 dropped and suddenly mixtral is dead to me. downloaded the 72b. ran it once. went back to 7b cause i dont actually need 72b for anything i do\n\n\n\ngot like 200gb of models sitting on my drive. couldnt tell you the difference between half of them without checking the folder names\n\n\n\nevery week theres a new one thats supposedly better and i gotta have it. run some vibes check. wow this one feels smarter. back to doing the same three things i always do\n\n\n\nits like im collecting pokemon but the pokemon just sit there",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rc8xvy/spend_more_time_downloading_models_than_actually/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o6wmdak",
          "author": "peppaz",
          "text": "You can be like me and just keep testing dozens of model's performance on a benchmarking app you made and not doing anything with them because you pay $100 a month for Claude code\n\nhttps://github.com/uncSoft/anubis-oss",
          "score": 23,
          "created_utc": "2026-02-23 05:36:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x6iv1",
              "author": "ukSurreyGuy",
              "text": "split your benchmarking\n\nbefore downloading : plenty of ai benchmarks available testing xyz...use them \n\n\ndownload then test : find a really good benchmark one that test for whatever you do (want to do)...\n\nif something better appears : then move to new model else stay with original model\n\ndefinitely check out your GitHub anubis-oss : \nhttps://github.com/uncSoft/anubis-oss",
              "score": 5,
              "created_utc": "2026-02-23 08:38:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6wqjd2",
          "author": "Witty_Mycologist_995",
          "text": "r/DataHoarder",
          "score": 6,
          "created_utc": "2026-02-23 06:10:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x7vl1",
              "author": "Eznix86",
              "text": "It is more like ModelHoarder",
              "score": 4,
              "created_utc": "2026-02-23 08:52:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6x4t0j",
          "author": "jerrygreenest1",
          "text": "Since youâ€™re collecting them like PokÃ©monâ€™s then might as well run some benchmarks",
          "score": 2,
          "created_utc": "2026-02-23 08:21:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zu331",
          "author": "-PM_ME_UR_SECRETS-",
          "text": "Are there any models no longer available? It might actually be worth saving even if you donâ€™t use them.",
          "score": 2,
          "created_utc": "2026-02-23 18:29:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74m07a",
              "author": "CapitanM",
              "text": "Lots and lots",
              "score": 2,
              "created_utc": "2026-02-24 12:52:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o70mgp7",
          "author": "Longjumping_Fondant5",
          "text": "This is just distro-hopping with extra steps. Ten years ago it was \"maybe Arch is finally the one,\" now it's \"maybe Qwen 2.5 72b is finally the one.\" Same vibes, same outcome. You end up back on the thing that works and the new thing just sits there taking up disk space",
          "score": 2,
          "created_utc": "2026-02-23 20:42:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x8b8e",
          "author": "Ill-Bison-3941",
          "text": "I sympathize.",
          "score": 1,
          "created_utc": "2026-02-23 08:56:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y1nii",
          "author": "Super-Handle7395",
          "text": "Any I can dump photos to analyse? Vision can only take one photo at a time",
          "score": 1,
          "created_utc": "2026-02-23 13:08:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g2mcp",
              "author": "overand",
              "text": "Write a script to do it. (Or have the LLM help you write a script.)",
              "score": 1,
              "created_utc": "2026-02-26 02:41:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zq0dk",
          "author": "King0fFud",
          "text": "Do you actually have an end goal? I often feel like I donâ€™t so I test models and thatâ€™s as far as I go. I guess I keep hoping the OS models become efficient enough that I can avoid using paid services for coding tasks but my hardware ultimately holds me back.",
          "score": 1,
          "created_utc": "2026-02-23 18:11:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71k7yd",
          "author": "sunole123",
          "text": "it is spectacular. it detach you from your reality, solution looking for problem. start with your problem and work out you solution.",
          "score": 1,
          "created_utc": "2026-02-23 23:34:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74m5ho",
          "author": "CapitanM",
          "text": "That's part of the fun",
          "score": 1,
          "created_utc": "2026-02-24 12:53:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bm2rl",
          "author": "NoShoulder69",
          "text": "Maybe this would help u\n\nhttps://localops.tech",
          "score": 1,
          "created_utc": "2026-02-25 13:26:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zq6od",
          "author": "klawisnotwashed",
          "text": "stop using lowercase chatgpt to write reddit posts for you, or at least cite it at the bottom itâ€™s embarrassing in 2026",
          "score": 0,
          "created_utc": "2026-02-23 18:12:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71694h",
              "author": "Savantskie1",
              "text": "Yall just ainâ€™t happy unless it sounds as dumb as you are you?",
              "score": 3,
              "created_utc": "2026-02-23 22:19:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9zea9",
      "title": "SmarterRouter - A Smart LLM proxy for all your local models. (native Ollama support, loading/unloading models automatically)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r9zea9/smarterrouter_a_smart_llm_proxy_for_all_your/",
      "author": "peva3",
      "created_utc": "2026-02-20 16:06:53",
      "score": 59,
      "num_comments": 15,
      "upvote_ratio": 0.98,
      "text": "I've been working on this project to create a smarter LLM proxy primarily for my openwebui setup (but it's a standard openai compatible endpoint API, so it will work with anything that accepts that).\n\nThe idea is pretty simple, you see one frontend model in your system, but in the backend it can load whatever model is \"best\" for the prompt you send. When you first spin up Smarterrouter it profiles all your models, giving them scores for all the main types of prompts you could ask, as well as benchmark other things like model size, actual VRAM usage, etc. (you can even configure an external \"Judge\" AI to grade the responses the models give, i've found it improves the profile results, but it's optional). It will also detect and new or deleted models and start profiling them in the background, you don't need to do anything, just add your models to ollama and they will be added to SmarterRouter to be used.\n\nThere's a lot going on under the hood, but i've been putting it through it's paces and so far it's performing really well, It's extremely fast, It caches responses, and I'm seeing a negligible amount of time added to prompt response time. It will also automatically load and unload the models in Ollama (and any other backend that allows that).\n\nThe only caveat i've found is that currently it favors very small, high performing models, like Qwen coder 0.5B for example, but if small models are faster and they score really highly in the benchmarks... Is that really a bad response? I'm doing more digging, but so far it's working really well with all the test prompts i've given it to try (swapping to larger/different models for more complex questions or creative questions that are outside of the small models wheelhouse).\n\nHere's a high level summary of the biggest features:\n\n**Self-Correction via Hardware Profiling**: Instead of guessing performance, it runs a one-time benchmark on your specific GPU/CPU setup. It learns exactly how fast and capable your models are in your unique environment.\n\n**Active VRAM Guard**: It monitors nvidia-smi in real-time. If a model selection is about to trigger an Out-of-Memory (OOM) error, it proactively unloads idle models or chooses a smaller alternative to keep your system stable.\n\n**Semantic \"Smart\" Caching**: It doesn't just match exact text. It uses vector embeddings to recognize when youâ€™re asking a similar question to a previous one, serving the cached response instantly and saving your compute cycles.\n\n**The \"One Model\" Illusion**: It presents your entire collection of 20+ models as a single OpenAI-compatible endpoint. You just select SmarterRouter in your UI, and it handles the \"load, run, unload\" logic behind the scenes.\n\n**Intelligence-to-Task Routing**: It automatically analyzes your prompt's complexity. It won't waste your 70B model's time on a \"Hello,\" and it won't let a 0.5B model hallucinate its way through a complex Python refactor.\n\n**LLM-as-Judge Feedback**: It can use a high-end model (like a cloud GPT-4o or a local heavy-hitter) to periodically \"score\" the performance of your smaller models, constantly refining its own routing weights based on actual quality.\n\nGithub: https://github.com/peva3/SmarterRouter\n\nLet me know how this works for you, I have it running perfectly with a 4060 ti 16gb, so i'm positive that it will scale well to the massive systems some of y'all have.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r9zea9/smarterrouter_a_smart_llm_proxy_for_all_your/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o6gevzo",
          "author": "spookperson",
          "text": "This is an interesting project! I've been testing some related open-source projects recently. I posted this same comment over in a different subreddit but I'll repost here in case you haven't seen these projects and are potentially interested in considering other approaches (or combining code into your project): https://www.reddit.com/r/clawdbot/comments/1qzskfd/comment/o4du7f3/?\n\n\n\nOptions/inspiration:\n\n- LiteLLM has semantic routing integration now:Â https://docs.litellm.ai/docs/proxy/auto_routing\n- UIUC has an LLMRouter library with a ton of options:Â https://ulab-uiuc.github.io/LLMRouter/\n- Nvidia has an llm router blueprint on GitHub (the v1 (main branch) and the v2 (experimental branch) are pretty different in design, you might be interested in looking at both):Â https://build.nvidia.com/nvidia/llm-router",
          "score": 5,
          "created_utc": "2026-02-20 17:22:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gler9",
              "author": "peva3",
              "text": "That Clawdbot post is interesting and I'll be looking into it to see how they went about it (also kicking myself that I have such a similar name, I thought I did enough googling, doh), but overall, from the ground up I wanted my project to be local first, with the idea that people COULD use external models, but that the primary use case would be all totally local on the same machine or same local network. \n\nI see a future for local AI where you run your own entire stack and there's basically zero external model usage unless you want to enable that. This would allow something like openwebui to punch way above it's weight locally by being able to swap between all of the best models that you can fit in your GPU. \n\nSo the long long term idea is that this would be a sort of analog to how large AI companies are doing MoE prompt routing, but it would just live in your house instead of having to rely on anything outside your network.",
              "score": 2,
              "created_utc": "2026-02-20 17:52:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6hbj4z",
          "author": "AdditionalWeb107",
          "text": "You should look into preference-aware routing from [Plano](https://github.com/katanemo/plano). Used by the likes of HuggingFace https://x.com/ClementDelangue/status/1979256873669849195. ",
          "score": 2,
          "created_utc": "2026-02-20 19:53:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hh472",
              "author": "peva3",
              "text": "I really like how they have everything setup, but a core idea of my project is to not have to define anything. The system figures out what models you have and profiles them, even when you add new ones or delete ones. I want that sort of \"set and forget\" type of system.",
              "score": 1,
              "created_utc": "2026-02-20 20:20:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6jij9s",
          "author": "evilspyboy",
          "text": "I like the benchmarking. That is clever.",
          "score": 2,
          "created_utc": "2026-02-21 03:13:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jjdb1",
              "author": "peva3",
              "text": "Thank you! It's feeling pretty robust now, but if you see anything that needs to be fixed or tweaked let me know.",
              "score": 1,
              "created_utc": "2026-02-21 03:19:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gvq0r",
          "author": "Crafty_Ball_8285",
          "text": "How does this work for Mac without Nvidia",
          "score": 1,
          "created_utc": "2026-02-20 18:38:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gxkop",
              "author": "peva3",
              "text": "Great question, I think everything should work other than the VRAM stuff. Do you know what Mac uses command line to view VRAM usage? I could make a separate thing to enable just for Macs. Actually it might be better to find something that is OS/GPU agnostic to check the VRAM usage... I'll think on this some more.",
              "score": 1,
              "created_utc": "2026-02-20 18:47:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6tcvzt",
                  "author": "gregusmeus",
                  "text": "Yeah ARC Pro B50 user here, GPU agnostic would be great please!",
                  "score": 2,
                  "created_utc": "2026-02-22 18:27:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zvwfx",
          "author": "vcliment89",
          "text": "Could this be used to route to different subscriptions for ollama cloud? Or any alternative?",
          "score": 1,
          "created_utc": "2026-02-23 18:38:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zxdnz",
              "author": "peva3",
              "text": "In principle, you should be able to use any openai compatible API as a backend for Smarterrouter. But I haven't tested this extensively, so there might be bugs. I only was building/testing this with a local Ollama stack. \n\nIf you want to give it a shot please do and open a GitHub issue if you run into any issues.",
              "score": 1,
              "created_utc": "2026-02-23 18:44:49",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o72zt96",
              "author": "peva3",
              "text": "Hey! I was able to get external providers setup and it's included in the latest 2.1.3 version of SmarterRouter.",
              "score": 1,
              "created_utc": "2026-02-24 04:41:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o76gng6",
                  "author": "vcliment89",
                  "text": "in openrouter you pay api tokens. I want to use the Pro ollama subscription",
                  "score": 1,
                  "created_utc": "2026-02-24 18:16:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r8rgg8",
      "title": "I built a small CLI tool to help beginners see if their hardware can actually handle local LLMs",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r8rgg8/i_built_a_small_cli_tool_to_help_beginners_see_if/",
      "author": "Narrow-Detective9885",
      "created_utc": "2026-02-19 06:06:29",
      "score": 31,
      "num_comments": 13,
      "upvote_ratio": 0.95,
      "text": "Hey everyone,\n\nIâ€™ve been lurking here for a while and learning a ton from all the superusers and experts here. As a beginner myself, I often found it a bit overwhelming to figure out which models would actually run \"well\" on my specific machine versus just running \"slowly.\"\n\nTo help myself learn and to give something back to other newcomers, I put together a small CLI tool in Go called **RigRank**.\n\n**What it does:** Itâ€™s basically a simple benchmarking suite for Ollama. It doesnâ€™t measure how \"smart\" a model isâ€”there are way better tools for thatâ€”but it measures the \"snappiness\" of your actual hardware. It runs a few stages (code gen, summarization, reasoning, etc.) and gives you a \"Report Card\" with:\n\n* **TTFT (Time To First Token):** How long youâ€™re waiting for that first word.\n* **Writing Speed:** How fast it actually spits out text.\n* **Reading Speed:** How quickly it processes your prompts.\n\n**Who this is for:** Honestly, if you already have a complex benchmarking pipeline or a massive GPU cluster, this probably isn't for you. Itâ€™s designed for the person who just downloaded Ollama and wants to know: *\"Is Llama3-8B too heavy for my laptop, or is it just me?\"*\n\n**I would love your feedback**\n\n**Repo:** [https://github.com/rohanelukurthy/rig-rank](https://github.com/rohanelukurthy/rig-rank)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r8rgg8/i_built_a_small_cli_tool_to_help_beginners_see_if/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o67f8rn",
          "author": "joost00719",
          "text": "I run MiniMaxi-2.5 with like 10 tokens/sec (128gb ddr4 + 5070ti).\nIt's dogshit slow. But it's fine if you write a full spec document the Ai can use as a prompt and also a progress tracker. This enables it to not lose focus when the context window gets cleared.\n\nIt build a small weight tracking app in several hours without intervention. I know cloud based Ai can do it in minutes. But for a local machine I was quite impressed with the quality and that it was even possible to begin with.",
          "score": 5,
          "created_utc": "2026-02-19 07:38:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o683je7",
              "author": "Keensworth",
              "text": "A 5070ti is slow? Imagine my GTX 1660 Super",
              "score": 1,
              "created_utc": "2026-02-19 11:28:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o69ytgs",
                  "author": "joost00719",
                  "text": "It's just using the memory, and doesn't really compute much in my case. I offloaded the MoE to the CPU so it's not moving between GPU and System ram all the time, which increases system responsiveness. Not sure if it speeds up or slows down generation much, but probably not that much unless you have more vram.",
                  "score": 1,
                  "created_utc": "2026-02-19 17:42:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o678gkc",
          "author": "EverythingIsFnTaken",
          "text": "https://preview.redd.it/rcpjnw27dekg1.png?width=1918&format=png&auto=webp&s=60b2691eb7d65cd9d037b9ce2091ea4dc923293d",
          "score": 2,
          "created_utc": "2026-02-19 06:39:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6846vn",
          "author": "smcgann",
          "text": "I like this.  I have been working on a utility that pulls the system specs and then advises what software and configuration changes are needed before installing an LLM.  It also gives recommendations on model size.  Something like your tool would be great for post installation testing.  The only drawback for beginners is the prerequisite to having Go installed.  I doubt many beginners would have Go installed or need it.",
          "score": 1,
          "created_utc": "2026-02-19 11:33:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e1qom",
              "author": "Narrow-Detective9885",
              "text": "That's very cool. Looking forward to try your project. Thanks for the feedback, I added some CI and made the precompiled binaries available. ",
              "score": 2,
              "created_utc": "2026-02-20 08:32:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6c0p5x",
          "author": "LithiumToast",
          "text": "I'm very busy but I'll take a look this weekend. Anything that helps people start tinkering and using local offline models is a step in the right direction.",
          "score": 1,
          "created_utc": "2026-02-19 23:52:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c8a3v",
          "author": "Snoo_24581",
          "text": "Nice tool! Model compatibility checking is something beginners definitely need. Is it open source? Would love to check it out.",
          "score": 1,
          "created_utc": "2026-02-20 00:35:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6drq7w",
          "author": "superdav42",
          "text": "I ran into version problems with go:\n```\n$ go build -o rigrank\ngo: errors parsing go.mod:\n/home/dave/rig-rank/go.mod:3: invalid go version '1.25.3': must match format 1.23\n```\nIt doesn't seem worth the trouble to fix it. But is golang really necessary for this? Seems like a simple bash script could accomplish the same goal with much fewer dependencies.",
          "score": 1,
          "created_utc": "2026-02-20 06:59:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e1wnf",
              "author": "Narrow-Detective9885",
              "text": "Good feedback, Thank you ",
              "score": 1,
              "created_utc": "2026-02-20 08:34:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rcf94q",
      "title": "What GPU do you use?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/",
      "author": "Keensworth",
      "created_utc": "2026-02-23 11:43:10",
      "score": 31,
      "num_comments": 45,
      "upvote_ratio": 0.93,
      "text": "I've recently started using Ollama with an old GPU I had laying around.\n\nProblem is that my GTX 1660S only got 6Gb VRAM and you can't do much with that.\n\nI can run Mistral 7B Instruct but he sucks.\n\nWhat hardware are you using?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o6y7t3q",
          "author": "tecneeq",
          "text": "Two 6000 Blackwell with combined 192GB VRAM at work.\n\nA 5090 and a 5070 Ti 16GB at home.\n\nIf you want to buy one i recommend to get one or two 5060 16GB as long as they are still available.",
          "score": 9,
          "created_utc": "2026-02-23 13:45:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ypftz",
              "author": "Keensworth",
              "text": "Yeah, I don't think I'm ready to spend more than 600â‚¬ to have a LLM at home...",
              "score": 6,
              "created_utc": "2026-02-23 15:20:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7fhpsy",
                  "author": "nacholunchable",
                  "text": "Thats fair and where i was at whenÂ I upgraded from my 2060 to a used 3090 with pretty good results. Its 24gigs of vram and when i got it they were floating around 650-750ish usd for a gently used clean card. dont know the prices these days. Once youve got vram like that you open yourself up to image gen and video too if ur into that. Ive since moved on to a dgx spark and its wonderful. One word of advice, if you can drive the os off the weak card and reserve the beefy one for ai, assuming ur psu and ports can handle both, it stops inference from locking up ur normal computer use. Ymmv in windows, but i was able to make it happen in ubuntu while keeping the good card on the faster bus.",
                  "score": 1,
                  "created_utc": "2026-02-26 00:42:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76n6mf",
                  "author": "melanov85",
                  "text": "You don't need to. And honestly, local LLMs are limited. And you don't need GPU to run one either. I have some free apps on HF you can try. One of them is specifically for running models on low end hardware. Truth is, without fine-tuning small models to be what you need if to be. You get the results of what it's packed with. But if you want a local model to play with. Try my app. www.melanovproducts.com, follow the link to diget lite. Or Melanov85 on hugging face. Just download the app installer and give it go. Offline, local.",
                  "score": 0,
                  "created_utc": "2026-02-24 18:45:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o765irq",
              "author": "castinup",
              "text": "I have two 5060tis 16gb and they work great! Definitely would recommend",
              "score": 1,
              "created_utc": "2026-02-24 17:26:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y65cm",
          "author": "vir_db",
          "text": "RTX3090 24GB + RTX 2060 12 GB + 32GB RAM, I can run glm4.7-flash, qwen3-coder:30b, gemma3:27b-it-q8_0 or translategemma:27b-it-q8_0 with full context",
          "score": 9,
          "created_utc": "2026-02-23 13:35:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xrm83",
          "author": "janups",
          "text": "I got the nVidia GB10, qwen3-coder-next and ML tasks are running great.",
          "score": 6,
          "created_utc": "2026-02-23 11:56:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xwo1v",
          "author": "The_Splasi",
          "text": "I've got arc b580, can run the gpt-oss 20b with a tiny overflow to my CPU but it gets around 10t/s, same with glm4.7 flash but overflow is bigger there. Gemma3 models run just fine at 25>t/s if it's fully on my gpu, otherwise it's really slow.",
          "score": 6,
          "created_utc": "2026-02-23 12:35:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xt5uo",
          "author": "daisseur_",
          "text": "I got a Nvidia GTX 870ti ðŸ˜ƒ\nIt runs really smoothly with models like gemma3, llama3.2...",
          "score": 3,
          "created_utc": "2026-02-23 12:08:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xtf4k",
              "author": "Keensworth",
              "text": "Are they good? I was looking for a LLM general use case like gpt-oss:20B but with less VRAM",
              "score": 1,
              "created_utc": "2026-02-23 12:10:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6yzt1e",
                  "author": "gregusmeus",
                  "text": "Iâ€™m running that model on an Arc Pro B50. It works just about. Not sure itâ€™s snappy enough to be an agentic coding buddy for vibe coding though.",
                  "score": 1,
                  "created_utc": "2026-02-23 16:09:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o75dqqp",
                  "author": "tecneeq",
                  "text": "They are good for what they are. A large model will know more and makes for better conversation. Small or tiny models still have their uses, for example research, agentic systems administration, log parsing, email triage and so on.",
                  "score": 1,
                  "created_utc": "2026-02-24 15:20:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7053wb",
          "author": "SFsports87",
          "text": "Rtx 3060 12gb, can actually do a lot with MoE models\n\nUpgrade paths : 5060 ti 16gb, 3090 24gb (used), rtx 4000 pro blackwell 24gb, rtx 4500 pro blackwell 32gb\n\nPick which one fits your budget.",
          "score": 4,
          "created_utc": "2026-02-23 19:20:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y2vy6",
          "author": "tom-mart",
          "text": "I used to run my LLM on 6GB RTX A2000 so it wasn't really usable in any form other than experimenting. I added 24GB RTX 3090 and can run decent models now. It powers my AI Assistant and it works very well.",
          "score": 3,
          "created_utc": "2026-02-23 13:16:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yrost",
          "author": "Mikicrep",
          "text": "none",
          "score": 2,
          "created_utc": "2026-02-23 15:31:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o758wp2",
              "author": "-PM_ME_UR_SECRETS-",
              "text": "Cloud? Or Mini",
              "score": 1,
              "created_utc": "2026-02-24 14:57:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7590ix",
                  "author": "Mikicrep",
                  "text": "nope, literally just cpu",
                  "score": 1,
                  "created_utc": "2026-02-24 14:57:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yrxq6",
          "author": "PermanentLiminality",
          "text": "Currently 2x P40 that were about $300 for both.  Current model I've settled on is Qwen3-coder-next 80b in a q4.  I get 500 tk/s prompt and about 31 tk/s token generation. and I can do 60k context.",
          "score": 2,
          "created_utc": "2026-02-23 15:32:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79awat",
              "author": "m94301",
              "text": "That's quite fast.  What tool, ollama with llama.cpp?",
              "score": 1,
              "created_utc": "2026-02-25 02:48:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79fqlm",
                  "author": "PermanentLiminality",
                  "text": "Llama.cpp and a whole lot of llama+bench runs to find what works best with a bunch of models.",
                  "score": 1,
                  "created_utc": "2026-02-25 03:15:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zg5uv",
          "author": "x8code",
          "text": "RTX 5080 + RTX 5060 Ti 16 GB let's me run slightly larger models like GLM 4.7 Flash or NVIDIA Nemotron 3 Nano.",
          "score": 2,
          "created_utc": "2026-02-23 17:25:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71tly5",
          "author": "BringOutYaThrowaway",
          "text": "Iâ€™ve relegated a 3090 to my Ollama / OpenWebUI server, and use a 5090 for LM Studio and Cyberpunk.",
          "score": 2,
          "created_utc": "2026-02-24 00:26:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73b3ci",
          "author": "vivus-ignis",
          "text": "P100 16Gb + V100 32Gb",
          "score": 2,
          "created_utc": "2026-02-24 06:07:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73wvqk",
          "author": "j0x7be",
          "text": "I'm running on a dual 1080Ti setup (22GB vRAM, 32GB RAM, i7 7700).",
          "score": 2,
          "created_utc": "2026-02-24 09:26:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bbkvp",
          "author": "Zyj",
          "text": "3dfx voodoo",
          "score": 2,
          "created_utc": "2026-02-25 12:20:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yxi7q",
          "author": "Responsible-Stock462",
          "text": "RTX5060TI X2 can run 80b models in 4q with little CPU Off-loading.",
          "score": 1,
          "created_utc": "2026-02-23 15:58:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yy9n5",
          "author": "Frogy_mcfrogyface",
          "text": "I was running an rx6800 16gb for local LLMs. Ran them great.Â ",
          "score": 1,
          "created_utc": "2026-02-23 16:02:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ejqm",
          "author": "FlyByPC",
          "text": "RTX 4070 12GB. \n\nI think I'm the target audience for gpt-oss-20b, which is my usual go-to. I can run up to Qwen3:235b (128GB system RAM), but it's sloooow.",
          "score": 1,
          "created_utc": "2026-02-23 20:04:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75f771",
              "author": "tecneeq",
              "text": "I plan to test which parts of a model can be offloaded to CPU without having too much of an impact.\n\nAlso using flash attention and quantizing the kv-caches can speed up things. I have made perplexity benchmarks with different kv-quants and didn't find much value in anything over Q8. The default in Ollama i think is F16, so you save half the ram or you can use double the context.",
              "score": 2,
              "created_utc": "2026-02-24 15:27:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o70p4hx",
          "author": "Longjumping_Fondant5",
          "text": "Not sure if you have a Mac lying around but if you've got anything M1 or newer with 16GB, it'll actually do more useful work than your 1660S for local LLMs. Unified memory means all 16GB is available for model loading, so you can run Gemma3 12B, Qwen3 14B, stuff that literally won't fit in 6GB VRAM.\n\nThe tradeoff is tok/s",
          "score": 1,
          "created_utc": "2026-02-23 20:55:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71b6oa",
          "author": "otosan69",
          "text": "Rtx4600 ti with 16gb ram",
          "score": 1,
          "created_utc": "2026-02-23 22:44:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71cjse",
          "author": "CryptographerLow6360",
          "text": "i use glm-4.7-flast:latest on a 4070 ti super and i find speed means nothing if your agent is working 24/7 on it. builds everything i asked for, just takes some time and im not trying to make money with it, just my own personal localclaw agent (with a robot body no less)",
          "score": 1,
          "created_utc": "2026-02-23 22:51:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73o96y",
          "author": "stonelox",
          "text": "4x 2080Ti",
          "score": 1,
          "created_utc": "2026-02-24 08:03:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73v68r",
          "author": "Western_Courage_6563",
          "text": "Tesla p40",
          "score": 1,
          "created_utc": "2026-02-24 09:09:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73vbuq",
          "author": "congard",
          "text": "RX 7900XTX + 64gb of RAM. I'm able to run Qwen3 Coder Next at 30-40t/s",
          "score": 1,
          "created_utc": "2026-02-24 09:11:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o779dxd",
              "author": "Own-Concentrate2128",
              "text": "Me too. Good value for money (besides the ram now) and descent speed to work with.",
              "score": 1,
              "created_utc": "2026-02-24 20:26:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o73z5tp",
          "author": "NoobMLDude",
          "text": "Not a GPU, \nAn Apple Mac M2 Max",
          "score": 1,
          "created_utc": "2026-02-24 09:48:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74087q",
          "author": "Thin-Bit-876",
          "text": "I donâ€™t use a single GPU type but rather select the one suited for the model I want to run. I generally go to https://advisor.forwardcompute.ai that does a pretty good job at mapping models from hugginface to the GPU that have the right spec depending on my use case",
          "score": 1,
          "created_utc": "2026-02-24 09:58:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74cema",
          "author": "Odd_Butterfly_455",
          "text": "B580 with qwen3-vl-8b for ocr and multi purpose ai like traduction",
          "score": 1,
          "created_utc": "2026-02-24 11:44:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74ifzl",
          "author": "Comfortable_Ad_8117",
          "text": "I had two 3060 (12GB) and then recently swapped on for a 5060 (16GB) - So my Ai â€œserverâ€ is a 12GB 3060 and a 16GB 5060 - Runs very good can comfortable run 30b models with reasonable performance",
          "score": 1,
          "created_utc": "2026-02-24 12:28:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79apxl",
          "author": "m94301",
          "text": "P40 is a cheap old card with 24gb vram.  If you just want to dabble, you can get 10-20t/s on 10-20b models",
          "score": 1,
          "created_utc": "2026-02-25 02:47:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7euowq",
          "author": "Express_Quail_1493",
          "text": "The new ministral3-3-reasoning is great generalist at your current hardware given him a try. But to get really good intelligence you need at least 16gbVram minimum.",
          "score": 1,
          "created_utc": "2026-02-25 22:38:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra9wsv",
      "title": "my portable ollama now has persistent memory",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/gallery/1ra9wsv",
      "author": "VaguneBob",
      "created_utc": "2026-02-20 22:39:02",
      "score": 23,
      "num_comments": 1,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1ra9wsv/my_portable_ollama_now_has_persistent_memory/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6m78z4",
          "author": "Swimming_Ad_5205",
          "text": "ÐŸÐ¸ÑˆÐµÑ‚ÑÑ Ð² Ñ„Ð°Ð¹Ð»? )))) Ð¾Ð½Ð¾ ÐµÐ³Ð¾ Ñ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ Ð¿Ð¾Ñ‚Ð¾Ð¼ ÑƒÑÑ‚Ð°Ð½ÐµÑ‚ Ð¸ Ð±ÑƒÐ´ÐµÑ‚ ÑÐ±Ð¸Ð²Ð°Ñ‚ÑŒÑÑ",
          "score": 0,
          "created_utc": "2026-02-21 15:49:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd920r",
      "title": "Built an app that connects Ollama to your clipboard with âŒ¥C (macOS, open source)",
      "subreddit": "ollama",
      "url": "https://v.redd.it/hhklml241elg1",
      "author": "morning-cereals",
      "created_utc": "2026-02-24 06:44:32",
      "score": 17,
      "num_comments": 4,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rd920r/built_an_app_that_connects_ollama_to_your/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o73xuwm",
          "author": "bukaro",
          "text": "Looks very cool, definitly I will give it a try .. now :-)",
          "score": 1,
          "created_utc": "2026-02-24 09:35:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7445o3",
              "author": "morning-cereals",
              "text": "Exciting! Let me know how smooth the onboarding is and if you have any feedback/ideas :)",
              "score": 1,
              "created_utc": "2026-02-24 10:34:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7c33rm",
          "author": "VictorFoxSub",
          "text": "I'm playing with it, it's so cool !\n\nTo be honest I was going to do something similar with bash scripts and zenity so you saved me some time for a better result.\n\nI suggest to add the option to change the keyboard shortcut. The current one conflicts with some of my shortcuts (which I could change) and some app produce Â© (like intellij terminal or firefox) when typing option+c (which you cannot change). I find this app so usefull that I intend to dedicate an key just for that.",
          "score": 1,
          "created_utc": "2026-02-25 14:57:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cbazr",
              "author": "morning-cereals",
              "text": "Great feedback, makes a lot of sense! I'll include the option to change the shortcut via the settings menu in the next release :)",
              "score": 1,
              "created_utc": "2026-02-25 15:36:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1reeixj",
      "title": "From Pikachu to ZYRON: We Built a Fully Local AI Desktop Assistant That Runs Completely Offline",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1reeixj/from_pikachu_to_zyron_we_built_a_fully_local_ai/",
      "author": "No-Mess-8224",
      "created_utc": "2026-02-25 14:22:10",
      "score": 13,
      "num_comments": 0,
      "upvote_ratio": 0.84,
      "text": "A few months ago I posted here about a small personal project I was building called Pikachu, a local desktop voice assistant. Since then the project has grown way bigger than I expected, got contributions from some really talented people, and evolved into something much more serious. We renamed it to ZYRON and it has basically turned into a full local AI desktop assistant that runs entirely on your own machine.\n\nThe main goal has always been simple. I love the idea of AI assistants, but I hate the idea of my files, voice, screenshots, and daily computer activity being uploaded to cloud services. So we built the opposite. ZYRON runs fully offline using a local LLM through Ollama, and the entire system is designed around privacy first. Nothing gets sent anywhere unless I explicitly ask it to send something to my own Telegram.\n\nYou can control the PC with voice by saying a wake word and then speaking normally. It can open apps, control media, set volume, take screenshots, shut down the PC, search the web in the background, and run chained commands like opening a browser and searching something in one go. It also responds back using offline text to speech, which makes it feel surprisingly natural to use day to day.\n\nThe remote control side became one of the most interesting parts. From my phone I can message a Telegram bot and basically control my laptop from anywhere. If I forget a file, I can ask it to find the document I opened earlier and it sends the file directly to me. It keeps a 30 day history of file activity and lets me search it using natural language. That feature alone has already saved me multiple times.\n\nWe also leaned heavily into security and monitoring. ZYRON can silently capture screenshots, take webcam photos, record short audio clips, and send them to Telegram. If a laptop gets stolen and connects to the internet, it can report IP address, ISP, city, coordinates, and a Google Maps link. Building and testing that part honestly felt surreal the first time it worked.\n\nOn the productivity side it turned into a full system monitor. It can report CPU, RAM, battery, storage, running apps, and even read all open browser tabs. There is a clipboard history logger so copied text is never lost. There is a focus mode that kills distracting apps and closes blocked websites automatically. There is even a â€œzombie processâ€ monitor that detects apps eating RAM in the background and lets you kill them remotely.\n\nOne feature I personally love is the stealth research mode. There is a Firefox extension that creates a bridge between the browser and the assistant, so it can quietly open a background tab, read content, and close it without any window appearing. Asking random questions and getting answers from a laptop that looks idle is strangely satisfying.\n\nThe whole philosophy of the project is that it does not try to compete with giant cloud models at writing essays. Instead it focuses on being a powerful local system automation assistant that respects privacy. The local model is smaller, but for controlling a computer it is more than enough, and the tradeoff feels worth it.\n\nWe are planning a lot next. Linux and macOS support, geofence alerts, motion triggered camera capture, scheduling and automation, longer memory, and eventually a proper mobile companion app instead of Telegram. As local models improve, the assistant will naturally get smarter too.\n\nThis started as a weekend experiment and slowly turned into something I now use daily. I would genuinely love feedback, ideas, or criticism from people here. If you have ever wanted an AI assistant that lives only on your own machine, I think you might find this interesting.\n\nGitHub Repo -Â [Link](https://github.com/Surajkumar5050/zyron-assistant)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1reeixj/from_pikachu_to_zyron_we_built_a_fully_local_ai/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rd8cu5",
      "title": "Built an open-source Ollama/MLX/OpenAI benchmark and leaderboard site with in-app submissions. Trying to test and collect more data.",
      "subreddit": "ollama",
      "url": "https://i.redd.it/tcn61r39rdlg1.png",
      "author": "peppaz",
      "created_utc": "2026-02-24 06:07:36",
      "score": 12,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rd8cu5/built_an_opensource_ollamamlxopenai_benchmark_and/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o73b5w0",
          "author": "peppaz",
          "text": "Apple silicon only for now - other platform forks in progress.\n\n[Homepage](https://devpadapp.com/anubis-oss.html) \n\n[Leaderboard Page](https://devpadapp.com/leaderboard.html)\n\n[Github](https://github.com/uncSoft/anubis-oss)\n\n[Latest dev cert signed release](https://github.com/uncSoft/anubis-oss/releases/latest) \n\n[It generates exportable reports as well](https://imgur.com/a/sBj2xWR)\n\nI designed Anubis, a native macOS app for benchmarking, comparing, and managing local large language models using any OpenAI-compatible endpoint - Ollama, MLX, LM Studio Server, OpenWebUI, Docker Models, etc. Built with SwiftUI for Apple Silicon, it provides real-time hardware telemetry correlated with full, history-saved inference performance - something no CLI tool or chat wrapper offers. Export benchmarks directly without having to screenshot, and export the raw data as .MD or .CSV from the history. You can even OLLAMA PULL models directly within the app. \n\nI am trying to get to 75 stars so I can submit to homebrew as a Cask. Check it out and I'd love some feedback! You can even choose the actual process to track memory use when running models, some model runners spawn child node processes that may not get auto-detected.",
          "score": 1,
          "created_utc": "2026-02-24 06:07:48",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1re6pgx",
      "title": "qwen3.5:35b-a3b is here.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1re6pgx/qwen3535ba3b_is_here/",
      "author": "Space__Whiskey",
      "created_utc": "2026-02-25 07:26:55",
      "score": 12,
      "num_comments": 12,
      "upvote_ratio": 0.8,
      "text": "yey. that is all.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1re6pgx/qwen3535ba3b_is_here/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o7agzqo",
          "author": "geek_at",
          "text": "guess I have to wait for the patch :(\n\n`ollama     | llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'qwen35moe'`",
          "score": 6,
          "created_utc": "2026-02-25 07:51:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ai788",
              "author": "Space__Whiskey",
              "text": "use [https://github.com/ollama/ollama/releases/tag/v0.17.1-rc1](https://github.com/ollama/ollama/releases/tag/v0.17.1-rc1) it works",
              "score": 6,
              "created_utc": "2026-02-25 08:02:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bo8pv",
                  "author": "WaitformeBumblebee",
                  "text": "I've tried bazobehram/qwen3.5-flash-27b:latest with 17.1-rc1 and get this error:\nError: 500 Internal Server Error: unable to load model: /usr/share/ollama/.ollama/models/blobs/sha256-d4d089fbfa2a2ef034faa5c99a1743523ce69a18c562f7de09007a07ca07d4af\n\nI pulled the model and the 16GB file is there",
                  "score": 2,
                  "created_utc": "2026-02-25 13:39:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7akgqb",
                  "author": "geek_at",
                  "text": "nice thanks that worked. Tried it yesterday with 0.17.1-rc0 and it didn't work",
                  "score": 2,
                  "created_utc": "2026-02-25 08:24:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bfird",
          "author": "Dui999",
          "text": "The 27b model GGUF still does not work on the latest release :(",
          "score": 1,
          "created_utc": "2026-02-25 12:47:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bim6e",
              "author": "MrMrsPotts",
              "text": "Did you try rc1?",
              "score": 1,
              "created_utc": "2026-02-25 13:06:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bndfq",
                  "author": "Dui999",
                  "text": "Yes, very odd.\n\nBut I guess it's related to the fact that they still didn't release this model on their website as they did with the MoE ones.",
                  "score": 1,
                  "created_utc": "2026-02-25 13:34:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7cfg9e",
          "author": "zelkovamoon",
          "text": "For those that currently have it running\n\n1. Is tps basically on par with what you'd expect?\n\n2. Is tool calling working?",
          "score": 1,
          "created_utc": "2026-02-25 15:55:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7d0vc6",
              "author": "InternationalNebula7",
              "text": "Compared to GLM 4.7 flash on the same hardware  \n\\- TPS is lower  \n\\- Tool calling (particularly web-search) seems to be worse",
              "score": 2,
              "created_utc": "2026-02-25 17:33:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7d1k2r",
                  "author": "zelkovamoon",
                  "text": "You've got flash working in ollama? It still basically doesn't function for me - are you using the library version?",
                  "score": 1,
                  "created_utc": "2026-02-25 17:36:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rc4l9j",
      "title": "16GB VRAM for mode agent",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rc4l9j/16gb_vram_for_mode_agent/",
      "author": "ColdTransition5828",
      "created_utc": "2026-02-23 01:56:50",
      "score": 11,
      "num_comments": 12,
      "upvote_ratio": 0.87,
      "text": "I was hoping to enjoy something similar to Cursor on my PC. I even bought what was supposed to be a mid-range card. But the results are disappointing.\n\n\n\nAfter studying it, I realized I'm missing the core agent and a better Ollama model that accepts tools. But honestly, I'm bored. What do you recommend I do to get the most out of local models with my 16GB of VRAM?\n\n\n\nI mostly do full-track coding and Java.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rc4l9j/16gb_vram_for_mode_agent/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o6vvnm5",
          "author": "scooter_de",
          "text": "I just spent most of the weekend finding the answer to this very question. Iâ€™m running on a RTX 5080 + 128g RAM on windows 11 with Ollama. Iâ€™m trying to get Claude Code up and running usefully. The best model so far was qwen3:14b. I usually run it with 64k context size. That should fit completely on the GPU (ollama ps). Itâ€™s way slower than the big boys but it works.\n\nI also tried a downquantized version of qwen3-coder:30b which can be pulled with\n\n    ollama pull hf.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:UD-IQ3_XXS",
          "score": 8,
          "created_utc": "2026-02-23 02:32:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72qb9h",
              "author": "mickexd",
              "text": "I'm with on this one, just tried every model and Qwen3: 14b has been working wonderfully also GPT OSS a bit slower but as accurate, I ended up combining Qwen3 14b with a RAG and skills (skills.sh) and a few MCP servers.\n\nWorks like a charm",
              "score": 0,
              "created_utc": "2026-02-24 03:37:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6x56ju",
          "author": "InfraScaler",
          "text": "16GB VRAM is not mid-range inside the GPU world. Mid-range I'd say starts with 2x 3090 24GB. You are severely restrained by VRAM and I'd guess that card is not a XX90 either...",
          "score": 7,
          "created_utc": "2026-02-23 08:25:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70x6by",
              "author": "ColdTransition5828",
              "text": "Ryzen 5 7600x + Antec A30 NEO \n\n\n\nBoardb650e wifi asus tuf \n\n\n\nRAM: 16gb 5600mhz\n\n\n\nRTX 5060 ti de 16gb",
              "score": 1,
              "created_utc": "2026-02-23 21:35:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o70yfc6",
                  "author": "InfraScaler",
                  "text": "Yeah that's low end inside GPU world. Anything below that (like iGPUs) it's just not even considered. You may be able to run qwen2.5 quantised or similar and that may work good-ish for you.",
                  "score": 3,
                  "created_utc": "2026-02-23 21:41:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yixsy",
          "author": "blackhawk00001",
          "text": "How much RAM do you have?  Iâ€™m using qwen3-coder next q4_k_m on my 64GB 5080 desktop and it seems decent compared to the q8 Iâ€™m running on another machine.  250-300 prompt and 40t/s response hosted on locally compiled llama.cpp server. 200000 context.\n\nIâ€™ve been using it for python, c, c++, java, and node/angular so far.\n\nItâ€™s not perfect and requires merging fixes from another llamacpp branch and providing the chat template at server start, but itâ€™s useful over iterations.",
          "score": 2,
          "created_utc": "2026-02-23 14:47:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ykl2c",
              "author": "mzinz",
              "text": "How do you measure t/s?\n\nAlso, is that model way too big for the 5080?",
              "score": 1,
              "created_utc": "2026-02-23 14:55:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6yl5e6",
                  "author": "blackhawk00001",
                  "text": "Llama.cpp logs the response times as it stores checkpoints.  \n\nItâ€™s a 49Gb file size so 15.5 GB sits on the 5080 and the rest goes on system ram.  Iâ€™m sitting around 48-50gb ram used when loaded up between the server, windows, and whatever else.",
                  "score": 2,
                  "created_utc": "2026-02-23 14:58:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o73tiyu",
          "author": "lum4chi",
          "text": "I'm also equipped with 16GB VRAM. The best tradeoff I've obtained so far is unsloth/ glm 4.7 flash quantized to IQ2__\nXXS, suggested param for tool calling and 64k context.\nNo benchmark run so far but it get \"some\" work done with opencode and vscode+continue. Give it a try!",
          "score": 2,
          "created_utc": "2026-02-24 08:53:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zsse5",
          "author": "klawisnotwashed",
          "text": "[ Removed by Reddit ]",
          "score": 1,
          "created_utc": "2026-02-23 18:24:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70xgck",
          "author": "ColdTransition5828",
          "text": "PC:\n\nRyzen 5 7600x + Antec A30 NEO \n\n\n\nBoardb650e wifi asus tuf \n\n\n\nRAM: 16gb 5600mhz\n\n\n\nRTX 5060 ti de 16gb",
          "score": 1,
          "created_utc": "2026-02-23 21:36:43",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o71hxh9",
          "author": "Mysterious-String420",
          "text": "The context window will still cripple you if you try to do something more complicated than the crappy \"checklist apps\" the YouTubers flaunt.\n\nI have 16gb VRAM on my 5060ti and 64gb of RAM. It's really good for image and video models, I'm enjoying the current gen hype toys, but oooooh boy are language models a different BEAST! For LLM ? Mid-range gaming PCs like mine are just inadequate.\n\nMy current setup allows me to barely skirt at 64k context window. That is a very low number if you intend to give your coder agent a project with any kind of scope. And it's slow as hell.\n\nYou wanna dip your toes in agent coding ?\n\nTry the free vscode/cursor/whatever IDE extensions like roo code and kilo code, they give you access to free models with industrial-capacity RAM so the agent won't forget your code halfway while writing it.\n\nOr just buy some 10000$ Uber-machine with 128gb VRAM, whichever is simplest!",
          "score": 1,
          "created_utc": "2026-02-23 23:21:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd5ely",
      "title": "Full GPU Acceleration for Ollama on Mac Pro 2013 (Dual FirePro D700) - Linux",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rd5ely/full_gpu_acceleration_for_ollama_on_mac_pro_2013/",
      "author": "manu7irl",
      "created_utc": "2026-02-24 04:23:21",
      "score": 10,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "\\# \\[Guide\\] Full GPU Acceleration for Ollama on Mac Pro 2013 (Dual FirePro D700) - Linux\n\n\n\nHey everyone! I finally managed to get full GPU acceleration working for \\*\\*Ollama\\*\\* on the legendary \\*\\*Mac Pro 6.1 (2013 \"Trashcan\")\\*\\* running Nobara Linux (and it should work on other distros too).\n\n\n\nThe problem with these machines is that they have dual \\*\\*AMD FirePro D700s (Tahiti XT)\\*\\*. By default, Linux uses the legacy \\`radeon\\` driver for these cards. While \\`radeon\\` works for display, it \\*\\*does not support Vulkan or ROCm\\*\\*, meaning Ollama defaults to the CPU, which is slow as molasses.\n\n\n\n\\### My Setup:\n\n\\- \\*\\*Model:\\*\\* Mac Pro 6,1 (Late 2013)\n\n\\- \\*\\*CPU:\\*\\* Xeon E5-1680 v2 (8C/16T @ 3.0 GHz)\n\n\\- \\*\\*RAM:\\*\\* 32GB\n\n\\- \\*\\*GPU:\\*\\* Dual AMD FirePro D700 (6GB each, 12GB total VRAM)\n\n\\- \\*\\*OS:\\*\\* Nobara Linux (Fedora 40/41 base)\n\n\n\n\\### The Solution:\n\nWe need to force the \\`amdgpu\\` driver for the Southern Islands (SI) architecture. Once \\`amdgpu\\` is active, Vulkan is enabled, and Ollama picks up both GPUs automatically!\n\n\n\n\\### Performance (The Proof):\n\nI'm currently testing \\*\\*\\`qwen2.5-coder:14b\\`\\*\\* (9GB model). \n\n\\- \\*\\*GPU Offload:\\*\\* 100% (49/49 layers)\n\n\\- \\*\\*VRAM Split:\\*\\* Perfectly balanced across both D700s (\\~4GB each)\n\n\\- \\*\\*Speed:\\*\\* \\*\\*\\~11.5 tokens/second\\*\\* ðŸš€\n\n\\- \\*\\*Total Response Time:\\*\\* \\~13.8 seconds for a standard coding prompt.\n\n\n\nOn CPU alone, this model was barely usable at <2 tokens/sec. This fix makes the Trashcan a viable local LLM workstation in 2026!\n\n\n\n\\### How to do it:\n\n\n\n\\*\\*1. Update Kernel Parameters\\*\\*\n\nAdd these to your GRUB configuration:\n\n\\`radeon.si\\_support=0 amdgpu.si\\_support=1\\`\n\n\n\nOn Fedora/Nobara:\n\n\\`\\`\\`bash\n\nsudo sed -i 's/GRUB\\_CMDLINE\\_LINUX\\_DEFAULT=\"/GRUB\\_CMDLINE\\_LINUX\\_DEFAULT=\"radeon.si\\_support=0 amdgpu.si\\_support=1 /' /etc/default/grub\n\nsudo grub2-mkconfig -o /boot/grub2/grub.cfg\n\n\\`\\`\\`\n\n\n\n\\*\\*2. Reboot\\*\\*\n\n\\`sudo reboot\\`\n\n\n\n\\*\\*3. Container Config (Crucial!)\\*\\*\n\nIf you're running Ollama in a container (Podman or Docker), you MUST:\n\n\\- Pass \\`/dev/dri\\` to the container.\n\n\\- Set \\`OLLAMA\\_VULKAN=1\\`.\n\n\\- Disable security labels (SecurityLabel=disable in Quadlet).\n\n\n\n\\*\\*Result:\\*\\*\n\nMy D700s are now identified as \\*\\*Vulkan0\\*\\* and \\*\\*Vulkan1\\*\\* in Ollama logs, and they split the model VRAM perfectly! ðŸš€\n\n\n\nI've put together a GitHub-ready folder with scripts and configs here: \\[Link to your repo\\]\n\n\n\nHope this helps any fellow Trashcan owners out there trying to run local LLMs!\n\n\n\n\\#MacPro #Linux #Ollama #SelfHosted #AMD #FireProD700",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rd5ely/full_gpu_acceleration_for_ollama_on_mac_pro_2013/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o73bisv",
          "author": "BringOutYaThrowaway",
          "text": "That sounds, frankly, incredible. If I were to even attempt that, I would not use Doctor. Just run Ollama natively. \n\nWhy? Because Docker splits memory use between system managed, and the Docker managed, right?\n\nYou think you could just run Ollama natively?",
          "score": 2,
          "created_utc": "2026-02-24 06:10:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73h2et",
              "author": "manu7irl",
              "text": "I am sure it is possible, I am not aware of any split in memory so far.\nIf you can show what you mean by that I'll be happy to check that.\nIf natively running will boost that performance, I will try it.",
              "score": 1,
              "created_utc": "2026-02-24 06:58:15",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7aa04k",
              "author": "manu7irl",
              "text": "https://preview.redd.it/sy8ew2uw7llg1.png?width=1600&format=png&auto=webp&s=62405a5d300854974c221e886cc923bf3798a476\n\n  \nhere it is on native ollama my friend!",
              "score": 1,
              "created_utc": "2026-02-25 06:49:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7az1ne",
                  "author": "BringOutYaThrowaway",
                  "text": "Do you see any performance difference running native vs. Docker?\n\nAlso, do you HAVE to run it on Fedora / Nobara, or could you use Ubuntu / Debian?\n\nI also run Ollama / OpenWebUI on a Linux VM, but I'm using a 3090 via PCIe passthrough on Proxmox.  I might have to pull out my trash can to try this - my specs are similar to yours (3.0 Ghz 8-core CPU, 64GB, dual D700s).",
                  "score": 1,
                  "created_utc": "2026-02-25 10:39:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o79n005",
          "author": "t00r99r00t",
          "text": "i tried to get this to work on MacOS with a pair of D500's.  It compiled and ran with a qwen 7b model, but it segfaulted. Close but no cigar for me :/",
          "score": 1,
          "created_utc": "2026-02-25 03:59:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a99tp",
              "author": "manu7irl",
              "text": "Do you run it over linux or mac os?",
              "score": 1,
              "created_utc": "2026-02-25 06:43:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7aa3ls",
                  "author": "manu7irl",
                  "text": "https://preview.redd.it/fu16ob8d8llg1.png?width=1600&format=png&auto=webp&s=7a27ac8f6765e665aed39f14b2d43f0e7cee5ed9\n\n",
                  "score": 1,
                  "created_utc": "2026-02-25 06:50:20",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o7cx1jc",
                  "author": "t00r99r00t",
                  "text": "macos. i just realized im using the wrong version. i need to downgrade. ill be back and share my update after i downgrade my os.",
                  "score": 1,
                  "created_utc": "2026-02-25 17:16:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bccig",
          "author": "manu7irl",
          "text": "I can try to benchmark it, but I got less than a minute replies, around 30-40seconds top on qwen2.5-coder",
          "score": 1,
          "created_utc": "2026-02-25 12:26:00",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1rarsuo",
      "title": "TIFU/PSA: didnâ€™t check which GPU ollama was using and was stuck wondering why so slow",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rarsuo/tifupsa_didnt_check_which_gpu_ollama_was_using/",
      "author": "IAmANobodyAMA",
      "created_utc": "2026-02-21 13:59:43",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 0.91,
      "text": "Not sure where to tell this story (megathread?) but it made me laugh and has a teachable moment so I thought I would share.\n\nTL;DR: I was running ollama on an old GPU by mistake ðŸ¤¦â€â™‚ï¸\n\nI have two local machines running ollama. My gaming rig has a 5070ti 16gb but isnâ€™t always on, and my â€œdedicatedâ€ unraid server \\*\\*had\\*\\* a 3060ti 8gb that was going to be my AI workhorse.\n\nI chose models that would kick ass on the 5070 when online and more conservative models for the 3060 otherwise.\n\nThis is all still experimental/for learning so this setup is fines for me â€¦ except the unraid server was painfully slow. Took me way too long to figure out thatâ€™s because it was hitting an old GTX 1650 4gb card!! I forgot I swapped out the cards because I was going to build a gaming rig for my kid with the 3060.\n\nI spent way too long researching models and trying to figure out why my â€œ3060â€ was offloading over 50% of qwen3:4b to my CPU. Since this is hosted on unraid I was convinced that another service (plex?) was using my GPU without permission. Nope, Iâ€™m just a doofus.\n\nIt wasnâ€™t until running \\`nvidia-smi\\` in terminal that I realized my error.\n\nAnyways, hope this makes someone chuckle as much as me. Anyone else have some fun â€œdohâ€ moments?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rarsuo/tifupsa_didnt_check_which_gpu_ollama_was_using/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rdzgyb",
      "title": "AI toolkit â€” LiteLLM + n8n + Open WebUI in one Docker Compose",
      "subreddit": "ollama",
      "url": "https://github.com/wa91h/local-ai-toolkit",
      "author": "Puzzleheaded-Dig-492",
      "created_utc": "2026-02-25 01:30:39",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rdzgyb/ai_toolkit_litellm_n8n_open_webui_in_one_docker/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1rdl1fd",
      "title": "What's the best model to run on mac m1 pro 16gb?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rdl1fd/whats_the_best_model_to_run_on_mac_m1_pro_16gb/",
      "author": "Embarrassed-Baby3964",
      "created_utc": "2026-02-24 16:33:37",
      "score": 9,
      "num_comments": 15,
      "upvote_ratio": 0.74,
      "text": "I have an old m1 mac pro with 16gb ram. Was wondering if there are any good performing models in 2026 that I can run on this hardware? And if so, what is the best one in your opinion?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rdl1fd/whats_the_best_model_to_run_on_mac_m1_pro_16gb/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o76ag8d",
          "author": "NoobMLDude",
          "text": "Asking for a â€œBest Modelâ€ is like asking for the â€œBest Foodâ€. It all depends on what you wish to achieve . \n\nDo you need it for coding, general chat, image creation, meeting summaries, etc?\n\nThere are model sizes available from 300 million (runs on web site) to Trillion parameters. 7B size is good for your device. But which 7B model depends on what you want to do with the model.",
          "score": 8,
          "created_utc": "2026-02-24 17:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o774oca",
              "author": "misha1350",
              "text": "Thanks Qwen",
              "score": 3,
              "created_utc": "2026-02-24 20:04:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75xkj0",
          "author": "Mindless-Direction60",
          "text": "Pretty much any 7B model, personally I like Qwen 2.5 7B but thereâ€™s lots of good options.",
          "score": 7,
          "created_utc": "2026-02-24 16:50:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a0due",
              "author": "IngloriousBastrd7908",
              "text": "If I may ask:\nWhy Qwen 2.5 7B if there is Qwen3 already?",
              "score": 1,
              "created_utc": "2026-02-25 05:31:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o77f74h",
          "author": "st0ut717",
          "text": "Install lmstudio.   \nIt will give you compatible models for your machine.",
          "score": 7,
          "created_utc": "2026-02-24 20:53:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76y4ze",
          "author": "mirlan_irokez",
          "text": "I used ministral-3:8B, mistral:7b, gemma3 on M1 / 16gb / MacbookAir.  \ndeepsek and qwen models stuck in reasoning :)   \nI moved to M4 / 16gb, a little bit better, but didn't try qwen, I assume 16Gb is not enough for reasoning models ",
          "score": 3,
          "created_utc": "2026-02-24 19:34:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o774fnq",
          "author": "misha1350",
          "text": "Try out GLM 4.6v Flash (9B model) at 4-bit MLX. Qwen2.5 7B is extremely old and outdated, so is Gemma 3.",
          "score": 2,
          "created_utc": "2026-02-24 20:03:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77shp0",
              "author": "klawisnotwashed",
              "text": "thanks, will be trying this",
              "score": 1,
              "created_utc": "2026-02-24 21:54:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7786qp",
          "author": "CapitalShake3085",
          "text": "Qwen3 is the best model in every task, use the 8b",
          "score": 2,
          "created_utc": "2026-02-24 20:21:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7af0ur",
              "author": "misha1350",
              "text": "Vanilla Qwen3 8B hasn't been updated in July 2025 like how Qwen3 4B 2507 was, so it's not really that good, especially nowadays. DeepSeek R1 8B distill based on Qwen3-8B performs quite better than vanilla Qwen3 8B.",
              "score": 1,
              "created_utc": "2026-02-25 07:33:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o76md2d",
          "author": "dsecareanu2020",
          "text": "Ask ollama :)",
          "score": 1,
          "created_utc": "2026-02-24 18:41:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76xmol",
          "author": "gamesta2",
          "text": "Depends on use. Some models* perform very good on paper, but may suck at tool calling. All depends on your client platform and system prompt",
          "score": 1,
          "created_utc": "2026-02-24 19:32:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77rfyg",
          "author": "Dubious-Decisions",
          "text": "If you don't want to deal with the fabricated explanations that new models offer up, stick with the older llama models like llama3 or llama3.2 (3.1 was a mess). They're big enough to work sensibly, but not so big that it's 30 seconds between responses. You aren't gonna be writing the next great American novel with either of these. But they are solid and are good examples of small, general purpose models that you can run on that hardware.",
          "score": 1,
          "created_utc": "2026-02-24 21:50:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78pzry",
          "author": "FistoWutini",
          "text": "I found under 10GB but needed to use MLX to get better performance over Ollama. Thereâ€™s some overhead tha Ollama has that others donâ€™t.",
          "score": 1,
          "created_utc": "2026-02-25 00:50:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7agg2s",
          "author": "x8code",
          "text": "You could try Granite4 ... it's a good model, but don't expect it to operate like a larger model or a frontier model (Anthropic, Gemini, etc.).\n\nSmall models work well for specific use cases, like \"summarize this chat for me.\" \n\nYou're not gonna use a small model like Granite4 for coding advanced applications. ",
          "score": 1,
          "created_utc": "2026-02-25 07:46:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcxt3m",
      "title": "How are you monitoring your Ollama calls/usage?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/",
      "author": "gkarthi280",
      "created_utc": "2026-02-23 23:32:55",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 0.73,
      "text": "I've been using Ollama in my LLM applications and wanted some feedback on what type of metrics people here would find useful to track in an app that eventually would go into prod. I used OpenTelemetry to instrument my app by following this[Â Ollama observability guide](https://signoz.io/docs/ollama-monitoring/)Â and was able to create this dashboard.\n\n[Ollama dashboard](https://preview.redd.it/b8gxcch9xblg1.png?width=3024&format=png&auto=webp&s=bc90458a61e2e80c8ed5e283edc3e914ccbddcd6)\n\nIt tracks things like:\n\n* token usage\n* error rate\n* number of requests\n* latency\n* LLM provider and model & token distribution\n* logs and errors\n\nAre there any important metrics that you would want to keep track of in prod for monitoring your Ollama usage that aren't included here? And have you guys found any other ways to monitor these llm calls made through ollama?\n\n[](https://www.reddit.com/submit/?source_id=t3_1r8j5ob)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o72io60",
          "author": "immediate_a982",
          "text": "Not trolling nor hating just my point of view after reviewing big box enterprise LLMs. Ollama is not fit for grand scale production. Telemetry is more important there. Ollama telemetry is more appropriate for prototyping.",
          "score": 4,
          "created_utc": "2026-02-24 02:50:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74keaz",
          "author": "firedog7881",
          "text": "https://github.com/bmeyer99/Ollama_Proxy_Wrapper",
          "score": 1,
          "created_utc": "2026-02-24 12:41:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbq3dy",
      "title": "Ollama for Dummies",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rbq3dy/ollama_for_dummies/",
      "author": "catbutchie",
      "created_utc": "2026-02-22 16:24:37",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 0.67,
      "text": "Someone needs to write a book. Right now my mentor is ChatGPT. There are so many parameters i just tell it my issue and it tells me what to change. Some small tweaks are significant performance adjustment. Iâ€™m new obviously. Iâ€™d like to know why Iâ€™m doing what Iâ€™m doing so I can be more in control. Iâ€™m using silly tavern and ollama for self hosted chat. Suggestions needed. Thanks.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rbq3dy/ollama_for_dummies/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o6twhmq",
          "author": "teff",
          "text": "You should be able to ask chatgpt to explain what and why it's suggesting something, if it's giving you an explanation that is beyond your level of understanding, tell it to start again at a 5th grader level. Ask it to give you references for it's explanations and follow up on those and make sure the references make sense to you too.\n\nAny terms or references it uses that you don't understand, make sure to ask it to define it explain again. \n\nEvery 10 or 15 chat messages, ask it to remind you where you are, where you started and summarise everything you've discussed so far, this will help keep it on track, keep the context fresh and give you a chance to review where you're at as well.",
          "score": 2,
          "created_utc": "2026-02-22 20:01:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6udkjn",
              "author": "catbutchie",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-02-22 21:27:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6urv4i",
          "author": "Ryanmonroe82",
          "text": "How much VRAM do you have and what is the model name? which quant are you using?",
          "score": 1,
          "created_utc": "2026-02-22 22:41:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9a0gv",
      "title": "I built a local-first code search tool with Ollama + CocoIndex to save tokens when chatting about codebases.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r9a0gv/i_built_a_localfirst_code_search_tool_with_ollama/",
      "author": "VioletCranberryy",
      "created_utc": "2026-02-19 20:13:57",
      "score": 6,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "Wanted to play with Ollama, CocoIndex, and how semantic vs hybrid search actually works (RRF fusion and all that) - and ended up building something I actually use daily, more and more. The idea is pretty old: index your codebase locally so AI assistants can search it semantically instead of stuffing entire files into context. Fewer tokens, better results.\n\nIt comes with a web dashboard, MCP server, CLI, and works as a Claude Code plugin.\n\nAs a DevOps engineer, I also wanted to make it expandable with custom \"grammars\" â€” so things like Helm charts, GitHub Actions, Docker Compose, and Terraform get proper structure-aware chunking instead of being treated as generic YAML/HCL. And of course, a Docker Compose setup to start everything with a single command :) \n\nGithub: [https://github.com/VioletCranberry/coco-search](https://github.com/VioletCranberry/coco-search) \n\nP.S.   \nYeah, I used AI assistants heavily during development which is kind of fitting since the tool is built to make AI-assisted coding better. I still don't know how to feel about it, first project of this type of mine.   \n  \nP.P.S.  \nWould love feedback, especially on embedding model choice for code. Something else as small as nomic-embed-text and as powerful? CocoIndex updates are incremental, but on large codebases you still need to wait when building the first index.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r9a0gv/i_built_a_localfirst_code_search_tool_with_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o6di395",
          "author": "SrihariLeo",
          "text": "You should definitely post this on the CocoIndex discord server (https://discord.gg/zpA9S2DR7s) in the show-and-tell channel!! The maintainers would love it and so would the folks in the community!\n\n  \n*PS: An active community member here :)*",
          "score": 1,
          "created_utc": "2026-02-20 05:36:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gkomh",
              "author": "VioletCranberryy",
              "text": "Done and done! :) ",
              "score": 1,
              "created_utc": "2026-02-20 17:49:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6dypxq",
          "author": "Whole-Assignment6240",
          "text": "amazing!!",
          "score": 1,
          "created_utc": "2026-02-20 08:04:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dzl4h",
          "author": "Whole-Assignment6240",
          "text": "amazing, love what you build! just gave a star!!",
          "score": 1,
          "created_utc": "2026-02-20 08:12:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gfheo",
              "author": "VioletCranberryy",
              "text": "Thanks! :) ",
              "score": 1,
              "created_utc": "2026-02-20 17:25:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6efdjx",
          "author": "lish202E",
          "text": "Is Windows supported? i got ModuleNotFoundError: No module named 'readline'",
          "score": 1,
          "created_utc": "2026-02-20 10:40:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gff2x",
              "author": "VioletCranberryy",
              "text": "CocoSearch should work on Windows, but the interactive REPL (cocosearch search -i) was importing a Unix-only module. I just made it to gracefully degrade - the REPL will work on Windows but without arrow key history. All other functionality (indexing, search, MCP server, dashboard) should be unaffected. Available from v0.1.18 (latest version at the time of writing).",
              "score": 1,
              "created_utc": "2026-02-20 17:25:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}