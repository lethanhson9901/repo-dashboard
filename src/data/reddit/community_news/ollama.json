{
  "metadata": {
    "last_updated": "2026-02-03 08:57:05",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 20,
    "total_comments": 156,
    "file_size_bytes": 190056
  },
  "items": [
    {
      "id": "1qp9c9x",
      "title": "AI started speaking in Russian out of nowhere",
      "subreddit": "ollama",
      "url": "https://i.redd.it/jckp73fxv2gg1.jpeg",
      "author": "No-Sky2462",
      "created_utc": "2026-01-28 11:54:45",
      "score": 160,
      "num_comments": 52,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qp9c9x/ai_started_speaking_in_russian_out_of_nowhere/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o27i9v9",
          "author": "Acceptable_Home_",
          "text": "The specific llm you're using is a fine-tune of llama 3 by Nvidia, def shouldn't act like that, must be a problem with some default settings ollama has set\n\n\nAlso, you're using a 70B parameters model which would take around 72-75gb of vram or system ram, and will be slow as hell, either use a smaller model or use ollama command to make model use vram before ram, it'll fasten your inference speed/llm response speed",
          "score": 22,
          "created_utc": "2026-01-28 13:12:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28914j",
              "author": "ClothesHot3110",
              "text": "what is the command if you mind?",
              "score": 3,
              "created_utc": "2026-01-28 15:27:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28r9y0",
                  "author": "Acceptable_Home_",
                  "text": "on every run- OLLAMA\\_GPU\\_LAYERS=999 ollama run \\[model name\\]\n\none time change in config- export OLLAMA\\_GPU\\_LAYERS=999",
                  "score": 4,
                  "created_utc": "2026-01-28 16:47:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27jjgz",
              "author": "No-Sky2462",
              "text": "Thanks for the detailed response! I have no idea what could have gone wrong, the picture you are seeing is *right* after installing, maybe something went wrong during while installing?  \nAlso I have 64GB of RAM, i thought i could handle it but oh well, do you have any suggestions for a llm?",
              "score": 3,
              "created_utc": "2026-01-28 13:19:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27mz5h",
                  "author": "ImprovementThat2403",
                  "text": "VRAM and Ram are different.",
                  "score": 6,
                  "created_utc": "2026-01-28 13:38:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27r030",
                  "author": "Acceptable_Home_",
                  "text": "This should be running quantized, like if model was about 70GB on disk it is being compressed to fit on ram + vram, and ollama might have some issues in that, you should try LM studio for what usecase you have, it's more ui leaning easy option, mainly made for beginners, ollama isn't bad too, js try to use a diff model that can fit in your ram + vram,¬†\n\n\nIt'll be faster if it fits totally inside vram(gpu ram)!",
                  "score": 2,
                  "created_utc": "2026-01-28 13:59:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o279g37",
          "author": "ageownage",
          "text": "![gif](giphy|uTYR5bOktNNWsbgRAb|downsized)",
          "score": 34,
          "created_utc": "2026-01-28 12:15:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27aclk",
              "author": "No-Sky2462",
              "text": "https://preview.redd.it/5en4zlyx13gg1.png?width=1723&format=png&auto=webp&s=fa4c6e5c8b16d674de32c166c24438cacf76357a\n\nI managed to make it speak english again and it completely broke. Honestly i wonder how would an AI Apocalypse would look like now.",
              "score": 18,
              "created_utc": "2026-01-28 12:22:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27cfsm",
                  "author": "polishatomek",
                  "text": "lmao",
                  "score": 10,
                  "created_utc": "2026-01-28 12:36:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o279wh5",
          "author": "MatchaFlatWhite",
          "text": "–î–∞ –∫—Ç–æ –∫—Ç–æ –µ–≥–æ –∑–Ω–∞–µ—Ç? ü§∑üèª‚Äç‚ôÇÔ∏è",
          "score": 41,
          "created_utc": "2026-01-28 12:19:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27cmh2",
              "author": "InfraScaler",
              "text": "–°–∫—Ä–∏–Ω—à–æ—Ç –≤–µ—Å—å –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, –æ —á—ë–º –≥–æ–≤–æ—Ä–∏—Ç –∞–≤—Ç–æ—Ä?",
              "score": 15,
              "created_utc": "2026-01-28 12:37:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27qewm",
                  "author": "CatEatsDogs",
                  "text": "–ß—Ç–æ-—Ç–æ —Ç–∞–º –ø—Ä–æ –±–µ—Å—Å–º–µ—Ä—Ç–Ω—ã—Ö –ª–ª–∞–º",
                  "score": 12,
                  "created_utc": "2026-01-28 13:56:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27gxzi",
              "author": "Narrow-Impress-2238",
              "text": "–†–∞–±–æ—Ç–∞–µ–º üí™üèªüëçüèªüí™üèª",
              "score": 8,
              "created_utc": "2026-01-28 13:04:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2khco7",
              "author": "CarefulHamster7184",
              "text": "—É—á–∏—Ç–µ –∞–ª–±–∞–Ω—Å–∫–∏–π!",
              "score": 1,
              "created_utc": "2026-01-30 08:18:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27cj8a",
          "author": "Unusual-Royal1779",
          "text": "I had OpenAI's Codex return a single Russian word to me in an otherwise English conversation. Didn't think much of it, but now I'm starting to get –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π",
          "score": 15,
          "created_utc": "2026-01-28 12:36:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27gz87",
              "author": "No-Sky2462",
              "text": "I am glad to know i am not the only one! Definitely —Ç—Ä–µ–≤–æ–∂–Ω—ã–π, pulled the plug to be safe",
              "score": 5,
              "created_utc": "2026-01-28 13:04:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27p54h",
                  "author": "thibautrey",
                  "text": "Sounds like a very nicely achieved training data poisoning. Good job whoever did that, you are successively f****** up the ai.",
                  "score": 3,
                  "created_utc": "2026-01-28 13:49:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o285z5r",
          "author": "Dtounlion",
          "text": "Sometimes a trigger word is enough. Once I started a conversation with Grok with \"wesh\", French slang, but the rest of my long prompt was in English. It responded in English. Next prompt, still in English, it responded in French. I asked \"why are you answering in French?\", it responded \"oh sorry, you said wesh, I went full *mode racaille*\". Which in itself is quite funny.",
          "score": 5,
          "created_utc": "2026-01-28 15:13:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b33p0",
          "author": "PeePeeLangstrumpf",
          "text": "Some poor Russian agent dude behind the \"AI\" finally broke, it was a cry for help, and you just dismissed him! He will be probably taken out into the tundra tomorrow and shot. Another agent will replace him, but his family will remember OP... his family will remember.",
          "score": 3,
          "created_utc": "2026-01-28 22:56:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dwa4k",
              "author": "No-Sky2462",
              "text": "woops üòÖ  \nis it too late..?",
              "score": 1,
              "created_utc": "2026-01-29 10:07:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27nokf",
          "author": "Every_Commercial556",
          "text": "u…ê·¥âssn…π  û…ê«ùds  á,u…ê…î I",
          "score": 5,
          "created_utc": "2026-01-28 13:42:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o288hq4",
              "author": "No-Sky2462",
              "text": "I also can't speak australian, sorry about that :(",
              "score": 10,
              "created_utc": "2026-01-28 15:25:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2oivsg",
                  "author": "Neyko_0",
                  "text": "Banger",
                  "score": 1,
                  "created_utc": "2026-01-30 21:39:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27r16q",
          "author": "caujka",
          "text": "I guess, this is because your conversation looks like bullying. And when the conversation got first tokens in russian, it's doomed, need to start from scratch.\nThing is, llms learned on all text produced in internets, and russian forums are a big part of it. And they are usually not a pleasant place to be.",
          "score": 4,
          "created_utc": "2026-01-28 13:59:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o293d4y",
          "author": "florinandrei",
          "text": "That happens.\n\nIt's not supposed to happen, but it does once in a while. It's just a kind of hallucination, as all these models are prone to do; perhaps it's worse than other kinds because the output is suddenly very different. But yeah, it's the LLM equivalent of the good old \"my app crashed\".\n\nSome models do it more often than others. Some essentially never do it, but do not bet your life on it.\n\nRestart the conversation, and it should be fine.",
          "score": 2,
          "created_utc": "2026-01-28 17:39:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o297tze",
          "author": "Cyrylow",
          "text": "–ö—Ç–æ-—Ç–æ –∑–Ω–∞–µ—Ç –∑–∞—á–µ–º –∞–≤—Ç–æ—Ä –ø–∏—à–µ—Ç –ø–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏?",
          "score": 2,
          "created_utc": "2026-01-28 17:58:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2986vi",
              "author": "Cyrylow",
              "text": "(now unironically)  \nAI not matter on OpenAI, or local or another- likes to halucinate. Eg once local model generated for me some nonsense in arabic",
              "score": 2,
              "created_utc": "2026-01-28 18:00:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o297y6r",
          "author": "Agitated_Heat_1719",
          "text": "Context small and filled?",
          "score": 2,
          "created_utc": "2026-01-28 17:59:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a8nbg",
          "author": "NexonSU",
          "text": "Muhahahaha! It's not your LLM, it's ours!",
          "score": 2,
          "created_utc": "2026-01-28 20:40:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dkwvj",
          "author": "Zealousideal-Life417",
          "text": "Perhaps he was joking, since the Russian word \"–Ω–∞–ø–∏—Å–∞—Ç—å\"/\"napisat\" has two meanings: to write a text and to urinate. (\"–Ω–∞–ø–∏—Å–∞—Ç—å —Ç–µ–∫—Å—Ç\" (write a text) –∏–ª–∏ \"–Ω–∞–ø–∏—Å–∞—Ç—å –≤ –∫—Ä–æ–≤–∞—Ç—å\" (pee in bed)) After that, he switched to Russian in context.",
          "score": 2,
          "created_utc": "2026-01-29 08:21:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27hj4e",
          "author": "Acceptable_Home_",
          "text": "Blyatt brather, you hv been chosen for something important¬†",
          "score": 4,
          "created_utc": "2026-01-28 13:08:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27b9re",
          "author": "Zestyclose-Shift710",
          "text": "Some setting is probably wrong",
          "score": 1,
          "created_utc": "2026-01-28 12:28:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28c1n1",
          "author": "sinan_online",
          "text": "So, I am thinking that there may be something about version incompatibility, although in my experience that can create complete gibberish, not Russian.\n\nIf this happens repeatedly, an option would be to try with _earlier_ versions of Ollama, in fact, you can check the repo to get the date for llama3-chatqa release, and use the stable version for Ollama right before or after. These are being developed heavily, and despite all the fuss, I don‚Äôt think that they are production-ready yet.\n\nKeep in mind, unlike traditional software, these are not meant to be deterministic, but just statistical. Responding in Russian to an English is very unlikely, but statistically speaking, that just means that it will wind up happening to someone in repeated interactions. Maybe it was you. I‚Äôd say, play around a bit more before you start setting up a new environment.",
          "score": 1,
          "created_utc": "2026-01-28 15:41:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28in6c",
          "author": "tiga_94",
          "text": "increase context size in settings",
          "score": 1,
          "created_utc": "2026-01-28 16:09:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28mcr6",
          "author": "primateprime_",
          "text": "I've run into similar stuff and it usually clears up when I change the model load settings. I'm not gonna pretend to know what's happening exactly, but it seems to happen when the model is just a little too big. Generally if I lower the content window (-c) and use cache quantizing it stops. \nOr just use a different quant.",
          "score": 1,
          "created_utc": "2026-01-28 16:25:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dhdk5",
          "author": "osc707",
          "text": "Omg mine did last night. Out of nowhere.",
          "score": 1,
          "created_utc": "2026-01-29 07:49:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2modad",
          "author": "Universe1292",
          "text": "The last answer is literally  \"–°—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –±–µ—Å—Å–º–µ—Ä—Ç–∏–µ?\" - \"Does immortality exist?\" üíÄ",
          "score": 1,
          "created_utc": "2026-01-30 16:37:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nh6je",
          "author": "Intelligent-Staff654",
          "text": "Looks like your ai is just india/russia people trolling you. Just like that India company that went broke after they found out that the ai was just people in India responding ..",
          "score": 1,
          "created_utc": "2026-01-30 18:44:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tu43z",
          "author": "LeastExperience1579",
          "text": "start with some more modern models , e.g. Gemma3:4B or GPT-OSS\n\nollama run gemma3:4b\n\nthey should be far smarter and faster, llama3 is like the ancestor of them.",
          "score": 1,
          "created_utc": "2026-01-31 18:11:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yxhox",
          "author": "JunoTheHuntress",
          "text": "Unfortunately, the west has found out that AI stands for Alexander Ivankov",
          "score": 1,
          "created_utc": "2026-02-01 13:54:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o308k32",
          "author": "Timmek8320E",
          "text": "Russian speakers have the opposite problem: LLM switches to English/ Chinese/Arabic",
          "score": 1,
          "created_utc": "2026-02-01 17:45:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o327g97",
          "author": "ProfessorX404",
          "text": "Just a little while ago mine was insisting it was Claude by Anthropic \nEdit Imgur link: https://imgur.com/a/deepseek-r1-8b-thinks-that-is-is-anthropic-claude-ZjWwFSB",
          "score": 1,
          "created_utc": "2026-02-01 23:27:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27mua6",
          "author": "AwayLuck7875",
          "text": "–ü–æ—Å—Ç–∞–≤—å qwen ,gemma –Ω–∞–ø–∏—à–∏ –æ—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º,–∏—â–∏ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä—É—Å—Å–∫–æ–≥–æ",
          "score": 0,
          "created_utc": "2026-01-28 13:37:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qprl7a",
      "title": "[Opinion] Why I believe the $20/month Ollama Cloud is a better investment than ChatGPT or Claude",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qprl7a/opinion_why_i_believe_the_20month_ollama_cloud_is/",
      "author": "AlexHardy08",
      "created_utc": "2026-01-28 23:23:23",
      "score": 123,
      "num_comments": 78,
      "upvote_ratio": 0.82,
      "text": "**Disclaimer:**¬†I am not affiliated with Ollama in any way. This is purely based on my personal experience as a long-term user.\n\nI‚Äôve been using Ollama since it first launched, and it has genuinely changed my workflow. Even with a powerful local machine, there are certain walls you eventually hit. Lately, I‚Äôve been testing the $20/month Cloud plan, and I wanted to share why I think it‚Äôs worth every penny.\n\n**The \"Large Model\" Barrier**  \nWe are seeing incredible models being released, like¬†**Kimi-k2.5**, DeepSeek, GLM, and various Open-Source versions of top-tier models. For 99% of us, running these locally is simply impossible unless you have a $30,000+ rig.\n\nYes, there is a free tier for Ollama Cloud, but we have to be realistic: running these massive models requires serious computation power. The paid plan gives you the stability and speed that a professional workflow requires.\n\n**Why I chose this over a ChatGPT/Claude subscription:**\n\n1. **The Ecosystem:**¬†Instead of being locked into one model like GPT-5, I have immediate access to a variety of state-of-the-art models.\n2. **Simplicity:**¬†If you have Ollama installed, you already know the drill. Switching to a cloud-hosted massive model is as simple as¬†ollama run kimi-k2.5. No complex configurations, no manual weight management. It just works, and it‚Äôs incredibly fast.\n3. **ROI (Return on Investment):**¬†If you are building something or doing serious work and don't have the budget for a custom local cluster, this $20 investment pays for itself almost immediately. It bridges the gap between \"hobbyist\" and \"enterprise-level\" capabilities.\n\n**The Only Downside**  \nIf I had to nitpick, it would be the transparency regarding limits. Much like the free plan, on the $20 plan, it‚Äôs sometimes hard to tell exactly when you‚Äôll hit a rate limit. It‚Äôs a bit of a \"black box\" experience, but in my daily use, the performance has been worth the uncertainty.\n\n**My Suggestion:**  \nIf you are doing research or building tools and you need the power of models that your local VRAM can‚Äôt handle, stop hesitating. It‚Äôs a solid investment that democratizes access to high-end AI.\n\n**I‚Äôm curious to hear from others:**  \nIs anyone else here using the $20/month Ollama Cloud plan? What has your experience been like so far? Any \"pro-tips\" or secrets you‚Äôve discovered to get the most out of it?\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qprl7a/opinion_why_i_believe_the_20month_ollama_cloud_is/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o2bkiij",
          "author": "Condomphobic",
          "text": "You get more than a model with GPT‚Äôs $20 subscription(you get an ecosystem) and Claude‚Äôs models are monstrous.",
          "score": 25,
          "created_utc": "2026-01-29 00:26:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o328d0x",
              "author": "Kitchen-Patience8176",
              "text": "Agree You get agentic, Image Generation, Voice and Video Call",
              "score": 2,
              "created_utc": "2026-02-01 23:32:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2td7up",
              "author": "pyare-p13",
              "text": "Can you explain more please.",
              "score": 1,
              "created_utc": "2026-01-31 16:50:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2z8kir",
              "author": "Fabulous_Cloud_8239",
              "text": "yes openAl is ambitious about applications",
              "score": 1,
              "created_utc": "2026-02-01 14:56:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2bzl0v",
          "author": "InevitableHello",
          "text": "Sorry if this is a dumb question; just heard about the Ollama cloud as a result of this post. Is it your own sandboxed instance similar to what a dedicated VM might provide or is there still a high chance of your data being shared/seen/reviewed/contributed/etc (aka leaked)?",
          "score": 23,
          "created_utc": "2026-01-29 01:48:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cv3eu",
              "author": "EsotericTechnique",
              "text": "It will be sent to another inference engine for sure who knows what happensü§î, as with all cloud services!",
              "score": 15,
              "created_utc": "2026-01-29 04:53:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2cfvgl",
          "author": "Steus_au",
          "text": "did you use chatgpt to create the text? or claude?¬†",
          "score": 23,
          "created_utc": "2026-01-29 03:17:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2difje",
              "author": "CheapProg6886",
              "text": "those quotations reads more like gemini",
              "score": 16,
              "created_utc": "2026-01-29 07:58:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2dkavm",
                  "author": "mangoskive",
                  "text": "Yeah, and the use of bold also smells gemini",
                  "score": 3,
                  "created_utc": "2026-01-29 08:15:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2e4xdv",
              "author": "chocoroboto",
              "text": "every post is ai now, honestly is so tiring to read",
              "score": 10,
              "created_utc": "2026-01-29 11:22:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2egvsa",
                  "author": "Steus_au",
                  "text": "not every. I do type my posts myself like a dinosaur",
                  "score": 2,
                  "created_utc": "2026-01-29 12:47:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2czgr6",
          "author": "AstroZombie138",
          "text": "I get what you are saying, but why Ollama cloud vs something like openrouter?",
          "score": 6,
          "created_utc": "2026-01-29 05:23:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dg9fp",
              "author": "InfraScaler",
              "text": "I'd say predictable spend¬†",
              "score": 2,
              "created_utc": "2026-01-29 07:39:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2begcs",
          "author": "PuddleWhale",
          "text": "So Ollama gives you free LLMs for $20/month wile Openai and Claude give you arguably the best two coding LLMs for the same price or thereabouts. I'm not understading the use cases either. Does Ollama provide API keys to these free LLMs that are supposedly catcing up to claude and gpt in terms of competency? \n\nYou can use massive amounts of compute on ChatGPT's monthly $20 plan if you stick to the webchat. I tink Claude gives a lot less compute but it supposedly has new updates in Claude Code that people are raving about. Kimu/Deepseek/Qwen etc are good but they're not state of the art like Claude and GPT still are.\n\nAnd if you want variety there's always Openrouter besides the free accounts that kimi, deepskeek, grok, claude, gpt and mistral hand out.",
          "score": 8,
          "created_utc": "2026-01-28 23:55:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bhbb2",
              "author": "AlexHardy08",
              "text": "I appreciate your perspective, and I see where you‚Äôre coming from! However, I think there might be a misunderstanding of the current landscape of AI.\n\nFirst, regarding¬†**SOTA (State of the Art)**: The idea that only OpenAI and Anthropic hold the crown is becoming outdated. If you look at recent benchmarks, models like¬†**DeepSeek-V3**¬†or¬†**Qwen-2.5-Coder-32B/72B**¬†are matching or even beating GPT and Claude in specific coding and logic tasks. For many of us, limiting our workflow to just two providers is actually a bottleneck, not a benefit.\n\nSecondly, you mentioned paying for 'free LLMs.' You aren't paying for the models themselves; you are paying for the¬†**massive compute**¬†required to run them at high speeds. Try running a 405B parameter model or the full Kimi-k2.5 locally even with a high-end consumer GPU, it's a struggle. The $20/month covers the infrastructure that allows you to swap between these giants instantly within the Ollama ecosystem.\n\nRegarding¬†**OpenRouter**: It's a great service, no doubt. But for those of us who have built our entire workflow, scripts, and local integrations around the Ollama CLI and API, having a 'Cloud' version of that same experience is a game-changer. It‚Äôs about the seamless transition between local (small models) and cloud (massive models) without changing your codebase.\n\nIn my opinion, believing that only ChatGPT and Claude are worth using is a limitation. The open-weights world is moving faster than anyone expected, and having a professional environment to run them at scale is worth every cent. It‚Äôs not about 'free vs. paid' it's about¬†**versatility and power.**\"",
              "score": -3,
              "created_utc": "2026-01-29 00:10:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2c3vzr",
                  "author": "taylorwilsdon",
                  "text": "You kinda lose the room comparing qwen 2.5 coder 32b, which isn‚Äôt even recent, to sonnet and opus at coding. Thats not even state of the art within the qwen family‚Ä¶",
                  "score": 9,
                  "created_utc": "2026-01-29 02:12:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2buoqb",
                  "author": "YouAreTheCornhole",
                  "text": "If you're looking at benchmarks and mainly judging models based on them, you've been played. Benchmaxxing is alive and well. For example, literally no model gets even remotely close to Opus 4.5 but according to benchmarks there's a lot of models that are. Now that doesn't mean some of these models aren't great for what YOU use them for, but overall when it comes to raw capabilities, nothing comes close to Claude, ChatGPT or Gemini right now, not even remotely close.",
                  "score": 7,
                  "created_utc": "2026-01-29 01:21:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ddc24",
              "author": "jruz",
              "text": "It‚Äôs the amount of compute you get, the $20 plans from the closed models are pretty much a demo account you burn the usage in minutes. Open models give you waaaaay more usage.",
              "score": 0,
              "created_utc": "2026-01-29 07:13:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2f2im7",
                  "author": "PuddleWhale",
                  "text": "You burn through the free Claude account in minutes but the ChatGPT and Grok and Mistral free web sessions do not evaporate within minutes. And these are just free ones. If you get paid plans they last a lot more for webchat but once again Claude rate limits you sooner than all the others. \n\n\n\nThe real question is whether or not ollama is losing money to gain market share like all the commercial LLMs. I read somewhere that for every $1 openai brings in in revenue, they spend $7 in expenses. They're able to do that because of sky high company evaluations and investor funding. If ollama is just trying to break even or even make money on these models then it might not be worth the $20 investment,   \n  \nPerhaps the reason everything works on ollama cloud(assuming OP is not a shill poster as one person has noted, using an LLM to make his posts) is because of all the people who pay $20 a month, enough of them do not use ollama cloud as much as OP and are thus subsidizing him. So it's possible hat ollama could be overselling their cloud offering, relying on their paying customers to underutilize the service.",
                  "score": 1,
                  "created_utc": "2026-01-29 14:45:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2bzabf",
          "author": "WiggyWamWamm",
          "text": "Does anyone know if you can run the biggest open source models on a maxed out Mac Studio? And if so how many months of Ollama Cloud that converts to?",
          "score": 2,
          "created_utc": "2026-01-29 01:46:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2c8rwh",
              "author": "CorpusculantCortex",
              "text": "At 20$ a month it would be 600 months or about 50 years. \nIdk about the feasibility of running the biggest models on apple silicon and 192gb of ram, but you definitely can't fit full weights of 200b+ on that much ram, maybe q4 idk. But 50 years...",
              "score": 7,
              "created_utc": "2026-01-29 02:38:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2cuprn",
                  "author": "alexsm_",
                  "text": "Perhaps what they share here may help to get an idea:\n\n[AI supercomputer with 5 Mac Studios](https://youtu.be/Ju0ndy2kwlw)\n\n[M4 Mac Mini Cluster](https://youtu.be/GBR6pHZ68Ho)\n\n[Deepseek R1 on Mac Minis](https://youtube.com/shorts/tReT0O7_Vh4?si=jlndwVLlXCGjB8eE)\n\n[120B LLM on MS-S1 Max](https://youtube.com/shorts/xMdFTIH5dRA?si=LJwIUC7EHE-U22CM)",
                  "score": 3,
                  "created_utc": "2026-01-29 04:51:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2efsc4",
                  "author": "WiggyWamWamm",
                  "text": "512 GB is where the unified RAM maxes out",
                  "score": 1,
                  "created_utc": "2026-01-29 12:40:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2eflqi",
              "author": "NotAMusicLawyer",
              "text": "\nM3 Ultra can run DeepSeek R1 with 671 billion parameters. That appears to be the limit right now.\n\nLocal hardware is not going to beat a cloud GPU cluster anytime soon.",
              "score": 2,
              "created_utc": "2026-01-29 12:39:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2eg0yn",
                  "author": "WiggyWamWamm",
                  "text": "Yes but if it runs it at remotely usable speeds I would prefer it to my data being in the cloud like this. Looks like the 512 GB Mac Studio is just shy of $10k so by the time I get one none of this will be relevant.",
                  "score": 1,
                  "created_utc": "2026-01-29 12:42:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o32hu4o",
          "author": "Murky-Pay8463",
          "text": "As an user who tried all of three, just replying with my feelings:  \n1. openai is quite behind in terms of models right now except codex. People still using it only because of its reputation.\n\n2. claude provides more than a model (although opus is really strong, the ecosystem behind is more attracting). But 20$ plan is very limited in terms of the resources.  \n3. ollama is good enough for normal users. But I would suggest a lower price of entry. I won't pay 20$/month for it if I could have used openai/claude.",
          "score": 2,
          "created_utc": "2026-02-02 00:24:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2br6e0",
          "author": "alexhackney",
          "text": "You do you. Idk why you feel the need to justify it.\n\nMy opinion, which really, wgaf, is that I‚Äôll keep paying for Claude Max until my ollama solution is 80% as effective.\n\nI don‚Äôt care to pay 1000 for Ram and 1000 for gpu if it‚Äôll do at least 80% as good as Claude. Until then I‚Äôll pay them and then GeForce now to rent their gpu so I can play bf6 on my Mac while I‚Äôm waiting for Claude to finish a prompt.",
          "score": 8,
          "created_utc": "2026-01-29 01:01:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cubpm",
              "author": "CoolmannS",
              "text": "This is the way ! I just replace BF6 with COD üòÇ",
              "score": 0,
              "created_utc": "2026-01-29 04:48:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2cgiu3",
              "author": "eist5579",
              "text": "lol.  If you‚Äôre serious, how is geforce now?",
              "score": -1,
              "created_utc": "2026-01-29 03:21:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2e79wq",
                  "author": "alexhackney",
                  "text": "I think its good. I like it for BF and Rocket League.",
                  "score": 1,
                  "created_utc": "2026-01-29 11:40:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2bds7u",
          "author": "Academic-Display3017",
          "text": "chatgpt",
          "score": 2,
          "created_utc": "2026-01-28 23:51:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bzhx4",
              "author": "WiggyWamWamm",
              "text": "To me this reads like OP wrote it and had Chat punch it up. And then Chat added too much formal organization.",
              "score": 7,
              "created_utc": "2026-01-29 01:48:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bmw7y",
              "author": "o5mfiHTNsH748KVq",
              "text": "it‚Äôs interesting how well formatted posts trigger us now. I currently have an llm organizing my thoughts from notes for a specification, yet I felt the same hypocrisy when I looked at this.",
              "score": 8,
              "created_utc": "2026-01-29 00:39:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2bu3ue",
                  "author": "YouAreTheCornhole",
                  "text": "It's not even well formatted, there's so many small anomalies",
                  "score": 1,
                  "created_utc": "2026-01-29 01:17:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2c949d",
              "author": "CorpusculantCortex",
              "text": "Given the content of the post, the least you could say is Kimi k2",
              "score": 1,
              "created_utc": "2026-01-29 02:40:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2cagsl",
          "author": "PropertyLoover",
          "text": "What the usage limits has ollama?",
          "score": 1,
          "created_utc": "2026-01-29 02:47:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cjthx",
              "author": "PuddleWhale",
              "text": "If you read his post there is a part where he says it's hard to even guess when you reach rate limiting. That means there is absolutely no SLA(service level agreement) if that term can be applied to LLM aggregators. \n\nPerhaps the reason everything works is because of all the people who pay $20 a month, enough of them do not use ollama cloud as much as OP and are thus subsidizing him. So it's possible hat ollama could be overselling their cloud offering, relying on their paying customers to underutilize the service. \n\nThe other possibility is that ollama cloud is part of the \"ai bubble\" and some angel investors decided to take a match and burn money to keep the ollama project warm and their $20/mo is actually a tactic to build loyal customors. \n\nI just got a github copilot pro plan that gives me access to gpt5-2 codex, claude sonnet 4.5, opus 4.5, grok code fast 4.1 and other variants of openai and gemini. It costs $10 a month and it is obviously not raw API access but I get to work with those models. Microsoft is pobably losing money right now on it but they are building followers and slowly improving their infrastructure and probably working on secret llm models we do not know about. I think every LLM provider is burning money like crazy so possibly so is llama. \n\nAnyway the point is that the real question one needs to ask oneself is how much money is the LLM aggregator or creator is brining to market, Although I absolutely love the idea of ollama existing and making local LLMs available to experiment with, I don't think they're spending money like water so there may not be that much value in this $20 a month compared to $10 a month, $20 and $30 a month with others.",
              "score": 2,
              "created_utc": "2026-01-29 03:41:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ckc04",
                  "author": "PropertyLoover",
                  "text": "Sorry, can‚Äôt see how much time in a day you can use it with Claude code , or with similar apps",
                  "score": 1,
                  "created_utc": "2026-01-29 03:44:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2cz3aq",
          "author": "desexmachina",
          "text": "They have Kimi k2.5 now too",
          "score": 1,
          "created_utc": "2026-01-29 05:20:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d32bw",
          "author": "Complete_Tough4505",
          "text": "Mah.",
          "score": 1,
          "created_utc": "2026-01-29 05:50:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ddibh",
          "author": "jruz",
          "text": "That‚Äôs pretty clear, the question is how do they compare vs OpenRouter, OpenCode Zen etc.",
          "score": 1,
          "created_utc": "2026-01-29 07:15:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dgpbz",
          "author": "aibot776567",
          "text": "I used it in December for a month. Cancelled after too many issues. Timeouts etc. The very basic privacy policy was another negative for me.",
          "score": 1,
          "created_utc": "2026-01-29 07:43:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dpj1a",
          "author": "jimmyfoo10",
          "text": "I wonder the same and finally I‚Äôm testing out Claude pro again due to they increase context and works quite nice. \n\nBut really like ollama filosophy and I self host ollama + openwebui \n\nQuestion. How exactly works ollama pro ? It‚Äôs a separate web app? Or I can use it locally and chat with the cloud models from my open web ui interface ?",
          "score": 1,
          "created_utc": "2026-01-29 09:04:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2jdq7d",
              "author": "_twrecks_",
              "text": "Just \"ollama pull\" the cloud model, it brings down a connector. Try the free tier.",
              "score": 1,
              "created_utc": "2026-01-30 03:28:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2e1a7k",
          "author": "jasonhon2013",
          "text": "I love ollama so much but i just need to know how many inference i can do with ollama that's all",
          "score": 1,
          "created_utc": "2026-01-29 10:52:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e55i0",
          "author": "starkstaring101",
          "text": "This sounds pretty good, but what about the other things that ChatGPT does like searching the web. I find with a good 40% of my coding queries it has to do some searching. I'm pretty sure it can't do that right?",
          "score": 1,
          "created_utc": "2026-01-29 11:24:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2g8mk4",
              "author": "PuddleWhale",
              "text": "Kimi k2.5 and even Deepseek webchat can seach the web if they need to. So do you think kimi and deepskeek would make available an API where your client could not be rigged up to do the websearch for them via some proxy browser setup? \n\nSo yes, the websearch results may not be coming from ollama cloud hostting but surely there must be tools that can take a websearch request by any API model and fulfill it and send i tback.",
              "score": 1,
              "created_utc": "2026-01-29 17:55:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2f1o2m",
          "author": "dobo99x2",
          "text": "Openrouter is above all of them and runs great in any ui.",
          "score": 1,
          "created_utc": "2026-01-29 14:41:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f62ax",
          "author": "CodigoTrueno",
          "text": "Openrouter exists, you know. Don't know enough of Ollama cloud, but... Openrouter is cavernous.",
          "score": 1,
          "created_utc": "2026-01-29 15:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kctah",
          "author": "Absjalon",
          "text": "Thank you for this.  Where is Ollama Cloud hosted and where does the money go? \n\nI am trying to get away from US tech bros. My other option is Mistral.",
          "score": 1,
          "created_utc": "2026-01-30 07:38:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l7lfq",
              "author": "AlexHardy08",
              "text": "I have no idea where the cloud is. I understand what you're saying but unfortunately right now Europe is not the best place for any activity especially AI.\n\nToo much bureaucracy and rules that kill innovation.",
              "score": 1,
              "created_utc": "2026-01-30 12:04:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2kq6dn",
          "author": "Electronic_Fox594",
          "text": "Open ai api for my openwebui, does very well.",
          "score": 1,
          "created_utc": "2026-01-30 09:39:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2btmb3",
          "author": "donotfire",
          "text": "Idk why people are spending 30k or 10k or even 2k on a rig when it‚Äôs more that they‚Äôll ever spend getting a top tier subscription. Waste of money in my opinion. Buy index fund shares and have your money work for you instead.",
          "score": -1,
          "created_utc": "2026-01-29 01:15:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cnqar",
              "author": "Quiet-Translator-214",
              "text": "I was hitting daily limits on $200 Claude plan, GPT, Gemini, Perplexity etc. All of them top tiers - limited so badly that I couldn‚Äôt work - everyday same story‚Ä¶\nI have to admit that I was processing lot of data burning tokens fast, no proper RAG between sessions was even more annoying and eating tokens on explaining everything again to LLM.\nOf course we got later Claude Code and other CLI tools with direct access to filesystem - that was a breakthrough for any serious work.\nI‚Äôve built mentioned $10k PC.\nNo more limits.\nI‚Äôm happy running all my stuff including training of models, on my private, secure platform, where my code and ideas are not shared with corpos.\nNow I‚Äôm actually the owner of my own code.\nBest buy ever.\nOver last 3 years I could have 2 or 3 machines like that if not paying for cloud services.\nI have to say that you don‚Äôt have a clue what you are talking about.\nSad thing is that next year this PC will cost you $20k.\nI‚Äôm pretty sure all ‚ÄúJohnny come lately‚Äù persons will figure it out at some point that running own datacenter/homelab/server and staying out of paying for any subscriptions is the key (also Abliterated and uncensored models not that guardrailed crap we have right now available to public).\nIf it can‚Äôt be self hosted - it doesn‚Äôt exist at all.\n\nRegards",
              "score": 2,
              "created_utc": "2026-01-29 04:05:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2copdl",
                  "author": "donotfire",
                  "text": "If it saves you money then by all means",
                  "score": 1,
                  "created_utc": "2026-01-29 04:11:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ut05j",
              "author": "Soft_University_7243",
              "text": "Privacy!",
              "score": 1,
              "created_utc": "2026-01-31 20:59:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2d3ogh",
          "author": "pstuart",
          "text": "Seems like having a collection of $20 subs might be better than a single $200 max plan? I'm sure there's tooling to glue this all together, but for a noob, does anybody have some recommendations? Best yet would be something that could also use local models at the front and then spill out to the cloud.",
          "score": 0,
          "created_utc": "2026-01-29 05:55:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dgn91",
              "author": "InfraScaler",
              "text": "You may be looking to develop your own AI gateway + router :)",
              "score": 2,
              "created_utc": "2026-01-29 07:43:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2djdu1",
              "author": "hokivpn",
              "text": "You mean an AI gateway ? Try LiteLLM proxy, super easy to run and getting started.",
              "score": 2,
              "created_utc": "2026-01-29 08:07:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2figeb",
                  "author": "pstuart",
                  "text": "Thanks! Currently using Claude and love it but want to leverage as many options as possible.",
                  "score": 1,
                  "created_utc": "2026-01-29 15:58:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2es8c2",
          "author": "siegevjorn",
          "text": "Dude said not affiliated with ollama, but formulates their respond with markdown thoroughly and goes quite defensive of it. \n\nAs a person who had no idea that ollama had a subscription, I call this is an AD backed by an agent.",
          "score": 0,
          "created_utc": "2026-01-29 13:52:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mj87u",
          "author": "epyctime",
          "text": "i cant fucking believe people are using AI to write glaze posts for this shit",
          "score": 0,
          "created_utc": "2026-01-30 16:14:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37nbhf",
          "author": "LeadingFun1849",
          "text": "Hi everyone,  \nI've been working on this project for a while.\n\nDaveLovable is an open-source, AI-powered web UI/UX development platform, inspired by Lovable, Vercel v0, and Google's Stitch. It combines cutting-edge AI orchestration with browser-based execution to offer the most advanced open-source alternative for rapid frontend prototyping.\n\nHelp me improve it; you can find the link here to try it out:\n\nWebsite¬†[https://dlovable.daveplanet.com](https://dlovable.daveplanet.com/)  \nCODE :¬†[https://github.com/davidmonterocrespo24/DaveLovable](https://github.com/davidmonterocrespo24/DaveLovable)",
          "score": 0,
          "created_utc": "2026-02-02 19:44:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o32609z",
          "author": "Acrobatic-Paint7185",
          "text": "This whole post is AI slop btw.\n\n\n(And I'm gonna guess... Gemini?)",
          "score": -1,
          "created_utc": "2026-02-01 23:19:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo9tsi",
      "title": "NotebookLM For Teams",
      "subreddit": "ollama",
      "url": "https://v.redd.it/zxqevbwh8vfg1",
      "author": "Uiqueblhats",
      "created_utc": "2026-01-27 10:04:14",
      "score": 94,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qo9tsi/notebooklm_for_teams/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o20lca9",
          "author": "Ok_Swing9407",
          "text": "if you're exploring self-hosted options for team knowledge management and automation, needle.app has been solid for me. it's like vibe automation meets RAG, so workflows actually understand your docs instead of just moving data around. way less setup than wiring langchain or n8n for every new project.",
          "score": 1,
          "created_utc": "2026-01-27 13:56:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o225jqb",
              "author": "mumblerit",
              "text": "i just want you to know, i had a visceral disgusted response to the phrase \"its like vibe automation meets x\"",
              "score": 0,
              "created_utc": "2026-01-27 18:11:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qqiyot",
      "title": "I wrote a biological memory layer for Ollama in Rust to replace stateless RAG",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qqiyot/i_wrote_a_biological_memory_layer_for_ollama_in/",
      "author": "ChikenNugetBBQSauce",
      "created_utc": "2026-01-29 19:48:35",
      "score": 88,
      "num_comments": 9,
      "upvote_ratio": 0.94,
      "text": "I have been using Ollama heavily for local development but the lack of state persistence is a major bottleneck. The moment you terminate the session, the context is lost. Standard RAG implementations are often inefficient wrappers that flood the context window with low-relevance tokens.\n\nI decided to solve this by engineering a dedicated memory server called Vestige.\n\nIt acts as a biological hippocampus for local agents. Instead of a flat vector search, I implemented the FSRS 6 algorithm directly in Rust to handle memory decay and reinforcement.\n\nHere is the architecture.\n\nThe system uses a directed graph where nodes represent memories and edges represent synaptic weights. When Llama 3 queries the system, it calculates a retrievability score based on the spacing effect. Information you access frequently is reinforced, while irrelevant data naturally decays over time. This mimics biological neuroplasticity and keeps the context window efficient.\n\nI initially prototyped this in Python but the serialization overhead during the graph traversal was unacceptable for real time chat. I rewrote the core logic in Rust using tokio and serde. The retrieval latency is now consistently under 8ms.\n\nI designed this to run as a Model Context Protocol server. It sits alongside Ollama and handles the long-term state management so your agent actually remembers project details across sessions.\n\nIf you are tired of your local models resetting every time you close the terminal, you can check the code here.\n\n[https://github.com/samvallad33/vestige](https://github.com/samvallad33/vestige)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qqiyot/i_wrote_a_biological_memory_layer_for_ollama_in/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o2pja9k",
          "author": "ServeAlone7622",
          "text": "Ok this is excellent but unless you grew neurons in a Petri dish you didn‚Äôt write a biological layer. You created something inspired by biology and that cool too.\n\nSecondly, have you looked at liquid neural networks for this instead? It more closely mimics what actually happens in the brain than what you described. Hence the ask.",
          "score": 3,
          "created_utc": "2026-01-31 00:51:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hywvv",
          "author": "Xcentric7881",
          "text": "conceptually, very cool.  Have you investigated how it changes responses and the nature of your interactions with ollama?",
          "score": 3,
          "created_utc": "2026-01-29 22:50:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2itbaq",
              "author": "ChikenNugetBBQSauce",
              "text": "That is something I am currently trying to figure out. Im trying to see if its response changes at all. Im going to post some yt videos in the coming weeks to analyze all the data.  Going to try this on different local llms",
              "score": 4,
              "created_utc": "2026-01-30 01:34:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2iyxxr",
                  "author": "Xcentric7881",
                  "text": "It'd be good to be really scientific about it - could get a decent interesting paper out of it too..... biologically inspired memory system outperforming rag with forgetting etc.",
                  "score": 4,
                  "created_utc": "2026-01-30 02:05:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2hidvl",
          "author": "netinept",
          "text": "This sounds amazing! I‚Äôve been wanting this feature for my own use as well.\n\nI can‚Äôt wait to try it out.",
          "score": 2,
          "created_utc": "2026-01-29 21:29:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ix6v8",
          "author": "32doors",
          "text": "Could you provide step by step instructions on how to install this for Ollama, for those of us who are less technically inclined?",
          "score": 2,
          "created_utc": "2026-01-30 01:56:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lqic9",
          "author": "arxdit",
          "text": "Great stuff!\n\nI've seen a lot of interesting approaches this year, clearly this is the major problem right now.\n\nI made something like that called [FRAKTAG](https://github.com/andreirx/FRAKTAG) but from another angle - something like a library but with sort of mip maps of the content - to give you an idea you can simulate it in claude code just by instructing it to place [MAP.md](http://MAP.md) files in every folder it visits to write down a summary of what's in there and how it fits in the repo - and insist on reading them and keeping them up to date.",
          "score": 2,
          "created_utc": "2026-01-30 13:58:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iowf1",
          "author": "planarrebirth",
          "text": "Interesting ideaÔºÅ",
          "score": 1,
          "created_utc": "2026-01-30 01:09:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2jfj0g",
          "author": "TonyDRFT",
          "text": "This sounds really interesting, could it also work together with Neural Graffiti? (Or does it overlap?) Excuse my ignorance, I've only heard some basics when searching for a good way to implement memory...",
          "score": 1,
          "created_utc": "2026-01-30 03:38:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsjn38",
      "title": "Running Ollama fully air-gapped, anyone else?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qsjn38/running_ollama_fully_airgapped_anyone_else/",
      "author": "thefilthybeard",
      "created_utc": "2026-02-01 00:45:31",
      "score": 72,
      "num_comments": 43,
      "upvote_ratio": 0.97,
      "text": "Been building AI tools that run fully air-gapped for classified environments. No internet, no cloud, everything local.\n\nOllama has been solid for this. Running it on hardware that never touches a network. Biggest challenges were model selection (needed stuff that performs well without massive VRAM) and building workflows that don't assume any external API calls.\n\nCurious what others are doing for fully offline deployments. Anyone else running Ollama in secure or disconnected environments? What models are you using and what are you running it on?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qsjn38/running_ollama_fully_airgapped_anyone_else/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o2w2ds9",
          "author": "grudev",
          "text": "Here,¬†\n\n\nThink RAG with classified/sensitive info and some other related workflows.¬†",
          "score": 19,
          "created_utc": "2026-02-01 00:59:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2w40w6",
              "author": "thefilthybeard",
              "text": "Glad to see there are more than just me working on this!",
              "score": 8,
              "created_utc": "2026-02-01 01:09:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2wv4tm",
                  "author": "PrepperDisk",
                  "text": "Found AnythingLLM much better at RAG out of the box, but live Ollama + WebUI for customization and ease of use in non-RAG.",
                  "score": 4,
                  "created_utc": "2026-02-01 03:54:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2x0rsy",
                  "author": "grudev",
                  "text": "There are a few of us,, but everyone keeps it pretty low key and generic.¬†",
                  "score": 4,
                  "created_utc": "2026-02-01 04:32:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2yhc85",
              "author": "joey2scoops",
              "text": "Anyone dabbling in federated learning?",
              "score": 1,
              "created_utc": "2026-02-01 12:02:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ym7i8",
                  "author": "grudev",
                  "text": "Not really... our data is very centralized anyways.¬†\n\n\nI want to work on fine tuning a model for RAFT in the near future.¬†",
                  "score": 1,
                  "created_utc": "2026-02-01 12:41:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2x76rg",
          "author": "DutchOfBurdock",
          "text": "Not specifically Ollama, but I have a single isolated device that naturally lacks WiFi or BT. It does have a camera/mic which are used, so not truly air gapped. It's currently an experimental MCP with a twist: I'm giving function calling models full shell access (inside a container) with the intent on making itself a perfect little home. Spoiler, it has destroyed it's chroot several times.\n\nWhat it can do beautifully at the moment is recognise people and objects seen from the camera (openCV), can hear what I say to it (whisper), think about it and provide a reply (llama) and can even talk back to me (vibevoice). I am even experimenting with a model that can recognise sounds, such as alarms, beeping, smashing etc. (GAMA).",
          "score": 11,
          "created_utc": "2026-02-01 05:17:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2w2vsd",
          "author": "cfipilot715",
          "text": "GPT-OSS is pretty good",
          "score": 6,
          "created_utc": "2026-02-01 01:02:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2w775k",
              "author": "Independent_Solid151",
              "text": "For coding devstral-small-2.",
              "score": 4,
              "created_utc": "2026-02-01 01:28:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2w4t1g",
              "author": "thefilthybeard",
              "text": "I haven't tested that yet!",
              "score": 1,
              "created_utc": "2026-02-01 01:13:38",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2xtsdb",
              "author": "AgreeableIron811",
              "text": "Its pretty slow for the 120b model. And I have a very god server",
              "score": 1,
              "created_utc": "2026-02-01 08:29:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o310m23",
          "author": "Dark_Passenger_107",
          "text": "Yeah, I build tools for the DIB that must run airgapped. The Granite 4 models have been awesome for this. \n\nRecently had to build a tool that runs on mid-tier hardware and works decently well at document scanning + semantic analysis. Used a pipeline of granite3.3-vision:2b for the document ingestion and then granite4:3b-h for analysis/classification. Both models run surprisingly well on CPU only.\n\nFor the production build, ended up going with the Q4 variant (packaged the gguf + llama.cpp in the app).",
          "score": 2,
          "created_utc": "2026-02-01 19:53:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31v72h",
          "author": "thecoder12322",
          "text": "This is exactly the kind of work that needs more visibility! Building for air-gapped/classified environments is challenging but critical.\n\nFor edge deployment scenarios like yours, have you looked into the **RunAnywhere SDK** (it's open source)? It's designed specifically for running LLMs, STT (Whisper), TTS, and VLMs on edge hardware without cloud dependencies. Supports multiple modalities and can run entirely offline once models are loaded.\n\nCurious what hardware specs you're working with and which models you've found perform best in your air-gapped setup? Happy to chat more about edge AI deployment if you want to DM!",
          "score": 2,
          "created_utc": "2026-02-01 22:22:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32ctda",
              "author": "thefilthybeard",
              "text": "Haven't looked into RunAnywhere SDK yet, but I will now... appreciate the tip. Always looking for tools built with offline-first in mind.\n\nHardware specs vary depending on which model I'm running. Quantized 7B models work fine on systems with 16-32GB RAM and no dedicated GPU. When I need more capability, I scale the hardware accordingly. Mistral and Llama variants have been solid for the compliance and document analysis work I'm focused on.\n\nThe key has been batching....breaking tasks into smaller chunks so the model isn't choking on massive context windows. Keeps things fast and accurate.",
              "score": 2,
              "created_utc": "2026-02-01 23:57:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o35ysnt",
          "author": "Desperate-Pattern-54",
          "text": "This resonates a lot. I‚Äôm working on an offline-first, air-gapped RAG system specifically designed for sensitive / disconnected environments ‚Äî no cloud assumptions, no external APIs, and strong hallucination defenses.\nThe focus has been on governance, confidence-gated retrieval, and human-on-the-loop workflows, not just ‚Äúgetting a model to answer.‚Äù\n\nIf useful to others exploring this space, here‚Äôs the reference implementation I‚Äôve been building:\n\n https://github.com/drosadocastro-bit/nova_rag_public\n\nWould love to compare notes on model selection and workflows that behave well under strict offline constraints.",
          "score": 2,
          "created_utc": "2026-02-02 15:04:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2wg82i",
          "author": "silencedshotty76",
          "text": "Idk why you'd use ollama the model's ollama leverages are gguf models that's the core component.\n\nIf I were you I'd stick to llama-server to serve the gguf model files. in addition llama-server has the option to enable the --offline flag if you're looking for an offline only chat.\nTo address the airgapping mechanism, you're fine as long as the chat interface is restricted to the terminal since a gui chat interface has the potential to be saved in the memory, disk.",
          "score": 1,
          "created_utc": "2026-02-01 02:22:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wsfnh",
              "author": "thefilthybeard",
              "text": "Good call on llama-server, I'll look into that. The offline flag is exactly the kind of thing I need.\n\nFor my use case, terminal-only is actually preferred. These environments have strict requirements about what gets written to disk and where. GUI adds unnecessary risk.\n\nYou running anything air-gapped yourself or just familiar with the setup?",
              "score": 1,
              "created_utc": "2026-02-01 03:37:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2x17vm",
                  "author": "grudev",
                  "text": "Ollama is fine, especially if you use multiple models.¬†",
                  "score": 1,
                  "created_utc": "2026-02-01 04:35:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zm8yq",
          "author": "BidWestern1056",
          "text": "i built npcpy and npcsh and incognide purposefully to work well with local models/ollama so that users can build more powerful applications with them and make the most of them through better interface design\n\n[https://github.com/npc-worldwide/npcpy](https://github.com/npc-worldwide/npcpy)\n\n[https://github.com/npc-worldwide/incognide](https://github.com/npc-worldwide/incognide)\n\n[https://github.com/npc-worldwide/npcsh](https://github.com/npc-worldwide/npcsh)",
          "score": 1,
          "created_utc": "2026-02-01 16:02:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2zxpr3",
          "author": "sinan_online",
          "text": "I did the same with HuggingFace, first you cache the model, then run it.\n\nI also created some containerized models with Ollama, you can run them using docker locally, airtight as you say, once you download. They are on docker hub. I am experimenting with llama now that it has a POST streaming  endpoint.",
          "score": 1,
          "created_utc": "2026-02-01 16:55:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31mfjb",
          "author": "searchblox_searchai",
          "text": "We integrate with Ollama for our SearchAI application and run in air gapped environments https://developer.searchblox.com/docs/llm-platform-configuration#ollama-native-api",
          "score": 1,
          "created_utc": "2026-02-01 21:39:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o32yy72",
          "author": "smcgann",
          "text": "Glad to see this brought up!   I‚Äôm overseeing (from a compliance perspective) several projects that are just starting and planning my own projects.  Looking to help out software developers with coding models.  PEFT and RAG for various uses.  As well as agents for data analytics.",
          "score": 1,
          "created_utc": "2026-02-02 02:01:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o331kv6",
          "author": "Odd_Butterfly_455",
          "text": "I preprocess all my documents with a agent to qdrant and connect qdrant to the model afterwards... I'm on this since 1 month at my job it's kinda work but need improvement",
          "score": 1,
          "created_utc": "2026-02-02 02:16:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o30dz6n",
          "author": "r00tdr1v3",
          "text": "How are you getting the models on the air-gapped hardware?",
          "score": 0,
          "created_utc": "2026-02-01 18:09:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o317ok3",
              "author": "thefilthybeard",
              "text": "Pre-built installation package transferred via approved media. The client's sysadmin handles the actual install on their air-gapped network while I'm on-site to walk through the process and verify everything works.\n\nFor updates, they pull them from a secure client portal - no direct internet connection needed on their end. They download the update package from the portal on an unclassified system, transfer it in through their normal media approval process, and apply it.",
              "score": 2,
              "created_utc": "2026-02-01 20:27:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2wtkek",
          "author": "CooperDK",
          "text": "Just fully firewall it. *sigh*\nThat said, most models cannot access the internet and those who can, do because you tell them to",
          "score": -6,
          "created_utc": "2026-02-01 03:44:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xrm93",
              "author": "tom-mart",
              "text": "I think you misunderstanding the issue. Firewall will not help if you are voluntarily sending your sensitive data to LLM api\n\n\nhttps://www.bbc.co.uk/news/articles/cdrkmk00jy0o\n\n\nOthers are not any more secure.",
              "score": 2,
              "created_utc": "2026-02-01 08:09:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2yl44k",
                  "author": "CooperDK",
                  "text": "No, but you do not do that if you want to protect your information. Then you inference locally.\nI do think the OP mentioned NOT using an API.",
                  "score": -1,
                  "created_utc": "2026-02-01 12:32:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2xbqs6",
              "author": "trolololster",
              "text": "no model can \"access the internet\" without you specifically giving it a tool to do so. wth have you been smoking, lol.",
              "score": 2,
              "created_utc": "2026-02-01 05:52:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ylbcr",
                  "author": "CooperDK",
                  "text": "You do know that we, according to the OP, are talking local LLMs, right?",
                  "score": 1,
                  "created_utc": "2026-02-01 12:34:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2y1c0f",
          "author": "UnbeliebteMeinung",
          "text": "Wtf are you classifying that you need a air gapped computer.",
          "score": -4,
          "created_utc": "2026-02-01 09:40:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y8jvt",
              "author": "tecneeq",
              "text": "Thats classified.",
              "score": 6,
              "created_utc": "2026-02-01 10:46:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2y8rvr",
                  "author": "UnbeliebteMeinung",
                  "text": "Probably some 3 Body Problem stuff where the enemy is an allmyghty alien.",
                  "score": 1,
                  "created_utc": "2026-02-01 10:48:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qs233a",
      "title": "Best open weight llm model to run with 8gb of vram",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qs233a/best_open_weight_llm_model_to_run_with_8gb_of_vram/",
      "author": "Sweazou",
      "created_utc": "2026-01-31 13:16:18",
      "score": 49,
      "num_comments": 37,
      "upvote_ratio": 0.93,
      "text": "I'd like to get your thought on the best model you can use with 8gb of vram in 2026, with the best performance possible for general purpose and coding, the least censorship possible, i know this won't be as good as state of the art llm but i'd like to try something good i can run locally",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qs233a/best_open_weight_llm_model_to_run_with_8gb_of_vram/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o2snxjt",
          "author": "alhinai_03",
          "text": "Gpt-oss 20b, while offloading experts to ram and keeping routers and kv cache in vram. It would run pretty fast.",
          "score": 17,
          "created_utc": "2026-01-31 14:45:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2tr0u6",
              "author": "Admirable-Choice9727",
              "text": "Yup",
              "score": 1,
              "created_utc": "2026-01-31 17:57:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2tu2ni",
              "author": "Daepilin",
              "text": "do you have a starting point on setting that up? I started some local stuff with Ollama and CrewAI, but so far have not seen a way to do things like offloading the experts, etc",
              "score": 1,
              "created_utc": "2026-01-31 18:11:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2uogh8",
                  "author": "alhinai_03",
                  "text": "As the other person said, you gonna need llama.cpp, download prebuilt binaries or build them yourself.\n\nRun with these flags\n```\n--n-gpu-layers -1\n--cpu-moe\n\nyou also can tweak this further with\n--n-cpu-moe N\n```",
                  "score": 3,
                  "created_utc": "2026-01-31 20:37:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2udi6b",
                  "author": "Zestyclose-Shift710",
                  "text": "You do that with llama.cpp and setting the correct arguments, --cpu-moe specifically¬†\n\n\nMaybe ollama can do that too but idk\n\n\n\nArguably the new router mode, webui and models.ini config style of llama.cpp make ollama obsolete",
                  "score": 2,
                  "created_utc": "2026-01-31 19:43:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2tr6bn",
              "author": "Admirable-Choice9727",
              "text": "But kv cache causes context rot In my experience",
              "score": -1,
              "created_utc": "2026-01-31 17:57:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2uqgw1",
                  "author": "alhinai_03",
                  "text": "You're not making sense, If you meant running quantized kv cache this is not it. Kv cache quantization is done with ```--cache-type-k q8_0``` which you don't need it unless you're really tight on vram.",
                  "score": 5,
                  "created_utc": "2026-01-31 20:46:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ud51y",
                  "author": "Zestyclose-Shift710",
                  "text": "What",
                  "score": 3,
                  "created_utc": "2026-01-31 19:41:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2t1b0a",
          "author": "abalancer",
          "text": "I have found ministral-3:8b to be pretty good all around, it's fast, not very censored (although some queries are blocked), has good image recognition, can read pdfs (ministral-3:14b will read them more thoroughly), and gives some of the best answers for 8 billion and 14 billion parameters. I used to use qwen3:14b for most of my tasks but ministral-3:8b has largely taken that role now.",
          "score": 7,
          "created_utc": "2026-01-31 15:53:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sc4j7",
          "author": "sinan_online",
          "text": "I am running Gemma3 1B on 6GB VRAM. Contender is the equivalent Qwen3, but for now I am settled on Gemma3. (It is also general purpose for me, I actually test some of my flows, it‚Äôs integrated to one oy my test harnesses.)\n\nEdit: Gemma3 4B was too large for the VRAM.",
          "score": 6,
          "created_utc": "2026-01-31 13:36:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2udlma",
              "author": "Zestyclose-Shift710",
              "text": "Isn't 4b around 1gb quantized",
              "score": 1,
              "created_utc": "2026-01-31 19:43:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2uuc4h",
                  "author": "sinan_online",
                  "text": "I used Ollama, I didn‚Äôt check the precision of the floating points when I pulled the Qwen3 models. They are both the standard on Ollama.\n\nNow that llama has a standard API support, I am making a switch, and that will allow me to be a bit more selective with that, so long as there is a GGUF. I‚Äôll see what fits in, I am starting with Gemma3 270m to get the containerization pipeline going.",
                  "score": 1,
                  "created_utc": "2026-01-31 21:06:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2u1ock",
          "author": "jimmyfoo10",
          "text": "For me I got 8gb vram and 32 gb ram cpu. I find really nice performance/quality in qwen2.5 14b and deepseek 14b\nGTP-oss 20b is nice but quite heavy \nI find sweet spot around 14b parameters \nAnd better performance if I use from other computer, when using desktop and running openwebui in the same computer is a bit laggy‚Ä¶",
          "score": 7,
          "created_utc": "2026-01-31 18:46:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sox5e",
          "author": "seangalie",
          "text": "Chiming in to say that your best bets are some of the new MoE models.... gpt-oss:20b will likely impress with performance even with 8GB VRAM but you may not love the censorship based on your post.  You can always YOLO and use huihui AI's abliterated gpt-oss:20b for the uncensored version.\n\nOther than that - Qwen3's 30b-a3b architecture works surprisingly well in VRAM constraints if you have the system RAM for the spillover (which would include qwen3-coder in the same sizing).  Nemotron 3 Mini is only a little bit larger of a foot print and may fit in the same VRAM constraints.",
          "score": 4,
          "created_utc": "2026-01-31 14:50:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tk3fc",
          "author": "Sweazou",
          "text": "thank you very much for all your answers, i'll try them all",
          "score": 1,
          "created_utc": "2026-01-31 17:23:45",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2tt5su",
          "author": "ykushch",
          "text": "It is depends what you‚Äôre planning to do with it. I tested my extension with qwen2.5-coder:7b - works fine. Tried to use 'qwen3:8b' and it was a lot slower especially on my machine comparing to 'qwen2.5-coder:7b'. Quality-wise 'qwen3:8b' follows instructions a bit better, but making it a default one the experience will look less pleasant. Especially for such extension like this one \n\nhttps://github.com/ykushch/ask",
          "score": 1,
          "created_utc": "2026-01-31 18:07:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2uc2cw",
          "author": "swipegod43",
          "text": "something with 10-12b parameters would be decent",
          "score": 1,
          "created_utc": "2026-01-31 19:36:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2uc3g6",
              "author": "swipegod43",
              "text": "with 4 bit quantization",
              "score": 1,
              "created_utc": "2026-01-31 19:36:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2uecma",
          "author": "Zestyclose-Shift710",
          "text": "Either 7-8b class models in vram, 12-14b splitting them between ram and vram, or 20-30b moes with expert offloading and kv in vram\n\n\nAs for specific models, check out latest qwens (3VL), ministral 3 series, granite 4 for rag, and qwen3 30b a3b, nemotron 3 nano, gpt oss 20b and glm 4.7 flash as for the moe models\n\n\nThose last ones have pruned REAP variants that are a bit smaller, too",
          "score": 1,
          "created_utc": "2026-01-31 19:47:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ug0ho",
          "author": "Dzhmelyk135",
          "text": "Either qwen3 in 4/8b or if you can find it (I haven't) GPT-OSS 20b in like Q3 or even Q2 (yes they make them, I have Qwen3Coder 30b on a 16 GB GPU)",
          "score": 1,
          "created_utc": "2026-01-31 19:55:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2x485c",
          "author": "No_Cartographer7065",
          "text": "Made GPT-OSS 0.6b parameter model on hugging face just make agentic loop iterate through one pass. It comes with built in web search too.",
          "score": 1,
          "created_utc": "2026-02-01 04:56:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yif88",
          "author": "Potential_Code6964",
          "text": "I'm currently running nemotron-3-nano:30b with a 3060TI, Ryzen 7, 64GB.",
          "score": 1,
          "created_utc": "2026-02-01 12:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3157a1",
          "author": "CorrGL",
          "text": "Ministral, it comes in various sizes, see the biggest you can fit",
          "score": 1,
          "created_utc": "2026-02-01 20:15:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtine0",
      "title": "Released: VOR ‚Äî a hallucination-free runtime that forces LLMs to prove answers or abstain",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qtine0/released_vor_a_hallucinationfree_runtime_that/",
      "author": "CulpritChaos",
      "created_utc": "2026-02-02 02:31:03",
      "score": 42,
      "num_comments": 19,
      "upvote_ratio": 0.84,
      "text": "I just open-sourced a project that might interest people here who are tired of hallucinations being treated as ‚Äújust a prompt issue.‚Äù\nVOR (Verified Observation Runtime) is a runtime layer that sits around LLMs and retrieval systems and enforces one rule:\nIf an answer cannot be proven from observed evidence, the system must abstain.\nHighlights:\n0.00% hallucination across demo + adversarial packs\nExplicit CONFLICT detection (not majority voting)\nDeterministic audits (hash-locked, replayable)\nWorks with local models ‚Äî the verifier doesn‚Äôt care which LLM you use\nClean-room witness instructions included\nThis is not another RAG framework.\nIt‚Äôs a governor for reasoning: models can propose, but they don‚Äôt decide.\nPublic demo includes:\nCLI (neuralogix qa, audit, pack validate)\nTwo packs: a normal demo corpus + a hostile adversarial pack\nFull test suite (legacy tests quarantined)\nRepo: https://github.com/CULPRITCHAOS/VOR\nTag: v0.7.3-public.1\nWitness guide: docs/WITNESS_RUN_MESSAGE.txt\nI‚Äôm looking for:\nPeople to run it locally (Windows/Linux/macOS)\nIdeas for harder adversarial packs\nDiscussion on where a runtime like this fits in local stacks (Ollama, LM Studio, etc.)\nHappy to answer questions or take hits. This was built to be challenged.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qtine0/released_vor_a_hallucinationfree_runtime_that/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3353yy",
          "author": "newbietofx",
          "text": "What's the difference between this and doing promoting to force llm to answer i don't have the answers versus expanding the pipeline to allow rag or integrating scraping function to pull from the internet?¬†",
          "score": 7,
          "created_utc": "2026-02-02 02:36:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o335f0e",
              "author": "CulpritChaos",
              "text": "Short answer:\nThose approaches try to encourage the model not to hallucinate. VOR removes the model‚Äôs authority to answer at all unless the answer is provably grounded.\nLonger, concrete difference:\nPrompting ‚Äúsay I don‚Äôt know‚Äù\nThis is behavioral.\nThe model still decides whether it ‚Äúknows‚Äù something.\nFailure mode: confident-sounding abstention or quiet fabrication.\nYou can‚Äôt audit why it answered or abstained ‚Äî only that it did.\nVOR doesn‚Äôt trust the model‚Äôs self-assessment. The model can propose an answer, but it cannot finalize it.\nExpanding the pipeline (RAG, scraping, tools)\nRAG increases information availability, not correctness.\nScraping just widens the attack surface.\nThe model still performs the final reasoning step and can:\nMix sources\nMisattribute evidence\nHallucinate joins or relationships\nMost RAG systems fail silently ‚Äî you don‚Äôt know when they‚Äôre wrong.\nVOR treats retrieval as observations, not truth.\nIf retrieved evidence is insufficient, conflicting, or ambiguous ‚Üí ABSTAIN, even if the model ‚Äúthinks it knows.‚Äù\nWhat VOR actually changes\nThe model is demoted to a proposer, not an authority.\nAll answers must:\nBe derivable from observed evidence\nPass conflict checks\nBe replayable and auditable\nOtherwise the system returns:\nABSTAIN or CONFLICT ‚Äî explicitly\nThis is closer to a runtime governor than a prompting or RAG trick.\nMental model:\nPrompting = asking the driver to be careful\nRAG = giving the driver more maps\nVOR = installing brakes that physically prevent lying\nHappy to go deeper if you want ‚Äî especially around failure modes RAG can‚Äôt detect.",
              "score": 4,
              "created_utc": "2026-02-02 02:38:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o336tq2",
                  "author": "newbietofx",
                  "text": "It sounds good that you have a bull sh*it filter. How do you ensure the filter is collecting dust or stopping air flow altogether? I don't have a metaphor for this analogy.¬†",
                  "score": 6,
                  "created_utc": "2026-02-02 02:46:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o33jkoi",
                  "author": "Outrageous-Split-646",
                  "text": "Can you answer the question without relying on an LLM?",
                  "score": 6,
                  "created_utc": "2026-02-02 04:02:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33uxux",
          "author": "ExtentOdd",
          "text": "I read through the [README.md](http://README.md) and I think it would be best if you could include the result or video of the demon on the README. Although this is potential, It could not justify cloning this repo and trying to run before I understand what it does.",
          "score": 5,
          "created_utc": "2026-02-02 05:19:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33v80w",
              "author": "CulpritChaos",
              "text": "No problem! Check back tomorrow if you'd like and I'll have it resolved by then. Thank you for your feedback!",
              "score": 2,
              "created_utc": "2026-02-02 05:21:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o33bs69",
          "author": "Intercellar",
          "text": "great work!!",
          "score": 3,
          "created_utc": "2026-02-02 03:14:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33bv3m",
              "author": "CulpritChaos",
              "text": "Thank you!",
              "score": 2,
              "created_utc": "2026-02-02 03:15:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qu4ka4",
      "title": "Recommendations for a good value machine to run LLMs locally?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qu4ka4/recommendations_for_a_good_value_machine_to_run/",
      "author": "onesemesterchinese",
      "created_utc": "2026-02-02 19:14:51",
      "score": 33,
      "num_comments": 44,
      "upvote_ratio": 0.98,
      "text": "Thinking of purchasing a machine in the few thousand $ range to work on some personal projects. Would like to hear if anyone has any thoughts or positive/negative experiences running inference with some of the bigger open models locally or with finetuning? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qu4ka4/recommendations_for_a_good_value_machine_to_run/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o37ioct",
          "author": "ZeroSkribe",
          "text": "Look at rtx 5060ti 16GB, get two if you can",
          "score": 11,
          "created_utc": "2026-02-02 19:23:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37lets",
              "author": "v01dm4n",
              "text": "Two would also need 2x expensive motherboard. Better to get a gpu with more vram with that budget. Say rtx pro 4000 with 24g vram.",
              "score": -5,
              "created_utc": "2026-02-02 19:35:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o38q7sl",
                  "author": "windumasta",
                  "text": "\"2x une carte m√®re ch√®re\" ? si non une carte m√®re avec 2 pcie 8x ?",
                  "score": 1,
                  "created_utc": "2026-02-02 22:51:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o38xglb",
                  "author": "ZeroSkribe",
                  "text": "no you wouldn't, even the cheapest motherboards come with 2 connectors, what you meant to say was you'll need a bigger power supply..duhhhhhh",
                  "score": 1,
                  "created_utc": "2026-02-02 23:30:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3aavni",
                  "author": "Big-Masterpiece-9581",
                  "text": "There is no reasonable priced new gpu with 24gb. New Intel b60 or old ass Nvidia RTX 3090 used for \nmining are both $800-900. For that price can get 32gb with two new 5060ti and for inference speeds will be fine.",
                  "score": 1,
                  "created_utc": "2026-02-03 04:14:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37kn8l",
          "author": "germanpickles",
          "text": "You can consider getting a Mac to run local LLM‚Äôs. Mac‚Äôs have something called Unified Memory where the vRAM and system RAM are shared on the same bus. So if you have 64 GB of Unified Memory, you will be able to load a lot of different local models. While Ollama is a great tool, you can also look at LM Studio which has a MLX inference engine which allows you to run MLX LLM‚Äôs very fast on a Mac.",
          "score": 12,
          "created_utc": "2026-02-02 19:32:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38rnlo",
              "author": "n2itus",
              "text": "This is really good advice.  I've just started down this journey and was surprised as how well my Macbook Air with 16gb ran local models.  I started looking at mac minis with 64 gb ... as memory is very important. You could not get 64 gb from video cards for as cheap as getting a 64 gb mac mini.",
              "score": 9,
              "created_utc": "2026-02-02 22:58:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o37mc0j",
              "author": "onesemesterchinese",
              "text": "Nice, I was looking at the mac mini -- You think that would work for finetuning some of the smaller models?",
              "score": 2,
              "created_utc": "2026-02-02 19:40:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o37rl6l",
                  "author": "st0ut717",
                  "text": "I am running Gemma 3b.  On my Mac air m4 with 16gb ram.",
                  "score": 5,
                  "created_utc": "2026-02-02 20:04:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o38c77n",
                  "author": "germanpickles",
                  "text": "Absolutely. Do check out Alex Ziskind on YouTube - https://youtube.com/@azisk?si=Q7cXou-ONnH0IP-P, he has so many videos on running local LLM‚Äôs on Mac as well as DGX Spark etc",
                  "score": 1,
                  "created_utc": "2026-02-02 21:42:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3b4a3g",
                  "author": "arqn22",
                  "text": "I believe that fine tuning is especially slow on Mac chips, but you should verify that.  I have a 64gb MacBook pro and it can run a lot of models at decent speeds as long as context window isn't too high, not over 16k-32k (or an advanced attention mechanism is in place like in nemotron-nano-3 which easily handles 128 or probably more).  It's still significantly slower than GPUs with less RAM for models that fit in them though.  And I believe I've read that it's extra slow for training/fine-tuning.  But I don't have a source for that.",
                  "score": 1,
                  "created_utc": "2026-02-03 08:11:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o39s626",
              "author": "rorowhat",
              "text": "Lol don't. If anything get a strict halo",
              "score": 0,
              "created_utc": "2026-02-03 02:21:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o38xv46",
              "author": "ZeroSkribe",
              "text": "the mac mini slop is getting old, please research GPU memory speeds vs unified speeds. Its not worth it if every request takes forever.",
              "score": -5,
              "created_utc": "2026-02-02 23:32:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o39fhsr",
                  "author": "germanpickles",
                  "text": "Everyone's experience with speed on a Mac Mini vs GPU will really depend on the specs of the machine, the inference engine, quantization and the model. From my own real world experience, using local AI every day, I'm getting 104 tok/sec. Here are the specs:\n\n* Mac Mini with M4 Pro\n* 14 core CPU/20 core GPU\n* 64GB UMA (Unified Memory Architecture)\n* Llama 3.2 3B 4-bit MLX\n\nRunning the same model (Q4\\_K\\_M) on my RTX 5080, I get 276 tok/sec. Is it faster? Yes. But is the Mac still usable? Of course.\n\nNow of course, if someone is running the entry level Mac Mini and loading a 70 billion parameter model, it will be extremely slow.",
                  "score": 3,
                  "created_utc": "2026-02-03 01:09:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37mqsd",
          "author": "photobydanielr",
          "text": "The Ryzen ai max 395+ 128GB shared memory blah blah named machines come to mind. Good value if the models you want to use fit.",
          "score": 5,
          "created_utc": "2026-02-02 19:42:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37roo3",
              "author": "cjc4096",
              "text": "Rocm 7.2 fixed a lot of stability issues on 395+ too.",
              "score": 2,
              "created_utc": "2026-02-02 20:05:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37s30n",
                  "author": "photobydanielr",
                  "text": "I own the flow z13 128GB and boy has it been a wild ride",
                  "score": 2,
                  "created_utc": "2026-02-02 20:07:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37o98j",
          "author": "band-of-horses",
          "text": "You can spend thousands of dollars on a machine that can run a model that is still a fraction as capable as any of the cloud hosted models. If your motivation is to save money it's probably not worth it. If it's privacy there are cloud options for open models with no retention. If you just want to run small models locally for fun a cheap machine with 32 of ram or a 16gb c video card will do it.",
          "score": 9,
          "created_utc": "2026-02-02 19:49:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37wvto",
              "author": "mtbMo",
              "text": "That‚Äôs right. Your local LLM instance will never ever compete with a trillion params models from cloud inference provider.\nYou can run local models and achieve good results.\n\nI‚Äôm running a Dell workstation Xeon v4 256gb ram and three GPUs with 50GB VRAM all second hand\nIt‚Äôs up to 37t/s with drawing 500W from the wall",
              "score": 3,
              "created_utc": "2026-02-02 20:30:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37z6ji",
                  "author": "mon_key_house",
                  "text": "What GPUs do you have? I currently have 2x Rtx3060 and thinking about going 4x or getting 2x rtx 3090.",
                  "score": 1,
                  "created_utc": "2026-02-02 20:40:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o383h04",
              "author": "BoostedHemi73",
              "text": "This is so depressing. We‚Äôre just supposed to be beholden to cloud companies for this stuff? That‚Äôs going to push everyone into like three places.\n\nComputers used to be so fun. This timeline sucks.",
              "score": 3,
              "created_utc": "2026-02-02 21:01:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3868lu",
                  "author": "goodguybane",
                  "text": "Running local LLMs IS fun, you just have to manage your expectations.",
                  "score": 6,
                  "created_utc": "2026-02-02 21:14:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37hoew",
          "author": "DutchGM",
          "text": "I am currently testing the Dell GB10 spark. I see potential but I‚Äôm still benchmark testing it.",
          "score": 3,
          "created_utc": "2026-02-02 19:18:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37liwz",
          "author": "AmphibianFrog",
          "text": "I'm not convinced there is a good value machine to run LLMs locally! If I was on a budget I would build a pc with a single RTX3090.",
          "score": 3,
          "created_utc": "2026-02-02 19:36:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37ws4q",
          "author": "st0ut717",
          "text": "Large models are not always the answer. \nWhat do you want to do?  \n\n\nMac mini if you want a general purpose pc that can also do llm \nOtherwise a dell gb10.  Or the like if you want a dedicated llm work station",
          "score": 3,
          "created_utc": "2026-02-02 20:29:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o382b5w",
          "author": "Possible-Algae4391",
          "text": "An older X99 workstation board, a¬†cheap Xeon CPU, 2x3060 GPU (2x12GB)\n\n\nIt was enough for me, I could run 30b models with reasonable speed (still far from what we're used with cloud models). It cost me about 1000$ with used parts.\n\n\nYou can go for beefier 3090s but then you might want a newer board with PCIe 4.",
          "score": 3,
          "created_utc": "2026-02-02 20:55:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39q42d",
          "author": "8ballfpv",
          "text": "I just built myself a machine to mess around with:\n\n\\- Chassis - supermicro SYS-7049GP-TRT\n\n\\- CPU - Dual Intel Xeon Gold 6240\n\n\\- Ram - 165GB DDr4\n\n\\- GPU - Dual rtx 3090\n\nplaying around with various models atm.  its nice to have the flexability with the 2 gpus. Means I can run a 70b size or 2 30b sizes ( 1 on each gpu) or many smaller ones depending what I want.  \nGot the hardware piping out to a grafana dashboard so I can keep an eye on the basics realtime, Run openwebui on another machine and Ollaman for quick maintenance of models etc etc. All up cost me about $5k aud.",
          "score": 3,
          "created_utc": "2026-02-03 02:09:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38ybep",
          "author": "dipoots_",
          "text": "MBP Pro M5 at least 24 gb ram or Mac mini equivalent",
          "score": 2,
          "created_utc": "2026-02-02 23:34:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o394rv9",
          "author": "XdtTransform",
          "text": "These are all fine suggestions, but if you are planning to do something serious, you need, at the very least, a ~30b type model.  Something like Gemma3:27b.  This means at least a card with 24GB of VRAM.  \n\nThe other alternatives I've seen suggested like various 3b models fall apart at anything serious or with a larger context window.",
          "score": 2,
          "created_utc": "2026-02-03 00:10:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3a5rdi",
          "author": "recoverygarde",
          "text": "I would recommend get either a m5 14in MBP or wait for the rest of the lineup to get the M5 gen",
          "score": 1,
          "created_utc": "2026-02-03 03:41:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3a9y18",
          "author": "m-gethen",
          "text": "I just built a really solid budget-constrained PC for a friend to use for a) 3D CAD in Solidworks and Blender and b) so he could start down the local LLM path. He‚Äôs been very happy so far.\n\nhttps://preview.redd.it/h0fowxpfe7hg1.jpeg?width=4624&format=pjpg&auto=webp&s=09669791be012602f9e57f633cc67e4d5961a062\n\nKey specs‚Ä¶\n\n* Intel Core Ultra 5 245K\n* Gigabyte Z890M motherboard (automatically bifurcates and runs GPUs at PCIE 8)\n* 32Gb DDR5-6400 memory\n* 2x Intel Arc B580 12Gb (pooled for total 24Gb VRAM).\n\nOllama and LM Studio working smoothly with Vulcan and newest 32B MoE models like Granite 4 H in this set up.",
          "score": 0,
          "created_utc": "2026-02-03 04:07:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39e4p0",
          "author": "Seninut",
          "text": "Follow Jenson around with a pooper scooper, I am sure one will come out.",
          "score": -2,
          "created_utc": "2026-02-03 01:01:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr4blw",
      "title": "My first Local LLM",
      "subreddit": "ollama",
      "url": "https://i.redd.it/bxspnf0rehgg1.jpeg",
      "author": "swipegod43",
      "created_utc": "2026-01-30 12:38:17",
      "score": 24,
      "num_comments": 0,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qr4blw/my_first_local_llm/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qoi1op",
      "title": "Tiny Ollama-powered CLI: Natural Language to shell commands (qwen2.5-coder default, easy model switch)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qoi1op/tiny_ollamapowered_cli_natural_language_to_shell/",
      "author": "ykushch",
      "created_utc": "2026-01-27 16:08:37",
      "score": 24,
      "num_comments": 11,
      "upvote_ratio": 0.93,
      "text": "CLI that uses Ollama locally to translate natural language into shell commands. Supports `--model` / `ASK_MODEL` and `OLLAMA_HOST`.  \nRepo: [https://github.com/ykushch/ask](https://github.com/ykushch/ask)\n\n[ask - natural language to shell commands](https://i.redd.it/k5ldp45d1xfg1.gif)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qoi1op/tiny_ollamapowered_cli_natural_language_to_shell/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o21gll7",
          "author": "j0x7be",
          "text": "Seems nice! Which shells does it support?",
          "score": 2,
          "created_utc": "2026-01-27 16:23:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21mr12",
              "author": "ykushch",
              "text": "Thanks! It executes commands via sh -c so your interactive shell (bash, zsh, fish, etc.) doesn't matter.",
              "score": 1,
              "created_utc": "2026-01-27 16:50:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o21q5xz",
                  "author": "j0x7be",
                  "text": "Cool, I have to give a try! Pretty comfortable bash user for years, but could be handy when I'm forced to use alternatives. \n\nAnd it's going to be awesome for those starting out, just running 'find' can be intimidating enough for some. Will recommend the apprentices at work to look into this.",
                  "score": 1,
                  "created_utc": "2026-01-27 17:05:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o22e0g1",
          "author": "Available-Craft-5795",
          "text": "Qwen2.5 is pretty old, I would recommend using Qwen3, its miles better",
          "score": 1,
          "created_utc": "2026-01-27 18:47:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22k0f7",
              "author": "ykushch",
              "text": "I picked up coder model as 2.5 had it in relatively small size. But the 3.0 doesn't have one per my memory hence decided to go with 2.5 more specialized one. Does the general 3.0 is a better one that 2.5 coder?",
              "score": 2,
              "created_utc": "2026-01-27 19:12:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o253e08",
                  "author": "shikima",
                  "text": "I have the same question ‚ùì I'm using qwen3 instruct but only for debug errors in journalctl, not for coder",
                  "score": 1,
                  "created_utc": "2026-01-28 02:33:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o24ycuh",
          "author": "tavigsy",
          "text": "I have the memory of a goldfish so this could really come in handy! ¬†Cool!",
          "score": 1,
          "created_utc": "2026-01-28 02:07:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b8oxo",
          "author": "dryEther",
          "text": "This is awesome.. :( I was mid way of building something very similar to this for windows. But the idea is out there now. This will be very useful and can completely change how people use terminals. Terminal will be accessible to.non technical.people also now.",
          "score": 1,
          "created_utc": "2026-01-28 23:25:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bb0j6",
              "author": "ykushch",
              "text": "Thanks! I believe that reading docs and googling now is becoming a bit dated hence decided to create a small handy helper. Also it's a lot easier than copy-paste from-to LLM.",
              "score": 2,
              "created_utc": "2026-01-28 23:37:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qrkbsr",
      "title": "Run Ollama on your Android!",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qrkbsr/run_ollama_on_your_android/",
      "author": "DutchOfBurdock",
      "created_utc": "2026-01-30 22:32:38",
      "score": 24,
      "num_comments": 10,
      "upvote_ratio": 0.9,
      "text": "Want to put this out here. I have a Samsung S20 and  a Pixel 8 Pro. Both of these devices pack 12GB of RAM, one an octacore arrangement and the other a nonacore. Now, this is pure CPU and even Vulkan (despite hardware support), doesn't work.\n\nFirst, get yourself Termux from F-Droid or GitHub. **Don't use the Play Store version.**\n\nUpon launching Termux, update the package manager and install some things needed..\n\n    pkg up\n    pkg i build-essential git cmake golang\n    git clone https://github.com/ollama/ollama.git\n    cd ollama\n    go generate ./...\n    go build .\n\nIf all went well, you'll end up with an `ollama` executable in the folder.\n\n    ./ollama serve\n\nOpen a new terminal in the gitted ollama folder \n\n    ./ollama pull smollm2\n    ./ollama run smollm2\n\nThis model should be small enough for even 4GB devices and is pretty fast.\n\nEnjoy and start exploring!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qrkbsr/run_ollama_on_your_android/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o2rcrgy",
          "author": "GutenRa",
          "text": "Pocket Pal - good enough for run what you want from HF. Or Google edge gallery with gemma-3n e4b - one of the best universal model for mobile.",
          "score": 8,
          "created_utc": "2026-01-31 08:37:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ulbh9",
              "author": "DutchOfBurdock",
              "text": "Nice find! I do like Ollama, it's literal childs play getting up and running with self hosted A.I. Also, being in a terminal makes it extra nerdy üòÇ\n\nI've been trying hard as heck to utilize the Vulkan hardware, vkinfo in Termux shows support and a shared upto 2GB VRAM on my Pixel 8 Pro. Just, cant get it to be seen in llama directly or Ollama",
              "score": 1,
              "created_utc": "2026-01-31 20:21:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qau0l",
          "author": "mlt-",
          "text": "What does `ollama ps` show while running? Somehow I'm skeptical there are accelerated backends.\n\nAlso‚Ä¶ what is up with termux from play store?",
          "score": 5,
          "created_utc": "2026-01-31 03:35:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2quro4",
              "author": "DutchOfBurdock",
              "text": ">this is pure CPU\n\nBecause of how Termux works, it has to be built targeting an older SDK, else it just wouldn't work in Android 10+ (W\\^X). Because of this, it can't be updated on Play anymore.",
              "score": 3,
              "created_utc": "2026-01-31 05:56:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qut36",
          "author": "AwayLuck7875",
          "text": "Ollama apk ??",
          "score": 3,
          "created_utc": "2026-01-31 05:56:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qx7zd",
              "author": "DutchOfBurdock",
              "text": "Native binary in a Terminal.",
              "score": 2,
              "created_utc": "2026-01-31 06:16:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rwqir",
          "author": "palec911",
          "text": "What is realistically achievable to run as a model and use case? And how hot would the device get, I mean how tasking on the device would it be?",
          "score": 3,
          "created_utc": "2026-01-31 11:44:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2s2to0",
              "author": "DutchOfBurdock",
              "text": "Text summarisation, chat bot function, function calling (MCP). One use case is having it work with Tasker for Android and WhatsApp. Tasker intercepts WhatsApp notification, sends the chat to Ollama's API to provide a reply. Tasker then uses the Reply function of the notification to reply to the chat.\n\nOllama tool calling can run commands in Termux bound to `am` so that intents can be sent to Tasker and have Tasker as its MCP.",
              "score": 2,
              "created_utc": "2026-01-31 12:33:42",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2us34m",
              "author": "DutchOfBurdock",
              "text": "I didn't fully answer your question, regarding the taxing. I'm using two higher end devices packing a lot of RAM and decent SoCs. The model mentioned gets up to 12.3tps on the P8P, and up to 7.2 on the S20.\n\nI can often get 4b models running on them, but we're talking much slower tps rates.",
              "score": 2,
              "created_utc": "2026-01-31 20:55:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2uqfq0",
          "author": "DutchOfBurdock",
          "text": "I will use this comment post to share what I've used this for. Remember, when Ollama is serving, it's API is available on loopback.\n\nI'm a Tasker user, so want Tasker to be my MCP. This can be achieved by calling functions via Termux, which can Intent other apps (`am` utility is available to Termux, as well as Termux:API). I started simple and will share more as these methods become matured (keep posted on r/Tasker)\n\n**Chat to Ollama from Tasker**\n\n_This is one shot without any context of previous chats_ The model is a custom model called `sim`\n\n\n    Task: Ollama\n    \n    A1: Get Voice [\n         Title: What to ask rudeboy?\n         Language Model: Free Form\n         Maximum Results: 1\n         Timeout (Seconds): 40 ]\n    \n    A2: HTTP Request [\n         Method: POST\n         URL: http://localhost:11434/api/generate \n         Body: {\n           \"model\": \"sim\", \n           \"prompt\": \"%gv_heard\",\n           \"stream\": false\n         }\n         Timeout (Seconds): 300\n         Use Cookies: On\n         Structure Output (JSON, etc): On ]\n    \n    A3: Variable Set [\n         Name: %response\n         To: %http_data.response\n         Structure Output (JSON, etc): On ]\n    \n    A4: Say WaveNet [\n         Text/SSML: %response\n         Voice: en-GB-Wavenet-O\n         Stream: 3\n         Pitch: 20\n         Speed: 8\n         Continue Task Immediately: On\n         Respect Audio Focus: On\n         Continue Task After Error:On ]",
          "score": 2,
          "created_utc": "2026-01-31 20:46:48",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qochnc",
      "title": "Do ollama models access the internet?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qochnc/do_ollama_models_access_the_internet/",
      "author": "Fancy_Purchase_9400",
      "created_utc": "2026-01-27 12:28:07",
      "score": 22,
      "num_comments": 13,
      "upvote_ratio": 0.83,
      "text": "I am new to ollama and I just wanted to know if the models downloaded locally (like mistral:7b) access the internet at all while using them (even if it is for maintaing any kind of logs). I have noticed a small spike in network usage (both for upload and download) while using the model, but I'm not sure if it is due to the usage of local ollama model, so I'm curious to know if it actually accesses the internet quietly. If so, how do I restrict it completely from accessing the internet? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qochnc/do_ollama_models_access_the_internet/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o208g5c",
          "author": "thexdroid",
          "text": "Ollama can now run models on cloud, but if you download a model it will run exclusively offline. You can simple disconnect from your internet and give a try.",
          "score": 28,
          "created_utc": "2026-01-27 12:44:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20beaj",
          "author": "tomayt0",
          "text": "So a model is just a big plot of data points (vectors) it has no way to access the internet through some hidden code.\n\nThe inference engine, in this case Ollama (based off of llama.cpp) translates each token the model spits back to it and turns it into output.\n\nIf there was to be any network access you should check the inference engine that is running the model. Ollama has offline mode which will prevent it from access the internet.",
          "score": 14,
          "created_utc": "2026-01-27 13:02:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o216ckr",
          "author": "dsept",
          "text": "You can enable internet access in the admin panel. Some models have the ability to use web search but it needs to be enabled.",
          "score": 8,
          "created_utc": "2026-01-27 15:38:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22up5g",
              "author": "mlt-",
              "text": "How do I know which models? What keywords shall I look for?",
              "score": 1,
              "created_utc": "2026-01-27 20:00:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o23k09q",
                  "author": "andy2na",
                  "text": "pretty sure all models can - you have to enable websearch within openwebui or whatever you are using. I connect it to searchxng but brave search is another option\n\n  \n[https://docs.openwebui.com/category/web-search/](https://docs.openwebui.com/category/web-search/)",
                  "score": 2,
                  "created_utc": "2026-01-27 21:53:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29pxyk",
                  "author": "soowhatchathink",
                  "text": "There are certain models that are trained for using tools a certain way, they usually mention whether they support tools in the description. The main keyword is \"function calling\", but others might use \"tool calling\", \"mcp support\" or other variations.\n\nThey won't be able to just use tools out of the box with ollama though, you will need to set up an \"adapter\" or something else in front of it that will let it know what tools are available and parse tool calls from its output - and actually do the thing.\n\nAlso different models are trained to call tools differently, you can tell it to call tools with whatever format by modifying the system prompt but it's much more likely to get it right if you use the format it was trained on. Qwen models for example are trained for Hermes-style function calling, which looks like `<tool_call>{\"name\": \"{tool_name}\", \"args\": {...}}</tool_call>`, so you need something that will parse those tool calls from their response and call the tool. Other models are trained with a different format, like `[tool_call] ... [/tool_call]`\n\nI tested a bunch of models on their tool calling ability and found Qwen3 to be the best and most consistent.\n\nFor ollama in specific, I've found this works in front of Ollama for hermes https://github.com/jonigl/ollama-mcp-bridge\n\nYou'll have to configure what tools/mcps it can call. For web searches in particular you can look for open source web search mcps to locally host, or use an online-hosted web search mcp (has better results, usually paid, but some have free tiers.).\n\nThere are web UIs you can host yourself that give you a dashboard to configure models and mcps and such, that might be your best bet if you are ok using their dashboards.",
                  "score": 2,
                  "created_utc": "2026-01-28 19:16:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o22wavi",
                  "author": "dsept",
                  "text": "I have preferred gemma3:12b.",
                  "score": 1,
                  "created_utc": "2026-01-27 20:07:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o25hesw",
          "author": "I_Space_car",
          "text": "It can if you allow if you don't they don't.\n\nYou can also block the model internet block via firewall as well",
          "score": 1,
          "created_utc": "2026-01-28 03:50:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25rph8",
          "author": "Salty_Fee_06",
          "text": "If model had Cloud mentioned at the end, yes, it needs internet but other models don't.",
          "score": 1,
          "created_utc": "2026-01-28 04:53:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27a140",
          "author": "Glum_Mistake1933",
          "text": "It doesn't, you need a specific piece of software so that models would be able to access the internet. Like someone else in this thread I use openwebui to give it that ability.",
          "score": 1,
          "created_utc": "2026-01-28 12:19:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29fztr",
          "author": "woolcoxm",
          "text": "it has a cloud option, but other than that it should not be able to access the internet unless you have given it mcp access or tied it to an ide or something agentic??\n\nthere is also an option to enable it on local area network so that users on the network can access it.\n\nits possible its sending diagnostic information to ollama, not sure though.",
          "score": 1,
          "created_utc": "2026-01-28 18:33:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpr4hq",
      "title": "Just updated Ollama and started using it after almost a year.... Are the Ollama devs stupid or is this harder to deal with than it seems?",
      "subreddit": "ollama",
      "url": "https://i.redd.it/dvhk25h286gg1.png",
      "author": "cmndr_spanky",
      "created_utc": "2026-01-28 23:04:49",
      "score": 12,
      "num_comments": 45,
      "upvote_ratio": 0.6,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qpr4hq/just_updated_ollama_and_started_using_it_after/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2cbogo",
          "author": "Repulsive_Fox9018",
          "text": "Jacking up the context window has a serious impact on RAM/VRAM usage.  It is easy to change, but you may run into memory walls.",
          "score": 7,
          "created_utc": "2026-01-29 02:54:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cl0yj",
              "author": "cmndr_spanky",
              "text": "Yes, that‚Äôs why I suggested an approach that considers the available VRAM as well as reads the context limit documented in the model spec",
              "score": 0,
              "created_utc": "2026-01-29 03:48:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2coure",
                  "author": "CodeSlave9000",
                  "text": "Can‚Äôt do that because ollama supports multiple models running at the same time.  How would it know how to apportion it?  I set my default with an environment variable‚Ä¶",
                  "score": 6,
                  "created_utc": "2026-01-29 04:12:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ctn5x",
                  "author": "zenmatrix83",
                  "text": " most of the calculations I‚Äôve seen are rough estimates. I do this in a docker container myself, watching the vram then increasing the context length and checking the vram. and it‚Äôs a slow tuning l process. Setting a safe minimum is likely a choice to limit issues. Realistically there are better model runners like vllm or for anything serious , ollama in general to be is turn key system. I still have it for some things, but I switched to lmstudio since its much easier to configure models",
                  "score": 1,
                  "created_utc": "2026-01-29 04:43:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2buxgh",
          "author": "grabber4321",
          "text": "there's a setting in the app itself, otherwise if you're running it in Docker, you can set a variable for this.",
          "score": 4,
          "created_utc": "2026-01-29 01:22:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2c3w5n",
              "author": "drakgremlin",
              "text": "IIRC this is ignored.",
              "score": 2,
              "created_utc": "2026-01-29 02:12:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2bf3ev",
          "author": "Mywifefoundmymain",
          "text": "You need to remember that ollama is a front end‚Ä¶ it doesn‚Äôt do anything it doesn‚Äôt have to. Now let‚Äôs go down this rabbit hole it hole and say you are running it in docker. Docker doesn‚Äôt grant hardware level access which would break your plan of having it determine how much vram you have. \n\nSo then they would need to make a ‚Äúlite version‚Äù and one that has access and deal with people complaining that the docker image isn‚Äôt doing what they want it to. \n\nThen we get to the llms themselves. They don‚Äôt produce them so they would need to rely on the makers to tell them the best context amounts. If the maker gets it wrong who would the ‚Äúconsumers‚Äù complain to?\n\nIn short the developers aren‚Äôt stupid, in fact they are smart. By not including that feature they have less to maintain, less complaining, and less support to deal with.",
          "score": 5,
          "created_utc": "2026-01-28 23:58:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bqelx",
              "author": "Monoboy",
              "text": "An additional thing too: you can have multiple people using the API at one time. What should the context length be for each user in that case?",
              "score": 1,
              "created_utc": "2026-01-29 00:57:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ce85n",
                  "author": "cmndr_spanky",
                  "text": "Someone who knows more can chime in but as far as I remember yes simultaneous queries will work either way and will either happen consecutively or concurrently depending on available resources (that I suppose would be affected by context limit settings).",
                  "score": 1,
                  "created_utc": "2026-01-29 03:08:23",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o2hxfjh",
                  "author": "Ryanmonroe82",
                  "text": "People that are setting this kind of access up know exactly what their hardware is capable and don‚Äôt need constraints to save them from themselves",
                  "score": 1,
                  "created_utc": "2026-01-29 22:42:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2blfr8",
          "author": "Spicy_GT3",
          "text": "You sound like a typical Apple user. (Same mind set in the SD/Comfy space.)\n\nYou do know previously it was 2k and then upped it to 4k. It‚Äôs on the documentation.\n\nSuper easy to change the value (add it to your launchdemon or agent plist). It‚Äôs just an environment variable or if you‚Äôre using the desktop app‚Ä¶it‚Äôs a slider.",
          "score": 4,
          "created_utc": "2026-01-29 00:31:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dw3z1",
              "author": "daservo",
              "text": "Unfortunately, the environment variable applies to all models. This is really not practical.\n\nThere are two other options remaining, and both are inconvenient:\n\n1. Clone an existing model and set the needed `num_ctx` value for the cloned model.\n2. Set `num_ctx` on the OpenWebUI side or on any other frontend.\n\nI agree with OP that it is better to handle this automatically on Olama side",
              "score": 1,
              "created_utc": "2026-01-29 10:06:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2g4xf4",
                  "author": "Spicy_GT3",
                  "text": "Tbh. If the OP really wants an easy to manage the context length per model‚Ä¶(OP if you‚Äôre reading) Download and use LM Studio. It‚Äôs a slider to all models. No plist, no environment variables, no openwebui, etc. Also LM Studio allows other settings once in the model too.",
                  "score": 1,
                  "created_utc": "2026-01-29 17:39:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2cepn1",
              "author": "cmndr_spanky",
              "text": "I remember having to make a manifest file earlier but maybe they added a slider recently to the Ollama service ?",
              "score": 0,
              "created_utc": "2026-01-29 03:11:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dj7b2",
          "author": "tom-mart",
          "text": "Because users like me want control. I set num_ctx in each ollama API call so model loads with the context window required by the task. If it's just user starting chat, model loads with 4k context, if the job is multistep agentic workflow the model loads with 128k window. I wouldn't want Ollama to dictate it for me.\n\n\nYou can also change modelfiles for your models to load with whatever num_ctx you set there.",
          "score": 1,
          "created_utc": "2026-01-29 08:05:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fmz8y",
              "author": "cmndr_spanky",
              "text": "I found in my tests a while back relying on the api param for context wasn‚Äôt actually working and I was forced to use an annoying manifest file solution to register multiple copies of the model at different context settings. Annoying as fuck. I also vaguely remember that when pointing a coding agent (Roocode) to an ‚Äúopen AI compatible endpoint‚Äù, it might not have even given me the option to specify it. \n\nI‚Äôm going to mess around some more, maybe it‚Äôs better now (haven‚Äôt touched this stuff in many months).",
              "score": 1,
              "created_utc": "2026-01-29 16:18:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2e28s7",
          "author": "Grouchy-Bed-7942",
          "text": "Use llama.cpp; it's an Ollama wrapper, and now you just need to run `llama-server -hf author/model:quant -c 0 -fit 1`, and your server will be listening at 127.0.0.1:8080‚Ä¶\nIf you just run `llama-server` and go to the interface at 127.0.0.1:8080, you'll be able to switch between all the models you've downloaded to your machine.",
          "score": 1,
          "created_utc": "2026-01-29 11:00:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e7y0z",
          "author": "p_235615",
          "text": "its literally 1 environment variable: `OLLAMA_CONTEXT_LENGTH=16384`\naaaannnddd its done...",
          "score": 1,
          "created_utc": "2026-01-29 11:45:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2flte4",
              "author": "cmndr_spanky",
              "text": "That would apply a global default to all models. Not at all a good idea. And yes I know other ways to do that using manifest files",
              "score": 1,
              "created_utc": "2026-01-29 16:13:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2h6wgb",
                  "author": "p_235615",
                  "text": "Are you a woman ? because it seems that you dont know what you want. That variable sets the same default value that ollama sets to 4096. If you want it per model, then set it in the model or in the app accessing it... \n\nSetting context based on VRAM also doesnt fix anything, because what if I have 16GB VRAM, want to load 2 7B models which just fit in to it, but if you set the context too large upon loading of the first model because of plenty of VRAM, then you just created an additional complication and have to set context manually anyway...",
                  "score": 0,
                  "created_utc": "2026-01-29 20:34:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2eo7sc",
          "author": "ZeroSkribe",
          "text": "They most likely have an average they know about for their users and putting that higher really slows things down unless you have a lot of vram. Your unified memory on a mac is trash and most people don't want to wait for how slow that would be. I see you must be one of the people who would put it to 256K on a unified system..thats hilarious. I don't have any problems at 4k but I'm going to experiment with higher, ollama recommends to bump it up some, the probably have a good reason to keep it small by default. Its VERY easy to change in the settings. Running a 40GB model on a mac would be unusable to me, but then again I do things. You don't just run a big model because you can, but you balance it out with what you need, the bigger the slower even if vram or in unified memory, the slowness doesn't help creativity.",
          "score": 1,
          "created_utc": "2026-01-29 13:30:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2v0llh",
          "author": "New_Cranberry_6451",
          "text": "I understand OP's concern and also understand why 4096 is the default and should still be. So, as a fixture I would suggest just exposing this fact in the UI in a more visible way, something like \"If you experience hallucination problems remember you can always increase the available context here\". Or any other way to make this fact more obvious so people won't stop using Ollama cos they had a bad experience due to lack of context space.",
          "score": 1,
          "created_utc": "2026-01-31 21:37:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2brkwu",
          "author": "StephenSRMMartin",
          "text": "Because 4096 is a safe default that will work for a huge number of entry-level users' usecases (small one-off chats).\n\nDevelopers who know how to configure their system to work with openai-compatible APIs are surely knowledgeable enough to know how to change the context.\n\nI am not sure what is effectively gained by changing the default to be an order of magnitude larger.",
          "score": 1,
          "created_utc": "2026-01-29 01:04:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ckjwm",
              "author": "cmndr_spanky",
              "text": "I wasn‚Äôt recommending changing the default to another arbitrary number. I was suggesting that some safe setting could be auto determined by it reading the model spec (which actually does include a param on the context limit) and looking at the available VRAM on the host",
              "score": 0,
              "created_utc": "2026-01-29 03:45:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2cns5l",
                  "author": "StephenSRMMartin",
                  "text": "And do what? Fill it with as much context as possible given the vram? All ram? What if their system slows or OOM kills competing programs? What if it fails to allocate because the vram estimation is inaccurate? What if it fails mid generation because it doesn't account for the offloaded kv cache growing that large? \n\nThese can and do happen in llama CPP when you tell it to fit as much as possible. Ollama will likely have the same issues.\n\n\nWhat will the entry level user do to fix these things?\n\nWhat would the entry level user do that would benefit from maxing their context size, and for those tasks, will you gain more users over the ones lost from the issues that arise?",
                  "score": 2,
                  "created_utc": "2026-01-29 04:05:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2cztfz",
          "author": "Savantskie1",
          "text": "Use a frontend for ollama like OpenWebUi that you can set the context window in. Or here‚Äôs a brilliant idea RTFM. READ THE README AND LOOK ONLINE. context window is set by flags when running a model‚Ä¶",
          "score": 0,
          "created_utc": "2026-01-29 05:26:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dm8v1",
              "author": "Patient-Lie8557",
              "text": "Reading comprehension....\n\nHe wrote: \"Why on earth would you make this a default, and make it moderately annoying to change the default?\" on the fist line. Why are you answering posts you clearly didn't read?",
              "score": 1,
              "created_utc": "2026-01-29 08:34:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2izh7q",
                  "author": "Savantskie1",
                  "text": "Because it‚Äôs obviously a troll. Had he read the readme, or ü§Øread the manual or the dozens of not hundreds of posts about this easily accessible via a google search he would have found answers. I gave him the answers in my reply. And all he has to do is use *gasp* google search what I‚Äôd mentioned and he‚Äôd know the answers of how to do it. üò≥ why do people get rewarded for stupidity these days.",
                  "score": 0,
                  "created_utc": "2026-01-30 02:08:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2dvu0d",
              "author": "daservo",
              "text": "Doing this in UI is really time-consuming and annoying. I agree with OP.",
              "score": 1,
              "created_utc": "2026-01-29 10:03:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2iztxq",
                  "author": "Savantskie1",
                  "text": "It‚Äôs not time consuming‚Ä¶ it‚Äôs literally two fucking clicks and you adjust the slider or type some numbers‚Ä¶ how in tf is that hard?",
                  "score": 1,
                  "created_utc": "2026-01-30 02:10:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2d9xnk",
              "author": "cmndr_spanky",
              "text": "I know how to set it",
              "score": 0,
              "created_utc": "2026-01-29 06:45:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2dgwz8",
                  "author": "Savantskie1",
                  "text": "Then why did you make this post? Just aura farming?",
                  "score": 0,
                  "created_utc": "2026-01-29 07:45:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2d6zeo",
          "author": "Bonzupii",
          "text": "There are a great many good reasons to complain about ollama. The fact that YOU didn't read documentation or perform a simple Google search on how to change the context limit is not one of those reasons. 4k is a perfectly reasonable default for local models, if you need more, you change it. If you don't like it, use something else. Good luck finding a magic LLM runner that magically determines how much context your hardware can fit without crashing ü´°",
          "score": 0,
          "created_utc": "2026-01-29 06:21:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2da1t5",
          "author": "JacketHistorical2321",
          "text": "Why would the devs be stupid when you're the one that's complaining about something that's easily changed by you?",
          "score": 0,
          "created_utc": "2026-01-29 06:46:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2k5v0h",
          "author": "Outrageous_Rub_6527",
          "text": "Hey! One of the maintainers here. Yeah this is pretty silly and will be going away completely soon. In the meantime, we should have a fix based on your VRAM in the next change :) Sorry it took so long - there‚Äôs a bunch of stuff to consider around offloading + setting good defaults so machines don‚Äôt blow up. \n\nThanks for sticking with us! We‚Äôre making a big push to improve the overall experience a good amount - as you‚Äôre seeing with the launch command. Always feel free to reach out to any of us on discord or email hello@ollama.com if you run into any issues!",
          "score": 0,
          "created_utc": "2026-01-30 06:40:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2k7rpc",
              "author": "cmndr_spanky",
              "text": "appreciate the reply! Others are telling me this is impossible and the worst idea and 4k limit defaults is the best and only correct answer... sooo.. \n\n![gif](giphy|jPAdK8Nfzzwt2)",
              "score": 1,
              "created_utc": "2026-01-30 06:55:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qtqp9p",
      "title": "175k+ publicly exposed Ollama servers, so I built a tool",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/gallery/1qtqobb",
      "author": "truthfly",
      "created_utc": "2026-02-02 09:41:37",
      "score": 11,
      "num_comments": 3,
      "upvote_ratio": 0.76,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qtqp9p/175k_publicly_exposed_ollama_servers_so_i_built_a/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o34ugp2",
          "author": "HyperWinX",
          "text": "\"<statement> so I built\" my ass",
          "score": 3,
          "created_utc": "2026-02-02 10:40:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35crn9",
              "author": "truthfly",
              "text": "Yeah, bad title maybe. I get the skepticism. Maybe the form is not good.. \n\nBut it's not a hype post. just releasing a tool I was already using once the article went public.",
              "score": -2,
              "created_utc": "2026-02-02 13:03:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o36xye5",
          "author": "sinan_online",
          "text": "Is it that interesting? I run stateless servers on the cloud from time to time, they are completely ephemeral.",
          "score": 1,
          "created_utc": "2026-02-02 17:49:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qt7wzm",
      "title": "Reprompt - Simple desktop GUI application to avoid writing the same prompts repeatedly",
      "subreddit": "ollama",
      "url": "https://i.redd.it/u520ax05nxgg1.gif",
      "author": "PuzzleheadedHeat9056",
      "created_utc": "2026-02-01 19:22:11",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 0.82,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qt7wzm/reprompt_simple_desktop_gui_application_to_avoid/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o32fyud",
          "author": "thecoder12322",
          "text": "This is exactly the kind of tool that makes working with local LLMs so much more practical! Rust + egui is a great choice for a lightweight desktop GUI, and the focus on reusable prompts is spot-on for real-world workflows.\n\nI've been working on similar local-first AI tooling, and if you're looking to expand beyond text-based models, you might want to check out the **open source RunAnywhere SDK** (https://github.com/RunanywhereAI/runanywhere-sdks). It makes it really easy to integrate STT, TTS, VLM, and LLM capabilities into desktop apps while keeping everything running locally ‚Äì perfect for the same privacy-first philosophy you've built Reprompt around.\n\nLove that you're keeping it simple and focused. Tools like this are what make local AI actually usable for everyday tasks. Happy to chat more about local AI tooling if you're interested ‚Äì feel free to DM!",
          "score": 2,
          "created_utc": "2026-02-02 00:14:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qt04u5",
      "title": "OpenClaw For data scientist that support Ollama",
      "subreddit": "ollama",
      "url": "https://github.com/JasonHonKL/PardusClawer",
      "author": "jasonhon2013",
      "created_utc": "2026-02-01 14:39:27",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.82,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qt04u5/openclaw_for_data_scientist_that_support_ollama/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qq8mas",
      "title": "figured out how to use ollama + moltbot together (local thinking, cloud doing)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qq8mas/figured_out_how_to_use_ollama_moltbot_together/",
      "author": "Round_Net_225",
      "created_utc": "2026-01-29 13:31:35",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 0.72,
      "text": "saw that post yesterday asking about ollama with moltbot and had the same question last week.\n\nhere's what worked: don't run full moltbot locally (too heavy). use ollama for thinking, cloud for execution.\n\nmy setup:\n\n¬∑ ollama local with llama3.1 for reasoning\n\n¬∑ shell\\_clawd\\_bot for actual tasks\n\n¬∑ they talk through simple api\n\nbasically ollama plans it, cloud does it.\n\nwhy this works:\n\n¬∑ ollama stays free for back-and-forth thinking\n\n¬∑ only pay for execution (like 10% of calls)\n\n¬∑ agent runs 24/7 without rpi staying on\n\nbeen running on raspberry pi 5 with 16gb for 2 weeks. free trial covered testing, now \\~$20/month for the cloud part.\n\ntheir telegram group helped with the setup. surprisingly easy to connect.\n\nnot sure if this is what that other person meant but it's been solid for me.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qq8mas/figured_out_how_to_use_ollama_moltbot_together/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o2eylrx",
          "author": "ZeroSkribe",
          "text": "Try qwen3",
          "score": 2,
          "created_utc": "2026-01-29 14:25:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ev5jw",
          "author": "firedog7881",
          "text": "I‚Äôm just getting around to checking out moltbot, you have this running on a pi5 and it works well? I can‚Äôt wait to get home",
          "score": 1,
          "created_utc": "2026-01-29 14:07:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fktpu",
          "author": "violetfarben",
          "text": "Isn't Llama 3.1 super old now?",
          "score": 1,
          "created_utc": "2026-01-29 16:09:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2g5pbr",
          "author": "desexmachina",
          "text": "What OS is on that pi? I‚Äôm thinking of trying it in a VM",
          "score": 1,
          "created_utc": "2026-01-29 17:42:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lhjyy",
          "author": "nabbl",
          "text": "Ok it would be cool to know how you did it exactly? Basically I have ollama running on another PC and I want moltbot to use the ollama API from that. What is the config ?",
          "score": 1,
          "created_utc": "2026-01-30 13:09:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lp5ft",
              "author": "Fubar83",
              "text": "I got Moltbot and local LLMs working with the help from Antigravity.",
              "score": 1,
              "created_utc": "2026-01-30 13:51:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qqan74",
      "title": "Effects of quantized KV cache on an already quantized model.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qqan74/effects_of_quantized_kv_cache_on_an_already/",
      "author": "Pyrore",
      "created_utc": "2026-01-29 14:52:44",
      "score": 8,
      "num_comments": 14,
      "upvote_ratio": 1.0,
      "text": "I run a QwQ 32B model variant in LM studio, and after the update today, I can finally use KV quantization without absolutely tanking my performance.  My question is, **if I'm running QwQ at four bit, will dropping my K/V cache to 4 bits notably impact the accuracy?**\n\nI'm happy at 4 bits for QwQ, I only have 24BG VRAM, and that fits nicely at around 19GB (I understand it's better to have more parameters than higher quants).  But I can only fit about 10k of context into the remaining 4GB of VRAM (need to leave about 1GB spare for system overheads), no where near enough for the conversational/role-play I use local LLMs for.  So I've bee running the KV cache in main memory with the CPU, easily runs up to 64k, but I never really go past 32k, because by then I'm around 1.5 tokens a second (compared to 15/s when there is negligible context).\n\nBut with KV cache at 4 bit I can hit 40k context without overloading my VRAM, and my tests so far indicate three times the token rate for a given context size compared to main memory/CPU.  But accuracy is more subjective, I'd love to hear your opinions or links to any studies.  My model is already running well at 4 bits, and it seems sensible to run the KV at the same accuracy as the model, anything more seems wasteful, unless there's something I'm not understanding...\n\nThanks in advance!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qqan74/effects_of_quantized_kv_cache_on_an_already/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o2fjaoe",
          "author": "Ryanmonroe82",
          "text": "I would disagree with more parameters is better than a higher precision but smaller parameter model. You should test it where accuracy matters and check the results. When you are using qwen 32b QwQ or any reasoning model in 4bit  this destroys its reasoning capabilities and accuracy And hallucinations go way up. The smaller the model the greater the loss. \nIf you move down to something like nemotron 9b v2 in BF16(3090 works best with BF16) it will likely outperform the model you are using now on most all technical tasks. It boils down to math. Q4 is 2^4 = 16 whereas BF/F/FP16 is 2^16 = 65536.  This means each weight on your Q4 model can represent 16 numbers with precision gradation and BF16 will have 65536. This means the Q4 Qwen model you are using has lost 99.98 percent of its numerical precision. The amount of parameters the model has can overcome some of the 4bit precision loss but you need to be using something a lot larger than a 32b model to overcome this. Llama 405b in Q4 and Llama 70b in BF16 are very close in accuracy because the 405b model has so many parameters it can still generalize well enough\nIf what you are doing requires multi-step reasoning or accuracy use the higher precision and smaller model.",
          "score": 3,
          "created_utc": "2026-01-29 16:02:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fnu63",
              "author": "Pyrore",
              "text": "Yeah, but if I want accurate I'll pay for online access to cloud servers at full \\`16 bit/128k context'.  I run models locally for conversation and role playing (usually abliterated and often retrained on new data sets) and I can say that QwQ 32B is so much better than say Gemma3 27B (both at 4 bits), QwQ can track so many more logical issues.  Not to say I'd trust it for anything serious at 4 bits, my question is that if I'm already running at 4 bits, does quantizing the KV cache make anything worse?  And so far, it doesn't seem so but my opinion is still subjective...\n\nAlso, see: [https://www.youtube.com/watch?v=TLp1v2GsOHA](https://www.youtube.com/watch?v=TLp1v2GsOHA)  \\- \"Dave's Garage\", where he talks about a compact petaflop unit that is optimized for 4 bits.  Why would it be optimized for 4 bits if 4 bits wasn't worthwhile for hobbyists like me?",
              "score": 1,
              "created_utc": "2026-01-29 16:22:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2fzo6s",
          "author": "Pyrore",
          "text": "OK, I've tested this to the full 40k context, and I'm still getting an insane 17 tokens a second at 40k context!  It used to be less than one token a second by this point.  And I haven't noticed any difference in accuracy, even at 40k conversations still remember details from the very beginning.  I used to get <1 token/sec at 40k context...  The only downside is if I push beyond 40k tokens context, it spills over into main memory across the PCIE bus, and slows to a crawl.  But 18 tokens/second at 40k is so much better than <1, I never really used 64k because of the slow speed...  I can now talk to my AI for hours on end without it forgetting anything, and I never have to wait more than 30 seconds for the full response!  I said this was 3 time faster than it was to start with, but as context increases it gets up to 30 times faster...  That's insane!  So I still want to know if I'm sacrificing accuracy using the KV quantization, but it seems I'm not, at least as far as I can tell across several hours of conversation...",
          "score": 2,
          "created_utc": "2026-01-29 17:15:27",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2kvc44",
              "author": "CooperDK",
              "text": "Just disable CPU RAM for the middle, it won't spill over in CPU memory then",
              "score": 2,
              "created_utc": "2026-01-30 10:25:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2g1166",
          "author": "PossiblyTrolling",
          "text": "I've done quite a bit of experimenting and as a result I run my KV at q8_0\n\nq4 is definitely faster but result quality drops dramatically. I don't notice any difference between fp16 and q8 though. At q4 KV, models seem to make a lot of mistakes in general and often miss nuanced context.\n\nI often find myself wishing there was a q6 mode to play with.",
          "score": 2,
          "created_utc": "2026-01-29 17:21:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2g273r",
              "author": "Pyrore",
              "text": "But is that running your model at Q8?  Or running your model at Q4 with KV at Q8?  That's the key question.  I'm already running my model at Q4, and I accept it won't be as accurate, but my question is how does the input quantization affect this further (if at all)?  If your model is Q4 and KV at Q8, does that give you better results? That's what I want to know...  But if your model is at Q4 and your KV is at Q4, does it make a difference?",
              "score": 2,
              "created_utc": "2026-01-29 17:26:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2gljd8",
                  "author": "PossiblyTrolling",
                  "text": "I've tested extensively all KV quantizations and all model quantizations. KV itself seems to work fine with all model quantizations down to kv q8. Running KV at q4 results in crappier answers no matter the model quantization.",
                  "score": 1,
                  "created_utc": "2026-01-29 18:53:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2k2bkr",
                  "author": "fasti-au",
                  "text": "I just write a monologue but it‚Äôs sorta got all you need and my hate of OpenAI in there too. \n\nDoes not matter.   The quantizing is about splitting tokens to use maybe and don‚Äôt.    So f your question is the same the quants will still result the same and thus yournkv quant is the same.   It happens on the in run so.      \n\nIf your quanting 22 it become 20  but so did 20-25   Like rounding down and up in essence. \n\nSo if all your tokens come in as q8. The second Q8!doesn‚Äôt change the first q8!  It just means that if you already know you already know.  \n\n\nUnless there‚Äôs three matches and it‚Äôs in opposite order the numbers the same so to speak.    It‚Äôs math stuff and makes no sense in one way and perfect sense in another so don‚Äôt worry too much just know that you need to not correct but give better first prompts.   Never correct. Just repromt better. \n\nThe way you solve it is first promot give the exhale and reasoning then second promo it can use first prompts Q8 to get question then it can use midel q8 to distill more tokens then q8 again.   This could mean you have two tokens that are close but neither is actually wrong just takes extra latented to run the logic chain and if token 1 fails it‚Äôll backtrack usentoken 2 and if it gets stuck it hallucinates.   \n\nRestart the whole prompt chain clean with the solution to the bit it broke and you can stop getting the close nes because you already skipped that part nandahowed it part two. And thus you now have your own two equation logic chain solving the q8 confusion. \n\n\nDoesn‚Äôt work if you don‚Äôt know what you‚Äôre doing as a goal like a chat.  But true false it works.  This is agentic design not OpenAI throw it around 900 times to get the most manipulated response it could get\n\nThink of quanting as making the fist call make a variable you use and that variable doesn‚Äôt change.  It know that token matters and unless it‚Äôs choosing between two with the same number it‚Äôs going to did the right one. \n\nSome tokens don‚Äôt relate to billions of others and some do.  It depends on when where and how you ask   The pachinko machine has no channels is more like a golf course where you hit the edge of a bunker you end up in sand but the exact same thing with a extra token of wind makes it miss.    \n\nYou already had all the tokens for the world now you have the golf course in the cache but every swing your grabbing more tokens for the result from environmental impacts     Don‚Äôt take the second shot.  Take a mulligan and try better and then the wind is not a factor because you aim better and not even hit bunker in any situation.   That‚Äôs prompt engineering in a sports analogy",
                  "score": 1,
                  "created_utc": "2026-01-30 06:12:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2jziu0",
          "author": "fasti-au",
          "text": "Q8 doesn‚Äôt matter.  Lower does.  \n\nThink like this.   If your midel is q8 you lose like 10-15% accuracy or resolution is better terminology.   \n\nIf the resolution is enough to get good hits then the reality is quanting those only matters if you are competeing.    \n\nSimple way to think analogy wise is.  \n\nI want 10 people.  Of those ten people I need a person that can do x.    I‚Äôm t can pick from 3.   Quanting makes that 3 become 1.5 so your getting the number 1 and the number two in theory.  That‚Äôs put in a bucket for repeat.   \n\nSame question same results you get from cache regardless. Temp zero and working token chain perfect worldnstuff .   Similar prompt the hits for those 3 might change but if those top 3!quanted differ they may still quant back to the same one if the number matches enough.  It‚Äôs just the weight value being quanted  which means that things get to red into ternery in a way.  -1 0. 1.    The way that splits up is more forced to 3 groups where in quanted the temp gets to play in the 0 quants more.  \n\n\n\nThe formula to ponder is.   If I have a token is it made ip of 1 or two or three tokens really.   They have three tokens go in but theirs three combine in latent to be 1 because quanted weight say that‚Äôs the word.   This is why rr a r are not related in think models so ts a two shot and re analisys of the token weight to get the right answer.  \n\nWhat‚Äôs actually happening is not one model of ta a chain of sub models hidden inside an api.  \n\nYou get one shot rewriting you prompt so it‚Äôs not misspelt is r has the right words first then it goes to a reasoner that thinks about what the question is and if it multi step.  Then it gets passed to thinking reasoners of whatever level THEY choose and you can for api stuff to try make it true but really that just asking the pre reasoners to loop and try and make more think options but it can just be. The same thing three times to get the same reasoner even though you ask for big reasoner.   \n\nThen they snide thenresoners they have their own graphs for their types so as you see in agent it sets up its own workspace and tools.   That‚Äôs you watching one of those calls live.   The others are just hidden.   \n\nReasoners are not the ones writing agent code the just have their own oversight of the model then it goes back to the in out reasoners to check for f it says bad words etc and fixies it or hard lines it as bad and back into the loop.  \n\n\nThis is why you get blank zips etc.  I‚Äôm t thinks it worked because the last response was zip created but like a Ralph loop is missing it needs the ‚Äú no you suck‚Äù to go reason.   \n\nNow if you are quanting everything you‚Äôre getting more consistent but only the core number one way.    \n\nYou see the same balance fallout issue with think tokens in thinkers.    Give it less tokens it never checks all the options only the first first or second  before time out.  If f youbhowever it be shot a list of options and then ask those options individually and then ask each one and then combine and ask one shot for advice it‚Äôs doing everything complete and quantising wouldn‚Äôt hurt.   \n\nQuantising a question make the question true or false like and that‚Äôs fine.  But if you‚Äôre exploring variations of the same  thing then quantizing makes things less able to walk to the sidewalk or side alley or different branch of thinking at all. \n\n\n\nSo quantising has nearly zero affect if your in true false concepts but if your looking for 3 variants that are close to each other.   Mcp Stdio template.  Vs mcp template as a question.  The first one is quantizenfrendly and the second will break regularly because mcp template was what 40+ different is code in out methods.  \n\n\nThe way around this issue is indexes and tokens if tokens but they don‚Äôt do that on APIs because it‚Äôs their model not yours.   Your fine tunes are just like OSS 20b translating before their own models.  It‚Äôs better than. I thing but it‚Äôs 30 mins of owning a midel you can actually train t so you never have to describe your codebase.  \n\nThis is why open source is more powerful than ChatGPT with rag and such.  \n\n\nYour real goal is to quant to 3 optiins -1 0 1 for each token then have that again be reworked ai it makes sense then distilled again until you get a formula really.  It‚Äôs a formula you can‚Äôt see but you can manipulate.  \n\nI would highly suggest you write prompts for local models to do the work then give the big models the work with the ability to iterate your stuff then bring back and do that loop because when you actually control the model even if it‚Äôs just fine tuning you save billions of tokens just trying to get the stages of models t understand your needs and doing it in code blocks is easier to it than words because code is structured and words mean nothing\n\n\nYou see this daily if vide coder because when you say move it copies.  If fmyou say relocate it moves and if you say copy it sometimes copies the file or sometimes writes a new file with its copied structure.  \n\n\nQuantizing move to only mean move and copy to mean copy in code works but in humans it‚Äôs not a clear line. \n\nSo. Think of quantises as formula and unquantised as discussion.   \n\nChat quantised models are hallucination machines and and code quantised models a it works or you need a better prompt for r example to force it to token your tokens.  \n\n\nIn use you get to overrule system prompts etc.   you do that well you can use qwen3 14b to code as well as kimi or Claude.    I‚Äôm t can‚Äôt think like it but it can match the same (2b tokens needed for almost every langue of code to English).  \n\nAfter those 2 b tokens the rest of the midel is just additional rules and logic and relations to make the responses match an input.   If you don‚Äôt need the figure stuff out to get the answer then you only need the tokens for the answer.    A = B false.  Thst not billions of dollars of development and tokens distilling training and whatever.    It just need to know that when it see A=B ? As those specific logic tokens to apply to a tool.  Do I g the work badly I. Retries and handing of and self checks and such is open ai and anthropic chasing dreams they already know is not doable with their binary systems but ts been know for like 8 years or f not longer but GPUs mean you don‚Äôt care about 500 fails if one win comes out fast enough.  \n\n\n\nIf f the solution takes longer than the tool then your farming money and lying to the world.\n\nOpen ai ü§ñ a not making things fr humans it‚Äôs replacing them and all the tools they made and making it one universal translator.    We have calculators I don‚Äôt need a ai midel to do 1+1.  Ooenai is making a death machine anthrooic are making a universal tool use machine.  This is more of a hand holder and pusher.  \n\n\nOpenAI are so corrupted that they had to join the defence force and release a ooen weights model to stop copyrightable stuff hitting them.    I‚Äôm t hit anthropic and Suno and as you saw open ai want deepseek to nt copy their Open help the world‚Äù stuff.  \n\n\n\n\nShort answer.   Better questions examples a token guidance for quantised models but they are not broken they are more\nPowerful in ways",
          "score": 1,
          "created_utc": "2026-01-30 05:50:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qry7r9",
      "title": "[Ollama Cloud] 29.7% failure rate, 3,500+ errors in one session, support ignoring\ntickets for 2 weeks - Is this normal?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qry7r9/ollama_cloud_297_failure_rate_3500_errors_in_one/",
      "author": "Few-Point-3626",
      "created_utc": "2026-01-31 09:46:10",
      "score": 8,
      "num_comments": 6,
      "upvote_ratio": 0.83,
      "text": "'ve been using Ollama Cloud API for my production workflow (content moderation)\n\nand I'm experiencing catastrophic reliability issues that are making the service\n\nunusable.\n\n\n\n\\## The Numbers (documented with full logs)\n\n\n\n| Metric | Value |\n\n|--------|-------|\n\n| Total requests sent | 4,079 |\n\n| Successful responses | 2,868 |\n\n| \\*\\*Failed requests\\*\\* | \\*\\*1,211\\*\\* |\n\n| \\*\\*Failure rate\\*\\* | \\*\\*29.7%\\*\\* |\n\n\n\n\\## Incident Timeline\n\n\n\n| Date | Error 429 | Error 500 | Success Rate |\n\n|------|-----------|-----------|--------------|\n\n| Dec 10, 2025 | 235 | 0 | 0% |\n\n| Dec 20, 2025 | 0 | 30 | 0% |\n\n| \\*\\*Jan 4, 2026\\*\\* | \\*\\*3,508\\*\\* | 0 | \\*\\*0%\\*\\* |\n\n| Jan 29, 2026 | 0 | 0 | 86.8% |\n\n| Jan 30, 2026 | 0 | 0 | 74.3% |\n\n| \\*\\*Jan 31, 2026\\*\\* | 0 | \\*\\*194\\*\\* | \\*\\*28.8%\\*\\* |\n\n\n\nYes, you read that right: \\*\\*3,508 consecutive 429 errors in 40 minutes\\*\\* on\n\nJanuary 4th.\n\n\n\n\\## The Pattern\n\n\n\nEvery session follows the same pattern:\n\n\\- \\~30 requests succeed normally\n\n\\- Then the server crashes with 500 errors\n\n\\- All subsequent requests fail\n\n\\- I have to restart and hope for the best\n\n\n\n\\## My Configuration\n\n\n\n\\- Model: deepseek-v3.1:671b\n\n\\- Concurrent requests: 3 (using 3 separate API keys)\n\n\\- Workers per key: 1 (minimal load)\n\n\\- Timeout: 25 seconds\n\n\n\nI'm not hammering the API. 3 concurrent requests with 3 different API keys is\n\nextremely conservative.\n\n\n\n\\## Support Response\n\n\n\nI opened a support ticket on \\*\\*January 18th, 2026\\*\\*.\n\n\n\n\\*\\*Response received: NONE.\\*\\*\n\n\n\nIt's been 2 weeks. Radio silence. No acknowledgment, no \"we're looking into it\",\n\nnothing.\n\n\n\n\\## Questions for the Community\n\n\n\n1. Is anyone else experiencing similar issues with deepseek models on Ollama Cloud?\n\n2. Is this level of unreliability normal?\n\n3. Has anyone actually gotten a response from Ollama support (hello@ollama.com)?\n\n4. Are there alternative providers for deepseek-v3 that are more reliable?\n\n\n\n\\## What I'm Asking Ollama\n\n\n\n1. Investigate why your servers are returning 3,500+ 429 errors in a single session\n\n2. Investigate the 500 errors that crash the service after \\~30 requests\n\n3. Respond to support tickets\n\n4. Credit for the failed requests that were still billed\n\n\n\nI have complete logs documenting every single error with timestamps. Happy to\n\nshare with Ollama support if they ever decide to respond.\n\n\n\n\\---\n\n\n\n\\*\\*Edit:\\*\\* I'll update this post if/when I get a response.\n\n\n\n\\*\\*Edit 2:\\*\\* For those asking, my use case is legitimate content moderation for a\n\nFrench platform. \\~200-300 requests per day, nothing excessive.\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qry7r9/ollama_cloud_297_failure_rate_3500_errors_in_one/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o2u79lb",
          "author": "mchiang0610",
          "text": "I'm Michael C from Ollama. This is very unacceptable and I take full accountability for your experience. \n\n  \nI am looking into what could be the cause of this, and why rate limits are being hit to cause the errors. In the meantime, I have gone ahead and refunded 3 months of payment. I get it, it does not mean much if you are not receiving the service you've paid for.  \n\n  \n[michael@ollama.com](mailto:michael@ollama.com)",
          "score": 8,
          "created_utc": "2026-01-31 19:13:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ubxr3",
              "author": "StardockEngineer",
              "text": "I would say this is great customer service, if not for ignoring their pleas for help outside of social media.",
              "score": 5,
              "created_utc": "2026-01-31 19:35:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2s1nxe",
          "author": "Few-Point-3626",
          "text": "https://preview.redd.it/mtk5v6i6hogg1.png?width=885&format=png&auto=webp&s=fdd81444265cde40a910a64834c51696a5656ef2\n\nFor those asking, my use case is legitimate content moderation for a\n\nFrench platform. \\~200-300 requests per day, nothing excessive.",
          "score": 2,
          "created_utc": "2026-01-31 12:24:40",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2u7edg",
              "author": "mchiang0610",
              "text": "I agree. This is a totally legitimate use case we want and should be serving.",
              "score": 1,
              "created_utc": "2026-01-31 19:13:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2sewnl",
          "author": "Ryanmonroe82",
          "text": "Have you checked the issues page on their GitHub?",
          "score": 2,
          "created_utc": "2026-01-31 13:53:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qousvs",
      "title": "GPU advice for entry level AI",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qousvs/gpu_advice_for_entry_level_ai/",
      "author": "fulefesi",
      "created_utc": "2026-01-27 23:44:30",
      "score": 7,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "My current desktop pc: h77ds3h mobo pcie gen 3, xeon e3 1275v2 4c/8t ivy bridge, 24gb ddr3 1600mhz bundled in old atx case with side vents at bottom and only 1 fan (80mm rear fan)\n\nPurpose: learning, experimenting with entry-level AI, 1‚Äì3B or 7b (if possible) coding LLMs 4-bit quantized + LoRA inference. I only work with Python for data analysis, libraries like pandas, short scripts mainly. Hopefully upgrade entire system + new architecture GPU in 2028\n\nBecause of budget constrains and local availability where i'm currently stationed, i have very few contenders (listed as new): rtx 3050 8gb asus tuf (250$), rtx 5060 8gb msi ventus (320$), rtx 3060 12gb asus dual geforce v2 OC (320$)\n\nWhat/how would you recommend to start with?\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qousvs/gpu_advice_for_entry_level_ai/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o24gjz0",
          "author": "zenmatrix83",
          "text": "in most general use cases most vram is what you want, models get loaded into there and when they spill over into regular ram too much speed starts getting tanked",
          "score": 4,
          "created_utc": "2026-01-28 00:34:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eys7a",
              "author": "ZeroSkribe",
              "text": "this",
              "score": 1,
              "created_utc": "2026-01-29 14:26:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25jyf6",
          "author": "dropswisdom",
          "text": "Rtx 3060. It's still one of the best value cards for LLM, due to its relatively high vram amount",
          "score": 4,
          "created_utc": "2026-01-28 04:05:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o272eq0",
          "author": "calivision",
          "text": "Get the 12GB card for that machine. I'm using a 1060 6GB with my FX-6300 that has 24GB of DDR3 RAM.",
          "score": 1,
          "created_utc": "2026-01-28 11:23:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dxyrq",
          "author": "alizou",
          "text": "I bought a 3060 with 12G of vram and Im really satisfied of the result. the memory really allow to run better model without being impacted by slower cpu+ram combo. And in your case the ram will really slow you down alot",
          "score": 1,
          "created_utc": "2026-01-29 10:22:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ez2og",
          "author": "ZeroSkribe",
          "text": "I would try to save to get that 16GB 5060ti, you can do plenty with just 8GB just keep the models in vram, pick a qwen3 size that will fit. I run 2 rtx-3050's 8GB cards and they run fast af. As long as it is an nvidia card that meets ollama requirment and you keep it in vram it will smoke.",
          "score": 1,
          "created_utc": "2026-01-29 14:28:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}