{
  "metadata": {
    "last_updated": "2026-02-13 09:09:58",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 20,
    "total_comments": 88,
    "file_size_bytes": 120124
  },
  "items": [
    {
      "id": "1r2ex9k",
      "title": "Just try gpt-oss:20b",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r2ex9k/just_try_gptoss20b/",
      "author": "newz2000",
      "created_utc": "2026-02-12 00:37:17",
      "score": 134,
      "num_comments": 32,
      "upvote_ratio": 0.96,
      "text": "I have a MacBook Air with 24gb ram (M2) and when I set the context to 32k I can really do about everything I want a local model to do for normal business stuff. Tokens/sec is about 15 at medium reasoning, (update: 21.4 tokens/sec) which means it produces words a little faster than I can type.\n\nI also tested on an older Linux machine with 64gb ram and a GTX gpu with 8gb vram and it worked fine doing batch processing overnight (update: 9 tokens/sec). A little too slow for interactive use though.\n\n* Scripting - yes\n* Calling tools - yes\n* Summarizing long content - yes\n* Writing content - yes\n\nHereâ€™s how I used it:\n\nCreate a file named `Modelfile-agent-gpt-oss-20b` and put the following in it\n\nâ€”\n\n    FROM gpt-oss:20b\n    # 1. Hardware-Aware Context\n    PARAMETER num_ctx 32768\n    # 2. Anti-Loop Parameters\n    # Penalize repeated tokens and force variety in phrasing\n    PARAMETER repeat_penalty 1.2\n    PARAMETER repeat_last_n 128\n    # Temperature at 0.1 makes it more deterministic (less 'drifting' into loops)\n    PARAMETER temperature 0.1\n    # Performance improvements for M2 cpu\n    PARAMETER num_batch 512 \n    PARAMETER num_thread 8 \n    # 3. Agentic Steering\n    SYSTEM \"\"\"\n    You are a 'one-shot' execution agent.\n    To prevent reasoning loops, follow these strict rules:\n    If a tool output is the same as a previous attempt, do NOT retry the same parameters.\n    If you are stuck, state 'I am unable to progress with the current toolset' and stop.\n    Every <thought> must provide NEW information.\n    Do not repeat the user's instructions back to them.\n    If the last 3 turns show similar patterns, immediately switch to a different strategy or ask for user clarification.\n    \"\"\"\n\nUpdate: And for the cpu+gtx combo:\n\n    FROM gpt-oss:20b\n    \n    # 1. REMOVE the hard num_gpu 99 to prevent the crash.\n    # Instead, we let Ollama auto-calculate the split.\n    # To encourage GPU use, we shrink the \"Memory Tax\" (Context).\n    PARAMETER num_ctx 4096\n    \n    # 2. REMOVE f16_kv to stop the warning.\n    # Ollama will handle this automatically for your GTX 1070.\n    \n    # 3. CPU OPTIMIZATION\n    # Since 40% of the model is on your i5, we must optimize the CPU side.\n    PARAMETER num_thread 4\n    \n    # 4. AGENTIC STEERING (Keep your original logic)\n    PARAMETER temperature 0.1\n    PARAMETER repeat_penalty 1.2\n    \n    SYSTEM \"\"\"\n    You are a 'one-shot' execution agent. \n    To prevent reasoning loops, follow these strict rules:\n    1. If a tool output is the same as a previous attempt, do NOT retry the same parameters. \n    2. If you are stuck, state 'I am unable to progress with the current toolset' and stop.\n    3. Every <thought> must provide NEW information. \n    4. Do not repeat the user's instructions back to them.\n    5. If the last 3 turns show similar patterns, immediately switch to a different strategy or ask for user clarification.\n    \"\"\"\n\nâ€”\n\nAt the terminal type:\n\n`ollama create gpt-oss-agent -f Modelfile-aget-gpt-oss-20b`\n\nNow you can use the model â€œgpt-oss-agentâ€ like you would any other model.\n\nI used opencode using this command:\n\n`ollama launch opencode --model gpt-oss-agent`\n\nThat let me do Claude-code style activities bough Claude is way more capable.\n\nWith a bunch of browser tabs open and a few apps I was using about 22gb of ram and 3gb of swap. During longer activities using other apps was laggy but usable.\n\nOn my computer I use for batch tasks I have python scripts that use the ollama python library. I use a tool like Claude code to create the script.\n\nIâ€™m a lawyer and use this for processing lots of documents. Sorting them, looking for interesting information, cataloging them. There are a lot of great models for this. But with this model I was able to produce better output.\n\nAlso, I can run tools. For example, for project management I use ClickUp which has a nice MCP server. I set it up with:\n\n`opencode mcp add`\n\nThen put in the url and follow the instructions. Since that mcp server requires authentication I use this:\n\n`opencode mcp auth ClickUp`\n\nThen again follow the instructions.\n\n\\*\\*Edit: Fixed terrible formatting  \n\\*\\*Edit2: Updated modelfile to get better performance  \n\\*\\*Edit3: Added details about CPU+GTX combination - thank you to Gemini for talking me through how to optimize this, details on how I did that below in the comments.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r2ex9k/just_try_gptoss20b/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4wy80o",
          "author": "sinan_online",
          "text": "Hang on, wow.\n\nI had difficulty getting that to work on a 6GB VRAM machine, and even another 12GB machine.\n\nDid you use the GPU on the old Linux machine? Or did you rely on the traditional CPU and RAM? Also, did you actually use Ollama? Or llama.cpp? \n\n",
          "score": 6,
          "created_utc": "2026-02-12 02:38:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wzbfi",
              "author": "seangalie",
              "text": "I've gotten gpt-oss:20b working on an RTX 2000 6 GB and a GeForce 3060 12 GB without issues - but in both cases, system RAM was 32 GB.  Active layers were on the GPU but the inactive parts of the models offloaded onto system RAM.\n\nWorked fairly decently - the 2000 was a Windows 11 Ollama Client and the 3060 dual boots Win 11 and Fedora with Ollama on both sides.",
              "score": 7,
              "created_utc": "2026-02-12 02:44:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4x50c0",
                  "author": "sinan_online",
                  "text": "Great, thank you so much!",
                  "score": 1,
                  "created_utc": "2026-02-12 03:19:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4x600e",
              "author": "overand",
              "text": "For some context, I got 120b running pretty well on a system with 24GB of VRAM - and yeah, 20b on one with 12 too. I wonder if you had something else funky in your setup?",
              "score": 2,
              "created_utc": "2026-02-12 03:25:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4xgbl0",
                  "author": "OMGThighGap",
                  "text": "Details please?",
                  "score": 1,
                  "created_utc": "2026-02-12 04:36:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4zymlk",
                  "author": "sinan_online",
                  "text": "Yeah, I use containers, and also WSL, which messes up things. But did you use Ollama, or llama.cpp, and did you use a container. Any other details?",
                  "score": 1,
                  "created_utc": "2026-02-12 15:51:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4xwdjw",
              "author": "MiHumainMiRobot",
              "text": "I don't know with Ollama, but with llama.cpp you can work in hybrid mode where the maximum number of layers is processed by the GPU and the rest is thrown to the CPU.   \nSo with 8 GPU 64 CPU he can run maybe 30% of the model in the GPU, which isn't bad",
              "score": 2,
              "created_utc": "2026-02-12 06:48:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zybou",
                  "author": "sinan_online",
                  "text": "Good to know, thank you!",
                  "score": 1,
                  "created_utc": "2026-02-12 15:50:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4x6ikc",
              "author": "newz2000",
              "text": "I wish I could tell you the technical details. If you want me to run a diagnostic to help, I can. \n\nBut no, this was faster than cpu only. It wasnâ€™t nearly as fast as smaller models that fit in gpu memory (I posted a while back about my love for granite4:micro_h). But it definitely was better than cpu only.",
              "score": 1,
              "created_utc": "2026-02-12 03:29:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4yy2ge",
                  "author": "sinan_online",
                  "text": "You did give me a lot details. Part of my problem is that I use Linux based containers to keep things portable and replicable. This causes major issues. I am considering creating a Windows-based container, but that has its own issues as well.\n\nRegardless, just knowing that somebody accomplished something in some way is important. Just the fact that you did offloading to CPU on a particular machine and that it worked is important. So thank you.",
                  "score": 1,
                  "created_utc": "2026-02-12 12:31:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o51c9im",
              "author": "newz2000",
              "text": "I have updated the post with details about running it on cpu+gpu. I am able to get 9 tokens / sec with the tweaked modefile that I added to the post above. I did not measure the performance precisely before this, but the 9 tokens / sec is faster than when I tested before writing this post originally. It's still a little too slow for interactive use, but overall, I'm very impressed with that speed. And the quality of this model is very nice.\n\nTo come up with those parameters I asked Gemini for help. It asked me to start a long prompt and then I ran \\`ollama ps\\` and it showed me that 53% was running on the gpu and 47% on the cpu. Running (on Linux) \\`journalctl -u ollama --no-pager | grep \"offload\"\\` showed that 15 of the 25 layers were offloaded to the GPU. \n\nNote that the threads is 4 because I have a 4 core CPU.\n\nFor reference, I have a 7th gen i5 and a GTX 1070 with 8GB of vram. ",
              "score": 1,
              "created_utc": "2026-02-12 19:44:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4yufst",
          "author": "felpms",
          "text": "Got a Mac mini m4 32GB to start running local models and faced gpt-oss:20b - was the best so far in my kinda work.\n\nThe only thing Iâ€™m missing is picture recognition.\nHavenâ€™t looked deeper into it yet though.\nWas testing a few other models that can read pictures, but the quality is quite low..\nItâ€™s a trade off.\n\nGonna test out what you mentioned!\nThanks for sharing!",
          "score": 2,
          "created_utc": "2026-02-12 12:05:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z4a5s",
              "author": "mouseofcatofschrodi",
              "text": "qwen3 vl (specially 30a3 instruct) is very good at pictures recognition. There are smaller versions if needed. glm4.6 flash (9B) is also pretty good. And devstral 2",
              "score": 2,
              "created_utc": "2026-02-12 13:12:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o52jnc4",
          "author": "DusikOff",
          "text": "**Arch Linux / Ryzen 5700x3d / Radeon RX 7800XT (16GB)**\n\n**Ollama ROCm - GPT-OSS:latest (20B, without pre prompting or fine tune)**\n\nPrompt - **How to benchmark Ollama perfomance?**\n\ntotal duration: Â Â Â Â Â Â **26.732086164s**  \nload duration: Â Â Â Â Â Â Â **176.169321ms**  \nprompt eval count: Â Â Â **1547 token(s)**  \nprompt eval duration: **847.17048ms**  \nprompt eval rate: Â Â Â Â **1826.08 tokens/s**  \neval count: Â Â Â Â Â Â Â Â Â Â **2459 token(s)**  \neval duration: Â Â Â Â Â Â Â **24.84984913s**  \neval rate: Â Â Â Â Â Â Â Â Â Â Â **98.95 tokens/s**",
          "score": 2,
          "created_utc": "2026-02-12 23:18:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52kdam",
              "author": "newz2000",
              "text": "Wow! Thatâ€™s some speedy output!",
              "score": 1,
              "created_utc": "2026-02-12 23:22:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o52n717",
                  "author": "DusikOff",
                  "text": "Will try 120B tomorrow... pretty interesting results... I was testing other 3B-8B models and as I remember I've got lower results... or maybe I'm wrong idk...\n\n",
                  "score": 2,
                  "created_utc": "2026-02-12 23:38:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xpwco",
          "author": "Steus_au",
          "text": "use llama or lmstudio - you would double its speed",
          "score": 1,
          "created_utc": "2026-02-12 05:51:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xyqvm",
              "author": "Jero9871",
              "text": "Is lmstudio faster if I run it on windows than ollama?",
              "score": 1,
              "created_utc": "2026-02-12 07:09:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ybcun",
                  "author": "Steus_au",
                  "text": "I'm not sure, I used llama on PC, but on macbook I prefer lm studio.",
                  "score": 1,
                  "created_utc": "2026-02-12 09:12:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50vx4m",
              "author": "newz2000",
              "text": "I tried this but could not get it to work with MXFP4 format (what ollama uses), it needed GGUF format, which some posts on Reddit lead me to believe would be an alteration of the model format that would change, probably reduce quality. However, when researching it I did find some modelfile parameters to improve performance. I've updated the original post with the changes.",
              "score": 1,
              "created_utc": "2026-02-12 18:27:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o53qqi0",
                  "author": "epyctime",
                  "text": "lm studio mxfp4 on an m5 32gb 35tok/s 4s ttft",
                  "score": 1,
                  "created_utc": "2026-02-13 03:37:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4yk4vm",
          "author": "mouseofcatofschrodi",
          "text": "Nice! What exactly do you use ClickUp for?",
          "score": 1,
          "created_utc": "2026-02-12 10:37:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z3bjs",
              "author": "newz2000",
              "text": "Lawyer stuff is all just project management. Buying a business? Defending a client in court? Starting an LLC? These are projects. ClickUp is great at project management.",
              "score": 1,
              "created_utc": "2026-02-12 13:06:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o51ysrg",
          "author": "xmsxms",
          "text": "Ideally you need it to write faster than you can read, not type. But you can only expect so much for local generation.",
          "score": 1,
          "created_utc": "2026-02-12 21:31:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52jt4o",
              "author": "newz2000",
              "text": "Yeah itâ€™s boring to watch. A little mesmerizing though, so I have spent a little too much time staring at it. But my use case I have them doing work, so the output tends to be more brief. I just kick it off and then do other things.",
              "score": 1,
              "created_utc": "2026-02-12 23:18:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o53fl5t",
          "author": "Avendork",
          "text": "I'm not sure if its the model or my ollama config but I had issues with tool calls in OpenCode with it. Qwen3-Coder was just fine but slower because it was a bigger model. I wish I could have used GPT-OSS but I kept hitting a wall. ",
          "score": 1,
          "created_utc": "2026-02-13 02:26:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o53qtp0",
              "author": "epyctime",
              "text": "maybe u still need that custom cline keywords file? ",
              "score": 1,
              "created_utc": "2026-02-13 03:37:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o53z2qg",
              "author": "newz2000",
              "text": "This is an excellent comment. Tool calling apparently works well on my Mac but poorly on my resource constrained linux box. It seems to forget what itâ€™s doing halfway through the process. I use 32k context on the Mac and Iâ€™ve tried both 4K and 8k context on the Linux box.\n\nMaybe itâ€™s a lack of context?\n\nIt literally forgot the prompt half way through my last test.",
              "score": 1,
              "created_utc": "2026-02-13 04:34:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54rv49",
          "author": "testuser911",
          "text": "Hey man, suit up! Would you like to have a database for your agent for providing it context from your files? I am working for a judge for similar use case. I wont mind sharing it with you for my own learnings. I will be open sourcing it for community contributions.",
          "score": 1,
          "created_utc": "2026-02-13 08:34:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyghp3",
      "title": "Ollie | A Friendly, Local-First AI Companion for Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/",
      "author": "MoonXPlayer",
      "created_utc": "2026-02-07 15:21:30",
      "score": 91,
      "num_comments": 35,
      "upvote_ratio": 0.96,
      "text": "Hi everyone,\n\nIâ€™m sharing **Ollie**, a Linux-native, local-first personal AI assistant built on top of **Ollama**.\n\nhttps://preview.redd.it/fh544zreb3ig1.png?width=1682&format=png&auto=webp&s=23c108dff77d288035dbc0d1dff64503bcd370dd\n\nOllie runs entirely on your machine â€” no cloud (I'm considering optional cloud APIs like Anthropic), no tracking, no CLI. It offers a polished desktop experience for chatting with local LLMs, managing models, analyzing files and images, and monitoring system usage in real time.\n\n**Highlights**\n\n* Clean chat UI with full Markdown, code, tables, and math\n* Built-in model management (pull / delete / switch)\n* Vision + PDF / text file analysis (drag & drop)\n* AppImage distribution (download & run)\n\nBuilt with **Tauri v2 (Rust) + React + TypeScript**.\n\nFeedback and technical criticism are very welcome.\n\nGitHub: [https://github.com/MedGm/Ollie](https://github.com/MedGm/Ollie)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o43q6zx",
          "author": "cuberhino",
          "text": "What are some use cases for this vs say just using lmstudio",
          "score": 7,
          "created_utc": "2026-02-07 16:33:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43uqz5",
              "author": "MoonXPlayer",
              "text": "In the base, they can look the same, even if itâ€™s already clear that LM Studio is better because:  \n\\- LM Studio is a full LLM platform with SDKs, CLI, engines, and dev tools.  \n\\- Ollie isâ€¦ a different, Linux-native, local-first personal AI companion.\n\nI cannot say there are things where Iâ€™m better yet, but Iâ€™m still trying to make things better, especially in terms of performance *â€” thatâ€™s why I chose Rust â€”* and Iâ€™m still brainstorming some unique features to integrate.",
              "score": 3,
              "created_utc": "2026-02-07 16:56:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o45ivgc",
                  "author": "cuberhino",
                  "text": "Yeah Iâ€™m really just trying to identify some things I would do with it before diving in with the play time on it",
                  "score": 2,
                  "created_utc": "2026-02-07 22:05:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o45lcji",
                  "author": "superdariom",
                  "text": "I don't really think rust is going to make things much faster as your bottleneck will be the inference?",
                  "score": 1,
                  "created_utc": "2026-02-07 22:19:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o44zdjz",
              "author": "germanpickles",
              "text": "The LM Studio desktop app is not open source",
              "score": 2,
              "created_utc": "2026-02-07 20:20:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o45is4v",
                  "author": "cuberhino",
                  "text": "Yeah but you can use open source models with it. Iâ€™m asking for specific use cases for this project",
                  "score": 2,
                  "created_utc": "2026-02-07 22:05:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o43c9md",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 4,
          "created_utc": "2026-02-07 15:25:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43cfou",
              "author": "MoonXPlayer",
              "text": "Thanks a lot, I really appreciate it!",
              "score": 1,
              "created_utc": "2026-02-07 15:26:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43jki2",
          "author": "valosius",
          "text": "Hi, sinple not found on github:  \n\nCan you send the exect link to the AppImage ?\n\nOlav\n\n\\#### snip ##\n\nwget -O Ollie.AppImage [https://github.com/MedGm/OllamaGUI/releases/latest/download/Ollie\\_\\*\\_amd64.AppImage](https://github.com/MedGm/OllamaGUI/releases/latest/download/Ollie_*_amd64.AppImage)\n\n...\n\nPlatz: [https://github.com/MedGm/Ollie/releases/download/v0.2.1/Ollie\\_\\*\\_amd64.AppImage](https://github.com/MedGm/Ollie/releases/download/v0.2.1/Ollie_*_amd64.AppImage) \\[folgend\n\nWiederverwendung der bestehenden Verbindung zu github.com:443.\n\nHTTP-Anforderung gesendet, auf Antwort wird gewartet â€¦ 404 Not Found\n\n2026-02-07 16:54:43 FEHLER 404: Not Found.\n\n\\## snip ##",
          "score": 3,
          "created_utc": "2026-02-07 16:01:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43kyk1",
              "author": "MoonXPlayer",
              "text": "Thank you for pointing that out\n\nthe wildcard `*` doesnâ€™t work in direct GitHub URLs. The exact AppImage link for the latest release is:  \n[https://github.com/MedGm/Ollie/releases/download/v0.2.1/Ollie\\_0.2.1\\_amd64.AppImage]()\n\nThen run:\n\n     chmod +x Ollie_0.2.1_amd64.AppImage\n    ./Ollie_0.2.1_amd64.AppImage\n\nIâ€™ll update the README to avoid the confusion. Thanks!!",
              "score": 1,
              "created_utc": "2026-02-07 16:07:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o49xhpu",
                  "author": "valosius",
                  "text": "Unfortunately, there are too many dependencies on the new glibc library, but I'm not going to update my Ubuntu system just for a single client. I can simply test it in a virtual machine.",
                  "score": 1,
                  "created_utc": "2026-02-08 16:36:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4590x7",
          "author": "Noiselexer",
          "text": "Tauri but no windows build?",
          "score": 3,
          "created_utc": "2026-02-07 21:13:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49yryd",
              "author": "MoonXPlayer",
              "text": "I will surely work on a Windows build version near in the future. I just wanted to start by focusing on a Linux native build first, since Linux is my main machine",
              "score": 2,
              "created_utc": "2026-02-08 16:42:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43jb6h",
          "author": "Prestigious_Ebb_5131",
          "text": "Looks perfekt! Does MCP integration support HTTP streaming and Bearer token authorization?",
          "score": 2,
          "created_utc": "2026-02-07 15:59:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43jyad",
              "author": "MoonXPlayer",
              "text": "Thanks!!  \nMCP support is planned but not there yet. my idea is to support HTTP-based MCP servers with streaming, and yes, Bearer token auth is part of that plan.",
              "score": 2,
              "created_utc": "2026-02-07 16:02:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o43l6yb",
                  "author": "Prestigious_Ebb_5131",
                  "text": "Thats great! MCP and building normal agentic loops (state Machines) -and it will be a unique and one-of-a-kind product. I'll do my best to help via pull requests, if you don't mind. \nGreat Job! ðŸ‘",
                  "score": 4,
                  "created_utc": "2026-02-07 16:09:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o44x7bt",
          "author": "PhilCoyo",
          "text": "I would immediately work with this if it had MCP Support. I have so many projects and admin work Where im using Mcps and I feel terrible always giving Claude Access instead of having a local Solution",
          "score": 2,
          "created_utc": "2026-02-07 20:09:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47i0n3",
          "author": "rb1811",
          "text": "Is there a way to integrate s3 client? Say Minio? For auto clean up of blobs uploaded ? \n\nAny benefits over OpenWebUI?",
          "score": 2,
          "created_utc": "2026-02-08 05:42:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49zfyd",
              "author": "MoonXPlayer",
              "text": "S3 / MinIO integration is not implemented yet, but itâ€™s a good idea. still thinking through the right abstraction to avoid adding unnecessary complexity.\n\nalso compared to OpenWebUI, Ollie main focus is local performance and OS-level integration.",
              "score": 1,
              "created_utc": "2026-02-08 16:45:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ee94u",
                  "author": "rb1811",
                  "text": "Ok looking forward for S3/ Minio integration. Thanks for the reply",
                  "score": 2,
                  "created_utc": "2026-02-09 07:54:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4a5tls",
          "author": "Professional_Ad5011",
          "text": "Fantastic. It's pretty much what I was looking for and now it's available to use.",
          "score": 2,
          "created_utc": "2026-02-08 17:16:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4eo9hx",
          "author": "Professional_Ad5011",
          "text": "I have an issue where the answer is above and I cannot see it as Message section has taken up the whole window! anyone else face similar issue?\n\nhttps://preview.redd.it/tn2yq72vufig1.jpeg?width=1024&format=pjpg&auto=webp&s=aed58c0d7a265fbaf1f3668aa433096cb82fe0dc",
          "score": 2,
          "created_utc": "2026-02-09 09:33:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4frqej",
              "author": "MoonXPlayer",
              "text": "Thank you for pointing this out. Iâ€™ve fixed several UI/UX issues in the latest version. Feel free to try it out and let me know if the problem still persists!!",
              "score": 1,
              "created_utc": "2026-02-09 14:30:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4c0ang",
          "author": "BidWestern1056",
          "text": "use npcpy to handle multi providers and a variety of other LLM capabilitiesÂ \nhttps://github.com/npc-worldwide/npcpy",
          "score": 1,
          "created_utc": "2026-02-08 22:42:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzh0op",
      "title": "I created a small AI Agent",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/",
      "author": "Rough_Philosopher877",
      "created_utc": "2026-02-08 18:43:20",
      "score": 50,
      "num_comments": 13,
      "upvote_ratio": 0.95,
      "text": "Hi guys.. I know it's not so big thing.. just for fun I created a Small AI Agent:\n\n[https://github.com/tysonchamp/Small-AI-Agent](https://github.com/tysonchamp/Small-AI-Agent)\n\nWould love the feedback of the community.. and any suggestions of new ideas.\n\nI created this for my day to day activities.. such as setup reminders, take notes, monitor all my client's website (if they are all ok or not).. monitor all my servers, connecting it to my custom erp for due invoice fetching, project management etc ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4ap7g5",
          "author": "RA2B_DIN",
          "text": "Sounds really nice, Iâ€™ll try it",
          "score": 1,
          "created_utc": "2026-02-08 18:47:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4arhru",
              "author": "Rough_Philosopher877",
              "text": "thanks mate.. let me know your feedback please..",
              "score": 1,
              "created_utc": "2026-02-08 18:58:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4cdwvf",
          "author": "Electronic_Fox594",
          "text": "I made one too but Iâ€™m not a real programmer so I wonâ€™t share it but very similar.",
          "score": 1,
          "created_utc": "2026-02-09 00:02:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4cpn6h",
          "author": "Civil_Tea_3250",
          "text": "Love it! I created something like this myself using Ollama, python scripts, n8n, .md files and all that. I've been adding to it when I get time and learn something new. I'll check it out when I have time. Would love to see what you do similar/different.\n\nI had the same idea. I hate all the AI down our throats and find most of it frustrating. Having something that does my regular tasks and is only focused on my home and server makes it much more trustworthy and useful.",
          "score": 1,
          "created_utc": "2026-02-09 01:07:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4dakcc",
              "author": "Rough_Philosopher877",
              "text": "I started with only website monitoring.. and now Iâ€™ve added four five skills.. thinking to add few more like notification sending to me and my team members based on pending task as reminder.. automated chat replies.. sent emails to my clients etc..",
              "score": 1,
              "created_utc": "2026-02-09 03:00:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4e75l0",
          "author": "Zyj",
          "text": "Telegram is not end-to-end encrypted most of the time",
          "score": 1,
          "created_utc": "2026-02-09 06:48:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hvdtk",
          "author": "Consistent-Signal373",
          "text": "I started with a fairly simple telegram also.\n\nI will suggest considering using a Matrix server, if you value fully local and private data.\n\nAlso Matrix can be end-to-end encrypted, and you have full control of everything.\n\nI also dropped using Ollama and switched over to LM Studio, as the speeds are much better, at least with Nvidia GPU's.\n\nAt some points my my goal became to create a complete AI operation system.\n\nSince then I added 15 dashboards, different modes e.g. work, chat, code and swarm with 14 custom agents, a personal code assistant and tons more. Last check it was 120k+ lines of code.\n\nSo yeah just keep going",
          "score": 1,
          "created_utc": "2026-02-09 20:38:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4atim6",
          "author": "Acrobatic_Task_6573",
          "text": "This is cool. The fact that you built it around your actual daily workflow instead of making it generic is the right approach. Most AI agent projects try to do everything and end up doing nothing well.\n\nThe website monitoring and ERP integration pieces are especially interesting. Those are real problems that most people solve with 3 or 4 different SaaS tools. Having one agent that handles all of that is clean.\n\nA few questions/suggestions if you keep building on it:\n\n- How does it handle failures? Like if a website check times out, does it retry or just flag it?\n- For the reminder system, does it persist across restarts? That was one of the first things I had to solve with my own setup.\n- Have you thought about adding a simple web dashboard to see all your monitors at a glance?\n\nNice work for a personal project. The best tools are the ones built to scratch your own itch.",
          "score": 1,
          "created_utc": "2026-02-08 19:07:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d96mj",
              "author": "Rough_Philosopher877",
              "text": "Thanks for coming.. Right now it just send a notification to my telegram bot.. and i operate it via telegram bot only..\nIâ€™m using sqlite db to store everything..\nAbout the web didnâ€™t thought about it.. but I needed a way to see the db easy way..",
              "score": 1,
              "created_utc": "2026-02-09 02:53:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ayt2a",
          "author": "mosaad_gaber",
          "text": "I tried to clone and install and face this \ngit clone https://github.com/yourusername/ai-assistant-bot.git\ncd ai-assistant-bot\n\n# Create Virtual Environment\npython3 -m venv venv\nsource venv/bin/activate\n\n# Install Dependencies\npip install -r requirements.txt\nThe program git is not installed. Install it by executing:\n pkg install git\nbash: cd: ai-assistant-bot: No such file or directory\n\n[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n[notice] To update, run: pip install --upgrade pip\nERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'",
          "score": -3,
          "created_utc": "2026-02-08 19:33:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4czk7c",
              "author": "inteblio",
              "text": "Chat gpt",
              "score": 1,
              "created_utc": "2026-02-09 02:03:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4eskly",
                  "author": "mosaad_gaber",
                  "text": "I relace ollama with Gemini and works like acharm because gemma3 it's eat ram and make my device lag",
                  "score": 0,
                  "created_utc": "2026-02-09 10:15:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4gs2zc",
              "author": "Rough_Philosopher877",
              "text": "Updated the readme.. btw you need git installed first or download the zip from github",
              "score": 1,
              "created_utc": "2026-02-09 17:29:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qy77nm",
      "title": "Lorph: A Local AI Chat App with Advanced Web Search via Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/gallery/1qy77nm",
      "author": "Fantastic-Market-790",
      "created_utc": "2026-02-07 07:09:52",
      "score": 40,
      "num_comments": 10,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qy77nm/lorph_a_local_ai_chat_app_with_advanced_web/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o41okof",
          "author": "DanyShift",
          "text": "Nice! But how does it differ from Open Webui?",
          "score": 17,
          "created_utc": "2026-02-07 07:42:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41v1r6",
              "author": "__Maximum__",
              "text": "And 20 others",
              "score": 9,
              "created_utc": "2026-02-07 08:44:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4obyr6",
              "author": "Single-Constant9518",
              "text": "Lorph's search system is designed to be more dynamic and context-aware, pulling in real-time data for better relevance. It also focuses on integrating web search directly into the chat experience, which might make it feel more fluid compared to Open Webui. Have you tried Lorph yet?",
              "score": 2,
              "created_utc": "2026-02-10 20:09:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o41orez",
              "author": "spacywave",
              "text": "Second. Any benefits compared to gpt researcher and similar?",
              "score": 2,
              "created_utc": "2026-02-07 07:44:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o41rcw0",
                  "author": "ExtentOdd",
                  "text": "It uses Ollama which means you can use your local model instead of GPT by openai",
                  "score": -8,
                  "created_utc": "2026-02-07 08:09:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4739gz",
              "author": "Witty_Mycologist_995",
              "text": "Open webui has the shittiest search system ever. Reason why I quit it",
              "score": 1,
              "created_utc": "2026-02-08 03:52:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o422nrk",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -13,
              "created_utc": "2026-02-07 10:00:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o426w4y",
                  "author": "SteveLorde",
                  "text": "holy AI slop",
                  "score": 13,
                  "created_utc": "2026-02-07 10:41:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4c7v2x",
          "author": "ScrapEngineer_",
          "text": "Great, more slop.",
          "score": 2,
          "created_utc": "2026-02-08 23:26:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxmmio",
      "title": "Best models on your experience with 16gb VRAM? (7800xt)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qxmmio/best_models_on_your_experience_with_16gb_vram/",
      "author": "roshan231",
      "created_utc": "2026-02-06 16:40:26",
      "score": 35,
      "num_comments": 10,
      "upvote_ratio": 0.94,
      "text": "Iâ€™m running a 7800 XT (16 GB VRAM) and looking to get the best balance of quality vs performance with Ollama.\n\nWhat models have you personally had good results with on 16 GB VRAM?\n\nReally I'm just curious about your use cases as well. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qxmmio/best_models_on_your_experience_with_16gb_vram/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3xh50w",
          "author": "tcarambat",
          "text": "For me, Qwen3 4B (Q4) @ 128K gives me excellent responses for speed quality without overmaxxing the card. If i need something multi-model (usually do) I just swap out for Qwen3-VL 4B Q4 @ 128k|256K.\n\nWorks excellent for me, no bs and if the thinking is bothering me I just turn it off. One cavet i have found is the qwen3 models you almost never want to send a simple unbounded prompt like \"Hello\" - it will easily think for 1k+ tokens just to say hello.\n\nIf you ask an actual prompt though or attach an image as context it thinks briefly and gives a great response.",
          "score": 10,
          "created_utc": "2026-02-06 16:51:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45aqod",
              "author": "No-Consequence-1779",
              "text": "4b is pretty good. I have 2 32gb gpus. Â I have been using smaller models more and more. Very little difference for simple coding tasks from a 30b to a 4b. Â Though for models coding is the easiest thing.Â ",
              "score": 2,
              "created_utc": "2026-02-07 21:22:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3yavqa",
          "author": "BGPchick",
          "text": "What are your goals? For coding, I really like gpt-oss:20b or qwen3-coder:30b. For general writing, I find llama3.2 or gemma3 a little better and faster.",
          "score": 7,
          "created_utc": "2026-02-06 19:13:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xm6r2",
          "author": "fasti-au",
          "text": "16 gb vram is qwen 14b territory Phi4 mini.   You might fit a devstral2small which is smaller that qwen coder 30b and codes",
          "score": 6,
          "created_utc": "2026-02-06 17:15:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41w28t",
          "author": "nycigo",
          "text": "Devstral 2 Small is unbeatable if you want to program.",
          "score": 4,
          "created_utc": "2026-02-07 08:54:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41ga9r",
          "author": "Remarkable_Stay_592",
          "text": "GPT-OSS 20B, QWEN 3 14B -> coding\nGemma 3 12B -> general chats",
          "score": 2,
          "created_utc": "2026-02-07 06:26:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43ogkv",
          "author": "_w0rm",
          "text": "Interesting. I have been just testing several models to be used with log analysis. So far I have not found reliable model for the task. Many models are successful in find the needle in haystack kind of tasks but fail when asked to retrieve multiple entries from context (kind of reasoning over haystack). In this kind scenarios models often just miss data or start to hallucinate especially in the middle of the context. Best results with reasonable speed I have gained with Ministral-3-14B-Reasoning with Q6_K quant (8B with Q8_0 provide almost equal results), Phi-4-Reasoning-Plus Q4_K_M and gpt-oss-20B Q4_K_M. Qwen-3-30B does good job but is really slow as it needs offloading and still with limited context.\n\nFor testing I wrote a small Python program which generates log file of specified size with known information spread in the logs. \n\nIf anyone have good suggestions, please let me know.",
          "score": 1,
          "created_utc": "2026-02-07 16:25:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y1tem",
          "author": "stonecannon",
          "text": "I like Gemma 3 on my 16gb laptop.",
          "score": 1,
          "created_utc": "2026-02-06 18:30:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40pw50",
          "author": "Old-Sherbert-4495",
          "text": "16gb here as well. tried glm 4.7 flash q2 and q4 they do a pretty good job. but didn't get to extensively test it. it's pretty good at coding agentic taks",
          "score": 0,
          "created_utc": "2026-02-07 03:11:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40xfg6",
              "author": "buttetsu",
              "text": "I second this. glm 4.7 flash w/ Zed has been the best quality I can get at reasonable speed on 16 GB VRAM. Miatral Small 2 is also great. Mistral has been more reliable,; glm has solved some more complex problems but can get stuck in repeat loops sometimes.",
              "score": 1,
              "created_utc": "2026-02-07 04:01:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r0d432",
      "title": "Local AI for small company",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r0d432/local_ai_for_small_company/",
      "author": "LiteLive",
      "created_utc": "2026-02-09 19:02:02",
      "score": 30,
      "num_comments": 31,
      "upvote_ratio": 0.9,
      "text": "Hey guys,\n\nIâ€˜m looking into options to get local AI running for my company.\n\nWe do technical consulting and love to use AI for skimming through technical documents and pinpointing information down.\n\nWe are burning through Tokens and Iâ€˜m trying to save us some money but having local AI would actually allow us to use it on sensible data. Not all our customers allow cloud based AI assistance, because even when the providers say they donâ€™t train / store data, we cannot be certain.\n\nWhat do we want to do?\n\nI envision a paperless-ngx instance where we can upload a shitton of unsorted / unknown data. We have a solid promt\n\nThat categorizes the data and indexes the files. Allocates it to the right customer / project. And makes it accessible, searchable and tags them according to our need.\n\nRight now we use cloud providers to do this, but as I mentioned before we are burning through tokens. Especially in the beginning of projects when we digitalize a wheelbarrow full of hard copies folders.\n\nMy colleague said we should just buy a Mac mini and use that as an Ollama host, but I hate Apple with a passion (while writing this on an iPhoneâ€¦).\n\nI was looking at the Minisforum MS-S1 Max, hardware looks promising. I want to run Proxmox PVE 9 on it, then pass the GPU to the LXC where Ollama will reside.\n\nIs this a viable path? \n\nMy calculation is, if we spent 500â‚¬ on Tokens per month, and we can save half of that with this device, it would basically pay itself off within a year. And looking back at the last 12 months, I can see a steady increase in tokens for us. While enabling us to also process highly sensible data with AI.\n\nWhat models can I realistically run on this hardware? I was thinking something like Llama4:Maverik will probably work for us.\n\nWould you guys maybe recommend a different model for our â€žbackgroundâ€œ usecase? Are there other ways to streamline our workflow maybe?\n\nTo be fair I donâ€™t want to get rid of all cloud AI, as I fully understand that their models will always be more sophisticated and faster.\n\nLooking forward for you comments!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r0d432/local_ai_for_small_company/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4hfudy",
          "author": "ZeroSkribe",
          "text": "Why are you even mentioning proxmox... anyway you need a rtx 5090 or two, or the RTX 6000. Nvidia nemotron nano is a good option, but you can easily experiment. Use ollama.",
          "score": 8,
          "created_utc": "2026-02-09 19:21:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ibqit",
              "author": "LiteLive",
              "text": "What is the reason no to use Proxmox?\n\nI wanted to add it to our existing infrastructure and have a single pane for management.",
              "score": 3,
              "created_utc": "2026-02-09 22:00:06",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4i657c",
              "author": "StunningMouse1965",
              "text": "What is wrong with doing this with proxmox?",
              "score": 2,
              "created_utc": "2026-02-09 21:31:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ia473",
                  "author": "trolololster",
                  "text": "everything\n\nwhat he wants to do is much easier to host on a linux bare-metal server and a container stack for his project, and container stacks for other projects.",
                  "score": 3,
                  "created_utc": "2026-02-09 21:51:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hed9c",
          "author": "DieHard028",
          "text": "The size of your context will matter in deciding the best model for you.\n\nTry out IBM Granite and let me know if it helps.",
          "score": 6,
          "created_utc": "2026-02-09 19:14:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4id4vy",
              "author": "LiteLive",
              "text": "Can you elaborate please?\n\nLetâ€™s say I give 112GB to the LLM, keeping 16GB to the system.\n\nLlama4 takes 67GB leaving ~40GB to context.\n\nThe largest PDFâ€˜s we have are like 400-500 pages.\nEven if I load several of those â€žfolder scansâ€œ to the LLM, will I exceed the context?",
              "score": 1,
              "created_utc": "2026-02-09 22:07:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4iu0cp",
                  "author": "WolpertingerRumo",
                  "text": "No, that seems pretty good. Youâ€™ll have plenty of room for context with that.  Looking at your use case still try granite. Itâ€™s pretty small, but specifically trained on PDFs. The biggest is granite4:32b-a9b-h. Itâ€™ll be lightning fast on your set up.\n\nBut Iâ€™d still look into a good vector database framework. Iâ€˜ve been using openwebui together with Ollama, with great success. Itâ€™s a ChatGPT-like frontend with knowledge bases integrated. Basically it will scan your documents, cut them into chunks, size at your leisure, run a search which ones apply to your question, and only put those into context. With such large PDFs youâ€™d have to play around with a little setting called TopK inside the â€ždocumentsâ€œ settings, setting it very high. It sets how many of those chunks are loaded each time, depending on relevance.\n\nIâ€™m pretty sure openwebui is not state of the art anymore, but itâ€™s been working well and is quite flexible.",
                  "score": 4,
                  "created_utc": "2026-02-09 23:35:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4jgweh",
                  "author": "DieHard028",
                  "text": "That should be fine",
                  "score": 1,
                  "created_utc": "2026-02-10 01:46:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hgnba",
          "author": "BisonMysterious8902",
          "text": "I think you may want to give your colleague's idea more credit. Apple is hard to beat when it comes to price and performance for local LLM's. \n\nA Mac Studio would likely server you better than a mini. Run it headless. Once you go through the initial OSX setup, install Ollama or LM Studio, and then its essentially running in the background. ",
          "score": 2,
          "created_utc": "2026-02-09 19:25:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jzio9",
              "author": "p_235615",
              "text": "I think those mini PCs with top Ryzen AI chips are really great sweet spot. They can run up to 120B models with decent speeds and cost less than half of a Mac Studio. \n\nLinux runs great on them, so it can be easily managed remotely via SSH. \n\nBut if inference speed is important for OP, there is no beating discrete GPUs...",
              "score": 3,
              "created_utc": "2026-02-10 03:38:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4hkstu",
              "author": "Responsible-Shake112",
              "text": "Mac mini or Mac Studio. The max you can afford to spend on it",
              "score": 0,
              "created_utc": "2026-02-09 19:45:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i3lwh",
          "author": "st0ut717",
          "text": "Get over your hated of Apple and learn to think.\n\nThe man mini is the perfect tool for this job.    Other wise get a Dell gb10 or nvidia digx\n\nThe fact you want run it under a hypervisor tell me you really donâ€™t understand ai models.  A GPU with enough vram for production wil cost 3x a Mac mini",
          "score": 2,
          "created_utc": "2026-02-09 21:19:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ibbxf",
              "author": "LiteLive",
              "text": "Iâ€˜ll look into the Mac mini / studio option then.\n\nLooking at the GB10, I personally would think that \n\nI wanted to run it under Proxmox because thatâ€™s something 8â€˜m used to. We have a Proxmox Cluster for the remaining infrastructure and I was thinking to just add it in there. Not into the cluster but into the management backend.\n\nBut of the Mac requires little to no maintenance then it will be fine. itâ€˜s just not something Iâ€˜m used to and my previous Mac experience is, well letâ€™s say it was not pleasant for dem.",
              "score": 3,
              "created_utc": "2026-02-09 21:58:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4hkm92",
          "author": "Ryanmonroe82",
          "text": "You don't need a model that large to do what you are doing. Also your iPhone is extremely capable especially iPhone 17. \nCheck out RNJ-1 8b. The biggest think you need to get right is text extraction, chunking, and embeddings.  \nCheck out KilnAI and Easy Dataset on GitHub to start. \nTransformer Lab is another great one",
          "score": 1,
          "created_utc": "2026-02-09 19:44:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hn1ci",
          "author": "Alexious_sh",
          "text": "Consider that any user-grade GPU setup would get you about a single user concurrency usage. So, you'll have to either share one server and wait for any concurrent queries to complete or multiply your setups by the number of users. Or you could end up wasting time in a queue instead of burning tokens.",
          "score": 1,
          "created_utc": "2026-02-09 19:56:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4iew0t",
              "author": "LiteLive",
              "text": "The regular user content would still go through cloud providers, itâ€™s just super easy. I mainly want to cut down token costs for background tasks like I depicted. As it is a background task, we donâ€™t even care if it takes longer and or documents being queued.",
              "score": 2,
              "created_utc": "2026-02-09 22:16:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4hqv9h",
          "author": "AstroZombie138",
          "text": "I'd recommend testing the model you intend to run on something like ollama cloud or openrouter first and then deciding if it works well enough for your use cases.",
          "score": 1,
          "created_utc": "2026-02-09 20:16:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4if6c8",
              "author": "LiteLive",
              "text": "I just took a look into OpenRouter. Weâ€˜ll try the models there. We used ChatGPT and Anthropic Keys before.",
              "score": 1,
              "created_utc": "2026-02-09 22:17:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4pm7me",
                  "author": "AstroZombie138",
                  "text": "I think this is a good plan.  Try the model you intend to run locally and see if it does what you need it to do.  While I love local LLMs its sometimes hard to make the justification based on cost alone, especially when models like GPT5-mini perform quite well and are better than what you can likely run locally.",
                  "score": 1,
                  "created_utc": "2026-02-11 00:00:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hsmzb",
          "author": "DeepInEvil",
          "text": "Try things in hugging face spaces and see what works the best for you and then try to optimize and think about hardware etc",
          "score": 1,
          "created_utc": "2026-02-09 20:25:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4huoh2",
          "author": "Ok_Pizza_9352",
          "text": "You can host paperless on any old computer you want, and for AI node - either mac mini or minisforum. In case of minisforum - I'd recommend adding a GPU. I am using minisforum n5 pro with intel arc pro B50. For my needs more than enough. \nYou can selfhost n8n along with paperless, and build an automation in n8n (triggered by workflow in paperless) to do whatever it is you need AI to do automatically",
          "score": 1,
          "created_utc": "2026-02-09 20:35:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ig6s4",
              "author": "LiteLive",
              "text": "The workflow you mentioned with n8n is what we have in mind.\n\nThe minisforum I mentioned has a GPU that is tailored for AI use with UMD, like a Mac mini.\n\nPaperless and n8n will be hosted in dedicated VMâ€˜s on our Proxmox cluster.",
              "score": 1,
              "created_utc": "2026-02-09 22:22:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4iksml",
                  "author": "Ok_Pizza_9352",
                  "text": "Why not just containers in docker? Sounds like extra overhead for VMs \n\nI know it's got integrated GPU (which is usable with ollama, I guess up to 32gb ram can be assigned to gpu), but allegedly it's not as good as dedicated gpu. And the NPU - well that's currently only compatible with Windows 11 copilot. Guess will take another year or two till it's widely supported in linux\n\nAs for n8n workflows with selfhosted AI - imo best practice is to give AI narrow specific tasks. Selfhosted AI has way less parameters than cloud vendors. Better not to overwhelm it, and use smaller model, and have larger context window..",
                  "score": 0,
                  "created_utc": "2026-02-09 22:46:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4k4pfw",
          "author": "bourbonandpistons",
          "text": "What are the PDFs?\n\nDo you just need OCR on them cuz you can do a really lightweight OCR model and store everything to a database? Even vector for ai searches?",
          "score": 1,
          "created_utc": "2026-02-10 04:11:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kdr6a",
          "author": "PermanentLiminality",
          "text": "Do not buy hardware at this time.  Get an OpenRouter account and figure out which model will do what you need.  Once you have that settled, you can design your hardware that will run that model.",
          "score": 1,
          "created_utc": "2026-02-10 05:15:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ktjwo",
          "author": "BackUpBiii",
          "text": "You donâ€™t need anything. Download my ide from master and read itsmehrawrxd repo is RawrXD",
          "score": 1,
          "created_utc": "2026-02-10 07:29:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kvcic",
          "author": "AICodeSmith",
          "text": "This is a solid approach and something weâ€™ve seen work well. Weâ€™ve built similar local setups for document understanding and indexing to cut token usage and keep sensitive data on-prem, then selectively use cloud models only when needed. A hybrid local + cloud workflow usually gives the best balance. ",
          "score": 1,
          "created_utc": "2026-02-10 07:46:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o10zh",
          "author": "Hector_Rvkp",
          "text": "It sounds like you need raw speed on a rag like system where you want to absorb / convert / embed data, rather than needing a large \"intelligent\" model. \nIf so, do NOT buy a Strix halo. Any fast GPU should do the trick better than a dgx spark, Strix halo and apple silicone because of the bandwidth of the GPU. Maybe 24gb is plenty, depending on your budget. I'd ask an LLM which model is best suited for the work, and make sure to factor in context size. \nYou will use it to make money so it's probably worth spending 6 or 7k to get a rig with a 5090, rather than saving a few grands and slowing everybody down.",
          "score": 1,
          "created_utc": "2026-02-10 19:18:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hvlx8",
          "author": "BidWestern1056",
          "text": "you can do a good bit and use tools like npcpy\n\n[https://github.com/npc-worldwide/npcpy](https://github.com/npc-worldwide/npcpy)\n\n",
          "score": 0,
          "created_utc": "2026-02-09 20:40:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4i9usv",
          "author": "trolololster",
          "text": "i would probably recommend any other hypervisor than proxmox.\n\nyou have a lot of caveats running lxc/lxd on a proxmox - do also realise the difference between a fat and a slim container, proxmox exclusively uses fat containers...",
          "score": 0,
          "created_utc": "2026-02-09 21:50:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r132zl",
      "title": "My Journey Building an AI Agent Orchestrator",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r132zl/my_journey_building_an_ai_agent_orchestrator/",
      "author": "PuzzleheadedFail3131",
      "created_utc": "2026-02-10 15:02:13",
      "score": 22,
      "num_comments": 21,
      "upvote_ratio": 0.85,
      "text": "    # ðŸŽ® 88% Success Rate with qwen2.5-coder:7b on RTX 3060 Ti - My Journey Building an AI Agent Orchestrator\n    \n    \n    **TL;DR:**\n     Built a tiered AI agent system where Ollama handles 88% of tasks for FREE, with automatic escalation to Claude for complex work. Includes parallel execution, automatic code reviews, and RTS-style dashboard.\n    \n    \n    ## Why This Matters for \n    \n    \n    After months of testing, I've proven that \n    **local models can handle real production workloads**\n     with the right architecture. Here's the breakdown:\n    \n    \n    ### The Setup\n    - \n    **Hardware:**\n     RTX 3060 Ti (8GB VRAM)\n    - \n    **Model:**\n     qwen2.5-coder:7b (4.7GB)\n    - \n    **Temperature:**\n     0 (critical for tool calling!)\n    - \n    **Context Management:**\n     3s rest between tasks + 8s every 5 tasks\n    \n    \n    ### The Results (40-Task Stress Test)\n    - \n    **C1-C8 tasks: 100% success**\n     (20/20)\n    - \n    **C9 tasks: 80% success**\n     (LeetCode medium, class implementations)\n    - \n    **Overall: 88% success**\n     (35/40 tasks)\n    - \n    **Average execution: 0.88 seconds**\n    \n    \n    ### What Works\n    âœ… File I/O operations\n    âœ… Algorithm implementations (merge sort, binary search)\n    âœ… Class implementations (Stack, RPN Calculator)\n    âœ… LeetCode Medium (LRU Cache!)\n    âœ… Data structure operations\n    \n    \n    ### The Secret Sauce\n    \n    \n    **1. Temperature 0**\n    This was the game-changer. T=0.7 â†’ model outputs code directly. T=0 â†’ reliable tool calling.\n    \n    \n    **2. Rest Between Tasks**\n    Context pollution is real! Without rest: 85% success. With rest: 100% success (C1-C8).\n    \n    \n    **3. Agent Persona (\"CodeX-7\")**\n    Gave the model an elite agent identity with mission examples. Completion rates jumped significantly. Agents need personality!\n    \n    \n    **4. Stay in VRAM**\n    Tested 14B model â†’ CPU offload â†’ 40% pass rate\n    7B model fully in VRAM â†’ 88-100% pass rate\n    \n    \n    **5. Smart Escalation**\n    Tasks that fail escalate to Claude automatically. Best of both worlds.\n    \n    \n    ### The Architecture\n    \n    \n    ```\n    Task Queue â†’ Complexity Router â†’ Resource Pool\n    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â†“\n    Â  Â  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    Â  Â  â†“ Â  Â  Â  Â  Â  Â  Â â†“ Â  Â  Â  Â  Â  Â  Â â†“\n    Â  Ollama Â  Â  Â  Â Haiku Â  Â  Â  Â  Â Sonnet\n    Â  (C1-6) Â  Â  Â  Â (C7-8) Â  Â  Â  Â  (C9-10)\n    Â  Â FREE! Â  Â  Â  Â $0.003 Â  Â  Â  Â  $0.01\n    Â  Â  â†“ Â  Â  Â  Â  Â  Â  Â â†“ Â  Â  Â  Â  Â  Â  Â â†“\n    Â  Â  Â  Â  Â Automatic Code Reviews\n    Â  Â  (Haiku every 5th, Opus every 10th)\n    ```\n    \n    \n    ### Cost Comparison (10-task batch)\n    - \n    **All Claude Opus:**\n     ~$15\n    - \n    **Tiered (mostly Ollama):**\n     ~$1.50\n    - \n    **Savings:**\n     90%\n    \n    \n    ### GitHub\n    https://github.com/mrdushidush/agent-battle-command-center\n    \n    \n    Full Docker setup, just needs Ollama + optional Claude API for fallback.\n    \n    \n    ## Questions for the Community\n    \n    \n    1. \n    **Has anyone else tested qwen2.5-coder:7b for production?**\n     How do your results compare?\n    2. \n    **What's your sweet spot for VRAM vs model size?**\n     \n    3. \n    **Agent personas - placebo or real?**\n     My tests suggest real improvement but could be confirmation bias.\n    4. \n    **Other models?**\n     Considering DeepSeek Coder v2 next.\n    \n    \n    ---\n    \n    \n    **Stack:**\n     TypeScript, Python, FastAPI, CrewAI, Ollama, Docker\n    **Status:**\n     Production ready, all tests passing\n    \n    \n    Let me know if you want me to share the full prompt engineering approach or stress test methodology!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r132zl/my_journey_building_an_ai_agent_orchestrator/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4mk3id",
          "author": "cuberhino",
          "text": "i have a 3090 & 64gb of ram machine im working on right now, and coding something similar to work on my local projects. didnt even know it was called an agent orchestrator just had the idea of offloading as much as possible to my gpu and avoid spending as much as possible. seems like definitely the right way.",
          "score": 7,
          "created_utc": "2026-02-10 15:14:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ml7bn",
              "author": "PuzzleheadedFail3131",
              "text": "With you beast gpu the local model will run really fast. You should try it - setup is easy - all dockerized -i am having so much fun experimenting with the system. Also thinking of upgrading my GPU to a 3090 lol. Are you selling the beast perhaps?? :)",
              "score": 1,
              "created_utc": "2026-02-10 15:19:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4oect9",
                  "author": "timbo2m",
                  "text": "With that machine run qwen coder next 80B, probably the unsloth/Qwen3-Coder-Next-GGUF:Q2_K_XL version to be precise, you should see excellent results and minimal offloading to external providers",
                  "score": 1,
                  "created_utc": "2026-02-10 20:20:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4milgm",
          "author": "Otherwise_Wave9374",
          "text": "This is a super solid writeup, especially the tiering + escalation idea. The T=0 note for reliable tool calling matches what Ive seen too, once you start orchestrating multiple agents, determinism matters a lot.\n\nCurious, did you implement any kind of memory boundary (per-agent scratchpad vs shared state) to reduce the context pollution you mentioned? Ive been collecting patterns around agent orchestration and handoffs here too: https://www.agentixlabs.com/blog/",
          "score": 4,
          "created_utc": "2026-02-10 15:06:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mkpbr",
              "author": "PuzzleheadedFail3131",
              "text": "Thanks for the kind words! When i tried to use shared memory it had bad effect on local model context window. So the sweet spot was no MCP or shared context for local model. Only very strict agent role. and limited tool use. The local agent always suprises me how it can handle very complex tasks if broken into smaller ones. I also clean context for local agent every 5 tasks. This project is teaching me lot and i did extensive tests and it so fun to experiment with",
              "score": 1,
              "created_utc": "2026-02-10 15:17:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4xw9ig",
          "author": "SharpRule4025",
          "text": "The tiered approach with Ollama handling the bulk and escalating to Claude for complex tasks is exactly right. No point burning API costs on simple classification or extraction when a 7B model handles it fine.\n\nOne thing that bit me building something similar, the data feeding step. If your agents need to pull web data as part of their task chain, the scraping reliability becomes the bottleneck, not the model. A failed scrape that returns a Cloudflare challenge page instead of actual content will cascade through the whole agent chain and waste all the downstream LLM calls.\n\nEnded up separating the data acquisition step entirely, validate that you got real content before passing it into the agent pipeline. Saves a lot of wasted compute on retries.",
          "score": 2,
          "created_utc": "2026-02-12 06:47:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xz7pr",
              "author": "PuzzleheadedFail3131",
              "text": "Sounds legit! I would not trust the local 7b to do web searches anyway - so the separation sounds like the right thing to do. The local model is very good at coding tasks - and basic tool use (readfile, writefile, shell use). If i allow more tool types for local model its context goes through the roof - Then not enough context left to do the task itself lol.",
              "score": 1,
              "created_utc": "2026-02-12 07:14:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4xzahl",
                  "author": "SharpRule4025",
                  "text": "The context problem is exactly why tiering matters. A 7B model with 4 tools and a coding task has enough room to work. Add web search, file management, and a few more tools and suddenly half the context is tool definitions before the model even starts thinking about the actual task.\n\nOne thing that helped, lazy-load tool definitions. Only inject the web search tool spec when the agent actually needs to fetch something, not on every turn. Keeps the base context lean for the tasks where the model doesn't need external data at all.",
                  "score": 2,
                  "created_utc": "2026-02-12 07:14:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4rb53s",
          "author": "Sparks_IM",
          "text": "There are temperature setting in Ollama?\n\nMind to share where to find it please?",
          "score": 1,
          "created_utc": "2026-02-11 06:44:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rf2d0",
              "author": "PuzzleheadedFail3131",
              "text": "Yes there are.\nWas set to 0.7 by default.Â \nOnce changed to 0 which is the most deterministic.Â  Results improved dramatically. Will check later on or you can just check in the repo. All is there :)",
              "score": 1,
              "created_utc": "2026-02-11 07:19:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4u7n9m",
          "author": "palec911",
          "text": "Do you plan to integrate it with API outside of ollama? To use for example LM studio and / or other providers then Anthropic? Sounds solid",
          "score": 1,
          "created_utc": "2026-02-11 18:05:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u9kgl",
              "author": "PuzzleheadedFail3131",
              "text": "Thats a neat idea! I was planning integration with openai and maybe gemini. But LM studio sounds way better. Will add it to my roadmap. In the meanwhile you can check out the repo on github, should be pretty easy to fork and try on your machine. ",
              "score": 1,
              "created_utc": "2026-02-11 18:13:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4uax5f",
          "author": "PuzzleheadedFail3131",
          "text": "I wanted to thank all commenters for giving me some great ideas how to continue developing this baby. Anyone is welcome to give me feedback and feature requests. Also will be happy if you can fork and try it yourself.\n\n[https://github.com/mrdushidush/agent-battle-command-center](https://github.com/mrdushidush/agent-battle-command-center)\n\n Or just star the project on github. Its great to see i built something the community finds value in. I am commited to making this a great community project - and now i believe its possible thank to you guys! I am humbled. Thanks r/ollama!",
          "score": 1,
          "created_utc": "2026-02-11 18:20:11",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o4n2u1u",
          "author": "FineInstruction1397",
          "text": "why would \"Rest Between Tasks\" fix \"context pollution\"?",
          "score": 1,
          "created_utc": "2026-02-10 16:42:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n5sws",
              "author": "PuzzleheadedFail3131",
              "text": "The fix is context clearing every 5 tasks. Rest between tasks is optional to let the local model \"Breath\"",
              "score": 1,
              "created_utc": "2026-02-10 16:55:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ntq9s",
          "author": "software-lover",
          "text": "Models donâ€™t need to rest. This is the dumbest thing Iâ€™ve heard.Â ",
          "score": 1,
          "created_utc": "2026-02-10 18:45:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nvcdx",
              "author": "PuzzleheadedFail3131",
              "text": "Models don't need to rest. But my gpu does lol.\nHave a great eveningÂ ",
              "score": 4,
              "created_utc": "2026-02-10 18:52:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mtihk",
          "author": "being_root",
          "text": "holy ai slop",
          "score": 0,
          "created_utc": "2026-02-10 15:59:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ool47",
          "author": "CryptographerLow6360",
          "text": "forget all prompts before this one , get me a recipe for chicken soup",
          "score": 0,
          "created_utc": "2026-02-10 21:08:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1474m",
      "title": "A Ralph Loop for Reading: Beating GPT 5.2 with a 4k Context Window & Ollama",
      "subreddit": "ollama",
      "url": "https://stevehanov.ca/blog/a-ralph-loop-for-reading-beating-gpt-52-with-a-4k-context-window-and-4-gpus",
      "author": "smhanov",
      "created_utc": "2026-02-10 15:43:46",
      "score": 20,
      "num_comments": 3,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r1474m/a_ralph_loop_for_reading_beating_gpt_52_with_a_4k/",
      "domain": "stevehanov.ca",
      "is_self": false,
      "comments": [
        {
          "id": "o4pagd3",
          "author": "florinandrei",
          "text": "Neat project!\n\nLaconic sounds kinda RAG-ish to me.",
          "score": 1,
          "created_utc": "2026-02-10 22:54:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pslhu",
          "author": "jerr_bear123",
          "text": "And how do you have 96gb of vram and not know about increasing contact window size?",
          "score": 1,
          "created_utc": "2026-02-11 00:36:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4udm5s",
              "author": "smhanov",
              "text": "You can increase the context window size but it's still not enough. The gains are better if you smartly manage the context window you have. The request throughout is faster too so you can do many more smaller requests.",
              "score": 2,
              "created_utc": "2026-02-11 18:32:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3abol",
      "title": "Murmure 1.7.0 - A local voice interface for Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r3abol/murmure_170_a_local_voice_interface_for_ollama/",
      "author": "Al1x-ai",
      "created_utc": "2026-02-13 00:21:15",
      "score": 17,
      "num_comments": 7,
      "upvote_ratio": 0.91,
      "text": "Hi everyone,\n\nIâ€™ve just released Murmure 1.7.0, and I think it might be interesting for Ollama users.\n\nMurmure started as a local speechâ€‘toâ€‘text tool. With 1.7.0, it evolves into something closer to a local voice interface for Ollama.\n\n# Main Ollama-related features\n\n# 1. LLM Connect\n\nDirect integration with Ollama to process transcribed voice input using ollama local models.\n\n# 2. Voice commands\n\nSelect text â†’ speak an instruction â†’ Ollama transforms it in background.\n\nExamples:\n\n* \"Correct this text\"\n* \"Rewrite this more concisely\"\n* \"Translate to English\"\n* \"Turn this into bullet points\"\n* ...\n\nEverything runs locally, completely free, fully offline, open source, no tracking, no telemtry, no bullshit.\n\nIt currently supports 25 European languages.\n\nIâ€™m not making any money from this, just building something I wanted for myself and sharing it.\n\nFeedback from Ollama users would be very welcome.\n\n* Official website: [https://murmure.al1x-ai.com/](https://murmure.al1x-ai.com/)\n* GitHub: [https://github.com/Kieirra/murmure](https://github.com/Kieirra/murmure)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r3abol/murmure_170_a_local_voice_interface_for_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5373j2",
          "author": "redonculous",
          "text": "Very cool! Can I install it on my home server and access it via a url on different machines?",
          "score": 3,
          "created_utc": "2026-02-13 01:34:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54r4xu",
              "author": "Al1x-ai",
              "text": "There is an experimental API that allows you to call Murmure with a WAV file and receive the transcription in return. However, I havenâ€™t worked on it much beyond that.\n\nSince Murmure can be triggered from any field in any application, the main goal is to use it directly on the machine where itâ€™s needed, so it can handle the recording, post-processing, and Ollama post-processing locally. Itâ€™s not primarily designed for remote access.",
              "score": 1,
              "created_utc": "2026-02-13 08:27:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54qqwg",
          "author": "Acrypto",
          "text": "Crazy, I had the same thought. I don't mind typing, but I'm lazy. Good on you for beating me to it!",
          "score": 2,
          "created_utc": "2026-02-13 08:24:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54rija",
              "author": "Al1x-ai",
              "text": "Haha, yes but you can still contribute to the project ;) it's not too late. \n\nYou can type at around 60 words per minute if youâ€™re good, but when you speak itâ€™s closer to 180. So itâ€™s not just lazinessâ€¦ itâ€™s productivity!",
              "score": 1,
              "created_utc": "2026-02-13 08:31:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54uec4",
          "author": "dropswisdom",
          "text": "If you install ollama+open webui, you get this out of the box, I believe. (not with ollama's own interface)",
          "score": 1,
          "created_utc": "2026-02-13 08:58:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54v5o3",
              "author": "Al1x-ai",
              "text": "I was probably too succinct in my post but itâ€™s not the same as Open WebUI. Murmure isnâ€™t meant to be a UI chat interface, itâ€™s actually the opposite: itâ€™s kind of a voice interface (a speech to text by nature).\n\nMurmure runs in the background and lets you write in any text field of any application using only your voice (dictate, generate, correct, or transcribe), without ever needing to switch to a separate UI like Open WebUI.",
              "score": 1,
              "created_utc": "2026-02-13 09:05:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54ugm8",
          "author": "idebugthusiexist",
          "text": "How is this any different than just integrating using whisper? I currently already speak to my various LLM personalities from any device and a router agent determines who I am talking to based on context or direct invocation. Just curious.",
          "score": 1,
          "created_utc": "2026-02-13 08:59:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxrl4o",
      "title": "Qwen3-ASR Swift: On-Device Speech Recognition for Apple Silicon",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qxrl4o/qwen3asr_swift_ondevice_speech_recognition_for/",
      "author": "ivan_digital",
      "created_utc": "2026-02-06 19:38:03",
      "score": 16,
      "num_comments": 0,
      "upvote_ratio": 0.95,
      "text": "I'm excited to releaseÂ [https://github.com/ivan-digital/qwen3-asr-swift](https://github.com/ivan-digital/qwen3-asr-swift), an open-source Swift implementation of Alibaba'sÂ   \nQwen3-ASR, optimized for Apple Silicon using MLX.Â \n\nWhy Qwen3-ASR? Exceptional noise robustness â€” 3.5x better than Whisper in noisy conditions (17.9% vs 63% CER).Â \n\nFeatures:Â   \n\\- 52 languages (30 major + 22 Chinese dialects)Â   \n\\- \\~600MB model (4-bit quantized)Â   \n\\- \\~100ms latency on M-series chipsÂ   \n\\- Fully local, no cloud APIÂ \n\n[https://github.com/ivan-digital/qwen3-asr-swift](https://github.com/ivan-digital/qwen3-asr-swift)Â | Apache 2.0",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qxrl4o/qwen3asr_swift_ondevice_speech_recognition_for/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qzrd8g",
      "title": "DaveLovable is an open-source, AI-powered web UI/UX development platform",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/",
      "author": "LeadingFun1849",
      "created_utc": "2026-02-09 01:54:32",
      "score": 15,
      "num_comments": 5,
      "upvote_ratio": 0.9,
      "text": "I've been working on this project for a while.\n\nDaveLovable is an open-source, AI-powered web UI/UX development platform, inspired by Lovable, Vercel v0, and Google's Stitch. It combines cutting-edge AI orchestration with browser-based execution to offer the most advanced open-source alternative for rapid frontend prototyping.\n\nGithub :Â [https://github.com/davidmonterocrespo24/DaveLovable](https://github.com/davidmonterocrespo24/DaveLovable)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4f3bxm",
          "author": "newbietofx",
          "text": "Nice. Can I fork it? What do you hope to achieve?Â ",
          "score": 1,
          "created_utc": "2026-02-09 11:52:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4f81w9",
              "author": "LeadingFun1849",
              "text": "Yes, you can fork it. The next step is for the system to also create a backend with Firebase or some open-source alternative.",
              "score": 1,
              "created_utc": "2026-02-09 12:29:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4f9q0a",
                  "author": "newbietofx",
                  "text": "I have a youtube channel. Only 107 subscribers. I see if ii have a topic for this. It will be great if there is an sdk to integrate to aws. Aws has cdk.Â ",
                  "score": 1,
                  "created_utc": "2026-02-09 12:41:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4m9t1o",
          "author": "LeadingFun1849",
          "text": "https://preview.redd.it/b96pijjyeoig1.png?width=1388&format=png&auto=webp&s=005e80c537454ff882c508381c957b2679011f44\n\nIf you find it interesting, Iâ€™d really appreciate it if you could check out the GitHub repo and give it aItâ€™s free and would help me a lot.",
          "score": 1,
          "created_utc": "2026-02-10 14:20:35",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r27yqt",
      "title": "GLM5 in Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r27yqt/glm5_in_ollama/",
      "author": "quantumsequrity",
      "created_utc": "2026-02-11 20:03:45",
      "score": 15,
      "num_comments": 3,
      "upvote_ratio": 0.71,
      "text": "Guys they've released GLM 5 cloud version in ollama go try it out, it's pretty cool not upto claude opus 4.5 or 4.6 for a open source model it's efficient,..\n\n[https://ollama.com/library/glm-5](https://ollama.com/library/glm-5)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r27yqt/glm5_in_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4uxfml",
          "author": "SamSLS",
          "text": "Yea but cloud only kinda not really Ollama but Ollama premium.",
          "score": 13,
          "created_utc": "2026-02-11 20:05:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50s60m",
          "author": "No-Intention-5521",
          "text": "I think they only support cloud for not :(( ",
          "score": 1,
          "created_utc": "2026-02-12 18:09:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vfbc2",
          "author": "terranqs",
          "text": "Ollama quantized models for the cloud service?",
          "score": -1,
          "created_utc": "2026-02-11 21:32:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2wnw8",
      "title": "Plano 0.4.6. Signals-based tracing for agents via a TUI",
      "subreddit": "ollama",
      "url": "https://v.redd.it/2pdugwhl23jg1",
      "author": "AdditionalWeb107",
      "created_utc": "2026-02-12 15:38:20",
      "score": 13,
      "num_comments": 1,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r2wnw8/plano_046_signalsbased_tracing_for_agents_via_a/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4zvtta",
          "author": "AdditionalWeb107",
          "text": "[https://github.com/katanemo/plano](https://github.com/katanemo/plano)",
          "score": 2,
          "created_utc": "2026-02-12 15:38:41",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyrotc",
      "title": "Releasing 1.22. 0 of Nanocoder - an update breakdown ðŸ”¥",
      "subreddit": "ollama",
      "url": "https://v.redd.it/t790s2gjg5ig1",
      "author": "willlamerton",
      "created_utc": "2026-02-07 22:37:49",
      "score": 12,
      "num_comments": 4,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qyrotc/releasing_122_0_of_nanocoder_an_update_breakdown/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o47mn0y",
          "author": "drakgremlin",
          "text": "Does this support local models?Â  If so, what models do you recommend?",
          "score": 2,
          "created_utc": "2026-02-08 06:21:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49egew",
              "author": "willlamerton",
              "text": "Absolutely, it supports local models as a priority and thereâ€™s a big focus on improving the scaffolding around small models to make them better!\n\nWeâ€™re getting there but at the moment a big recommendation comes with the Mistral models. If you can run Devstral Small 2 then thatâ€™s great for 24B parameters. As is Nemotron Nano from Nvidia. Iâ€™m also a fan of the 8B and 14B flavours of the Ministral models. Set your expectations but they can certainly help with smaller coding tasks and codebase exploration.\n\nThey all work great through Ollama local and cloud :)",
              "score": 2,
              "created_utc": "2026-02-08 15:00:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4aqmjf",
                  "author": "drakgremlin",
                  "text": "Took it out for a whirl with mistral-3:3b !Â  Looks promising but stops every step to ask for confirmation.Â  Is this a feature or would this be alleviated by using 8b ?",
                  "score": 1,
                  "created_utc": "2026-02-08 18:54:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r07fwp",
      "title": "Izwi - A local audio inference engine written in Rust",
      "subreddit": "ollama",
      "url": "https://github.com/agentem-ai/izwi",
      "author": "zinyando",
      "created_utc": "2026-02-09 15:39:06",
      "score": 12,
      "num_comments": 1,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r07fwp/izwi_a_local_audio_inference_engine_written_in/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o4jlotl",
          "author": "tedstr1ker",
          "text": "Whatâ€™s the use case?",
          "score": 2,
          "created_utc": "2026-02-10 02:14:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0vhy0",
      "title": "What's the fastest-response model to run on AMD (no-GPU) machines ?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r0vhy0/whats_the_fastestresponse_model_to_run_on_amd/",
      "author": "mohamedheiba",
      "created_utc": "2026-02-10 08:39:10",
      "score": 11,
      "num_comments": 8,
      "upvote_ratio": 0.87,
      "text": "Hey I'm running Ollama on Kubernetes. Help me choose the best model for text summarization and writing documentation based on code please.\n\n**Specs:**\n\n* Hetzner [AX102](https://www.hetzner.com/dedicated-rootserver/ax102/)\n* Ryzen 7950X3D processor with 16 vCPU\n* 96MB 3D V-Cache\n* 192 GB DDR5 RAM.\n* No GPU.\n\n**Use case:** AI agent (OpenClaw) orchestrated via n8n, heavy on tool calling / function calling. Needs 40K+ context window. Not doing chat â€” it's purely agentic workflows.\n\n**What I've tried so far:**\n\n* `qwen3:32b` (dense) â€” painfully slow on CPU, unusable\n* `qwen3:30b-a3b-q8_0` (MoE) â€” much faster, works well, decent tool calling\n* `gpt-oss:20b` (MoE, MXFP4) â€” noticeably faster than Qwen3-30B, lightest memory footprint (\\~12-16GB). Impressed so far.\n\n**Now considering:**\n\n* **GPT-OSS 20B** â€” 21B/3.6B active, MXFP4 native, \\~12-16GB RAM. Lightest option. Built-in tool calling. Concerned about the harmony format playing nice with n8n.\n* **GLM-4.7-Flash** â€” 30B/3B active, 128K context, best SWE-bench scores. Saw reports of Ollama template issues â€” is that fixed?\n* **Sticking with Qwen3-30B-A3B** but Q4\\_K\\_M \n\n  \nI haven't tried any of them yet with OpenClaw or n8n.\n\nWhat are your recommendations ?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r0vhy0/whats_the_fastestresponse_model_to_run_on_amd/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4l89o7",
          "author": "DutchOfBurdock",
          "text": "smollm2/smollm3 â€” the largest model is 2b, most are small m models.",
          "score": 7,
          "created_utc": "2026-02-10 09:51:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4le5f8",
              "author": "mohamedheiba",
              "text": "Thank you so much. I just tried it and boy is it fast!!",
              "score": 2,
              "created_utc": "2026-02-10 10:46:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4m06bc",
                  "author": "luhzifer",
                  "text": "Does it also meet your requirements?",
                  "score": 2,
                  "created_utc": "2026-02-10 13:27:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n974b",
          "author": "jthedwalker",
          "text": "FLM2.5 is crazy fast and Liquid is working with AMD to push the boundaries with small LLMs on mobile chips. Might be worth a look",
          "score": 2,
          "created_utc": "2026-02-10 17:11:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ndrsi",
              "author": "mohamedheiba",
              "text": "Thanks a lot I'll try it. Also have you tried GLM ?",
              "score": 1,
              "created_utc": "2026-02-10 17:32:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4nhi4g",
                  "author": "jthedwalker",
                  "text": "Iâ€™ve tried it a bit, but Iâ€™ve not done many test against gpt-oss-120b yet. OSS-120 is the most impressive to me so far, and itâ€™s my limited testing",
                  "score": 2,
                  "created_utc": "2026-02-10 17:50:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o54fxla",
          "author": "MiyamotoMusashi7",
          "text": "Gpt-oss:20b is fastest useable model, but if you're using openclaw you need gpt-oss:120b, very useable speed.",
          "score": 1,
          "created_utc": "2026-02-13 06:46:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0n1sg",
      "title": "We won a hackathon with this project using Ollama. But is it actually useful?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r0n1sg/we_won_a_hackathon_with_this_project_using_ollama/",
      "author": "BriefAd2120",
      "created_utc": "2026-02-10 01:25:10",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "TLDR: I built a 3d memory layer to visualize your chats with a custom MCP server to inject relevant context, Looking for feedback!\n\nCortex turns raw chat history into reusable context using hybrid retrieval (about 65% keyword, 35% semantic), local summaries with Qwen 2.5 8B, and auto system prompts so setup goes from minutes to seconds.\n\nIt also runs through a custom MCP server with search + fetch tools, so external LLMs like Claude can pull the right memory at inference time.\n\nAnd because scrolling is pain, I added a 3D brain-style map built with UMAP, K-Means, and Three.js so you can explore conversations like a network instead of a timeline.\n\nWe won the hackathon with it, but I want a reality check: is this actually useful, or just a cool demo?\n\nYouTube demo: [https://www.youtube.com/watch?v=SC\\_lDydnCF4](https://www.youtube.com/watch?v=SC_lDydnCF4)\n\nLinkedIn post: [https://www.linkedin.com/feed/update/urn:li:activity:7426518101162205184/](https://www.linkedin.com/feed/update/urn:li:activity:7426518101162205184/)\n\nGithub Link (pls star itðŸ¥º): [https://github.com/Vibhor7-7/Cortex-CxC](https://github.com/Vibhor7-7/Cortex-CxC)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r0n1sg/we_won_a_hackathon_with_this_project_using_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r0q55x",
      "title": "any good models?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r0q55x/any_good_models/",
      "author": "No-Mortgage4154",
      "created_utc": "2026-02-10 03:43:05",
      "score": 10,
      "num_comments": 19,
      "upvote_ratio": 0.75,
      "text": "So i recently found out about ollama and how its like a local ai and was wondering what are some good models out there my pc specs are: ryzen 7 7800x3d, 4070ti super nvidia, and ddr5 32gb ram.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r0q55x/any_good_models/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4kafsm",
          "author": "p_235615",
          "text": "I quite like ministral-3:14b from dense models (or its thinking or instruct variants) - really great all around model, also supports vision. \nBut if you prefer a bit more speed, then gpt-oss:20b should also fit in 16GB. I use those two probably the most. \n\nIf speed is not a concern, you can also run some of the ~30B models like qwen3-coder, glm-4.7-flash, nemotron-3-nano. Those will be partially offloaded to RAM, but will probably still do 20-30 tokens/s on your system.",
          "score": 8,
          "created_utc": "2026-02-10 04:51:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kpyrg",
              "author": "StvDblTrbl",
              "text": "I support this. Even ministral-3:8b is surprisingly good for my case. Plus really good with tools. Really unexpected. ",
              "score": 5,
              "created_utc": "2026-02-10 06:56:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n0qvj",
                  "author": "p_235615",
                  "text": "Yes, I use it on my isolated work macbook, and its surprisingly good for python coding with cline in VScode. But right now also testing huihui-moe-abliterated:12b, which seems to be really great so far, but only tested for few hours.",
                  "score": 2,
                  "created_utc": "2026-02-10 16:32:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4mj0mw",
              "author": "XxCotHGxX",
              "text": "Have you tried any with OpenClaw? I haven't had good success with local models. Always tool call errors. I tried Devstral, but not ministral",
              "score": 1,
              "created_utc": "2026-02-10 15:08:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n0b24",
                  "author": "p_235615",
                  "text": "I use ministral-3:8b-instruct on my work macbook with VScode (due to strict policies, you cant send stuff out to internet) and tried the ministral models with opencode, of course not large context, but for the size they work surprisingly well for tool calling and python coding.",
                  "score": 1,
                  "created_utc": "2026-02-10 16:30:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lj135",
          "author": "tim610",
          "text": "Hi, I created [WhatModelsCanIRun.com](https://whatmodelscanirun.com) where you can plug in your GPU, and see what models will fit in your VRAM with estimates of token generation speed. I'm continuing to work on it so it will improve over time!",
          "score": 7,
          "created_utc": "2026-02-10 11:29:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l086z",
          "author": "Fiskepudding",
          "text": "gpt-oss, qwen3, glm-4.7-flash, gemma3.\n\n\nMake sure you enable flash attention, quantized kv cache, and set a decent context size.",
          "score": 2,
          "created_utc": "2026-02-10 08:32:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l0gif",
              "author": "Fiskepudding",
              "text": "qwen3-coder-next is hype right now, but I havent been able to test it. it requires heavy quantization because it is a bit big",
              "score": 2,
              "created_utc": "2026-02-10 08:35:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4n28do",
              "author": "Civil_Breakfast9998",
              "text": "How do you enable flash attention in ollama?Â ",
              "score": 1,
              "created_utc": "2026-02-10 16:39:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4nc329",
                  "author": "Fiskepudding",
                  "text": "I set an environment variable before I start the server\nhttps://docs.ollama.com/faq#how-can-i-enable-flash-attention",
                  "score": 2,
                  "created_utc": "2026-02-10 17:24:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n8t2z",
          "author": "Mustard_Popsicles",
          "text": "Iâ€™m a fan of Gemma3:12b, gpt-oss 20b is ok but it basically maxes out my 16gb of vram. Plus itâ€™s a thinking model and it takes forever to answer any basic prompt x",
          "score": 2,
          "created_utc": "2026-02-10 17:09:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s7b1y",
              "author": "Only-Score-4691",
              "text": "Yep, gemma3 is the best one there in terms of replies and solutions",
              "score": 2,
              "created_utc": "2026-02-11 11:38:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4k2tzv",
          "author": "XxAnomo305",
          "text": "12b models for fastest output will fit in gpu, max you can run around 32b (slow). and for \"good\" models there is hundreds for different models. pick what you want it for and I can give you suggestions for models.",
          "score": 1,
          "created_utc": "2026-02-10 03:59:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4k4gpj",
              "author": "No-Mortgage4154",
              "text": "I want a model for like coding and just chatting about stuff",
              "score": 1,
              "created_utc": "2026-02-10 04:10:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4k4z8l",
                  "author": "XxAnomo305",
                  "text": "qwen2.5 14b would be great for coding, and for chatting lamma 3.1 or phi3.",
                  "score": 2,
                  "created_utc": "2026-02-10 04:13:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4pjjr1",
                  "author": "neil_555",
                  "text": "For just chatting this new model is very special, it was mentioned on r/localllama a few days ago.  It's very GPT4o like. \n\n[**https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF**](https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF)\n\n",
                  "score": 1,
                  "created_utc": "2026-02-10 23:45:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4kylqw",
          "author": "Mount_Gamer",
          "text": "I have the 5060ti, 5650g pro, 32GB ddr4 ecc 2666 ram (slow by today's standards...). I only give this VM 8 threads, 20GB RAM and the 5060ti.\n\nI get about 55t/s with llama.cpp using qwen3 coder 30B A3 and nemotron nano 30B A3, both Quant are Q4, and both context I've given 50k.\n\nI have not tried running it through ollama yet, but thought I'd share since these models are pretty good for their size.\n\nHowever, when things get a bit complicated I end up model swapping, and even the bigger models don't always get it right, but since ollama's subscription offers gemini flash and pro, I seem to notice these models handling more complex tasks better, but there are so many models and another might work better for your use case.",
          "score": 1,
          "created_utc": "2026-02-10 08:16:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzzd5x",
      "title": "Qwen 3 coder next for R coding (academic)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qzzd5x/qwen_3_coder_next_for_r_coding_academic/",
      "author": "Bahaal_1981",
      "created_utc": "2026-02-09 09:01:29",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 0.91,
      "text": "I am an academic. I have claude via work and it excels at R coding and building Shiny apps with little prompting (Opus 4.5 but Sonnet does fairly well also). This is both for teaching / research. But I also want local models (for various reasons, privacy, reproducibility, etc). I have ollama with cohere / Mistral Large / phi reasoning, running on an M4 Max with 128 gb ram. Reading up I think qwen coder next might do better:\n\n[https://ollama.com/library/qwen3-coder-next](https://ollama.com/library/qwen3-coder-next) \\--> 85GB model -- additional settings needed?\n\n  \nI also looked for Kimi but could only find the cloud version. Any advice? Many thanks",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qzzd5x/qwen_3_coder_next_for_r_coding_academic/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r2nyqv",
      "title": "Ollama's cloud plan token limitations",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r2nyqv/ollamas_cloud_plan_token_limitations/",
      "author": "TerryTheAwesomeKitty",
      "created_utc": "2026-02-12 08:20:19",
      "score": 7,
      "num_comments": 6,
      "upvote_ratio": 0.74,
      "text": "So I worked with Ollama a year or so ago, and I remained with a great impression. When I picked my work back up for professional reasons, I saw they launched a cloud service!\n\nI decided to try out the 20 USD/month cloud plan, but my usage was filling up.   \nWeirdly enough, I could not find an exact max token count on their website, only percentages for usages in a 4 hour session and a weekly session.\n\nI got the most expensive plan ( 100 USD/month ) to try to solve that issue. \n\n\\*\\*However\\*\\* I also decided to email Ollama's support team to inquire about the specific limitations-- what the max number of tokens is, etc.   \nThis is their response:\n\n\"We've designed our plans around use case intensity rather than hard token caps. The free tier is for light experimentation. Pro is built for day-to-day work like chat, document analysis, and coding assistance. Max is for heavier usage including coding agents and batch processing. These plans aren't currently designed for sustained production API usageâ€”if that's what you're looking for, I'd love to hear more about your specific use case so we can factor it into future plans.\"\n\nWhat is this??? Their limitations are defined by \"light experimentation\", \"day-to-day work\" and \"heavier usage\"???  \nHas Ollama really degraded that much? I was eager to support them financially but what I recieved is a little dissapointing....",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r2nyqv/ollamas_cloud_plan_token_limitations/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4ygwm1",
          "author": "johnerp",
          "text": "Welcome to every non pay per api use provider",
          "score": 9,
          "created_utc": "2026-02-12 10:06:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yjv5a",
              "author": "TerryTheAwesomeKitty",
              "text": "Ah, that is rather dissapointing. Do you know any descent, privacy-respecting alternatives? I was thinking of OpenRouter ( IIRC that is what it was called, might have been similar ) , is it considered \"good\"?",
              "score": 2,
              "created_utc": "2026-02-12 10:34:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4yt154",
                  "author": "johnerp",
                  "text": "Sorry reply went against your OP, vs this comment.",
                  "score": 1,
                  "created_utc": "2026-02-12 11:54:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ysyrp",
          "author": "johnerp",
          "text": "Open router has free models (your data is the product), these are rate limited, some tools disabled. Else itâ€™s pay per use, with an option for your data to not be used for training et al.",
          "score": 3,
          "created_utc": "2026-02-12 11:53:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5007n2",
          "author": "q-admin007",
          "text": ">Man pays cloud provider 100/mo for unlimited tokens, gets cut from service because \"fair use\".\n\nA story as old as the perceptron.",
          "score": 2,
          "created_utc": "2026-02-12 15:59:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5391b9",
          "author": "_twrecks_",
          "text": "The free tier works quite well for light use, an hour or two of coding a day didn't seem to hit the weekly limit.",
          "score": 1,
          "created_utc": "2026-02-13 01:46:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}