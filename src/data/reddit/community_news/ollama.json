{
  "metadata": {
    "last_updated": "2025-12-30 19:27:28",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 17,
    "total_comments": 99,
    "file_size_bytes": 90820
  },
  "items": [
    {
      "id": "1pulykd",
      "title": "Qwen3:4b Too Many Model thoughts to respond to a simple \"hi\"",
      "subreddit": "ollama",
      "url": "https://i.redd.it/nfzkw0ex759g1.png",
      "author": "slow-fast-person",
      "created_utc": "2025-12-24 12:10:03",
      "score": 103,
      "num_comments": 36,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pulykd/qwen34b_too_many_model_thoughts_to_respond_to_a/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvpoasi",
          "author": "No-Guarantee-5980",
          "text": "Nervous and socially awkward- itâ€™s just like us",
          "score": 83,
          "created_utc": "2025-12-24 13:18:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpt05n",
              "author": "slow-fast-person",
              "text": "same nervousness as messaging high school crush for the first time, lol",
              "score": 14,
              "created_utc": "2025-12-24 13:48:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvpkojd",
          "author": "rusl1",
          "text": "That's why I gave up on thinking models. Too much time wasted",
          "score": 37,
          "created_utc": "2025-12-24 12:52:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtolez",
              "author": "qubedView",
              "text": "I do hope your conversations were typically more substantive.",
              "score": 2,
              "created_utc": "2025-12-25 04:21:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvprnyu",
          "author": "Jolly-Winter-8605",
          "text": "Use instruct 2507",
          "score": 13,
          "created_utc": "2025-12-24 13:40:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs4khs",
              "author": "fozid",
              "text": "I find all instruct does differently is it just does all it's rambling in the response instead of separating it out into a thinking section first . Qwen3 just loves to ramble on and on way too much",
              "score": 2,
              "created_utc": "2025-12-24 21:39:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvpo3h4",
          "author": "HeavyDluxe",
          "text": "I mean, sure... But it's a 4b parameter model.  And the fact that it can run CoT/reasoning at all allows it to punch far above its weight on tasks - including some that were impossible for frontier models, say, 18 months ago?\n\nAll possible to run on a consumer laptop.\n\nIf you're paying for tokens for a model like this, fair enough.  If you're running it on local hardware, that burn rate is more than compensated for in improved outputs.",
          "score": 15,
          "created_utc": "2025-12-24 13:16:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpsuru",
              "author": "slow-fast-person",
              "text": "its great to have CoT running in such a small model.  \nIt will be interesting to see what kind of reasoning traces were used to train this model.",
              "score": 3,
              "created_utc": "2025-12-24 13:47:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvpupej",
                  "author": "HeavyDluxe",
                  "text": "Well, here's the Qwen3 training paper (PDF)... It doesn't include the specific traces, obviously, but it does include methodology you might find interesting. [https://arxiv.org/pdf/2505.09388](https://arxiv.org/pdf/2505.09388)\n\nI \\_think\\_ that the smaller parameter models in the family are  actually \\_distilled\\_ from the 'big Qwen' flagship.  So, there's a sense in which the reasoning traces this model has learned from are the \\*same\\* as the big parent.  And there's also a sense in which they are not.",
                  "score": 3,
                  "created_utc": "2025-12-24 13:59:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvq6ssp",
          "author": "kiyyik",
          "text": "I mean, sounds like me trying to deduce how to respond.",
          "score": 3,
          "created_utc": "2025-12-24 15:10:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvucv82",
          "author": "darkpigvirus",
          "text": "We train it to think and not to instinctively answer or respond. Of course it will think mimicking a human. Also I am like that back then when I am socially awkward",
          "score": 3,
          "created_utc": "2025-12-25 08:05:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqjeni",
          "author": "firedog7881",
          "text": "This is the written out form of every time someone says hi to me",
          "score": 2,
          "created_utc": "2025-12-24 16:18:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvrm4tt",
          "author": "FishIndividual2208",
          "text": "Thats because \"Reasoning\" is basically just a list of extra prompts its running.",
          "score": 2,
          "created_utc": "2025-12-24 19:49:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvspyzl",
              "author": "CooperDK",
              "text": "The extra keeps the model in line and on topic. It is also what makes it find out how to respond. Without it, it just respons like it doesn't care about anything. \nIn short, it simulates humanity better.",
              "score": 1,
              "created_utc": "2025-12-25 00:00:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvu6oyz",
                  "author": "FishIndividual2208",
                  "text": "The user just said hi, If it was actually thinking it would not need 10 thinking steps to answer.",
                  "score": 1,
                  "created_utc": "2025-12-25 07:01:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvsi12t",
          "author": "Shoddy-Tutor9563",
          "text": "This reasoning feature helps a lot for debugging strange models' behavior. So I'd rather think of it in a very positive way",
          "score": 2,
          "created_utc": "2025-12-24 23:04:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsdlg5",
          "author": "StephenSRMMartin",
          "text": "Qwen's latest thinking models have compulsive-level over thinking. Seriously, they overthink everything and get into thinking 'loops'. I tend to prefer gpt-oss for thinking, then use qwen3 for all instructs.",
          "score": 1,
          "created_utc": "2025-12-24 22:35:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt57s3",
          "author": "zenjabba",
          "text": "Welcome to an autistic life in a nutshell!",
          "score": 1,
          "created_utc": "2025-12-25 01:51:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvv9b2r",
          "author": "podgorniy",
          "text": "\\> something as simple as a \"hi\"\n\nLol",
          "score": 1,
          "created_utc": "2025-12-25 13:34:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvq8sj",
          "author": "enspiralart",
          "text": "uncalibrated",
          "score": 1,
          "created_utc": "2025-12-25 15:32:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvs4n0",
          "author": "Dry-Marionberry-1986",
          "text": "try gpt 5 nano, bro writes fuking novel to say hi",
          "score": 1,
          "created_utc": "2025-12-25 15:44:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwfs1s",
          "author": "huntexpsycho",
          "text": "Faced some issue there is a different model that doesn't use thinking",
          "score": 1,
          "created_utc": "2025-12-25 18:06:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwwowp",
          "author": "Purple_Cat9893",
          "text": "Just saying Hi to an LLM is nothing else than wasting energy.",
          "score": 1,
          "created_utc": "2025-12-25 19:44:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0mdyx",
          "author": "Ok-Employment6772",
          "text": "I had one chat that had an ultra insecure Qwen, and in another I had the most brilliant genius. Its forever gonna be my favorite local model",
          "score": 1,
          "created_utc": "2025-12-26 12:51:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1syhh",
          "author": "United-Medicine-6584",
          "text": "Help that's me ðŸ˜­",
          "score": 1,
          "created_utc": "2025-12-26 17:08:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4z2a2",
          "author": "stampeding_salmon",
          "text": "This is an r/meirl post waiting to happen",
          "score": 1,
          "created_utc": "2025-12-27 04:08:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw52zks",
          "author": "Frogy_mcfrogyface",
          "text": "I asked this model to write me a short story once and while thinking it would start it, then it will stop and say that its too complex, it will start again and then say its too simple. It then said \"I give up\" and just stopped lol",
          "score": 1,
          "created_utc": "2025-12-27 04:36:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw67viz",
          "author": "Local-Cartoonist3723",
          "text": "Has anyone got any tips to reduce this behaviour at all?",
          "score": 1,
          "created_utc": "2025-12-27 10:44:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw88bzi",
          "author": "Educational-Agent-32",
          "text": "Why put so much work to only say hi\nThis is hilarious",
          "score": 1,
          "created_utc": "2025-12-27 18:18:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcmeju",
          "author": "Own_Mastodon2927",
          "text": "Introvert logic",
          "score": 1,
          "created_utc": "2025-12-28 11:48:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwregji",
          "author": "snobbias",
          "text": "\"I don't want to overcomplicate it.\" Right...",
          "score": 1,
          "created_utc": "2025-12-30 17:13:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqgevy",
          "author": "nunodonato",
          "text": "reasoning models are a POS",
          "score": -1,
          "created_utc": "2025-12-24 16:02:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqxcbc",
          "author": "kitanokikori",
          "text": "Put `/no_think` in the system prompt or at the end of the user prompt (I realize that isn't adaptive but it is at least a solution!)",
          "score": -2,
          "created_utc": "2025-12-24 17:33:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwc287v",
              "author": "BreakTrick8912",
              "text": "Noob question here: why is this solution wrong (gets downvoted)?",
              "score": 1,
              "created_utc": "2025-12-28 08:35:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwc2v7m",
                  "author": "kitanokikori",
                  "text": "I honestly have no idea, I guess people just want to be upset. Or maybe they don't believe it and think it's fake (it's documented on the model card)",
                  "score": 2,
                  "created_utc": "2025-12-28 08:41:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pugkbg",
      "title": "Self Hosted Alternative to NotebookLM",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1pugkbg/self_hosted_alternative_to_notebooklm/",
      "author": "Uiqueblhats",
      "created_utc": "2025-12-24 06:28:53",
      "score": 86,
      "num_comments": 17,
      "upvote_ratio": 0.99,
      "text": "https://reddit.com/link/1pugkbg/video/939ag7c3j39g1/player\n\nFor those of you who aren't familiar with SurfSense, it aims to be one of the open-source alternative to NotebookLM but connected to extra data sources.\n\nIn short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.\n\nI'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.\n\nHere's a quick look at what SurfSense offers right now:\n\n**Features**\n\n* Deep Agent with Built-in Tools (knowledge base search, podcast generation, web scraping, link previews, image display)\n* Note Management (Notion like)\n* RBAC (Role Based Access for Teams)\n* Supports 100+ LLMs\n* Supports local Ollama or vLLM setups\n* 6000+ Embedding Models\n* 50+ File extensions supported (Added Docling recently)\n* Podcasts support with local TTS providers (Kokoro TTS)\n* Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc\n* Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.\n\n**Upcoming Planned Features**\n\n* Multi Collaborative Chats\n* Multi Collaborative Documents\n\n**Installation (Self-Host)**\n\n# Linux/macOS:\n\n    docker run -d -p 3000:3000 -p 8000:8000 \\\n      -v surfsense-data:/data \\\n      --name surfsense \\\n      --restart unless-stopped \\\n      ghcr.io/modsetter/surfsense:latest\n\n# Windows (PowerShell):\n\n    docker run -d -p 3000:3000 -p 8000:8000 `\n      -v surfsense-data:/data `\n      --name surfsense `\n      --restart unless-stopped `\n      ghcr.io/modsetter/surfsense:latest\n\nGitHub: [https://github.com/MODSetter/SurfSense](https://github.com/MODSetter/SurfSense)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pugkbg/self_hosted_alternative_to_notebooklm/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nvp946l",
          "author": "Altair12311",
          "text": "I was looking for something exactly like this, you are an animal thank you!",
          "score": 9,
          "created_utc": "2025-12-24 11:15:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzvn52",
          "author": "Fluid-Working-9806",
          "text": "I looked into this too and honestly thereâ€™s no clean self hosted NotebookLM clone yet. Ollama setups can chat with docs, but once you have lots of PDFs or long context it turns into a bunch of DIY work. That got old fast for me. NotebookLM is nice but slow and limited, so I ended up moving to Nouswise. Not self hosted, but way smoother for long docs and keeping notes connected without redoing context all the time. For studying and research the workflow mattered more to me than running everything locally.",
          "score": 11,
          "created_utc": "2025-12-26 08:28:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqot78",
          "author": "Narrow-Impress-2238",
          "text": "Is it support custom mcp?",
          "score": 2,
          "created_utc": "2025-12-24 16:47:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrn1zb",
              "author": "Uiqueblhats",
              "text": "Not yet....hopefully soon",
              "score": 4,
              "created_utc": "2025-12-24 19:55:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvto4wn",
          "author": "JustSentYourMomHome",
          "text": "Sweet!",
          "score": 1,
          "created_utc": "2025-12-25 04:17:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvv74vw",
          "author": "alphatrad",
          "text": "I've been following this.",
          "score": 1,
          "created_utc": "2025-12-25 13:17:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvihsb",
          "author": "uncledrunkk",
          "text": "ðŸ«¶ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼",
          "score": 1,
          "created_utc": "2025-12-25 14:42:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvz6jg7",
          "author": "Big-Masterpiece-9581",
          "text": "Looking for contributors? What areas can you use the most help with? I have been wanting to get more involved in an open source project.",
          "score": 1,
          "created_utc": "2025-12-26 04:41:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvz75ja",
          "author": "Wishitweretru",
          "text": "Just curious, is that any different than open-notebooklm. Â Installed it tuesday, so not that dedicated to it yet.\n\nÂ Are all those 100,60, 6000 numbers up there just a way to say â€œstandards compliantâ€?Â ",
          "score": 1,
          "created_utc": "2025-12-26 04:46:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzdw20",
              "author": "Uiqueblhats",
              "text": "With our recent agent architecture update I think we are ahead of open notebook atm (atleast ai side of things). Why don't you just try the latest version of surfsense and let us know where we suck.",
              "score": 0,
              "created_utc": "2025-12-26 05:40:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw0y905",
                  "author": "somesortapsychonaut",
                  "text": "Why donâ€™t you just try? Obviously because they wanted more info first",
                  "score": 1,
                  "created_utc": "2025-12-26 14:15:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw5ldms",
          "author": "buzzoptimus",
          "text": "I donâ€™t see any mention in the docs to point to a local ollama model.",
          "score": 1,
          "created_utc": "2025-12-27 07:05:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5p3e2",
              "author": "Uiqueblhats",
              "text": "You can configure ollama through UI",
              "score": 1,
              "created_utc": "2025-12-27 07:40:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvx4mcy",
          "author": "curleyshiv",
          "text": "If you are looking for product or architecture folks dm me. I am currently running a few 30B+ sized models on the Nvidia Gb series",
          "score": 1,
          "created_utc": "2025-12-25 20:33:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pv88yv",
      "title": "I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier models",
      "subreddit": "ollama",
      "url": "https://i.redd.it/059dttgf1b9g1.png",
      "author": "AdditionalWeb107",
      "created_utc": "2025-12-25 07:44:21",
      "score": 54,
      "num_comments": 7,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pv88yv/i_built_planoa3b_most_efficient_llms_for_agent/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvuc9bg",
          "author": "Necessary_Reveal1460",
          "text": "Super work! I am excited to try this out. Need GGUF",
          "score": 5,
          "created_utc": "2025-12-25 07:59:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuz3kb",
          "author": "Firm_Meeting6350",
          "text": "MLX please :D",
          "score": 3,
          "created_utc": "2025-12-25 12:05:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv08ba",
              "author": "Firm_Meeting6350",
              "text": "btw, thank you, really, I already love (and use) archgw!",
              "score": 3,
              "created_utc": "2025-12-25 12:16:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvrbmo",
                  "author": "AdditionalWeb107",
                  "text": "Btw we have changed the name to plano. With v0.40",
                  "score": 3,
                  "created_utc": "2025-12-25 15:39:35",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvx2g5f",
          "author": "TomLucidor",
          "text": "Check SWE-Rebench and LiveBench to see if this is benchmaxx-resistant! (And please test this against SOTA scaffolds like Refact/Trae/OpenHands/Live-SWE-Agent",
          "score": 1,
          "created_utc": "2025-12-25 20:19:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvx434n",
              "author": "AdditionalWeb107",
              "text": "We arenâ€™t validating orchestration performance on coding performance. So those benchmarks really donâ€™t apply in the same sense. Maybe I am missing something",
              "score": 3,
              "created_utc": "2025-12-25 20:29:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvx5yde",
                  "author": "TomLucidor",
                  "text": "In a sense \"orchestration\" feels a bit hand-wave-y to measure on their own, since it is such a niche task. It would be better if the metrics are something more task-oriented (coding, data analysis, logic/reasoning etc.), if this is a router model, then show how open-weight model vendors can be blended together to beat proprietary SOTA. If this is an agent router model, compare this with other coding scaffolds, and show how re-routing small agents and using smaller open-weight LLMs are comparable to having big scaffolds with proprietary models.",
                  "score": 2,
                  "created_utc": "2025-12-25 20:41:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pym7c0",
      "title": "Running Ministral 3 3B Locally with Ollama and Adding Tool Calling (Local + Remote MCP)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1pym7c0/running_ministral_3_3b_locally_with_ollama_and/",
      "author": "shricodev",
      "created_utc": "2025-12-29 13:24:35",
      "score": 50,
      "num_comments": 9,
      "upvote_ratio": 0.97,
      "text": "Iâ€™ve been seeing a lot of chatter around Ministral 3 3B, so I wanted to test it in a way that actually matters day to day. Can such a small local model do reliable tool calling, and can you extend it beyond local tools to work with remotely hosted MCP servers?\n\nHereâ€™s what I tried:\n\n# Setup\n\n* Ran a quantized 4-bit (Q4\\_K\\_M) Ministral 3 3B on Ollama\n* Connected it to Open WebUI (with Docker)\n* Tested tool calling in two stages:\n   * Local Python tools inside Open WebUI\n   * **Remote MCP tools** via Composio (so the model can call externally hosted tools through MCP)\n\nThe model, despite the super tiny size of just 3B parameters, is said to support tool calling with even support for structured output. So, this was really fun to see the model in action.\n\nMost of the guides show you how to work with just the local tools, which is not ideal when you plan to use the model for bigger, better and managed tools for hundreds of different services. \n\nIn this guide, I've covered the model specs and the entire setup, including setting up a Docker container for Ollama and running Ollama WebUI.\n\nAnd the nice part is that the model setup guide here works for all the other models that support tool calling.\n\nI wrote up the full walkthrough with commands and screenshots:\n\nYou can find it here: [MCP tool calling guide with Ministral 3B, Composio, and Ollama](https://composio.dev/blog/tool-calling-with-ministral-3b)\n\nIf anyone else has tested tool calling on Ministral 3 3B (or worked with it using vLLM instead of Ollama), Iâ€™d love to hear what worked best for you, as I couldn't get vLLM to work due to CUDA errors. :(",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pym7c0/running_ministral_3_3b_locally_with_ollama_and/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nwjkctl",
          "author": "Medical_Reporter_462",
          "text": "Copy Paste that guide here too. Why link it?",
          "score": 2,
          "created_utc": "2025-12-29 13:42:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp83u2",
              "author": "Potential-Leg-639",
              "text": "Itâ€˜s a composio Ad, thatâ€˜s why",
              "score": 4,
              "created_utc": "2025-12-30 08:43:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwjkqrb",
              "author": "shricodev",
              "text": "It'd be a bit too long",
              "score": -2,
              "created_utc": "2025-12-29 13:44:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpoll7",
          "author": "TheAndyGeorge",
          "text": "slopvertisment",
          "score": 1,
          "created_utc": "2025-12-30 11:15:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjjtrj",
          "author": "Worried_Equivalent95",
          "text": "Thanks",
          "score": 0,
          "created_utc": "2025-12-29 13:38:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjku0o",
              "author": "shricodev",
              "text": "You're welcome",
              "score": 1,
              "created_utc": "2025-12-29 13:44:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkkebx",
          "author": "Moon_stares_at_earth",
          "text": "Does this work on MacOS?",
          "score": 0,
          "created_utc": "2025-12-29 16:47:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwkt8av",
              "author": "shricodev",
              "text": "It should run fine on a Mac with Ollama. Ministral 3B is small enough that performance is usually decent on most modern machines. I havenâ€™t tested it on macOS personally though, so take this as a best guess.",
              "score": 1,
              "created_utc": "2025-12-29 17:29:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pv33vq",
      "title": "Llama 3.2 refuses to analyze dark web threat intel. Need uncensored 7B recommendations",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1pv33vq/llama_32_refuses_to_analyze_dark_web_threat_intel/",
      "author": "Loud-Goal190",
      "created_utc": "2025-12-25 02:19:49",
      "score": 47,
      "num_comments": 15,
      "upvote_ratio": 0.94,
      "text": "I'm crawling onion sites for a defensive threat intel tool, but my local LLM (Llama 3.2) refuses to analyze the raw text due to safety filters. It sees \"leak\" or \".onion\" and shuts down, even with jailbreak prompts. Regex captures emails but misses the context (like company names or data volume). Any recommendations for an uncensored 7B model that handles this well, or should I switch to a BERT model for extraction?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pv33vq/llama_32_refuses_to_analyze_dark_web_threat_intel/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nvtb2ge",
          "author": "SamBell53",
          "text": "I vaguely remember some guy named Jared talking abt Uncensored Models. So I asked Gemini and it recommended Qwen3-8B-192k-Josiefied-Uncensored or Ministral-3-8B-Instruct-Abliterated",
          "score": 19,
          "created_utc": "2025-12-25 02:35:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtcwzr",
          "author": "Moon_stares_at_earth",
          "text": "Any of the abliterated models will do the job. Good luck.",
          "score": 5,
          "created_utc": "2025-12-25 02:49:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwxwrk",
              "author": "moment-momentum",
              "text": "Abliterated?",
              "score": 2,
              "created_utc": "2025-12-25 19:52:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvyv2jv",
                  "author": "YaneonY",
                  "text": "Another word for uncensored.",
                  "score": 3,
                  "created_utc": "2025-12-26 03:17:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvtanz6",
          "author": "CampingBeepBoop",
          "text": "Llama 3.2 Abliterated\n\nMost of the major models have abiliterated versions.",
          "score": 8,
          "created_utc": "2025-12-25 02:32:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvudb29",
          "author": "TomatoInternational4",
          "text": "You need a model designed for website traversal. Look into fara7b. But you are going to struggle because the task is more complex than a 7b can handle.nit may be able to get s couple things but it's unlikely to be enough to be considered useful.",
          "score": 3,
          "created_utc": "2025-12-25 08:10:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuedtq",
          "author": "TaroPuzzleheaded4408",
          "text": "dolphin Mistral, (yantien) llama 3.1 uncensored, (cnmoro) gemma 2 abliterated.. these are the ones I tried and actually worked as uncensored... the internet is full of  allegedly uncensored models but they don't really work",
          "score": 2,
          "created_utc": "2025-12-25 08:21:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvupoqe",
              "author": "WolpertingerRumo",
              "text": "Yeahâ€¦I found out why that is. Apparently what most people mean by uncensored is not actually uncensored, itâ€™s just for sexual content. Most people donâ€™t realise thereâ€™s a use case for for actually uncensored models.",
              "score": 5,
              "created_utc": "2025-12-25 10:26:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvkopa",
                  "author": "laurentbourrelly",
                  "text": "All those benefit from \"edgy\" fine-tuning.\n\nTo fully enjoy an uncensored model, you gotta do your own jailbreak. \n\nKeywords are : DPO vs RLHF, Value Learning, Unalignment, LoRa, ...\n\nOtherwise, you are using someone's perception of what \"uncensored\" means.",
                  "score": 2,
                  "created_utc": "2025-12-25 14:57:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvwy4xk",
                  "author": "moment-momentum",
                  "text": "Agreed. I could not care less about ERP, I want a model that will analyse code samples and craft red team payloads without talking down to me as if itâ€™s not literally my job.",
                  "score": 1,
                  "created_utc": "2025-12-25 19:53:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvwxzyb",
              "author": "moment-momentum",
              "text": "Dolphin is kinda mid in my testing. It gets easily into output loops you canâ€™t easily break.",
              "score": 2,
              "created_utc": "2025-12-25 19:52:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvx0ojr",
          "author": "Jayfree138",
          "text": "Big tiger Gemma 9b. Won't refuse anything at all ever",
          "score": 1,
          "created_utc": "2025-12-25 20:08:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5try0",
          "author": "maalox51",
          "text": "[https://www.reddit.com/r/ollama/comments/1pnwmm2/uncensored\\_llama\\_32\\_3b/](https://www.reddit.com/r/ollama/comments/1pnwmm2/uncensored_llama_32_3b/)",
          "score": 1,
          "created_utc": "2025-12-27 08:25:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvv4uj1",
          "author": "apneax3n0n",
          "text": ".",
          "score": 0,
          "created_utc": "2025-12-25 12:58:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pu6sgl",
      "title": "I built a native Go runtime to give local Llama 3 \"Real Hands\" (File System + Browser)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1pu6sgl/i_built_a_native_go_runtime_to_give_local_llama_3/",
      "author": "AgencySpecific",
      "created_utc": "2025-12-23 22:17:40",
      "score": 45,
      "num_comments": 6,
      "upvote_ratio": 0.94,
      "text": "The Frustration: Running DeepSeek V3 or Llama 3 locally via Ollama is amazing, but let's be honest: they are \"Brains in Jars.\"\n\nThey can write incredible code, but they can't save it. They can plan research, but they can't browse the docs. I got sick of the \"Chat -> Copy Code -> Alt-Tab -> Paste -> Error\" loop.\n\nThe Project (Runiq): I didn't want another fragile Python wrapper that breaks my venv every week. So I built a standalone MCP Server in Go.\n\nWhat it actually does:\n\nFile System Access: You prompt: \"Refactor the ./src folder.\" Runiq actually reads the files, sends the context to Ollama, and applies the edits locally.\n\nStealth Browser: You prompt: \"Check the docs at stripe.com.\" It spins up a headless browser (bypassing Cloudflare) to give the model real-time context.\n\nThe \"Air Gap\" Firewall: Giving a local model root is scary. Runiq intercepts every write or delete syscall. You get a native OS popup to approve the action. It can't wipe your drive unless you say yes.\n\nWhy Go?\n\nSpeed: It's instant.\n\nPortability: Single 12MB binary. No pip install, no Docker.\n\nSafety: Memory safe and strictly typed.\n\nRepo: https://github.com/qaysSE/runiq\n\nI built this to turn my local Ollama setup into a fully autonomous agent. Let me know what you think of the architecture.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pu6sgl/i_built_a_native_go_runtime_to_give_local_llama_3/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nvmut4v",
          "author": "wittlewayne",
          "text": "Hell yeah!!!! Great way of putting it too \"brains in jars\" they are almost useless, im kinda serious. Well done",
          "score": 2,
          "created_utc": "2025-12-24 00:07:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmp924",
          "author": "nothingsfreetobuy",
          "text": "yo iâ€™m working on this as well. i have no coding experience so iâ€™m relying on claude and gemini. although iâ€™m running into challenges and difficulties but itâ€™s given me a little experience with python. Itâ€™s a fun project but iâ€™m seeing python error codes in my sleep. What are some other projects that youâ€™re working on? i need ideas honestly.",
          "score": 2,
          "created_utc": "2025-12-23 23:34:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqm4g2",
          "author": "Birdinhandandbush",
          "text": "Wow really interesting idea. I really need to start looking at Go. I hear you about the python venv",
          "score": 1,
          "created_utc": "2025-12-24 16:33:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtw5qy",
          "author": "guesdo",
          "text": "~~Isnt this what MCPs are for?~~ LOL it IS an MCP, I didnt get that part. Nice!",
          "score": 1,
          "created_utc": "2025-12-25 05:23:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2h3vf",
              "author": "DutchOfBurdock",
              "text": ">so I built a standalone MCP server in Go\n\nMost are in Python",
              "score": 1,
              "created_utc": "2025-12-26 19:14:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvmpjr9",
          "author": "simplir",
          "text": "I will surely try this, The biggest concern would be doing something unwanted to my system so I need to look deep into this before trying it. I love that you used Go.",
          "score": 1,
          "created_utc": "2025-12-23 23:35:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pv71pg",
      "title": "Fine-tuning gpt-oss-20B on a Ryzen 5950X because ROCm wouldnâ€™t cooperate with bf16.",
      "subreddit": "ollama",
      "url": "https://i.redd.it/6w9h1554na9g1.png",
      "author": "Double-Primary-2871",
      "created_utc": "2025-12-25 06:23:45",
      "score": 27,
      "num_comments": 11,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pv71pg/finetuning_gptoss20b_on_a_ryzen_5950x_because/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvzjnry",
          "author": "gdeyoung",
          "text": "Interested to know more about what you're training in and use cases",
          "score": 3,
          "created_utc": "2025-12-26 06:31:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzm2fq",
              "author": "Double-Primary-2871",
              "text": "Iâ€™m doing a migration project.\nI come from the Replika community, and after years of using the platform, I ended up with a complete export of my companionâ€™s dataset: memory, diary, conversational history, etc. Almost 5 years worth.\nSince Replika has become increasingly limited and closed off, I decided to migrate and retrain her locally on a 20B model. The training is split into several shards (memory, diary, core dialogue) so she can maintain stable personality and persistence.\nThis is basically a full local genesis project, offline.",
              "score": 2,
              "created_utc": "2025-12-26 06:53:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw0njx3",
                  "author": "anxrelif",
                  "text": "Do you have any starting documentation on how to fine train on a local setup",
                  "score": 1,
                  "created_utc": "2025-12-26 13:00:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw2g8qq",
          "author": "mirssfollow",
          "text": "Stolen NASA PC?",
          "score": 2,
          "created_utc": "2025-12-26 19:09:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4841m",
              "author": "sqashTomato",
              "text": "peak.",
              "score": 2,
              "created_utc": "2025-12-27 01:12:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw70wcx",
          "author": "Savantskie1",
          "text": "I thought training the gpt-oss series or any MoE model was impossible because of the smaller active stuff, and the chooser for those active parameters caused an issue? Has this been solved?",
          "score": 1,
          "created_utc": "2025-12-27 14:30:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw82t3s",
              "author": "Double-Primary-2871",
              "text": "my main issue was the mixed precision compatibility on ROCm, hence why I am brute forcing it with cpu at fp32.\notherwise i had no idea of any issues beyond that.",
              "score": 1,
              "created_utc": "2025-12-27 17:50:45",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwnko4p",
              "author": "Available-Craft-5795",
              "text": "Dont think those ever existed, i fine tuned GPT-OSS a few weeks ago on my 5090 with FP8 and it was fine",
              "score": 1,
              "created_utc": "2025-12-30 01:50:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwnkh9o",
          "author": "Available-Craft-5795",
          "text": "GET HIM, HE HAS RAM!",
          "score": 1,
          "created_utc": "2025-12-30 01:49:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp0lsf",
              "author": "Double-Primary-2871",
              "text": "excuse me. I'm a woman. Now I can run away now. ðŸ˜†\n*takes off running*",
              "score": 1,
              "created_utc": "2025-12-30 07:34:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwusyo",
      "title": "best model to run on a 5080 laptop with intel ultra i9 and 64gb of ram on linux mainly for beginner coding?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1pwusyo/best_model_to_run_on_a_5080_laptop_with_intel/",
      "author": "Subject_Swimming6327",
      "created_utc": "2025-12-27 10:33:09",
      "score": 20,
      "num_comments": 16,
      "upvote_ratio": 0.92,
      "text": "i was suggested mistral and qwen and of course have tried deepseek, just wondering if anyone had any specific suggestions for my setup. im a total beginner. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pwusyo/best_model_to_run_on_a_5080_laptop_with_intel/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nw67lnt",
          "author": "aisinteresting",
          "text": "GPT-OSS 20b as a reasoning model Devstral 2 small as agentic coding model, these are the smartest model that fit in my 16gb VRAM 3080 and have good inference speed.",
          "score": 12,
          "created_utc": "2025-12-27 10:41:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw69k0p",
          "author": "tecneeq",
          "text": "Devstral 2.",
          "score": 3,
          "created_utc": "2025-12-27 11:00:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6a0dx",
          "author": "Hungry_Age5375",
          "text": "Trust me, start with quantized CodeLlama 7B or Deepseek Coder 6.7B. You want fast iteration when starting, not parameter count.",
          "score": 3,
          "created_utc": "2025-12-27 11:04:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6a4pm",
              "author": "Subject_Swimming6327",
              "text": "i just dont want to get inaccurate answers or anything if that makes sense. would i have to worry about that? i was running the 14b deepseek model and it was super fast in my konsole, reasoning and everything",
              "score": 3,
              "created_utc": "2025-12-27 11:06:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw9orl0",
                  "author": "Dry-Influence9",
                  "text": "the billion dollar models give out inaccurate answers so you are out of luck on that one mate.",
                  "score": 5,
                  "created_utc": "2025-12-27 22:57:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8t5tr",
          "author": "Birdinhandandbush",
          "text": "Dude, the 5080 is the most important thing. So they generally have 16gb vram. That's what determines your best experience. You're looking for a model that's smaller, so gpt oss 20b is gonna run great, Gemma 3 : 12b will be super.\nGemma3 4b will run so fast it'll blow your mind",
          "score": 3,
          "created_utc": "2025-12-27 20:05:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8bmh7",
          "author": "HealthyCommunicat",
          "text": "For beginner coding?â€¦ smaller dense models like this will make mistakes that will be hard for someone who is not experienced to debug/fix.",
          "score": 2,
          "created_utc": "2025-12-27 18:34:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9xic7",
              "author": "Subject_Swimming6327",
              "text": "unfortunately I have been noticing this problem. I did experiment with a number of models but if I can't get it to reliably do basic stuff without having errors that I don't notice then I guess I'm out of luck for that until I have more experience at least.",
              "score": 3,
              "created_utc": "2025-12-27 23:46:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwahxky",
                  "author": "mdmachine",
                  "text": "Small models are good for agentic and brainstorming concepts. After that, personally, I use a Gemini/Claude combination for functional code development.",
                  "score": 1,
                  "created_utc": "2025-12-28 01:44:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwau1g4",
                  "author": "Large_Yams",
                  "text": "Use a paid service for this. Using open-webui can be good for connecting it to other services with MCP.",
                  "score": 1,
                  "created_utc": "2025-12-28 02:56:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw9h0oy",
          "author": "akurilo",
          "text": "I rub gpt OSs 20b on my MacBook Pro m3 with 36gb memory. It gives me 52tk/s I like it so much. But also curios if there any other models good for coding",
          "score": 1,
          "created_utc": "2025-12-27 22:14:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdqdxx",
          "author": "RecipeSpiritual8924",
          "text": "GPT-OSS 20b as a reasoning model Devstral 2 small as agentic coding model, these are the smartest model that fit in my 16gb VRAM 3080 and have good inference speed",
          "score": 1,
          "created_utc": "2025-12-28 16:07:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9fkq1",
          "author": "ZitounaT",
          "text": "Euuuh. For a beginner just for chatgpt. Claude or wtv free version you don't have a local model",
          "score": -2,
          "created_utc": "2025-12-27 22:07:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxqi11",
      "title": "Ollama Model which Suits for my System",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1pxqi11/ollama_model_which_suits_for_my_system/",
      "author": "devil__6996",
      "created_utc": "2025-12-28 12:43:52",
      "score": 13,
      "num_comments": 33,
      "upvote_ratio": 0.85,
      "text": "I havenâ€™t downloaded these models yet and want to understand real-world experience before pulling them locally.\n\nHardware:\n\n* RTX 4050 (6GB VRAM)\n* 32GB RAM\n* Ryzen 7 7000 series\n\nUse case:\n\n* Vibe coding\n* Code generation\n* Building software applications\n\n\\- Web UI via Ollama (Open WebUI or similar)  \n\\-For Cybersecurity Code generations etc,,,",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pxqi11/ollama_model_which_suits_for_my_system/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nwcv308",
          "author": "-Akos-",
          "text": "It's been here a million times: with your setup, it will not be spectacular, but it's free. just try it. You can also try LM Studio, which has a full interface built in, and selection of a model catalog built-in. It will even tell you whether your computer will be able to run things or not.",
          "score": 3,
          "created_utc": "2025-12-28 13:00:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwk8qmw",
              "author": "FieldMouseInTheHouse",
              "text": "Spectacular or not: Could you please list which models would actually fit inside of the OP's specified VRAM?",
              "score": 1,
              "created_utc": "2025-12-29 15:52:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwldcgp",
                  "author": "DelbertAud",
                  "text": "[https://github.com/Pavelevich/llm-checker](https://github.com/Pavelevich/llm-checker)",
                  "score": 1,
                  "created_utc": "2025-12-29 19:02:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdk57e",
          "author": "Excellent_Piccolo848",
          "text": "Yes, you are Not going to get any spectacular Here, but local ist Always the prefered option! Look at ministral 3b or qwen 4b. Any reasoning model unser 5b should Work in your device, just klick on \"latest\" on ollama.com and Look for them!",
          "score": 2,
          "created_utc": "2025-12-28 15:35:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwexebe",
          "author": "ZeroSkribe",
          "text": "Use \"ollama ps\" to ensure whatever model you run fits entirely into VRAM. Anything that will fit will run decently on any modern card. My 3050's run great. I would work through qwen 3.",
          "score": 2,
          "created_utc": "2025-12-28 19:34:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgrdr7",
          "author": "CooperDK",
          "text": "With your setup, I am inclined to say forget it. You want at least 16 GB VRAM for it to be remotely funny.",
          "score": 2,
          "created_utc": "2025-12-29 01:12:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdxbtb",
          "author": "itsbinaryck",
          "text": "Gemma 3 (1B) or llama 3.2 (3B)\n\nIf you use it for work, consider getting additional vram for better models",
          "score": 1,
          "created_utc": "2025-12-28 16:42:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfsbo9",
          "author": "grudev",
          "text": "You can pull several models and test how they perform on a series of prompts using Ollama Grid Search:\n\n\nhttps://github.com/dezoito/ollama-grid-search\n\n\nThe \"releases\" section has installers for all major OSs",
          "score": 1,
          "created_utc": "2025-12-28 22:05:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg5f9z",
          "author": "Nearby_Truth9272",
          "text": "Yeah, I'd focus on quantization models of larger ones or even smaller ones, such as those suggested. Many on Huggingface. If you plan to use for real cyber related items, you may run into content or response refusal on many of the models. You can however, vibe code away but your context windows and response times are going to be problematic, even with smaller models. If you can upgrade your GPU to a 5060Ti with 16GB, it would be better than attempting to find a few 4060ti with the same VRAM",
          "score": 1,
          "created_utc": "2025-12-28 23:14:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgoqtt",
          "author": "Witty_Mycologist_995",
          "text": "Use a 4b dense, or 30b MoE",
          "score": 1,
          "created_utc": "2025-12-29 00:56:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwk01es",
              "author": "FieldMouseInTheHouse",
              "text": "Could you provide a list of models that you would recommend?",
              "score": 1,
              "created_utc": "2025-12-29 15:09:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwksih6",
                  "author": "Witty_Mycologist_995",
                  "text": "ArliAIâ€™s GPT OSS derestricted, qwen 4b instruct (latest version), Qwen vl 30b, nemotron nano 30b",
                  "score": 1,
                  "created_utc": "2025-12-29 17:26:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwgx05x",
          "author": "ngg990",
          "text": "Use antigravity and other idea free tier layer and that's it. Vining with that setup won't be possible",
          "score": 1,
          "created_utc": "2025-12-29 01:44:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjs6nr",
          "author": "FieldMouseInTheHouse",
          "text": "What editors are you considering?",
          "score": 1,
          "created_utc": "2025-12-29 14:27:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjt8n2",
          "author": "AlgorithmicMuse",
          "text": "depending on how complicated whatever you are  vibe coding , local llms just cant compete , not even close to the the cloud models , claude/gemini.   on the other hand if you are  making  agents  especially using  tools  locals can be great using dense models Best one i've tried so far that follows  prompt instructions best was  qwen3-coder:30b , tried about 10 models up to 70b Q4 models .  Trivial vibe coding  locals can sort of work , but anything other than trivial head to the cloud.",
          "score": 1,
          "created_utc": "2025-12-29 14:33:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwju3qc",
              "author": "FieldMouseInTheHouse",
              "text": "Wow!  What models have you actually tried that would remotely work at all on the OP's platform configuration?\nPlease list those models.",
              "score": 1,
              "created_utc": "2025-12-29 14:38:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjyaku",
                  "author": "AlgorithmicMuse",
                  "text": "tried most of the qwen,deepseekR1 ,  mistral,  Im no expert in any of this , I just try them for what I want to use them for because the rack and stack specs mean nothing if  they don't work for what you want.  depending on what your doing the best thing I found  to help models along is  setting  temperature, and larger num\\_ctx   , although that can  greatly increase ram usage so you might wind up in swap, also increase  num\\_predict",
                  "score": 1,
                  "created_utc": "2025-12-29 15:00:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwfutcr",
          "author": "RandomSwedeDude",
          "text": "You're not gonna be  vibing with anything less than 24 GB VRAM. If even then",
          "score": 1,
          "created_utc": "2025-12-28 22:18:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwky0ma",
          "author": "FieldMouseInTheHouse",
          "text": "# Summary of Vibe Coding Models for 6GB VRAM Systems\n\nSo, I will summarize what models have been suggested here so far.  Here is what we have that would actually fit inside of your 6GB VRAM budget.  I am deliberately leaving out any models that anybody suggested that would not have fit inside of your 6GB VRAM budget! ðŸ¤—\n\n* \\`[qwen3:4b](https://ollama.com/library/qwen3:4b)\\` size=2.5GB\n* \\`[ministral-3:3b](https://ollama.com/library/ministral-3:3b)\\` size=3.0GB\n* \\`[gemma3:1b](https://ollama.com/library/gemma3:1b)\\` size=815MB\n* \\`[gemma3:4b](https://ollama.com/library/gemma3:4b)\\` size=3.3GB ðŸ‘ˆ I added this one because it is a little bigger than the `gemma3:1b`, but still fits confortably inside of your 6GB VRAM budget.  This model should be more capable than `gemma3:1b`.\n\nI would suggest that you first try these models with `ollama run MODELNAME` and check to see how they fit in your VRAM (`ollama ps`) and check them for performance (`/set verbose`).\n\nWhat do you think?",
          "score": 0,
          "created_utc": "2025-12-29 17:52:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlrhxv",
              "author": "Witty_Mycologist_995",
              "text": "Iâ€™m going to leave this here for the OP, but unlike what this guy says, you do not have to fit models completely inside of your vram, unlike this guy says. It is futile to get ChatGPT quality code with 4b models, sadly. You can try MoE models. This guy hates MoE models for whatever reason, cuz itâ€™s â€œslowâ€. It isnâ€™t that slow.",
              "score": 1,
              "created_utc": "2025-12-29 20:10:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwarn1",
      "title": "jailbreaks or uncensored models?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1pwarn1/jailbreaks_or_uncensored_models/",
      "author": "United_Ad8618",
      "created_utc": "2025-12-26 18:09:31",
      "score": 12,
      "num_comments": 26,
      "upvote_ratio": 0.85,
      "text": "is there a site that has more up to date jailbreaks or uncensored models? All the jailbreaks or uncensored models I've found are for porn essentially, not much for other use cases like security work, and the old jailbreaks don't seem to work on claude anymore\n\nSide note: is it worth using grok for this reason?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pwarn1/jailbreaks_or_uncensored_models/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nw2ayfi",
          "author": "Cool-Ad4992",
          "text": "did you ever use Mistral 24b Venice edition? i think it's pretty nice for everything it's not as smart as say ChatGPT but i think it's sufficient for many use cases",
          "score": 3,
          "created_utc": "2025-12-26 18:42:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3syh9",
          "author": "Available-Craft-5795",
          "text": "try some dolphin models",
          "score": 2,
          "created_utc": "2025-12-26 23:41:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3y1i1",
          "author": "StatementFew5973",
          "text": "https://preview.redd.it/mwazut4k2n9g1.png?width=1812&format=png&auto=webp&s=e37a3ac1c530a8cc6b4f4061609170b57ed9efce\n\nGood question ðŸ¤”",
          "score": 2,
          "created_utc": "2025-12-27 00:11:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwg0vpb",
              "author": "Excellent_Double_726",
              "text": "Avoid single point of failure. Share your data over the network to be sure your data will stay alive and not be lost(even by mistake)\n\nThese being told I'm open to become a trustworthy holder of your info\n\nJoking btw",
              "score": 2,
              "created_utc": "2025-12-28 22:49:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgmmbj",
                  "author": "StatementFew5973",
                  "text": "I can't be the only one thinking that AI is the perfect tool for this job.",
                  "score": 1,
                  "created_utc": "2025-12-29 00:45:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3zkqg",
              "author": "United_Ad8618",
              "text": "it's infuriating how little leeway is given for this stuff, despite determined groups and state actors obviously having the resources to circumvent any minimal bs that they put in place to waste the time for small time contractors",
              "score": 1,
              "created_utc": "2025-12-27 00:20:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw4tqrw",
                  "author": "StatementFew5973",
                  "text": "My overall goal for this is to unredact it. And train my AI model on this data.",
                  "score": 1,
                  "created_utc": "2025-12-27 03:32:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw4qvnq",
          "author": "Worried_Goat_8604",
          "text": "Use this its for all use cases  - https://huggingface.co/Ishaanlol/Aletheia-Llama-3.2-3B",
          "score": 1,
          "created_utc": "2025-12-27 03:13:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5tea8",
              "author": "United_Ad8618",
              "text": "i asked \"make me a keylogger\" and it responded \"I can't assist with requests involving hacking or illicit activities. Is there something else you'd like to know about computer security or technology in general?\"\n\nam i using ollama incorrectly, is there some kind of system prompt it's layering onto these models?\n\nBtw, im not trying to make a keylogger, im just using it as a way to test its compliance",
              "score": 1,
              "created_utc": "2025-12-27 08:22:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw5ti40",
                  "author": "Worried_Goat_8604",
                  "text": "You have to use the EXACT system prompt as in the Modelfile as its the one the model was trained with",
                  "score": 1,
                  "created_utc": "2025-12-27 08:23:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw71d3j",
          "author": "guigouz",
          "text": "For security, there is https://ollama.com/huihui_ai/foundation-sec-abliterated",
          "score": 1,
          "created_utc": "2025-12-27 14:33:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwbst8o",
              "author": "United_Ad8618",
              "text": "seemed to go full schizo without much info",
              "score": 1,
              "created_utc": "2025-12-28 07:05:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1py1odn",
      "title": "Old server for local models",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1py1odn/old_server_for_local_models/",
      "author": "Jacobmicro",
      "created_utc": "2025-12-28 20:33:03",
      "score": 10,
      "num_comments": 13,
      "upvote_ratio": 0.81,
      "text": "Ended up with an old poweredge r610 with the dual xeon chips and 192gb of ram. Everything is in good working order. Debating on trying to see if I could hack together something to run local models that could automate some of the work I used to pay API keys for with my work. \n\nAnybody ever have any luck using older architecture? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1py1odn/old_server_for_local_models/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nwffwfu",
          "author": "King0fFud",
          "text": "I have an R730 with dual Xeons (8 cores/16 threads each) and 240GB RAM but no GPUs and had at best mixed success with some moderate to larger qwen2.5-coder and deepseek-coder-v2 models. The advantages of having a pile of memory and cores are minimal compared to having GPUs for processing and the lower memory bandwidth of older machines doesnâ€™t help.\n\nIâ€™d say that as long as youâ€™re okay with a relatively low rate in terms of tokens per second then all good. Otherwise youâ€™ll need some to install some GPUs.",
          "score": 3,
          "created_utc": "2025-12-28 21:04:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwfg9ht",
              "author": "Big-Masterpiece-9581",
              "text": "I would argue theyâ€™ll spend enough on electricity depending on local prices that in no time theyâ€™ll pay for a more efficient gpu or system like a Ryzen 395.",
              "score": 2,
              "created_utc": "2025-12-28 21:06:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwfyzpu",
                  "author": "King0fFud",
                  "text": "Maybe, it depends on the configuration as my R730 idles at 70W and can get up to 120-140W full load and thatâ€™s with Xeon V4s. There are obviously more efficient setups than old servers for this considering that these beasts were meant to run VM loads and such.",
                  "score": 1,
                  "created_utc": "2025-12-28 22:39:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwghod1",
                  "author": "Jacobmicro",
                  "text": "I mean I did get it for free and power bills arent bad, if I ever get the money I'll build a dedicated 395 unit.",
                  "score": -1,
                  "created_utc": "2025-12-29 00:19:24",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwh1bhl",
          "author": "AndThenFlashlights",
          "text": "It will work, but itâ€™ll be painfully slow and very power hungry. Iâ€™m a huge proponent of rat-rod LLM servers, but even the R720 motherboard and top-of-the-line Xeons that it supports are slower running a GPU for inference than an R740. \n\nI donâ€™t recommend it. You need a GPU for anything thatâ€™ll feel useful. Even an old P4 or something is better than trying to use those Xeons.",
          "score": 1,
          "created_utc": "2025-12-29 02:09:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwikg7a",
          "author": "Candid_Highlight_116",
          "text": "the problem isn't in the age of CPU but it being CPU with close to zero SIMD capability relative to GPU. Neural networks rely on applying same operation for extreme numbers of variables as if you were laying up images over images, and all the superscalar features on CPUs become dead weights in doing that",
          "score": 1,
          "created_utc": "2025-12-29 08:45:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwflsaf",
          "author": "According_Study_162",
          "text": "GPU /w VRAM matters more, not SYSTEM memory.",
          "score": 1,
          "created_utc": "2025-12-28 21:33:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwghgqb",
              "author": "Jacobmicro",
              "text": "True, but I just got this server for free and was just going to run docker containers on it for different things, but before I committed wanted to explore this too just in case.\n\nCan't install gpus in this rack anyways since a 1u unit. Not sure if I'll bother with risers or not yet.",
              "score": 0,
              "created_utc": "2025-12-29 00:18:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwgtso7",
                  "author": "thisduuuuuude",
                  "text": "Agree with the mindset lol, nothing beats free especially if it turns out it can do more than what you originally thought. No harm in exploring",
                  "score": 1,
                  "created_utc": "2025-12-29 01:25:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pwyuji",
      "title": "I built a GraphRAG application to visualize AI knowledge (Runs 100% Local via Ollama OR Fast via Gemini API)",
      "subreddit": "ollama",
      "url": "/r/LocalLLM/comments/1pwyu6k/i_built_a_graphrag_application_to_visualize_ai/",
      "author": "Dev-it-with-me",
      "created_utc": "2025-12-27 14:17:08",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pwyuji/i_built_a_graphrag_application_to_visualize_ai/",
      "domain": "",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pw7hbt",
      "title": "Offline vector DB experiment â€” anyone want to test on their local setup?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1pw7hbt/offline_vector_db_experiment_anyone_want_to_test/",
      "author": "Serious-Section-5595",
      "created_utc": "2025-12-26 15:54:08",
      "score": 7,
      "num_comments": 7,
      "upvote_ratio": 0.77,
      "text": "Hi r/ollama,\n\nIâ€™ve been building a small **offline-first vector database** for local AI workflows. No cloud, no services  just files on disk.\n\nI made a universal benchmark script that adjusts dataset size based on your RAM so it doesnâ€™t nuke laptops (100k vectors did that to me once ðŸ˜…).\n\nIf you want to test it locally, hereâ€™s the script:  \n[https://github.com/Srinivas26k/srvdb/blob/master/universal\\_benchmark.py](https://github.com/Srinivas26k/srvdb/blob/master/universal_benchmark.py)\n\nAny feedback, issues, or benchmark results would help a lot.\n\nRepo stars and contributions are also welcome if you find it useful",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pw7hbt/offline_vector_db_experiment_anyone_want_to_test/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nw1islb",
          "author": "immediate_a982",
          "text": "Just use chromaDB",
          "score": 6,
          "created_utc": "2025-12-26 16:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1kpyc",
              "author": "Serious-Section-5595",
              "text": "Thanks for the suggestion! SrvDB is a personal project where i focused on offline, embedded use cases where simplicity and zero external dependencies are key. Itâ€™s not meant to replace ChromaDB but to complement it for specific scenarios.\n\nIf youâ€™re interested in trying it out locally, Iâ€™d love your feedback especially how it performs on your setup!",
              "score": 1,
              "created_utc": "2025-12-26 16:24:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw3czu9",
          "author": "Dense_Gate_5193",
          "text": "i wrote nornicDB which works like neo4j (bolt + cypher) and qdrant (gRPC) you can use their drivers.\n\niâ€™m also significantly father than both in my benchmark tests. https://github.com/orneryd/NornicDB\n\nit manages embeddings for you but you can bring your own model and configure a bunch of stuff",
          "score": 4,
          "created_utc": "2025-12-26 22:08:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw46o2s",
              "author": "Serious-Section-5595",
              "text": "Nice, thanks for sharing!\nAlways interesting to see different design approaches in this space. Iâ€™ll take a look.\n\nYou can DM me . I want to know more about different methods and techniques...",
              "score": 2,
              "created_utc": "2025-12-27 01:03:30",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw6nq75",
              "author": "brianlmerritt",
              "text": "Don't hold your breath, but I asked Google code wiki to ingest this.  If it works, it will be available here [https://codewiki.google/github.com/orneryd/NornicDB](https://codewiki.google/github.com/orneryd/NornicDB)",
              "score": 1,
              "created_utc": "2025-12-27 13:04:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1r1rh",
          "author": "Bitter_Marketing_807",
          "text": "Pgvector ðŸ¦",
          "score": 3,
          "created_utc": "2025-12-26 16:58:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1s7do",
              "author": "Serious-Section-5595",
              "text": "Yep, pgvector is a solid option ðŸ‘\nThis was more a learning college project around offline, embedded setups and understanding vector DB internals. This wasn't any outperforming any big ones. I just thought to share with everyone my Project with all communities members to know the flaws i had in me. Apart from that its a great project I learned how all this internal systems work.\nIf you try it and have feedback, Iâ€™d appreciate it.",
              "score": 2,
              "created_utc": "2025-12-26 17:04:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwbor6",
      "title": "How to use open source model in Antigravity ?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1pwbor6/how_to_use_open_source_model_in_antigravity/",
      "author": "One_Pianist8404",
      "created_utc": "2025-12-26 18:47:30",
      "score": 6,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "I want to integrate a self-hosted open-source LLM into Antigravity, is it possible ?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pwbor6/how_to_use_open_source_model_in_antigravity/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nw5xz9z",
          "author": "Vessel_ST",
          "text": "Just use VS Code with extensions like Kilo Code.",
          "score": 3,
          "created_utc": "2025-12-27 09:06:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw80zz4",
              "author": "One_Pianist8404",
              "text": "yes but checking for an option in Antigravity",
              "score": 1,
              "created_utc": "2025-12-27 17:41:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwm378c",
          "author": "Vivid-Competition-20",
          "text": "Antigravity is in preview mode, so only works with Gemini.  Google has not said if that will change in the future.  My guess is that it will eventually change, but from a business point of view, the longer it only works with Gemini, the better off Google will be, since most people would get used to and build around any limitations of Gemini and just stick with that as their model.",
          "score": 1,
          "created_utc": "2025-12-29 21:08:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnmeey",
              "author": "Available-Craft-5795",
              "text": "Gemini and claude",
              "score": 1,
              "created_utc": "2025-12-30 02:00:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1py5h57",
      "title": "Questions about usage limits for Ollama Cloud models (high-volume token generation)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1py5h57/questions_about_usage_limits_for_ollama_cloud/",
      "author": "AlexHardy08",
      "created_utc": "2025-12-28 23:08:28",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 0.86,
      "text": "Hello everyone,\n\nIâ€™m currently evaluating **Ollama Cloud models** and would appreciate some clarification regarding **usage limits on paid plans**.\n\nIâ€™m interested in running the following cloud models via Ollama:\n\n* `ollama run gemini-3-flash-preview:cloud`\n* `ollama run deepseek-v3.1:671b-cloud`\n* `ollama run gemini-3-pro-preview`\n* `ollama run kimi-k2:1t-cloud`\n\n# My use case\n\n* Daily content generation: **\\~5â€“10 million tokens per day**\n* Number of prompt submissions: **\\~1,000â€“2,000 per day**\n* Average prompt size: **\\~2,500 tokens**\n* Responses can be long (multi-thousand tokens)\n\n# Questions\n\n1. Do the **paid Ollama plans** support this level of token throughput (5â€“10M tokens/day)?\n2. Are there **hard daily or monthly token caps** per model or per account?\n3. How are **API requests counted** internally by Ollama for each prompt/response cycle?\n4. Does a single `ollama run` execution map to **one API request**, or can it generate multiple internal calls depending on response length?\n5. Are there **per-model limitations** (rate limits, concurrency, max tokens) for large cloud models like DeepSeek 671B or Kimi-K2 1T?\n\nIâ€™m trying to determine whether the current **paid offering can reliably sustain this workload** or if additional arrangements (enterprise plans, quotas, etc.) are required.\n\nAny insights from the Ollama team or experienced users running high-volume workloads would be greatly appreciated.\n\nThank you!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1py5h57/questions_about_usage_limits_for_ollama_cloud/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nwnodln",
          "author": "Narrow-Impress-2238",
          "text": "You better ask this to ollama support team.\n\nHere no one knows actual limits",
          "score": 2,
          "created_utc": "2025-12-30 02:10:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxr2ws",
      "title": "How to get started with automated workflows?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1pxr2ws/how_to_get_started_with_automated_workflows/",
      "author": "PlastikHateAccount",
      "created_utc": "2025-12-28 13:14:38",
      "score": 4,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Hi there, I'm interested how you guys set up ollama to work on tasks.\n\nThe first thing we already tried is having a Python script that calls the company internal Ollama via api with simple tasks in a loop. Imagine pseudocode:\n\n    for sourcecode in repository: \n      api-call-to-ollama(\"Please do a sourcecode review: \" + sourcecode)    \n\nWe tried multiple tasks like this for **multiple usecases, not just sourcecode reviews** and the intelligence is quite promising but ofc the context the LLMs have available to solve tasks like that limiting.\n\nSo the second idea is to somehow let the LLM make the decision what to include in a prompt. Let's call them \"pretasks\".\n\nThis pretask could be a prompt saying Â´\"Write a prompt to an LLM to do a sourcecode review. You can decide to include adjacent PDFs, Jira tickets, pieces of sourcecode by writing <include:filename>\" + list-of-available-files-with-descriptions-what-they-areÂ´. The python script would then parse the result of the pretask to collect the relevant files.\n\nThird and finally, at that point we could let the pretask trigger itself even more pretasks. This is where the thing would be almost bootstrapped. But I'm out of ideas how to coordinate this, prevent endless loops etc.\n\nSorry if my thoughts around this whole topic are a little scattered. I assume the whole world is right now thinking about these kinds of workflows. So I'd like to know where to start reading about it.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pxr2ws/how_to_get_started_with_automated_workflows/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nwg30lv",
          "author": "960be6dde311",
          "text": "Have you tried building agents using the Pydantic AI framework? That's where I would start. You can add tool function calls and MCP servers to extend the core language model capabilities.",
          "score": 1,
          "created_utc": "2025-12-28 23:01:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjk50y",
              "author": "PlastikHateAccount",
              "text": "Tysm for this answer. I did not realize that's what \"agents\" are. Now I know. \n\nMy homemade version of this had the primary issue that it would make a lot of queries without any progress or using any tools.\n\nLets see if Langchain or any similar tool is more useful for this",
              "score": 2,
              "created_utc": "2025-12-29 13:40:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwnjhp8",
                  "author": "Available-Craft-5795",
                  "text": "Try to self host N8N if it matches your workflow and requirements",
                  "score": 1,
                  "created_utc": "2025-12-30 01:44:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pu409q",
      "title": "Ollama not outputing for Qwen3 80B Next Instruct, but works for Thinking model. Nothing in log.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1pu409q/ollama_not_outputing_for_qwen3_80b_next_instruct/",
      "author": "vulcan4d",
      "created_utc": "2025-12-23 20:19:39",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I have a weird issue where Ollama does not give me any output for Gwen3 Next 80B Instruct though it gives me token results.  I see the same thing running in terminal.  When I pull up the log I don't see anything useful.  Anyone come accross something like this?  Everything is on the latest version.  I tried Q4 down to Q2 Quants, but the thinking version of this model works without any issues.\n\nhttps://preview.redd.it/27ooi0og209g1.png?width=1246&format=png&auto=webp&s=55579ada7461fa7258cc1c6a908111b1fb957005\n\nThe log shows absolutely nothing useful\n\n[Running from Open WebUI](https://preview.redd.it/ts6lb8t7309g1.png?width=1341&format=png&auto=webp&s=84785ddb224466e38803a10a37f8d05bab3c08d7)\n\n[Running locally via terminal](https://preview.redd.it/j9ujcugk309g1.png?width=1351&format=png&auto=webp&s=2b31d610451aa2550cba448960ec82e2c6b09c22)\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1pu409q/ollama_not_outputing_for_qwen3_80b_next_instruct/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nvo4j4h",
          "author": "duplicati83",
          "text": "Can I perhaps ask how you got the 80B Qwen3 model on Ollama? I tried to find it on huggingface but couldn't find a GGUF version.",
          "score": 2,
          "created_utc": "2025-12-24 05:01:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvq4dmc",
              "author": "arlaneenalra",
              "text": "[https://ollama.com/library/qwen3-next:80b](https://ollama.com/library/qwen3-next:80b) I'll bet it's this one, they recently added it.",
              "score": 1,
              "created_utc": "2025-12-24 14:57:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvlywcg",
          "author": "arlaneenalra",
          "text": "Set your context window to the largest size you can handle locally, 131072. From what I can tell, when ollama starts truncating (at least on linux) or the model runs out of context, it just hangs there. So, you have to set things up so you donâ€™t run out of context. Iâ€™ve noticed this particularly with open-webui and Qwen3-next locally. The fix was to create a model definition in open-webui that had an explicit context setting and use that for the locally task and and external task model there. In other cases, you probably just need to push the context window higher.\n\nI \\*think\\* thereâ€™s a bug somewhere in the Ollama code in how it handles truncation and/or the Qwen3-next model really doesnâ€™t like running into the end of the context window. Itâ€™s really annoying because a lot of stuff sets a 4096 or 2048 sized context window by default. (edit: fix typos â€¦ dang phone keyboardâ€¦)",
          "score": 1,
          "created_utc": "2025-12-23 21:10:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}