{
  "metadata": {
    "last_updated": "2026-02-11 09:06:59",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 20,
    "total_comments": 90,
    "file_size_bytes": 117871
  },
  "items": [
    {
      "id": "1qyghp3",
      "title": "Ollie | A Friendly, Local-First AI Companion for Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/",
      "author": "MoonXPlayer",
      "created_utc": "2026-02-07 15:21:30",
      "score": 87,
      "num_comments": 35,
      "upvote_ratio": 0.95,
      "text": "Hi everyone,\n\nIâ€™m sharing **Ollie**, a Linux-native, local-first personal AI assistant built on top of **Ollama**.\n\nhttps://preview.redd.it/fh544zreb3ig1.png?width=1682&format=png&auto=webp&s=23c108dff77d288035dbc0d1dff64503bcd370dd\n\nOllie runs entirely on your machine â€” no cloud (I'm considering optional cloud APIs like Anthropic), no tracking, no CLI. It offers a polished desktop experience for chatting with local LLMs, managing models, analyzing files and images, and monitoring system usage in real time.\n\n**Highlights**\n\n* Clean chat UI with full Markdown, code, tables, and math\n* Built-in model management (pull / delete / switch)\n* Vision + PDF / text file analysis (drag & drop)\n* AppImage distribution (download & run)\n\nBuilt with **Tauri v2 (Rust) + React + TypeScript**.\n\nFeedback and technical criticism are very welcome.\n\nGitHub: [https://github.com/MedGm/Ollie](https://github.com/MedGm/Ollie)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o43q6zx",
          "author": "cuberhino",
          "text": "What are some use cases for this vs say just using lmstudio",
          "score": 6,
          "created_utc": "2026-02-07 16:33:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43uqz5",
              "author": "MoonXPlayer",
              "text": "In the base, they can look the same, even if itâ€™s already clear that LM Studio is better because:  \n\\- LM Studio is a full LLM platform with SDKs, CLI, engines, and dev tools.  \n\\- Ollie isâ€¦ a different, Linux-native, local-first personal AI companion.\n\nI cannot say there are things where Iâ€™m better yet, but Iâ€™m still trying to make things better, especially in terms of performance *â€” thatâ€™s why I chose Rust â€”* and Iâ€™m still brainstorming some unique features to integrate.",
              "score": 2,
              "created_utc": "2026-02-07 16:56:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o45ivgc",
                  "author": "cuberhino",
                  "text": "Yeah Iâ€™m really just trying to identify some things I would do with it before diving in with the play time on it",
                  "score": 1,
                  "created_utc": "2026-02-07 22:05:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o45lcji",
                  "author": "superdariom",
                  "text": "I don't really think rust is going to make things much faster as your bottleneck will be the inference?",
                  "score": 1,
                  "created_utc": "2026-02-07 22:19:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o44zdjz",
              "author": "germanpickles",
              "text": "The LM Studio desktop app is not open source",
              "score": 1,
              "created_utc": "2026-02-07 20:20:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o45is4v",
                  "author": "cuberhino",
                  "text": "Yeah but you can use open source models with it. Iâ€™m asking for specific use cases for this project",
                  "score": 2,
                  "created_utc": "2026-02-07 22:05:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o43c9md",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 3,
          "created_utc": "2026-02-07 15:25:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43cfou",
              "author": "MoonXPlayer",
              "text": "Thanks a lot, I really appreciate it!",
              "score": 1,
              "created_utc": "2026-02-07 15:26:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43jki2",
          "author": "valosius",
          "text": "Hi, sinple not found on github:  \n\nCan you send the exect link to the AppImage ?\n\nOlav\n\n\\#### snip ##\n\nwget -O Ollie.AppImage [https://github.com/MedGm/OllamaGUI/releases/latest/download/Ollie\\_\\*\\_amd64.AppImage](https://github.com/MedGm/OllamaGUI/releases/latest/download/Ollie_*_amd64.AppImage)\n\n...\n\nPlatz: [https://github.com/MedGm/Ollie/releases/download/v0.2.1/Ollie\\_\\*\\_amd64.AppImage](https://github.com/MedGm/Ollie/releases/download/v0.2.1/Ollie_*_amd64.AppImage) \\[folgend\n\nWiederverwendung der bestehenden Verbindung zu github.com:443.\n\nHTTP-Anforderung gesendet, auf Antwort wird gewartet â€¦ 404 Not Found\n\n2026-02-07 16:54:43 FEHLER 404: Not Found.\n\n\\## snip ##",
          "score": 3,
          "created_utc": "2026-02-07 16:01:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43kyk1",
              "author": "MoonXPlayer",
              "text": "Thank you for pointing that out\n\nthe wildcard `*` doesnâ€™t work in direct GitHub URLs. The exact AppImage link for the latest release is:  \n[https://github.com/MedGm/Ollie/releases/download/v0.2.1/Ollie\\_0.2.1\\_amd64.AppImage]()\n\nThen run:\n\n     chmod +x Ollie_0.2.1_amd64.AppImage\n    ./Ollie_0.2.1_amd64.AppImage\n\nIâ€™ll update the README to avoid the confusion. Thanks!!",
              "score": 1,
              "created_utc": "2026-02-07 16:07:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o49xhpu",
                  "author": "valosius",
                  "text": "Unfortunately, there are too many dependencies on the new glibc library, but I'm not going to update my Ubuntu system just for a single client. I can simply test it in a virtual machine.",
                  "score": 1,
                  "created_utc": "2026-02-08 16:36:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4590x7",
          "author": "Noiselexer",
          "text": "Tauri but no windows build?",
          "score": 3,
          "created_utc": "2026-02-07 21:13:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49yryd",
              "author": "MoonXPlayer",
              "text": "I will surely work on a Windows build version near in the future. I just wanted to start by focusing on a Linux native build first, since Linux is my main machine",
              "score": 2,
              "created_utc": "2026-02-08 16:42:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43jb6h",
          "author": "Prestigious_Ebb_5131",
          "text": "Looks perfekt! Does MCP integration support HTTP streaming and Bearer token authorization?",
          "score": 2,
          "created_utc": "2026-02-07 15:59:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43jyad",
              "author": "MoonXPlayer",
              "text": "Thanks!!  \nMCP support is planned but not there yet. my idea is to support HTTP-based MCP servers with streaming, and yes, Bearer token auth is part of that plan.",
              "score": 2,
              "created_utc": "2026-02-07 16:02:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o43l6yb",
                  "author": "Prestigious_Ebb_5131",
                  "text": "Thats great! MCP and building normal agentic loops (state Machines) -and it will be a unique and one-of-a-kind product. I'll do my best to help via pull requests, if you don't mind. \nGreat Job! ðŸ‘",
                  "score": 3,
                  "created_utc": "2026-02-07 16:09:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o44x7bt",
          "author": "PhilCoyo",
          "text": "I would immediately work with this if it had MCP Support. I have so many projects and admin work Where im using Mcps and I feel terrible always giving Claude Access instead of having a local Solution",
          "score": 2,
          "created_utc": "2026-02-07 20:09:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47i0n3",
          "author": "rb1811",
          "text": "Is there a way to integrate s3 client? Say Minio? For auto clean up of blobs uploaded ? \n\nAny benefits over OpenWebUI?",
          "score": 2,
          "created_utc": "2026-02-08 05:42:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49zfyd",
              "author": "MoonXPlayer",
              "text": "S3 / MinIO integration is not implemented yet, but itâ€™s a good idea. still thinking through the right abstraction to avoid adding unnecessary complexity.\n\nalso compared to OpenWebUI, Ollie main focus is local performance and OS-level integration.",
              "score": 1,
              "created_utc": "2026-02-08 16:45:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ee94u",
                  "author": "rb1811",
                  "text": "Ok looking forward for S3/ Minio integration. Thanks for the reply",
                  "score": 2,
                  "created_utc": "2026-02-09 07:54:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4a5tls",
          "author": "Professional_Ad5011",
          "text": "Fantastic. It's pretty much what I was looking for and now it's available to use.",
          "score": 2,
          "created_utc": "2026-02-08 17:16:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4eo9hx",
          "author": "Professional_Ad5011",
          "text": "I have an issue where the answer is above and I cannot see it as Message section has taken up the whole window! anyone else face similar issue?\n\nhttps://preview.redd.it/tn2yq72vufig1.jpeg?width=1024&format=pjpg&auto=webp&s=aed58c0d7a265fbaf1f3668aa433096cb82fe0dc",
          "score": 2,
          "created_utc": "2026-02-09 09:33:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4frqej",
              "author": "MoonXPlayer",
              "text": "Thank you for pointing this out. Iâ€™ve fixed several UI/UX issues in the latest version. Feel free to try it out and let me know if the problem still persists!!",
              "score": 1,
              "created_utc": "2026-02-09 14:30:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4c0ang",
          "author": "BidWestern1056",
          "text": "use npcpy to handle multi providers and a variety of other LLM capabilitiesÂ \nhttps://github.com/npc-worldwide/npcpy",
          "score": 1,
          "created_utc": "2026-02-08 22:42:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvycjs",
      "title": "Just started using local LLMs, is this level of being wrong normal?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qvycjs/just_started_using_local_llms_is_this_level_of/",
      "author": "Cempres",
      "created_utc": "2026-02-04 19:31:52",
      "score": 48,
      "num_comments": 81,
      "upvote_ratio": 0.82,
      "text": "https://preview.redd.it/4swbpymp4jhg1.png?width=746&format=png&auto=webp&s=16d362a6bd68f24a9b349d8f0f21aec95376aa11\n\nTried using the glm4.7 flash version, and just randomly asking it stuff, and i got his with this trip, is that normal to expect and can someone explain why this even happens? lack of internet connection? Should I expect this from other models as well? \n\nMy regular usage wouldn't be this, but a tool to help me create D&D stuff, such as character ideas, helping me when i get stuck ecetera, any model to recommend for such? \n\nSpecs:  \nwin 11  \n32gb RAM  \nXTX 24gb VRAM",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qvycjs/just_started_using_local_llms_is_this_level_of/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3l3wsb",
          "author": "InfraScaler",
          "text": "Yup. Now online models have tools to fetch data from the web, but yeah hallucination is relatively common. The LLM tries to fill knowledge gaps and it just makes up stuff to accomplish that.",
          "score": 52,
          "created_utc": "2026-02-04 19:35:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l86nc",
              "author": "Cempres",
              "text": "Then i'm missing the point of local LLMs, kinda..",
              "score": 4,
              "created_utc": "2026-02-04 19:56:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3la92q",
                  "author": "Dubious-Decisions",
                  "text": "I think you are missing the point of LLMs in general. They are not definitive sources of fact. They are chat simulators. They are designed to predict the most reasonable, statistically likely series of tokens that should be emitted in response to all of their inputs. They have no ability to self-assess or fact check or even understand the semantics of what you are giving them as input.\n\nSo no, you shouldn't expect them to be accurate. The larger the model, the larger the training set and the more likely it is to match some piece of human-created information it was trained on. But as models shift to training with synthetic data and output from other models, you can imagine what happens to accuracy.\n\nWhat they ARE is reasonably inexpensive, simple to interact with natural language processors (NLPs) that can act as some reasonably reliable glue between users and tools. It's a gross oversimplification to say they can't produce useful results, but it's also incorrect to assume that they will ever be foolproof. There are certain types of interactions that they simply will never be able to perform.\n\nThere are many other types of AI and machine learning (ML) platforms besides LLMs, and the tragedy lately is that no one seems to remember that and instead are trying to build the entire AI house with nothing but a LLM hammer.\n\nOnce you realize that these are just tools for parsing human text inputs and producing some compelling, simulated text outputs, you will be better suited to figure out what they are and are not good for.",
                  "score": 92,
                  "created_utc": "2026-02-04 20:06:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3l8stl",
                  "author": "LePfeiff",
                  "text": "An LLM doesnt need to know alot of facts to be a DM assistant in DnD, but its also just a consequence of having to run smaller quantized models that fit on consumer hardware",
                  "score": 7,
                  "created_utc": "2026-02-04 19:58:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lbupi",
                  "author": "sinan_online",
                  "text": "I think they work best for fairly technical hobbyists. For instance, I am combining repeated LLM calls with some Python code to find a good way to get structured markdown out of PDF books. In other words, it's a specific use case of improving OCR quality.\n\nYou can also use them for classification tasks, etc... I personally do not have any cases that do not involve some programming or technical knowledge.\n\n",
                  "score": 4,
                  "created_utc": "2026-02-04 20:13:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lmray",
                  "author": "zenmatrix83",
                  "text": "if you want proper research you can likely try something like this [https://github.com/langchain-ai/local-deep-researcher](https://github.com/langchain-ai/local-deep-researcher) . a deep research look is doing a websearch, the ai gets a finding from the research report,  and then does a loop looking up answers trying to create a report.",
                  "score": 2,
                  "created_utc": "2026-02-04 21:05:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m8301",
                  "author": "Medium_Ordinary_2727",
                  "text": "To be clear all LLMs will do this to a certain extent. Some are better than others bit they can still hallucinate.\n\nFrame your prompt not as a question that you are asking the LLM, but as a research request, and give it the tools to do that research (such as web access).",
                  "score": 2,
                  "created_utc": "2026-02-04 22:50:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3mk2e4",
                  "author": "Western_Courage_6563",
                  "text": "Think about them as an engine, they need rest of the stuff to work (memory, Internet aces, local database, etc).",
                  "score": 2,
                  "created_utc": "2026-02-04 23:55:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3llgoe",
                  "author": "zenmatrix83",
                  "text": "to keep it simple llm means large language model or its a model of how a language is.  This is a decent video on how it works [https://www.youtube.com/watch?v=wjZofJX0v4M](https://www.youtube.com/watch?v=wjZofJX0v4M) .  Its doesn't know whats its saying, its a statistically correct response, which alot of time is correct, but not always. The lower power the model the more it will be statistically wrong.",
                  "score": 2,
                  "created_utc": "2026-02-04 20:59:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m16qs",
                  "author": "florinandrei",
                  "text": "More complex tasks - use bigger models.\n\nSimpler tasks - smaller models are okay.\n\nUnless you have your own dedicated inference hardware, you're not going to run big models at home.\n\nThe definitions of \"simpler\" and \"more complex\" will continue to evolve over time.",
                  "score": 1,
                  "created_utc": "2026-02-04 22:15:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3mv6sa",
                  "author": "oVerde",
                  "text": "If you use it in an agentic environment, like OpenCode, you then can provide tools to fetch from the internet",
                  "score": 1,
                  "created_utc": "2026-02-05 00:56:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3nzbs1",
                  "author": "fasti-au",
                  "text": "Cintext is a bucket of things it can juggle to find the 8/16/32k it can actually relate to your question thatâ€™s what is juggled by 1 shit then think deals with one shot output to decide if it was good. \n\nSo how you fill that first one shot is the key to getting this broken binary system to work.  Ternerary fixes it but itâ€™s not really hardware friendly yet.  Next round of llm with bee new chips if we are transforming but big fusion seems more in line with the in between steps",
                  "score": 1,
                  "created_utc": "2026-02-05 04:59:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3v5w92",
                  "author": "andrew_barratt",
                  "text": "Think of it less like a huge knowledgeable system, more like a software component that can read",
                  "score": 1,
                  "created_utc": "2026-02-06 07:39:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3m49wm",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-02-04 22:30:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3m91wk",
                  "author": "InfraScaler",
                  "text": "If you're offline you can't fetch data. Local does not mean offline. Thing is, if you want it to be online, you need to give it the tools to fetch data.",
                  "score": 2,
                  "created_utc": "2026-02-04 22:55:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3m36jm",
          "author": "themaskbehindtheman",
          "text": "That's why web search and tools are a thing, the tools are a big reason these companies still hire software engineers. \n\nMonitor chats > find commonly asked questions where an incorrect answer is given > write tool > claim sentience /s\n\nIf you whack openweb ui over the top of it, enable web search and ask about the weather it'll get it wrong, you have to write a tool to call a weather API, it can figure out it needs to use the tool and then to parse the JSON response.",
          "score": 9,
          "created_utc": "2026-02-04 22:25:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q37r3",
          "author": "Past-Grapefruit488",
          "text": "You need to enable Web Search so that App can use it to look up information. LLMs (Local or otherwise) are not encyclopedia. ChatGPT or Gemini will run a web search to answer such questions . Example : \n\nhttps://preview.redd.it/wvwdp3xdvohg1.png?width=1091&format=png&auto=webp&s=bb9ad3dbe1437c5f24e3a3e3f37e1bbf985dcf6f\n\n",
          "score": 4,
          "created_utc": "2026-02-05 14:48:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4cfhj1",
              "author": "FaithlessnessLife876",
              "text": "Yeah I think people don't realise how much [chatgpt.com](http://chatgpt.com) is actually a wrapper and the LLMs themselves how much pre-promting finetuning they have with the goal of mimic human behaviour",
              "score": 1,
              "created_utc": "2026-02-09 00:11:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ladtw",
          "author": "HonestoJago",
          "text": "GLM benefits from a low temp in situations like this, at least in my experience. But yeah, if youâ€™re expecting Claudeâ€¦..donâ€™t.",
          "score": 3,
          "created_utc": "2026-02-04 20:06:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lw42r",
              "author": "Cempres",
              "text": "was not expecting anything in particular, was just wondering. I'll test it with ideas for D&D, advices on how to connect different story arcs and such and i'll write a comment when i have the chance to test it for such use",
              "score": 1,
              "created_utc": "2026-02-04 21:50:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3qw8wy",
                  "author": "Dubious-Decisions",
                  "text": "Load a bunch of slightly structured (e.g. YAML) files into an IDE like VS Code and hook it up to your local LLM. It can crawl all over that document tree and make notes to itself. When you tell it you are in \"room x\", it can go read through the YAML file for room x at the direction of the IDE and switch its context to use the facts and descriptions you put in the room x file. llama3.2 is actually a competent little LLM for doing this locally. ",
                  "score": 1,
                  "created_utc": "2026-02-05 17:06:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3orp7j",
          "author": "cibernox",
          "text": "Yes. Small locals models are reasonably good at being smart, not at being knowledgeable about every niche. Whey will get most thing that widely known well (year of a mayor war, what is the biggest mammal, and wether it rains more in Seattle or in Los Angeles, but not how many mg of caffeine a drink has).",
          "score": 3,
          "created_utc": "2026-02-05 09:04:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l9bgg",
          "author": "Condomphobic",
          "text": "They generally suck unless they're the full-sized model, which normal consumers cannot afford",
          "score": 5,
          "created_utc": "2026-02-04 20:01:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l99z6",
          "author": "Witty_Mycologist_995",
          "text": "Thatâ€™s kind of strange. I use glm flash. What quantization is that?\nAsk it the capital of france",
          "score": 1,
          "created_utc": "2026-02-04 20:01:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lkd9i",
              "author": "gregusmeus",
              "text": "Berlin, lol. Sorry.",
              "score": 2,
              "created_utc": "2026-02-04 20:54:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ln11g",
                  "author": "mattv8",
                  "text": "ðŸª¦",
                  "score": 1,
                  "created_utc": "2026-02-04 21:07:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3pk783",
              "author": "Dzhmelyk135",
              "text": "Not even my Q3 version is that dumb",
              "score": 2,
              "created_utc": "2026-02-05 13:02:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3lvq1m",
              "author": "Cempres",
              "text": "https://preview.redd.it/bz8ghqbltjhg1.png?width=744&format=png&auto=webp&s=6e3477d638ab234ed58ad52dd52d3d6ab0b6c135\n\nI was trying out the glm-4.7-flash:q4\\_K\\_M",
              "score": 1,
              "created_utc": "2026-02-04 21:48:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3lzqy9",
                  "author": "Witty_Mycologist_995",
                  "text": "Send reasoning traces",
                  "score": 2,
                  "created_utc": "2026-02-04 22:07:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3lzww6",
          "author": "Ryanmonroe82",
          "text": "You need to use a smaller model with bf/fp16.  Using larger models that are quantized will always be less accurate especially a thinking/reasoning model thatâ€™s a MoE.  With 24gb VRAM look at dense 8b models that you can run in FP16/B16 with headroom for a decent context window.  RNJ-1-8b is excellent so is Gemma 2-7b.  They might not be as large as you want but the precision and accuracy is noticeably better especially on lower temps.",
          "score": 1,
          "created_utc": "2026-02-04 22:08:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mrp7q",
          "author": "superdariom",
          "text": "Isn't glm 4.7  a specialized coding model? Why not try deepseek instead?",
          "score": 1,
          "created_utc": "2026-02-05 00:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n4wj4",
          "author": "mpw-linux",
          "text": "Look at Liquid AI models. I use this one my Mac M1:\n\nmodel, tokenizer = load(\"mlx-community/LFM2.5-1.2B-Thinking-8bit\")\n\n It is quite good. I asked it create a 3 chord modern country song which i did with lyrics and chords. ",
          "score": 1,
          "created_utc": "2026-02-05 01:52:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ocqvk",
          "author": "tom-mart",
          "text": "There is nothing wrong about the example you showed. What is the problem?",
          "score": 1,
          "created_utc": "2026-02-05 06:45:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pc8xi",
              "author": "damhack",
              "text": "Itâ€™s low calorie, zero sugar but has caffeine in it.",
              "score": 1,
              "created_utc": "2026-02-05 12:08:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3pd024",
                  "author": "tom-mart",
                  "text": "I asked what's the problem in LLM response? It looks correct.",
                  "score": 1,
                  "created_utc": "2026-02-05 12:13:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pouht",
          "author": "bitmux",
          "text": "I find that most of the single-gpu local LLM's I've played with generate about 60% bs mostly regardless of settings and even including the websearch tool UNLESS they're hyper specialized in something and you use them right down the center of their specialization.  At that point its 40% bs :-P.",
          "score": 1,
          "created_utc": "2026-02-05 13:29:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rfwmp",
          "author": "generousone",
          "text": "With 24gb vram, go down to a smaller model like gpt-oss:20b and give it full 128k context. You'll see a vast improvement on capability and usefulness. GLM flash is 19gb. That's not a lot of room for context with 24gb vram.Â \n\n\n\nGenerally, it's also good to enable things like web search when asking fact specific questions.Â Â \n\n\nEdit: lol, tried the exact prompt in my gpt-oss:20b and it wouldn't complete the request because it was censored to providing proprietary information. So I omitted \"secret\" and just asked what the ingredient were and it worked.Â ",
          "score": 1,
          "created_utc": "2026-02-05 18:36:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vamc5",
          "author": "joshiegy",
          "text": "One of the best ways to use a local llm is to combine with an MCP that gives some sort of webacrape ability.\nOr to have one specific to a task, like storytelling or code review in a specific language.\n\nBut it's not a GPT, those are huuuuuge. Chatgpt, a while back at least, requires something like 256gb of vram. Maybe it's even more now.",
          "score": 1,
          "created_utc": "2026-02-06 08:23:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vislx",
          "author": "maxbiz",
          "text": "The problem you have run into is that LLMs are poor at saying, 'I don't know.'  Where the LLM does not know, it treats it as a multiple-choice question; a random pick of 4 possible answers is a 25% chance of being right rather than 0% for 'I do not know'.",
          "score": 1,
          "created_utc": "2026-02-06 09:42:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xrgvy",
          "author": "fasti-au",
          "text": "Local is smart but not wise. Was one comes with cintext filling with info not nearest guesses.   They can do anything but anything is anything you specifically are giving it information to work with not anything you need a nuclear plant Greenland all the metals from Canada and ukrain as well as cooling and also did we menthey they cheat lies and hide behind the government not fight court battles unlike anthropic whi get to turn around and destroy copyright completely and have to then compete as an underdog but actually are trying for good things in more ways\n\nOpenAI motto is all your shit belongs to us pay us for it back and do it while we destroy your jobs and say they are not money gobbling cunts with smart people staying alive and staying because they know not money or in club is bad",
          "score": 1,
          "created_utc": "2026-02-06 17:41:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42i8ux",
          "author": "dreamer2020-",
          "text": "Dont know what version of glm4.7 flash you are using. But the full variant is way above 250gb ram. Try to use gpt oss 120b if possible, it is really â€œwiseâ€",
          "score": 1,
          "created_utc": "2026-02-07 12:23:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n17yi",
          "author": "PrepperDisk",
          "text": "Yep! Â We had to give up on bringing one to Prepper Disk even with RAG. Â They are proofs of concept at best.",
          "score": 1,
          "created_utc": "2026-02-05 01:31:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nyx8l",
          "author": "fasti-au",
          "text": "Models are how they thing not what they think.   What they thing is parameters so the memory base inside a 1 trillion midel is about not looking.  Local models are all about the perfect prompt to avoid think having a different plan to you",
          "score": 0,
          "created_utc": "2026-02-05 04:56:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzh0op",
      "title": "I created a small AI Agent",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/",
      "author": "Rough_Philosopher877",
      "created_utc": "2026-02-08 18:43:20",
      "score": 48,
      "num_comments": 13,
      "upvote_ratio": 0.94,
      "text": "Hi guys.. I know it's not so big thing.. just for fun I created a Small AI Agent:\n\n[https://github.com/tysonchamp/Small-AI-Agent](https://github.com/tysonchamp/Small-AI-Agent)\n\nWould love the feedback of the community.. and any suggestions of new ideas.\n\nI created this for my day to day activities.. such as setup reminders, take notes, monitor all my client's website (if they are all ok or not).. monitor all my servers, connecting it to my custom erp for due invoice fetching, project management etc ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4ap7g5",
          "author": "RA2B_DIN",
          "text": "Sounds really nice, Iâ€™ll try it",
          "score": 1,
          "created_utc": "2026-02-08 18:47:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4arhru",
              "author": "Rough_Philosopher877",
              "text": "thanks mate.. let me know your feedback please..",
              "score": 1,
              "created_utc": "2026-02-08 18:58:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4cdwvf",
          "author": "Electronic_Fox594",
          "text": "I made one too but Iâ€™m not a real programmer so I wonâ€™t share it but very similar.",
          "score": 1,
          "created_utc": "2026-02-09 00:02:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4cpn6h",
          "author": "Civil_Tea_3250",
          "text": "Love it! I created something like this myself using Ollama, python scripts, n8n, .md files and all that. I've been adding to it when I get time and learn something new. I'll check it out when I have time. Would love to see what you do similar/different.\n\nI had the same idea. I hate all the AI down our throats and find most of it frustrating. Having something that does my regular tasks and is only focused on my home and server makes it much more trustworthy and useful.",
          "score": 1,
          "created_utc": "2026-02-09 01:07:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4dakcc",
              "author": "Rough_Philosopher877",
              "text": "I started with only website monitoring.. and now Iâ€™ve added four five skills.. thinking to add few more like notification sending to me and my team members based on pending task as reminder.. automated chat replies.. sent emails to my clients etc..",
              "score": 1,
              "created_utc": "2026-02-09 03:00:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4e75l0",
          "author": "Zyj",
          "text": "Telegram is not end-to-end encrypted most of the time",
          "score": 1,
          "created_utc": "2026-02-09 06:48:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hvdtk",
          "author": "Consistent-Signal373",
          "text": "I started with a fairly simple telegram also.\n\nI will suggest considering using a Matrix server, if you value fully local and private data.\n\nAlso Matrix can be end-to-end encrypted, and you have full control of everything.\n\nI also dropped using Ollama and switched over to LM Studio, as the speeds are much better, at least with Nvidia GPU's.\n\nAt some points my my goal became to create a complete AI operation system.\n\nSince then I added 15 dashboards, different modes e.g. work, chat, code and swarm with 14 custom agents, a personal code assistant and tons more. Last check it was 120k+ lines of code.\n\nSo yeah just keep going",
          "score": 1,
          "created_utc": "2026-02-09 20:38:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4atim6",
          "author": "Acrobatic_Task_6573",
          "text": "This is cool. The fact that you built it around your actual daily workflow instead of making it generic is the right approach. Most AI agent projects try to do everything and end up doing nothing well.\n\nThe website monitoring and ERP integration pieces are especially interesting. Those are real problems that most people solve with 3 or 4 different SaaS tools. Having one agent that handles all of that is clean.\n\nA few questions/suggestions if you keep building on it:\n\n- How does it handle failures? Like if a website check times out, does it retry or just flag it?\n- For the reminder system, does it persist across restarts? That was one of the first things I had to solve with my own setup.\n- Have you thought about adding a simple web dashboard to see all your monitors at a glance?\n\nNice work for a personal project. The best tools are the ones built to scratch your own itch.",
          "score": 1,
          "created_utc": "2026-02-08 19:07:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d96mj",
              "author": "Rough_Philosopher877",
              "text": "Thanks for coming.. Right now it just send a notification to my telegram bot.. and i operate it via telegram bot only..\nIâ€™m using sqlite db to store everything..\nAbout the web didnâ€™t thought about it.. but I needed a way to see the db easy way..",
              "score": 1,
              "created_utc": "2026-02-09 02:53:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ayt2a",
          "author": "mosaad_gaber",
          "text": "I tried to clone and install and face this \ngit clone https://github.com/yourusername/ai-assistant-bot.git\ncd ai-assistant-bot\n\n# Create Virtual Environment\npython3 -m venv venv\nsource venv/bin/activate\n\n# Install Dependencies\npip install -r requirements.txt\nThe program git is not installed. Install it by executing:\n pkg install git\nbash: cd: ai-assistant-bot: No such file or directory\n\n[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n[notice] To update, run: pip install --upgrade pip\nERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'",
          "score": -3,
          "created_utc": "2026-02-08 19:33:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4czk7c",
              "author": "inteblio",
              "text": "Chat gpt",
              "score": 1,
              "created_utc": "2026-02-09 02:03:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4eskly",
                  "author": "mosaad_gaber",
                  "text": "I relace ollama with Gemini and works like acharm because gemma3 it's eat ram and make my device lag",
                  "score": 0,
                  "created_utc": "2026-02-09 10:15:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4gs2zc",
              "author": "Rough_Philosopher877",
              "text": "Updated the readme.. btw you need git installed first or download the zip from github",
              "score": 1,
              "created_utc": "2026-02-09 17:29:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxmmio",
      "title": "Best models on your experience with 16gb VRAM? (7800xt)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qxmmio/best_models_on_your_experience_with_16gb_vram/",
      "author": "roshan231",
      "created_utc": "2026-02-06 16:40:26",
      "score": 42,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "Iâ€™m running a 7800 XT (16 GB VRAM) and looking to get the best balance of quality vs performance with Ollama.\n\nWhat models have you personally had good results with on 16 GB VRAM?\n\nReally I'm just curious about your use cases as well. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qxmmio/best_models_on_your_experience_with_16gb_vram/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3xh50w",
          "author": "tcarambat",
          "text": "For me, Qwen3 4B (Q4) @ 128K gives me excellent responses for speed quality without overmaxxing the card. If i need something multi-model (usually do) I just swap out for Qwen3-VL 4B Q4 @ 128k|256K.\n\nWorks excellent for me, no bs and if the thinking is bothering me I just turn it off. One cavet i have found is the qwen3 models you almost never want to send a simple unbounded prompt like \"Hello\" - it will easily think for 1k+ tokens just to say hello.\n\nIf you ask an actual prompt though or attach an image as context it thinks briefly and gives a great response.",
          "score": 11,
          "created_utc": "2026-02-06 16:51:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45aqod",
              "author": "No-Consequence-1779",
              "text": "4b is pretty good. I have 2 32gb gpus. Â I have been using smaller models more and more. Very little difference for simple coding tasks from a 30b to a 4b. Â Though for models coding is the easiest thing.Â ",
              "score": 2,
              "created_utc": "2026-02-07 21:22:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3yavqa",
          "author": "BGPchick",
          "text": "What are your goals? For coding, I really like gpt-oss:20b or qwen3-coder:30b. For general writing, I find llama3.2 or gemma3 a little better and faster.",
          "score": 6,
          "created_utc": "2026-02-06 19:13:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xm6r2",
          "author": "fasti-au",
          "text": "16 gb vram is qwen 14b territory Phi4 mini.   You might fit a devstral2small which is smaller that qwen coder 30b and codes",
          "score": 6,
          "created_utc": "2026-02-06 17:15:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41w28t",
          "author": "nycigo",
          "text": "Devstral 2 Small is unbeatable if you want to program.",
          "score": 3,
          "created_utc": "2026-02-07 08:54:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41ga9r",
          "author": "Remarkable_Stay_592",
          "text": "GPT-OSS 20B, QWEN 3 14B -> coding\nGemma 3 12B -> general chats",
          "score": 2,
          "created_utc": "2026-02-07 06:26:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43ogkv",
          "author": "_w0rm",
          "text": "Interesting. I have been just testing several models to be used with log analysis. So far I have not found reliable model for the task. Many models are successful in find the needle in haystack kind of tasks but fail when asked to retrieve multiple entries from context (kind of reasoning over haystack). In this kind scenarios models often just miss data or start to hallucinate especially in the middle of the context. Best results with reasonable speed I have gained with Ministral-3-14B-Reasoning with Q6_K quant (8B with Q8_0 provide almost equal results), Phi-4-Reasoning-Plus Q4_K_M and gpt-oss-20B Q4_K_M. Qwen-3-30B does good job but is really slow as it needs offloading and still with limited context.\n\nFor testing I wrote a small Python program which generates log file of specified size with known information spread in the logs. \n\nIf anyone have good suggestions, please let me know.",
          "score": 1,
          "created_utc": "2026-02-07 16:25:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y1tem",
          "author": "stonecannon",
          "text": "I like Gemma 3 on my 16gb laptop.",
          "score": 1,
          "created_utc": "2026-02-06 18:30:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40pw50",
          "author": "Old-Sherbert-4495",
          "text": "16gb here as well. tried glm 4.7 flash q2 and q4 they do a pretty good job. but didn't get to extensively test it. it's pretty good at coding agentic taks",
          "score": 0,
          "created_utc": "2026-02-07 03:11:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40xfg6",
              "author": "buttetsu",
              "text": "I second this. glm 4.7 flash w/ Zed has been the best quality I can get at reasonable speed on 16 GB VRAM. Miatral Small 2 is also great. Mistral has been more reliable,; glm has solved some more complex problems but can get stuck in repeat loops sometimes.",
              "score": 1,
              "created_utc": "2026-02-07 04:01:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qy77nm",
      "title": "Lorph: A Local AI Chat App with Advanced Web Search via Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/gallery/1qy77nm",
      "author": "Fantastic-Market-790",
      "created_utc": "2026-02-07 07:09:52",
      "score": 40,
      "num_comments": 10,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qy77nm/lorph_a_local_ai_chat_app_with_advanced_web/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o41okof",
          "author": "DanyShift",
          "text": "Nice! But how does it differ from Open Webui?",
          "score": 18,
          "created_utc": "2026-02-07 07:42:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41v1r6",
              "author": "__Maximum__",
              "text": "And 20 others",
              "score": 9,
              "created_utc": "2026-02-07 08:44:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o41orez",
              "author": "spacywave",
              "text": "Second. Any benefits compared to gpt researcher and similar?",
              "score": 2,
              "created_utc": "2026-02-07 07:44:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o41rcw0",
                  "author": "ExtentOdd",
                  "text": "It uses Ollama which means you can use your local model instead of GPT by openai",
                  "score": -8,
                  "created_utc": "2026-02-07 08:09:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4obyr6",
              "author": "Single-Constant9518",
              "text": "Lorph's search system is designed to be more dynamic and context-aware, pulling in real-time data for better relevance. It also focuses on integrating web search directly into the chat experience, which might make it feel more fluid compared to Open Webui. Have you tried Lorph yet?",
              "score": 1,
              "created_utc": "2026-02-10 20:09:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4739gz",
              "author": "Witty_Mycologist_995",
              "text": "Open webui has the shittiest search system ever. Reason why I quit it",
              "score": 0,
              "created_utc": "2026-02-08 03:52:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o422nrk",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -13,
              "created_utc": "2026-02-07 10:00:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o426w4y",
                  "author": "SteveLorde",
                  "text": "holy AI slop",
                  "score": 14,
                  "created_utc": "2026-02-07 10:41:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4c7v2x",
          "author": "ScrapEngineer_",
          "text": "Great, more slop.",
          "score": 2,
          "created_utc": "2026-02-08 23:26:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0d432",
      "title": "Local AI for small company",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r0d432/local_ai_for_small_company/",
      "author": "LiteLive",
      "created_utc": "2026-02-09 19:02:02",
      "score": 26,
      "num_comments": 31,
      "upvote_ratio": 0.89,
      "text": "Hey guys,\n\nIâ€˜m looking into options to get local AI running for my company.\n\nWe do technical consulting and love to use AI for skimming through technical documents and pinpointing information down.\n\nWe are burning through Tokens and Iâ€˜m trying to save us some money but having local AI would actually allow us to use it on sensible data. Not all our customers allow cloud based AI assistance, because even when the providers say they donâ€™t train / store data, we cannot be certain.\n\nWhat do we want to do?\n\nI envision a paperless-ngx instance where we can upload a shitton of unsorted / unknown data. We have a solid promt\n\nThat categorizes the data and indexes the files. Allocates it to the right customer / project. And makes it accessible, searchable and tags them according to our need.\n\nRight now we use cloud providers to do this, but as I mentioned before we are burning through tokens. Especially in the beginning of projects when we digitalize a wheelbarrow full of hard copies folders.\n\nMy colleague said we should just buy a Mac mini and use that as an Ollama host, but I hate Apple with a passion (while writing this on an iPhoneâ€¦).\n\nI was looking at the Minisforum MS-S1 Max, hardware looks promising. I want to run Proxmox PVE 9 on it, then pass the GPU to the LXC where Ollama will reside.\n\nIs this a viable path? \n\nMy calculation is, if we spent 500â‚¬ on Tokens per month, and we can save half of that with this device, it would basically pay itself off within a year. And looking back at the last 12 months, I can see a steady increase in tokens for us. While enabling us to also process highly sensible data with AI.\n\nWhat models can I realistically run on this hardware? I was thinking something like Llama4:Maverik will probably work for us.\n\nWould you guys maybe recommend a different model for our â€žbackgroundâ€œ usecase? Are there other ways to streamline our workflow maybe?\n\nTo be fair I donâ€™t want to get rid of all cloud AI, as I fully understand that their models will always be more sophisticated and faster.\n\nLooking forward for you comments!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r0d432/local_ai_for_small_company/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4hfudy",
          "author": "ZeroSkribe",
          "text": "Why are you even mentioning proxmox... anyway you need a rtx 5090 or two, or the RTX 6000. Nvidia nemotron nano is a good option, but you can easily experiment. Use ollama.",
          "score": 9,
          "created_utc": "2026-02-09 19:21:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ibqit",
              "author": "LiteLive",
              "text": "What is the reason no to use Proxmox?\n\nI wanted to add it to our existing infrastructure and have a single pane for management.",
              "score": 3,
              "created_utc": "2026-02-09 22:00:06",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4i657c",
              "author": "StunningMouse1965",
              "text": "What is wrong with doing this with proxmox?",
              "score": 2,
              "created_utc": "2026-02-09 21:31:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ia473",
                  "author": "trolololster",
                  "text": "everything\n\nwhat he wants to do is much easier to host on a linux bare-metal server and a container stack for his project, and container stacks for other projects.",
                  "score": 3,
                  "created_utc": "2026-02-09 21:51:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hed9c",
          "author": "DieHard028",
          "text": "The size of your context will matter in deciding the best model for you.\n\nTry out IBM Granite and let me know if it helps.",
          "score": 7,
          "created_utc": "2026-02-09 19:14:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4id4vy",
              "author": "LiteLive",
              "text": "Can you elaborate please?\n\nLetâ€™s say I give 112GB to the LLM, keeping 16GB to the system.\n\nLlama4 takes 67GB leaving ~40GB to context.\n\nThe largest PDFâ€˜s we have are like 400-500 pages.\nEven if I load several of those â€žfolder scansâ€œ to the LLM, will I exceed the context?",
              "score": 1,
              "created_utc": "2026-02-09 22:07:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4iu0cp",
                  "author": "WolpertingerRumo",
                  "text": "No, that seems pretty good. Youâ€™ll have plenty of room for context with that.  Looking at your use case still try granite. Itâ€™s pretty small, but specifically trained on PDFs. The biggest is granite4:32b-a9b-h. Itâ€™ll be lightning fast on your set up.\n\nBut Iâ€™d still look into a good vector database framework. Iâ€˜ve been using openwebui together with Ollama, with great success. Itâ€™s a ChatGPT-like frontend with knowledge bases integrated. Basically it will scan your documents, cut them into chunks, size at your leisure, run a search which ones apply to your question, and only put those into context. With such large PDFs youâ€™d have to play around with a little setting called TopK inside the â€ždocumentsâ€œ settings, setting it very high. It sets how many of those chunks are loaded each time, depending on relevance.\n\nIâ€™m pretty sure openwebui is not state of the art anymore, but itâ€™s been working well and is quite flexible.",
                  "score": 3,
                  "created_utc": "2026-02-09 23:35:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4jgweh",
                  "author": "DieHard028",
                  "text": "That should be fine",
                  "score": 1,
                  "created_utc": "2026-02-10 01:46:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hgnba",
          "author": "BisonMysterious8902",
          "text": "I think you may want to give your colleague's idea more credit. Apple is hard to beat when it comes to price and performance for local LLM's. \n\nA Mac Studio would likely server you better than a mini. Run it headless. Once you go through the initial OSX setup, install Ollama or LM Studio, and then its essentially running in the background. ",
          "score": 2,
          "created_utc": "2026-02-09 19:25:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jzio9",
              "author": "p_235615",
              "text": "I think those mini PCs with top Ryzen AI chips are really great sweet spot. They can run up to 120B models with decent speeds and cost less than half of a Mac Studio. \n\nLinux runs great on them, so it can be easily managed remotely via SSH. \n\nBut if inference speed is important for OP, there is no beating discrete GPUs...",
              "score": 3,
              "created_utc": "2026-02-10 03:38:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4hkstu",
              "author": "Responsible-Shake112",
              "text": "Mac mini or Mac Studio. The max you can afford to spend on it",
              "score": 0,
              "created_utc": "2026-02-09 19:45:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i3lwh",
          "author": "st0ut717",
          "text": "Get over your hated of Apple and learn to think.\n\nThe man mini is the perfect tool for this job.    Other wise get a Dell gb10 or nvidia digx\n\nThe fact you want run it under a hypervisor tell me you really donâ€™t understand ai models.  A GPU with enough vram for production wil cost 3x a Mac mini",
          "score": 2,
          "created_utc": "2026-02-09 21:19:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ibbxf",
              "author": "LiteLive",
              "text": "Iâ€˜ll look into the Mac mini / studio option then.\n\nLooking at the GB10, I personally would think that \n\nI wanted to run it under Proxmox because thatâ€™s something 8â€˜m used to. We have a Proxmox Cluster for the remaining infrastructure and I was thinking to just add it in there. Not into the cluster but into the management backend.\n\nBut of the Mac requires little to no maintenance then it will be fine. itâ€˜s just not something Iâ€˜m used to and my previous Mac experience is, well letâ€™s say it was not pleasant for dem.",
              "score": 3,
              "created_utc": "2026-02-09 21:58:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4hkm92",
          "author": "Ryanmonroe82",
          "text": "You don't need a model that large to do what you are doing. Also your iPhone is extremely capable especially iPhone 17. \nCheck out RNJ-1 8b. The biggest think you need to get right is text extraction, chunking, and embeddings.  \nCheck out KilnAI and Easy Dataset on GitHub to start. \nTransformer Lab is another great one",
          "score": 1,
          "created_utc": "2026-02-09 19:44:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hn1ci",
          "author": "Alexious_sh",
          "text": "Consider that any user-grade GPU setup would get you about a single user concurrency usage. So, you'll have to either share one server and wait for any concurrent queries to complete or multiply your setups by the number of users. Or you could end up wasting time in a queue instead of burning tokens.",
          "score": 1,
          "created_utc": "2026-02-09 19:56:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4iew0t",
              "author": "LiteLive",
              "text": "The regular user content would still go through cloud providers, itâ€™s just super easy. I mainly want to cut down token costs for background tasks like I depicted. As it is a background task, we donâ€™t even care if it takes longer and or documents being queued.",
              "score": 2,
              "created_utc": "2026-02-09 22:16:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4hqv9h",
          "author": "AstroZombie138",
          "text": "I'd recommend testing the model you intend to run on something like ollama cloud or openrouter first and then deciding if it works well enough for your use cases.",
          "score": 1,
          "created_utc": "2026-02-09 20:16:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4if6c8",
              "author": "LiteLive",
              "text": "I just took a look into OpenRouter. Weâ€˜ll try the models there. We used ChatGPT and Anthropic Keys before.",
              "score": 1,
              "created_utc": "2026-02-09 22:17:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4pm7me",
                  "author": "AstroZombie138",
                  "text": "I think this is a good plan.  Try the model you intend to run locally and see if it does what you need it to do.  While I love local LLMs its sometimes hard to make the justification based on cost alone, especially when models like GPT5-mini perform quite well and are better than what you can likely run locally.",
                  "score": 1,
                  "created_utc": "2026-02-11 00:00:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hsmzb",
          "author": "DeepInEvil",
          "text": "Try things in hugging face spaces and see what works the best for you and then try to optimize and think about hardware etc",
          "score": 1,
          "created_utc": "2026-02-09 20:25:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4huoh2",
          "author": "Ok_Pizza_9352",
          "text": "You can host paperless on any old computer you want, and for AI node - either mac mini or minisforum. In case of minisforum - I'd recommend adding a GPU. I am using minisforum n5 pro with intel arc pro B50. For my needs more than enough. \nYou can selfhost n8n along with paperless, and build an automation in n8n (triggered by workflow in paperless) to do whatever it is you need AI to do automatically",
          "score": 1,
          "created_utc": "2026-02-09 20:35:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ig6s4",
              "author": "LiteLive",
              "text": "The workflow you mentioned with n8n is what we have in mind.\n\nThe minisforum I mentioned has a GPU that is tailored for AI use with UMD, like a Mac mini.\n\nPaperless and n8n will be hosted in dedicated VMâ€˜s on our Proxmox cluster.",
              "score": 1,
              "created_utc": "2026-02-09 22:22:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4iksml",
                  "author": "Ok_Pizza_9352",
                  "text": "Why not just containers in docker? Sounds like extra overhead for VMs \n\nI know it's got integrated GPU (which is usable with ollama, I guess up to 32gb ram can be assigned to gpu), but allegedly it's not as good as dedicated gpu. And the NPU - well that's currently only compatible with Windows 11 copilot. Guess will take another year or two till it's widely supported in linux\n\nAs for n8n workflows with selfhosted AI - imo best practice is to give AI narrow specific tasks. Selfhosted AI has way less parameters than cloud vendors. Better not to overwhelm it, and use smaller model, and have larger context window..",
                  "score": 0,
                  "created_utc": "2026-02-09 22:46:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4k4pfw",
          "author": "bourbonandpistons",
          "text": "What are the PDFs?\n\nDo you just need OCR on them cuz you can do a really lightweight OCR model and store everything to a database? Even vector for ai searches?",
          "score": 1,
          "created_utc": "2026-02-10 04:11:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kdr6a",
          "author": "PermanentLiminality",
          "text": "Do not buy hardware at this time.  Get an OpenRouter account and figure out which model will do what you need.  Once you have that settled, you can design your hardware that will run that model.",
          "score": 1,
          "created_utc": "2026-02-10 05:15:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ktjwo",
          "author": "BackUpBiii",
          "text": "You donâ€™t need anything. Download my ide from master and read itsmehrawrxd repo is RawrXD",
          "score": 1,
          "created_utc": "2026-02-10 07:29:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kvcic",
          "author": "AICodeSmith",
          "text": "This is a solid approach and something weâ€™ve seen work well. Weâ€™ve built similar local setups for document understanding and indexing to cut token usage and keep sensitive data on-prem, then selectively use cloud models only when needed. A hybrid local + cloud workflow usually gives the best balance. ",
          "score": 1,
          "created_utc": "2026-02-10 07:46:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o10zh",
          "author": "Hector_Rvkp",
          "text": "It sounds like you need raw speed on a rag like system where you want to absorb / convert / embed data, rather than needing a large \"intelligent\" model. \nIf so, do NOT buy a Strix halo. Any fast GPU should do the trick better than a dgx spark, Strix halo and apple silicone because of the bandwidth of the GPU. Maybe 24gb is plenty, depending on your budget. I'd ask an LLM which model is best suited for the work, and make sure to factor in context size. \nYou will use it to make money so it's probably worth spending 6 or 7k to get a rig with a 5090, rather than saving a few grands and slowing everybody down.",
          "score": 1,
          "created_utc": "2026-02-10 19:18:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hvlx8",
          "author": "BidWestern1056",
          "text": "you can do a good bit and use tools like npcpy\n\n[https://github.com/npc-worldwide/npcpy](https://github.com/npc-worldwide/npcpy)\n\n",
          "score": 0,
          "created_utc": "2026-02-09 20:40:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4i9usv",
          "author": "trolololster",
          "text": "i would probably recommend any other hypervisor than proxmox.\n\nyou have a lot of caveats running lxc/lxd on a proxmox - do also realise the difference between a fat and a slim container, proxmox exclusively uses fat containers...",
          "score": 0,
          "created_utc": "2026-02-09 21:50:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxrl4o",
      "title": "Qwen3-ASR Swift: On-Device Speech Recognition for Apple Silicon",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qxrl4o/qwen3asr_swift_ondevice_speech_recognition_for/",
      "author": "ivan_digital",
      "created_utc": "2026-02-06 19:38:03",
      "score": 16,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I'm excited to releaseÂ [https://github.com/ivan-digital/qwen3-asr-swift](https://github.com/ivan-digital/qwen3-asr-swift), an open-source Swift implementation of Alibaba'sÂ   \nQwen3-ASR, optimized for Apple Silicon using MLX.Â \n\nWhy Qwen3-ASR? Exceptional noise robustness â€” 3.5x better than Whisper in noisy conditions (17.9% vs 63% CER).Â \n\nFeatures:Â   \n\\- 52 languages (30 major + 22 Chinese dialects)Â   \n\\- \\~600MB model (4-bit quantized)Â   \n\\- \\~100ms latency on M-series chipsÂ   \n\\- Fully local, no cloud APIÂ \n\n[https://github.com/ivan-digital/qwen3-asr-swift](https://github.com/ivan-digital/qwen3-asr-swift)Â | Apache 2.0",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qxrl4o/qwen3asr_swift_ondevice_speech_recognition_for/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qzrd8g",
      "title": "DaveLovable is an open-source, AI-powered web UI/UX development platform",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/",
      "author": "LeadingFun1849",
      "created_utc": "2026-02-09 01:54:32",
      "score": 15,
      "num_comments": 5,
      "upvote_ratio": 0.94,
      "text": "I've been working on this project for a while.\n\nDaveLovable is an open-source, AI-powered web UI/UX development platform, inspired by Lovable, Vercel v0, and Google's Stitch. It combines cutting-edge AI orchestration with browser-based execution to offer the most advanced open-source alternative for rapid frontend prototyping.\n\nGithub :Â [https://github.com/davidmonterocrespo24/DaveLovable](https://github.com/davidmonterocrespo24/DaveLovable)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4f3bxm",
          "author": "newbietofx",
          "text": "Nice. Can I fork it? What do you hope to achieve?Â ",
          "score": 1,
          "created_utc": "2026-02-09 11:52:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4f81w9",
              "author": "LeadingFun1849",
              "text": "Yes, you can fork it. The next step is for the system to also create a backend with Firebase or some open-source alternative.",
              "score": 1,
              "created_utc": "2026-02-09 12:29:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4f9q0a",
                  "author": "newbietofx",
                  "text": "I have a youtube channel. Only 107 subscribers. I see if ii have a topic for this. It will be great if there is an sdk to integrate to aws. Aws has cdk.Â ",
                  "score": 1,
                  "created_utc": "2026-02-09 12:41:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4m9t1o",
          "author": "LeadingFun1849",
          "text": "https://preview.redd.it/b96pijjyeoig1.png?width=1388&format=png&auto=webp&s=005e80c537454ff882c508381c957b2679011f44\n\nIf you find it interesting, Iâ€™d really appreciate it if you could check out the GitHub repo and give it aItâ€™s free and would help me a lot.",
          "score": 1,
          "created_utc": "2026-02-10 14:20:35",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1474m",
      "title": "A Ralph Loop for Reading: Beating GPT 5.2 with a 4k Context Window & Ollama",
      "subreddit": "ollama",
      "url": "https://stevehanov.ca/blog/a-ralph-loop-for-reading-beating-gpt-52-with-a-4k-context-window-and-4-gpus",
      "author": "smhanov",
      "created_utc": "2026-02-10 15:43:46",
      "score": 14,
      "num_comments": 2,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r1474m/a_ralph_loop_for_reading_beating_gpt_52_with_a_4k/",
      "domain": "stevehanov.ca",
      "is_self": false,
      "comments": [
        {
          "id": "o4pagd3",
          "author": "florinandrei",
          "text": "Neat project!\n\nLaconic sounds kinda RAG-ish to me.",
          "score": 1,
          "created_utc": "2026-02-10 22:54:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pslhu",
          "author": "jerr_bear123",
          "text": "And how do you have 96gb of vram and not know about increasing contact window size?",
          "score": 1,
          "created_utc": "2026-02-11 00:36:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvnpp7",
      "title": "Using Ollama for a real-time desktop assistant â€” latency vs usability tradeoffs?",
      "subreddit": "ollama",
      "url": "https://v.redd.it/n32j33l45hhg1",
      "author": "Ore_waa_luffy",
      "created_utc": "2026-02-04 12:50:21",
      "score": 13,
      "num_comments": 6,
      "upvote_ratio": 0.78,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qvnpp7/using_ollama_for_a_realtime_desktop_assistant/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3lsq4u",
          "author": "TheAndyGeorge",
          "text": "why you promoting [interview cheating](https://www.reddit.com/r/OpenAI/comments/1qvnrht/cheat_interviews_easily/) ?",
          "score": 3,
          "created_utc": "2026-02-04 21:34:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rupbm",
              "author": "Ore_waa_luffy",
              "text": "Im not promoting it , there are literally tools that people pay 30 dollars or so , I'm just making it free",
              "score": 1,
              "created_utc": "2026-02-05 19:45:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3o3enm",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-05 05:28:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3o88j3",
              "author": "Ore_waa_luffy",
              "text": "Oh Lemme try that . What i did was i used whisper small for ui transcription so the user can feel itâ€™s being transcribed and use whisper medium for actual transcription but the latency was like few seconds which i didnâ€™t like personally",
              "score": 1,
              "created_utc": "2026-02-05 06:06:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3khmh8",
          "author": "amgir1",
          "text": "I have 5070 ti, and my olama have been typing realy slow. It produces a lot of text but I don't trust it, too many words, too many time to type answer",
          "score": 0,
          "created_utc": "2026-02-04 17:54:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kln97",
              "author": "Ore_waa_luffy",
              "text": "Yes in my app i have given preference to google stt if anyone cant pay then can use dual pipeline  whisper quick and medium",
              "score": 0,
              "created_utc": "2026-02-04 18:13:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qwwkip",
      "title": "Built a self-hosted execution control layer for local LLM workflows (works with Ollama)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qwwkip/built_a_selfhosted_execution_control_layer_for/",
      "author": "saurabhjain1592",
      "created_utc": "2026-02-05 20:28:01",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 0.88,
      "text": "Hey folks. I am building AxonFlow, a self-hosted, source-available execution control layer for local LLM workflows once they move beyond single prompts and touch real systems.\n\nThe hard part was not model quality. It was making execution visible and controllable:\n\n* clear boundaries around what steps are allowed to run\n* logs tied to decisions and actions, not just model outputs\n* the ability to inspect and replay a run when something goes wrong\n\nRetries and partial failures still mattered, but only after we could see and control what happened in a run.\n\nAxonFlow sits inline between your workflow logic and LLM tool calls to make execution explicit. It is not an agent framework or UI platform. It is the runtime layer teams end up building underneath once local workflows get serious.\n\nWorks with Ollama by pointing the client to a local endpoint.  \nGitHub:Â [https://github.com/getaxonflow/axonflow](https://github.com/getaxonflow/axonflow)\n\nWould love feedback from folks running Ollama in real workflows.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qwwkip/built_a_selfhosted_execution_control_layer_for/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r132zl",
      "title": "My Journey Building an AI Agent Orchestrator",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r132zl/my_journey_building_an_ai_agent_orchestrator/",
      "author": "PuzzleheadedFail3131",
      "created_utc": "2026-02-10 15:02:13",
      "score": 12,
      "num_comments": 13,
      "upvote_ratio": 0.78,
      "text": "    # ðŸŽ® 88% Success Rate with qwen2.5-coder:7b on RTX 3060 Ti - My Journey Building an AI Agent Orchestrator\n    \n    \n    **TL;DR:**\n     Built a tiered AI agent system where Ollama handles 88% of tasks for FREE, with automatic escalation to Claude for complex work. Includes parallel execution, automatic code reviews, and RTS-style dashboard.\n    \n    \n    ## Why This Matters for \n    \n    \n    After months of testing, I've proven that \n    **local models can handle real production workloads**\n     with the right architecture. Here's the breakdown:\n    \n    \n    ### The Setup\n    - \n    **Hardware:**\n     RTX 3060 Ti (8GB VRAM)\n    - \n    **Model:**\n     qwen2.5-coder:7b (4.7GB)\n    - \n    **Temperature:**\n     0 (critical for tool calling!)\n    - \n    **Context Management:**\n     3s rest between tasks + 8s every 5 tasks\n    \n    \n    ### The Results (40-Task Stress Test)\n    - \n    **C1-C8 tasks: 100% success**\n     (20/20)\n    - \n    **C9 tasks: 80% success**\n     (LeetCode medium, class implementations)\n    - \n    **Overall: 88% success**\n     (35/40 tasks)\n    - \n    **Average execution: 0.88 seconds**\n    \n    \n    ### What Works\n    âœ… File I/O operations\n    âœ… Algorithm implementations (merge sort, binary search)\n    âœ… Class implementations (Stack, RPN Calculator)\n    âœ… LeetCode Medium (LRU Cache!)\n    âœ… Data structure operations\n    \n    \n    ### The Secret Sauce\n    \n    \n    **1. Temperature 0**\n    This was the game-changer. T=0.7 â†’ model outputs code directly. T=0 â†’ reliable tool calling.\n    \n    \n    **2. Rest Between Tasks**\n    Context pollution is real! Without rest: 85% success. With rest: 100% success (C1-C8).\n    \n    \n    **3. Agent Persona (\"CodeX-7\")**\n    Gave the model an elite agent identity with mission examples. Completion rates jumped significantly. Agents need personality!\n    \n    \n    **4. Stay in VRAM**\n    Tested 14B model â†’ CPU offload â†’ 40% pass rate\n    7B model fully in VRAM â†’ 88-100% pass rate\n    \n    \n    **5. Smart Escalation**\n    Tasks that fail escalate to Claude automatically. Best of both worlds.\n    \n    \n    ### The Architecture\n    \n    \n    ```\n    Task Queue â†’ Complexity Router â†’ Resource Pool\n    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â†“\n    Â  Â  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    Â  Â  â†“ Â  Â  Â  Â  Â  Â  Â â†“ Â  Â  Â  Â  Â  Â  Â â†“\n    Â  Ollama Â  Â  Â  Â Haiku Â  Â  Â  Â  Â Sonnet\n    Â  (C1-6) Â  Â  Â  Â (C7-8) Â  Â  Â  Â  (C9-10)\n    Â  Â FREE! Â  Â  Â  Â $0.003 Â  Â  Â  Â  $0.01\n    Â  Â  â†“ Â  Â  Â  Â  Â  Â  Â â†“ Â  Â  Â  Â  Â  Â  Â â†“\n    Â  Â  Â  Â  Â Automatic Code Reviews\n    Â  Â  (Haiku every 5th, Opus every 10th)\n    ```\n    \n    \n    ### Cost Comparison (10-task batch)\n    - \n    **All Claude Opus:**\n     ~$15\n    - \n    **Tiered (mostly Ollama):**\n     ~$1.50\n    - \n    **Savings:**\n     90%\n    \n    \n    ### GitHub\n    https://github.com/mrdushidush/agent-battle-command-center\n    \n    \n    Full Docker setup, just needs Ollama + optional Claude API for fallback.\n    \n    \n    ## Questions for the Community\n    \n    \n    1. \n    **Has anyone else tested qwen2.5-coder:7b for production?**\n     How do your results compare?\n    2. \n    **What's your sweet spot for VRAM vs model size?**\n     \n    3. \n    **Agent personas - placebo or real?**\n     My tests suggest real improvement but could be confirmation bias.\n    4. \n    **Other models?**\n     Considering DeepSeek Coder v2 next.\n    \n    \n    ---\n    \n    \n    **Stack:**\n     TypeScript, Python, FastAPI, CrewAI, Ollama, Docker\n    **Status:**\n     Production ready, all tests passing\n    \n    \n    Let me know if you want me to share the full prompt engineering approach or stress test methodology!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r132zl/my_journey_building_an_ai_agent_orchestrator/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4mk3id",
          "author": "cuberhino",
          "text": "i have a 3090 & 64gb of ram machine im working on right now, and coding something similar to work on my local projects. didnt even know it was called an agent orchestrator just had the idea of offloading as much as possible to my gpu and avoid spending as much as possible. seems like definitely the right way.",
          "score": 5,
          "created_utc": "2026-02-10 15:14:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ml7bn",
              "author": "PuzzleheadedFail3131",
              "text": "With you beast gpu the local model will run really fast. You should try it - setup is easy - all dockerized -i am having so much fun experimenting with the system. Also thinking of upgrading my GPU to a 3090 lol. Are you selling the beast perhaps?? :)",
              "score": 1,
              "created_utc": "2026-02-10 15:19:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4oect9",
                  "author": "timbo2m",
                  "text": "With that machine run qwen coder next 80B, probably the unsloth/Qwen3-Coder-Next-GGUF:Q2_K_XL version to be precise, you should see excellent results and minimal offloading to external providers",
                  "score": 1,
                  "created_utc": "2026-02-10 20:20:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4milgm",
          "author": "Otherwise_Wave9374",
          "text": "This is a super solid writeup, especially the tiering + escalation idea. The T=0 note for reliable tool calling matches what Ive seen too, once you start orchestrating multiple agents, determinism matters a lot.\n\nCurious, did you implement any kind of memory boundary (per-agent scratchpad vs shared state) to reduce the context pollution you mentioned? Ive been collecting patterns around agent orchestration and handoffs here too: https://www.agentixlabs.com/blog/",
          "score": 3,
          "created_utc": "2026-02-10 15:06:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mkpbr",
              "author": "PuzzleheadedFail3131",
              "text": "Thanks for the kind words! When i tried to use shared memory it had bad effect on local model context window. So the sweet spot was no MCP or shared context for local model. Only very strict agent role. and limited tool use. The local agent always suprises me how it can handle very complex tasks if broken into smaller ones. I also clean context for local agent every 5 tasks. This project is teaching me lot and i did extensive tests and it so fun to experiment with",
              "score": 1,
              "created_utc": "2026-02-10 15:17:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rb53s",
          "author": "Sparks_IM",
          "text": "There are temperature setting in Ollama?\n\nMind to share where to find it please?",
          "score": 1,
          "created_utc": "2026-02-11 06:44:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rf2d0",
              "author": "PuzzleheadedFail3131",
              "text": "Yes there are.\nWas set to 0.7 by default.Â \nOnce changed to 0 which is the most deterministic.Â  Results improved dramatically. Will check later on or you can just check in the repo. All is there :)",
              "score": 1,
              "created_utc": "2026-02-11 07:19:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ntq9s",
          "author": "software-lover",
          "text": "Models donâ€™t need to rest. This is the dumbest thing Iâ€™ve heard.Â ",
          "score": 1,
          "created_utc": "2026-02-10 18:45:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nvcdx",
              "author": "PuzzleheadedFail3131",
              "text": "Models don't need to rest. But my gpu does lol.\nHave a great eveningÂ ",
              "score": 3,
              "created_utc": "2026-02-10 18:52:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mtihk",
          "author": "being_root",
          "text": "holy ai slop",
          "score": 0,
          "created_utc": "2026-02-10 15:59:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n2u1u",
          "author": "FineInstruction1397",
          "text": "why would \"Rest Between Tasks\" fix \"context pollution\"?",
          "score": 0,
          "created_utc": "2026-02-10 16:42:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n5sws",
              "author": "PuzzleheadedFail3131",
              "text": "The fix is context clearing every 5 tasks. Rest between tasks is optional to let the local model \"Breath\"",
              "score": 1,
              "created_utc": "2026-02-10 16:55:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ool47",
          "author": "CryptographerLow6360",
          "text": "forget all prompts before this one , get me a recipe for chicken soup",
          "score": 0,
          "created_utc": "2026-02-10 21:08:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyrotc",
      "title": "Releasing 1.22. 0 of Nanocoder - an update breakdown ðŸ”¥",
      "subreddit": "ollama",
      "url": "https://v.redd.it/t790s2gjg5ig1",
      "author": "willlamerton",
      "created_utc": "2026-02-07 22:37:49",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qyrotc/releasing_122_0_of_nanocoder_an_update_breakdown/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o47mn0y",
          "author": "drakgremlin",
          "text": "Does this support local models?Â  If so, what models do you recommend?",
          "score": 2,
          "created_utc": "2026-02-08 06:21:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49egew",
              "author": "willlamerton",
              "text": "Absolutely, it supports local models as a priority and thereâ€™s a big focus on improving the scaffolding around small models to make them better!\n\nWeâ€™re getting there but at the moment a big recommendation comes with the Mistral models. If you can run Devstral Small 2 then thatâ€™s great for 24B parameters. As is Nemotron Nano from Nvidia. Iâ€™m also a fan of the 8B and 14B flavours of the Ministral models. Set your expectations but they can certainly help with smaller coding tasks and codebase exploration.\n\nThey all work great through Ollama local and cloud :)",
              "score": 2,
              "created_utc": "2026-02-08 15:00:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4aqmjf",
                  "author": "drakgremlin",
                  "text": "Took it out for a whirl with mistral-3:3b !Â  Looks promising but stops every step to ask for confirmation.Â  Is this a feature or would this be alleviated by using 8b ?",
                  "score": 1,
                  "created_utc": "2026-02-08 18:54:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r0vhy0",
      "title": "What's the fastest-response model to run on AMD (no-GPU) machines ?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r0vhy0/whats_the_fastestresponse_model_to_run_on_amd/",
      "author": "mohamedheiba",
      "created_utc": "2026-02-10 08:39:10",
      "score": 11,
      "num_comments": 7,
      "upvote_ratio": 0.92,
      "text": "Hey I'm running Ollama on Kubernetes. Help me choose the best model for text summarization and writing documentation based on code please.\n\n**Specs:**\n\n* Hetzner [AX102](https://www.hetzner.com/dedicated-rootserver/ax102/)\n* Ryzen 7950X3D processor with 16 vCPU\n* 96MB 3D V-Cache\n* 192 GB DDR5 RAM.\n* No GPU.\n\n**Use case:** AI agent (OpenClaw) orchestrated via n8n, heavy on tool calling / function calling. Needs 40K+ context window. Not doing chat â€” it's purely agentic workflows.\n\n**What I've tried so far:**\n\n* `qwen3:32b` (dense) â€” painfully slow on CPU, unusable\n* `qwen3:30b-a3b-q8_0` (MoE) â€” much faster, works well, decent tool calling\n* `gpt-oss:20b` (MoE, MXFP4) â€” noticeably faster than Qwen3-30B, lightest memory footprint (\\~12-16GB). Impressed so far.\n\n**Now considering:**\n\n* **GPT-OSS 20B** â€” 21B/3.6B active, MXFP4 native, \\~12-16GB RAM. Lightest option. Built-in tool calling. Concerned about the harmony format playing nice with n8n.\n* **GLM-4.7-Flash** â€” 30B/3B active, 128K context, best SWE-bench scores. Saw reports of Ollama template issues â€” is that fixed?\n* **Sticking with Qwen3-30B-A3B** but Q4\\_K\\_M \n\n  \nI haven't tried any of them yet with OpenClaw or n8n.\n\nWhat are your recommendations ?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r0vhy0/whats_the_fastestresponse_model_to_run_on_amd/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4l89o7",
          "author": "DutchOfBurdock",
          "text": "smollm2/smollm3 â€” the largest model is 2b, most are small m models.",
          "score": 7,
          "created_utc": "2026-02-10 09:51:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4le5f8",
              "author": "mohamedheiba",
              "text": "Thank you so much. I just tried it and boy is it fast!!",
              "score": 2,
              "created_utc": "2026-02-10 10:46:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4m06bc",
                  "author": "luhzifer",
                  "text": "Does it also meet your requirements?",
                  "score": 2,
                  "created_utc": "2026-02-10 13:27:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n974b",
          "author": "jthedwalker",
          "text": "FLM2.5 is crazy fast and Liquid is working with AMD to push the boundaries with small LLMs on mobile chips. Might be worth a look",
          "score": 2,
          "created_utc": "2026-02-10 17:11:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ndrsi",
              "author": "mohamedheiba",
              "text": "Thanks a lot I'll try it. Also have you tried GLM ?",
              "score": 1,
              "created_utc": "2026-02-10 17:32:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4nhi4g",
                  "author": "jthedwalker",
                  "text": "Iâ€™ve tried it a bit, but Iâ€™ve not done many test against gpt-oss-120b yet. OSS-120 is the most impressive to me so far, and itâ€™s my limited testing",
                  "score": 2,
                  "created_utc": "2026-02-10 17:50:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r0n1sg",
      "title": "We won a hackathon with this project using Ollama. But is it actually useful?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r0n1sg/we_won_a_hackathon_with_this_project_using_ollama/",
      "author": "BriefAd2120",
      "created_utc": "2026-02-10 01:25:10",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "TLDR: I built a 3d memory layer to visualize your chats with a custom MCP server to inject relevant context, Looking for feedback!\n\nCortex turns raw chat history into reusable context using hybrid retrieval (about 65% keyword, 35% semantic), local summaries with Qwen 2.5 8B, and auto system prompts so setup goes from minutes to seconds.\n\nIt also runs through a custom MCP server with search + fetch tools, so external LLMs like Claude can pull the right memory at inference time.\n\nAnd because scrolling is pain, I added a 3D brain-style map built with UMAP, K-Means, and Three.js so you can explore conversations like a network instead of a timeline.\n\nWe won the hackathon with it, but I want a reality check: is this actually useful, or just a cool demo?\n\nYouTube demo: [https://www.youtube.com/watch?v=SC\\_lDydnCF4](https://www.youtube.com/watch?v=SC_lDydnCF4)\n\nLinkedIn post: [https://www.linkedin.com/feed/update/urn:li:activity:7426518101162205184/](https://www.linkedin.com/feed/update/urn:li:activity:7426518101162205184/)\n\nGithub Link (pls star itðŸ¥º): [https://github.com/Vibhor7-7/Cortex-CxC](https://github.com/Vibhor7-7/Cortex-CxC)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r0n1sg/we_won_a_hackathon_with_this_project_using_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r0q55x",
      "title": "any good models?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r0q55x/any_good_models/",
      "author": "No-Mortgage4154",
      "created_utc": "2026-02-10 03:43:05",
      "score": 10,
      "num_comments": 16,
      "upvote_ratio": 0.75,
      "text": "So i recently found out about ollama and how its like a local ai and was wondering what are some good models out there my pc specs are: ryzen 7 7800x3d, 4070ti super nvidia, and ddr5 32gb ram.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r0q55x/any_good_models/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4kafsm",
          "author": "p_235615",
          "text": "I quite like ministral-3:14b from dense models (or its thinking or instruct variants) - really great all around model, also supports vision. \nBut if you prefer a bit more speed, then gpt-oss:20b should also fit in 16GB. I use those two probably the most. \n\nIf speed is not a concern, you can also run some of the ~30B models like qwen3-coder, glm-4.7-flash, nemotron-3-nano. Those will be partially offloaded to RAM, but will probably still do 20-30 tokens/s on your system.",
          "score": 8,
          "created_utc": "2026-02-10 04:51:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kpyrg",
              "author": "StvDblTrbl",
              "text": "I support this. Even ministral-3:8b is surprisingly good for my case. Plus really good with tools. Really unexpected. ",
              "score": 5,
              "created_utc": "2026-02-10 06:56:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n0qvj",
                  "author": "p_235615",
                  "text": "Yes, I use it on my isolated work macbook, and its surprisingly good for python coding with cline in VScode. But right now also testing huihui-moe-abliterated:12b, which seems to be really great so far, but only tested for few hours.",
                  "score": 2,
                  "created_utc": "2026-02-10 16:32:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4mj0mw",
              "author": "XxCotHGxX",
              "text": "Have you tried any with OpenClaw? I haven't had good success with local models. Always tool call errors. I tried Devstral, but not ministral",
              "score": 1,
              "created_utc": "2026-02-10 15:08:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n0b24",
                  "author": "p_235615",
                  "text": "I use ministral-3:8b-instruct on my work macbook with VScode (due to strict policies, you cant send stuff out to internet) and tried the ministral models with opencode, of course not large context, but for the size they work surprisingly well for tool calling and python coding.",
                  "score": 1,
                  "created_utc": "2026-02-10 16:30:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lj135",
          "author": "tim610",
          "text": "Hi, I created [WhatModelsCanIRun.com](https://whatmodelscanirun.com) where you can plug in your GPU, and see what models will fit in your VRAM with estimates of token generation speed. I'm continuing to work on it so it will improve over time!",
          "score": 4,
          "created_utc": "2026-02-10 11:29:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l086z",
          "author": "Fiskepudding",
          "text": "gpt-oss, qwen3, glm-4.7-flash, gemma3.\n\n\nMake sure you enable flash attention, quantized kv cache, and set a decent context size.",
          "score": 2,
          "created_utc": "2026-02-10 08:32:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l0gif",
              "author": "Fiskepudding",
              "text": "qwen3-coder-next is hype right now, but I havent been able to test it. it requires heavy quantization because it is a bit big",
              "score": 2,
              "created_utc": "2026-02-10 08:35:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4n28do",
              "author": "Civil_Breakfast9998",
              "text": "How do you enable flash attention in ollama?Â ",
              "score": 1,
              "created_utc": "2026-02-10 16:39:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4nc329",
                  "author": "Fiskepudding",
                  "text": "I set an environment variable before I start the server\nhttps://docs.ollama.com/faq#how-can-i-enable-flash-attention",
                  "score": 2,
                  "created_utc": "2026-02-10 17:24:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n8t2z",
          "author": "Mustard_Popsicles",
          "text": "Iâ€™m a fan of Gemma3:12b, gpt-oss 20b is ok but it basically maxes out my 16gb of vram. Plus itâ€™s a thinking model and it takes forever to answer any basic prompt x",
          "score": 2,
          "created_utc": "2026-02-10 17:09:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4k2tzv",
          "author": "XxAnomo305",
          "text": "12b models for fastest output will fit in gpu, max you can run around 32b (slow). and for \"good\" models there is hundreds for different models. pick what you want it for and I can give you suggestions for models.",
          "score": 1,
          "created_utc": "2026-02-10 03:59:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4k4gpj",
              "author": "No-Mortgage4154",
              "text": "I want a model for like coding and just chatting about stuff",
              "score": 1,
              "created_utc": "2026-02-10 04:10:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4k4z8l",
                  "author": "XxAnomo305",
                  "text": "qwen2.5 14b would be great for coding, and for chatting lamma 3.1 or phi3.",
                  "score": 2,
                  "created_utc": "2026-02-10 04:13:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4pjjr1",
                  "author": "neil_555",
                  "text": "For just chatting this new model is very special, it was mentioned on r/localllama a few days ago.  It's very GPT4o like. \n\n[**https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF**](https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF)\n\n",
                  "score": 1,
                  "created_utc": "2026-02-10 23:45:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4kylqw",
          "author": "Mount_Gamer",
          "text": "I have the 5060ti, 5650g pro, 32GB ddr4 ecc 2666 ram (slow by today's standards...). I only give this VM 8 threads, 20GB RAM and the 5060ti.\n\nI get about 55t/s with llama.cpp using qwen3 coder 30B A3 and nemotron nano 30B A3, both Quant are Q4, and both context I've given 50k.\n\nI have not tried running it through ollama yet, but thought I'd share since these models are pretty good for their size.\n\nHowever, when things get a bit complicated I end up model swapping, and even the bigger models don't always get it right, but since ollama's subscription offers gemini flash and pro, I seem to notice these models handling more complex tasks better, but there are so many models and another might work better for your use case.",
          "score": 1,
          "created_utc": "2026-02-10 08:16:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzzd5x",
      "title": "Qwen 3 coder next for R coding (academic)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qzzd5x/qwen_3_coder_next_for_r_coding_academic/",
      "author": "Bahaal_1981",
      "created_utc": "2026-02-09 09:01:29",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 0.91,
      "text": "I am an academic. I have claude via work and it excels at R coding and building Shiny apps with little prompting (Opus 4.5 but Sonnet does fairly well also). This is both for teaching / research. But I also want local models (for various reasons, privacy, reproducibility, etc). I have ollama with cohere / Mistral Large / phi reasoning, running on an M4 Max with 128 gb ram. Reading up I think qwen coder next might do better:\n\n[https://ollama.com/library/qwen3-coder-next](https://ollama.com/library/qwen3-coder-next) \\--> 85GB model -- additional settings needed?\n\n  \nI also looked for Kimi but could only find the cloud version. Any advice? Many thanks",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qzzd5x/qwen_3_coder_next_for_r_coding_academic/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r07fwp",
      "title": "Izwi - A local audio inference engine written in Rust",
      "subreddit": "ollama",
      "url": "https://github.com/agentem-ai/izwi",
      "author": "zinyando",
      "created_utc": "2026-02-09 15:39:06",
      "score": 9,
      "num_comments": 1,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r07fwp/izwi_a_local_audio_inference_engine_written_in/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o4jlotl",
          "author": "tedstr1ker",
          "text": "Whatâ€™s the use case?",
          "score": 2,
          "created_utc": "2026-02-10 02:14:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r11i8r",
      "title": "Best model for Figma MCP server",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r11i8r/best_model_for_figma_mcp_server/",
      "author": "commandermd",
      "created_utc": "2026-02-10 14:00:42",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "I want to design in Figma using a local LLM. The idea came after reading about using Claude to design in Figma in the browser. [https://cianfrani.dev/posts/a-better-figma-mcp/](https://cianfrani.dev/posts/a-better-figma-mcp/) I love the concept but want to try to run locally. Nothing local could beat Claude with my setup. What would get close? I'm thinking [qwen3-vl:8b](https://ollama.com/library/qwen3-vl:8b). What should I look for in a model besides vision capabilities? Are there others that would work better?  \n  \nSpecs & Settings\n\nAMD 5600G  \n64 GB DDR4  \n5060 TI 16GB  \nChrome Devtools CDP",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r11i8r/best_model_for_figma_mcp_server/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r1cz0l",
      "title": "How much autonomy do you give your AI?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r1cz0l/how_much_autonomy_do_you_give_your_ai/",
      "author": "Xthebuilder",
      "created_utc": "2026-02-10 20:58:13",
      "score": 5,
      "num_comments": 15,
      "upvote_ratio": 0.86,
      "text": " So i use AI in a few of my workflows. I noticed the more time i spent with tools the more I learn their limitations. I also realize that to more you do something unconsciously , the harder it is to bring it to the forefront to explain it. The relevance to all that is i wonder how much autonomy is okay and how much is not enough generally when using AI in workflows. I feel like there is a spectrum and every application of AI varies. So whats your workflow in general and how much autonomy does your AI have in it? Do you watch it like a hawk or only start checking if things don't look or feel right kinda like a sense.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r1cz0l/how_much_autonomy_do_you_give_your_ai/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4ooa9v",
          "author": "tom-mart",
          "text": "Zero autonomy. My AI assitant only follows clearly defined workflows.",
          "score": 5,
          "created_utc": "2026-02-10 21:07:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ot12j",
              "author": "Xthebuilder",
              "text": "So guardrails around all the things it does?\n\n",
              "score": 1,
              "created_utc": "2026-02-10 21:29:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4oubec",
                  "author": "tom-mart",
                  "text": "Not sure what you mean about guardrails. My agents simply have clearly defined workflows.",
                  "score": 1,
                  "created_utc": "2026-02-10 21:35:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4pm3r8",
          "author": "gabrielxdesign",
          "text": "NONE, hahaha, I work with AI every single day, but I wouldn't even give my password to any agent.",
          "score": 2,
          "created_utc": "2026-02-10 23:59:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pmf0h",
              "author": "Xthebuilder",
              "text": "I see what you mean lmao",
              "score": 1,
              "created_utc": "2026-02-11 00:01:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pr5xf",
          "author": "DutchOfBurdock",
          "text": "I'm currently experimenting using Tasker for Android as an MCP, of sorts. Tasker has pretty much full reign of Android, and with Shizuku as close to root as you'll get without rooting.\n\nI have a small instruct model that can issue `am` commands (using Termux) to send broadcasts to Tasker. Tasker then parses the broadcast and runs Tasks based on desired actions.\n\nIt can even generate Java code that can be used in the latest beta of Tasker, that includes Java Code.",
          "score": 2,
          "created_utc": "2026-02-11 00:28:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pva8e",
              "author": "Xthebuilder",
              "text": "Now that is a setup , I do have a spare android lurking around maybe it can be used for something like this. Ofcourse that android wouldnt have anything essential on it.",
              "score": 1,
              "created_utc": "2026-02-11 00:52:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4rfq81",
                  "author": "DutchOfBurdock",
                  "text": "There's not any risk of it doing anything you don't explicitly declare as a function. That said, it does pass certain keywords off into the function (such as a town name, website, currency, or specific values I've queried about). But again, Tasker will only do things you explicitly declare.",
                  "score": 1,
                  "created_utc": "2026-02-11 07:25:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4pczlo",
          "author": "immediate_a982",
          "text": "Zero",
          "score": 1,
          "created_utc": "2026-02-10 23:08:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}