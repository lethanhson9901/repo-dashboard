{
  "metadata": {
    "last_updated": "2026-01-18 16:49:53",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 16,
    "total_comments": 42,
    "file_size_bytes": 73722
  },
  "items": [
    {
      "id": "1qbrx4b",
      "title": "Open Source Enterprise Search Engine (Generative AI Powered)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qbrx4b/open_source_enterprise_search_engine_generative/",
      "author": "Effective-Ad2060",
      "created_utc": "2026-01-13 13:55:10",
      "score": 51,
      "num_comments": 4,
      "upvote_ratio": 0.97,
      "text": "Hey everyone!\n\nI‚Äôm excited to share something we‚Äôve been building for the past 6 months, a¬†**fully open-source Enterprise Search Platform**¬†designed to bring powerful Enterprise Search to every team, without vendor lock-in. The platform brings all your business data together and makes it searchable. It connects with apps like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, Local file uploads and more. You can deploy it and run it with just one docker compose command.\n\nYou can run the full platform locally. Recently, one of our users tried¬†**qwen3-vl:8b (16 FP)**¬†with¬†**Ollama**¬†and got very good results.\n\nThe entire system is built on a¬†**fully event-streaming architecture powered by Kafka**, making indexing and retrieval scalable, fault-tolerant, and real-time across large volumes of data.\n\nAt the core, the system uses an **Agentic Graph RAG approach**, where retrieval is guided by an enterprise knowledge graph and reasoning agents. Instead of treating documents as flat text, agents reason over relationships between users, teams, entities, documents, and permissions, allowing more accurate, explainable, and permission-aware answers.\n\n**Key features**\n\n* Deep understanding of documents, user, organization and teams with enterprise knowledge graph\n* Connect to any AI model of your choice including OpenAI, Gemini, Claude, or Ollama\n* Use any provider that supports OpenAI compatible endpoints\n* Choose from 1,000+ embedding models\n* Visual Citations for every answer\n* Vision-Language Models and OCR for visual or scanned docs\n* Login with Google, Microsoft, OAuth, or SSO\n* Rich REST APIs for developers\n* All major file types support including pdfs with images, diagrams and charts\n* Agent Builder - Perform actions like Sending mails, Schedule Meetings, etc along with Search, Deep research, Internet search and more\n* Reasoning Agent that plans before executing tasks\n* 40+ Connectors allowing you to connect to your entire business apps\n\nCheck it out and share your thoughts or feedback. Your feedback is immensely valuable and is much appreciated:  \n[https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)\n\nDemo Video:  \n[https://www.youtube.com/watch?v=xA9m3pwOgz8](https://www.youtube.com/watch?v=xA9m3pwOgz8)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qbrx4b/open_source_enterprise_search_engine_generative/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nzfb76t",
          "author": "Green-Ad-3964",
          "text": "wow, this sounds awesome and I'll surely test it. As a local LLM, would you suggest a VL or a plain LLM? Thanks.",
          "score": 2,
          "created_utc": "2026-01-13 21:27:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhkrq0",
              "author": "Effective-Ad2060",
              "text": "VL",
              "score": 2,
              "created_utc": "2026-01-14 05:00:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzmrd48",
          "author": "stealthagents",
          "text": "If you're aiming for better accuracy and context understanding, I‚Äôd lean towards a VL model. They tend to handle specific queries and nuances in data better, especially with all that enterprise info. Plus, the combination with your knowledge graph should really enhance the search quality.",
          "score": 1,
          "created_utc": "2026-01-14 23:16:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzocyl7",
              "author": "Effective-Ad2060",
              "text": "yes.. we already use VL model",
              "score": 1,
              "created_utc": "2026-01-15 04:54:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qd4snz",
      "title": "Building Opensource client sided Code Intelligence Engine -- Potentially deeper than Deep wiki :-) ( Need suggestions and feedback )",
      "subreddit": "ollama",
      "url": "https://v.redd.it/clkvriftsedg1",
      "author": "DeathShot7777",
      "created_utc": "2026-01-15 00:39:37",
      "score": 43,
      "num_comments": 12,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qd4snz/building_opensource_client_sided_code/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzn9gez",
          "author": "Spaceman_Splff",
          "text": "I don‚Äôt even know what‚Äôs happening here but it looks awesome. Good work.",
          "score": 6,
          "created_utc": "2026-01-15 00:54:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nznaawx",
              "author": "DeathShot7777",
              "text": "![gif](giphy|sWBzg2D15WwQjHcxbt)",
              "score": 3,
              "created_utc": "2026-01-15 00:58:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzn91nd",
          "author": "UseHopeful8146",
          "text": "That looks pretty cool actually, any plans to write up docker configs?",
          "score": 2,
          "created_utc": "2026-01-15 00:51:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzn9typ",
              "author": "DeathShot7777",
              "text": "Right now I m trying to integrate ollama, external DB connect ( Neo4j ) and some way to auto track github for updates. If u wanna use it somewhere or got any ideas I would work on creating docker config though",
              "score": 2,
              "created_utc": "2026-01-15 00:56:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzodn5u",
                  "author": "UseHopeful8146",
                  "text": "No pressure, I‚Äôll pm you or something if I put it together - gotta get the food off the plate I already got first lol",
                  "score": 2,
                  "created_utc": "2026-01-15 04:59:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzwx3ho",
          "author": "koldbringer77",
          "text": "This is most beautifull thing ive dreamt to code, the messy mess that works,  I propouse sth like Engram filtering to enchance human readability",
          "score": 2,
          "created_utc": "2026-01-16 13:10:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx1grm",
              "author": "DeathShot7777",
              "text": "Thanks means a lot. Idk what's Engram filter will check it out.\nIt crossed 300 stars I m so happy üò≠",
              "score": 1,
              "created_utc": "2026-01-16 13:35:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzt6le9",
          "author": "RGBrayan",
          "text": "I don't know much about it, but it looks very good. What does it consist of?",
          "score": 1,
          "created_utc": "2026-01-15 22:08:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztp7m7",
              "author": "DeathShot7777",
              "text": "That graph you are seeing is the knowledge graph created by parsing all the relations of your codebase. So this gives very deep insight about the codebases to LLMs as well as let's humans visualize it. \n\nI am trying to resolve the fundamental gap of incomplete context of the codebase in coding agents like cursor , claude code, etc. leading to them making breaking changes",
              "score": 2,
              "created_utc": "2026-01-15 23:43:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o008k4w",
          "author": "BallinwithPaint",
          "text": "This sounds super interesting! I've been looking for something that handles code relations better than standard context tools. I‚Äôm going to give it a spin soon‚Äîif it clicks, I‚Äôd love to dig into the repo and see if I can contribute some of those ideas you mentioned. Just starred it!",
          "score": 1,
          "created_utc": "2026-01-16 22:29:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o008z2x",
              "author": "DeathShot7777",
              "text": "Thanks..feel free to raise issues or DM me or use the github discussions",
              "score": 1,
              "created_utc": "2026-01-16 22:31:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qb4jaq",
      "title": "Chat With Your Favorite GitHub Repositories via CLI with the new RAGLight Feature",
      "subreddit": "ollama",
      "url": "https://v.redd.it/2lu95uwtyycg1",
      "author": "Labess40",
      "created_utc": "2026-01-12 19:27:02",
      "score": 25,
      "num_comments": 3,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qb4jaq/chat_with_your_favorite_github_repositories_via/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzhts0g",
          "author": "Whyme-__-",
          "text": "Broski it‚Äôs admirable to build something of your own. for local setup using ollama it‚Äôs great. But if you have multiple GitHub repos and you are trying to extract algorithms. Then just dump the Md file into Gemini and go to town with it. It‚Äôs 1m context length is unmatched to any opensource model",
          "score": 1,
          "created_utc": "2026-01-14 06:09:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhwmg1",
              "author": "Labess40",
              "text": "You're right, but if you are in an industrial context or you're data are sensible, sometimes you don't want or you can't share your data with a remote LLM provider. And RAGLight is more than a CLI tool. You can use it in your codebase to setup easily a RAG or an Agentic RAG with freedom to modify some pieces of it (data readers, models, providers,...).\nBut I agree, for many usecases, using gemini 1m contexte length is better, but for your private or professional usecases, have an alternative is also useful.",
              "score": 1,
              "created_utc": "2026-01-14 06:33:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzhwpwv",
                  "author": "Whyme-__-",
                  "text": "I don‚Äôt disagree with your points",
                  "score": 2,
                  "created_utc": "2026-01-14 06:33:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qb06tm",
      "title": "Docker on Linux or Nah?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qb06tm/docker_on_linux_or_nah/",
      "author": "Honest-Cheesecake275",
      "created_utc": "2026-01-12 16:53:05",
      "score": 12,
      "num_comments": 21,
      "upvote_ratio": 0.8,
      "text": "My ADHD impulses got the better of me and I jumped the gun and installed Ollama locally. Then installed the Docker container then saw that there is a Docker container that streamlines setup of WebUI. \n\nWhat‚Äôs the most idiot proof way to set this up?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qb06tm/docker_on_linux_or_nah/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nz74q2j",
          "author": "florinandrei",
          "text": "> What‚Äôs the most idiot proof way to set this up?\n\nDepends on the idiot.\n\nOn Linux, this idiot runs Ollama in a container, and Open WebUI in another container, pointing at Ollama. I even have two machines running Ollama, with very different amounts of RAM, and so different models saved; Open WebUI uses both as backends (and runs on a third machine because that's how my home network rolls).\n\nBut you could definitely do it all on one machine with two containers.\n\nI have shell scripts that I run and just pull the latest images and re-launch the containers. Upgrades are no-brainers this way. This is the main reason why I run Ollama in Docker. If I reboot the machine, the containers start again automatically. This is the Ollama update script (the one for Open WebUI is similar):\n\n    docker stop ollama && echo \"stopped the ollama container\" || echo \"could not stop the ollama container\"\n    docker rm ollama && echo \"removed the ollama container\" || echo \"could not remove the ollama container\"\n    docker pull ollama/ollama:latest\n    \n    # the host part is not really needed for Ollama\n    docker run -d --restart always \\\n    \t--gpus=all \\\n    \t-p 11434:11434 \\\n    \t--add-host=host.docker.internal:host-gateway \\\n    \t-v ollama:/root/.ollama \\\n    \t--name ollama \\\n    \tollama/ollama:latest\n    \n    (\n    \tsleep 1\n    \techo; echo\n    \tdocker logs -f ollama &\n    \tDOCKER_LOGS_PID=$!\n    \tsleep 30\n    \tkill $DOCKER_LOGS_PID\n    ) &\n\nYou need to install the NVIDIA Docker compatibility layer, for containers to access the GPU. On Ubuntu, that's a package called `nvidia-container-toolkit`.\n\nYou can even invoke the `ollama` command from the terminal as usual, even when it runs in a container. But you do have to distinguish between interactive and non-interactive sessions, so I have this in `~/.bash_aliases` :\n\n    # shell functions\n    ollama() {\n        if [ -t 0 ]; then\n    \t# this is a terminal, allocate a pseudo-TTY\n            docker exec -it ollama ollama \"$@\"\n        else\n    \t# this is not a terminal, do not allocate a pseudo-TTY\n            docker exec -i ollama ollama \"$@\"\n        fi\n    }\n\nAnd that allows me to invoke the `ollama` command as if it were running on the host.\n\nCheck the status of your containers with `docker ps -a`.",
          "score": 8,
          "created_utc": "2026-01-12 17:39:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz7aawn",
              "author": "Honest-Cheesecake275",
              "text": "Legend",
              "score": 2,
              "created_utc": "2026-01-12 18:04:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz7bqfb",
                  "author": "florinandrei",
                  "text": "Here's the update script for Open WebUI:\n\n    docker stop open-webui\n    docker rm open-webui\n    docker pull ghcr.io/open-webui/open-webui:main\n    \n    docker run -d -p 3000:8080 \\\n    \t--add-host=host.docker.internal:host-gateway \\\n    \t-v open-webui:/app/backend/data \\\n    \t--name open-webui \\\n    \t--restart always \\\n    \tghcr.io/open-webui/open-webui:main\n    \n    docker logs -f open-webui\n\nThis one does not let go of logs at the end, Open WebUI is a bit more finicky that way, so I prefer to wait until it's stable before I let it go. Keep in mind, this is the Open WebUI version without GPU enabled (GPU access is only enabled for Ollama); you may prefer otherwise.\n\n`host.docker.internal` is the host. You will use this in the Open WebUI settings if Ollama runs on the same machine. Otherwise it's not important.\n\nBoth update scripts simply restart the containers if there's nothing to update. They are also \"installer\" scripts. In other words, idiot-proof. I like it that way.",
                  "score": 2,
                  "created_utc": "2026-01-12 18:11:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzhimxj",
              "author": "p_235615",
              "text": "You dont need update scripts... You can either have a docker management like Arcane, where you can update stuff with a click of a button in a webUI, or even better like I use is a docker container called Watchtower. \nYou can set it to either update all the docker containers automatically, or set that only the ones you add the docker label \"com.centurylinklabs.watchtower.enable=true\" are being automatically updated.\nYou can set the timer for it and even send notifications.\n\nAnd I run both ollama + open-webui in a single docker-compose.yaml file, that way its easier to setup the connection between them.",
              "score": 1,
              "created_utc": "2026-01-14 04:45:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzktqam",
                  "author": "florinandrei",
                  "text": "If only there were one single solution to all the world's problems. /s",
                  "score": 1,
                  "created_utc": "2026-01-14 17:56:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz6vwhi",
          "author": "UnbeliebteMeinung",
          "text": "You should always do all stuff inside docker.",
          "score": 5,
          "created_utc": "2026-01-12 16:58:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz70eh3",
          "author": "nicksterling",
          "text": "The most difficult part of using ollama with docker is to ensure the GPU passthrough is working properly otherwise you‚Äôll only be on CPU inferencing.",
          "score": 4,
          "created_utc": "2026-01-12 17:19:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz75rhm",
              "author": "florinandrei",
              "text": "It's just one package that needs to be installed, and using `--gpus=all` when launching the container. It's not hard.",
              "score": 3,
              "created_utc": "2026-01-12 17:44:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz9tuj5",
                  "author": "nicksterling",
                  "text": "The nvidia container toolkit can be a little finicky to get configured on some distros. It‚Äôs better than it used to be at least.",
                  "score": 2,
                  "created_utc": "2026-01-13 01:32:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzc8bs9",
              "author": "ComedianObjective572",
              "text": "Agree docker config took a while hahaha",
              "score": 1,
              "created_utc": "2026-01-13 12:10:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz73lu0",
              "author": "UseHopeful8146",
              "text": "I‚Äôve been meaning to look into this since I did a big hardware upgrade - I could absolutely find it myself with enough time but do you know off top where to get info on configuring ollama containers for gpu pass through?",
              "score": 0,
              "created_utc": "2026-01-12 17:34:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz74q6c",
                  "author": "nicksterling",
                  "text": "The best place is to look at the Dockerhub page for Ollama: https://hub.docker.com/r/ollama/ollama",
                  "score": 3,
                  "created_utc": "2026-01-12 17:39:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz7atfu",
          "author": "Bakkario",
          "text": "I am actually using distrobox- I have both ollama and webui installed inside distrobox instead of docker üòÖ",
          "score": 1,
          "created_utc": "2026-01-12 18:06:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7f8vb",
          "author": "microcandella",
          "text": "most that use docker heavily hate it on windows for what it's worth.",
          "score": 1,
          "created_utc": "2026-01-12 18:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz96d0s",
              "author": "florinandrei",
              "text": "Yeah, Docker Desktop is not really meant to run \"services\". I have the same problem on macOS also, and again it boils down to Docker Desktop being, well, a desktop app. You can make it work, eventually, but it's brittle and it's not elegant.\n\nOn Linux, you [install Docker CE](https://docs.docker.com/engine/install/), and persistent containers behave the way you'd expect a service to behave. Solid, reliable, elegant.\n\nAs long as you run containers in one-off mode, Windows and macOS are fine. I build Docker images and run ad-hoc containers all the time on both macOS and Windows, and both are perfectly fine this way, including GPU support. It's persistent containers that are brittle on these platforms.\n\nOn Windows and macOS I just install the Ollama app, and I don't mess with it very much. I have a dual-boot machine, Linux and Windows, I run Ollama on both disks: on Linux via Docker CE, on Windows via the native app. Both are backends for Open WebUI running in Docker on another machine. Both backends are equally reliable.",
              "score": 1,
              "created_utc": "2026-01-12 23:24:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz72c9p",
          "author": "merguel",
          "text": "I did that with an RTX 3060 12GB, and Ollama leaves the RTX's temperature at 90¬∞C and higher, even when you stop using it. With KoboldCPP or LM Studio, that doesn't happen; they don't go above 73¬∞C. Apparently, they're gentler on the hardware. Ollama pushes it to its limits and leaves it at that temperature.",
          "score": 1,
          "created_utc": "2026-01-12 17:28:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz75idk",
              "author": "florinandrei",
              "text": "Your setup is just broken.",
              "score": 2,
              "created_utc": "2026-01-12 17:43:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz7b0bp",
              "author": "Just-Syllabub-2194",
              "text": "you can fix that issue with following command\n\n**docker update --cpus \"2.0\"  your-ollama-container**\n\njust limit the cpus or gpus usage and the hardware temperature will stay low",
              "score": 1,
              "created_utc": "2026-01-12 18:07:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz86sqf",
                  "author": "florinandrei",
                  "text": "Their problem is bigger than a mere limit. Something is fundamentally wrong with their setup. Ollama should not be doing that.",
                  "score": 1,
                  "created_utc": "2026-01-12 20:33:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qepvki",
      "title": "Do you actually need prompt engineering to get value from AI?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qepvki/do_you_actually_need_prompt_engineering_to_get/",
      "author": "Xthebuilder",
      "created_utc": "2026-01-16 19:36:53",
      "score": 11,
      "num_comments": 41,
      "upvote_ratio": 0.92,
      "text": "\n\nI‚Äôve been using AI daily for about 6 months while building a local AI inferencing app, and one thing that surprised me is how little prompt engineering mattered compared to other factors.\n\nWhat ended up making the biggest difference for me was:\n\n* giving the model enough **context**\n* iterating on ideas *with* the model before writing real code\n* choosing models that are actually good at the specific task\n\nBecause LLMs have some randomness, I found they‚Äôre most useful early on, when you‚Äôre still figuring things out. Iterating with the model helped surface bad assumptions before I committed to an approach. They‚Äôre especially good at starting broad and narrowing down if you keep the conversation going so context builds up.\n\nWhen I add new features now, I don‚Äôt explain my app‚Äôs architecture anymore. I just link the relevant GitHub repos so the model can see how things are structured. That alone cut feature dev time from weeks to about a day in one case.\n\nI‚Äôm not saying prompt engineering is useless, just that for most practical work, context, iteration, and model choice mattered more for me.\n\nCurious how others here approach this. Has prompt engineering been critical for you, or have you seen similar results?\n\n(I wrote up the full experience here if anyone wants more detail: [https://xthebuilder.github.io](https://xthebuilder.github.io))\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qepvki/do_you_actually_need_prompt_engineering_to_get/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nzzcjbw",
          "author": "Dry_Yam_4597",
          "text": "I dont prompt engineer. I ask an llm to write a prompt for another llm when needed. Context is more important.",
          "score": 15,
          "created_utc": "2026-01-16 19:57:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzpr49",
              "author": "Xthebuilder",
              "text": "I agree I also use that method when I want to use an engineered prompt for a workflow I just have another model make it though my language request",
              "score": 3,
              "created_utc": "2026-01-16 20:59:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o004pqy",
                  "author": "Dry_Yam_4597",
                  "text": "Nice. I actually run a couple of P100s just for that. One to analyse images, one to write a prompt, one to refine stuff and is fine tuned, and then i send the final prompt to say comfyui. Pretty neat. That's also how I work around nano banana's lame ai \"logo\". I just recreate images on local.",
                  "score": 2,
                  "created_utc": "2026-01-16 22:10:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzzozca",
          "author": "Purple-Programmer-7",
          "text": "Yes, it‚Äôs critical. And we‚Äôre calling it ‚Äúcontext engineering‚Äù now.",
          "score": 7,
          "created_utc": "2026-01-16 20:55:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzq5fo",
              "author": "Xthebuilder",
              "text": "I like that term better than prompt engineering .",
              "score": 1,
              "created_utc": "2026-01-16 21:01:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzzns21",
          "author": "tom-mart",
          "text": "I'd argue that adding context is prompt engineering.",
          "score": 10,
          "created_utc": "2026-01-16 20:50:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzplac",
              "author": "Xthebuilder",
              "text": "Okay thankyou for the clarification",
              "score": 1,
              "created_utc": "2026-01-16 20:58:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzzt1jz",
          "author": "DeepInEvil",
          "text": "Pretty much this, prompt engineering is bs and for people who are new in the field. Afair r/airealist had an article on that.",
          "score": 5,
          "created_utc": "2026-01-16 21:14:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o004od6",
          "author": "Shoddy-Tutor9563",
          "text": "I can't stop laughing seeing how others are sometimes trying to squeeze out a bit of additional performance from a model in agentic tasks by putting different personas \"hat\" on it. E.g. \"you are super senior 10x engineer ...\"",
          "score": 2,
          "created_utc": "2026-01-16 22:10:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00gev7",
              "author": "Xthebuilder",
              "text": "Lmaooo you are so right I have personally never had much  use of personalities for the LLM workflows I have been using",
              "score": 1,
              "created_utc": "2026-01-16 23:09:55",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o00y705",
              "author": "track0x2",
              "text": "I‚Äôve found that only useful for emulation. ‚ÄúYou are Dr. Seuss.‚Äù",
              "score": 1,
              "created_utc": "2026-01-17 00:51:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o011pfa",
              "author": "NoAdministration6906",
              "text": "I know its funny but it actually works. Response are much better",
              "score": 1,
              "created_utc": "2026-01-17 01:12:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o00ahv3",
          "author": "MrSomethingred",
          "text": "From my understanding of the history of it, \"Prompt Engineering\" was more important in the GPT4 era when you had to say things like \"fix this or I will explode 4000 puppies with Napalm\" and there was nuance in the art of getting them to actually follow instructions. E.g. getting them to actually output in JSON consistently was a nightmare\n\n\nModern models are pretty good at rule following these days so there isn't really any secrets to prompting them \"correctly\" these days\n\n\nAs you highlighted, context management is far more important these days, hence the new buzzword is \"context engineering\"¬†",
          "score": 2,
          "created_utc": "2026-01-16 22:39:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00guz2",
              "author": "Xthebuilder",
              "text": "I like that , context engineering feels more like what I‚Äôm doing and you can relate that to say just having a conversation , regular folk can get to understand that suing AI effectively isn‚Äôt rocket science",
              "score": 1,
              "created_utc": "2026-01-16 23:12:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00g7ji",
          "author": "HeavyDluxe",
          "text": "Prompt engineering is (IMHO) best understood as one \\_facet\\_ of context engineering.  The model makes its predictions based on the contextual information it's delivered.  In early models, carefully crafting a prompt was really the best you could do to ground the model's output.  We have many, many more tools available to us now to set a meaningful foundation for the model to use in generating output.\n\nIf you give the model tons of \\_really good\\_ information in  the codebase, supporting documents, etc etc etc, the user prompt becomes less and less critical.\n\nThe illustration I use at work is to imagine a random stranger comes up at you with a pile of papers, tells you to summarize the data therein for them, and, if you do a good job, you get $1M.  Imagine how you the quality of your product improves if the data has good labels... or if there's a previous report drawing on similar data that's there as an example. Or if the person also tells you what industry they're in.  Or if they tell you to \"imagine you're a customer service manager\" or whatever.\n\nEach little bit of information available improves your output.  The prompt is vital if that's all you give the model.  But context is \\_everything\\_.",
          "score": 2,
          "created_utc": "2026-01-16 23:08:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02rmqo",
              "author": "Xthebuilder",
              "text": "Basically it‚Äôs like data cleaning for your AI output pipeline, it‚Äôs really conceptual but it cuts across a lot of LLM interactions as the base of what‚Äôs controlling the models response",
              "score": 1,
              "created_utc": "2026-01-17 08:54:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00gr19",
          "author": "Additional-Actuary-8",
          "text": "I think the key is to ask what exactly you want rather than design tons of prompts. \n\nAs a human I also want my ai to answer shorter and concise.",
          "score": 2,
          "created_utc": "2026-01-16 23:11:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00h1zm",
              "author": "Xthebuilder",
              "text": "I find myself wanting the models to be more concise across the board too , you‚Äôre correct about being specific about what you want from the model , sounds more like communciation skills",
              "score": 1,
              "created_utc": "2026-01-16 23:13:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00jt2d",
          "author": "Low-Opening25",
          "text": "no, you need context management",
          "score": 2,
          "created_utc": "2026-01-16 23:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00onyo",
              "author": "Xthebuilder",
              "text": "Agreed agreed",
              "score": 1,
              "created_utc": "2026-01-16 23:56:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00zxfy",
          "author": "DaleCooperHS",
          "text": "I dont see the difference. prompt engineering is not writing a prompt, is the process that ends with writing a prompt. Now that process can be done mentally or trought iteration and research . cna be pages long of iteration with an LLm or one 30 lines prompt. Is still prompt engineering.  \nHowever context is not cheap.. so ideally you want to condense all the necessary context and prune out all taht is not.. leaving with the shorter set of instructions and context necessary to do the job, so that the maximum ammount of still avaible context can be used to actually do the job.",
          "score": 2,
          "created_utc": "2026-01-17 01:01:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0120v8",
          "author": "NoAdministration6906",
          "text": "There is also a fine line in giving context, too much context also make the model hallucinations. Be aware of the context window tokens for a model.",
          "score": 2,
          "created_utc": "2026-01-17 01:14:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01mo1i",
              "author": "Xthebuilder",
              "text": "Good point I haven‚Äôt really considered context in token window size too much but maybe that adjustment will lead to further optimization",
              "score": 1,
              "created_utc": "2026-01-17 03:25:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o017umx",
          "author": "cointalkz",
          "text": "Most LLMs, diffusion models or whatever you are using have documentation. Take that documentation into your LLM of choice and train it on how to best write prompts for said model. Profit!",
          "score": 2,
          "created_utc": "2026-01-17 01:52:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01ms94",
              "author": "Xthebuilder",
              "text": "Ooo like full circle training the model on its own developer written documentation",
              "score": 1,
              "created_utc": "2026-01-17 03:26:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o01nxl8",
                  "author": "cointalkz",
                  "text": "Bingo",
                  "score": 1,
                  "created_utc": "2026-01-17 03:34:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o01lqth",
          "author": "fasti-au",
          "text": "It‚Äôs a tipping scale.  \n\n1 million context but your really only pulling lie 16k tokens to process.  So in some cases you get cleaner runs but you need to one shot not correct and now think is breaking things to boiler plating it‚Äôs now more important to back track not correct. \n\nDoes prompting matter.   Not until It does.  \n\nI‚Äôm bad at words out to human but I can cintext a model better than most and build library systems and I tells ya.   Code the tool teacher model to know how to pull and when.  How you get that code is irrelevant in many ways because it‚Äôs all exiting concepts and process with small adjustments so nothing is special beyond the results \n\n\nSo promt engineering is about repetative stability not the day to day in many ways",
          "score": 2,
          "created_utc": "2026-01-17 03:19:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01mjmh",
              "author": "Xthebuilder",
              "text": "I like how you put it , if you can get the same result many times over you can trust the system more overall",
              "score": 1,
              "created_utc": "2026-01-17 03:25:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o02sp52",
          "author": "generate-addict",
          "text": "Nope. And even ‚Äúcontext engineering‚Äù. It changes from model to model, generation to generation. People just gaslight themselves.",
          "score": 2,
          "created_utc": "2026-01-17 09:04:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02stv2",
              "author": "Xthebuilder",
              "text": "I‚Äôm really glad I asked the community because I thought I was tripping seeing all the ‚Äúprompt engineering ‚Äú buzzwords for testing new people about using AI bc I learned though trying to build and use them that fr fr I don‚Äôt need none of that shit üòÇü§£",
              "score": 1,
              "created_utc": "2026-01-17 09:06:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o03ky5r",
          "author": "ZynthCode",
          "text": "promt-crafting is helpful to correct *undesired* behavior.   \n  \n\\*whips LLM on the wrist\\* No comments in code!",
          "score": 2,
          "created_utc": "2026-01-17 13:09:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05suao",
              "author": "Xthebuilder",
              "text": "ü§£ü§£üòÇ good way to put it , it‚Äôs at the edge of the computation , won‚Äôt change too much but can tweak stuff for sure",
              "score": 1,
              "created_utc": "2026-01-17 19:45:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o09yxvd",
          "author": "stocky789",
          "text": "I never bother with prompt engineering \nJust stay specific and concise when communicating with it",
          "score": 1,
          "created_utc": "2026-01-18 11:47:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o01t58f",
          "author": "YouAreTheCornhole",
          "text": "Yes you do, and most people in this thread seem to not know what prompt engineering actually means. Straight up garbage in garbage out",
          "score": 0,
          "created_utc": "2026-01-17 04:09:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02ra7k",
              "author": "Xthebuilder",
              "text": "Good way to add to the discussion brother üëçüèΩ",
              "score": 0,
              "created_utc": "2026-01-17 08:51:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o02shgs",
                  "author": "YouAreTheCornhole",
                  "text": "You literally described your way of prompt engineering, acting like you don't need prompt engineering, because you prompt engineer in a certain way.",
                  "score": 1,
                  "created_utc": "2026-01-17 09:02:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qen6w1",
      "title": "The Preprocessing Gap Between RAG and Agentic",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qen6w1/the_preprocessing_gap_between_rag_and_agentic/",
      "author": "OnyxProyectoUno",
      "created_utc": "2026-01-16 17:59:44",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "RAG is the standard way to connect documents to LLMs. Most people building RAGs know the steps by now: parse documents, chunk them, embed, store vectors, retrieve at query time. But something different happens when you're building systems that act rather than answer.\n\n### The RAG mental model\n\nRAG preprocessing optimizes for retrieval. Someone asks a question, you find relevant chunks, you synthesize an answer. The whole pipeline is designed around that interaction pattern.\n\nThe work happens before anyone asks anything. Documents get parsed into text, extracting content from PDFs, Word docs, HTML, whatever format you're working with. Then chunking splits that text into pieces sized for context windows. You choose a strategy based on your content: split on paragraphs, headings, or fixed token counts. Overlap between chunks preserves context across boundaries. Finally, embedding converts each chunk into a vector where similar meanings cluster together. \"The contract expires in December\" ends up near \"Agreement termination date: 12/31/2024\" even though they share few words. That's what makes semantic search work.\n\nRetrieval is similarity search over those vectors. Query comes in, gets embedded, you find the nearest chunks in vector space. For Q&A, this works well. You ask a question, the system finds relevant passages, an LLM synthesizes an answer. The whole architecture assumes a query-response pattern.\n\nThe requirements shift when you're building systems that act instead of answer.\n\n### What agentic actually needs\n\nConsider a contract monitoring system. It tracks obligations across hundreds of agreements: Example Bank owes a quarterly audit report by the 15th, so the system sends a reminder on the 10th, flags it as overdue on the 16th, and escalates to legal on the 20th. The system doesn't just find text about deadlines. It acts on them.\n\nThat requires something different at the data layer. The system needs to understand that Party A owes Party B deliverable X by date Y under condition Z. And it needs to connect those facts across documents. Not just find text about obligations, but actually know what's owed to whom and when.\n\nThe preprocessing has to pull out that structure, not just preserve text for later search. You're not chunking paragraphs. You're turning \"Example Bank shall submit quarterly compliance reports within 15 days of quarter end\" into data you can query: party, obligation type, deadline, conditions. Think rows in a database, not passages in a search index.\n\n### Two parallel paths\n\nThe architecture ends up looking completely different.\n\nRAG has a linear pipeline. Documents go in, chunking happens, embeddings get created, vectors get stored. At query time, search, retrieve, generate.\n\nAgentic systems need two tracks running in parallel. The main one pulls structured data out of documents. An LLM reads each contract, extracts the obligations, parties, dates, and conditions, and writes them to a graph database. Why a graph? Because you're not just storing isolated facts, you're storing how they connect. Example Bank owes a report. That report is due quarterly. The obligation comes from Section 4.2 of Contract #1847. Those connections between entities are what graph databases are built for. This is what powers the actual monitoring.\n\nBut you still need embeddings. Just for different reasons.\n\nThe second track catches what extraction misses. Sometimes \"the Lender\" in paragraph 12 needs to connect to \"Example Bank\" from paragraph 3. Sometimes you don't know what patterns matter until you see them repeated across documents. The vector search helps you find connections that weren't obvious enough to extract upfront.\n\nSo you end up with two databases working together. The graph database stores entities and their relationships: who owes what to whom by when. The vector database helps you find things you didn't know to look for.\n\nI wrote the rest on my [blog](https://nickrichu.me/posts/the-preprocessing-gap-between-rag-and-agentic).",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qen6w1/the_preprocessing_gap_between_rag_and_agentic/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qbk8l4",
      "title": "Privacy-First Voice Assistant with AI web-enabled search",
      "subreddit": "ollama",
      "url": "https://i.redd.it/gt9geaosa2dg1.jpeg",
      "author": "dsept",
      "created_utc": "2026-01-13 06:37:21",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qbk8l4/privacyfirst_voice_assistant_with_ai_webenabled/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qepzot",
      "title": "Polymcp Integrates Ollama ‚Äì Local and Cloud Execution Made Simple",
      "subreddit": "ollama",
      "url": "https://github.com/poly-mcp/Polymcp",
      "author": "Just_Vugg_PolyMCP",
      "created_utc": "2026-01-16 19:41:16",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qepzot/polymcp_integrates_ollama_local_and_cloud/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qbwnjf",
      "title": "Seeking Advice: Deploying Local LLMs for a Large-Scale Food & Goods Distributor",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qbwnjf/seeking_advice_deploying_local_llms_for_a/",
      "author": "JPedrroo",
      "created_utc": "2026-01-13 17:04:14",
      "score": 6,
      "num_comments": 4,
      "upvote_ratio": 0.76,
      "text": "Hi everyone! I‚Äôm a Software Analyst and Developer for a major distribution company in the state of Bahia, Brazil. We handle a massive operation ranging from food and beverages to cosmetics and hygiene products, serving basically the entire state in terms of city coverage.\n\nI am currently exploring the possibility of implementing a local AI infrastructure to enhance productivity while maintaining strict privacy over our data. I am not an expert in AI, so I am still figuring out the best way to start. I have tested some local LLMs on my laptop, but I am unfamiliar with the technical nuances involved in a large-scale corporate implementation.\n\nInitially, I thought of developing a system that reads database entries regarding expiry dates and turnover rates in our warehouse. The goal would be to automatically recommend flash promotions or stock transfers to our retail branches before products expire.\n\nI'm seeking any feedback on this‚Äîpast experiences, technical advice, additional use case ideas, or anything relevant. Thank you all for your time and for any insights you can share!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qbwnjf/seeking_advice_deploying_local_llms_for_a/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nzmimxb",
          "author": "AdditionalWeb107",
          "text": "I am not sure if I fully understood your use case in terms on when/how would users interact with these agents. Are these long-running tasks or would users chat with the agent too? What's on your checklist in terms of key features you need for the system.",
          "score": 1,
          "created_utc": "2026-01-14 22:31:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzrfqfu",
              "author": "JPedrroo",
              "text": "The model would mostly work on general data analysis, such as suggesting areas for cost reduction, finding patterns and alerting about deviations from those patterns, generating reports...\n\nIt would also be great if users could interact with the model like a chat, since in that sense it would be great for increasing productivity while guaranteeing the privacy of company data.\n\nI'm still developing the idea, I'm in a phase of understanding if it's really viable and where I could apply it here. Honestly, there are so many things that can be done... that's why I want to ask for advice on this, this world of AI in a corporate environment is very new to me.",
              "score": 1,
              "created_utc": "2026-01-15 17:21:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzsh3yl",
                  "author": "AdditionalWeb107",
                  "text": "you are really starting at the very top then - If you want to talk to a local model you have several options. But I would build an instruction prompt, setup ollama, use the openai client to talk to it via v1/chat/completions by updating the base\\_url field and run some workflow scenarios by appending some context and see how far you can get. \n\nThen comes multi-agent orchestration, observability, moderation -- delivery concerns:",
                  "score": 1,
                  "created_utc": "2026-01-15 20:10:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qd7dmx",
      "title": "Hey all- I built a self-hosted MCP server to run AI semantic search over your own databases, files, and codebases. Supports Ollama and cloud providers if you want. Thought you all might find a good use for it.",
      "subreddit": "ollama",
      "url": "/r/selfhosted/comments/1qbv3fm/ragtime_a_selfhosted_mcp_server_to_run_ai/",
      "author": "mattv8",
      "created_utc": "2026-01-15 02:33:30",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qd7dmx/hey_all_i_built_a_selfhosted_mcp_server_to_run_ai/",
      "domain": "",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qfauq9",
      "title": "Help a noob figure out how to achieve something in a game engine with Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qfauq9/help_a_noob_figure_out_how_to_achieve_something/",
      "author": "MountainPlantation",
      "created_utc": "2026-01-17 11:26:10",
      "score": 5,
      "num_comments": 4,
      "upvote_ratio": 0.86,
      "text": "Hey!\n\nI want to use Ollama to integrate it with a game engine. It's already in the engine and working, but I have some questions on what model I should use, and any tips in general for the experiments I want to do. I understand most LLMs running locally will take a while to think and generate a response, but for now let's ignore that.\n\n1. NPC Chat with commands: I know most people have tried doing NPC chatbots in engines, but I was thinking I could try to spice that up by integrating commands on it. Like the LLM would have a list of commands, given by me, that it could use contextually, like /laugh /cry /givePlayer(item), things like that. And I can make a system that parses the string and extracts/executes the commands. I attempted this one time, not in engine, just by using regular chat GPT and it would eventually come up with its own commands that were not stipulated by me. How to avoid that? Is there a model I should use for that?\n2. NPC consistency in character. I also tried one time to keep chat GPT in character, a peasant from the medieval ages, but I would ask about modern events like COVID and it would eventually break and talk about it as if he knew what it was.\n3. NPC Memory. What if I wanted to have NPCs remember things they have witnessed? I imagine I should make a log system that keeps every action done to that npc (NPC was hit by Player. NPC killed bandit. NPC found 1 gold etc) and then adding it to the beggining of the prompt as a little memory. Is that enough?\n4. Can I reliably limit the response length or is it finicky? Like, setting a limit of how many words per response \n5. Is there a way to guarantee responses are always in character? Because sometimes some of the LLMs will say \"I cannot answer to things related to that\" and that would be a big immersion breaker \n\nAnd another general question, is there a way to train certain models to get them used to a certani context? like i said, using commands I create in my game, or training them to act like a specific type of character etc.\n\nAgain, other than my experiments with just the chat GPT window, I am pretty new to this. If you have advice on what models to use or best practices, I'm listening.\n\nThank you!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qfauq9/help_a_noob_figure_out_how_to_achieve_something/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o040jq8",
          "author": "CooperDK",
          "text": "Just a comment, don't use ollama. Provide koboldcpp in a variant specific to the individual user's hardware. Ollama is too slow. In most cases, kobold is much faster.",
          "score": 1,
          "created_utc": "2026-01-17 14:39:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05cp8s",
              "author": "MountainPlantation",
              "text": "Can it do those things you mentioned?\n\n\nUpon testing something like deepseek takes roughly 16 seconds to respond¬†\n\n\nSomething like tinyllama which is supposed to be fast takes 3 which is still long for most applications\n\n\nIs koboldcpp JUST for character? Because I wanted to make it do other things like game direction and level generation¬†",
              "score": 1,
              "created_utc": "2026-01-17 18:29:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o09219a",
                  "author": "alt-weeb",
                  "text": "respectfully nobody's gonna play your fucking game if the entire thing is ai generated, hoping that's not your intention bc if so you're wasting your time",
                  "score": 1,
                  "created_utc": "2026-01-18 06:48:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qeatiw",
      "title": "New version of Raspberry Pie Generative AI card (HAT+ 2)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qeatiw/new_version_of_raspberry_pie_generative_ai_card/",
      "author": "Unique_Winner_5927",
      "created_utc": "2026-01-16 08:43:42",
      "score": 4,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "Perfect for private assistants, industrial equipment,¬†proof of concept, ...\n\n[https://www.raspberrypi.com/news/introducing-the-raspberry-pi-ai-hat-plus-2-generative-ai-on-raspberry-pi-5/](https://www.raspberrypi.com/news/introducing-the-raspberry-pi-ai-hat-plus-2-generative-ai-on-raspberry-pi-5/)\n\n\\#RaspberryPi #DataSovereignty #EmbeddedAI",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qeatiw/new_version_of_raspberry_pie_generative_ai_card/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nzw85q6",
          "author": "-Akos-",
          "text": "Jeffrey Geerling was less impressed: [https://youtu.be/jRQaur0LdLE](https://youtu.be/jRQaur0LdLE)\n\nBesides, all models I‚Äôve seen run on this board were 1.5B parameters or less, and older models at that.",
          "score": 1,
          "created_utc": "2026-01-16 10:01:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwoo6q",
              "author": "Comfortable_Ad_8117",
              "text": "Was that the original board or the +2 -The new +2 has 8gb of onboard ram and 2x speed",
              "score": 1,
              "created_utc": "2026-01-16 12:16:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzwu1yx",
                  "author": "-Akos-",
                  "text": "It's all in the video. In the end, you're better off buying a second Pi5 than this thing.",
                  "score": 2,
                  "created_utc": "2026-01-16 12:52:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzwp5t7",
                  "author": "danishkirel",
                  "text": "Still slower than the pi 5 cpu and there are models with more system memory too.",
                  "score": 1,
                  "created_utc": "2026-01-16 12:19:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzy284k",
          "author": "Unique_Winner_5927",
          "text": "Or we can directly install Ollama on Pi 5 : [https://monraspberry.com/installer-ollama-raspberry-pi/](https://monraspberry.com/installer-ollama-raspberry-pi/)",
          "score": 1,
          "created_utc": "2026-01-16 16:30:44",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o069278",
          "author": "tecneeq",
          "text": "The problem is the slow access to ram. You can fit up to 9b models i think, but they are all dense models, so everything is extremely slow.\n\nHowever, it has no load on the CPU, so that is great. It even works with a 1GB PI5.",
          "score": 1,
          "created_utc": "2026-01-17 21:07:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcsn8m",
      "title": "We tried to automate product labeling in one prompt. It failed. 27 steps later, we've processed 10,000+ products.",
      "subreddit": "ollama",
      "url": "/r/LocalLLaMA/comments/1qcsmww/we_tried_to_automate_product_labeling_in_one/",
      "author": "No-Reindeer-9968",
      "created_utc": "2026-01-14 16:58:42",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.83,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qcsn8m/we_tried_to_automate_product_labeling_in_one/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "nzonye7",
          "author": "tom-mart",
          "text": "Sounds like a skill issue.",
          "score": 1,
          "created_utc": "2026-01-15 06:18:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qabwuw",
      "title": "Docker ollama running on windows using system RAM, despite using VRAM and having plenty more available.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qabwuw/docker_ollama_running_on_windows_using_system_ram/",
      "author": "Fit_Code_2107",
      "created_utc": "2026-01-11 21:39:05",
      "score": 3,
      "num_comments": 19,
      "upvote_ratio": 0.8,
      "text": "**Edit: Found the solution to this problem (tldr; WSL2 sucks), updated the post with the solution at the bottom of the post**\n\nHey everyone,\n\nI'm trying to run ollama on docker (windows), and it looks like there's some memory double dipping going on and I'm not sure why. I'm trying to run a 20GB model on a 5090, I'm seeing BOTH my system and VRAM memory go up as much when I load the model.\n\nSystem settings:\n\n* 64 GB of RAM\n* RTX 5090 (32 GB of VRAM)\n* Model:  olmo-3.1:32b-think (takes \\~20Gb of RAM to load)\n* Docker version 29.1.3, build f52814d (running on WSL2)\n\nfwiw, `ollama ps` does show the model loaded 100% on my GPU. Ran `nvidia-smi` in the ollama container, and it looks *fine* (I can see the ollama process running). While Windows task manager isn't able to pin down what process is responsible for the high gpu util, it does reflect memory utilization accurately. So I am using my GPU, I have plenty more VRAM to work with, so I'm not at all sure why system memory util spikes up 20GB during use.\n\nI installed the windows native version of ollama to see if I could replicate, and I do not see my system memory spike using that approach. So it seems like the involvement of docker here is introducing some funk.\n\nI've read through some similar posts here and saw there were issues a few years ago with docker on WSL2 and utilizing VRAM, but those issues seem to have since been resolved so hitting a dead end here. Wondering if anyone has had the same issue and has any tips?\n\nThanks\n\n# Solution/workaround\n\nFound the cause of the issue, it's a known [WSL issue](https://github.com/microsoft/WSL/issues/4166) (that's been open for 7 years...). Apparently WSL doesn't do a great job with memory management and sometimes never releases the memory used back to the system (I say sometimes but for me it's ALL the time). In this case with ollama, never releases the RAM it uses to load the model to VRAM.\n\nYou can manually confirm this, and release the memory with the following commands (s/o to Kirk, a random commenter on this [medium article I found](https://medium.com/@Tanzim/how-to-run-ollama-in-windows-via-wsl-8ace765cee12))\n\n    # Display memory usage\n    free -h\n    \n    sudo su <<EOF\n    # force memory write back to disk\n    sync\n    \n    # clear cache from memory\n    echo 3 > /proc/sys/vm/drop_caches\n    \n    EOF\n    # display memory again (should be cleared)\n\nAlso, looks like someone has a more automated solution out here too (haven't tried this so can't vouch but looks promising) : [https://github.com/arkane-systems/wsl-drop-cache?tab=readme-ov-file](https://github.com/arkane-systems/wsl-drop-cache?tab=readme-ov-file)\n\nNote: I am only sharing what I found.\n\nWhile these are solutions, they may not be the best or most appropriate solution if you're doing anything *real*. atm I'm just experimenting with this setup for personal reasons so it works for me.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qabwuw/docker_ollama_running_on_windows_using_system_ram/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nz1xv6v",
          "author": "cmdwedge75",
          "text": "Are you exposing /dev/dri via Docker?",
          "score": 1,
          "created_utc": "2026-01-11 22:21:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz47id6",
              "author": "Fit_Code_2107",
              "text": "as far as ik, exposing /dev/dri is how you expose your GPU to docker right? That part seems to be working fine, my GPU is definitely in use and the model is loaded up just fine.\n\nThe issue I'm trying to get to the bottom of is the fact that my system memory util also goes up by 20Gb. It's like my system is loading the model on both RAM and VRAM.",
              "score": 1,
              "created_utc": "2026-01-12 05:53:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz1xnwl",
          "author": "StardockEngineer",
          "text": "Why are you running ollama in a container at all?",
          "score": 1,
          "created_utc": "2026-01-11 22:20:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz20dh1",
              "author": "danny_094",
              "text": "Why not? Isolated environments are very practical.",
              "score": 6,
              "created_utc": "2026-01-11 22:33:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz20qwh",
                  "author": "StardockEngineer",
                  "text": "It‚Äôs a poor solution.  Just use a better inference server at this point or one of Docker‚Äôs own pods.  I mean, if you‚Äôre going to this level of effort, might as well do it right.\n\nEdit: Folks, its OK to like Ollama.  But be real about its use case.",
                  "score": -3,
                  "created_utc": "2026-01-11 22:35:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o00z3qr",
              "author": "Due-Rooster3471",
              "text": "Hi there, Im new to LLM's. Just download ollama last night. What is a container so O dont use it?",
              "score": 1,
              "created_utc": "2026-01-17 00:56:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz21b37",
          "author": "danny_094",
          "text": "WS12 is to blame. The Linux kernel. Everything ollama loads ends up in Linux userspace and is stored in RAM in Windows.",
          "score": 0,
          "created_utc": "2026-01-11 22:38:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4c0xv",
              "author": "KneelB4S8n",
              "text": "Please, elaborate.",
              "score": 1,
              "created_utc": "2026-01-12 06:30:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz5zpur",
                  "author": "danny_094",
                  "text": "es l√§uft √ºber Hyper-v und ist im grunde eine Virtuelle Maschine. Darum.",
                  "score": 0,
                  "created_utc": "2026-01-12 14:24:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qb46gv",
      "title": "How to Evaluate AI Agents? (Part 2)",
      "subreddit": "ollama",
      "url": "/r/AIEval/comments/1qb43wg/how_to_evaluate_ai_agents_part_2/",
      "author": "Ok_Constant_9886",
      "created_utc": "2026-01-12 19:14:13",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qb46gv/how_to_evaluate_ai_agents_part_2/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "nzqi3pk",
          "author": "stealthagents",
          "text": "It really helps to set clear metrics for performance before diving in, like how well they handle specific tasks or adapt to changes. Also, running some A/B tests can reveal a lot about their decision-making in real-world scenarios.",
          "score": 1,
          "created_utc": "2026-01-15 14:47:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qef38r",
      "title": "Prompt tool I built/use with Ollama daily - render prompt variations without worrying about text files",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qef38r/prompt_tool_i_builtuse_with_ollama_daily_render/",
      "author": "springwasser",
      "created_utc": "2026-01-16 12:49:45",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.8,
      "text": "I posted this to another subreddit, but should have posted it here.. sorry if you've seen it.\n\nThis is a tool I built because I use it in local development. I know there are solutions for these things mixed into other software, but this is standalone and does just one thing really well for me.\n\n\\- create/version/store prompts.. don't have to worry about text files unless I want to  \n\\- runs from command line, can pipe stdout into anything.. eg Ollama, ci, git hooks  \n\\- easily render variations of prompts on the fly, inject {{variables}} or inject files.. e.g. git diffs or documents  \n\\- can store prompts globally or in projects, run anywhere\n\nBasic usage:\n\n    # Create a prompt.. paste in text\n    $ promptg prompt new my-prompt \n    \n    # -or-\n    $ echo \"Create a prompt with pipe\" | promptg prompt save hello\n    \n    # Then.. \n    $ promptg get my-prompt | ollama run deepseek-r1\n\nOr more advanced, render with dynamic variables and insert files..\n\n    # before..\n    cat prompt.txt | sed \"s/{{lang}}/Python/g; s/{{code}}/$(cat myfile.py)/g\" | ollama run mistral\n    \n    # now, replace dynamic {{templateValue}} and insert code/file.\n    promptg get code-review --var lang=Python --var code@myfile.py | ollama run mistral\n\nInstall:\n\n    npm install -g @promptg/cli",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qef38r/prompt_tool_i_builtuse_with_ollama_daily_render/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nzwtsz5",
          "author": "springwasser",
          "text": "Should have posted the Gitub:\n\ngithub dot com/promptg/cli",
          "score": 1,
          "created_utc": "2026-01-16 12:50:33",
          "is_submitter": true,
          "replies": []
        }
      ]
    }
  ]
}