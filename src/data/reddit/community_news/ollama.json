{
  "metadata": {
    "last_updated": "2026-01-29 17:09:57",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 20,
    "total_comments": 139,
    "file_size_bytes": 178397
  },
  "items": [
    {
      "id": "1qm9cgp",
      "title": "Ollama Models Ranked by VRAM Requirements",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qm9cgp/ollama_models_ranked_by_vram_requirements/",
      "author": "AdventurousLion9548",
      "created_utc": "2026-01-25 04:36:31",
      "score": 497,
      "num_comments": 40,
      "upvote_ratio": 0.97,
      "text": "1250.08 GB  |  cogito-2.1:latest\n\n1250.08 GB  |  cogito-2.1:671b\n\n 376.71 GB  |  deepseek-v3.1:latest\n\n 376.71 GB  |  deepseek-v3.1:671b\n\n 376.65 GB  |  deepseek-r1:671b\n\n 376.65 GB  |  deepseek-v3:latest\n\n 376.65 GB  |  deepseek-v3:671b\n\n 376.65 GB  |  r1-1776:671b\n\n 270.14 GB  |  qwen3-coder:480b\n\n 226.38 GB  |  llama3.1:405b\n\n 213.14 GB  |  hermes3:405b\n\n 133.43 GB  |  qwen3-vl:235b\n\n 132.39 GB  |  qwen3:235b\n\n 123.78 GB  |  deepseek-coder-v2:236b\n\n 123.78 GB  |  deepseek-v2:236b\n\n 123.78 GB  |  deepseek-v2.5:latest\n\n 123.78 GB  |  deepseek-v2.5:236b\n\n  94.51 GB  |  falcon:180b\n\n  74.05 GB  |  zephyr:141b\n\n  69.75 GB  |  devstral-2:latest\n\n  69.75 GB  |  devstral-2:123b\n\n   69.1 GB  |  dbrx:latest\n\n   69.1 GB  |  dbrx:132b\n\n  68.19 GB  |  mistral-large:latest\n\n  68.19 GB  |  mistral-large:123b\n\n   63.1 GB  |  megadolphin:latest\n\n   63.1 GB  |  megadolphin:120b\n\n  62.81 GB  |  llama4:latest\n\n  62.52 GB  |  command-a:latest\n\n  62.52 GB  |  command-a:111b\n\n  60.88 GB  |  gpt-oss:120b\n\n  60.88 GB  |  gpt-oss-safeguard:120b\n\n  58.57 GB  |  qwen:110b\n\n  55.15 GB  |  command-r-plus:latest\n\n  55.15 GB  |  command-r-plus:104b\n\n  50.87 GB  |  llama3.2-vision:90b\n\n  46.89 GB  |  qwen3-next:latest\n\n  46.89 GB  |  qwen3-next:80b\n\n  45.36 GB  |  qwen2.5vl:72b\n\n  44.16 GB  |  athene-v2:latest\n\n  44.16 GB  |  athene-v2:72b\n\n  44.16 GB  |  qwen2.5:72b\n\n   39.6 GB  |  cogito:70b\n\n   39.6 GB  |  deepseek-r1:70b\n\n   39.6 GB  |  llama3.1:70b\n\n   39.6 GB  |  llama3.3:latest\n\n   39.6 GB  |  llama3.3:70b\n\n   39.6 GB  |  nemotron:latest\n\n   39.6 GB  |  nemotron:70b\n\n   39.6 GB  |  r1-1776:latest\n\n   39.6 GB  |  r1-1776:70b\n\n   39.6 GB  |  tulu3:70b\n\n   38.4 GB  |  qwen2:72b\n\n   38.4 GB  |  qwen2-math:72b\n\n  38.18 GB  |  qwen:72b\n\n  37.22 GB  |  dolphin-llama3:70b\n\n  37.22 GB  |  firefunction-v2:latest\n\n  37.22 GB  |  firefunction-v2:70b\n\n  37.22 GB  |  hermes3:70b\n\n  37.22 GB  |  llama3:70b\n\n  37.22 GB  |  llama3-chatqa:70b\n\n  37.22 GB  |  llama3-gradient:70b\n\n  37.22 GB  |  llama3-groq-tool-use:70b\n\n  37.22 GB  |  reflection:latest\n\n  37.22 GB  |  reflection:70b\n\n   36.2 GB  |  codellama:70b\n\n   36.2 GB  |  llama2:70b\n\n   36.2 GB  |  llama2-uncensored:70b\n\n   36.2 GB  |  meditron:70b\n\n   36.2 GB  |  orca-mini:70b\n\n   36.2 GB  |  stable-beluga:70b\n\n   36.2 GB  |  wizard-math:70b\n\n  35.53 GB  |  deepseek-llm:67b\n\n  24.63 GB  |  dolphin-mixtral:latest\n\n  24.63 GB  |  mixtral:latest\n\n  24.63 GB  |  notux:latest\n\n  24.63 GB  |  nous-hermes2-mixtral:latest\n\n   22.6 GB  |  nemotron-3-nano:latest\n\n   22.6 GB  |  nemotron-3-nano:30b\n\n  22.17 GB  |  alfred:latest\n\n  22.17 GB  |  alfred:40b\n\n  22.17 GB  |  falcon:40b\n\n  19.71 GB  |  qwen2.5vl:32b\n\n  19.47 GB  |  qwen3-vl:32b\n\n  18.84 GB  |  aya:35b\n\n  18.81 GB  |  qwen3:32b\n\n  18.78 GB  |  llava:34b\n\n  18.49 GB  |  cogito:32b\n\n  18.49 GB  |  deepseek-r1:32b\n\n  18.49 GB  |  openthinker:32b\n\n  18.49 GB  |  qwen2.5:32b\n\n  18.49 GB  |  qwen2.5-coder:32b\n\n  18.49 GB  |  qwq:latest\n\n  18.49 GB  |  qwq:32b\n\n  18.44 GB  |  aya-expanse:32b\n\n  18.25 GB  |  qwen3-vl:30b\n\n  18.14 GB  |  olmo-3:32b\n\n  18.14 GB  |  olmo-3.1:latest\n\n  18.14 GB  |  olmo-3.1:32b\n\n  18.13 GB  |  nous-hermes2:34b\n\n  18.13 GB  |  yi:34b\n\n  18.02 GB  |  exaone-deep:32b\n\n  18.02 GB  |  exaone3.5:32b\n\n  17.92 GB  |  granite-code:34b\n\n  17.74 GB  |  codebooga:latest\n\n  17.74 GB  |  codebooga:34b\n\n  17.74 GB  |  codellama:34b\n\n  17.74 GB  |  phind-codellama:latest\n\n  17.74 GB  |  phind-codellama:34b\n\n  17.53 GB  |  deepseek-coder:33b\n\n  17.53 GB  |  wizardcoder:33b\n\n  17.43 GB  |  command-r:latest\n\n  17.43 GB  |  command-r:35b\n\n  17.28 GB  |  qwen3:30b\n\n  17.28 GB  |  qwen3-coder:latest\n\n  17.28 GB  |  qwen3-coder:30b\n\n  17.23 GB  |  qwen:32b\n\n   17.1 GB  |  vicuna:33b\n\n   17.1 GB  |  wizard-vicuna-uncensored:30b\n\n   16.2 GB  |  gemma3:27b\n\n  16.17 GB  |  translategemma:27b\n\n   15.5 GB  |  shieldgemma:27b\n\n  14.56 GB  |  gemma2:27b\n\n  14.42 GB  |  mistral-small3.1:latest\n\n  14.42 GB  |  mistral-small3.1:24b\n\n  14.14 GB  |  devstral-small-2:latest\n\n  14.14 GB  |  devstral-small-2:24b\n\n  14.14 GB  |  mistral-small3.2:latest\n\n  14.14 GB  |  mistral-small3.2:24b\n\n  13.35 GB  |  devstral:latest\n\n  13.35 GB  |  devstral:24b\n\n  13.35 GB  |  magistral:latest\n\n  13.35 GB  |  magistral:24b\n\n  13.35 GB  |  mistral-small:latest\n\n  13.35 GB  |  mistral-small:24b\n\n  12.85 GB  |  gpt-oss:latest\n\n  12.85 GB  |  gpt-oss:20b\n\n  12.85 GB  |  gpt-oss-safeguard:latest\n\n  12.85 GB  |  gpt-oss-safeguard:20b\n\n   12.4 GB  |  solar-pro:latest\n\n   12.4 GB  |  solar-pro:22b\n\n  11.71 GB  |  codestral:latest\n\n  11.71 GB  |  codestral:22b\n\n  11.71 GB  |  mistral-small:22b\n\n  10.82 GB  |  sailor2:20b\n\n  10.76 GB  |  granite-code:20b\n\n  10.55 GB  |  internlm2:20b\n\n  10.35 GB  |  phi4-reasoning:latest\n\n  10.35 GB  |  phi4-reasoning:14b\n\n   8.64 GB  |  qwen3:14b\n\n   8.46 GB  |  ministral-3:14b\n\n   8.44 GB  |  dolphincoder:15b\n\n   8.44 GB  |  starcoder2:15b\n\n   8.43 GB  |  phi4:latest\n\n   8.43 GB  |  phi4:14b\n\n   8.37 GB  |  cogito:14b\n\n   8.37 GB  |  deepcoder:latest\n\n   8.37 GB  |  deepcoder:14b\n\n   8.37 GB  |  deepseek-r1:14b\n\n   8.37 GB  |  qwen2.5:14b\n\n   8.37 GB  |  qwen2.5-coder:14b\n\n   8.37 GB  |  sqlcoder:15b\n\n   8.37 GB  |  starcoder:15b\n\n   8.29 GB  |  deepseek-coder-v2:latest\n\n   8.29 GB  |  deepseek-coder-v2:16b\n\n   8.29 GB  |  deepseek-v2:latest\n\n   8.29 GB  |  deepseek-v2:16b\n\n   7.78 GB  |  olmo2:13b\n\n   7.62 GB  |  qwen:14b\n\n   7.59 GB  |  gemma3:12b\n\n   7.55 GB  |  translategemma:12b\n\n   7.46 GB  |  llava:13b\n\n   7.35 GB  |  phi3:14b\n\n   7.28 GB  |  llama3.2-vision:latest\n\n   7.28 GB  |  llama3.2-vision:11b\n\n   7.03 GB  |  gemma3n:latest\n\n   6.86 GB  |  codellama:13b\n\n   6.86 GB  |  codeup:latest\n\n   6.86 GB  |  codeup:13b\n\n   6.86 GB  |  everythinglm:latest\n\n   6.86 GB  |  everythinglm:13b\n\n   6.86 GB  |  llama2:13b\n\n   6.86 GB  |  llama2-chinese:13b\n\n   6.86 GB  |  nexusraven:latest\n\n   6.86 GB  |  nexusraven:13b\n\n   6.86 GB  |  nous-hermes:13b\n\n   6.86 GB  |  open-orca-platypus2:latest\n\n   6.86 GB  |  open-orca-platypus2:13b\n\n   6.86 GB  |  orca-mini:13b\n\n   6.86 GB  |  orca2:13b\n\n   6.86 GB  |  stable-beluga:13b\n\n   6.86 GB  |  vicuna:13b\n\n   6.86 GB  |  wizard-math:13b\n\n   6.86 GB  |  wizard-vicuna:latest\n\n   6.86 GB  |  wizard-vicuna:13b\n\n   6.86 GB  |  wizard-vicuna-uncensored:13b\n\n   6.86 GB  |  wizardlm-uncensored:latest\n\n   6.86 GB  |  wizardlm-uncensored:13b\n\n   6.86 GB  |  xwinlm:13b\n\n   6.86 GB  |  yarn-llama2:13b\n\n   6.59 GB  |  mistral-nemo:latest\n\n   6.59 GB  |  mistral-nemo:12b\n\n   6.49 GB  |  stablelm2:12b\n\n   6.23 GB  |  deepseek-ocr:latest\n\n   6.23 GB  |  deepseek-ocr:3b\n\n   5.94 GB  |  falcon2:latest\n\n   5.94 GB  |  falcon2:11b\n\n   5.86 GB  |  falcon3:10b\n\n   5.72 GB  |  qwen3-vl:latest\n\n   5.72 GB  |  qwen3-vl:8b\n\n   5.66 GB  |  nous-hermes2:latest\n\n   5.66 GB  |  nous-hermes2:10.7b\n\n   5.66 GB  |  solar:latest\n\n   5.66 GB  |  solar:10.7b\n\n   5.61 GB  |  ministral-3:latest\n\n   5.61 GB  |  ministral-3:8b\n\n   5.56 GB  |  qwen2.5vl:latest\n\n   5.56 GB  |  qwen2.5vl:7b\n\n5.4 GB  |  granite3-guardian:8b\n\n   5.37 GB  |  shieldgemma:latest\n\n   5.37 GB  |  shieldgemma:9b\n\n   5.16 GB  |  llava-llama3:latest\n\n   5.16 GB  |  llava-llama3:8b\n\n5.1 GB  |  minicpm-v:latest\n\n5.1 GB  |  minicpm-v:8b\n\n   5.08 GB  |  codegeex4:latest\n\n   5.08 GB  |  codegeex4:9b\n\n   5.08 GB  |  glm4:latest\n\n   5.08 GB  |  glm4:9b\n\n   5.07 GB  |  gemma2:latest\n\n   5.07 GB  |  gemma2:9b\n\n   4.88 GB  |  sailor2:latest\n\n   4.88 GB  |  sailor2:8b\n\n   4.87 GB  |  deepseek-r1:latest\n\n   4.87 GB  |  deepseek-r1:8b\n\n   4.87 GB  |  qwen3:latest\n\n   4.87 GB  |  qwen3:8b\n\n   4.76 GB  |  rnj-1:latest\n\n   4.76 GB  |  rnj-1:8b\n\n   4.71 GB  |  aya-expanse:latest\n\n   4.71 GB  |  aya-expanse:8b\n\n   4.71 GB  |  command-r7b:latest\n\n   4.71 GB  |  command-r7b:7b\n\n   4.71 GB  |  command-r7b-arabic:latest\n\n   4.71 GB  |  command-r7b-arabic:7b\n\n   4.69 GB  |  yi:9b\n\n   4.69 GB  |  yi-coder:latest\n\n   4.69 GB  |  yi-coder:9b\n\n   4.67 GB  |  codegemma:latest\n\n   4.67 GB  |  codegemma:7b\n\n   4.67 GB  |  gemma:latest\n\n   4.67 GB  |  gemma:7b\n\n   4.65 GB  |  granite3.1-dense:latest\n\n   4.65 GB  |  granite3.1-dense:8b\n\n4.6 GB  |  granite3-dense:8b\n\n4.6 GB  |  granite3.2:latest\n\n4.6 GB  |  granite3.2:8b\n\n4.6 GB  |  granite3.3:latest\n\n4.6 GB  |  granite3.3:8b\n\n   4.58 GB  |  cogito:latest\n\n   4.58 GB  |  cogito:8b\n\n   4.58 GB  |  dolphin3:latest\n\n   4.58 GB  |  dolphin3:8b\n\n   4.58 GB  |  llama-guard3:latest\n\n   4.58 GB  |  llama-guard3:8b\n\n   4.58 GB  |  llama3.1:latest\n\n   4.58 GB  |  llama3.1:8b\n\n   4.58 GB  |  tulu3:latest\n\n   4.58 GB  |  tulu3:8b\n\n   4.47 GB  |  aya:latest\n\n   4.47 GB  |  aya:8b\n\n   4.44 GB  |  exaone-deep:latest\n\n   4.44 GB  |  exaone-deep:7.8b\n\n   4.44 GB  |  exaone3.5:latest\n\n   4.44 GB  |  exaone3.5:7.8b\n\n   4.41 GB  |  bakllava:latest\n\n   4.41 GB  |  bakllava:7b\n\n   4.41 GB  |  llama-pro:latest\n\n   4.41 GB  |  llava:latest\n\n   4.41 GB  |  llava:7b\n\n   4.41 GB  |  opencoder:latest\n\n   4.41 GB  |  opencoder:8b\n\n   4.39 GB  |  bespoke-minicheck:latest\n\n   4.39 GB  |  bespoke-minicheck:7b\n\n   4.36 GB  |  deepseek-r1:7b\n\n   4.36 GB  |  marco-o1:latest\n\n   4.36 GB  |  marco-o1:7b\n\n   4.36 GB  |  openthinker:latest\n\n   4.36 GB  |  openthinker:7b\n\n   4.36 GB  |  qwen2.5:latest\n\n   4.36 GB  |  qwen2.5:7b\n\n   4.36 GB  |  qwen2.5-coder:latest\n\n   4.36 GB  |  qwen2.5-coder:7b\n\n   4.36 GB  |  qwen3-embedding:latest\n\n   4.36 GB  |  qwen3-embedding:8b\n\n   4.34 GB  |  dolphin-llama3:latest\n\n   4.34 GB  |  dolphin-llama3:8b\n\n   4.34 GB  |  hermes3:latest\n\n   4.34 GB  |  hermes3:8b\n\n   4.34 GB  |  llama3:latest\n\n   4.34 GB  |  llama3:8b\n\n   4.34 GB  |  llama3-chatqa:latest\n\n   4.34 GB  |  llama3-chatqa:8b\n\n   4.34 GB  |  llama3-gradient:latest\n\n   4.34 GB  |  llama3-gradient:8b\n\n   4.34 GB  |  llama3-groq-tool-use:latest\n\n   4.34 GB  |  llama3-groq-tool-use:8b\n\n   4.28 GB  |  granite-code:8b\n\n   4.26 GB  |  falcon3:latest\n\n   4.26 GB  |  falcon3:7b\n\n4.2 GB  |  qwen:7b\n\n   4.16 GB  |  olmo-3:latest\n\n   4.16 GB  |  olmo-3:7b\n\n   4.16 GB  |  olmo2:latest\n\n   4.16 GB  |  olmo2:7b\n\n   4.15 GB  |  internlm2:latest\n\n   4.15 GB  |  internlm2:7b\n\n   4.13 GB  |  qwen2:latest\n\n   4.13 GB  |  qwen2:7b\n\n   4.13 GB  |  qwen2-math:latest\n\n   4.13 GB  |  qwen2-math:7b\n\n   4.07 GB  |  mistral:latest\n\n   4.07 GB  |  mistral:7b\n\n4.0 GB  |  starcoder:7b\n\n   3.94 GB  |  dolphincoder:latest\n\n   3.94 GB  |  dolphincoder:7b\n\n   3.92 GB  |  falcon:latest\n\n   3.92 GB  |  falcon:7b\n\n   3.89 GB  |  codeqwen:latest\n\n   3.89 GB  |  codeqwen:7b\n\n   3.83 GB  |  dolphin-mistral:latest\n\n   3.83 GB  |  dolphin-mistral:7b\n\n   3.83 GB  |  mathstral:latest\n\n   3.83 GB  |  mathstral:7b\n\n   3.83 GB  |  mistral-openorca:latest\n\n   3.83 GB  |  mistral-openorca:7b\n\n   3.83 GB  |  mistrallite:latest\n\n   3.83 GB  |  mistrallite:7b\n\n   3.83 GB  |  neural-chat:latest\n\n   3.83 GB  |  neural-chat:7b\n\n   3.83 GB  |  notus:latest\n\n   3.83 GB  |  notus:7b\n\n   3.83 GB  |  openchat:latest\n\n   3.83 GB  |  openchat:7b\n\n   3.83 GB  |  openhermes:latest\n\n   3.83 GB  |  samantha-mistral:latest\n\n   3.83 GB  |  samantha-mistral:7b\n\n   3.83 GB  |  sqlcoder:latest\n\n   3.83 GB  |  sqlcoder:7b\n\n   3.83 GB  |  starling-lm:latest\n\n   3.83 GB  |  starling-lm:7b\n\n   3.83 GB  |  wizard-math:latest\n\n   3.83 GB  |  wizard-math:7b\n\n   3.83 GB  |  wizardlm2:latest\n\n   3.83 GB  |  wizardlm2:7b\n\n   3.83 GB  |  yarn-mistral:latest\n\n   3.83 GB  |  yarn-mistral:7b\n\n   3.83 GB  |  zephyr:latest\n\n   3.83 GB  |  zephyr:7b\n\n   3.77 GB  |  starcoder2:7b\n\n   3.73 GB  |  deepseek-llm:latest\n\n   3.73 GB  |  deepseek-llm:7b\n\n   3.56 GB  |  codellama:latest\n\n   3.56 GB  |  codellama:7b\n\n   3.56 GB  |  deepseek-coder:6.7b\n\n   3.56 GB  |  duckdb-nsql:latest\n\n   3.56 GB  |  duckdb-nsql:7b\n\n   3.56 GB  |  llama2:latest\n\n   3.56 GB  |  llama2:7b\n\n   3.56 GB  |  llama2-chinese:latest\n\n   3.56 GB  |  llama2-chinese:7b\n\n   3.56 GB  |  llama2-uncensored:latest\n\n   3.56 GB  |  llama2-uncensored:7b\n\n   3.56 GB  |  magicoder:latest\n\n   3.56 GB  |  magicoder:7b\n\n   3.56 GB  |  meditron:latest\n\n   3.56 GB  |  meditron:7b\n\n   3.56 GB  |  medllama2:latest\n\n   3.56 GB  |  medllama2:7b\n\n   3.56 GB  |  nous-hermes:latest\n\n   3.56 GB  |  nous-hermes:7b\n\n   3.56 GB  |  orca-mini:7b\n\n   3.56 GB  |  orca2:latest\n\n   3.56 GB  |  orca2:7b\n\n   3.56 GB  |  stable-beluga:latest\n\n   3.56 GB  |  stable-beluga:7b\n\n   3.56 GB  |  vicuna:latest\n\n   3.56 GB  |  vicuna:7b\n\n   3.56 GB  |  wizard-vicuna-uncensored:latest\n\n   3.56 GB  |  wizard-vicuna-uncensored:7b\n\n   3.56 GB  |  xwinlm:latest\n\n   3.56 GB  |  xwinlm:7b\n\n   3.56 GB  |  yarn-llama2:latest\n\n   3.56 GB  |  yarn-llama2:7b\n\n   3.37 GB  |  smallthinker:latest\n\n   3.37 GB  |  smallthinker:3b\n\n   3.32 GB  |  deepscaler:latest\n\n   3.32 GB  |  deepscaler:1.5b\n\n   3.24 GB  |  yi:latest\n\n   3.24 GB  |  yi:6b\n\n   3.11 GB  |  gemma3:latest\n\n   3.11 GB  |  gemma3:4b\n\n   3.07 GB  |  qwen3-vl:4b\n\n   3.07 GB  |  translategemma:latest\n\n   3.07 GB  |  translategemma:4b\n\n   3.04 GB  |  granite4:1b\n\n   2.98 GB  |  qwen2.5vl:3b\n\n   2.94 GB  |  phi4-mini-reasoning:latest\n\n   2.94 GB  |  phi4-mini-reasoning:3.8b\n\n   2.75 GB  |  ministral-3:3b\n\n   2.73 GB  |  llava-phi3:latest\n\n   2.73 GB  |  llava-phi3:3.8b\n\n   2.51 GB  |  granite3-guardian:latest\n\n   2.51 GB  |  granite3-guardian:2b\n\n   2.51 GB  |  nemotron-mini:latest\n\n   2.51 GB  |  nemotron-mini:4b\n\n   2.33 GB  |  qwen3:4b\n\n   2.33 GB  |  qwen3-embedding:4b\n\n   2.32 GB  |  phi4-mini:latest\n\n   2.32 GB  |  phi4-mini:3.8b\n\n   2.27 GB  |  granite3.2-vision:latest\n\n   2.27 GB  |  granite3.2-vision:2b\n\n   2.17 GB  |  qwen:latest\n\n   2.17 GB  |  qwen:4b\n\n   2.09 GB  |  cogito:3b\n\n   2.03 GB  |  nuextract:latest\n\n   2.03 GB  |  nuextract:3.8b\n\n   2.03 GB  |  phi3:latest\n\n   2.03 GB  |  phi3:3.8b\n\n   2.03 GB  |  phi3.5:latest\n\n   2.03 GB  |  phi3.5:3.8b\n\n   1.96 GB  |  granite4:3b\n\n   1.92 GB  |  granite3-moe:3b\n\n1.9 GB  |  granite3.1-moe:latest\n\n1.9 GB  |  granite3.1-moe:3b\n\n   1.88 GB  |  hermes3:3b\n\n   1.88 GB  |  llama3.2:latest\n\n   1.88 GB  |  llama3.2:3b\n\n   1.87 GB  |  falcon3:3b\n\n   1.86 GB  |  granite-code:latest\n\n   1.86 GB  |  granite-code:3b\n\n   1.84 GB  |  orca-mini:latest\n\n   1.84 GB  |  orca-mini:3b\n\n1.8 GB  |  qwen2.5:3b\n\n1.8 GB  |  qwen2.5-coder:3b\n\n   1.76 GB  |  qwen3-vl:2b\n\n   1.71 GB  |  starcoder:latest\n\n   1.71 GB  |  starcoder:3b\n\n1.7 GB  |  smollm2:latest\n\n1.7 GB  |  smollm2:1.7b\n\n   1.66 GB  |  falcon3:1b\n\n   1.62 GB  |  moondream:latest\n\n   1.62 GB  |  moondream:1.8b\n\n   1.59 GB  |  shieldgemma:2b\n\n   1.59 GB  |  starcoder2:latest\n\n   1.59 GB  |  starcoder2:3b\n\n   1.56 GB  |  gemma:2b\n\n   1.53 GB  |  exaone-deep:2.4b\n\n   1.53 GB  |  exaone3.5:2.4b\n\n   1.52 GB  |  gemma2:2b\n\n1.5 GB  |  stable-code:latest\n\n1.5 GB  |  stable-code:3b\n\n1.5 GB  |  stablelm-zephyr:latest\n\n1.5 GB  |  stablelm-zephyr:3b\n\n   1.49 GB  |  dolphin-phi:latest\n\n   1.49 GB  |  dolphin-phi:2.7b\n\n   1.49 GB  |  granite3-dense:latest\n\n   1.49 GB  |  granite3-dense:2b\n\n   1.49 GB  |  llama-guard3:1b\n\n   1.49 GB  |  phi:latest\n\n   1.49 GB  |  phi:2.7b\n\n   1.46 GB  |  granite3.1-dense:2b\n\n   1.44 GB  |  codegemma:2b\n\n   1.44 GB  |  granite3.2:2b\n\n   1.44 GB  |  granite3.3:2b\n\n   1.32 GB  |  granite3.1-moe:1b\n\n   1.32 GB  |  opencoder:1.5b\n\n   1.27 GB  |  qwen3:1.7b\n\n   1.23 GB  |  llama3.2:1b\n\n   1.08 GB  |  bge-m3:latest\n\n   1.08 GB  |  snowflake-arctic-embed2:latest\n\n   1.04 GB  |  deepcoder:1.5b\n\n   1.04 GB  |  deepseek-r1:1.5b\n\n   1.04 GB  |  internlm2:1.8b\n\n   1.04 GB  |  qwen:1.8b\n\n   0.98 GB  |  sailor2:1b\n\n   0.92 GB  |  qwen2.5:1.5b\n\n   0.92 GB  |  qwen2.5-coder:1.5b\n\n   0.92 GB  |  smollm:latest\n\n   0.92 GB  |  smollm:1.7b\n\n   0.92 GB  |  stablelm2:latest\n\n   0.92 GB  |  stablelm2:1.6b\n\n   0.87 GB  |  qwen2:1.5b\n\n   0.87 GB  |  qwen2-math:1.5b\n\n   0.87 GB  |  reader-lm:latest\n\n   0.87 GB  |  reader-lm:1.5b\n\n   0.81 GB  |  yi-coder:1.5b\n\n   0.77 GB  |  granite3-moe:latest\n\n   0.77 GB  |  granite3-moe:1b\n\n   0.76 GB  |  gemma3:1b\n\n   0.72 GB  |  deepseek-coder:latest\n\n   0.72 GB  |  deepseek-coder:1.3b\n\n   0.68 GB  |  lfm2.5-thinking:latest\n\n   0.68 GB  |  lfm2.5-thinking:1.2b\n\n   0.68 GB  |  starcoder:1b\n\n   0.62 GB  |  bge-large:latest\n\n   0.62 GB  |  mxbai-embed-large:latest\n\n   0.62 GB  |  snowflake-arctic-embed:latest\n\n0.6 GB  |  qwen3-embedding:0.6b\n\n   0.59 GB  |  tinydolphin:latest\n\n   0.59 GB  |  tinydolphin:1.1b\n\n   0.59 GB  |  tinyllama:latest\n\n   0.59 GB  |  tinyllama:1.1b\n\n   0.58 GB  |  embeddinggemma:latest\n\n   0.52 GB  |  paraphrase-multilingual:latest\n\n   0.49 GB  |  qwen3:0.6b\n\n   0.37 GB  |  qwen:0.5b\n\n   0.37 GB  |  qwen2.5:0.5b\n\n   0.37 GB  |  qwen2.5-coder:0.5b\n\n   0.33 GB  |  qwen2:0.5b\n\n   0.33 GB  |  reader-lm:0.5b\n\n   0.28 GB  |  functiongemma:latest\n\n   0.26 GB  |  nomic-embed-text:latest\n\n   0.06 GB  |  granite-embedding:latest\n\n   0.04 GB  |  all-minilm:latest",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qm9cgp/ollama_models_ranked_by_vram_requirements/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1kljht",
          "author": "merica420_69",
          "text": "At what quant?",
          "score": 17,
          "created_utc": "2026-01-25 05:59:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kxbyk",
              "author": "triynizzles1",
              "text": "q4 since that is what ollama defaults to. This is a list of models so if you wanted to download and run any of them you would enter ‚Äúollama run‚Äù before the name of the model. Then the file it downloads will be the size listed in the chart.",
              "score": 11,
              "created_utc": "2026-01-25 07:33:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ked3h",
          "author": "triynizzles1",
          "text": "Good post üëç",
          "score": 28,
          "created_utc": "2026-01-25 05:10:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1km6wl",
              "author": "Tall_Instance9797",
              "text": "Looks like a copy and paste from this website: [https://llm-explorer.com/](https://llm-explorer.com/)",
              "score": 33,
              "created_utc": "2026-01-25 06:03:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1l5bs0",
          "author": "arm2armreddit",
          "text": "Ollama defaults to a 4k context length; unfortunately, this is realistically unuseful for real tasks. It would be good to see the true memory usage with the 100% supported context length by the given model.",
          "score": 12,
          "created_utc": "2026-01-25 08:42:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27cugh",
              "author": "Sizzin",
              "text": "And FP8, FP16 comparison too.",
              "score": 1,
              "created_utc": "2026-01-28 12:38:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1kctku",
          "author": "Gombaoxo",
          "text": "Where are all huihui models? LLM is not fun without abliterated models.",
          "score": 18,
          "created_utc": "2026-01-25 05:00:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kyshj",
              "author": "darkpigvirus",
              "text": "Back then I liked huihui models but intelligence suffers when abliterating a model unless you really want those censored topics. Also there is a technique better than abliteration that lessen the intelligence suffering I just forgot it.",
              "score": 2,
              "created_utc": "2026-01-25 07:45:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1lkfnt",
                  "author": "alhinai_03",
                  "text": "You probably mean heretic",
                  "score": 1,
                  "created_utc": "2026-01-25 10:56:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1p2ssh",
                  "author": "According-Delivery44",
                  "text": "What technique? Prompyt injection?",
                  "score": 0,
                  "created_utc": "2026-01-25 21:29:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1l7c0n",
          "author": "leo-the-great",
          "text": "üòÖ damn I have to scroll way way down to check what I can run.",
          "score": 10,
          "created_utc": "2026-01-25 09:00:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kmcg7",
          "author": "Tall_Instance9797",
          "text": "Is this post just a copy and paste from this website: [https://llm-explorer.com/](https://llm-explorer.com/) ?",
          "score": 12,
          "created_utc": "2026-01-25 06:05:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kp0eu",
              "author": "AdventurousLion9548",
              "text": "Actually, I copied the code that ChatGPT generated to scrape and rank from ollama/models and got the output. üòÇüòÇüòÇ import re\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\nLIBRARY_URL = \"https://ollama.com/library/\"\nMANIFEST_URL = \"https://registry.ollama.ai/v2/library/{name}/manifests/{tag}\"\nOUTPUT_FILE = \"ollama_models_ranked_by_vram.txt\"\n\nHEADERS = {\n    \"User-Agent\": \"ollama-vram-ranker/1.1\"\n}\n\nWEIGHT_MEDIA_TYPES = {\n    \"application/vnd.ollama.image.model\",\n    \"application/vnd.ollama.image.projector\",  # vision models\n}\n\nsession = requests.Session()\nsession.headers.update(HEADERS)\n\n\n# -------------------- helpers --------------------\n\ndef get_html(url):\n    r = session.get(url, timeout=30)\n    r.raise_for_status()\n    return r.text\n\n\ndef bytes_to_gb(b):\n    return b / (1024 ** 3)\n\n\n# -------------------- scraping --------------------\n\ndef get_all_models():\n    html = get_html(LIBRARY_URL)\n    soup = BeautifulSoup(html, \"html.parser\")\n\n    models = set()\n    for a in soup.select('a[href^=\"/library/\"]'):\n        href = a.get(\"href\", \"\")\n        m = re.fullmatch(r\"/library/([^/]+)\", href)\n        if m:\n            models.add(m.group(1))\n\n    return sorted(models)\n\n\ndef get_model_tags(model):\n    url = urljoin(LIBRARY_URL, model)\n    html = get_html(url)\n    soup = BeautifulSoup(html, \"html.parser\")\n\n    text = soup.get_text(\" \", strip=True).lower()\n    tags = set()\n\n    for token in text.split():\n        if re.fullmatch(r\"\\d+(\\.\\d+)?b\", token):\n            tags.add(token)\n        elif token == \"latest\":\n            tags.add(token)\n\n    if not tags:\n        tags.add(\"latest\")\n\n    # latest first, then numeric\n    def sort_key(t):\n        if t == \"latest\":\n            return (0, 0)\n        m = re.match(r\"(\\d+(\\.\\d+)?)b\", t)\n        return (1, float(m.group(1)) if m else 999)\n\n    return sorted(tags, key=sort_key)\n\n\n# -------------------- registry --------------------\n\ndef get_manifest_weight_bytes(model, tag):\n    url = MANIFEST_URL.format(name=model, tag=tag)\n\n    try:\n        r = session.get(url, timeout=30)\n        if r.status_code != 200:\n            return None\n\n        manifest = r.json()\n\n        # √∞¬ü¬î¬¥ CRITICAL FIX: validate manifest\n        if not isinstance(manifest, dict):\n            return None\n\n        layers = manifest.get(\"layers\")\n        if not isinstance(layers, list):\n            return None\n\n        total = 0\n        for layer in layers:\n            if (\n                isinstance(layer, dict)\n                and layer.get(\"mediaType\") in WEIGHT_MEDIA_TYPES\n                and isinstance(layer.get(\"size\"), int)\n            ):\n                total += layer[\"size\"]\n\n        return total if total > 0 else None\n\n    except Exception:\n        return None\n\n\n# -------------------- main --------------------\n\ndef main():\n    print(\"Fetching Ollama model list...\")\n    models = get_all_models()\n\n    results = []\n\n    for idx, model in enumerate(models, 1):\n        print(f\"[{idx}/{len(models)}] {model}\")\n\n        try:\n            tags = get_model_tags(model)\n        except Exception:\n            continue\n\n        for tag in tags:\n            size_bytes = get_manifest_weight_bytes(model, tag)\n            if size_bytes:\n                results.append({\n                    \"model\": model,\n                    \"tag\": tag,\n                    \"gb\": round(bytes_to_gb(size_bytes), 2)\n                })\n\n    results.sort(key=lambda x: x[\"gb\"], reverse=True)\n\n    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"Ollama Models Ranked by VRAM Requirement (Weights Only)\\n\")\n        f.write(\"=\" * 60 + \"\\n\\n\")\n\n        for r in results:\n            f.write(f\"{r['gb']:>7} GB  |  {r['model']}:{r['tag']}\\n\")\n\n    print(f\"\\n√¢¬ú¬î Output written to: {OUTPUT_FILE}\")\n    print(f\"√¢¬ú¬î {len(results)} valid model entries written\")\n\n\nif __name__ == \"__main__\":\n    main()",
              "score": 10,
              "created_utc": "2026-01-25 06:25:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1klbna",
          "author": "nitinmms1",
          "text": "This is very useful",
          "score": 9,
          "created_utc": "2026-01-25 05:57:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1km1wn",
              "author": "Tall_Instance9797",
              "text": "Looks like a copy and paste from this website: [https://llm-explorer.com/](https://llm-explorer.com/) \\- which honestly is far more useful.",
              "score": 8,
              "created_utc": "2026-01-25 06:02:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1o1du4",
                  "author": "Puzzled_Surprise_383",
                  "text": "Amigo √© o seu terceiro coment√°rio querendo acabar com o OP, j√° entendemos sua revolta. \nAgrade√ßo a ele por postar pois n√£o conhecia o site. \nObrigado, OP, post muito √∫til",
                  "score": 6,
                  "created_utc": "2026-01-25 18:46:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1utz62",
                  "author": "jamalakj",
                  "text": "We don't care",
                  "score": 2,
                  "created_utc": "2026-01-26 17:38:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1kj8bj",
          "author": "slow-fast-person",
          "text": "How to calculate this for corresponding requirements on apple metal?",
          "score": 3,
          "created_utc": "2026-01-25 05:43:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rhqcx",
          "author": "MLExpert000",
          "text": "Who is actually serving a 1250GB model locally? To run that cogito model, you‚Äôd need about 53 RTX 3090s daisy-chained together. at that point, it‚Äôs not a home lab, it‚Äôs a fire hazard.",
          "score": 3,
          "created_utc": "2026-01-26 04:46:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yvtrx",
              "author": "TedditBlatherflag",
              "text": "You don‚Äôt have an HGX H200 rack running in your HVAC, heating your home?",
              "score": 1,
              "created_utc": "2026-01-27 05:55:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1lhfit",
          "author": "austrobergbauernbua",
          "text": "I can just highly recommend the granite 4 models (hybrid architecture). For small tasks like simple Q&A or rewriting they are extremely efficient (fast memory loading - extremely fast inference).",
          "score": 2,
          "created_utc": "2026-01-25 10:30:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1m4rez",
          "author": "Electronic-Set-2413",
          "text": "Nice one",
          "score": 2,
          "created_utc": "2026-01-25 13:31:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1md7hd",
          "author": "Bonzupii",
          "text": "Really dude",
          "score": 2,
          "created_utc": "2026-01-25 14:19:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nxtyg",
          "author": "gamesta2",
          "text": "Looks like 4k context length for all. I can confirm that most of these take as much as double the vram once you increase context to 48k+. For example, my 4b model that supports my home assistant takes 16gb vram at 128k context.",
          "score": 2,
          "created_utc": "2026-01-25 18:31:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1out0a",
              "author": "SundayButtermilk",
              "text": "Are you using this for home assistant voice? What hardware are you running it on?",
              "score": 2,
              "created_utc": "2026-01-25 20:55:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1povhv",
                  "author": "gamesta2",
                  "text": "I use it for tool calling. The model i run is qwen3:4b-instruct-q_8 (for better precision). I lied about context, mine is set to 96k. It has to be this high for hass because for some reason each call is 50k+ tokens, depending on how many devices you expose.\n\nThe hardware is a separate server on the same local network. nothing fancy. Ryzen 7 9700x, 64gb ddr5, and dual rtx 3060 do the hard lifting. Llms are hosted by ollama. \n\nIm not too worried about offloading into ram for most Ai usage when utilizing other models for things like image analysis (qwen3-vl:30b) but for hass tool calling I had to go with a model small enough to fit into 24gb vram to maximize response time, and at the same time handle 96k context.\nBiggest ragret is not getting additional 64gb of ram back when it was 240$.",
                  "score": 2,
                  "created_utc": "2026-01-25 23:07:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1pqxos",
                  "author": "gamesta2",
                  "text": "As for voice, yes. I use hass assistant on my phone and my watch to give it voice commands. Nabu transcribes it to text (or you can use the stock stt that comes with hass) and the assistant either takes own action if its something simple, or makes a call to llm if its something complex like if-then or multiple actions.",
                  "score": 2,
                  "created_utc": "2026-01-25 23:16:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rmwlq",
          "author": "KiranjotSingh",
          "text": "Incorrect. Most of these are MoE and vram requirement differs a lot in that case",
          "score": 2,
          "created_utc": "2026-01-26 05:21:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l35ym",
          "author": "vir_db",
          "text": "With maximum num_ctx?",
          "score": 1,
          "created_utc": "2026-01-25 08:23:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l6hnx",
          "author": "pmv143",
          "text": "We run all of these models on the same 16√óH100 pool. Big models take the whole beam, smaller ones take slices. Fast snapshot restore lets us swap between them in ~1‚Äì2s instead of pinning GPUs.",
          "score": 1,
          "created_utc": "2026-01-25 08:52:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ldnpu",
          "author": "WildDogOne",
          "text": "and now, if someone could do that, but with maxed out context window",
          "score": 1,
          "created_utc": "2026-01-25 09:56:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ldpyp",
          "author": "zenmatrix83",
          "text": "Context length matters and the default is too small for anything not 100 chat based, my current use case is using 32k but I can get 64k worth from qwen3 from my 4090 at decent speeds",
          "score": 1,
          "created_utc": "2026-01-25 09:57:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1mg8vl",
          "author": "AUFairhope1104",
          "text": "Per",
          "score": 1,
          "created_utc": "2026-01-25 14:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nujze",
          "author": "Mangostickyrice1999",
          "text": "Source?",
          "score": 1,
          "created_utc": "2026-01-25 18:18:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ocmn9",
              "author": "AdventurousLion9548",
              "text": "From ollama/models site. Also here https://www.reddit.com/r/ollama/s/HaUw1tIu6W",
              "score": 2,
              "created_utc": "2026-01-25 19:34:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rpzdb",
          "author": "eagledoto",
          "text": "Can you please let me know What are the best models for image captioning or turning image to prompt for flux2/z image? I've got rtx 2060 with 12vram and 32gigs of system ram",
          "score": 1,
          "created_utc": "2026-01-26 05:42:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2enpar",
              "author": "Hour-Entertainer-478",
              "text": "not sure what do you mean by turning image to prompt, but for image captioning, i've found moondream a nice option. check that out.",
              "score": 1,
              "created_utc": "2026-01-29 13:28:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o203g46",
          "author": "Informal-Victory8655",
          "text": "Any model better than qwen2.5:14b in agentic tool calling capabilities? Anyone has any experience?",
          "score": 1,
          "created_utc": "2026-01-27 12:09:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29nvgo",
          "author": "v01dm4n",
          "text": "Ollama isn't the best if you want to make the most out of your vram. LMstudio gives me way better performance at 30b MoE models with 16G vram. ~30tps. Very much usable. Nemotron, qwen coder, flash glm 4.7. Dense models auch as gemma27b work but slowly ~10tps.",
          "score": 1,
          "created_utc": "2026-01-28 19:07:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f4t34",
          "author": "uhateonhaters",
          "text": "Is this like video games where this is the system minimum and best performance = 2x min?",
          "score": 1,
          "created_utc": "2026-01-29 14:56:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp9c9x",
      "title": "AI started speaking in Russian out of nowhere",
      "subreddit": "ollama",
      "url": "https://i.redd.it/jckp73fxv2gg1.jpeg",
      "author": "No-Sky2462",
      "created_utc": "2026-01-28 11:54:45",
      "score": 109,
      "num_comments": 41,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qp9c9x/ai_started_speaking_in_russian_out_of_nowhere/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o27i9v9",
          "author": "Acceptable_Home_",
          "text": "The specific llm you're using is a fine-tune of llama 3 by Nvidia, def shouldn't act like that, must be a problem with some default settings ollama has set\n\n\nAlso, you're using a 70B parameters model which would take around 72-75gb of vram or system ram, and will be slow as hell, either use a smaller model or use ollama command to make model use vram before ram, it'll fasten your inference speed/llm response speed",
          "score": 19,
          "created_utc": "2026-01-28 13:12:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28914j",
              "author": "ClothesHot3110",
              "text": "what is the command if you mind?",
              "score": 2,
              "created_utc": "2026-01-28 15:27:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28r9y0",
                  "author": "Acceptable_Home_",
                  "text": "on every run- OLLAMA\\_GPU\\_LAYERS=999 ollama run \\[model name\\]\n\none time change in config- export OLLAMA\\_GPU\\_LAYERS=999",
                  "score": 3,
                  "created_utc": "2026-01-28 16:47:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27jjgz",
              "author": "No-Sky2462",
              "text": "Thanks for the detailed response! I have no idea what could have gone wrong, the picture you are seeing is *right* after installing, maybe something went wrong during while installing?  \nAlso I have 64GB of RAM, i thought i could handle it but oh well, do you have any suggestions for a llm?",
              "score": 2,
              "created_utc": "2026-01-28 13:19:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27mz5h",
                  "author": "ImprovementThat2403",
                  "text": "VRAM and Ram are different.",
                  "score": 7,
                  "created_utc": "2026-01-28 13:38:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27r030",
                  "author": "Acceptable_Home_",
                  "text": "This should be running quantized, like if model was about 70GB on disk it is being compressed to fit on ram + vram, and ollama might have some issues in that, you should try LM studio for what usecase you have, it's more ui leaning easy option, mainly made for beginners, ollama isn't bad too, js try to use a diff model that can fit in your ram + vram,¬†\n\n\nIt'll be faster if it fits totally inside vram(gpu ram)!",
                  "score": 2,
                  "created_utc": "2026-01-28 13:59:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o279g37",
          "author": "ageownage",
          "text": "![gif](giphy|uTYR5bOktNNWsbgRAb|downsized)",
          "score": 33,
          "created_utc": "2026-01-28 12:15:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27aclk",
              "author": "No-Sky2462",
              "text": "https://preview.redd.it/5en4zlyx13gg1.png?width=1723&format=png&auto=webp&s=fa4c6e5c8b16d674de32c166c24438cacf76357a\n\nI managed to make it speak english again and it completely broke. Honestly i wonder how would an AI Apocalypse would look like now.",
              "score": 17,
              "created_utc": "2026-01-28 12:22:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27cfsm",
                  "author": "polishatomek",
                  "text": "lmao",
                  "score": 6,
                  "created_utc": "2026-01-28 12:36:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o279wh5",
          "author": "MatchaFlatWhite",
          "text": "–î–∞ –∫—Ç–æ –∫—Ç–æ –µ–≥–æ –∑–Ω–∞–µ—Ç? ü§∑üèª‚Äç‚ôÇÔ∏è",
          "score": 39,
          "created_utc": "2026-01-28 12:19:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27cmh2",
              "author": "InfraScaler",
              "text": "–°–∫—Ä–∏–Ω—à–æ—Ç –≤–µ—Å—å –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, –æ —á—ë–º –≥–æ–≤–æ—Ä–∏—Ç –∞–≤—Ç–æ—Ä?",
              "score": 15,
              "created_utc": "2026-01-28 12:37:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27qewm",
                  "author": "CatEatsDogs",
                  "text": "–ß—Ç–æ-—Ç–æ —Ç–∞–º –ø—Ä–æ –±–µ—Å—Å–º–µ—Ä—Ç–Ω—ã—Ö –ª–ª–∞–º",
                  "score": 11,
                  "created_utc": "2026-01-28 13:56:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27gxzi",
              "author": "Narrow-Impress-2238",
              "text": "–†–∞–±–æ—Ç–∞–µ–º üí™üèªüëçüèªüí™üèª",
              "score": 8,
              "created_utc": "2026-01-28 13:04:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27cj8a",
          "author": "Unusual-Royal1779",
          "text": "I had OpenAI's Codex return a single Russian word to me in an otherwise English conversation. Didn't think much of it, but now I'm starting to get –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π",
          "score": 11,
          "created_utc": "2026-01-28 12:36:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27gz87",
              "author": "No-Sky2462",
              "text": "I am glad to know i am not the only one! Definitely —Ç—Ä–µ–≤–æ–∂–Ω—ã–π, pulled the plug to be safe",
              "score": 5,
              "created_utc": "2026-01-28 13:04:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27p54h",
                  "author": "thibautrey",
                  "text": "Sounds like a very nicely achieved training data poisoning. Good job whoever did that, you are successively f****** up the ai.",
                  "score": 4,
                  "created_utc": "2026-01-28 13:49:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o285z5r",
          "author": "Dtounlion",
          "text": "Sometimes a trigger word is enough. Once I started a conversation with Grok with \"wesh\", French slang, but the rest of my long prompt was in English. It responded in English. Next prompt, still in English, it responded in French. I asked \"why are you answering in French?\", it responded \"oh sorry, you said wesh, I went full *mode racaille*\". Which in itself is quite funny.",
          "score": 5,
          "created_utc": "2026-01-28 15:13:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b33p0",
          "author": "PeePeeLangstrumpf",
          "text": "Some poor Russian agent dude behind the \"AI\" finally broke, it was a cry for help, and you just dismissed him! He will be probably taken out into the tundra tomorrow and shot. Another agent will replace him, but his family will remember OP... his family will remember.",
          "score": 3,
          "created_utc": "2026-01-28 22:56:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dwa4k",
              "author": "No-Sky2462",
              "text": "woops üòÖ  \nis it too late..?",
              "score": 1,
              "created_utc": "2026-01-29 10:07:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27nokf",
          "author": "Every_Commercial556",
          "text": "u…ê·¥âssn…π  û…ê«ùds  á,u…ê…î I",
          "score": 5,
          "created_utc": "2026-01-28 13:42:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o288hq4",
              "author": "No-Sky2462",
              "text": "I also can't speak australian, sorry about that :(",
              "score": 9,
              "created_utc": "2026-01-28 15:25:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27r16q",
          "author": "caujka",
          "text": "I guess, this is because your conversation looks like bullying. And when the conversation got first tokens in russian, it's doomed, need to start from scratch.\nThing is, llms learned on all text produced in internets, and russian forums are a big part of it. And they are usually not a pleasant place to be.",
          "score": 3,
          "created_utc": "2026-01-28 13:59:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o293d4y",
          "author": "florinandrei",
          "text": "That happens.\n\nIt's not supposed to happen, but it does once in a while. It's just a kind of hallucination, as all these models are prone to do; perhaps it's worse than other kinds because the output is suddenly very different. But yeah, it's the LLM equivalent of the good old \"my app crashed\".\n\nSome models do it more often than others. Some essentially never do it, but do not bet your life on it.\n\nRestart the conversation, and it should be fine.",
          "score": 2,
          "created_utc": "2026-01-28 17:39:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o297tze",
          "author": "Cyrylow",
          "text": "–ö—Ç–æ-—Ç–æ –∑–Ω–∞–µ—Ç –∑–∞—á–µ–º –∞–≤—Ç–æ—Ä –ø–∏—à–µ—Ç –ø–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏?",
          "score": 2,
          "created_utc": "2026-01-28 17:58:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2986vi",
              "author": "Cyrylow",
              "text": "(now unironically)  \nAI not matter on OpenAI, or local or another- likes to halucinate. Eg once local model generated for me some nonsense in arabic",
              "score": 2,
              "created_utc": "2026-01-28 18:00:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o297y6r",
          "author": "Agitated_Heat_1719",
          "text": "Context small and filled?",
          "score": 2,
          "created_utc": "2026-01-28 17:59:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a8nbg",
          "author": "NexonSU",
          "text": "Muhahahaha! It's not your LLM, it's ours!",
          "score": 2,
          "created_utc": "2026-01-28 20:40:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dkwvj",
          "author": "Zealousideal-Life417",
          "text": "Perhaps he was joking, since the Russian word \"–Ω–∞–ø–∏—Å–∞—Ç—å\"/\"napisat\" has two meanings: to write a text and to urinate. (\"–Ω–∞–ø–∏—Å–∞—Ç—å —Ç–µ–∫—Å—Ç\" (write a text) –∏–ª–∏ \"–Ω–∞–ø–∏—Å–∞—Ç—å –≤ –∫—Ä–æ–≤–∞—Ç—å\" (pee in bed)) After that, he switched to Russian in context.",
          "score": 2,
          "created_utc": "2026-01-29 08:21:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27hj4e",
          "author": "Acceptable_Home_",
          "text": "Blyatt brather, you hv been chosen for something important¬†",
          "score": 3,
          "created_utc": "2026-01-28 13:08:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27b9re",
          "author": "Zestyclose-Shift710",
          "text": "Some setting is probably wrong",
          "score": 1,
          "created_utc": "2026-01-28 12:28:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28c1n1",
          "author": "sinan_online",
          "text": "So, I am thinking that there may be something about version incompatibility, although in my experience that can create complete gibberish, not Russian.\n\nIf this happens repeatedly, an option would be to try with _earlier_ versions of Ollama, in fact, you can check the repo to get the date for llama3-chatqa release, and use the stable version for Ollama right before or after. These are being developed heavily, and despite all the fuss, I don‚Äôt think that they are production-ready yet.\n\nKeep in mind, unlike traditional software, these are not meant to be deterministic, but just statistical. Responding in Russian to an English is very unlikely, but statistically speaking, that just means that it will wind up happening to someone in repeated interactions. Maybe it was you. I‚Äôd say, play around a bit more before you start setting up a new environment.",
          "score": 1,
          "created_utc": "2026-01-28 15:41:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28in6c",
          "author": "tiga_94",
          "text": "increase context size in settings",
          "score": 1,
          "created_utc": "2026-01-28 16:09:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28mcr6",
          "author": "primateprime_",
          "text": "I've run into similar stuff and it usually clears up when I change the model load settings. I'm not gonna pretend to know what's happening exactly, but it seems to happen when the model is just a little too big. Generally if I lower the content window (-c) and use cache quantizing it stops. \nOr just use a different quant.",
          "score": 1,
          "created_utc": "2026-01-28 16:25:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dhdk5",
          "author": "osc707",
          "text": "Omg mine did last night. Out of nowhere.",
          "score": 1,
          "created_utc": "2026-01-29 07:49:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27mua6",
          "author": "AwayLuck7875",
          "text": "–ü–æ—Å—Ç–∞–≤—å qwen ,gemma –Ω–∞–ø–∏—à–∏ –æ—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º,–∏—â–∏ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä—É—Å—Å–∫–æ–≥–æ",
          "score": 0,
          "created_utc": "2026-01-28 13:37:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo9tsi",
      "title": "NotebookLM For Teams",
      "subreddit": "ollama",
      "url": "https://v.redd.it/zxqevbwh8vfg1",
      "author": "Uiqueblhats",
      "created_utc": "2026-01-27 10:04:14",
      "score": 87,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qo9tsi/notebooklm_for_teams/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o20lca9",
          "author": "Ok_Swing9407",
          "text": "if you're exploring self-hosted options for team knowledge management and automation, needle.app has been solid for me. it's like vibe automation meets RAG, so workflows actually understand your docs instead of just moving data around. way less setup than wiring langchain or n8n for every new project.",
          "score": 1,
          "created_utc": "2026-01-27 13:56:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o225jqb",
              "author": "mumblerit",
              "text": "i just want you to know, i had a visceral disgusted response to the phrase \"its like vibe automation meets x\"",
              "score": 0,
              "created_utc": "2026-01-27 18:11:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qprl7a",
      "title": "[Opinion] Why I believe the $20/month Ollama Cloud is a better investment than ChatGPT or Claude",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qprl7a/opinion_why_i_believe_the_20month_ollama_cloud_is/",
      "author": "AlexHardy08",
      "created_utc": "2026-01-28 23:23:23",
      "score": 75,
      "num_comments": 61,
      "upvote_ratio": 0.79,
      "text": "**Disclaimer:**¬†I am not affiliated with Ollama in any way. This is purely based on my personal experience as a long-term user.\n\nI‚Äôve been using Ollama since it first launched, and it has genuinely changed my workflow. Even with a powerful local machine, there are certain walls you eventually hit. Lately, I‚Äôve been testing the $20/month Cloud plan, and I wanted to share why I think it‚Äôs worth every penny.\n\n**The \"Large Model\" Barrier**  \nWe are seeing incredible models being released, like¬†**Kimi-k2.5**, DeepSeek, GLM, and various Open-Source versions of top-tier models. For 99% of us, running these locally is simply impossible unless you have a $30,000+ rig.\n\nYes, there is a free tier for Ollama Cloud, but we have to be realistic: running these massive models requires serious computation power. The paid plan gives you the stability and speed that a professional workflow requires.\n\n**Why I chose this over a ChatGPT/Claude subscription:**\n\n1. **The Ecosystem:**¬†Instead of being locked into one model like GPT-5, I have immediate access to a variety of state-of-the-art models.\n2. **Simplicity:**¬†If you have Ollama installed, you already know the drill. Switching to a cloud-hosted massive model is as simple as¬†ollama run kimi-k2.5. No complex configurations, no manual weight management. It just works, and it‚Äôs incredibly fast.\n3. **ROI (Return on Investment):**¬†If you are building something or doing serious work and don't have the budget for a custom local cluster, this $20 investment pays for itself almost immediately. It bridges the gap between \"hobbyist\" and \"enterprise-level\" capabilities.\n\n**The Only Downside**  \nIf I had to nitpick, it would be the transparency regarding limits. Much like the free plan, on the $20 plan, it‚Äôs sometimes hard to tell exactly when you‚Äôll hit a rate limit. It‚Äôs a bit of a \"black box\" experience, but in my daily use, the performance has been worth the uncertainty.\n\n**My Suggestion:**  \nIf you are doing research or building tools and you need the power of models that your local VRAM can‚Äôt handle, stop hesitating. It‚Äôs a solid investment that democratizes access to high-end AI.\n\n**I‚Äôm curious to hear from others:**  \nIs anyone else here using the $20/month Ollama Cloud plan? What has your experience been like so far? Any \"pro-tips\" or secrets you‚Äôve discovered to get the most out of it?\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qprl7a/opinion_why_i_believe_the_20month_ollama_cloud_is/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o2bzl0v",
          "author": "InevitableHello",
          "text": "Sorry if this is a dumb question; just heard about the Ollama cloud as a result of this post. Is it your own sandboxed instance similar to what a dedicated VM might provide or is there still a high chance of your data being shared/seen/reviewed/contributed/etc (aka leaked)?",
          "score": 19,
          "created_utc": "2026-01-29 01:48:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cv3eu",
              "author": "EsotericTechnique",
              "text": "It will be sent to another inference engine for sure who knows what happensü§î, as with all cloud services!",
              "score": 6,
              "created_utc": "2026-01-29 04:53:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2bkiij",
          "author": "Condomphobic",
          "text": "You get more than a model with GPT‚Äôs $20 subscription(you get an ecosystem) and Claude‚Äôs models are monstrous.",
          "score": 17,
          "created_utc": "2026-01-29 00:26:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cfvgl",
          "author": "Steus_au",
          "text": "did you use chatgpt to create the text? or claude?¬†",
          "score": 10,
          "created_utc": "2026-01-29 03:17:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2difje",
              "author": "CheapProg6886",
              "text": "those quotations reads more like gemini",
              "score": 7,
              "created_utc": "2026-01-29 07:58:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2dkavm",
                  "author": "mangoskive",
                  "text": "Yeah, and the use of bold also smells gemini",
                  "score": 2,
                  "created_utc": "2026-01-29 08:15:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2e4xdv",
              "author": "chocoroboto",
              "text": "every post is ai now, honestly is so tiring to read",
              "score": 3,
              "created_utc": "2026-01-29 11:22:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2egvsa",
                  "author": "Steus_au",
                  "text": "not every. I do type my posts myself like a dinosaur",
                  "score": 1,
                  "created_utc": "2026-01-29 12:47:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2czgr6",
          "author": "AstroZombie138",
          "text": "I get what you are saying, but why Ollama cloud vs something like openrouter?",
          "score": 3,
          "created_utc": "2026-01-29 05:23:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dg9fp",
              "author": "InfraScaler",
              "text": "I'd say predictable spend¬†",
              "score": 2,
              "created_utc": "2026-01-29 07:39:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2bzabf",
          "author": "WiggyWamWamm",
          "text": "Does anyone know if you can run the biggest open source models on a maxed out Mac Studio? And if so how many months of Ollama Cloud that converts to?",
          "score": 2,
          "created_utc": "2026-01-29 01:46:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2c8rwh",
              "author": "CorpusculantCortex",
              "text": "At 20$ a month it would be 600 months or about 50 years. \nIdk about the feasibility of running the biggest models on apple silicon and 192gb of ram, but you definitely can't fit full weights of 200b+ on that much ram, maybe q4 idk. But 50 years...",
              "score": 6,
              "created_utc": "2026-01-29 02:38:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2cuprn",
                  "author": "alexsm_",
                  "text": "Perhaps what they share here may help to get an idea:\n\n[AI supercomputer with 5 Mac Studios](https://youtu.be/Ju0ndy2kwlw)\n\n[M4 Mac Mini Cluster](https://youtu.be/GBR6pHZ68Ho)\n\n[Deepseek R1 on Mac Minis](https://youtube.com/shorts/tReT0O7_Vh4?si=jlndwVLlXCGjB8eE)\n\n[120B LLM on MS-S1 Max](https://youtube.com/shorts/xMdFTIH5dRA?si=LJwIUC7EHE-U22CM)",
                  "score": 2,
                  "created_utc": "2026-01-29 04:51:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2efsc4",
                  "author": "WiggyWamWamm",
                  "text": "512 GB is where the unified RAM maxes out",
                  "score": 1,
                  "created_utc": "2026-01-29 12:40:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2eflqi",
              "author": "NotAMusicLawyer",
              "text": "\nM3 Ultra can run DeepSeek R1 with 671 billion parameters. That appears to be the limit right now.\n\nLocal hardware is not going to beat a cloud GPU cluster anytime soon.",
              "score": 2,
              "created_utc": "2026-01-29 12:39:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2eg0yn",
                  "author": "WiggyWamWamm",
                  "text": "Yes but if it runs it at remotely usable speeds I would prefer it to my data being in the cloud like this. Looks like the 512 GB Mac Studio is just shy of $10k so by the time I get one none of this will be relevant.",
                  "score": 1,
                  "created_utc": "2026-01-29 12:42:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2br6e0",
          "author": "alexhackney",
          "text": "You do you. Idk why you feel the need to justify it.\n\nMy opinion, which really, wgaf, is that I‚Äôll keep paying for Claude Max until my ollama solution is 80% as effective.\n\nI don‚Äôt care to pay 1000 for Ram and 1000 for gpu if it‚Äôll do at least 80% as good as Claude. Until then I‚Äôll pay them and then GeForce now to rent their gpu so I can play bf6 on my Mac while I‚Äôm waiting for Claude to finish a prompt.",
          "score": 6,
          "created_utc": "2026-01-29 01:01:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cubpm",
              "author": "CoolmannS",
              "text": "This is the way ! I just replace BF6 with COD üòÇ",
              "score": 0,
              "created_utc": "2026-01-29 04:48:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2cgiu3",
              "author": "eist5579",
              "text": "lol.  If you‚Äôre serious, how is geforce now?",
              "score": -1,
              "created_utc": "2026-01-29 03:21:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2e79wq",
                  "author": "alexhackney",
                  "text": "I think its good. I like it for BF and Rocket League.",
                  "score": 1,
                  "created_utc": "2026-01-29 11:40:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2begcs",
          "author": "PuddleWhale",
          "text": "So Ollama gives you free LLMs for $20/month wile Openai and Claude give you arguably the best two coding LLMs for the same price or thereabouts. I'm not understading the use cases either. Does Ollama provide API keys to these free LLMs that are supposedly catcing up to claude and gpt in terms of competency? \n\nYou can use massive amounts of compute on ChatGPT's monthly $20 plan if you stick to the webchat. I tink Claude gives a lot less compute but it supposedly has new updates in Claude Code that people are raving about. Kimu/Deepseek/Qwen etc are good but they're not state of the art like Claude and GPT still are.\n\nAnd if you want variety there's always Openrouter besides the free accounts that kimi, deepskeek, grok, claude, gpt and mistral hand out.",
          "score": 3,
          "created_utc": "2026-01-28 23:55:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ddc24",
              "author": "jruz",
              "text": "It‚Äôs the amount of compute you get, the $20 plans from the closed models are pretty much a demo account you burn the usage in minutes. Open models give you waaaaay more usage.",
              "score": 1,
              "created_utc": "2026-01-29 07:13:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2f2im7",
                  "author": "PuddleWhale",
                  "text": "You burn through the free Claude account in minutes but the ChatGPT and Grok and Mistral free web sessions do not evaporate within minutes. And these are just free ones. If you get paid plans they last a lot more for webchat but once again Claude rate limits you sooner than all the others. \n\n\n\nThe real question is whether or not ollama is losing money to gain market share like all the commercial LLMs. I read somewhere that for every $1 openai brings in in revenue, they spend $7 in expenses. They're able to do that because of sky high company evaluations and investor funding. If ollama is just trying to break even or even make money on these models then it might not be worth the $20 investment,   \n  \nPerhaps the reason everything works on ollama cloud(assuming OP is not a shill poster as one person has noted, using an LLM to make his posts) is because of all the people who pay $20 a month, enough of them do not use ollama cloud as much as OP and are thus subsidizing him. So it's possible hat ollama could be overselling their cloud offering, relying on their paying customers to underutilize the service.",
                  "score": 1,
                  "created_utc": "2026-01-29 14:45:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2bhbb2",
              "author": "AlexHardy08",
              "text": "I appreciate your perspective, and I see where you‚Äôre coming from! However, I think there might be a misunderstanding of the current landscape of AI.\n\nFirst, regarding¬†**SOTA (State of the Art)**: The idea that only OpenAI and Anthropic hold the crown is becoming outdated. If you look at recent benchmarks, models like¬†**DeepSeek-V3**¬†or¬†**Qwen-2.5-Coder-32B/72B**¬†are matching or even beating GPT and Claude in specific coding and logic tasks. For many of us, limiting our workflow to just two providers is actually a bottleneck, not a benefit.\n\nSecondly, you mentioned paying for 'free LLMs.' You aren't paying for the models themselves; you are paying for the¬†**massive compute**¬†required to run them at high speeds. Try running a 405B parameter model or the full Kimi-k2.5 locally even with a high-end consumer GPU, it's a struggle. The $20/month covers the infrastructure that allows you to swap between these giants instantly within the Ollama ecosystem.\n\nRegarding¬†**OpenRouter**: It's a great service, no doubt. But for those of us who have built our entire workflow, scripts, and local integrations around the Ollama CLI and API, having a 'Cloud' version of that same experience is a game-changer. It‚Äôs about the seamless transition between local (small models) and cloud (massive models) without changing your codebase.\n\nIn my opinion, believing that only ChatGPT and Claude are worth using is a limitation. The open-weights world is moving faster than anyone expected, and having a professional environment to run them at scale is worth every cent. It‚Äôs not about 'free vs. paid' it's about¬†**versatility and power.**\"",
              "score": 1,
              "created_utc": "2026-01-29 00:10:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2buoqb",
                  "author": "YouAreTheCornhole",
                  "text": "If you're looking at benchmarks and mainly judging models based on them, you've been played. Benchmaxxing is alive and well. For example, literally no model gets even remotely close to Opus 4.5 but according to benchmarks there's a lot of models that are. Now that doesn't mean some of these models aren't great for what YOU use them for, but overall when it comes to raw capabilities, nothing comes close to Claude, ChatGPT or Gemini right now, not even remotely close.",
                  "score": 5,
                  "created_utc": "2026-01-29 01:21:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2c3vzr",
                  "author": "taylorwilsdon",
                  "text": "You kinda lose the room comparing qwen 2.5 coder 32b, which isn‚Äôt even recent, to sonnet and opus at coding. Thats not even state of the art within the qwen family‚Ä¶",
                  "score": 5,
                  "created_utc": "2026-01-29 02:12:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2bds7u",
          "author": "Academic-Display3017",
          "text": "chatgpt",
          "score": 2,
          "created_utc": "2026-01-28 23:51:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bzhx4",
              "author": "WiggyWamWamm",
              "text": "To me this reads like OP wrote it and had Chat punch it up. And then Chat added too much formal organization.",
              "score": 6,
              "created_utc": "2026-01-29 01:48:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bmw7y",
              "author": "o5mfiHTNsH748KVq",
              "text": "it‚Äôs interesting how well formatted posts trigger us now. I currently have an llm organizing my thoughts from notes for a specification, yet I felt the same hypocrisy when I looked at this.",
              "score": 8,
              "created_utc": "2026-01-29 00:39:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2bu3ue",
                  "author": "YouAreTheCornhole",
                  "text": "It's not even well formatted, there's so many small anomalies",
                  "score": 1,
                  "created_utc": "2026-01-29 01:17:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2c949d",
              "author": "CorpusculantCortex",
              "text": "Given the content of the post, the least you could say is Kimi k2",
              "score": 1,
              "created_utc": "2026-01-29 02:40:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2cagsl",
          "author": "PropertyLoover",
          "text": "What the usage limits has ollama?",
          "score": 1,
          "created_utc": "2026-01-29 02:47:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cjthx",
              "author": "PuddleWhale",
              "text": "If you read his post there is a part where he says it's hard to even guess when you reach rate limiting. That means there is absolutely no SLA(service level agreement) if that term can be applied to LLM aggregators. \n\nPerhaps the reason everything works is because of all the people who pay $20 a month, enough of them do not use ollama cloud as much as OP and are thus subsidizing him. So it's possible hat ollama could be overselling their cloud offering, relying on their paying customers to underutilize the service. \n\nThe other possibility is that ollama cloud is part of the \"ai bubble\" and some angel investors decided to take a match and burn money to keep the ollama project warm and their $20/mo is actually a tactic to build loyal customors. \n\nI just got a github copilot pro plan that gives me access to gpt5-2 codex, claude sonnet 4.5, opus 4.5, grok code fast 4.1 and other variants of openai and gemini. It costs $10 a month and it is obviously not raw API access but I get to work with those models. Microsoft is pobably losing money right now on it but they are building followers and slowly improving their infrastructure and probably working on secret llm models we do not know about. I think every LLM provider is burning money like crazy so possibly so is llama. \n\nAnyway the point is that the real question one needs to ask oneself is how much money is the LLM aggregator or creator is brining to market, Although I absolutely love the idea of ollama existing and making local LLMs available to experiment with, I don't think they're spending money like water so there may not be that much value in this $20 a month compared to $10 a month, $20 and $30 a month with others.",
              "score": 2,
              "created_utc": "2026-01-29 03:41:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ckc04",
                  "author": "PropertyLoover",
                  "text": "Sorry, can‚Äôt see how much time in a day you can use it with Claude code , or with similar apps",
                  "score": 1,
                  "created_utc": "2026-01-29 03:44:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2cz3aq",
          "author": "desexmachina",
          "text": "They have Kimi k2.5 now too",
          "score": 1,
          "created_utc": "2026-01-29 05:20:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d32bw",
          "author": "Complete_Tough4505",
          "text": "Mah.",
          "score": 1,
          "created_utc": "2026-01-29 05:50:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ddibh",
          "author": "jruz",
          "text": "That‚Äôs pretty clear, the question is how do they compare vs OpenRouter, OpenCode Zen etc.",
          "score": 1,
          "created_utc": "2026-01-29 07:15:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dgpbz",
          "author": "aibot776567",
          "text": "I used it in December for a month. Cancelled after too many issues. Timeouts etc. The very basic privacy policy was another negative for me.",
          "score": 1,
          "created_utc": "2026-01-29 07:43:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dpj1a",
          "author": "jimmyfoo10",
          "text": "I wonder the same and finally I‚Äôm testing out Claude pro again due to they increase context and works quite nice. \n\nBut really like ollama filosophy and I self host ollama + openwebui \n\nQuestion. How exactly works ollama pro ? It‚Äôs a separate web app? Or I can use it locally and chat with the cloud models from my open web ui interface ?",
          "score": 1,
          "created_utc": "2026-01-29 09:04:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e1a7k",
          "author": "jasonhon2013",
          "text": "I love ollama so much but i just need to know how many inference i can do with ollama that's all",
          "score": 1,
          "created_utc": "2026-01-29 10:52:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e55i0",
          "author": "starkstaring101",
          "text": "This sounds pretty good, but what about the other things that ChatGPT does like searching the web. I find with a good 40% of my coding queries it has to do some searching. I'm pretty sure it can't do that right?",
          "score": 1,
          "created_utc": "2026-01-29 11:24:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2es8c2",
          "author": "siegevjorn",
          "text": "Dude said not affiliated with ollama, but formulates their respond with markdown thoroughly and goes quite defensive of it. \n\nAs a person who had no idea that ollama had a subscription, I call this is an AD backed by an agent.",
          "score": 1,
          "created_utc": "2026-01-29 13:52:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f1o2m",
          "author": "dobo99x2",
          "text": "Openrouter is above all of them and runs great in any ui.",
          "score": 1,
          "created_utc": "2026-01-29 14:41:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f62ax",
          "author": "CodigoTrueno",
          "text": "Openrouter exists, you know. Don't know enough of Ollama cloud, but... Openrouter is cavernous.",
          "score": 1,
          "created_utc": "2026-01-29 15:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2btmb3",
          "author": "donotfire",
          "text": "Idk why people are spending 30k or 10k or even 2k on a rig when it‚Äôs more that they‚Äôll ever spend getting a top tier subscription. Waste of money in my opinion. Buy index fund shares and have your money work for you instead.",
          "score": -1,
          "created_utc": "2026-01-29 01:15:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cnqar",
              "author": "Quiet-Translator-214",
              "text": "I was hitting daily limits on $200 Claude plan, GPT, Gemini, Perplexity etc. All of them top tiers - limited so badly that I couldn‚Äôt work - everyday same story‚Ä¶\nI have to admit that I was processing lot of data burning tokens fast, no proper RAG between sessions was even more annoying and eating tokens on explaining everything again to LLM.\nOf course we got later Claude Code and other CLI tools with direct access to filesystem - that was a breakthrough for any serious work.\nI‚Äôve built mentioned $10k PC.\nNo more limits.\nI‚Äôm happy running all my stuff including training of models, on my private, secure platform, where my code and ideas are not shared with corpos.\nNow I‚Äôm actually the owner of my own code.\nBest buy ever.\nOver last 3 years I could have 2 or 3 machines like that if not paying for cloud services.\nI have to say that you don‚Äôt have a clue what you are talking about.\nSad thing is that next year this PC will cost you $20k.\nI‚Äôm pretty sure all ‚ÄúJohnny come lately‚Äù persons will figure it out at some point that running own datacenter/homelab/server and staying out of paying for any subscriptions is the key (also Abliterated and uncensored models not that guardrailed crap we have right now available to public).\nIf it can‚Äôt be self hosted - it doesn‚Äôt exist at all.\n\nRegards",
              "score": 1,
              "created_utc": "2026-01-29 04:05:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2copdl",
                  "author": "donotfire",
                  "text": "If it saves you money then by all means",
                  "score": 1,
                  "created_utc": "2026-01-29 04:11:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2d3ogh",
          "author": "pstuart",
          "text": "Seems like having a collection of $20 subs might be better than a single $200 max plan? I'm sure there's tooling to glue this all together, but for a noob, does anybody have some recommendations? Best yet would be something that could also use local models at the front and then spill out to the cloud.",
          "score": 0,
          "created_utc": "2026-01-29 05:55:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dgn91",
              "author": "InfraScaler",
              "text": "You may be looking to develop your own AI gateway + router :)",
              "score": 2,
              "created_utc": "2026-01-29 07:43:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2djdu1",
              "author": "hokivpn",
              "text": "You mean an AI gateway ? Try LiteLLM proxy, super easy to run and getting started.",
              "score": 2,
              "created_utc": "2026-01-29 08:07:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2figeb",
                  "author": "pstuart",
                  "text": "Thanks! Currently using Claude and love it but want to leverage as many options as possible.",
                  "score": 1,
                  "created_utc": "2026-01-29 15:58:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qnersl",
      "title": "I built a \"Spatial\" website for Ollama because I hate linear chats. (Local-first, no DB)",
      "subreddit": "ollama",
      "url": "https://v.redd.it/utanig3onofg1",
      "author": "yibie",
      "created_utc": "2026-01-26 12:31:18",
      "score": 65,
      "num_comments": 11,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qnersl/i_built_a_spatial_website_for_ollama_because_i/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1tr7v8",
          "author": "General_Sandwich_353",
          "text": "what's the license? this is an interesting concept and I'd like to know if it's alright to reuse the code and under what terms",
          "score": 2,
          "created_utc": "2026-01-26 14:47:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ttx56",
              "author": "yibie",
              "text": "GPLv3.",
              "score": 5,
              "created_utc": "2026-01-26 15:00:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o24199p",
                  "author": "hopeseekr",
                  "text": "www.fuckgpl.com\n\nAnd, yes, after several years dead, I resurrected and updated that site.",
                  "score": 1,
                  "created_utc": "2026-01-27 23:16:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1tkbuu",
          "author": "zelkovamoon",
          "text": "This is a really cool idea I think. Nice work, I'll be trying it.",
          "score": 1,
          "created_utc": "2026-01-26 14:13:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ttulo",
              "author": "yibie",
              "text": "Thank you. LOL",
              "score": 1,
              "created_utc": "2026-01-26 15:00:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vieiy",
          "author": "ddoice",
          "text": "Great idea!",
          "score": 1,
          "created_utc": "2026-01-26 19:21:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xoqge",
              "author": "yibie",
              "text": "Thank you\n\n![gif](giphy|pAHAgWYYjWIE9DNLcC)",
              "score": 2,
              "created_utc": "2026-01-27 01:33:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vmav4",
          "author": "austrobergbauernbua",
          "text": "I really like it! It would be cool if it would arrange the notes automatically based on a similarity -> embedding clustering for example -> add a label. Only as an additional view.\n\n  \nCould also be used for writing Business Model Canvas or Lean Canvas for programming projects.\n\nKeep up the good work! Maybe I am motivated to contribute :)",
          "score": 1,
          "created_utc": "2026-01-26 19:38:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1za668",
              "author": "yibie",
              "text": "https://i.redd.it/3gcuy2s8lufg1.gif\n\nHi, do you mean this?",
              "score": 2,
              "created_utc": "2026-01-27 07:53:55",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1wvlj5",
              "author": "yibie",
              "text": "Thank you. This project was indeed designed from the beginning to allow users to talk directly to AI in an environment similar to Business Model Canvas.\n\nI find it interesting about the auto-arrange feature you mentioned, but I don't quite understand what this addtional view is, thanks for your attention.",
              "score": 1,
              "created_utc": "2026-01-26 23:02:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qkokhv",
      "title": "Ollama Image Generator",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qkokhv/ollama_image_generator/",
      "author": "nickinnov",
      "created_utc": "2026-01-23 11:40:26",
      "score": 49,
      "num_comments": 21,
      "upvote_ratio": 0.93,
      "text": "Hey fellow Ollama fans, I'm delighted that image generation is available so I have written a web app you can run on your own computer (alongside Ollama) to make it easier to generate, save and delete images.\n\nOK it ain't no ComfyUI but makes things tidier and, unlike using the terminal Ollama CLI, images don't clutter up your home folder!  \nRepo at [https://github.com/nicklansley/OllamaImageGenerator](https://github.com/nicklansley/OllamaImageGenerator)\n\n... but all you really need to download is:\n\n* [**server.py**](https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/server.py) \\- small proxying server which you can run without extra packages on your machine as long as you have python 3.9 or higher. No need for a venv as I've just used built-in packages like *http.server* \\- run from terminal as ***python3*** [***server.py***](https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/server.py)\n* [index.html](https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/index.html) \\- does all the grunt work in your web browser on port 8080. Generated images are saved to local storage along with generation settings, and can be deleted individually. Once [server.py](https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/server.py) is up and running, take your web browser to [http://localhost:8080](http://localhost:8080)\n\n[README.md](https://github.com/nicklansley/OllamaImageGenerator/blob/main/README.md) gives more info but quick instructions:\n\n* Download an image generation model in terminal - currently '***ollama pull x/z-image-turbo:bf16***' and '***ollama pull x/flux2-klein:latest***' are supported.\n* Type a prompt, set image width and height, choose a seed and the number of steps, then click 'Generate Image'.\n* During image progression, a 'step N of X' message appears to denote progress.\n* Images are saved to a side panel (actually they are in localStorage so they survive from one session to the next).\n* Save an image onto your machine with right-click 'Save Image As..' or drag if out of the main window and into a folder.\n* Double click a saved image to move it to the main window along with the settings that created it (the image can be recreated as long as seed 0 (Ollama internal random seed) was not used.\n* Single click an image then click 'x' to delete it. The image is removed from the image list saved in localStorage.\n\nWhen more image models become available:\n\n* Download a model with 'image pull image\\_gen\\_model\\_name:tag'\n* Update image list variable IMAGE\\_GEN\\_MODEL\\_LIST with the same model name and tag at the top of index.html\n\n[Screenshot of Ollama Image Generator](https://preview.redd.it/n8kap3a823fg1.png?width=1369&format=png&auto=webp&s=accb2258e20e62c356823ad8b85439aab8424f4a)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qkokhv/ollama_image_generator/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1867zs",
          "author": "vini_stoffel",
          "text": "Only in Mac?",
          "score": 3,
          "created_utc": "2026-01-23 12:26:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o187csu",
              "author": "simplir",
              "text": "Image generation on Ollama is still only Mac at the moment",
              "score": 3,
              "created_utc": "2026-01-23 12:34:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1cna9u",
                  "author": "FlyByPC",
                  "text": "Are there plans to add this to the Windows client anytime soon?",
                  "score": 1,
                  "created_utc": "2026-01-24 01:46:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o18aaup",
          "author": "planetearth80",
          "text": "Doesn‚Äôt the Ollama GUI do the same thing?",
          "score": 5,
          "created_utc": "2026-01-23 12:53:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18y01i",
              "author": "nickinnov",
              "text": "Not (yet) - it only understands /api/chat",
              "score": 7,
              "created_utc": "2026-01-23 15:00:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1bti1g",
          "author": "CoDMplayer_",
          "text": "Any image+prompt->image capability?",
          "score": 2,
          "created_utc": "2026-01-23 23:02:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e4utp",
              "author": "nickinnov",
              "text": "Yes I'd like that too - plus text encoders + loras! I suspect their curren work will be to make image gen work on Windows and Linux (Mac only right now). But hey they read this r/ollama stuff so hopefully they are creating a backlog that includes image+prompt->image too (please!).",
              "score": 2,
              "created_utc": "2026-01-24 08:01:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o182vmk",
          "author": "Narrow-Impress-2238",
          "text": "Is that work for windows too already?",
          "score": 1,
          "created_utc": "2026-01-23 12:03:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18yd0f",
              "author": "nickinnov",
              "text": "BY all means give it a go -  [server.py](http://server.py) will work fine as long as you have Python 3.9 or later, but you may need to check the Ollama app for Windows. At its heart [server.py](http://server.py) simply proxies the Ollama API (avoids CORS) plus has its own history saving of images capability.",
              "score": 2,
              "created_utc": "2026-01-23 15:01:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o18atuv",
          "author": "Birdinhandandbush",
          "text": "So why no quantized Z-Image models? that one is like 32gb. At least Klien will work for me",
          "score": 1,
          "created_utc": "2026-01-23 12:56:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18ykmq",
              "author": "nickinnov",
              "text": "Out of my control I'm afraid! Klein is very fast and works well (I am using Apple Metal GPU).",
              "score": 2,
              "created_utc": "2026-01-23 15:03:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o18zacd",
          "author": "nickinnov",
          "text": "UPDATE: Yes so soon used up local storage! Now saves to a history folder that [server.py](http://server.py) creates in its own folder instead. Who knew that web browser local storage can only save about 10MB per web site. Not me it seemed... üòë",
          "score": 1,
          "created_utc": "2026-01-23 15:06:31",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o190nlk",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-01-23 15:13:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o198s10",
              "author": "nickinnov",
              "text": "I agree - like much of Ollama it is simplified to work cleanly without any complexity. Comfy is absolutely at the opposite spectrum. However, I think it's useful to have a simple tool that can be used with little setup. And I think the images are pretty good for all that lack of ComfyUI fine tuning.",
              "score": 1,
              "created_utc": "2026-01-23 15:50:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o19g4tv",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-01-23 16:23:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o19dd35",
          "author": "Total-Context64",
          "text": "You should take a look at [ALICE](https://github.com/SyntheticAutonomicMind/ALICE), it's pretty advanced and can already do this without Ollama.  I'm curious though, what does Ollama add over just using the diffusion models directly?",
          "score": 1,
          "created_utc": "2026-01-23 16:11:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a8y11",
              "author": "nickinnov",
              "text": "For me it's about Ollama's simplicity (with this and all its models). It abstracts away model loading and inference for local 'offline' use. If anyone wants to be more serious about image generation then really Comfy-UI is really the best way to go.",
              "score": 2,
              "created_utc": "2026-01-23 18:34:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1abbw7",
                  "author": "Total-Context64",
                  "text": "Ahh, fair enough.  With ALICE you do need to set up the backend, ROCm or whatever but that's mostly automated.  I created it to interface with SAM so I can use AI assistants to generate images using old hardware that I just had sitting around.  It works very well, and it integrates with HuggingFace and CivitAI directly so you can download models right from the web interface.  It even has LoRA support.",
                  "score": 1,
                  "created_utc": "2026-01-23 18:44:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ytr0z",
                  "author": "ZeroSkribe",
                  "text": "This",
                  "score": 1,
                  "created_utc": "2026-01-27 05:39:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o19z2wl",
          "author": "Euphoric-Tank-6791",
          "text": "what OS? it does not seem to run on w11 wsl2 as of last night",
          "score": 1,
          "created_utc": "2026-01-23 17:50:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a9brw",
              "author": "nickinnov",
              "text": "MacOS only at present for image generation - a limitation imposed by Ollama itself. I'm sure they are working on Windows and Linux though. I suspect they are developing Ollama on Macs üòÅ",
              "score": 1,
              "created_utc": "2026-01-23 18:35:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qkrpnx",
      "title": "I gave my local LLM pipeline  a brain - now it thinks before it speaks",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qkrpnx/i_gave_my_local_llm_pipeline_a_brain_now_it/",
      "author": "danny_094",
      "created_utc": "2026-01-23 14:06:41",
      "score": 42,
      "num_comments": 7,
      "upvote_ratio": 0.87,
      "text": "Jarvis/TRION has received a major update after weeks of implementation. Jarvis (soon to be TRION) has now been provided with a self-developed SEQUENTIAL THINKING MCP.\n\nI would love to explain everything it can do in this Reddit post. But I don't have the space, and neither do you have the patience. u/frank_brsrk Provided a self-developed CIM framework That's hard twisted with Sequential Thinking. So Claude help for the answer:\n\nüß† Gave my local Ollama setup \"extended thinking\" - like Claude, but 100% local\n\nTL;DR: Built a Sequential Thinking system that lets DeepSeek-R1\n\n\"think out loud\" step-by-step before answering. All local, all Ollama.\n\nWhat it does:\n\n\\- Complex questions ‚Üí AI breaks them into steps\n\n\\- You SEE the reasoning live (not just the answer)\n\n\\- Reduces hallucinations significantly\n\nThe cool part: The AI decides WHEN to use deep thinking.\n\nSimple questions ‚Üí instant answer.\n\nComplex questions ‚Üí step-by-step reasoning first.\n\nBuilt with: Ollama + DeepSeek-R1 + custom MCP servers\n\nShoutout to u/frank_brsrk for the CIM framework that makes\n\nthe reasoning actually make sense.\n\nGitHub: [https://github.com/danny094/Jarvis/tree/main](https://github.com/danny094/Jarvis/tree/main)\n\nHappy to answer questions! This took weeks to build üòÖ\n\nOther known issues:\n\n\\- excessively long texts, skipping the control layer - Solution in progress\n\n\\- The side panel is still being edited and will be integrated as a canvas with MCP support.\n\n  \n\n\nhttps://reddit.com/link/1qkrpnx/video/zb6z5muax3fg1/player\n\nhttps://preview.redd.it/el6uhfy6q4fg1.png?width=1147&format=png&auto=webp&s=a16a9525fc50ba59b710f6932cdb3626c2562074\n\nhttps://preview.redd.it/j1ol6fy6q4fg1.png?width=863&format=png&auto=webp&s=f7726aee3e5079419dc665959fc0b779b6d37571\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qkrpnx/i_gave_my_local_llm_pipeline_a_brain_now_it/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o18q7ov",
          "author": "frank_brsrk",
          "text": "Here above is the architecture of the causal intelligence module that decouples thought and augments reasoning on demand. Thanks u/danny094 for the trust. \" Everyday is the day that the project should be finished\" :D\n\nhttps://preview.redd.it/cr7y0qimy3fg1.png?width=2800&format=png&auto=webp&s=3fa05b50978d7f2ea14c2ae7854c682db888331d\n\ncompliments and from the bottom of my heart's void all the best!!! :",
          "score": 6,
          "created_utc": "2026-01-23 14:20:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eoboz",
              "author": "Batinium",
              "text": "Pic Quality too low can't read. On what tool did you prepare them?",
              "score": 2,
              "created_utc": "2026-01-24 10:59:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1eurxj",
                  "author": "frank_brsrk",
                  "text": "On draw.io, I will pubblish soon the open source with polished version for n8n template. Otherwise send dm ur email and i will send u the diagram in higher quality",
                  "score": 1,
                  "created_utc": "2026-01-24 11:56:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1evb3j",
                  "author": "Awkward--Panda",
                  "text": "I guess it's readable here: https://www.reddit.com/r/LocalLLM/s/Hf6UO4Laeo\n\n(via Google picture search. I hope it reflects the actual content)",
                  "score": 1,
                  "created_utc": "2026-01-24 12:00:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1q603c",
          "author": "dropswisdom",
          "text": "I'm building the docker image locally now. However, I'd love a online maintained docker image. and please fix your discord server. it is invalid (from the git repo page).",
          "score": 1,
          "created_utc": "2026-01-26 00:29:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qa2v6",
          "author": "dropswisdom",
          "text": "Getting bad request from fastmcp (192.168.128.9:56052 - \"POST /mcp HTTP/1.1\" 400 Bad Request), communicates in german without an option to change interface language, and goes into a loop. oh, and can't use system prompt.. there's some (A LOT) of work to do.",
          "score": 1,
          "created_utc": "2026-01-26 00:49:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qzos9",
              "author": "danny_094",
              "text": "**In short, to avoid any further misunderstandings:** Jarvis is¬†**not**¬†intended to replace existing web UIs. Jarvis is a¬†**pipeline / bridge layer**¬†that sits between¬†**Ollama and any web UI**, allowing advanced routing, memory, agents, and processing. The¬†**Jarvis Web UI**¬†itself is an¬†**administration and control panel**, not a full chat UI, and will be expanded gradually over time. There will definitely be more bugs  this is an evolving system. I‚Äôm grateful to every user who runs into them and takes the time to report them. :)",
              "score": 1,
              "created_utc": "2026-01-26 02:58:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qnjk4p",
      "title": "SHELLper üêö: Qwen3 0.6B for More Reliable Multi-Turn Function Calling",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qnjk4p/shellper_qwen3_06b_for_more_reliable_multiturn/",
      "author": "gabucz",
      "created_utc": "2026-01-26 15:44:43",
      "score": 37,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "We fine-tuned a 0.6B model for converting English to executable bash commands. It's small enough to run locally on your laptop, giving you full data privacy.\n\nMulti-turn tool calling is incredibly challenging for small models - before fine-tuning, Qwen3-0.6B had 84% single-call accuracy, which collapses to **only 42% across 5 turns**! After our tuning, it reaches 100% on our test set, providing dependable multi-turn capabilities.\n\n|Model|Parameters|Tool call accuracy (test set)|=> 5-turn tool call accuracy|\n|:-|:-|:-|:-|\n|Qwen3 235B Instruct (teacher)|235B|99%|95%|\n|Qwen3 0.6B (base)|0.6B|84%|42%|\n|**Qwen3 0.6B (tuned)**|**0.6B**|**100%**|**100%**|\n\nRepo: [https://github.com/distil-labs/distil-SHELLper](https://github.com/distil-labs/distil-SHELLper)\n\nHuggingface model: [https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper](https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper)\n\n# Quick Start\n\nSet up the environment:\n\n    # Set up environment\n    python -m venv .venv\n    . .venv/bin/activate\n    pip install openai huggingface_hub\n    \n\nDowload the model:\n\n    hf download distil-labs/distil-qwen3-0.6b-SHELLper --local-dir distil_model\n    cd distil_model\n    ollama create distil_model -f Modelfile\n    cd ..\n    \n\nRun the assistant:\n\n    python filesystem_demo.py\n    \n\nThe demo asks for permission before executing commands (for safety) and restricts dangerous operations (like `rm -r /`), so don't hesitate to try it!\n\n# How We Trained SHELLper\n\n# The Problem\n\nMulti-turn tool calling is exceptionally hard for small models - performance breaks down as tool calls chain, degrading with each turn. If prediction errors are independent (e.g. due to bad parameter values), an 80% accurate model has just a 33% chance of succeeding across 5 turns.\n\n|Single tool call accuracy|5-turn tool call accuracy|\n|:-|:-|\n|80%|33%|\n|90%|59%|\n|95%|77%|\n|99%|95%|\n\nIn this demo, we explored whether we could substantially boost a small model's multi-turn performance. We picked a task from the [Berkeley function calling leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html) \\- the [gorilla file system tool calling task](https://github.com/ShishirPatil/gorilla/blob/main/berkeley-function-call-leaderboard/bfcl_eval/data/BFCL_v4_multi_turn_base.json). Our modifications were:\n\n* The original allows multiple tool calls per assistant turn ‚Üí we permit only one\n* Maximum 5 turns\n* Commands map to real bash (not gorilla filesystem functions)\n* Tool call outputs aren't added to the conversation history\n\nIn short, an identical tool set, but simpler [train/test data.](https://github.com/distil-labs/distil-SHELLper/tree/main/data)\n\n# Training Pipeline\n\n1. **Seed Data:** We wrote 20 simplified training conversations that span the available tools while remaining reasonably realistic.\n2. **Synthetic Expansion:** Through our [data synthesis pipeline](https://www.distillabs.ai/blog/small-expert-agents-from-10-examples/?utm_source=github&utm_medium=referral&utm_campaign=shellper), we scaled to thousands of examples.\n\nFor handling variable conversation lengths, we split each conversation into subsets ending with a tool call. For instance:\n\n    [Input] User: List all files => Model: ls -al => User: go to directory models\n    [Output] Model: cd models\n    \n\n... becomes 2 training points:\n\n    [Input] User: List all files\n    [Output] Model: ls -al\n    \n\n    [Input] User: List all files => Model: ls -al => User: go to directory models\n    [Output] Model: cd models`\n    \n\n1. **Fine-tuning:** We went with **Qwen3-0.6B** as the [best fine-tunable sub-1B](https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning) model on our platform that handles tool calling.\n\n# Usage Examples\n\nThe assistant takes natural language input, generates bash commands, and optionally runs them (after Y/N confirmation).\n\n**Basic filesystem operations**\n\n    > python filesystem_demo.py\n    \n    USER: List all files in the current directory\n    COMMAND: ls\n    USER: Create a new directory called test_folder\n    COMMAND: mkdir test_folder\n    USER: Navigate to test_folder COMMAND: cd test_folder\n    \n\n# Limitations and Next Steps\n\nRight now, we're limited to a basic bash tool set:\n\n* no pipes, compound commands, or multiple tool calls per turn\n* no validation of invalid commands/parameters\n* 5-turn conversation maximum\n\nWe prioritized getting the simple case right before expanding to more complex scenarios. Up next: support for multiple tool calls (enabling more sophisticated agent workflows) and benchmarking on [BFCL](https://gorilla.cs.berkeley.edu/leaderboard.html).\n\nIf you're using this in your bash workflows, track failing commands, append them to `data/train.jsonl`, and retrain with the updated dataset (or experiment with a larger student model!).\n\n# Discussion\n\nInterested to hear from the community:\n\n* Is anyone else working on fine-tuning small models for multi-turn tool calling?\n* What other \"narrow but valuable\" tasks could benefit from local, privacy-preserving models?\n\nLet us know your thoughts!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qnjk4p/shellper_qwen3_06b_for_more_reliable_multiturn/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1ufkep",
          "author": "sinan_online",
          "text": "Oh wow, I think I‚Äôll write a wrapper for this! Unless you did so already.",
          "score": 4,
          "created_utc": "2026-01-26 16:36:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ugi9e",
          "author": "sinan_online",
          "text": "I just want to add two things: I definitely believe that narrow-but-valuable is the way to go. Niche needs create businesses. The VC world has gone too far into this pointless philosophical chase into AGI.\n\nI also use Gemma3 270M for local testing, it works surprisingly well, so far. I haven‚Äôt gone into tool calling yet. It runs sufficiently fast without a GPU on the MacBook. Qwen3 1.7B is great on a years-old 6GB machine. So there is actually quite a bit of ‚Äúnarrow-but-valuable‚Äù possibilities for people who own a regular piece of equipment, if the implementation is done.",
          "score": 4,
          "created_utc": "2026-01-26 16:40:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1up3j9",
              "author": "gabucz",
              "text": "Yeah, I think local setup has a huge potential for small projects especially :) and not only that, I feel like you don't need a huge LLM for most narrow tasks\n\n\nGemma wasn't that great for function calling last time we tried it. But recently they released function gemma, we plan to try it out",
              "score": 2,
              "created_utc": "2026-01-26 17:17:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uxcin",
          "author": "mangoskive",
          "text": "I'm a total beginner and amateur, but I'm currently about to dive into fine-tuning Gemma 3 1b for exactly this, in a swarm of gemma 1b models. I've been thinking about maybe running three in parallel with temp variations, prompt variations or some other parameter tweaking for variation, with validation and judging by the same model. I was considering fine-tuning functiongemma, running it on the CPU, but I decided to work with only the 1b for easy vLLM setup and less complexity. I am pondering about how to fine tune, have been thinking about self-play maybe? I'm primarily just learning and trying to just have some fun with it. See the little agents at work.",
          "score": 2,
          "created_utc": "2026-01-26 17:53:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1za8ua",
              "author": "gabucz",
              "text": "Self-play is an interesting suggestion - we generated data via a much stronger model, but maybe you could make the small model progressively refine itself if the task is limited enough! I'd be curious to know how it went :) \n\nTool calling itself is a bit tough - earlier models (including gemma) weren't really pre-trained to do that, and so you mostly do it via instructions in the prompt. This could limit how good the model is for you, because it needs to learn both the tool calling format AND the task itself. That's why we went with the Qwen family, because these had tool calling in pre-training (and Llama 3.2 can also do it, although I'm not sure how well/if it does multi-turn tools).\n\nI'm not saying you can't do gemma, but if your performance isn't great, this could be one of the reasons. Qwen3 and Llama3.2 have vLLM integration too in case you want to try multiple families (or any particular reason why you want gemma? :) )",
              "score": 1,
              "created_utc": "2026-01-27 07:54:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1xmnm9",
          "author": "sinan_online",
          "text": "I want to try. Is there an easy way to get it running with Ollama? \n\nIf you say yes, I will proceed to use ChatGPT to do that.\n\nIf you say no, I will turn your venv into a container, expose an endpoint, and try it that way‚Ä¶",
          "score": 1,
          "created_utc": "2026-01-27 01:22:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xmtal",
              "author": "sinan_online",
              "text": "Never mind, I see it in the instructions!",
              "score": 1,
              "created_utc": "2026-01-27 01:23:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1z9l3e",
                  "author": "gabucz",
                  "text": "Nice! Reach out in case you need help or if anything's unclear in the instructions :)",
                  "score": 1,
                  "created_utc": "2026-01-27 07:48:38",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1z8mrg",
          "author": "BombardierComfy",
          "text": "Really clear and much needed!\nGreat post thx!",
          "score": 1,
          "created_utc": "2026-01-27 07:40:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21j2vi",
          "author": "cirejr",
          "text": "Hi, is it better than a fine-tuned functiongemma ?",
          "score": 1,
          "created_utc": "2026-01-27 16:34:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28bia1",
              "author": "gabucz",
              "text": "Hi, I checked out functiongemma now (including using its native template). It starts lower, only 12%, and reaches 95% - so it's tunable, but a bit worse.\n\nAccording to google they trained it only for single-turn & parallel tool calls, so this is not the best task for functiongemma: [https://ai.google.dev/gemma/docs/functiongemma/formatting-and-best-practices](https://ai.google.dev/gemma/docs/functiongemma/formatting-and-best-practices)\n\nInteresting that you can still make it somehow work however :)",
              "score": 2,
              "created_utc": "2026-01-28 15:38:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o28cld2",
                  "author": "cirejr",
                  "text": "Thanks for the stats. I guess I'll be moving forward with Qwen3 0.6B. I was indeed having troubles making it work with MCP and its multiple tools for example getting tools list from a Neon MCP, calling list_db, then calling db_tables then calling run_sql. I didn't try the fine-tuned version as I switched to another model. I'll be trying your approach thanks",
                  "score": 2,
                  "created_utc": "2026-01-28 15:43:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qlvud9",
      "title": "Mac Studio as host for Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qlvud9/mac_studio_as_host_for_ollama/",
      "author": "amgsus",
      "created_utc": "2026-01-24 19:10:24",
      "score": 27,
      "num_comments": 36,
      "upvote_ratio": 0.86,
      "text": "Hello,\n\nI‚Äôm wondering if it worth buying Mac Studio M4 Max (64 GB) for hosting Ollama. Does anyone have experience with this box? Or better to build a cluster of GPUs like RTX 3090, etc.?\n\nPrimarily, I will be using LLMs for coding. Rarely, for media content generation.\n\nKind regards",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qlvud9/mac_studio_as_host_for_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1h9z86",
          "author": "tom-mart",
          "text": "Define \"better\". What do you carr about? Raw power? 2 z 3090 will blow 64GB Mac out of the water. Ease of use? Mac will be miles easier to set up than multiple GPU's. Running cost? Mac wins again. Ability to fine tune, RTX takes the lead.",
          "score": 9,
          "created_utc": "2026-01-24 19:21:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hdbat",
              "author": "ZeroSkribe",
              "text": "There is no setup for multiple GPU's, it just works. Ollama has had this working out of the box for a while.",
              "score": 4,
              "created_utc": "2026-01-24 19:36:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1s4mpk",
                  "author": "cmk1523",
                  "text": "Power can get complicated unless you just go super big",
                  "score": 1,
                  "created_utc": "2026-01-26 07:38:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1hc986",
          "author": "Whospakka",
          "text": "If I say I‚Äôm running an Ollama on a Mac Mini M4 base config and it‚Äôs working awesome for an 8B model (plus a small embedding model at the same time), what do you think? üòÅ",
          "score": 4,
          "created_utc": "2026-01-24 19:31:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hewvv",
              "author": "CMPUTX486",
              "text": "Same here.. I did a bit more with Gemma 3 12b",
              "score": 5,
              "created_utc": "2026-01-24 19:43:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1kqfd0",
              "author": "rorowhat",
              "text": "even my phone can run a 8B models these days at good speeds, its not impressive.",
              "score": 3,
              "created_utc": "2026-01-25 06:36:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1jl52l",
          "author": "Popular-Diamond-9928",
          "text": "For folks using the Mac studios what is your time to first token? I‚Äôm curious at the optimizations you all are making. I am using Ollama and I‚Äôve seen posts below about llama.cpp which I‚Äôm seriously considering now lol. \n\nFor me, I have a classifier that classifies the query, determines if rag is needed, conversation is needed, and even additional context (I don‚Äôt use tool calling because it‚Äôs just faster for the routing to identify entities needed and then using sql to fetch)\n\nBut even when I‚Äôm controlling the token consumption I‚Äôm getting about 30seconds for my time to first token. \n\nI‚Äôm using Qwen2.5 8B or 7B I don‚Äôt remember \nAnd a small embedding model.",
          "score": 1,
          "created_utc": "2026-01-25 02:17:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jl7pl",
              "author": "Popular-Diamond-9928",
              "text": "I‚Äôm running my setup on a Mac mini base which has been amazing to setup.",
              "score": 1,
              "created_utc": "2026-01-25 02:17:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1kon7l",
          "author": "mhjor70",
          "text": "Yes i use it with a 32GB M3 and it works. It really depends on what you want to do with it and what models you want to load.",
          "score": 1,
          "created_utc": "2026-01-25 06:22:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kpcsx",
          "author": "Odd_Butterfly6003",
          "text": "In my case, i used 256gb ram m3 ultra.\nQwen3 30B took about 5 second per one request, but Qwen3 VL 235B model took 20 seconds per one request.",
          "score": 1,
          "created_utc": "2026-01-25 06:28:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kq8yl",
          "author": "rorowhat",
          "text": "Stick with Nvidia, way higher memory BW and compute power. Your TTFT and TS will be 2x of the Mac.\n\nM4 Max: Up to \\~546 GB/s  \nRTX 3090: \\~936 GB/s",
          "score": 1,
          "created_utc": "2026-01-25 06:35:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lyjy1",
          "author": "Weak_Ad9730",
          "text": "Have a m3u using Mlx-vllm and reality impressed of the Performance the Switch from lmstudio to vllm was a Hugh Jump in Processing time and Speed. I use my Studio in an Agent Zero setup. Realy recommend those Apple Silikon for llm work. My Go to Models are qwen3-vl-32b , got-oss-120b and minimax-m2.1",
          "score": 1,
          "created_utc": "2026-01-25 12:50:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nyyxf",
          "author": "bishopLucas",
          "text": "the best mac studio you can get is \"Apple M4¬†Max chip with 16‚Äëcore CPU, 40‚Äëcore GPU, 16‚Äëcore Neural¬†Engine\", mine has 64GB/1TB.\n\nIt is like MAGIC.  I came from a surface book laptop and didn't really appreciate how contained I was.\n\nI may create a cluster when ever the Ultra chip is released.\n\nEven at 64GB/1TB so many more possibilities are open to you.",
          "score": 1,
          "created_utc": "2026-01-25 18:36:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rreg3",
          "author": "Glum_Mistake1933",
          "text": "I run a 64 GB M4 Pro with 10 cores. Totally worth it. Models usually communicate using LM-Studio, works fine. If you have continued workload, the mac studio might be your choice, since it has better cooling.",
          "score": 1,
          "created_utc": "2026-01-26 05:53:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1shd2r",
          "author": "bharattrader",
          "text": "On my Mac Mini M4 Pro, (64 GB) I can able to run GLM4.7-Flash at Q4 using llama.cpp backend. Since I run parallel = 1 I can get 32K context for one client. I can run other models too, LM Stuido also and MLX is a little faster. I do not use Ollama.",
          "score": 1,
          "created_utc": "2026-01-26 09:32:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tilyi",
          "author": "Affectionate-Hat-536",
          "text": "Not Mac Studio, but Macbook pro max 64gb.\nI got both ollama and LM studio working and found GLM models quite useful for coding.\nIf you are in tech, you will move to llama.cpp easily which has got a UI as well.\nLlama.cpp will have better inference performance as compared to Ollama.",
          "score": 1,
          "created_utc": "2026-01-26 14:04:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25h809",
          "author": "RegularPerson2020",
          "text": "Get a 250 dollar mini pc and subscribe to Ollama cloud for $20 a month.  Better return on investment.",
          "score": 1,
          "created_utc": "2026-01-28 03:49:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dzrjd",
          "author": "jruz",
          "text": "Not worth it, you are better off with Ollama cloud subscription.\n\nYou can't run frontier models, you will be limited to distilled versions and with a gpu that ages by the day while models keep getting bigger.",
          "score": 1,
          "created_utc": "2026-01-29 10:38:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eee0p",
          "author": "wgnragency",
          "text": "I run medium-sized Ollama models on an M2 without issue so I think you would do well with the M4. If you can afford it, try to upgrade your VRAM to 120GB so you can run even larger models.",
          "score": 1,
          "created_utc": "2026-01-29 12:31:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1hgpqy",
          "author": "Le_mele",
          "text": "I highly recommend it, it works great even for models up to 32b",
          "score": 1,
          "created_utc": "2026-01-24 19:51:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1hd3cr",
          "author": "ZeroSkribe",
          "text": "Naw, you really need to get full GPU vram coverage, a cluster of 3050's would be way faster",
          "score": 0,
          "created_utc": "2026-01-24 19:35:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1hlc92",
          "author": "g_rich",
          "text": "I have a Mac Studio M4 Max, 64GB, 1TB which can easily run Devstral-Small-2-24B, Qwen3-Coder-30B and GLM-4.7-Flash; performance is acceptable but by no means fast. \n\nA cluster of RTX 3090‚Äôs will be faster and a better option if you intend on doing anything requiring CUDA. However that doesn‚Äôt mean it‚Äôs the better option for everyone, heat, power and likely cost will be higher made especially worse with the current GPU and RAM prices. \n\nAlso consider that you‚Äôll need at least 256-512GB of memory to get anywhere near Claude, Gemini or ChatGPT. So while you will get useful performance from the local models with 64GB and learn a lot you‚Äôll still be dependent on cloud hosted foundation models to do any serious work. \n\nOne other thing to consider the Mac Studio supports RDMA over Thunderbolt 5 which offers an interesting option to cluster Mac‚Äôs to run larger models; so it‚Äôs possible to start with one Mac Studio and add another down the road. There are a few videos on YouTube with Jeff Geerling doing a great job of demoing this capability.\n\nLastly don‚Äôt use Ollama, llama.cpp and AI Studio are both much better options.",
          "score": 0,
          "created_utc": "2026-01-24 20:13:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hybyd",
              "author": "BoostedHemi73",
              "text": "\\> Lastly don‚Äôt use Ollama, llama.cpp and AI Studio are both much better options.\n\nCan you say more on this? I've been so impressed with ollama on my M4 Max.",
              "score": 1,
              "created_utc": "2026-01-24 21:14:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1i5bwx",
                  "author": "g_rich",
                  "text": "The software itself is a ripoff of llama.cpp, they are not good stewards of open source and they are trying to lock users into their ecosystem. They have also had a few security vulnerabilities over the last few years and have shown little alarm or urgency to address them when they are discovered. \n\nllama.cpp is better but more complex, although not difficult to get up and running; especially when peered with llama-swap. It will also give you better performance and is overall a much better open source project. \n\nLM Studio is the easy choice if you are just looking for a quick and easy way to run local models.",
                  "score": 0,
                  "created_utc": "2026-01-24 21:47:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1i0ofj",
          "author": "Pale_Reputation_511",
          "text": "For Mac use mlx better performance and less ram usage",
          "score": 0,
          "created_utc": "2026-01-24 21:25:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ie2hn",
          "author": "No_Entertainer6253",
          "text": "Just dont",
          "score": 0,
          "created_utc": "2026-01-24 22:29:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ieggf",
              "author": "No_Entertainer6253",
              "text": "I bought 192gb studio, still paying 200$/ month to use cc",
              "score": 3,
              "created_utc": "2026-01-24 22:31:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1irn5z",
                  "author": "not-really-adam",
                  "text": "Have the 256GB M3 Ultra, and still pay $200/mo to claude. The local stuff just isn‚Äôt as good or as fast as the cloud. For privacy of personal transcriptions and other things you don‚Äôt want to leak to the cloud, it is really nice though. And, I think we are getting closer to being able to use Opus for planning and thinking and local models for research and execution which means either lower Claude bill, or ability to do loops for bug finding and/or optimizations.",
                  "score": 3,
                  "created_utc": "2026-01-24 23:39:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1iouh2",
                  "author": "AstroZombie138",
                  "text": "Have you tried GLM 4.7?  It works well for me.",
                  "score": 1,
                  "created_utc": "2026-01-24 23:24:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1iz9vz",
          "author": "rockinyp",
          "text": "Memory is more important than the CPU. Find a used M1 Mac Studio with more memory so you can run larger models. Running larger models means you'll often get better responses, which means you're more likely to use it. I have an M1 Ultra with 128 GB of memory running 30b+ models just fine for multiple users via Open WebUI.",
          "score": 0,
          "created_utc": "2026-01-25 00:18:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qoi1op",
      "title": "Tiny Ollama-powered CLI: Natural Language to shell commands (qwen2.5-coder default, easy model switch)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qoi1op/tiny_ollamapowered_cli_natural_language_to_shell/",
      "author": "ykushch",
      "created_utc": "2026-01-27 16:08:37",
      "score": 24,
      "num_comments": 11,
      "upvote_ratio": 0.93,
      "text": "CLI that uses Ollama locally to translate natural language into shell commands. Supports `--model` / `ASK_MODEL` and `OLLAMA_HOST`.  \nRepo: [https://github.com/ykushch/ask](https://github.com/ykushch/ask)\n\n[ask - natural language to shell commands](https://i.redd.it/k5ldp45d1xfg1.gif)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qoi1op/tiny_ollamapowered_cli_natural_language_to_shell/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o21gll7",
          "author": "j0x7be",
          "text": "Seems nice! Which shells does it support?",
          "score": 2,
          "created_utc": "2026-01-27 16:23:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21mr12",
              "author": "ykushch",
              "text": "Thanks! It executes commands via sh -c so your interactive shell (bash, zsh, fish, etc.) doesn't matter.",
              "score": 1,
              "created_utc": "2026-01-27 16:50:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o21q5xz",
                  "author": "j0x7be",
                  "text": "Cool, I have to give a try! Pretty comfortable bash user for years, but could be handy when I'm forced to use alternatives. \n\nAnd it's going to be awesome for those starting out, just running 'find' can be intimidating enough for some. Will recommend the apprentices at work to look into this.",
                  "score": 1,
                  "created_utc": "2026-01-27 17:05:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o22e0g1",
          "author": "Available-Craft-5795",
          "text": "Qwen2.5 is pretty old, I would recommend using Qwen3, its miles better",
          "score": 1,
          "created_utc": "2026-01-27 18:47:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22k0f7",
              "author": "ykushch",
              "text": "I picked up coder model as 2.5 had it in relatively small size. But the 3.0 doesn't have one per my memory hence decided to go with 2.5 more specialized one. Does the general 3.0 is a better one that 2.5 coder?",
              "score": 2,
              "created_utc": "2026-01-27 19:12:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o253e08",
                  "author": "shikima",
                  "text": "I have the same question ‚ùì I'm using qwen3 instruct but only for debug errors in journalctl, not for coder",
                  "score": 1,
                  "created_utc": "2026-01-28 02:33:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o24ycuh",
          "author": "tavigsy",
          "text": "I have the memory of a goldfish so this could really come in handy! ¬†Cool!",
          "score": 1,
          "created_utc": "2026-01-28 02:07:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b8oxo",
          "author": "dryEther",
          "text": "This is awesome.. :( I was mid way of building something very similar to this for windows. But the idea is out there now. This will be very useful and can completely change how people use terminals. Terminal will be accessible to.non technical.people also now.",
          "score": 1,
          "created_utc": "2026-01-28 23:25:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bb0j6",
              "author": "ykushch",
              "text": "Thanks! I believe that reading docs and googling now is becoming a bit dated hence decided to create a small handy helper. Also it's a lot easier than copy-paste from-to LLM.",
              "score": 2,
              "created_utc": "2026-01-28 23:37:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qochnc",
      "title": "Do ollama models access the internet?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qochnc/do_ollama_models_access_the_internet/",
      "author": "Fancy_Purchase_9400",
      "created_utc": "2026-01-27 12:28:07",
      "score": 21,
      "num_comments": 13,
      "upvote_ratio": 0.82,
      "text": "I am new to ollama and I just wanted to know if the models downloaded locally (like mistral:7b) access the internet at all while using them (even if it is for maintaing any kind of logs). I have noticed a small spike in network usage (both for upload and download) while using the model, but I'm not sure if it is due to the usage of local ollama model, so I'm curious to know if it actually accesses the internet quietly. If so, how do I restrict it completely from accessing the internet? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qochnc/do_ollama_models_access_the_internet/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o208g5c",
          "author": "thexdroid",
          "text": "Ollama can now run models on cloud, but if you download a model it will run exclusively offline. You can simple disconnect from your internet and give a try.",
          "score": 28,
          "created_utc": "2026-01-27 12:44:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20beaj",
          "author": "tomayt0",
          "text": "So a model is just a big plot of data points (vectors) it has no way to access the internet through some hidden code.\n\nThe inference engine, in this case Ollama (based off of llama.cpp) translates each token the model spits back to it and turns it into output.\n\nIf there was to be any network access you should check the inference engine that is running the model. Ollama has offline mode which will prevent it from access the internet.",
          "score": 15,
          "created_utc": "2026-01-27 13:02:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o216ckr",
          "author": "dsept",
          "text": "You can enable internet access in the admin panel. Some models have the ability to use web search but it needs to be enabled.",
          "score": 9,
          "created_utc": "2026-01-27 15:38:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22up5g",
              "author": "mlt-",
              "text": "How do I know which models? What keywords shall I look for?",
              "score": 1,
              "created_utc": "2026-01-27 20:00:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o23k09q",
                  "author": "andy2na",
                  "text": "pretty sure all models can - you have to enable websearch within openwebui or whatever you are using. I connect it to searchxng but brave search is another option\n\n  \n[https://docs.openwebui.com/category/web-search/](https://docs.openwebui.com/category/web-search/)",
                  "score": 2,
                  "created_utc": "2026-01-27 21:53:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29pxyk",
                  "author": "soowhatchathink",
                  "text": "There are certain models that are trained for using tools a certain way, they usually mention whether they support tools in the description. The main keyword is \"function calling\", but others might use \"tool calling\", \"mcp support\" or other variations.\n\nThey won't be able to just use tools out of the box with ollama though, you will need to set up an \"adapter\" or something else in front of it that will let it know what tools are available and parse tool calls from its output - and actually do the thing.\n\nAlso different models are trained to call tools differently, you can tell it to call tools with whatever format by modifying the system prompt but it's much more likely to get it right if you use the format it was trained on. Qwen models for example are trained for Hermes-style function calling, which looks like `<tool_call>{\"name\": \"{tool_name}\", \"args\": {...}}</tool_call>`, so you need something that will parse those tool calls from their response and call the tool. Other models are trained with a different format, like `[tool_call] ... [/tool_call]`\n\nI tested a bunch of models on their tool calling ability and found Qwen3 to be the best and most consistent.\n\nFor ollama in specific, I've found this works in front of Ollama for hermes https://github.com/jonigl/ollama-mcp-bridge\n\nYou'll have to configure what tools/mcps it can call. For web searches in particular you can look for open source web search mcps to locally host, or use an online-hosted web search mcp (has better results, usually paid, but some have free tiers.).\n\nThere are web UIs you can host yourself that give you a dashboard to configure models and mcps and such, that might be your best bet if you are ok using their dashboards.",
                  "score": 2,
                  "created_utc": "2026-01-28 19:16:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o22wavi",
                  "author": "dsept",
                  "text": "I have preferred gemma3:12b.",
                  "score": 1,
                  "created_utc": "2026-01-27 20:07:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o25hesw",
          "author": "I_Space_car",
          "text": "It can if you allow if you don't they don't.\n\nYou can also block the model internet block via firewall as well",
          "score": 1,
          "created_utc": "2026-01-28 03:50:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25rph8",
          "author": "Salty_Fee_06",
          "text": "If model had Cloud mentioned at the end, yes, it needs internet but other models don't.",
          "score": 1,
          "created_utc": "2026-01-28 04:53:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27a140",
          "author": "Glum_Mistake1933",
          "text": "It doesn't, you need a specific piece of software so that models would be able to access the internet. Like someone else in this thread I use openwebui to give it that ability.",
          "score": 1,
          "created_utc": "2026-01-28 12:19:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29fztr",
          "author": "woolcoxm",
          "text": "it has a cloud option, but other than that it should not be able to access the internet unless you have given it mcp access or tied it to an ide or something agentic??\n\nthere is also an option to enable it on local area network so that users on the network can access it.\n\nits possible its sending diagnostic information to ollama, not sure though.",
          "score": 1,
          "created_utc": "2026-01-28 18:33:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkcg6a",
      "title": "Boom, the greatest repo yet from Lucas Valbuena ( x1xhlol)",
      "subreddit": "ollama",
      "url": "https://i.redd.it/dspr44juxzeg1.jpeg",
      "author": "OriginalZebraPoo",
      "created_utc": "2026-01-23 00:57:26",
      "score": 18,
      "num_comments": 0,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qkcg6a/boom_the_greatest_repo_yet_from_lucas_valbuena/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qnpc7a",
      "title": "Vulkan vs ROCm on RX 9070XT (RDNA4): 9% faster, 50% less power",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qnpc7a/vulkan_vs_rocm_on_rx_9070xt_rdna4_9_faster_50/",
      "author": "Due_Pea_372",
      "created_utc": "2026-01-26 19:01:07",
      "score": 17,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "Benchmarked Ollama 0.15.1 with qwen3-coder:30b on my RX 9070 XT (gfx1201, 16GB VRAM).\n\n**Results:**\n\n|Metric|Vulkan|ROCm|Difference|\n|:-|:-|:-|:-|\n|Tokens/s|52.5|48.2|\\+8.9%|\n|Power|68 W|149 W|\\-54%|\n|VRAM|16.1 GB|15.8 GB|\\+2%|\n|TTFT|27.8 ms|22.9 ms|\\+21%|\n|Temp|51¬∞C|47¬∞C|\\+8%|\n\n**Key takeaway:** Vulkan is not only faster but dramatically more power efficient on RDNA4. ROCm draws 2x the power for worse performance.\n\n**Setup:** Cachyos, ollama-vulkan / ollama-rocm from AUR, \\~35 runs each.\n\nScript used for benchmarking: [https://github.com/maeddesg/spielwiese](https://github.com/maeddesg/spielwiese)\n\nAnyone else seeing similar results on RDNA4? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qnpc7a/vulkan_vs_rocm_on_rx_9070xt_rdna4_9_faster_50/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1vz57t",
          "author": "Nerdinat0r",
          "text": "RemindMe! Tomorrow",
          "score": 1,
          "created_utc": "2026-01-26 20:34:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vzanx",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 day on [**2026-01-27 20:34:52 UTC**](http://www.wolframalpha.com/input/?i=2026-01-27%2020:34:52%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/ollama/comments/1qnpc7a/vulkan_vs_rocm_on_rx_9070xt_rdna4_9_faster_50/o1vz57t/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Follama%2Fcomments%2F1qnpc7a%2Fvulkan_vs_rocm_on_rx_9070xt_rdna4_9_faster_50%2Fo1vz57t%2F%5D%0A%0ARemindMe%21%202026-01-27%2020%3A34%3A52%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qnpc7a)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-26 20:35:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1vzhcq",
              "author": "Nerdinat0r",
              "text": "I don‚Äòt have time right now but I have a 9070XT and currently use ollama-rocm. Let me try some things tomorrow and I will report back :)",
              "score": 1,
              "created_utc": "2026-01-26 20:36:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wisxb",
          "author": "Badger-Purple",
          "text": "prompt processing is slower.",
          "score": 1,
          "created_utc": "2026-01-26 22:02:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yxg2n",
          "author": "AwayLuck7875",
          "text": "–ï—Å–ª–µ –±—Ä–∞—Ç—å —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —ç–ª–µ–∫—Ç—Ä–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ –≤—É–ª–∫–∞–Ω –≥–æ—Ä–∞–∑–¥–æ —ç—Ñ–µ–∫—Ç–∏–≤–Ω–µ–π",
          "score": 1,
          "created_utc": "2026-01-27 06:07:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zqyra",
          "author": "AvocadoParticular619",
          "text": "These Power\\\\Temp numbers make no sense",
          "score": 1,
          "created_utc": "2026-01-27 10:28:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29ckyp",
              "author": "Due_Pea_372",
              "text": "Good catch!   \nA few clarifications:   \n1. Runs were short (\\~25 sec each), so thermal equilibrium wasn't reached. ROCm would likely run hotter in sustained workloads.   \n2. I'm measuring edge temp (temp1\\_input), not junction. Junction is typically 10-20¬∞C higher.",
              "score": 1,
              "created_utc": "2026-01-28 18:19:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o258qgd",
          "author": "p_235615",
          "text": "I have similar experience with ollama-vulkan, was using it on a RX9060XT, my older RX6800 and also on a Intel A380. Prompt tokens were usually slightly slower, but its often still very fast, and the main issue on these lower end cards is the main inference t/s. Two of my most used models are ministral-3:14b and gpt-oss:20b and most of the time, the prompt processing is well under a second, then you waiting quite long time to actually get some response stream in...",
          "score": 1,
          "created_utc": "2026-01-28 03:01:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26uvhi",
          "author": "wesmo1",
          "text": "What rocm version and quantization of qwen-coder-30b  are you using?",
          "score": 1,
          "created_utc": "2026-01-28 10:19:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27f5fs",
              "author": "Due_Pea_372",
              "text": "ollama show qwen3-coder:30b | grep -i quant  \n¬†¬†¬†**quant**ization ¬†¬†¬†¬†¬†¬†¬†Q4\\_K\\_M ¬†¬†¬†¬†¬†  \n  \nrocminfo | grep -i \"version\"  \nRuntime **Version**: ¬†¬†¬†¬†¬†¬†¬†¬†1.18  \nRuntime Ext **Version**: ¬†¬†¬†¬†1.14",
              "score": 1,
              "created_utc": "2026-01-28 12:53:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2e8oqg",
          "author": "Dtounlion",
          "text": "Thanks for this.\nI have the same card, I'll give it a try this weekend",
          "score": 1,
          "created_utc": "2026-01-29 11:51:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmteov",
      "title": "Claraverse is not dead, now you can use AI with more fun, more productivity, and more PRIVACY.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/gallery/1qmteov",
      "author": "aruntemme",
      "created_utc": "2026-01-25 19:52:48",
      "score": 15,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qmteov/claraverse_is_not_dead_now_you_can_use_ai_with/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1ol7id",
          "author": "Available-Craft-5795",
          "text": "TL;DR:  \nVibe coded N8N clone but you dont do any of the work",
          "score": 1,
          "created_utc": "2026-01-25 20:13:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1p8q3t",
          "author": "immediate_a982",
          "text": "Has it been cyber attack pentested",
          "score": 1,
          "created_utc": "2026-01-25 21:55:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmmp22",
      "title": "Claude Code stuck on <function=TaskList> when using Ollama + Qwen3-Coder",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qmmp22/claude_code_stuck_on_functiontasklist_when_using/",
      "author": "Healthy-Laugh-6745",
      "created_utc": "2026-01-25 15:52:04",
      "score": 15,
      "num_comments": 16,
      "upvote_ratio": 0.94,
      "text": "Hi everyone,\n\nI'm struggling to get Claude Code working with Ollama on my Mac M4 Max (48GB RAM). I strictly followed the official Ollama integration guide (https://docs.ollama.com/integrations/claude-code), but I'm stuck in a loop.\n\nEvery time I ask the model to perform a file-based task (e.g., \"create a txt file\"), the process hangs indefinitely.\n\nThe model acknowledges the request.\n\nIt outputs: ‚ùØ <function=TaskList> ‚è∫\n\nNothing happens after that. No file is created, and the terminal just sits there with the \"active\" dot.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qmmp22/claude_code_stuck_on_functiontasklist_when_using/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1ngiam",
          "author": "Outrageous_Rub_6527",
          "text": "Hey! Bump up your context length before running claude code with Ollama - https://docs.ollama.com/context-length",
          "score": 4,
          "created_utc": "2026-01-25 17:19:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1oy32w",
              "author": "jmorganca",
              "text": "This. Sorry it's not obvious right now - we're working on improving this so context length size automatically grows (up to an acceptable amount on your hardware)",
              "score": 1,
              "created_utc": "2026-01-25 21:09:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1nnebn",
              "author": "Healthy-Laugh-6745",
              "text": "correct! thanks a lot at least 64k. sorry i'm a newbie :)",
              "score": 1,
              "created_utc": "2026-01-25 17:49:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1n4fds",
          "author": "paq85",
          "text": "I would be really surprised to see Claude Code work with model like Qwen3 Coder 30B... but, please let me know if you manage to make it work... from my experience those local models are way too limited.",
          "score": 3,
          "created_utc": "2026-01-25 16:27:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1p4kil",
              "author": "nunodonato",
              "text": "I've used it with a 4B model :D",
              "score": 2,
              "created_utc": "2026-01-25 21:37:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1vtedu",
                  "author": "paq85",
                  "text": "I really doubt it could do anything, even at a level of tic tac toe game.\n\nWhat model was that?",
                  "score": 0,
                  "created_utc": "2026-01-26 20:09:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1n62m3",
              "author": "Healthy-Laugh-6745",
              "text": "Thanks! According to the Ollama docs, open models can be used with Claude Code via Ollama‚Äôs Anthropic-compatible API (for example, glm-4.7, qwen3-coder, or gpt-oss).\nAre you saying I should try a different model?",
              "score": 1,
              "created_utc": "2026-01-25 16:34:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1npe3p",
                  "author": "paq85",
                  "text": "I'm afraid there's no local model up to 30B that's good enought for tools like Claude Code, Cline etc... [Has anyone got GLM 4.7 flash to not be shit? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/)\n\nHere's GLM 4.7 Flash with Cline (VS Code) struggling to provide a simple answer... \n\nhttps://preview.redd.it/wb9yr8j3bjfg1.png?width=667&format=png&auto=webp&s=0195981d970e6235018e7efe9ee4028f89c0718e",
                  "score": 1,
                  "created_utc": "2026-01-25 17:57:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1n6qk2",
          "author": "960be6dde311",
          "text": "Have you tried using OpenCode CLI instead? I love the concept of running AI models locally, and have done a fair amount of experimentation, but they often have \"issues\" ranging from:\n\n* Hanging on generating responses\n* Infinite MCP tool calling loops\n* Spending a disproportionate amount of time \"thinking\" (reasoning)\n* Garbling tool calls or responses\n\nYou really have to find a client tool that has been **thoroughly tested** with the specific model you're using. Without adequate testing and bug fixes, there's a reasonably high probability you'll run into some kind of issue.\n\nI'm sure we will see these kinds of issues get worked out over time, but for now, you'll need to spend time with trial and error to see what works and what doesn't.",
          "score": 1,
          "created_utc": "2026-01-25 16:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1pgtqb",
          "author": "Savantskie1",
          "text": "This is a bit of a noobish question but we have to establish competency. Have you given it the tools you‚Äôre trying to get it to use? Ollama doesn‚Äôt automatically make it an AI that can do everything",
          "score": 1,
          "created_utc": "2026-01-25 22:31:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s66yz",
              "author": "Healthy-Laugh-6745",
              "text": "correct it was a newbie question, i'm learning the ollama and Ai. the problem was the context",
              "score": 1,
              "created_utc": "2026-01-26 07:52:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1yma2u",
                  "author": "Savantskie1",
                  "text": "Ok, yeah you need to build the tools or download tools and set them up with an internal API that the model can call. Or use a platform like lm studio or OpenWebUi or a frontend that can import the tools. Then the model can use tool calling if it‚Äôs built in with the model and it can call those tools so long as they‚Äôre broadcast to the model that they‚Äôre there. But until then all you can do with ollama is chat with them",
                  "score": 1,
                  "created_utc": "2026-01-27 04:47:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1qdsfu",
          "author": "Embarrassed_Sun_7807",
          "text": "Post settings or we can't help really.¬†",
          "score": 1,
          "created_utc": "2026-01-26 01:07:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s5ym1",
              "author": "Healthy-Laugh-6745",
              "text": "the problem was the context, thanks bro",
              "score": 1,
              "created_utc": "2026-01-26 07:50:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qlssc0",
      "title": "HashIndex: An alternative to a page that doesn't require RAG but can still perform indexing well.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qlssc0/hashindex_an_alternative_to_a_page_that_doesnt/",
      "author": "jasonhon2013",
      "created_utc": "2026-01-24 17:19:08",
      "score": 13,
      "num_comments": 5,
      "upvote_ratio": 0.94,
      "text": "The Pardus AI team has decided to open source our memory system, which is similar to PageIndex. However, instead of using a B+ tree, we use a hash map to handle data. This feature allows you to parse the document only once, while achieving retrieval performance on par with PageIndex and significantly better than embedding vector search. It also supports Ollama. Give it a try and consider implementing it in your system ‚Äî you might like it! Give us a star maybe hahahaha\n\n[ https://github.com/JasonHonKL/HashIndex/tree/main ](https://github.com/JasonHonKL/HashIndex/tree/main)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qlssc0/hashindex_an_alternative_to_a_page_that_doesnt/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1gncij",
          "author": "immediate_a982",
          "text": "Your sample code does to show Ollama usage",
          "score": 2,
          "created_utc": "2026-01-24 17:44:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1gnq4w",
              "author": "jasonhon2013",
              "text": "Hi all you need is to change the base url in the .env file ! Hope you enjoy it",
              "score": 2,
              "created_utc": "2026-01-24 17:46:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ha2to",
          "author": "zair",
          "text": "Does this have value for other structured document types, eg spreadsheets? And is the benefit mainly for very long documents or also large numbers of short documents?",
          "score": 1,
          "created_utc": "2026-01-24 19:22:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jv624",
              "author": "jasonhon2013",
              "text": "Ah it‚Äôs mainly focusing on long context documents",
              "score": 2,
              "created_utc": "2026-01-25 03:12:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sjsnf",
          "author": "numbworks",
          "text": "The idea is very interesting, but I have two questions:\n\n1. Does it support PDFs with tables inside?\n2. Does it support other file formats such as Markdown or Asciidoc?",
          "score": 1,
          "created_utc": "2026-01-26 09:55:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmlc8p",
      "title": "My AI Open Source Workflow",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qmlc8p/my_ai_open_source_workflow/",
      "author": "candidosales",
      "created_utc": "2026-01-25 15:01:05",
      "score": 12,
      "num_comments": 1,
      "upvote_ratio": 0.88,
      "text": "Lately, I have been studying AI and Open Source Workflows. I thought it would be interesting to share a bit of what I'm learning: [https://www.candidosales.me/blog/my-ai-open-source-workflow](https://www.candidosales.me/blog/my-ai-open-source-workflow)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qmlc8p/my_ai_open_source_workflow/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1y2pz1",
          "author": "PropertyLoover",
          "text": "Good job!",
          "score": 1,
          "created_utc": "2026-01-27 02:50:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkl6rr",
      "title": "GitHub - FlorinAndrei/pipe-llama: Put an LLM in your shell scripts and command-line pipelines. Dead simple.",
      "subreddit": "ollama",
      "url": "https://github.com/FlorinAndrei/pipe-llama",
      "author": "florinandrei",
      "created_utc": "2026-01-23 08:14:30",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qkl6rr/github_florinandreipipellama_put_an_llm_in_your/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o196rdm",
          "author": "smith288",
          "text": "Pretty slick. Could you throw in an ascii progress while it makes the request?",
          "score": 3,
          "created_utc": "2026-01-23 15:41:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dob7e",
              "author": "florinandrei",
              "text": "It's meant to use small, fast models, doing relatively simple jobs with short or very short prompts. It's not meant for inference jobs that take a long time. Thinking is disabled by default. It has a warm-up mode. It has a one-line mode. I did everything I could think of to make it go faster.\n\nYou can always put `pv` in front of it in the pipeline, if you need a progress bar for a large file where many inference requests are issued to the endpoint for that one file. I'll add it to the examples.",
              "score": 2,
              "created_utc": "2026-01-24 05:40:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1eyx9t",
                  "author": "smith288",
                  "text": "All fair. But it still has delay especially when my ollama server is on another computer. Just spitting out an idea.",
                  "score": 1,
                  "created_utc": "2026-01-24 12:29:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1bphjy",
              "author": "1800not4you",
              "text": "+1 to this",
              "score": 1,
              "created_utc": "2026-01-23 22:41:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qnpy7y",
      "title": "Fine tuning open models and prep for robust deployment",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qnpy7y/fine_tuning_open_models_and_prep_for_robust/",
      "author": "codes_astro",
      "created_utc": "2026-01-26 19:21:32",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I was at a tech event recently and lots of devs mentioned about problem with ML projects, and most common was deployments and production issues.\n\n**note:**¬†I'm part of the KitOps community\n\nTraining a model is crucial but usually the easy part due to tools like Unsloth and lots of other options. You fine-tune it, it works, results look good. But when you start building a product, everything gets messy:\n\n* model files in notebooks\n* configs and prompts not tracked properly\n* deployment steps that only work on one machine\n* datasets or other assets are lying somewhere else\n\nEven when training is clean, moving the model forward feels challenging with real products.\n\nSo I tried a full¬†train ‚Üí push ‚Üí pull ‚Üí run¬†flow to see if it could actually be simple.\n\nI fine-tuned a model using¬†Unsloth.\n\nIt was fast, becasue I kept it simple for testing purpose, and ran fine using official cookbook. Nothing fancy, just a real dataset and a IBM-Granite-4.0 model.\n\nTraining wasn‚Äôt the issue though. What mattered was what came next.\n\nInstead of manually moving files around, I pushed the fine-tuned model to¬†Hugging Face, then imported it into¬†Jozu ML. Jozu treats models like proper versioned artifacts, not random folders.\n\nFrom there, I used¬†KitOps¬†to pull the model locally. One command and I had everything - weights, configs, metadata in the right place.\n\nAfter that, running inference or deploying was straightforward.\n\n**Now, let me give context on why Jozu or KitOps?**\n\n\\- Kitops is only open-source AIML tool for packaging and versioning for ML and it follows best practices for Devops while taking care of AI usecases.\n\n\\- Jozu is enterprise platform which can be run on-prem on any existing infra and when it comes to problems like hot reload and cold start or pods going offline when making changes in large scale application, it's 7x faster then other in terms of GPU optimization.\n\n**The main takeaway for me:**\n\nMost ML pain isn‚Äôt about training better models.  \nIt‚Äôs about¬†keeping things clean at scale.\n\nUnsloth made training easy.  \nKitOps kept things organized with versioning and packaging.  \nJozu handled production side things like tracking, security and deployment.\n\nI wrote a detailed article¬†[here](https://mranand.substack.com/p/from-training-to-deployment-push).",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qnpy7y/fine_tuning_open_models_and_prep_for_robust/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1vimrq",
          "author": "Creepy-Row970",
          "text": "this is honestly a great read! i have played around with KitOps myself, and it is a powerful tool and completely OSS",
          "score": 3,
          "created_utc": "2026-01-26 19:22:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlw48b",
      "title": "‚ÄúLocal‚ÄØLLHAMA‚Äù project - Orchestration Middleware for Ollama and HomeAssistant",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qlw48b/local_llhama_project_orchestration_middleware_for/",
      "author": "NicolaZanarini533",
      "created_utc": "2026-01-24 19:20:31",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.91,
      "text": "**What‚Äôs the problem/why build this?**\n\nIn a world where, to cite some other posts, the \"enshittification of AI\" is a trend, having the ability to run effective AI systems locally, even on modest hardware, becomes more and more important. This of course comes with its own problems, which this project aims to address.\n\nThe main idea here is that raw model size isn‚Äôt the blocker for smart‚Äëhome control and smart-home assistants ‚Äì it‚Äôs¬†*routing & context*.\n\nTypical setups struggle with:\n\n* Multi‚Äëintent utterances (e.g., ‚Äúturn off lights AND set alarm AND check weather‚Äù)\n* Exact device names / lack of fuzzy/multi‚Äëlang matching\n* Base‚Äëprompt control & external‚Äëdata integration\n* Conversation memory & user/system management\n* Working without cloud APIs\n\n**What I‚Äôm building**\n\nAn¬†**orchestration middleware**¬†that sits¬†*between Home Assistant and Ollama*:\n\n* Decomposes intents in parallel\n* Routes each to the right backend (HA API, PostgreSQL, weather API, etc.)\n* Injects only the needed context\n* Auto‚Äëscales the prompt window\n* Synthesizes a single, natural‚Äëlanguage reply\n* Uses memory to include previous conversation\n\nResult: 2‚Äì5‚ÄØs for multi‚Äëintent commands; sub‚Äëminute even with web searches ‚Äì all offline.\n\n**Hardware‚Äëvalidated presets**\n\n|**VRAM**|**Model**|**Languages**|\n|:-|:-|:-|\n||||\n|**8‚ÄØGB**|Qwen2.5‚Äë8B|English only|\n|**16‚ÄØGB**|Qwen2.5‚Äë14B|6+ languages|\n|**24‚ÄØGB**|GPT‚ÄëOSS‚Äë20B|6+ languages|\n\nTested on:\n\n\\-¬†¬†¬†¬†¬†¬†¬†¬†¬† Xeon‚ÄØE5‚Äë2640‚ÄØv4‚ÄØ+‚ÄØRTX‚ÄØ4060‚ÄØTi‚ÄØ16‚ÄØGB\n\n\\-¬†¬†¬†¬†¬†¬†¬†¬†¬† i7‚Äë12700H‚ÄØ+‚ÄØRTX‚ÄØ4060‚ÄØ8‚ÄØGB (mobile)\n\n\\-¬†¬†¬†¬†¬†¬†¬†¬†¬† Xeon‚ÄØE5‚Äë2640‚ÄØv4‚ÄØ + RTX‚ÄØ2080‚ÄØTi + Ollama VM with‚ÄØRTX‚ÄØ4060‚ÄØTi‚ÄØ16‚ÄØGB.\n\n**Example commands (single utterance)**\n\n* ‚ÄúTurn off the kitchen light, set my 7‚ÄØam alarm and tell me the weather for tomorrow‚Äù\n* ‚Äú¬øCu√°les son las noticias de Par√≠s? ¬øQu√© lugares interesantes hay para ver all√≠?¬†‚Äù\n* ‚ÄúRappelle‚Äëmoi d‚Äôaller √† l‚ÄôAlexanderplatz demain ‚Äì comment devrais‚Äëje m‚Äôhabiller‚ÄØ? Aussi r√®gle le thermostat √† 22‚ÄØ¬∞C¬†‚Äù\n* ‚ÄúSpegni la luce della cucina e parlami di Roma‚Äù\n\nThe system auto‚Äëdetects language, fuzzy‚Äëmatches entities, and calls the appropriate functions.\n\n**Architecture highlights**\n\n* Multi‚Äëpass prompt engineering (base ‚Üí decision ‚Üí safety ‚Üí format)\n* Adaptive context windows\n* Parallel backend routing (HA + PostgreSQL + web APIs)\n* Reflection‚Äëbased function discovery\n* Per‚Äëuser conversation memory\n* Zero‚Äëcloud, privacy‚Äëfirst (no telemetry)\n\n**Tech stack**\n\n* Python‚ÄØ3.10+ (3.12 recommended)\n* Ollama (any model; Qwen2.5 / GPT‚ÄëOSS tested)\n* Home Assistant (local or remote)\n* PostgreSQL (history + embeddings)\n* OpenWakeWord + Whisper + Piper TTS\n* Flask + WebSocket chat UI\n\nOne‚Äëcommand setup with an interactive wizard.\n\n**Potential Other Uses**\n\nThe base structure of the project allows creating RAG-enhanced assistants, integrating with other systems and in general having full control over an Ai assistant that runs locally, but which can perform close to cloud solutions. I've used it to create a translation bot, a URS analysis bot, and many others.\n\n**License & repo**\n\n* CC‚ÄØBY‚ÄØ4.0\n* GitHub:¬†[https://github.com/Nemesis533/Local\\_LLHAMA](https://github.com/Nemesis533/Local_LLHAMA)\n\nThe project had started a while back but after the recent trends in \"public AI\", has evolved to the state it is in today - happy to answer questions and get your feedback!\n\nhttps://preview.redd.it/q4oyj5hhlcfg1.png?width=1919&format=png&auto=webp&s=7140dd5fe64945e4622f108184ae27792a3e1731\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qlw48b/local_llhama_project_orchestration_middleware_for/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    }
  ]
}