{
  "metadata": {
    "last_updated": "2026-02-23 03:09:58",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 20,
    "total_comments": 106,
    "file_size_bytes": 139721
  },
  "items": [
    {
      "id": "1rajqj6",
      "title": "15,000+ tok/s on ChatJimmy: Is the \"Model-on-Silicon\" era finally starting?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/gallery/1rajqj6",
      "author": "Significant-Topic433",
      "created_utc": "2026-02-21 06:19:08",
      "score": 440,
      "num_comments": 103,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6khfgk",
          "author": "Pantoffel86",
          "text": "Guess we'll start using kilotokens then. \n\n15ktok/s",
          "score": 88,
          "created_utc": "2026-02-21 07:53:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ko51t",
              "author": "JohnnyLovesData",
              "text": "Maybe even Trillion Kilo tokens. A TiKtok, if you will.",
              "score": 81,
              "created_utc": "2026-02-21 08:59:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6kreiv",
                  "author": "insanemal",
                  "text": "![gif](giphy|ac7MA7r5IMYda)",
                  "score": 53,
                  "created_utc": "2026-02-21 09:32:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6lnsh4",
                  "author": "KaMaFour",
                  "text": "So glad SI solved this problem before it could appearÂ ",
                  "score": 7,
                  "created_utc": "2026-02-21 14:01:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6kpybt",
                  "author": "Pantoffel86",
                  "text": "I like your thinking, but wouldn't a trillion kilo tokens be a petatok?",
                  "score": 4,
                  "created_utc": "2026-02-21 09:17:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6lt76n",
              "author": "Nzkx",
              "text": "Or approximately 60 kilobytes per seconds (assuming a token is on average 4 byte).",
              "score": 1,
              "created_utc": "2026-02-21 14:33:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6qupub",
              "author": "sfscsdsf",
              "text": "or ktps",
              "score": 1,
              "created_utc": "2026-02-22 09:20:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6p15qd",
              "author": "OilTechnical3488",
              "text": "I created https://github.com/0xMH/chatjimmy-api",
              "score": 0,
              "created_utc": "2026-02-22 00:52:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6qosd1",
                  "author": "Significant-Topic433",
                  "text": "From chatjimmy team about api\n\nhttps://preview.redd.it/pyw62d59a0lg1.jpeg?width=720&format=pjpg&auto=webp&s=975ea42613688bc0770863e21ae4eea2b5bd0084",
                  "score": 3,
                  "created_utc": "2026-02-22 08:23:28",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6kjpcj",
          "author": "MrPhatBob",
          "text": "I think that an ASIC will need a pretty well defined model that you're happy provides you with sensible responses, with the speed that models are being released at the moment you may find that you have a reel of outdated models fused on the silicon within a month or so.\n\nYou could do something with an FPGA accelerator at Â£15k + development time, but you'd need to be up on the tools and technology. There are a few start-ups in my area who are looking to get AI on silicon, so the era is showing signs of life.\n\nhttps://www.mouser.co.uk/ProductDetail/BittWare/IA-860m-0037?qs=%252BICfH0Hx1eTAUbkQBMisxA%3D%3D",
          "score": 44,
          "created_utc": "2026-02-21 08:15:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lcwmt",
              "author": "Sixstringsickness",
              "text": "FPGA was exactly where my mind went as well.Â  Hardcoding a model into silicon, while an amazing proof of concept seems unfeasible for the rate these models are evolving.Â  By the time you are able to scale the number required for concurrency you would be very far behind.Â Â ",
              "score": 18,
              "created_utc": "2026-02-21 12:48:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lobeq",
                  "author": "KaMaFour",
                  "text": "I don't see why silicon version of a model released now and delivered in 2 months (if their claims are true) wouldn't be useful for the next year or maybe longer if it can be served for virtually free to almost anyone. It could realistically replace \"fast\" stupider model tiers",
                  "score": 5,
                  "created_utc": "2026-02-21 14:04:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6lzq9l",
                  "author": "New-Employer-2539",
                  "text": "This would have a place in the expert fields where knowledge base doesn't change fast. Like law knowledge of specific country. Or maybe even healthcare knowledge.",
                  "score": 3,
                  "created_utc": "2026-02-21 15:10:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6lg2um",
              "author": "xRmg",
              "text": "Well combine a few or these asics with tor conditional or role chaining, feed it into a GPU powered model if it needs it and you can get rid of a lot of slow expensive computing. \n\nHaving an ASIC a pre or post filter can offload quite a lot of stuff. Routing becomes fast and cheap.",
              "score": 1,
              "created_utc": "2026-02-21 13:11:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6p1a0o",
              "author": "xmsxms",
              "text": "For consumers maybe. But for the big guys running these hot 24/7 they can probably treat them as disposable and make their money back on electricity and reduced scale savings within a month. Their 'pro' tier models trickle down to the free tier, so the models and the units will get twice the life out of it. With the heavy usage they probably die before being outdated.\n\nI'm no expert, but I would imagine you could use something like this as a first line of routing to the correct model. i.e \"is the prompt for code, image, search or medical?\" and route to the appropriate model.",
              "score": 1,
              "created_utc": "2026-02-22 00:53:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6q1rmt",
              "author": "Lyuseefur",
              "text": "So - yâ€™all know whatâ€™s going to really blow your minds?\n\nMany of the models have similar guts. Kinda like how x64 architecture internals are similar.\n\nSoooâ€¦yeahâ€¦ itâ€™s going to get wild.\n\nOh and RAM wonâ€™t mean as much anymore",
              "score": 1,
              "created_utc": "2026-02-22 04:59:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6qv69p",
              "author": "doppleron",
              "text": "My guess is they are actually burning ROM to dramatically reduce time required to access weights. This is much, much simpler and cheaper than an FPGA and could be quickly customized. Remember when your boot code was burned into a ROM chip that your processor used to get going? Like that. Much faster and more durable than RAM, or God help us, a hard drive. (Yeah, I'm old.)",
              "score": 1,
              "created_utc": "2026-02-22 09:24:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6toz22",
                  "author": "RoutineNet4283",
                  "text": "Man this is so cool, How can I build some POC like this myself.",
                  "score": 1,
                  "created_utc": "2026-02-22 19:23:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6tgolb",
              "author": "Huge_Freedom3076",
              "text": "The thing is this we just need reasoning in this ASIC. Nothing more. All constants, raw data can called from rags. So I'm not a professional in the field but in the end of the days we will have a good model etched on a good ASIC and this is our new CPU .",
              "score": 1,
              "created_utc": "2026-02-22 18:44:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ldbn6",
          "author": "8B1_t",
          "text": "Its so fucking fast also this is the website if anyone wants to try it   \n[https://chatjimmy.ai/](https://chatjimmy.ai/)",
          "score": 24,
          "created_utc": "2026-02-21 12:51:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n2h2e",
              "author": "NVC541",
              "text": "Holy shit that is absurdly fast",
              "score": 10,
              "created_utc": "2026-02-21 18:25:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6qnswi",
              "author": "hurdurdur7",
              "text": "I gave it a bunch of prompts. Yes this speed is the killer. All basic jobs will be taken by this. Yes they need to implement a bigger model - but still, this beats nvidia's approach or anyone else's approach by a light year. \n\nEven if the hardware for an instance will cost 100k , it will pay for itself in a correct implementation in no time what so ever. Time to learn growing potatoes or building power plants. ",
              "score": 4,
              "created_utc": "2026-02-22 08:14:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s4iut",
                  "author": "jerrygreenest1",
                  "text": ">it will pay for itself in a correct implementation in no time what so ever\n\nLike OpenAI? Can you remind us, is OpenAI profitable yet? Oh wait, it never wasâ€¦",
                  "score": 0,
                  "created_utc": "2026-02-22 15:03:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6qmqhy",
              "author": "smallfried",
              "text": "That is super quick. It can basically create webpages on user demand. With those speeds (and J/t energy efficiency) you could have it parse a bunch of stuff and react to it quickly.",
              "score": 1,
              "created_utc": "2026-02-22 08:04:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s0r4g",
                  "author": "floppypancakes4u",
                  "text": "If it were smart enough, sure, but this is a very old model that does not yet have the intelligence for it.\n\n",
                  "score": 1,
                  "created_utc": "2026-02-22 14:44:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6kkzm3",
          "author": "thedarkbobo",
          "text": "Its fast. But the quality hmm...for my purpose it seems low.",
          "score": 12,
          "created_utc": "2026-02-21 08:28:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lusgm",
              "author": "LumbarJam",
              "text": "Itâ€™s basically a PoC to get a feel for speedâ€”almost an alpha. This first version is running with 3-bit INT quantization. According to their site, the next version (mid-year) will be bigger, faster, and use FP4 quantization.",
              "score": 15,
              "created_utc": "2026-02-21 14:43:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6km3j1",
              "author": "SinnersDE",
              "text": "Its POC just 8B Model. Scale it up to 120B and think about it again. \nNVIDIA Short (they will buy it afterwards)",
              "score": 9,
              "created_utc": "2026-02-21 08:39:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6koq1l",
                  "author": "ghoarder",
                  "text": "Can it scale to a 120B model though? If the model is etched direct into the silicon and that 8B chip looks quite big, a 120B would need to be about 4 times the physical size in each axis, or 15 times the area.  That would be a pretty large bit of silicon.",
                  "score": 3,
                  "created_utc": "2026-02-21 09:05:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6kr82z",
                  "author": "thedarkbobo",
                  "text": "That would be good.",
                  "score": 1,
                  "created_utc": "2026-02-21 09:30:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ng9tl",
          "author": "p0u1",
          "text": "Everyone is missing the fact that this could be used as a speculative decoding model and speed up any model built on the same instructions",
          "score": 7,
          "created_utc": "2026-02-21 19:33:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6poi1y",
              "author": "austrobergbauernbua",
              "text": "Could you please elaborate?",
              "score": 3,
              "created_utc": "2026-02-22 03:24:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6qir0z",
                  "author": "p0u1",
                  "text": "So you use a smaller model in general to draft the output then the larger model runs on top to fill in the bits that canâ€™t be drafted.\n\nThatâ€™s a very basic explanation.\n\nhttps://youtu.be/qmAbco38pXA?si=dZrQqCLhAeujApAJ\n\nQuite a good video showing it in use.",
                  "score": 3,
                  "created_utc": "2026-02-22 07:26:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6kj8i6",
          "author": "frank_brsrk",
          "text": "It is indeed super fast, yesterday was trying it. It explicitly says that it collects your data",
          "score": 5,
          "created_utc": "2026-02-21 08:10:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qo8u6",
              "author": "smallfried",
              "text": "I assume all online LLM services are.",
              "score": 5,
              "created_utc": "2026-02-22 08:18:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nrfky",
          "author": "synth_jarvis",
          "text": "Honestly, etching model weights directly into silicon sounds wild! I get why folks might be worried about being stuck with one model, especially since AI is evolving at breakneck speed. But on the flip side, the idea of super fast and cheap inference is pretty appealing, especially for use cases where you just need reliable performance and aren't as concerned about having the absolute latest model. It reminds me of how GPUs transformed gaming performance. This could really shake things up for certain applications. What do you guys thinkâ€”worth the risk or too limiting?",
          "score": 2,
          "created_utc": "2026-02-21 20:32:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6l8rb7",
          "author": "thatguy122",
          "text": "Seeing the post for Taalas yesterday made me feel as uneasy as ChatGPT when it was first revealed. This development hits different than the model progress we've been seeing to-date.Â \n\n\nThe implications of this...assuming it checks out...seems absolutely staggering. Not to be dramatic, but, perhaps the final warning shot for action.Â ",
          "score": 14,
          "created_utc": "2026-02-21 12:15:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6m1z22",
              "author": "ductiletoaster",
              "text": "Iâ€™m not sure Iâ€™m following how this is a warning shot? Taalas is essentially just taking an existing model and creating specialized hardware for it.\n\nThe results will be incredibly efficient (power and speed) at the cost of being immutable (e.g. requiring hardware swapping to change models).\n\nI guarantee google is already doing this behind closed doors. This is just economics at scale. \n\nWhat is it youâ€™re sounding an alarm on?",
              "score": 8,
              "created_utc": "2026-02-21 15:22:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rk6k5",
                  "author": "thatguy122",
                  "text": "When I say warning shots, I mean those who have been ignorant to the progress and development of models in the past 2.5 years need to open their eyes. That includes devs, policymakers, etc.\n\n\nTake as you will but I don't think it's being dramatic for those unaware to become aware.Â \n\n\n\n",
                  "score": 1,
                  "created_utc": "2026-02-22 13:06:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6qncl5",
              "author": "smallfried",
              "text": "Without saying why you're talking about warning shots, you're definitely being dramatic.",
              "score": 1,
              "created_utc": "2026-02-22 08:09:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rk800",
                  "author": "thatguy122",
                  "text": "When I say warning shots, I mean those who have been ignorant to the progress and development of models in the past 2.5 years need to open their eyes. That includes devs, policymakers, etc.\n\nTake as you will but I don't think it's being dramatic for those unaware to become aware.Â \n\n",
                  "score": 1,
                  "created_utc": "2026-02-22 13:06:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6lbud4",
          "author": "evilbarron2",
          "text": "Wait - you didnâ€™t think things were gonna remain unchanging in the world of AI, the fastest-evolving tech weâ€™ve seen in decades, did you?Â ",
          "score": 2,
          "created_utc": "2026-02-21 12:40:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lc0oc",
          "author": "Mice_With_Rice",
          "text": "Is this kind of hardware able to support LoRA? Otherwise your stuck in a single base model forever.",
          "score": 2,
          "created_utc": "2026-02-21 12:41:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6s4a24",
              "author": "TaiMaiShu-71",
              "text": "They claim it supports lora.",
              "score": 2,
              "created_utc": "2026-02-22 15:02:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6leglp",
          "author": "Simple_Library_2700",
          "text": "Cool development, but it being an asic means you cannot change the model being used. With the speed things are moving that is not ideal. \n\nNot to mention the size issue if you wanted to scale past 8B the size of the asic would scale linearly with the model size and you would end up with a gargantuan chip that would be impossible to produce. But maybe you could leverage chiplets or something to do with MoE.",
          "score": 2,
          "created_utc": "2026-02-21 13:00:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ri6s3",
              "author": "Imp_erk",
              "text": "That's the challenge for sure. Any chip that runs a SotA models without absurd quantization will be beyond current capabilities to produce. Even small models will be incredibly costly. Nevermind LoRA and other techniques not being possible on these.",
              "score": 1,
              "created_utc": "2026-02-22 12:51:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6syoi4",
              "author": "Specter_Origin",
              "text": "May be its like a dvd with a game, you can't change the game that came on dvd. As in chip becomes cheap enough to become a package for weights. ",
              "score": 1,
              "created_utc": "2026-02-22 17:21:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6mb687",
          "author": "ba2sYd",
          "text": "It is cool but every few months new llm models comes out so I wonder if it would be that efficient...",
          "score": 2,
          "created_utc": "2026-02-21 16:09:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mqye1",
              "author": "momono75",
              "text": "I guess we will not be able to tell the differences between new models in the near future due to LLMs surpassing humans. At that point, this approach can be a good choice.",
              "score": 2,
              "created_utc": "2026-02-21 17:28:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6mxsmd",
                  "author": "Former-Ad-5757",
                  "text": "It can be a good choice already before that, this kind of speedup can change a whole lot of games, just because you can bruteforce through the errors/hallucinations. Wanna have better performance for a problem, just ask it 3x and take the average.",
                  "score": 3,
                  "created_utc": "2026-02-21 18:02:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ntiyf",
          "author": "mockingtruth",
          "text": "Having a reasonably good model but in raspberry pi sized form and for less than 100 would be good.  Also plugging openclaw into this might be interesting",
          "score": 2,
          "created_utc": "2026-02-21 20:43:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rk5b4",
          "author": "Silver_Jaguar_24",
          "text": "Wow, it's blazingly fast. But the quality of the LLM used, **Llama 3.1 8B**, is obviously sub-par. But it proves a point, that you don't need a supercomputer to have mind-blowingly fast LLM responses.\n\n**ChatJimmy** isn't just a fast chatbot; it's a proof of concept that the future of AI might not be giant, power-hungry data centers, but specialized \"AI bricks\" that are cheap enough to put in everything from your toaster to your glasses.\n\nIf running \"locally\", you might have to upgrade the chip once a year maybe, to get the current/latest model LLM of choice. Looks like this might be the future for local-LLMs. Try it here - [https://chatjimmy.ai/](https://chatjimmy.ai/)",
          "score": 2,
          "created_utc": "2026-02-22 13:05:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k8te2",
          "author": "pamidur",
          "text": "It was inevitable, look at Bitcoin",
          "score": 3,
          "created_utc": "2026-02-21 06:32:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6l36eb",
          "author": "Suitable-Program-181",
          "text": "If its cheaper than a mac M chip I will like it if not, they can have the sillicon but they still lack the kernels, just like M chips. Rather fight asahi and a solid beast like m3 - m4 than another APU. \n\nAMD APU's are actually dope if you can build around them so the sillicon being the model is the sauce no cap!",
          "score": 1,
          "created_utc": "2026-02-21 11:26:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lfiv6",
              "author": "Mice_With_Rice",
              "text": "Even if it were that cheap which it wont be, the physical dimensions of the chip if it were made for a more usfull model size with its current node process would be larger than the MAC itself. Have to return to ATX towers.",
              "score": 1,
              "created_utc": "2026-02-21 13:07:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6oh89p",
          "author": "electricleisure",
          "text": "When a new version of the model comes out, could they come out with something like an expansion board rather than needing a whole new board? I imagine this would require some special training to account for the fact that the first X layers arenâ€™t changing.",
          "score": 1,
          "created_utc": "2026-02-21 22:51:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pce9g",
          "author": "ailee43",
          "text": "Failed the R's in strawberry test, but after telling it 3 times, it got it",
          "score": 1,
          "created_utc": "2026-02-22 02:04:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qdod2",
          "author": "joemacross",
          "text": "trying out chat jimmy and it's blowing my mind on how quick it is",
          "score": 1,
          "created_utc": "2026-02-22 06:39:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qol3i",
          "author": "smallfried",
          "text": "They could scale it down a bit. Add STT and TTS into it. Get it to a couple of Watt for generation and stick it in toys. Could run some chatty LLM on batteries like Teddy from the AI movie.",
          "score": 1,
          "created_utc": "2026-02-22 08:21:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qu4rq",
          "author": "doppleron",
          "text": "I think most of this discussion is missing a big point: The vast majority of applications don't need a huge context window (Think agents). They need a compact and streamlined \"brain\" to make many small, fast, consistent decisions. These chips, with supplemental data, could easily run your car, house, a small business, combat drone,  etc. Or a ~~Starlink~~ Skynet bot...",
          "score": 1,
          "created_utc": "2026-02-22 09:14:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r6yth",
          "author": "Ambadeblu",
          "text": "This would positively impact consumer hardware prices right?",
          "score": 1,
          "created_utc": "2026-02-22 11:17:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s8k1o",
          "author": "Auto_17",
          "text": "Exactly what i was thinking we need to figure out a way to make our current tech more efficient to use ai models not just adding more computers together. Our brain doesnt go get another brain to get better at a task it simply hardrewires our current neurons(aka practice).",
          "score": 1,
          "created_utc": "2026-02-22 15:23:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6si6nw",
          "author": "DateOk9511",
          "text": "wtf!!! it is soooo fast! where do we buy the hardware?\n\n",
          "score": 1,
          "created_utc": "2026-02-22 16:07:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t510i",
          "author": "Fair-Cookie9962",
          "text": "You can assume things will change quickly - and integrate that into your plans, including 10x performance in 12 months.",
          "score": 1,
          "created_utc": "2026-02-22 17:51:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tray1",
          "author": "Secret-Collar-1941",
          "text": "I asked it to count to 15,000. it failed to respond :(",
          "score": 1,
          "created_utc": "2026-02-22 19:35:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tsqtk",
          "author": "PaneerEater101",
          "text": "Can someone explain this to me? I don't understand. How are they getting such fast speeds? I tried it out on chat jimmy but it couldn't really explain how it was so fast",
          "score": 1,
          "created_utc": "2026-02-22 19:42:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uix3i",
          "author": "egil87",
          "text": "will probably go the way of bitcoin/blockchain. general purpose until asic makes it impossible to compete.",
          "score": 1,
          "created_utc": "2026-02-22 21:54:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v91j9",
          "author": "TheNotSoEvilEngineer",
          "text": "Hope so, then they can focus on their ASIC designs instead of ruining the GPU market.",
          "score": 1,
          "created_utc": "2026-02-23 00:18:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vghy5",
          "author": "sooham",
          "text": "Expect the downside of being stuck with the same model architecture throughout the life time of the machine",
          "score": 1,
          "created_utc": "2026-02-23 01:00:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6l33yc",
          "author": "jerrygreenest1",
          "text": "I donâ€™t quite get the logic behind this idea because transistor does not equal neuron, a transistor can be either 0 or 1, while a neuron can be any state between 0 and 1, â€“ how could you Â«printÂ» neurons to a board?Â ",
          "score": -3,
          "created_utc": "2026-02-21 11:25:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lbrwo",
              "author": "Mice_With_Rice",
              "text": "Easy, it uses more than a single bit, just like every other computer does.",
              "score": 3,
              "created_utc": "2026-02-21 12:40:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lkb62",
                  "author": "jerrygreenest1",
                  "text": "Then I donâ€™t quite see the theoretical benefit behind such a board, you donâ€™t save on anything. Please donâ€™t just say Â«but they did it canâ€™t you see it worksÂ», I am talking theory here",
                  "score": 0,
                  "created_utc": "2026-02-21 13:39:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ltpbi",
              "author": "Nzkx",
              "text": "You can model 0-1 range in electronic and the proof is easy to demonstrate.\n\nSuppose that wouldn't be the case and we can not model 0-1 range in electronics.\n\nIf this is true, how your computer can store a floatting point number like 0.0023233 ? It's made of the same electronic components, and we know a computer can operate on floatting point numbers. So it must be false, hence it's possible, otherwise your computer wouldn't be able to express the 0-1 range.",
              "score": 2,
              "created_utc": "2026-02-21 14:36:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6m21hz",
                  "author": "jerrygreenest1",
                  "text": ">You can model 0-1 range in electronic\n\nOf course you can, what are you talking about. It's not the question. Question is â€“ how then it's different to the current approach? Where do we get benefit from?",
                  "score": 1,
                  "created_utc": "2026-02-21 15:22:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6m2jmf",
              "author": "snowgirl9",
              "text": "https://en.wikipedia.org/wiki/IEEE_754",
              "score": 1,
              "created_utc": "2026-02-21 15:25:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6m4hws",
              "author": "ApprehensiveDelay238",
              "text": "AI doesn't use neurons. ",
              "score": 1,
              "created_utc": "2026-02-21 15:35:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6my5zo",
                  "author": "jerrygreenest1",
                  "text": "And what do you know about AI? I have learned about this in an university, and you? Probably know nothing? AI also known as Â«neural networksÂ» â€“ are neural structures that consists of you guess... NEURONS. Even though not biological ones and instead mimicked with bytes that eventually are a bunch of transistors...",
                  "score": 2,
                  "created_utc": "2026-02-21 18:04:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r735eo",
      "title": "Self Hosted Alternative to NotebookLM",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r735eo/self_hosted_alternative_to_notebooklm/",
      "author": "Uiqueblhats",
      "created_utc": "2026-02-17 11:06:53",
      "score": 95,
      "num_comments": 9,
      "upvote_ratio": 0.98,
      "text": "For those of you who aren't familiar with SurfSense, SurfSense is an open-source alternative to NotebookLM, Perplexity, and Glean.\n\nIt connects any LLM to your internal knowledge sources, then lets teams chat, comment, and collaborate in real time. Think of it as a team-first research workspace with citations, connectors, and agentic workflows.\n\nIâ€™m looking for contributors. If youâ€™re into AI agents, RAG, search, browser extensions, or open-source research tooling, would love your help.\n\n**Current features**\n\n* Self-hostable (Docker)\n* 25+ external connectors (search engines, Drive, Slack, Teams, Jira, Notion, GitHub, Discord, and more)\n* Realtime Group Chats\n* Hybrid retrieval (semantic + full-text) with cited answers\n* Deep agent architecture (planning + subagents + filesystem access)\n* Supports 100+ LLMs and 6000+ embedding models (via OpenAI-compatible APIs + LiteLLM)\n* 50+ file formats (including Docling/local parsing options)\n* Podcast generation (multiple TTS providers)\n* Cross-browser extension to save dynamic/authenticated web pages\n* RBAC roles for teams\n\n**Upcoming features**\n\n* Slide creation support\n* Multilingual podcast support\n* Video creation agent\n\nGitHub: [https://github.com/MODSetter/SurfSense](https://github.com/MODSetter/SurfSense)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r735eo/self_hosted_alternative_to_notebooklm/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5v41u6",
          "author": "MrKBC",
          "text": "I've tried to get SurfSense and the other two available options to run multiple times each. I'll just stick to Google's.",
          "score": 5,
          "created_utc": "2026-02-17 13:40:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60i4sy",
              "author": "Uiqueblhats",
              "text": "Makes sense, as Google is still the best, but what is your feedback on why you did not like all three options?",
              "score": 2,
              "created_utc": "2026-02-18 07:11:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o66n6qg",
                  "author": "MrKBC",
                  "text": "I can't form an opinion on something I could never get to operate as intended. ",
                  "score": 2,
                  "created_utc": "2026-02-19 04:01:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o62woph",
          "author": "ThickCranberry3813",
          "text": "Just in time. Thank you",
          "score": 1,
          "created_utc": "2026-02-18 16:43:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o651n1e",
          "author": "jnz_go",
          "text": "I am currently developing a RAG tool which supports different sources as well. So not too far away. I made a python version with Claude and the current version I am developing manually is in golang. Mainly to learn. I guess I try yours to see if it fullfills what I imagine for my tool.",
          "score": 1,
          "created_utc": "2026-02-18 22:33:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5z8exr",
          "author": "SirAdelaide",
          "text": "But by linking to online LLMs, the data still leaves the premises and is given to the LLM owner.\nFor self hosting, the LLM needs to run locally and offline.\nDoes it have offline capabilities?",
          "score": 1,
          "created_utc": "2026-02-18 02:02:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60i740",
              "author": "Uiqueblhats",
              "text": "Yes, you can also use local LLMs using Ollama, LM Studio, etc.",
              "score": 2,
              "created_utc": "2026-02-18 07:12:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6i9au4",
                  "author": "Birdsky7",
                  "text": "Which lightweight models will work best for that? I was eyeing nvidia models lately thinking to try them. I also have llama installed",
                  "score": 1,
                  "created_utc": "2026-02-20 22:42:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r72my6",
      "title": "My experience with running small scale open source models on my own PC.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r72my6/my_experience_with_running_small_scale_open/",
      "author": "Dibru9109_4259",
      "created_utc": "2026-02-17 10:36:47",
      "score": 75,
      "num_comments": 46,
      "upvote_ratio": 0.93,
      "text": "I recently got exposed to **Ollama** and the realization that I could take the 2 Billion 3 Billion parameter models and run them locally in my small pc with limited capacity of **8 GB RAM** and just an **Intel i3** CPU and without any GPU made me so excited and amazed. \n\nThough  the experience of running such Billions parameter models with 2-5 GB  RAM consumption  was not a smooth experience. Firstly I run the \"**Mistral 7B**\" model in my ollama. The response was well structured and the reasoning was good but given the limitations of my hardwares, it took about **3-4 minutes** in generating every response.\n\nFor a smoother expereience, I decided to run a smaller model. I choose Microsoft's **phi3:mini** model which was trained on around **3.8 Billion** parameters. The experience with this model was quite smoother compared to the pervious Minstral 7B model. phi3:mini took about  7-8 secods for the cold start and once it was started, it was generating responses with **less than 0.5  seconds** of prompting. I tried to measure the token generating speed using my phone's stopwatch and the number of words generated by the model (NOTE: **1 token = 0.75 word**, on average). I found out that this model was generating 7.5 tokens per second on my PC. The experience was pretty smooth with such a speed and it was also able to do all kinds of basic chat and reasoning.\n\nAfter this I decided to test the limits so I downloaded two even smaller models - **tinyLLama**. While the model was much compact with just **1.1 Billion** parameters and just 0.67GB download size for the **4-bit (Q4\\_K\\_M) version**, its performance deteriorated sharply.\n\nWhen I first gave a simple Hi to this model it responded with a random unrelated texts about \"nothingness\" and the paradox of nothingness. I tried to make it talk to me but it kept elaborating in its own cilo about the great philosophies around the concept of nothingness thereby not responding to  whatever prompt I gave to it. Afterwards I also tried my hand at the **smoLlm**  and this one also hallucinated massively.\n\n**My Conclusion :**\n\nMy *hardware* capacity affected the *speed* of Token generated by the different models. While the 7B parameter Mistral model took several minutes to respond each time, *this problem was eliminated entirely once I went 3.8 Billion parameters and less.* All of the phi3:mini and even the ones that hallucinated heavily - smolLm and tinyLlama generated tokens instantly.\n\nThe *number of parameters determines the extent of intelligence* of the LLM. Going below the 3.8 Billion parameter phi3:mini f, all the tiny models hallucinated excessively even though they were generating those rubbish responses very quickly and almost instantly.\n\nThere was *a tradeoff between* ***speed*** *and* ***accuracy.*** Given the limited hardware capacity of my pc, going below 3.8 Billion parameter model gave instant speed but extremely bad accuracy while going above it gave slow speed but higher accuracy.\n\nSo this was my experience about experimenting with Edge AI and various open source models. **Please feel free to correct me whereever you think I might be wrong. Questions are absolutely welcome!**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r72my6/my_experience_with_running_small_scale_open/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5uhysz",
          "author": "p_235615",
          "text": "you can try https://ollama.com/library/granite3.3 the 2b model should run OK. \nOr https://ollama.com/library/lfm2.5-thinking, this one should run relatively fast.",
          "score": 15,
          "created_utc": "2026-02-17 11:06:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5unqla",
              "author": "Dibru9109_4259",
              "text": "Thanks man! Taking note of these 2 models! I'll update their performances on my hardware!",
              "score": 5,
              "created_utc": "2026-02-17 11:54:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5za2eq",
                  "author": "BigDry3037",
                  "text": "Granite 4 micro is definitely what you need to try",
                  "score": 3,
                  "created_utc": "2026-02-18 02:10:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5uv8i3",
              "author": "InfraScaler",
              "text": "In theory this is good at tool calling, too. Didn't try it yet [hadad/LFM2.5-1.2B](https://ollama.com/hadad/LFM2.5-1.2B)",
              "score": 3,
              "created_utc": "2026-02-17 12:47:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5uteoo",
          "author": "Blinkinlincoln",
          "text": "Just want to say that I am really happy you just wrote this post without AI editing. It's refreshing on reddit now to see typos.Â ",
          "score": 19,
          "created_utc": "2026-02-17 12:35:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5v7wio",
              "author": "Dibru9109_4259",
              "text": "Glad that you took the typos to complement me insteas of pointing out the mistakes! I guess, in this AI age, making some typo mistakes is gonna be not just okay but a marker of originality ðŸ˜…",
              "score": 7,
              "created_utc": "2026-02-17 14:01:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5y8b1h",
                  "author": "sassyhusky",
                  "text": "Itâ€™s very refreshing and I dread the fact that pretty soon LLMs will be trained to make typos and feel natural so we wonâ€™t be able to tell any more",
                  "score": 2,
                  "created_utc": "2026-02-17 22:47:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5zmc7y",
              "author": "NoleMercy05",
              "text": "You tell how come?",
              "score": 1,
              "created_utc": "2026-02-18 03:19:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5uv5xb",
          "author": "InfraScaler",
          "text": "Yeah you can run small models on CPU, however when context starts to grow you're dead - also those models are not great at tool calling etc... so, more than anything they're just very interesting curiosities :)",
          "score": 4,
          "created_utc": "2026-02-17 12:46:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vz4z0",
              "author": "trolololster",
              "text": "functiongemma:270m would like a word",
              "score": 1,
              "created_utc": "2026-02-17 16:19:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wkmn1",
                  "author": "InfraScaler",
                  "text": "Good with tooling?",
                  "score": 1,
                  "created_utc": "2026-02-17 18:05:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5vjhys",
          "author": "Majinsei",
          "text": "Models below 3b always need fine-tuning~\n\nThey're practically made for that~ you train them on your tools and do a little transfer learning~",
          "score": 3,
          "created_utc": "2026-02-17 15:03:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vytwb",
          "author": "trolololster",
          "text": "write MOAR!\n\nedge inference is SO interesting.",
          "score": 2,
          "created_utc": "2026-02-17 16:18:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wl51y",
              "author": "Dibru9109_4259",
              "text": "This was my first evet detailed post on reddit. I am so happy to get such appreciation! Thank you so soo much!!",
              "score": 2,
              "created_utc": "2026-02-17 18:07:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5wlg2b",
                  "author": "trolololster",
                  "text": "we are going to need edge so much now that all the ram and disks are bought until 2028.\n\ncompute is now the frontier.",
                  "score": 1,
                  "created_utc": "2026-02-17 18:09:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o60gopx",
          "author": "reckless_avacado",
          "text": "try gemma3:1b or qwen2.5:0.5. they are fast and are surprisingly good.",
          "score": 2,
          "created_utc": "2026-02-18 06:59:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62z0yl",
          "author": "MarketingGui",
          "text": "If you're booking for a good 3B modelo, try the nanbeige: https://ollama.com/fauxpaslife/nanbeige4.1:latest\n\nThis little guy is outperforming bigger models.",
          "score": 2,
          "created_utc": "2026-02-18 16:53:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v8432",
          "author": "Prigozhin2023",
          "text": "Which models can be used for tools? E.g. with opencode cli.\n\n\nCan't seems like there's a gap in the market for small models that can be used with tools.",
          "score": 1,
          "created_utc": "2026-02-17 14:02:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vyzwl",
              "author": "trolololster",
              "text": "you could do a graph or some other agentic flow where you use something like functiongemma (270m parameters) for the tool-calling",
              "score": 1,
              "created_utc": "2026-02-17 16:19:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5vzu37",
                  "author": "Prigozhin2023",
                  "text": "tried functiongemma.. it seems to be focused on tasks mgmt, etc. not much of a talkers or collaborator.. haha.",
                  "score": 1,
                  "created_utc": "2026-02-17 16:23:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5vz7bk",
              "author": "Rtjandrews",
              "text": "Im using qwen3:8b on an rtx 3080 with tool calling. Early days but after a slight delay in loading up the model initially responses are reasonably fast. Certainly enough for my early experiments of using tool calling anyway",
              "score": 1,
              "created_utc": "2026-02-17 16:20:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5w0c2i",
                  "author": "Prigozhin2023",
                  "text": "the initial load is pretty lengthy, thereafter seems pretty ok. others that slightly faster are ministral-3:3b & lfm2.5-thinking:1.2b ... qwen3:4b is pretty ok as well. \n\n",
                  "score": 1,
                  "created_utc": "2026-02-17 16:25:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5vi2xw",
          "author": "CorneZen",
          "text": "I also tried a couple of the really small SLM in Ollama. My first question to every model is always, what is your purpose? These SLMs all hallucinated about nothingness, one about climbing a ladder. \nI think the way the basic Ollama app calls these SLMs may be causing this. Need to investigate some more.\n(I made sure to check the models were â€˜Ollamaâ€™ compatible on huggingface)",
          "score": 1,
          "created_utc": "2026-02-17 14:55:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vj30x",
              "author": "Dibru9109_4259",
              "text": "Damn! How coincidental that when I too ran the \"smolLm\" , when I first said Hi to it, it responded with a random sermon about \"NOTHINGNESS\" and paradox of notingness! Lol!!\n\n",
              "score": 2,
              "created_utc": "2026-02-17 15:00:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wmesa",
          "author": "TheCoffeeGuy13",
          "text": "I've just started doing this too, but knowing what your GPU is would help also. \n\nRunning the models through ollama I found to be hit and miss if it uses the GPU or not. Slow model speeds means it's running on the CPU, or possibly half and half. \n\nI run ollama in a container, with the GPU enabled, so even on 8GB of vram with a 4Billion parameter model I get around 25 tokens per second.",
          "score": 1,
          "created_utc": "2026-02-17 18:13:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wn92p",
              "author": "Dibru9109_4259",
              "text": "great man! 25 tokens/s is pretty awesome! What models are you running?\n\n",
              "score": 1,
              "created_utc": "2026-02-17 18:17:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5wqpl6",
                  "author": "TheCoffeeGuy13",
                  "text": "Currently I've got llama3.1 and qwen2.5 (sorry, I bit less than the 4B I mentioned) and they run on a Nvidia GTX1080. \n\nNot as fast as using ChatGPT, but not far off it. \n\nI suspect your Ollama is running 50/50 CPU/GPU due to the length of time it's taking. \n\nInstall Docker and make a container to run Ollama in. Then push it through to the GPU and you will get better speeds. (Honestly it's not that difficult to do, all the instructions are online)",
                  "score": 1,
                  "created_utc": "2026-02-17 18:33:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o60v7uw",
          "author": "ayoubhak",
          "text": "Did you try ollama cloud ?",
          "score": 1,
          "created_utc": "2026-02-18 09:12:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61zj9n",
              "author": "Dibru9109_4259",
              "text": "Not yet! Have you? If yes, how has your experience been so far?",
              "score": 1,
              "created_utc": "2026-02-18 14:05:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o673hyy",
          "author": "Beginning_Front_6274",
          "text": "In near same setup i use ollama with ministral-3 3b, but take it from unsloth repo. Not so fine without gpu, but useful.\n\n",
          "score": 1,
          "created_utc": "2026-02-19 05:58:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6i71xg",
          "author": "fab_space",
          "text": "just put a semantic-symbolic-math-logic router/mcp and you will see small models flying high, faster, cheaper when needed and with the same validated accuracy. when no opus 4.6 or gemini 3.1 of course.",
          "score": 1,
          "created_utc": "2026-02-20 22:30:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uf81u",
          "author": "TerryTheAwesomeKitty",
          "text": "Those are very valid and correct observations! It is kiiiind of impossible right now to run a generally well performing model on local, especially edge hardware. However what we did in my company's deployment was using REALLY specific, instruct models ( very good at doing predefined tasks ) for specific things. Now we have a lot of 8b and smaller models running for singular, predictable operations. They do great ( albeit a bit slow )!",
          "score": 1,
          "created_utc": "2026-02-17 10:42:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5unl9j",
              "author": "Dibru9109_4259",
              "text": "When I was experimenting with these models running on edge,  the first question that came to my mind was - given the limitations of these small models, WHAT COULD BE THE SPECIFIC BUSINESS OR NORMAL USECASES in which these early limited capacity small models can be used. So if you woldn't mind may I ask the particular use cases where you applied them in real life?",
              "score": 2,
              "created_utc": "2026-02-17 11:53:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5v178n",
                  "author": "TerryTheAwesomeKitty",
                  "text": "Well see, we deal with contracts for A LOT of clients, they all include a standard subsection from our greater set of services, as well as a start date, end date, value and company that the contract is with. \n\nIf we had any issue or needed to check, someone would have to manually open and look in the file. They are mostly .docx or .pdf documents, so sometimes that process was slow.   \n  \nSince in the end we had to parse text to find predictable and consistent occurrences ( obviously all contracts have the name of the company and the price, service value, etc., just sometimes worded differently ) and static parsing was not doing so well, we used Gemma fully locally to extract the key data from the markdown-converted documents and then save it in an internal database. ",
                  "score": 1,
                  "created_utc": "2026-02-17 13:24:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r9zea9",
      "title": "SmarterRouter - A Smart LLM proxy for all your local models. (native Ollama support, loading/unloading models automatically)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r9zea9/smarterrouter_a_smart_llm_proxy_for_all_your/",
      "author": "peva3",
      "created_utc": "2026-02-20 16:06:53",
      "score": 58,
      "num_comments": 10,
      "upvote_ratio": 0.98,
      "text": "I've been working on this project to create a smarter LLM proxy primarily for my openwebui setup (but it's a standard openai compatible endpoint API, so it will work with anything that accepts that).\n\nThe idea is pretty simple, you see one frontend model in your system, but in the backend it can load whatever model is \"best\" for the prompt you send. When you first spin up Smarterrouter it profiles all your models, giving them scores for all the main types of prompts you could ask, as well as benchmark other things like model size, actual VRAM usage, etc. (you can even configure an external \"Judge\" AI to grade the responses the models give, i've found it improves the profile results, but it's optional). It will also detect and new or deleted models and start profiling them in the background, you don't need to do anything, just add your models to ollama and they will be added to SmarterRouter to be used.\n\nThere's a lot going on under the hood, but i've been putting it through it's paces and so far it's performing really well, It's extremely fast, It caches responses, and I'm seeing a negligible amount of time added to prompt response time. It will also automatically load and unload the models in Ollama (and any other backend that allows that).\n\nThe only caveat i've found is that currently it favors very small, high performing models, like Qwen coder 0.5B for example, but if small models are faster and they score really highly in the benchmarks... Is that really a bad response? I'm doing more digging, but so far it's working really well with all the test prompts i've given it to try (swapping to larger/different models for more complex questions or creative questions that are outside of the small models wheelhouse).\n\nHere's a high level summary of the biggest features:\n\n**Self-Correction via Hardware Profiling**: Instead of guessing performance, it runs a one-time benchmark on your specific GPU/CPU setup. It learns exactly how fast and capable your models are in your unique environment.\n\n**Active VRAM Guard**: It monitors nvidia-smi in real-time. If a model selection is about to trigger an Out-of-Memory (OOM) error, it proactively unloads idle models or chooses a smaller alternative to keep your system stable.\n\n**Semantic \"Smart\" Caching**: It doesn't just match exact text. It uses vector embeddings to recognize when youâ€™re asking a similar question to a previous one, serving the cached response instantly and saving your compute cycles.\n\n**The \"One Model\" Illusion**: It presents your entire collection of 20+ models as a single OpenAI-compatible endpoint. You just select SmarterRouter in your UI, and it handles the \"load, run, unload\" logic behind the scenes.\n\n**Intelligence-to-Task Routing**: It automatically analyzes your prompt's complexity. It won't waste your 70B model's time on a \"Hello,\" and it won't let a 0.5B model hallucinate its way through a complex Python refactor.\n\n**LLM-as-Judge Feedback**: It can use a high-end model (like a cloud GPT-4o or a local heavy-hitter) to periodically \"score\" the performance of your smaller models, constantly refining its own routing weights based on actual quality.\n\nGithub: https://github.com/peva3/SmarterRouter\n\nLet me know how this works for you, I have it running perfectly with a 4060 ti 16gb, so i'm positive that it will scale well to the massive systems some of y'all have.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r9zea9/smarterrouter_a_smart_llm_proxy_for_all_your/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o6gevzo",
          "author": "spookperson",
          "text": "This is an interesting project! I've been testing some related open-source projects recently. I posted this same comment over in a different subreddit but I'll repost here in case you haven't seen these projects and are potentially interested in considering other approaches (or combining code into your project): https://www.reddit.com/r/clawdbot/comments/1qzskfd/comment/o4du7f3/?\n\n\n\nOptions/inspiration:\n\n- LiteLLM has semantic routing integration now:Â https://docs.litellm.ai/docs/proxy/auto_routing\n- UIUC has an LLMRouter library with a ton of options:Â https://ulab-uiuc.github.io/LLMRouter/\n- Nvidia has an llm router blueprint on GitHub (the v1 (main branch) and the v2 (experimental branch) are pretty different in design, you might be interested in looking at both):Â https://build.nvidia.com/nvidia/llm-router",
          "score": 4,
          "created_utc": "2026-02-20 17:22:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gler9",
              "author": "peva3",
              "text": "That Clawdbot post is interesting and I'll be looking into it to see how they went about it (also kicking myself that I have such a similar name, I thought I did enough googling, doh), but overall, from the ground up I wanted my project to be local first, with the idea that people COULD use external models, but that the primary use case would be all totally local on the same machine or same local network. \n\nI see a future for local AI where you run your own entire stack and there's basically zero external model usage unless you want to enable that. This would allow something like openwebui to punch way above it's weight locally by being able to swap between all of the best models that you can fit in your GPU. \n\nSo the long long term idea is that this would be a sort of analog to how large AI companies are doing MoE prompt routing, but it would just live in your house instead of having to rely on anything outside your network.",
              "score": 2,
              "created_utc": "2026-02-20 17:52:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6hbj4z",
          "author": "AdditionalWeb107",
          "text": "You should look into preference-aware routing from [Plano](https://github.com/katanemo/plano). Used by the likes of HuggingFace https://x.com/ClementDelangue/status/1979256873669849195. ",
          "score": 2,
          "created_utc": "2026-02-20 19:53:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hh472",
              "author": "peva3",
              "text": "I really like how they have everything setup, but a core idea of my project is to not have to define anything. The system figures out what models you have and profiles them, even when you add new ones or delete ones. I want that sort of \"set and forget\" type of system.",
              "score": 1,
              "created_utc": "2026-02-20 20:20:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6jij9s",
          "author": "evilspyboy",
          "text": "I like the benchmarking. That is clever.",
          "score": 2,
          "created_utc": "2026-02-21 03:13:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jjdb1",
              "author": "peva3",
              "text": "Thank you! It's feeling pretty robust now, but if you see anything that needs to be fixed or tweaked let me know.",
              "score": 1,
              "created_utc": "2026-02-21 03:19:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gvq0r",
          "author": "Crafty_Ball_8285",
          "text": "How does this work for Mac without Nvidia",
          "score": 1,
          "created_utc": "2026-02-20 18:38:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gxkop",
              "author": "peva3",
              "text": "Great question, I think everything should work other than the VRAM stuff. Do you know what Mac uses command line to view VRAM usage? I could make a separate thing to enable just for Macs. Actually it might be better to find something that is OS/GPU agnostic to check the VRAM usage... I'll think on this some more.",
              "score": 1,
              "created_utc": "2026-02-20 18:47:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6tcvzt",
                  "author": "gregusmeus",
                  "text": "Yeah ARC Pro B50 user here, GPU agnostic would be great please!",
                  "score": 2,
                  "created_utc": "2026-02-22 18:27:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r87tdt",
      "title": "packaged claude code + omi + terminator, now it's better than openclaw",
      "subreddit": "ollama",
      "url": "https://v.redd.it/on9h6a286akg1",
      "author": "Deep_Ad1959",
      "created_utc": "2026-02-18 16:34:35",
      "score": 43,
      "num_comments": 7,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r87tdt/packaged_claude_code_omi_terminator_now_its/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o632zqx",
          "author": "Keensworth",
          "text": "How did you get the Opus 4.6 (Claude AI) on your ollama?",
          "score": 4,
          "created_utc": "2026-02-18 17:11:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63ja77",
              "author": "iotsov",
              "text": "OP distilled it. OP is literally China.",
              "score": 5,
              "created_utc": "2026-02-18 18:23:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o63lu87",
                  "author": "Keensworth",
                  "text": "How do you distille an IA? Why does it matter that OP is in China?",
                  "score": -1,
                  "created_utc": "2026-02-18 18:35:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o688v3k",
          "author": "Strict-Growth3180",
          "text": "With a little imagination you could maybe crack the card number. Blur people, blur\n\nhttps://preview.redd.it/oia6jzuvzfkg1.png?width=622&format=png&auto=webp&s=966d06b8deb4176bef9897d6abaea2063f339334\n\n",
          "score": 5,
          "created_utc": "2026-02-19 12:09:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63q6zr",
          "author": "allisonmaybe",
          "text": "I just use Claude and Chrome Devtools MCP",
          "score": 1,
          "created_utc": "2026-02-18 18:54:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68bug1",
          "author": "Comfortable_Ad_8117",
          "text": "We all want our local Ai to do this, I donâ€™t think its possible with even with a 30b model. I have tried BrowserOS and it always wants to be helpful but seems to forget that I asked it to do something about 1/4 of the way in.",
          "score": 1,
          "created_utc": "2026-02-19 12:31:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r858n6",
      "title": "new interactive UI for my CLI OLLAMA (LLM Checker) command palette, hardware-aware model picks, MCP support .....",
      "subreddit": "ollama",
      "url": "https://v.redd.it/cymz3w7tm9kg1",
      "author": "pzarevich",
      "created_utc": "2026-02-18 15:00:17",
      "score": 35,
      "num_comments": 3,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r858n6/new_interactive_ui_for_my_cli_ollama_llm_checker/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o65nppf",
          "author": "Own-Swan2646",
          "text": "Nice keep it going big help",
          "score": 2,
          "created_utc": "2026-02-19 00:32:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67pq26",
          "author": "eightaceman",
          "text": "Will give it a go",
          "score": 2,
          "created_utc": "2026-02-19 09:20:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fyb38",
          "author": "DueAbrocoma3012",
          "text": "Thanks, will try later on my home PC",
          "score": 1,
          "created_utc": "2026-02-20 16:06:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8rgg8",
      "title": "I built a small CLI tool to help beginners see if their hardware can actually handle local LLMs",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r8rgg8/i_built_a_small_cli_tool_to_help_beginners_see_if/",
      "author": "Narrow-Detective9885",
      "created_utc": "2026-02-19 06:06:29",
      "score": 33,
      "num_comments": 13,
      "upvote_ratio": 0.97,
      "text": "Hey everyone,\n\nIâ€™ve been lurking here for a while and learning a ton from all the superusers and experts here. As a beginner myself, I often found it a bit overwhelming to figure out which models would actually run \"well\" on my specific machine versus just running \"slowly.\"\n\nTo help myself learn and to give something back to other newcomers, I put together a small CLI tool in Go called **RigRank**.\n\n**What it does:** Itâ€™s basically a simple benchmarking suite for Ollama. It doesnâ€™t measure how \"smart\" a model isâ€”there are way better tools for thatâ€”but it measures the \"snappiness\" of your actual hardware. It runs a few stages (code gen, summarization, reasoning, etc.) and gives you a \"Report Card\" with:\n\n* **TTFT (Time To First Token):** How long youâ€™re waiting for that first word.\n* **Writing Speed:** How fast it actually spits out text.\n* **Reading Speed:** How quickly it processes your prompts.\n\n**Who this is for:** Honestly, if you already have a complex benchmarking pipeline or a massive GPU cluster, this probably isn't for you. Itâ€™s designed for the person who just downloaded Ollama and wants to know: *\"Is Llama3-8B too heavy for my laptop, or is it just me?\"*\n\n**I would love your feedback**\n\n**Repo:** [https://github.com/rohanelukurthy/rig-rank](https://github.com/rohanelukurthy/rig-rank)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r8rgg8/i_built_a_small_cli_tool_to_help_beginners_see_if/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o67f8rn",
          "author": "joost00719",
          "text": "I run MiniMaxi-2.5 with like 10 tokens/sec (128gb ddr4 + 5070ti).\nIt's dogshit slow. But it's fine if you write a full spec document the Ai can use as a prompt and also a progress tracker. This enables it to not lose focus when the context window gets cleared.\n\nIt build a small weight tracking app in several hours without intervention. I know cloud based Ai can do it in minutes. But for a local machine I was quite impressed with the quality and that it was even possible to begin with.",
          "score": 3,
          "created_utc": "2026-02-19 07:38:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o683je7",
              "author": "Keensworth",
              "text": "A 5070ti is slow? Imagine my GTX 1660 Super",
              "score": 1,
              "created_utc": "2026-02-19 11:28:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o69ytgs",
                  "author": "joost00719",
                  "text": "It's just using the memory, and doesn't really compute much in my case. I offloaded the MoE to the CPU so it's not moving between GPU and System ram all the time, which increases system responsiveness. Not sure if it speeds up or slows down generation much, but probably not that much unless you have more vram.",
                  "score": 1,
                  "created_utc": "2026-02-19 17:42:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o678gkc",
          "author": "EverythingIsFnTaken",
          "text": "https://preview.redd.it/rcpjnw27dekg1.png?width=1918&format=png&auto=webp&s=60b2691eb7d65cd9d037b9ce2091ea4dc923293d",
          "score": 2,
          "created_utc": "2026-02-19 06:39:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6846vn",
          "author": "smcgann",
          "text": "I like this.  I have been working on a utility that pulls the system specs and then advises what software and configuration changes are needed before installing an LLM.  It also gives recommendations on model size.  Something like your tool would be great for post installation testing.  The only drawback for beginners is the prerequisite to having Go installed.  I doubt many beginners would have Go installed or need it.",
          "score": 1,
          "created_utc": "2026-02-19 11:33:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e1qom",
              "author": "Narrow-Detective9885",
              "text": "That's very cool. Looking forward to try your project. Thanks for the feedback, I added some CI and made the precompiled binaries available. ",
              "score": 2,
              "created_utc": "2026-02-20 08:32:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6c0p5x",
          "author": "LithiumToast",
          "text": "I'm very busy but I'll take a look this weekend. Anything that helps people start tinkering and using local offline models is a step in the right direction.",
          "score": 1,
          "created_utc": "2026-02-19 23:52:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c8a3v",
          "author": "Snoo_24581",
          "text": "Nice tool! Model compatibility checking is something beginners definitely need. Is it open source? Would love to check it out.",
          "score": 1,
          "created_utc": "2026-02-20 00:35:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6drq7w",
          "author": "superdav42",
          "text": "I ran into version problems with go:\n```\n$ go build -o rigrank\ngo: errors parsing go.mod:\n/home/dave/rig-rank/go.mod:3: invalid go version '1.25.3': must match format 1.23\n```\nIt doesn't seem worth the trouble to fix it. But is golang really necessary for this? Seems like a simple bash script could accomplish the same goal with much fewer dependencies.",
          "score": 1,
          "created_utc": "2026-02-20 06:59:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e1wnf",
              "author": "Narrow-Detective9885",
              "text": "Good feedback, Thank you ",
              "score": 1,
              "created_utc": "2026-02-20 08:34:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r6tj0j",
      "title": "Lots of pain, finally a small breakthrough, is it enough? Sharing what I've done",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r6tj0j/lots_of_pain_finally_a_small_breakthrough_is_it/",
      "author": "Minimum-Two-8093",
      "created_utc": "2026-02-17 02:17:04",
      "score": 25,
      "num_comments": 10,
      "upvote_ratio": 0.84,
      "text": "I'm not overly knowledgeable about self-hosting models, but I do have a software engineering background and time on my hands to try to make it work, as well as an employer that's encouraging us to figure shit out (read: I'm being paid to do this).\n\nComparing cloud agents with local agents is never a good idea, but that's been my only frame of reference. I have an absolutely huge solution for an economic simulation that I've been building, recently in conjunction with cloud agents.\n\nUp until now, I've been using Chat-GPT (web) as a designer and prompt engineer, and Opus 4.5 as a coding model through Claude Code. I've been treating this approach as if I'm the architect, and the agents are my junior to middling developers. This has been working very well.\n\nBut, I've wanted to find a use-case where my local machine is used for some of the work - I'm sick of paying for cloud agents, running out of quota continually, and sharing my information freely.\n\nFor the past 3 weeks, I've been struggling with Ollama hosted models, trying to find the right use-cases. I have an RTX4090 and have been dancing between Qwen, GPT-OSS, and DeepSeek derivatives of Qwen. I have docker running on an old 1U server in my garage, currently only hosting Open WebUI, this is exposing Ollama hosted models to all of my devices via Tailscale.\n\nI'm using [Continue.dev](http://Continue.dev) in VS Code.\n\nThat's the background, now the problem statement:\n\n>When trying to use my self-hosted \"agents\" for coding tasks, nothing *felt* good. Attempting to edit files and failing to reference them properly just felt like *friction*. Unintended files ended up overwritten.\n\nI have the following folder structure for Continue to load when VS Code is opened:\n\n>./.continue/rules/\n\nAnd inside that folder are the following:\n\n* 00-path-grounding .md\n* 00-project-context .md\n* 08-path-grounding-hard-stop .md\n* 08b-no-fake-tools .md\n* 09-repomap-maintenance .md\n* 10-determinism .md\n* 11-simulation-boundaries .md\n* 12-contract-invariants .md\n\nI've also got another repo with my project agnostic rules files and a script which copies them into the same *rules* folder. This is so that I can keep all of my projects consistent with each other if they're using my local models.\n\n* shared-01-general .md\n* shared-02-safe-edits .md\n* shared-03-tests-first .md\n* shared-04-diff-discipline .md\n\nThis has been hit and miss, mainly because tool usage has also been hit and miss. What's improved this however are the built-in custom providers for Continue.\n\n[https://docs.continue.dev/customize/deep-dives/custom-providers#built-in-context-providers](https://docs.continue.dev/customize/deep-dives/custom-providers#built-in-context-providers)\n\nHere's the Continue config.yaml file I've settled on, including the providers I've chosen:\n\n    // config.yaml\n    \n    name: Local Config\n    version: 1.0.0\n    schema: v1\n    \n    context:\n    Â  - provider: file\n    Â  - provider: code\n    Â  - provider: open\n    Â  Â  params:\n    Â  Â  Â  onlyPinned: true\n    Â  - provider: clipboard\n    Â  - provider: tree\n    Â  - provider: repo-map\n    Â  Â  params:\n    Â  Â  Â  includeSignatures: false # default true\n    \n    models:\n    Â  - name: GPT-OSS Chat\n    Â  Â  provider: ollama\n    Â  Â  apiBase: http://localhost:11434\n    Â  Â  model: gpt-oss:20b\n    Â  Â  roles:\n    Â  Â  Â  - chat\n    \n    Â  - name: Qwen3 Coder\n    Â  Â  provider: ollama\n    Â  Â  apiBase: http://localhost:11434\n    Â  Â  model: qwen3-coder:30b\n    Â  Â  roles:\n    Â  Â  Â  - chat\n    Â  Â  Â  - edit\n    Â  Â  Â  - apply\n    \n    Â  - name: Qwen2.5 Autocomplete\n    Â  Â  provider: ollama\n    Â  Â  apiBase: http://localhost:11434\n    Â  Â  model: qwen2.5-coder:1.5b-base\n    Â  Â  roles:\n    Â  Â  Â  - autocomplete\n    \n    Â  - name: Nomic Embed\n    Â  Â  provider: ollama\n    Â  Â  apiBase: http://localhost:11434\n    Â  Â  model: nomic-embed-text:latest\n    Â  Â  roles:\n    Â  Â  Â  - embed\n\nThe end result seems to be that I have finally settled on something *kinda* useful.\n\nThis may look simple (it is), but it's the first scoped refactor of an existing piece of code where the agent hasn't screwed *something* up.\n\nhttps://preview.redd.it/jgt5b0hmnyjg1.png?width=2728&format=png&auto=webp&s=25e57ca54e0d0d35116c47df872e7d7dc7f1e18a\n\nAs mentioned, this code base is quite large and set to get bigger.\n\nI've been used to cloud agents achieving quite amazing things while significantly boosting throughput (at least 20x what I'm capable of, probably more).\n\nI'd wanted to have my local models do the same, but in reality that was completely unrealistic; a 4090 cannot compete with cloud inference.\n\nWhere I *think* I've settled now (especially since a significant portion of my simulation is complete) is that my local agents can probably help to augment me more now that I am moving onto front-end implementation. I think that if I can constrain myself to only expect the local agents to help me with *the boring shit* like boilerplate and mass data entry, I will still save myself significant amounts of time; e.g. that edit would have taken me a couple of minutes of data entry, the prompt took a few seconds to write, and a few seconds to execute. I think it's likely that I'll keep using cloud agents for gnarly work, and local agents for the simpler things. That's not bad.\n\nPerhaps that's the sweet-spot.\n\nI don't really know what I want to get from this post, perhaps just to start a conversation.\n\n**Are you working on large projects with locally hosted models?**\n\n**How has your experience been?**\n\nIf I'm missing anything obvious, let me know.\n\nEdit: The markdown filenames attempted to be clickable links, edited to add a space to stop it.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r6tj0j/lots_of_pain_finally_a_small_breakthrough_is_it/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5tej80",
          "author": "Professional-Yak4359",
          "text": "Thank you for sharing your experience. I am a scientist and I work on MatLab and cuda for computational purposes (mostly FP64 compute) so I am not as advanced as you are. \n\nMy experience for local llm has been decent actually. I used Opus (Claude Pro) to plan and write clear instruction. Then I use locally hosted qwen coder 30b (8 x 5070 ti) via cline and coder to implement the instruction by opus. Then I use Opus to verify. Once confirmed, I used qwen3 coder to run and monitor the running of the codes and for bugs (sometime overnight as my code is computationally expensive). The compute code itself runs on a tesla v100. \n\nThe dual machine setup works quite well for me. Qwen3 monitor and debug typos and dimensional mismatch.",
          "score": 7,
          "created_utc": "2026-02-17 05:13:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tjs3r",
              "author": "chensium",
              "text": "This is a cleverly efficient setup",
              "score": 5,
              "created_utc": "2026-02-17 05:54:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5u3cgj",
          "author": "social_tech_10",
          "text": "I'm in a similar boat, but running llama.cpp server in the garage, as both the api and webui, using llama-swap to see model status and live logs.  I have long years of coding experience, but only days of agentic coding experience, so I'm still trying to figure out what works.  For what it's worth, I've been using KiloCode plugin for VScode, which seems to work pretty well with a variety of models.  It's not flawless, but at least I haven't had any unintended files overwritten.  My workflow is git-based, but it's still something I'm glad I haven't had to deal with while using kilocode.  I've been test-driving a variety of different local models.  I'm currently enjoying Qwen3-Next-80B-A3B-Thinking-Q4, which runs faster and smarter than most I've tried.\n\nWould you mind sharing a link to your markdown files?  I'm not sure what I can learn from them, but that's kinda why I'm here, and anything might help.",
          "score": 2,
          "created_utc": "2026-02-17 08:50:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o616a18",
          "author": "cointegration",
          "text": "Chatgpt for architecture, sense making and wire framing. Local LLM (Qwen3 Coder) are only allowed to code up to class level/ui (coz I hate coding uis), nothing else, i get fine tuned control, the trade off is speed, but for an experienced dev it shouldn't be too bad. You can use opus to vibe, but i've seen too many horror stories trying to change stuff for edge cases. ",
          "score": 2,
          "created_utc": "2026-02-18 10:53:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6243vl",
          "author": "Ok_Hospital_5265",
          "text": "Still searching for an acceptable definition of â€œworksâ€â€¦ got Qwen 2.5 Coder 7B working w Claude Code locally (M1 16gigsâ€¦ yeah I knowâ€¦) but while the file creation/code writing â€œworksâ€, it screwed up something I thought was stupid simple (create a .md containing a Fibonacci sequence) and included duplicate numbers while also eating my limited context in fell swoop. \n\nI think thereâ€™s def a ground floor here hardware-wise for what local models can do unless youâ€™re strictly bored and looking for something to chat with. Iâ€™m obviously well below that floor.",
          "score": 2,
          "created_utc": "2026-02-18 14:29:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uk8i8",
          "author": "alias454",
          "text": "I tried using continue on my local box. It does work and would connect, but for me, the models were just too slow. I've only got an rtx2060 on my laptop. However, I've been meaning to setup that plugin to point at my little M4 Mac mini to see if that makes it bearable to use. I use that plugin with pycharm too.",
          "score": 1,
          "created_utc": "2026-02-17 11:26:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wfrtj",
              "author": "Minimum-Two-8093",
              "text": "Inference is one thing, but the single most important metric is VRAM on your GPU. Or in the case of MacOS; unified memory. How much RAM does your Mac mini have? If it's the same or less than your GPU on your PC, it's not going to be amazing (though your inference may be quicker).\n\nDon't factor in your system RAM on your PC, it's irrelevant, you don't ever want to swap to system RAM, it tanks your inference speed significantly.\n\nYou need to be able to fit your chosen model into your GPU VRAM entirely, and still have leftovers for context, if you can't do that don't even bother trying, you won't have even an adequate experience (same with Mac, but the reason they can be better is because they can have a single RAM allocation that's bigger than what a PC GPU can have - even my 4090 would technically be worse than a MacBook Air that has say 48GB unified memory, even though inference would still be faster for me, the MacBook could fit a much bigger model and have significantly more context, and that's the best indicator of how useful models are).",
              "score": 1,
              "created_utc": "2026-02-17 17:42:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6il217",
          "author": "stealthagents",
          "text": "Sounds like youâ€™ve got a solid system going. I get the frustration with cloud quotas, it can be such a pain when you hit the limits right when youâ€™re in the zone. Have you considered tweaking your local setup to balance some of the heavy lifting? Might save you some headaches and cash in the long run.",
          "score": 1,
          "created_utc": "2026-02-20 23:48:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ili6u",
          "author": "stealthagents",
          "text": "If you've already put in so much work, maybe try tweaking your SEO settings instead of starting over. There are also some SEO tools that can help analyze your site and point out issues without switching platforms. Donâ€™t give up just yet!",
          "score": 0,
          "created_utc": "2026-02-20 23:51:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jgnsb",
              "author": "Minimum-Two-8093",
              "text": "This isn't a site, SEO has nothing to do with it.",
              "score": 1,
              "created_utc": "2026-02-21 03:01:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ra9wsv",
      "title": "my portable ollama now has persistent memory",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/gallery/1ra9wsv",
      "author": "VaguneBob",
      "created_utc": "2026-02-20 22:39:02",
      "score": 22,
      "num_comments": 1,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1ra9wsv/my_portable_ollama_now_has_persistent_memory/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6m78z4",
          "author": "Swimming_Ad_5205",
          "text": "ÐŸÐ¸ÑˆÐµÑ‚ÑÑ Ð² Ñ„Ð°Ð¹Ð»? )))) Ð¾Ð½Ð¾ ÐµÐ³Ð¾ Ñ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ Ð¿Ð¾Ñ‚Ð¾Ð¼ ÑƒÑÑ‚Ð°Ð½ÐµÑ‚ Ð¸ Ð±ÑƒÐ´ÐµÑ‚ ÑÐ±Ð¸Ð²Ð°Ñ‚ÑŒÑÑ",
          "score": 0,
          "created_utc": "2026-02-21 15:49:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8iea9",
      "title": "ClawCache: Free local caching + tracking for Ollama calls â€“ cuts waste on repeats in agents/scripts inspired by efficiency threads here",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r8iea9/clawcache_free_local_caching_tracking_for_ollama/",
      "author": "SignificantClub4279",
      "created_utc": "2026-02-18 23:03:40",
      "score": 17,
      "num_comments": 6,
      "upvote_ratio": 0.91,
      "text": "In local LLM communities like this one, a common observation is that while local runs give excellent privacy and avoid cloud per-token fees, agents and scripts frequently re-process the same or similar prompts â€” especially in loops, tool calls, or debugging sessions â€” leading to wasted compute and time.\n\n**Common issues I've seen in recent posts:**\n\n* Long runs repeating queries and burning unnecessary GPU cycles\n* Agent frameworks chaining similar calls without reuse\n* Debugging sessions re-inferring the same content repeatedly\n* Searches for caching layers, context optimizations, or efficiency tools\n* Hybrid Ollama + API setups still incurring token costs on repeats\n* No easy built-in exact (or semantic) caching across most local workflows\n\n**The core problem:** Ollama makes local inference straightforward and powerful, but when building agents or complex scripts, exact repeats are frequent â€” and without caching, every call re-runs the model from scratch.\n\n**So I built ClawCache â€“ inspired by these local optimization discussions.**\n\nIt does three simple things:\n\n1. **Tracks every call** â€“ Logs Ollama (or any LLM) usage with token counts\n2. **Caches locally** â€“ SQLite-based, serves exact repeats from disk (proven \\~58% hit rate in agent-like workflows â†’ skips re-inference)\n3. **Gives reports** â€“ Daily CLI summaries: calls, hits, saved compute\n\n**Results from my own Ollama setups (agents + scripts):**\n\n* Tested on repetitive tasks/loops\n* \\~58% of calls cached\n* **Real savings** â€“ no re-running the model on repeats\n* Foundation for semantic caching (coming in Pro â€“ matches similar prompts)\n\n**Why it fits Ollama/local runs:**\n\n100% local â€“ everything on your machine, no telemetry. Works as a lightweight wrapper around any Ollama call (or other providers in hybrid).\n\n**Free forever. MIT licensed.**\n\n*bash*\n\n*pip install clawcache-free*\n\n**GitHub:**[ https://github.com/AbYousef739/clawcache-free](https://github.com/AbYousef739/clawcache-free)\n\n**If it saves you GPU cycles, a  on GitHub helps others find it.** \n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r8iea9/clawcache_free_local_caching_tracking_for_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o65s718",
          "author": "redonculous",
          "text": "Which model do you recommend is used with this?",
          "score": 2,
          "created_utc": "2026-02-19 00:57:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o666yjf",
              "author": "SignificantClub4279",
              "text": "It is completely model-agnostic.  it works as a wrapper around any LLM call (local or API), so you can use whatever model fits your OpenClaw or similar local agent setup.\n\n",
              "score": 1,
              "created_utc": "2026-02-19 02:23:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6c13e1",
          "author": "LithiumToast",
          "text": "Thanks. Really busy but I'll take a look this weekend. Thanks for your contributions and ideas.",
          "score": 2,
          "created_utc": "2026-02-19 23:54:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cwosv",
              "author": "SignificantClub4279",
              "text": "Thank you.",
              "score": 1,
              "created_utc": "2026-02-20 03:05:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6h4vvw",
          "author": "mjuevos",
          "text": "i want to try this for sure once i get everything tuned/optimized. because while setting up and testing is important, youre using the same prompt over and over and you want to see behavior change.. not a previously cached response.Â ",
          "score": 2,
          "created_utc": "2026-02-20 19:21:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ma17r",
              "author": "SignificantClub4279",
              "text": "Thank you for trying it. Please let me know about your experience with it, especially if it enhances anything for you. That would mean a lot to me.",
              "score": 1,
              "created_utc": "2026-02-21 16:03:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r7tga7",
      "title": "Intelligent (local + cloud) routing for OpenClaw via Plano",
      "subreddit": "ollama",
      "url": "https://i.redd.it/g9cwxqtwl6kg1.png",
      "author": "AdditionalWeb107",
      "created_utc": "2026-02-18 04:36:05",
      "score": 17,
      "num_comments": 5,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r7tga7/intelligent_local_cloud_routing_for_openclaw_via/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5zyci3",
          "author": "AdditionalWeb107",
          "text": "[https://github.com/katanemo/plano](https://github.com/katanemo/plano)Â   \n[https://github.com/katanemo/plano/tree/main/demos/llm\\_routing/openclaw\\_routing](https://github.com/katanemo/plano/tree/main/demos/llm_routing/openclaw_routing)",
          "score": 2,
          "created_utc": "2026-02-18 04:36:20",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o60fbsb",
          "author": "RA2B_DIN",
          "text": "Looks interesting but is there also a tool which just reduces token usage a lot?",
          "score": 2,
          "created_utc": "2026-02-18 06:47:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61x3yo",
          "author": "c0wpig",
          "text": "OpenClaw is just cyberwarfare disguised as a productivity app.\n\nDon't install this",
          "score": 2,
          "created_utc": "2026-02-18 13:53:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63hb97",
              "author": "Zealousideal_Bowl4",
              "text": "Sorry if itâ€™s obvious but can you elaborate?",
              "score": 1,
              "created_utc": "2026-02-18 18:15:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o669s81",
                  "author": "c0wpig",
                  "text": "Just running an application on your machine and giving it access to the internet and your personal information at the same time, when that application is coded by bots that have no concept of security is bad enough.\n\nBut then connecting that to language model providers? That's just crazy.\n\nhttps://simonwillison.net/2025/Jun/16/the-lethal-trifecta/\n\nYou probably shouldn't run these kinds of tools at all, but if you have to, use sandboxes + network proxies with guardrails\n\nAnd then there's the problem of sending all your private data to big companies that have a financial incentive to retain your private data and exploit it for profit.\n\nIt's a [dark forest](https://en.wikipedia.org/wiki/Dark_forest_hypothesis) out there, be careful.",
                  "score": 1,
                  "created_utc": "2026-02-19 02:39:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r87oyn",
      "title": "Mac mini m4 24gb",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r87oyn/mac_mini_m4_24gb/",
      "author": "Aromatic_Radio1650",
      "created_utc": "2026-02-18 16:30:21",
      "score": 16,
      "num_comments": 13,
      "upvote_ratio": 0.84,
      "text": "hi guys i was wondering what models i could run on a mac mini m4 24gb ram, would i be able to run gpt-oss:20b? or even a 30b model? or do i need to lower my standards and run a 14b model?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r87oyn/mac_mini_m4_24gb/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o62zgny",
          "author": "band-of-horses",
          "text": "gpt-oss:20b will run, but it's kinda slow and you can't use much context. You want the MLX version, which I believe is still easier to do in LM Studio than Ollama (but haven't tried in ollama lately) to run a bit better on apple silicon. And you're pretty much gonna want to stick to a 4k context window as you don't have enough ram for the model plus a large amount of context. \n\nIf you need to work with more data in the context window, you'll need to use a smaller model to have more ram for it. Qwen and Mistrial 4b - 8b models work decent depending on what you are doing. But will be FAR behind any cloud model in capabilities. ",
          "score": 6,
          "created_utc": "2026-02-18 16:55:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63gftj",
              "author": "strayduck0007",
              "text": "For certain kinds of tasks you can use Goose AI and point it to use ollama. It breaks tasks into smaller pieces that can often be completed individually within your given context window:\n\nGoose: [https://block.github.io/goose/docs/getting-started/installation/](https://block.github.io/goose/docs/getting-started/installation/)\n\nReference: [https://www.zdnet.com/article/claude-code-alternative-free-local-open-source-goose/](https://www.zdnet.com/article/claude-code-alternative-free-local-open-source-goose/) \n\nEven with this, yes, you will still be FAR behind cloud model in capabilities. But good enough for casual usage. ",
              "score": 1,
              "created_utc": "2026-02-18 18:11:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o648tmb",
          "author": "newz2000",
          "text": "Yes. I literally just posted exact details in this group one week ago. I ran it on my m2 with 24gb and gave all the parameters and settings needed.\n\nhttps://www.reddit.com/r/ollama/s/KPynFMGKQr",
          "score": 3,
          "created_utc": "2026-02-18 20:20:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62zhfa",
          "author": "v01dm4n",
          "text": "For local inference, usually q4 is preferred. Meaning \\~4-bits per parameter. gpt-oss:20b at q4 is about 13G. Plus some more memory for kv-cache etc. depending on context length.  So that should fit in the memory. \n\nBut 30b models are about 18-20g in size. That plus activations & kv-cache would be too much load imo. Because the OS also shares the same unified memory. So that'd increase memory pressure a lot.",
          "score": 2,
          "created_utc": "2026-02-18 16:55:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63qe2b",
          "author": "digitalnoises",
          "text": "https://github.com/Pavelevich/llm-checker\nresults will give you smaller then needed models\n\n\nOllama defaults to the following context lengths based on VRAM:\n< 24 GiB VRAM: 4k context\n24-48 GiB VRAM: 32k context\n>= 48 GiB VRAM: 256k context\n\nplus model",
          "score": 2,
          "created_utc": "2026-02-18 18:55:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65nji3",
          "author": "tonkajeep34",
          "text": "Iâ€™ve got an m4 air with 24gb and it runs gpt-oss20b decently, Iâ€™m not trying to do anything super crazy with it. Tried qwen coder today and had it do a python script for renaming pictures from exit data and it worked but was a little slow. Did 3 revisions on the script and they all worked with a little delay between the prompt and the start of the response. But was usable especially on a laptop.",
          "score": 2,
          "created_utc": "2026-02-19 00:31:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67y97l",
          "author": "Sharp-Mouse9049",
          "text": "20b q4 is the sweet spot on 24gb imo.\n30b technically fits but youâ€™ll feel the pressure fast once context grows. mac unified mem makes it less forgiving.",
          "score": 2,
          "created_utc": "2026-02-19 10:42:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62udnc",
          "author": "callmrplowthatsme",
          "text": "Yea",
          "score": 2,
          "created_utc": "2026-02-18 16:32:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62uhge",
              "author": "Aromatic_Radio1650",
              "text": "which question is that for?",
              "score": 2,
              "created_utc": "2026-02-18 16:33:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o63z7m2",
                  "author": "pioo84",
                  "text": "exactly",
                  "score": 1,
                  "created_utc": "2026-02-18 19:35:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o63er38",
          "author": "strayduck0007",
          "text": "I was just looking at this the other day. Qwen-code:30b with 64k context was using about 25GB on my M3 with 36GB of RAM and that was just about all it could handle. \n\ngpt-oss:20b was using 15GB. ",
          "score": 1,
          "created_utc": "2026-02-18 18:04:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63f2oz",
              "author": "Aromatic_Radio1650",
              "text": "so would you say 32gb is enought to efficiently run a 30b model?",
              "score": 2,
              "created_utc": "2026-02-18 18:05:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o63fwn6",
                  "author": "strayduck0007",
                  "text": "I don't know of the edge cases in which it would fail, but yes, you should be able to do basic things with a 30b model with 32GB of RAM.",
                  "score": 1,
                  "created_utc": "2026-02-18 18:09:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rc3srb",
      "title": "I ran ClawBot with Ollama locally on my Mac â€” setup, gotchas, and honest review",
      "subreddit": "ollama",
      "url": "https://v.redd.it/1ee663elb5lg1",
      "author": "Spirited-Wind6803",
      "created_utc": "2026-02-23 01:19:58",
      "score": 16,
      "num_comments": 0,
      "upvote_ratio": 0.77,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r8a164",
      "title": "ðŸ–‹ï¸ Just released AI-Writer: A free, offline desktop app for AI-assisted writing powered by Ollama + PyQt5\nBody:",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r8a164/just_released_aiwriter_a_free_offline_desktop_app/",
      "author": "Reasonable_Brief578",
      "created_utc": "2026-02-18 17:52:42",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 0.81,
      "text": "I'm excited to share a project I've been working on: \\*\\*AI-Writer\\*\\* â€” a sleek, privacy-focused desktop app that lets you write with your \\*local\\* LLMs via Ollama. No API keys, no cloud, no telemetry. Just you, your words, and your model.\n\nðŸ”— \\*\\*GitHub:\\*\\* [https://github.com/Laszlobeer/AI-Writer](https://github.com/Laszlobeer/AI-Writer)\n\nhttps://preview.redd.it/3oi0wof5kakg1.png?width=1076&format=png&auto=webp&s=1687bc6dd7da2820e48afdf040d6365e689416a1\n\n\\### âœ¨ What it does:\n\n\\- ðŸ¤– \\*\\*AI Text Completion\\*\\*: Highlight text or place your cursor and let your local model continue your story, article, or notes\n\n\\- ðŸŽ¨ \\*\\*Light/Dark Mode\\*\\*: Because eyes matter\n\n\\- ðŸŒ¡ï¸ \\*\\*Temperature & Token Controls\\*\\*: Fine-tune creativity vs. focus on the fly\n\n\\- ðŸ“š \\*\\*Model Switching\\*\\*: Instantly swap between any Ollama models you have installed\n\n\\- ðŸ’¾ \\*\\*Export Flexibility\\*\\*: Save your work as \\`.txt\\` or \\`.docx\\` (Word-compatible)\n\n\\- âŒ¨ï¸ \\*\\*Keyboard Shortcuts\\*\\*: Write faster with intuitive hotkeys\n\n\\### ðŸ› ï¸ Built with:\n\n\\- Python 3.8+\n\n\\- PyQt5 for the GUI\n\n\\- Requests for Ollama API communication\n\n\\- python-docx for Word export\n\n\\### ðŸš€ Quick Start:\n\n\\`\\`\\`bash\n\nollama pull thewindmom/hermes-3-llama-3.1-8b\n\n\\# 2. Clone & install\n\ngit clone [https://github.com/Laszlobeer/AI-Writer.git](https://github.com/Laszlobeer/AI-Writer.git)\n\ncd AI-Writer\n\npip install -r requirements.txt\n\n\\# 3. Launch\n\npython ai\\_writer.py\n\n\\`\\`\\`\n\n\\### ðŸ’¡ Why I built this:\n\nI wanted a distraction-free writing environment that leverages local AI \\*without\\* sending my drafts to the cloud. Whether you're drafting fiction, technical docs, or journaling â€” AI-Writer keeps your workflow private and under your control.\n\n\\### ðŸ™ Feedback welcome!\n\nThis is my first major PyQt5 project, so I'd love to hear:\n\n\\- What features would make your writing workflow better?\n\n\\- Any bugs or UX quirks you spot?\n\n\\- Ideas for export formats or integrations?\n\nAll contributions and suggestions are welcome on GitHub! ðŸ™Œ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r8a164/just_released_aiwriter_a_free_offline_desktop_app/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r7c951",
      "title": "Is a AI HX370 with 96GB Laptop Good enough for LLMs?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r7c951/is_a_ai_hx370_with_96gb_laptop_good_enough_for/",
      "author": "PersonSuitTV",
      "created_utc": "2026-02-17 17:17:24",
      "score": 12,
      "num_comments": 17,
      "upvote_ratio": 0.88,
      "text": "I am looking to get a laptop and was wondering if one with an AMD Ryzen AI 9 HX PRO 370 and 96GB of unified memory would be good for LLMs. And I mean in the sense of will this Laptop perform well enough, or is that CPU just not going to cut it for running a LLM?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r7c951/is_a_ai_hx370_with_96gb_laptop_good_enough_for/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5wexmn",
          "author": "boba-cat02",
          "text": "Yes good to host gpt oss models",
          "score": 3,
          "created_utc": "2026-02-17 17:38:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wpkol",
          "author": "a_pimpnamed",
          "text": "Ooo yeah that's plenty glm4.7 flash and qwen3_coder_next quantitized would work just fine on your laptop.",
          "score": 3,
          "created_utc": "2026-02-17 18:27:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6epkwf",
              "author": "tedstr1ker",
              "text": "Glm4.7-Flash can be run on the NPU? Did I miss something?",
              "score": 1,
              "created_utc": "2026-02-20 12:03:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wj8gj",
          "author": "XxCotHGxX",
          "text": "You should be able to run 70B-80B models with no problems",
          "score": 2,
          "created_utc": "2026-02-17 17:58:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62tkva",
              "author": "Responsible-Bonus568",
              "text": "What limits do 70B-80B have?",
              "score": 1,
              "created_utc": "2026-02-18 16:29:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o63qqvf",
                  "author": "XxCotHGxX",
                  "text": "They are reasonably intelligent. They can handle everyday tasks. You will still have to use a high reasoning model for difficult tasks. I would not use a local model to run a open claw bot. They are not as smart as the frontier models and are open to attack.",
                  "score": 2,
                  "created_utc": "2026-02-18 18:56:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5xag6d",
          "author": "vuduguru",
          "text": "Absolutely",
          "score": 2,
          "created_utc": "2026-02-17 20:05:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wcz2q",
          "author": "Civil_Tea_3250",
          "text": "Unified RAM? I had no idea that was an option on a laptop other than a Mac.\n\nIt sounds like you'll be able to run a lot of great LLMs without issue. Depends on how big you're trying to go.",
          "score": 1,
          "created_utc": "2026-02-17 17:29:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wdzsb",
              "author": "PersonSuitTV",
              "text": "ya AMD did that with their AI 300 series CPUs to try and compete with what Apple is doing. They claim they support up to 256GB, but I have never see an offering with more than 96GB.",
              "score": 5,
              "created_utc": "2026-02-17 17:34:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5xohbz",
                  "author": "InfraScaler",
                  "text": "There are AMD AI MAX+ 395 with 128GB everywhere :)",
                  "score": 3,
                  "created_utc": "2026-02-17 21:12:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5wem2r",
                  "author": "Civil_Tea_3250",
                  "text": "Ah, that's awesome. I had no idea.\n\nOnly thing I will say, if you can get it in a desktop variation then do that. Laptops will always have more limitations due to size, power draw, or heat. Idk if thats an option for you, and I'm sure it'll be good, but why not get that extra oomph and be able to upgrade cooling or whatever too.",
                  "score": 1,
                  "created_utc": "2026-02-17 17:37:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5y9sza",
                  "author": "No_Clock2390",
                  "text": "AMD APUs have had unified RAM for years. You can buy their new AI 395 chip with 128GB unified RAM.",
                  "score": 1,
                  "created_utc": "2026-02-17 22:55:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5y5nsu",
              "author": "scrawnylifter",
              "text": "Itâ€™s becoming more common with the hardware supply issues too. My company uses Dell and theyâ€™ve made some with unified RAM here and there, but now with the shortages itâ€™s more common",
              "score": 1,
              "created_utc": "2026-02-17 22:34:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ydmxe",
                  "author": "Civil_Tea_3250",
                  "text": "I hate to hear that. I can see use cases like AI to increase RAM without increasing price as much, or companies that never let you upgrade it like Apple doing it with their M chips, but it's a shame this is how bad it is.",
                  "score": 1,
                  "created_utc": "2026-02-17 23:16:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5y91xd",
          "author": "RickLXI",
          "text": "I think a 30b model with 32k context window would work well. ",
          "score": 1,
          "created_utc": "2026-02-17 22:51:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ykca8",
          "author": "Zyj",
          "text": "Itâ€˜s going to be slow because itâ€™s dual channel RAM unlike Ryzen AI Max+ which has quad channel RAM.",
          "score": 1,
          "created_utc": "2026-02-17 23:53:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xsthx",
          "author": "The_Establishmnt",
          "text": "Yes, it is possible to run a 70-billion parameter (70B) Large Language Model (LLM) on a system with anÂ \n\n[**AMD Ryzen AI 9 HX 370**](https://www.google.com/search?ibp=oshop&prds=pvt:hg,pvo:29,imageDocid:259565229188313429,headlineOfferDocid:9452828027042716451,productDocid:9452828027042716451&q=product&sa=X&ved=2ahUKEwiQ-O7CvOGSAxVm1vACHRWwFL8Qxa4PegYIAQgCEAM)\n\n, provided you have sufficientÂ **system RAM (64GB or more)**Â and utilize specific software optimizations like quantization and offloading. However, performance will be a limiting factor, and it will not be as fast as a dedicated multi-GPU setup.Â \n\nFeasibility and Performance\n\nTheÂ \n\nAMD Ryzen AI 9 HX 370\n\nÂ uses unified memory, meaning the integrated GPU (iGPU) and NPU share the main system RAM. The primary limitation for running large models like a 70B LLM is memory capacity and bandwidth, not raw compute power.Â \n\n* **Memory Requirements**: A 70B model requires significant memory. In its original format, it could need over 80GB of VRAM or unified memory. However, using quantization (reducing the precision of the model's parameters, e.g., to 4-bit or 8-bit), the memory footprint can be reduced to around 40-56GB. Therefore, at least 64GB of system RAM is needed, with 96GB or 128GB being preferable for a smoother experience and larger context windows.\n* **Performance Expectations**: While the model can \"run,\" the speed will be slow compared to professional-grade hardware.\n   * One user successfully ran a quantized 80B model on anÂ  [AMD Ryzen AI 9 HX PRO 370](https://www.google.com/search?ibp=oshop&prds=pvt:hg,pvo:29,mid:576462826270682533,imageDocid:6622906445780665015,gpcid:1922910637295993356,headlineOfferDocid:11974420056489500570,catalogid:8464406605464093926,productDocid:3926129318797190370&q=product&sa=X&ved=2ahUKEwiQ-O7CvOGSAxVm1vACHRWwFL8Qxa4PegYIAQgFEAQ) Â system with 96GB of DDR5-5600 RAM and achieved an inference speed of aroundÂ **18 tokens per second**Â (tok/s) using the integrated graphics andÂ `llama.cpp`Â optimizations.\n   * For comparison, high-end dedicated GPUs can achieve 60-100 tok/s on similar models. TheÂ  Ryzen AI 9 HX 370 's NPU, while powerful for its class (50 TOPS), is better suited for smaller 7B or 13B models and AI-specific tasks like noise reduction, rather than running a full 70B model entirely.\n* **Optimization**: Performance heavily relies on software optimization tools likeÂ `llama.cpp`Â and frameworks that leverage the integrated GPU (using Vulkan offloading) to manage the substantial memory demands efficiently.Â ",
          "score": -1,
          "created_utc": "2026-02-17 21:32:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r77o7d",
      "title": "REASONING AUGMENTED RETRIEVAL (RAR) is the production-grade successor to single-pass RAG.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r77o7d/reasoning_augmented_retrieval_rar_is_the/",
      "author": "frank_brsrk",
      "created_utc": "2026-02-17 14:37:05",
      "score": 12,
      "num_comments": 9,
      "upvote_ratio": 0.83,
      "text": "Single-pass rag retrieves once and hopes the model stitches fragments into coherent reasoning. It fails on multi-hop questions, contradictions, temporal dependencies, or cases needing follow-up fetches.Rar puts reasoning first. The system decomposes the problem, identifies gaps, issues precise (often multiple, reformulated, or negated) retrievals.  \nintegrates results into an ongoing chain-of-thought, discards noise or conflicts, and loops until the logic closes with high confidence.\n\nMeasured gains in production:\n\n\\-35â€“60% accuracy lift on multi-hop, regulatory, and long-document tasks  \n\\-far fewer confident-but-wrong answers  \n\\-built-in uncertainty detection and gap admission  \n\\-traceable retrieval decisions\n\nTraining data must include:  \n\\-interleaved reasoning + retrieval + reflection traces  \n\\-negative examples forcing rejection of misleading chunks  \n\\-synthetic trajectories with hidden multi-hop needs  \n\\-confidence rules that trigger extra cycles\n\nRar turns retrieval into an active part of thinking instead of a one  time lookup. Systems still using single  pass dense retrieval in 2026 accept unnecessary limits on depth, reliability, and explainability. RAR is the necessary direction.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r77o7d/reasoning_augmented_retrieval_rar_is_the/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5vl7cl",
          "author": "immediate_a982",
          "text": "Sure, but whereâ€™s the research paper to back this up",
          "score": 2,
          "created_utc": "2026-02-17 15:11:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w0eag",
              "author": "frank_brsrk",
              "text": "[https://arxiv.org/pdf/2509.22713](https://arxiv.org/pdf/2509.22713)\n\nRAR2 : Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval\n\n\\---\n\nand here you can find a solid dataset example of rar , augmented with graph instructions, CoT, (included)\n\n[https://huggingface.co/datasets/frankbrsrk/causal-ability-injectors](https://huggingface.co/datasets/frankbrsrk/causal-ability-injectors)",
              "score": 3,
              "created_utc": "2026-02-17 16:26:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5w3w1g",
          "author": "UseMoreBandwith",
          "text": "is this any different from  'corrective RAG' ?",
          "score": 1,
          "created_utc": "2026-02-17 16:44:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5z13ak",
          "author": "Electrical-Cod9132",
          "text": "Is this different from tool calling and memory management?",
          "score": 1,
          "created_utc": "2026-02-18 01:25:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60mlr8",
              "author": "frank_brsrk",
              "text": "No different from tool calling, it's RAG, but retrieved data, \"injects \" constraint enforcements, total behavior override (100%) it ensures less model drift  even after long iterations + multi step Cot for reasoning trace ,\n to sort of offload cognition from ai, and let it use compute necessary for the rest of the query with reasoning already constructed.\n\nYou just upsert dataset in a rag, with clear metadata, and you expect it to be retrieved on every call opportunistically, or you keep it in a namespace separate with top k 1, so u always get that flavored 1 row constraint",
              "score": 1,
              "created_utc": "2026-02-18 07:52:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o606jz1",
          "author": "fasti-au",
          "text": "5 times the loops and guessing for context on a fragment base is fairly cool but how do you deal with think being itâ€™s own graph in debug or are you series one shotting?",
          "score": 1,
          "created_utc": "2026-02-18 05:35:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67yfl1",
          "author": "SharpRule4025",
          "text": "The multi-pass retrieval approach is solid in theory but it amplifies the data quality problem. If your corpus has noisy chunks from messy extraction, one bad retrieval in single-pass RAG becomes three or four bad retrievals in a reasoning loop. Each pass compounds the noise.\n\nThe accuracy lifts they claim probably come as much from the re-formulated queries as from the multi-pass approach itself. A decomposed question that targets specific fields (price, date, section heading) retrieves better than a vague semantic search, regardless of how many passes you do.\n\nI'd focus on the ingestion quality first. If your source data comes in as structured fields instead of raw markdown dumps, even single-pass RAG gets you most of the way there. The multi-pass stuff helps when the question genuinely requires combining info from multiple documents, but that's a smaller percentage of queries than people think.",
          "score": 1,
          "created_utc": "2026-02-19 10:43:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68a0yw",
              "author": "frank_brsrk",
              "text": "Hey mate, excellent breakdown ;))\n\nI'd love that u look at the data, I've passed an update on it. U'd appreciate the corpus structure!\n\nhttps://huggingface.co/datasets/frankbrsrk/causal-ability-injectors\n\nWrite me for any feedback r/SharpRule4025",
              "score": 1,
              "created_utc": "2026-02-19 12:18:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o68i5l9",
                  "author": "SharpRule4025",
                  "text": "Thanks, appreciate it. Will check out the dataset and the update. The causal structure looks interesting from the corpus design side.",
                  "score": 2,
                  "created_utc": "2026-02-19 13:12:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r7cfaz",
      "title": "I built a beginnerâ€‘friendly PowerShell installer for custom GGUF models in Ollama (autoâ€‘detects GGUF, builds Modelfile, no file moving needed",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r7cfaz/i_built_a_beginnerfriendly_powershell_installer/",
      "author": "lAVENTUSl",
      "created_utc": "2026-02-17 17:23:14",
      "score": 9,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "I kept seeing people struggle with installing custom GGUF models into Ollama on Windows, so I built a **simple, colorâ€‘coded PowerShell installer** that handles the whole process automatically.\n\n**Features:**\n\n* Run the script from *any* folder\n* Autoâ€‘detects your GGUF file\n* Creates the model folder for you\n* Generates a valid Modelfile\n* Registers the model with Ollama\n* Optional: run the model immediately\n* Beginnerâ€‘friendly, clean output, no guesswork\n\nI also added a fix + explanation for the common PowerShell error:  \n**â€œrunning scripts is disabled on this systemâ€**  \nso new users donâ€™t get stuck.\n\nGitHub repo:  \n[**Ollama-model-installer**](https://github.com/Glenn762702/ollama-model-installer)\n\nIf you try it, Iâ€™d love feedback or suggestions for new features.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r7cfaz/i_built_a_beginnerfriendly_powershell_installer/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o5xgdra",
          "author": "TonyDRFT",
          "text": "Thank you for creating and sharing this! Does it work with VL versions too? (That have the seperate extra file)",
          "score": 1,
          "created_utc": "2026-02-17 20:33:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5y7il9",
              "author": "lAVENTUSl",
              "text": "The script only assumes 1 gguf file, but it could be easily modified for both.",
              "score": 1,
              "created_utc": "2026-02-17 22:43:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rarsuo",
      "title": "TIFU/PSA: didnâ€™t check which GPU ollama was using and was stuck wondering why so slow",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rarsuo/tifupsa_didnt_check_which_gpu_ollama_was_using/",
      "author": "IAmANobodyAMA",
      "created_utc": "2026-02-21 13:59:43",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "Not sure where to tell this story (megathread?) but it made me laugh and has a teachable moment so I thought I would share.\n\nTL;DR: I was running ollama on an old GPU by mistake ðŸ¤¦â€â™‚ï¸\n\nI have two local machines running ollama. My gaming rig has a 5070ti 16gb but isnâ€™t always on, and my â€œdedicatedâ€ unraid server \\*\\*had\\*\\* a 3060ti 8gb that was going to be my AI workhorse.\n\nI chose models that would kick ass on the 5070 when online and more conservative models for the 3060 otherwise.\n\nThis is all still experimental/for learning so this setup is fines for me â€¦ except the unraid server was painfully slow. Took me way too long to figure out thatâ€™s because it was hitting an old GTX 1650 4gb card!! I forgot I swapped out the cards because I was going to build a gaming rig for my kid with the 3060.\n\nI spent way too long researching models and trying to figure out why my â€œ3060â€ was offloading over 50% of qwen3:4b to my CPU. Since this is hosted on unraid I was convinced that another service (plex?) was using my GPU without permission. Nope, Iâ€™m just a doofus.\n\nIt wasnâ€™t until running \\`nvidia-smi\\` in terminal that I realized my error.\n\nAnyways, hope this makes someone chuckle as much as me. Anyone else have some fun â€œdohâ€ moments?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rarsuo/tifupsa_didnt_check_which_gpu_ollama_was_using/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r8gfwa",
      "title": "Schedule mode is coming to Nanocoder... Run project background tasks on a cron schedule ðŸš€",
      "subreddit": "ollama",
      "url": "https://v.redd.it/4c72kn5rnbkg1",
      "author": "willlamerton",
      "created_utc": "2026-02-18 21:48:14",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r8gfwa/schedule_mode_is_coming_to_nanocoder_run_project/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o673cqz",
          "author": "Own-Swan2646",
          "text": "So kind like openclaw but without all the other stuff?",
          "score": 2,
          "created_utc": "2026-02-19 05:57:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67vy4w",
              "author": "willlamerton",
              "text": "Yes, coding agent automations specifically. It cuts out a lot of the unnecessary stuff that OpenClaw has",
              "score": 2,
              "created_utc": "2026-02-19 10:20:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r8kj39",
      "title": "Building local for general chatbot, code assistance, and content creation - Need help with model selection",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r8kj39/building_local_for_general_chatbot_code/",
      "author": "RobDoesData",
      "created_utc": "2026-02-19 00:33:12",
      "score": 7,
      "num_comments": 6,
      "upvote_ratio": 0.89,
      "text": "I work in data engineering and analytics. I am building a a locally hosted chatbot to help me in my work. I have it setup so that I can have 3 standalone chats at one time and jump between them while their context and conversation is isolated from one another. Each chat has its own specified temp, top\\_p, max tokens, system prompt, and LLM.\n\nI'm after recommendations for models to use, system prompts and parameter values for the following usecases:\n\n1. General chatbot for all sorts of things related to work, tech, reporting, etc.\n2. Code generation (Python) and Azure\n3. Content review, editing, and brainstorming\n\nI'm running a 5070TI with 12 GB VRAM. Looking forward to your responses.\n\n  \nI've tried Llama3, Phi4, and Mistral-Nemo 12B thus far.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r8kj39/building_local_for_general_chatbot_code/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o66ud66",
          "author": "Own-Swan2646",
          "text": "Llm-checker",
          "score": 1,
          "created_utc": "2026-02-19 04:49:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67aoss",
          "author": "Crafty_Ball_8285",
          "text": "I just bought a 5070ti with 16gb VRAM. Are you sure yours is 12?",
          "score": 1,
          "created_utc": "2026-02-19 06:58:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68qmvl",
              "author": "RobDoesData",
              "text": "Yes it's 12",
              "score": 1,
              "created_utc": "2026-02-19 14:01:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6g6of5",
          "author": "Ok-Sink-286",
          "text": "You can add your setup on Hugging Face, it will tell you which model you can run !",
          "score": 1,
          "created_utc": "2026-02-20 16:44:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6judmj",
              "author": "RobDoesData",
              "text": "I want more than just the model to run. I want input for the model parameters",
              "score": 1,
              "created_utc": "2026-02-21 04:35:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}