{
  "metadata": {
    "last_updated": "2026-01-21 08:44:59",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 20,
    "total_comments": 88,
    "file_size_bytes": 98772
  },
  "items": [
    {
      "id": "1qh5z0j",
      "title": "I built a voice-first AI mirror that runs fully on Ollama.",
      "subreddit": "ollama",
      "url": "https://v.redd.it/bjeyts2qibeg1",
      "author": "DirectorChance4012",
      "created_utc": "2026-01-19 14:43:25",
      "score": 314,
      "num_comments": 44,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qh5z0j/i_built_a_voicefirst_ai_mirror_that_runs_fully_on/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0hf0l3",
          "author": "ThomasNowProductions",
          "text": "How far we have come ;-) It is real nice tho",
          "score": 23,
          "created_utc": "2026-01-19 14:47:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hfop5",
              "author": "DirectorChance4012",
              "text": "Thanks!",
              "score": 6,
              "created_utc": "2026-01-19 14:51:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0hr4is",
                  "author": "mlt-",
                  "text": "Did you aim for that Pal thing look from Netflix cartoon The Mitchells vs. the Machines?",
                  "score": 6,
                  "created_utc": "2026-01-19 15:46:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0lqmr5",
                  "author": "redditissocoolyoyo",
                  "text": "Start a Kickstarter! Make it a real product. People will buy it!",
                  "score": 3,
                  "created_utc": "2026-01-20 03:31:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0i5cwe",
          "author": "croninsiglos",
          "text": "Need one with a vision model that both gives compliments and body shames me when appropriate. Must support tool use and connect with my scale.\n\nYou know, it also needs a more humanoid avatar that can  answer a simple question about who is the fairest of them all. The answer should be obvious, but I just want to be sure.",
          "score": 7,
          "created_utc": "2026-01-19 16:49:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0j0wgi",
              "author": "ScoreUnique",
              "text": "SmolVlm",
              "score": 2,
              "created_utc": "2026-01-19 19:10:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0tfvmu",
              "author": "Fuzzy_Independent241",
              "text": "WARNING: might have serious consequences if left to control prescription magical drugs for others in your kingdom!",
              "score": 1,
              "created_utc": "2026-01-21 07:28:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0hgxfn",
          "author": "Money-Frame7664",
          "text": "That is really fun ! Congratulations üëè \nDo you have connectors with any home automation system? (Or MCP capacity?)",
          "score": 5,
          "created_utc": "2026-01-19 14:57:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ho7ss",
          "author": "Travelosaur",
          "text": "Mirror mirror on the wall... You made it a reality bro. Nice job!",
          "score": 4,
          "created_utc": "2026-01-19 15:32:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jhgq8",
          "author": "ServeAlone7622",
          "text": "This is the future",
          "score": 4,
          "created_utc": "2026-01-19 20:27:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jtr8r",
          "author": "klei10",
          "text": "Where did you get the mirror ?",
          "score": 3,
          "created_utc": "2026-01-19 21:25:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0l4fx0",
              "author": "DirectorChance4012",
              "text": "I covered the details here: [https://noted.lol/mirrormate/](https://noted.lol/mirrormate/).  \nThe mirror was purchased from [https://www.e-kagami.com/](https://www.e-kagami.com/).",
              "score": 1,
              "created_utc": "2026-01-20 01:29:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0lamvu",
          "author": "KernelFlux",
          "text": "That‚Äôs very cool and I want to build one!",
          "score": 3,
          "created_utc": "2026-01-20 02:03:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mjfif",
          "author": "Easy_Cable6224",
          "text": "so cool, this is something I wouldn't imagine back when I was a kid, we are IN the future now!",
          "score": 3,
          "created_utc": "2026-01-20 06:49:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hki0u",
          "author": "kiyyik",
          "text": "Thank you for sharing this! Looking forward to doing something similar.",
          "score": 2,
          "created_utc": "2026-01-19 15:15:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hs0z2",
          "author": "stoopwafflestomper",
          "text": "Very cool!",
          "score": 2,
          "created_utc": "2026-01-19 15:50:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hss7j",
              "author": "DirectorChance4012",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-19 15:53:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ht3w8",
          "author": "kkiran",
          "text": "Awesome, kudos! Mac Studio hosted AI mirror sounds cool. I wanted to justify my home lab spending and this fits part of the bill. Thank you!",
          "score": 2,
          "created_utc": "2026-01-19 15:54:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0htbp3",
              "author": "DirectorChance4012",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-19 15:55:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0hyitp",
          "author": "irodov4030",
          "text": "super cool!",
          "score": 2,
          "created_utc": "2026-01-19 16:19:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hzg6o",
              "author": "DirectorChance4012",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-19 16:23:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0i36ve",
          "author": "Reasonable_Brief578",
          "text": "Beautiful",
          "score": 2,
          "created_utc": "2026-01-19 16:39:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jk7b9",
          "author": "roshan231",
          "text": "That‚Äôs cool",
          "score": 2,
          "created_utc": "2026-01-19 20:40:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ju6s9",
          "author": "After_Construction72",
          "text": "Was only looking into similar over the weekend",
          "score": 2,
          "created_utc": "2026-01-19 21:27:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jyutj",
          "author": "No_Thing8294",
          "text": "Thanks for sharing! Very nice project! I love the face of the agent!",
          "score": 2,
          "created_utc": "2026-01-19 21:51:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kjwd0",
          "author": "thunder-wear",
          "text": "This is really awesome. I love it!",
          "score": 2,
          "created_utc": "2026-01-19 23:38:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ksxo6",
          "author": "Relevant_Middle_4779",
          "text": "Wow!!!",
          "score": 2,
          "created_utc": "2026-01-20 00:26:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rlzzf",
          "author": "zaschmaen",
          "text": "Damn nice! Thats what i wanna make but with an comic character or smth like thisüëå",
          "score": 2,
          "created_utc": "2026-01-21 00:18:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0syczv",
          "author": "wittlewayne",
          "text": "shit.... beat me to it !",
          "score": 2,
          "created_utc": "2026-01-21 05:06:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0t9gdr",
          "author": "beast_modus",
          "text": "Good Job‚Ä¶Great‚Ä¶",
          "score": 2,
          "created_utc": "2026-01-21 06:32:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tg0t5",
          "author": "Fuzzy_Independent241",
          "text": "That looks cool! Did you embed the hardware or is it calling Ollama over your lan / VPN ?",
          "score": 2,
          "created_utc": "2026-01-21 07:29:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tg6nd",
              "author": "Fuzzy_Independent241",
              "text": "PS - noticed your GH link on my phone. Will check the repo. Thx!",
              "score": 2,
              "created_utc": "2026-01-21 07:31:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0iv6rl",
          "author": "BringOutYaThrowaway",
          "text": "I need to know where you got the glass?",
          "score": 1,
          "created_utc": "2026-01-19 18:45:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0l4h3z",
              "author": "DirectorChance4012",
              "text": "I covered the details here: [https://noted.lol/mirrormate/](https://noted.lol/mirrormate/).  \nThe mirror was purchased from [https://www.e-kagami.com/](https://www.e-kagami.com/).",
              "score": 1,
              "created_utc": "2026-01-20 01:29:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0l6nc9",
          "author": "MeatballStroganoff",
          "text": "How long is it from prompt to response? It seems like you cut the video right before it starts speaking.",
          "score": 1,
          "created_utc": "2026-01-20 01:41:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0l8eh0",
              "author": "DirectorChance4012",
              "text": "I takes roughly 5-7 second.",
              "score": 2,
              "created_utc": "2026-01-20 01:51:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0l92mv",
                  "author": "MeatballStroganoff",
                  "text": " Not bad, thanks!",
                  "score": 1,
                  "created_utc": "2026-01-20 01:55:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0n7ehn",
          "author": "JacketHistorical2321",
          "text": "How does the vision plugin work? I don't see any documentation besides referencing it exists",
          "score": 1,
          "created_utc": "2026-01-20 10:28:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0plcut",
          "author": "Affectionate_Bus_884",
          "text": "![gif](giphy|ZRICsU0zv4tJS)",
          "score": 1,
          "created_utc": "2026-01-20 18:27:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0skjvs",
          "author": "Ordinary_Marketing36",
          "text": "Having a Ktop of 5 means that you will always recover 5 of the best memories?",
          "score": 1,
          "created_utc": "2026-01-21 03:35:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nw31u",
          "author": "NickMcGurkThe3rd",
          "text": "the problem with those videos is that they are always cut to cache/hide the fact that it takes the like 10 seconds for that thing to respond and make it seem seemless",
          "score": 1,
          "created_utc": "2026-01-20 13:30:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nwd8p",
              "author": "DirectorChance4012",
              "text": "yeah, it actually takes 5-7second.\nhttps://www.reddit.com/r/ollama/s/8b6TeRsRFx",
              "score": 3,
              "created_utc": "2026-01-20 13:32:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qd4snz",
      "title": "Building Opensource client sided Code Intelligence Engine -- Potentially deeper than Deep wiki :-) ( Need suggestions and feedback )",
      "subreddit": "ollama",
      "url": "https://v.redd.it/clkvriftsedg1",
      "author": "DeathShot7777",
      "created_utc": "2026-01-15 00:39:37",
      "score": 47,
      "num_comments": 15,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qd4snz/building_opensource_client_sided_code/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzn9gez",
          "author": "Spaceman_Splff",
          "text": "I don‚Äôt even know what‚Äôs happening here but it looks awesome. Good work.",
          "score": 6,
          "created_utc": "2026-01-15 00:54:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nznaawx",
              "author": "DeathShot7777",
              "text": "![gif](giphy|sWBzg2D15WwQjHcxbt)",
              "score": 6,
              "created_utc": "2026-01-15 00:58:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzn91nd",
          "author": "UseHopeful8146",
          "text": "That looks pretty cool actually, any plans to write up docker configs?",
          "score": 2,
          "created_utc": "2026-01-15 00:51:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzn9typ",
              "author": "DeathShot7777",
              "text": "Right now I m trying to integrate ollama, external DB connect ( Neo4j ) and some way to auto track github for updates. If u wanna use it somewhere or got any ideas I would work on creating docker config though",
              "score": 2,
              "created_utc": "2026-01-15 00:56:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzodn5u",
                  "author": "UseHopeful8146",
                  "text": "No pressure, I‚Äôll pm you or something if I put it together - gotta get the food off the plate I already got first lol",
                  "score": 2,
                  "created_utc": "2026-01-15 04:59:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzwx3ho",
          "author": "koldbringer77",
          "text": "This is most beautifull thing ive dreamt to code, the messy mess that works,  I propouse sth like Engram filtering to enchance human readability",
          "score": 2,
          "created_utc": "2026-01-16 13:10:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx1grm",
              "author": "DeathShot7777",
              "text": "Thanks means a lot. Idk what's Engram filter will check it out.\nIt crossed 300 stars I m so happy üò≠",
              "score": 1,
              "created_utc": "2026-01-16 13:35:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzt6le9",
          "author": "RGBrayan",
          "text": "I don't know much about it, but it looks very good. What does it consist of?",
          "score": 1,
          "created_utc": "2026-01-15 22:08:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztp7m7",
              "author": "DeathShot7777",
              "text": "That graph you are seeing is the knowledge graph created by parsing all the relations of your codebase. So this gives very deep insight about the codebases to LLMs as well as let's humans visualize it. \n\nI am trying to resolve the fundamental gap of incomplete context of the codebase in coding agents like cursor , claude code, etc. leading to them making breaking changes",
              "score": 2,
              "created_utc": "2026-01-15 23:43:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o008k4w",
          "author": "BallinwithPaint",
          "text": "This sounds super interesting! I've been looking for something that handles code relations better than standard context tools. I‚Äôm going to give it a spin soon‚Äîif it clicks, I‚Äôd love to dig into the repo and see if I can contribute some of those ideas you mentioned. Just starred it!",
          "score": 1,
          "created_utc": "2026-01-16 22:29:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o008z2x",
              "author": "DeathShot7777",
              "text": "Thanks..feel free to raise issues or DM me or use the github discussions",
              "score": 1,
              "created_utc": "2026-01-16 22:31:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0oms7s",
          "author": "dellorsod",
          "text": "How does it work for repos with multiple languages used? I have repo with 90%+ of codebase being kotlin, and small portion of few files being in typescript. I uploaded it and it indexed the typescript code in-depth but entire kotlin codebase was just files/folders? Any way to change the language it indexes?",
          "score": 1,
          "created_utc": "2026-01-20 15:48:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0p639i",
              "author": "DeathShot7777",
              "text": "So right now it supports only python, js and ts, and it wont have any problem with multilanguage codebases as long as it has supported languages. It will detect the file type (.js / .ts / .tsx / .py ) etc. I will be adding support for java, C and Go soon.",
              "score": 1,
              "created_utc": "2026-01-20 17:17:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0p6q4t",
          "author": "DeathShot7777",
          "text": "Guys my project just crossed 340 stars. I never imagined my final year college project would actually get such recognition. I am extremely grateful üò≠\n\nThank you all so much. Stars, ideas, discussion all helped so much ‚ù§Ô∏è",
          "score": 1,
          "created_utc": "2026-01-20 17:20:41",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfufg3",
      "title": "Claude Code with Anthropic API compatibility",
      "subreddit": "ollama",
      "url": "https://ollama.com/blog/claude",
      "author": "GhettoFob",
      "created_utc": "2026-01-18 01:01:12",
      "score": 31,
      "num_comments": 10,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qfufg3/claude_code_with_anthropic_api_compatibility/",
      "domain": "ollama.com",
      "is_self": false,
      "comments": [
        {
          "id": "o07x7s2",
          "author": "iron_coffin",
          "text": "The questions are whether anthropic is onboard with it and if they can block it.",
          "score": 3,
          "created_utc": "2026-01-18 02:19:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o088i85",
              "author": "xxdesmus",
              "text": "The seem more worried with Claude models being used by 3rd party clients and less concerned with this inverse scenario. Hopefully that doesn‚Äôt change.",
              "score": 5,
              "created_utc": "2026-01-18 03:22:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o09svlh",
              "author": "StardockEngineer",
              "text": "There is nothing they can do to stop it as long as Claude Code can work with an API",
              "score": 2,
              "created_utc": "2026-01-18 10:53:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0b7flp",
                  "author": "iron_coffin",
                  "text": "Nothing, when their control both the closed source app and api? X to doubt. They could at least make it inconvenient",
                  "score": 1,
                  "created_utc": "2026-01-18 16:14:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0atpiq",
              "author": "kiwibonga",
              "text": "They are currently preventing anyone from doing the first time user set up without a paid api key.",
              "score": 2,
              "created_utc": "2026-01-18 15:07:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0b46k7",
                  "author": "iron_coffin",
                  "text": "It sounds like codex is a better bet because the system prompt is shorter, also.",
                  "score": 1,
                  "created_utc": "2026-01-18 15:58:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mvn76",
          "author": "uwemaurer",
          "text": "Did anyone get this to work with a local model?\n\nI tried with gpt-oss:20b but I get the error:\n\n     harmony parser: no reverse mapping found for function name \"harmonyFunctionName=assistant<|channel|>AskUserQuestion\"\n    \n\nFor gwen3-coder, I get \n\n    model does not support thinking, relaxing thinking to nil",
          "score": 1,
          "created_utc": "2026-01-20 08:38:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0poqww",
              "author": "GhettoFob",
              "text": "Did you try bumping up the context length? My Ollama was defaulting to 4k but it started working after I set it to 64k.",
              "score": 2,
              "created_utc": "2026-01-20 18:42:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0qpsph",
                  "author": "uwemaurer",
                  "text": "Thank you! this was the problem. It worked for me with `gpt-oss:20b` and 32k context. \n\n    OLLAMA_CONTEXT_LENGTH=32768 ollama serve",
                  "score": 1,
                  "created_utc": "2026-01-20 21:32:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qgxulm",
      "title": "Would Anthropic Block Ollama?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qgxulm/would_anthropic_block_ollama/",
      "author": "Lopsided_Dot_4557",
      "created_utc": "2026-01-19 07:36:07",
      "score": 21,
      "num_comments": 17,
      "upvote_ratio": 0.84,
      "text": "Few hours ago, Ollama announced following:\n\nOllama now has Anthropic API compatibility. This enables tools like Claude Code to be used with open-source models.\n\nOllama Blog:¬†[Claude Code with Anthropic API compatibility ¬∑ Ollama Blog](https://ollama.com/blog/claude)\n\nHands-on Guide:¬†[https://youtu.be/Pbsn-6JEE2s?si=7pdAv5LU9GiBx7aN](https://youtu.be/Pbsn-6JEE2s?si=7pdAv5LU9GiBx7aN)\n\nFor now it's working but for how long?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qgxulm/would_anthropic_block_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0fwcc2",
          "author": "Kholtien",
          "text": "I don‚Äôt think they could. You can use Claude code (the application, not the service Claude) with just about any LLM these days. It‚Äôs all local on your computer.",
          "score": 14,
          "created_utc": "2026-01-19 07:49:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0i42ty",
              "author": "sinan_online",
              "text": "Hey, so I got a question to ask. If you are doing this, what are you doing, exactly, and what‚Äôs the VRAM? \n\nMy understanding is that the Claude endpoint does quite a bit of orchestration, not just coding, when used under GitHub CoPilot. Also the model they are using seems to require more VRAM than I have locally (12GB) to respond in a reasonable amount of time.",
              "score": 1,
              "created_utc": "2026-01-19 16:43:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0ktgnp",
              "author": "Big-Masterpiece-9581",
              "text": "It works but only with hacks and you‚Äôre at Claude‚Äôs mercy anytime they want to throw a curveball and break everything else. I would just use opencode.",
              "score": 1,
              "created_utc": "2026-01-20 00:29:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0l87yb",
                  "author": "StardockEngineer",
                  "text": "They're not hacks.  It's just pointing the base url to another server.  It's a documented feature in Claude Code.",
                  "score": 5,
                  "created_utc": "2026-01-20 01:50:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0jhwpr",
          "author": "ShadoWolf",
          "text": "The Claude Code situation isn‚Äôt about Anthropic‚Äôs standard API.  \nWhat was happening is that people were signing up for Claude Code, then using tools like OpenCode to redirect it and reuse the Claude Code credential.  \nThat credential wasn‚Äôt a normal Anthropic API key, it was tied to the Claude Code plan itself, which is priced as a product SKU, not as metered API access. Anthropic will happy take your money for API usage through their normal API plans. They just don‚Äôt want the Claude Code plan being used as a cheap substitute for the standard API pricing",
          "score": 3,
          "created_utc": "2026-01-19 20:29:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g5sld",
          "author": "gabrielxdesign",
          "text": "Why would they block it? I've been using Open WebUI for years, which uses Ollama for Open Source LLM, and I use it together with paid APIs. I don't see the problem with using a platform that can handle local and remote.",
          "score": 4,
          "created_utc": "2026-01-19 09:16:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0gsxig",
          "author": "UnbeliebteMeinung",
          "text": "They blocked the other way because of their costs. Why would they block it when they have no costs?",
          "score": 2,
          "created_utc": "2026-01-19 12:38:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0it8hw",
          "author": "croninsiglos",
          "text": "They are pretty confident in their model‚Äôs ability over open source models and the CEO seems to think you‚Äôll never be hosting the more performant open models locally. \n\nSo does it matter?\n\nI‚Äôm sure they‚Äôd love people to standardize on their APO vs openai‚Äôs API.",
          "score": 1,
          "created_utc": "2026-01-19 18:36:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0osgxo",
              "author": "Practical-Plan-2560",
              "text": "Under that logic, they should open source Claude Code. If the model is the magic, why keep tools to use the model so closed?",
              "score": 1,
              "created_utc": "2026-01-20 16:14:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0izx4z",
          "author": "roger_ducky",
          "text": "It‚Äôs the same as the OpenAI API compatibility.\n\nAs long as anthropic doesn‚Äôt suddenly make major breaking changes, it‚Äôd work.\n\nThis isn‚Äôt the same thing anthropic banned recently‚Äî that‚Äôs piggybacking on the actual Claude using a person‚Äôs monthly paid subscription, rather than APIs.\n\nThis is using Claude code the CLI with your local models.",
          "score": 1,
          "created_utc": "2026-01-19 19:06:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ju0js",
          "author": "Clay_Ferguson",
          "text": "Anthropic realized that if OpenCode and/or LangChain OpenWork open source solutions are what people start using for local LLMs, then it gets people out of the Claude Code way of doing things. So there was no disadvantage to letting Claude Code be used for local inference, but there was an advantage, which is to make Claude Code be what people want to use regardless of whether they're using local or cloud-based LLMs.",
          "score": 1,
          "created_utc": "2026-01-19 21:26:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nhtd1",
          "author": "Ok_Hospital_5265",
          "text": "And just as I installed Crush‚Ä¶ ü§¶üèª‚Äç‚ôÇÔ∏è",
          "score": 1,
          "created_utc": "2026-01-20 11:56:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0p7oio",
              "author": "seangalie",
              "text": "Sometimes it's useful to have Opencode, Crush, Codex, and Claude all co-existing.  Good way to jump between models and such without having to play around with configs everytime.  I use Ollama's cloud models in Opencode, and local models in Crush (since Crush supports a big task/little task setup).",
              "score": 1,
              "created_utc": "2026-01-20 17:25:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0m8y9k",
          "author": "stocky789",
          "text": "Would anyone want to? \nClaude code isn't really anything to write home about",
          "score": 0,
          "created_utc": "2026-01-20 05:26:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qh10xr",
      "title": "Demo: On-device browser agent (Qwen) running locally in Chrome",
      "subreddit": "ollama",
      "url": "https://v.redd.it/ljp6zwzfcaeg1",
      "author": "thecoder12322",
      "created_utc": "2026-01-19 10:48:47",
      "score": 20,
      "num_comments": 3,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qh10xr/demo_ondevice_browser_agent_qwen_running_locally/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0k28rw",
          "author": "TigerOk6003",
          "text": "Would be very curious to see how data centers will be rendered useless if small models get better and hardware for edge inference gets better too",
          "score": 3,
          "created_utc": "2026-01-19 22:07:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0j7059",
          "author": "New_Inflation_6927",
          "text": "Next challenge for the team could be trying out VLM within it?",
          "score": 1,
          "created_utc": "2026-01-19 19:38:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0n4kha",
              "author": "thecoder12322",
              "text": "We tried and we‚Äôre fixing a few bugs for that, it‚Äôll make it even more accurate! Open for any contributions, please check our runanywhere-sdks as well here: https://github.com/RunanywhereAI/runanywhere-sdks\n\nWe‚Äôll be adding web-gpu and vlm support as well which",
              "score": 1,
              "created_utc": "2026-01-20 10:02:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qen6w1",
      "title": "The Preprocessing Gap Between RAG and Agentic",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qen6w1/the_preprocessing_gap_between_rag_and_agentic/",
      "author": "OnyxProyectoUno",
      "created_utc": "2026-01-16 17:59:44",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "RAG is the standard way to connect documents to LLMs. Most people building RAGs know the steps by now: parse documents, chunk them, embed, store vectors, retrieve at query time. But something different happens when you're building systems that act rather than answer.\n\n### The RAG mental model\n\nRAG preprocessing optimizes for retrieval. Someone asks a question, you find relevant chunks, you synthesize an answer. The whole pipeline is designed around that interaction pattern.\n\nThe work happens before anyone asks anything. Documents get parsed into text, extracting content from PDFs, Word docs, HTML, whatever format you're working with. Then chunking splits that text into pieces sized for context windows. You choose a strategy based on your content: split on paragraphs, headings, or fixed token counts. Overlap between chunks preserves context across boundaries. Finally, embedding converts each chunk into a vector where similar meanings cluster together. \"The contract expires in December\" ends up near \"Agreement termination date: 12/31/2024\" even though they share few words. That's what makes semantic search work.\n\nRetrieval is similarity search over those vectors. Query comes in, gets embedded, you find the nearest chunks in vector space. For Q&A, this works well. You ask a question, the system finds relevant passages, an LLM synthesizes an answer. The whole architecture assumes a query-response pattern.\n\nThe requirements shift when you're building systems that act instead of answer.\n\n### What agentic actually needs\n\nConsider a contract monitoring system. It tracks obligations across hundreds of agreements: Example Bank owes a quarterly audit report by the 15th, so the system sends a reminder on the 10th, flags it as overdue on the 16th, and escalates to legal on the 20th. The system doesn't just find text about deadlines. It acts on them.\n\nThat requires something different at the data layer. The system needs to understand that Party A owes Party B deliverable X by date Y under condition Z. And it needs to connect those facts across documents. Not just find text about obligations, but actually know what's owed to whom and when.\n\nThe preprocessing has to pull out that structure, not just preserve text for later search. You're not chunking paragraphs. You're turning \"Example Bank shall submit quarterly compliance reports within 15 days of quarter end\" into data you can query: party, obligation type, deadline, conditions. Think rows in a database, not passages in a search index.\n\n### Two parallel paths\n\nThe architecture ends up looking completely different.\n\nRAG has a linear pipeline. Documents go in, chunking happens, embeddings get created, vectors get stored. At query time, search, retrieve, generate.\n\nAgentic systems need two tracks running in parallel. The main one pulls structured data out of documents. An LLM reads each contract, extracts the obligations, parties, dates, and conditions, and writes them to a graph database. Why a graph? Because you're not just storing isolated facts, you're storing how they connect. Example Bank owes a report. That report is due quarterly. The obligation comes from Section 4.2 of Contract #1847. Those connections between entities are what graph databases are built for. This is what powers the actual monitoring.\n\nBut you still need embeddings. Just for different reasons.\n\nThe second track catches what extraction misses. Sometimes \"the Lender\" in paragraph 12 needs to connect to \"Example Bank\" from paragraph 3. Sometimes you don't know what patterns matter until you see them repeated across documents. The vector search helps you find connections that weren't obvious enough to extract upfront.\n\nSo you end up with two databases working together. The graph database stores entities and their relationships: who owes what to whom by when. The vector database helps you find things you didn't know to look for.\n\nI wrote the rest on my [blog](https://nickrichu.me/posts/the-preprocessing-gap-between-rag-and-agentic).",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qen6w1/the_preprocessing_gap_between_rag_and_agentic/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qepvki",
      "title": "Do you actually need prompt engineering to get value from AI?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qepvki/do_you_actually_need_prompt_engineering_to_get/",
      "author": "Xthebuilder",
      "created_utc": "2026-01-16 19:36:53",
      "score": 11,
      "num_comments": 45,
      "upvote_ratio": 0.92,
      "text": "\n\nI‚Äôve been using AI daily for about 6 months while building a local AI inferencing app, and one thing that surprised me is how little prompt engineering mattered compared to other factors.\n\nWhat ended up making the biggest difference for me was:\n\n* giving the model enough **context**\n* iterating on ideas *with* the model before writing real code\n* choosing models that are actually good at the specific task\n\nBecause LLMs have some randomness, I found they‚Äôre most useful early on, when you‚Äôre still figuring things out. Iterating with the model helped surface bad assumptions before I committed to an approach. They‚Äôre especially good at starting broad and narrowing down if you keep the conversation going so context builds up.\n\nWhen I add new features now, I don‚Äôt explain my app‚Äôs architecture anymore. I just link the relevant GitHub repos so the model can see how things are structured. That alone cut feature dev time from weeks to about a day in one case.\n\nI‚Äôm not saying prompt engineering is useless, just that for most practical work, context, iteration, and model choice mattered more for me.\n\nCurious how others here approach this. Has prompt engineering been critical for you, or have you seen similar results?\n\n(I wrote up the full experience here if anyone wants more detail: [https://xthebuilder.github.io](https://xthebuilder.github.io))\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qepvki/do_you_actually_need_prompt_engineering_to_get/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nzzcjbw",
          "author": "Dry_Yam_4597",
          "text": "I dont prompt engineer. I ask an llm to write a prompt for another llm when needed. Context is more important.",
          "score": 14,
          "created_utc": "2026-01-16 19:57:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzpr49",
              "author": "Xthebuilder",
              "text": "I agree I also use that method when I want to use an engineered prompt for a workflow I just have another model make it though my language request",
              "score": 3,
              "created_utc": "2026-01-16 20:59:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o004pqy",
                  "author": "Dry_Yam_4597",
                  "text": "Nice. I actually run a couple of P100s just for that. One to analyse images, one to write a prompt, one to refine stuff and is fine tuned, and then i send the final prompt to say comfyui. Pretty neat. That's also how I work around nano banana's lame ai \"logo\". I just recreate images on local.",
                  "score": 2,
                  "created_utc": "2026-01-16 22:10:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzzozca",
          "author": "Purple-Programmer-7",
          "text": "Yes, it‚Äôs critical. And we‚Äôre calling it ‚Äúcontext engineering‚Äù now.",
          "score": 7,
          "created_utc": "2026-01-16 20:55:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzq5fo",
              "author": "Xthebuilder",
              "text": "I like that term better than prompt engineering .",
              "score": 1,
              "created_utc": "2026-01-16 21:01:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzzns21",
          "author": "tom-mart",
          "text": "I'd argue that adding context is prompt engineering.",
          "score": 9,
          "created_utc": "2026-01-16 20:50:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzplac",
              "author": "Xthebuilder",
              "text": "Okay thankyou for the clarification",
              "score": 1,
              "created_utc": "2026-01-16 20:58:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzzt1jz",
          "author": "DeepInEvil",
          "text": "Pretty much this, prompt engineering is bs and for people who are new in the field. Afair r/airealist had an article on that.",
          "score": 5,
          "created_utc": "2026-01-16 21:14:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o004od6",
          "author": "Shoddy-Tutor9563",
          "text": "I can't stop laughing seeing how others are sometimes trying to squeeze out a bit of additional performance from a model in agentic tasks by putting different personas \"hat\" on it. E.g. \"you are super senior 10x engineer ...\"",
          "score": 2,
          "created_utc": "2026-01-16 22:10:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00gev7",
              "author": "Xthebuilder",
              "text": "Lmaooo you are so right I have personally never had much  use of personalities for the LLM workflows I have been using",
              "score": 1,
              "created_utc": "2026-01-16 23:09:55",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o00y705",
              "author": "track0x2",
              "text": "I‚Äôve found that only useful for emulation. ‚ÄúYou are Dr. Seuss.‚Äù",
              "score": 1,
              "created_utc": "2026-01-17 00:51:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o011pfa",
              "author": "NoAdministration6906",
              "text": "I know its funny but it actually works. Response are much better",
              "score": 1,
              "created_utc": "2026-01-17 01:12:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o00ahv3",
          "author": "MrSomethingred",
          "text": "From my understanding of the history of it, \"Prompt Engineering\" was more important in the GPT4 era when you had to say things like \"fix this or I will explode 4000 puppies with Napalm\" and there was nuance in the art of getting them to actually follow instructions. E.g. getting them to actually output in JSON consistently was a nightmare\n\n\nModern models are pretty good at rule following these days so there isn't really any secrets to prompting them \"correctly\" these days\n\n\nAs you highlighted, context management is far more important these days, hence the new buzzword is \"context engineering\"¬†",
          "score": 2,
          "created_utc": "2026-01-16 22:39:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00guz2",
              "author": "Xthebuilder",
              "text": "I like that , context engineering feels more like what I‚Äôm doing and you can relate that to say just having a conversation , regular folk can get to understand that suing AI effectively isn‚Äôt rocket science",
              "score": 1,
              "created_utc": "2026-01-16 23:12:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00g7ji",
          "author": "HeavyDluxe",
          "text": "Prompt engineering is (IMHO) best understood as one \\_facet\\_ of context engineering.  The model makes its predictions based on the contextual information it's delivered.  In early models, carefully crafting a prompt was really the best you could do to ground the model's output.  We have many, many more tools available to us now to set a meaningful foundation for the model to use in generating output.\n\nIf you give the model tons of \\_really good\\_ information in  the codebase, supporting documents, etc etc etc, the user prompt becomes less and less critical.\n\nThe illustration I use at work is to imagine a random stranger comes up at you with a pile of papers, tells you to summarize the data therein for them, and, if you do a good job, you get $1M.  Imagine how you the quality of your product improves if the data has good labels... or if there's a previous report drawing on similar data that's there as an example. Or if the person also tells you what industry they're in.  Or if they tell you to \"imagine you're a customer service manager\" or whatever.\n\nEach little bit of information available improves your output.  The prompt is vital if that's all you give the model.  But context is \\_everything\\_.",
          "score": 2,
          "created_utc": "2026-01-16 23:08:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02rmqo",
              "author": "Xthebuilder",
              "text": "Basically it‚Äôs like data cleaning for your AI output pipeline, it‚Äôs really conceptual but it cuts across a lot of LLM interactions as the base of what‚Äôs controlling the models response",
              "score": 1,
              "created_utc": "2026-01-17 08:54:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00gr19",
          "author": "Additional-Actuary-8",
          "text": "I think the key is to ask what exactly you want rather than design tons of prompts. \n\nAs a human I also want my ai to answer shorter and concise.",
          "score": 2,
          "created_utc": "2026-01-16 23:11:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00h1zm",
              "author": "Xthebuilder",
              "text": "I find myself wanting the models to be more concise across the board too , you‚Äôre correct about being specific about what you want from the model , sounds more like communciation skills",
              "score": 1,
              "created_utc": "2026-01-16 23:13:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00jt2d",
          "author": "Low-Opening25",
          "text": "no, you need context management",
          "score": 2,
          "created_utc": "2026-01-16 23:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00onyo",
              "author": "Xthebuilder",
              "text": "Agreed agreed",
              "score": 1,
              "created_utc": "2026-01-16 23:56:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00zxfy",
          "author": "DaleCooperHS",
          "text": "I dont see the difference. prompt engineering is not writing a prompt, is the process that ends with writing a prompt. Now that process can be done mentally or trought iteration and research . cna be pages long of iteration with an LLm or one 30 lines prompt. Is still prompt engineering.  \nHowever context is not cheap.. so ideally you want to condense all the necessary context and prune out all taht is not.. leaving with the shorter set of instructions and context necessary to do the job, so that the maximum ammount of still avaible context can be used to actually do the job.",
          "score": 2,
          "created_utc": "2026-01-17 01:01:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0120v8",
          "author": "NoAdministration6906",
          "text": "There is also a fine line in giving context, too much context also make the model hallucinations. Be aware of the context window tokens for a model.",
          "score": 2,
          "created_utc": "2026-01-17 01:14:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01mo1i",
              "author": "Xthebuilder",
              "text": "Good point I haven‚Äôt really considered context in token window size too much but maybe that adjustment will lead to further optimization",
              "score": 1,
              "created_utc": "2026-01-17 03:25:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o017umx",
          "author": "cointalkz",
          "text": "Most LLMs, diffusion models or whatever you are using have documentation. Take that documentation into your LLM of choice and train it on how to best write prompts for said model. Profit!",
          "score": 2,
          "created_utc": "2026-01-17 01:52:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01ms94",
              "author": "Xthebuilder",
              "text": "Ooo like full circle training the model on its own developer written documentation",
              "score": 1,
              "created_utc": "2026-01-17 03:26:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o01nxl8",
                  "author": "cointalkz",
                  "text": "Bingo",
                  "score": 1,
                  "created_utc": "2026-01-17 03:34:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o01lqth",
          "author": "fasti-au",
          "text": "It‚Äôs a tipping scale.  \n\n1 million context but your really only pulling lie 16k tokens to process.  So in some cases you get cleaner runs but you need to one shot not correct and now think is breaking things to boiler plating it‚Äôs now more important to back track not correct. \n\nDoes prompting matter.   Not until It does.  \n\nI‚Äôm bad at words out to human but I can cintext a model better than most and build library systems and I tells ya.   Code the tool teacher model to know how to pull and when.  How you get that code is irrelevant in many ways because it‚Äôs all exiting concepts and process with small adjustments so nothing is special beyond the results \n\n\nSo promt engineering is about repetative stability not the day to day in many ways",
          "score": 2,
          "created_utc": "2026-01-17 03:19:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01mjmh",
              "author": "Xthebuilder",
              "text": "I like how you put it , if you can get the same result many times over you can trust the system more overall",
              "score": 1,
              "created_utc": "2026-01-17 03:25:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o02sp52",
          "author": "generate-addict",
          "text": "Nope. And even ‚Äúcontext engineering‚Äù. It changes from model to model, generation to generation. People just gaslight themselves.",
          "score": 2,
          "created_utc": "2026-01-17 09:04:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02stv2",
              "author": "Xthebuilder",
              "text": "I‚Äôm really glad I asked the community because I thought I was tripping seeing all the ‚Äúprompt engineering ‚Äú buzzwords for testing new people about using AI bc I learned though trying to build and use them that fr fr I don‚Äôt need none of that shit üòÇü§£",
              "score": 1,
              "created_utc": "2026-01-17 09:06:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o03ky5r",
          "author": "ZynthCode",
          "text": "promt-crafting is helpful to correct *undesired* behavior.   \n  \n\\*whips LLM on the wrist\\* No comments in code!",
          "score": 2,
          "created_utc": "2026-01-17 13:09:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05suao",
              "author": "Xthebuilder",
              "text": "ü§£ü§£üòÇ good way to put it , it‚Äôs at the edge of the computation , won‚Äôt change too much but can tweak stuff for sure",
              "score": 1,
              "created_utc": "2026-01-17 19:45:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0gk5gp",
          "author": "d_28ight",
          "text": "Just came across this thread,\n\nI've been researching how to develop LLM responses, beyond just clarifying my intention. I think roleplaying has assisted, but personally, it depends heavily on whether you have a foundational understanding of that role/industry to begin with.   \n\n\nCurrently, I have been scrapping prompts I come across and writing my own renditions specific to my requests, but now I came across JSON writers. I am thinking of still maintaining my context in my written notes, then using another LLM to conduct a meta-analysis of this as a collaborator to get further understanding and refine it, but it depends on my ability to articulate my intent, then using another LLM to convert this into JSON and paste that to my original LLMs  \n\n\nPersonally, I think the ability to articulate your thoughts, both at a direct and subconscious level, is more important than overloading it with context at every pivot.",
          "score": 2,
          "created_utc": "2026-01-19 11:29:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ir150",
              "author": "Xthebuilder",
              "text": "And I think what has been revealed though these discussions is that since you‚Äôre talking us f natural language , interaction with llms follows more principals adjacent to communication than computer science",
              "score": 1,
              "created_utc": "2026-01-19 18:26:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0ir8a9",
              "author": "Xthebuilder",
              "text": "Because I think you‚Äôre right I got better at communicating to the agents and just telling them exactly what I needed , a lot of the time I found the limitation was myself in the loop",
              "score": 1,
              "created_utc": "2026-01-19 18:27:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o09yxvd",
          "author": "stocky789",
          "text": "I never bother with prompt engineering \nJust stay specific and concise when communicating with it",
          "score": 1,
          "created_utc": "2026-01-18 11:47:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0bfh8y",
              "author": "Xthebuilder",
              "text": "right right ,more like simple communication.",
              "score": 1,
              "created_utc": "2026-01-18 16:52:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o01t58f",
          "author": "YouAreTheCornhole",
          "text": "Yes you do, and most people in this thread seem to not know what prompt engineering actually means. Straight up garbage in garbage out",
          "score": 0,
          "created_utc": "2026-01-17 04:09:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02ra7k",
              "author": "Xthebuilder",
              "text": "Good way to add to the discussion brother üëçüèΩ",
              "score": 0,
              "created_utc": "2026-01-17 08:51:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o02shgs",
                  "author": "YouAreTheCornhole",
                  "text": "You literally described your way of prompt engineering, acting like you don't need prompt engineering, because you prompt engineer in a certain way.",
                  "score": 1,
                  "created_utc": "2026-01-17 09:02:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qijhth",
      "title": "Weekend Project: An Open-Source Claude Cowork That Can Handle Skills",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qijhth/weekend_project_an_opensource_claude_cowork_that/",
      "author": "Frequent_Cash2598",
      "created_utc": "2026-01-21 00:59:50",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "I spent last weekend building something I had been thinking about for a while. Claude Cowork is great, but I wanted an open-source, lightweight version that could run with any model, so I created Open Cowork.\n\nIt's written entirely in Rust, which I had never used before. Starting from scratch meant no heavy dependencies, no Python bloat, and no reliance on existing agent SDKs. Just a tiny, fast binary that works anywhere.\n\nSecurity was a big concern since the agents can execute code. Open Cowork handles this by running tasks inside temporary Docker containers. Everything stays isolated, but you can still experiment freely.\n\nYou can plug in any model you want. OpenAI, Anthropic, or even fully offline LLMs through Ollama are all supported. You keep full control over your API keys and your data.\n\nIt already comes with built-in skills for handling documents like PDFs and Excel files. I was surprised by how useful it became right away.\n\nThe development experience was wild. An AI agent helped me build a secure, open-source version of itself, and I learned Rust along the way. It was one of those projects where everything just clicked together in a weekend.\n\nThe code is live on GitHub: [https://github.com/kuse-ai/kuse\\_cowork](https://github.com/kuse-ai/kuse_cowork) . It's still early, but I'd love to hear feedback from anyone who wants to try it out or contribute.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qijhth/weekend_project_an_opensource_claude_cowork_that/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0rvzc8",
          "author": "Available-Craft-5795",
          "text": "Im pretty sure this is AI.   \nLoads of emojis in readme  \nLoads of commments in code that no sane dev would add  \nDone in a weekend? How?",
          "score": 3,
          "created_utc": "2026-01-21 01:13:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0s2p5t",
              "author": "gingeropolous",
              "text": "Sounds like they used Claude code.",
              "score": 3,
              "created_utc": "2026-01-21 01:51:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0ss3zn",
              "author": "Frequent_Cash2598",
              "text": "Yes, it is Claude Code making a Rust alternative of itself. \n\nSo you are right. :)",
              "score": 2,
              "created_utc": "2026-01-21 04:23:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0sepez",
          "author": "Efficient_Click_2689",
          "text": "cool project bro, would love to try out!",
          "score": 1,
          "created_utc": "2026-01-21 03:00:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qepzot",
      "title": "Polymcp Integrates Ollama ‚Äì Local and Cloud Execution Made Simple",
      "subreddit": "ollama",
      "url": "https://github.com/poly-mcp/Polymcp",
      "author": "Just_Vugg_PolyMCP",
      "created_utc": "2026-01-16 19:41:16",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qepzot/polymcp_integrates_ollama_local_and_cloud/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qi8ouc",
      "title": "Plano 0.4.3 ‚≠êÔ∏è Filter Chains via MCP and OpenRouter Integration",
      "subreddit": "ollama",
      "url": "https://i.redd.it/o590ks78pjeg1.png",
      "author": "AdditionalWeb107",
      "created_utc": "2026-01-20 18:12:47",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qi8ouc/plano_043_filter_chains_via_mcp_and_openrouter/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qhr198",
      "title": "GLM 4.7 is apparently almost ready on Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/",
      "author": "Savantskie1",
      "created_utc": "2026-01-20 04:18:14",
      "score": 8,
      "num_comments": 7,
      "upvote_ratio": 0.83,
      "text": "It's listed, just not downloadable yet. Trying in WebOllama, and in CLI gives weird excuses\n\nhttps://preview.redd.it/96ly2bgckfeg1.png?width=1723&format=png&auto=webp&s=d8bfc2386dd789ef4f28ff0516de4893bf5c6772",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0m0ppd",
          "author": "iansltx_",
          "text": "You can't use the GGUF. You \\*can\\* use the ones from the website if you have the latest ollama build (on GitHub, see that link).\n\nOne caveat: it runs slow on a unified-memory Mac for some reason, and splits 80/20 CPU/GPU. Not sure what's going on there. My Intel Mac runs faster.",
          "score": 1,
          "created_utc": "2026-01-20 04:30:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0oe8av",
          "author": "WaitformeBumblebee",
          "text": "this one? updated 19 hours ago:\n\nhttps://ollama.com/library/glm-4.7-flash\n\n\nHmm, glm-4.7-flash:latest and glm-4.7-flash:q4_K_M are the same size, are they the same?\n\n\nCan't pull the new model, it requires Ollama update, even right after updating...\n\nollama --version\nollama version is 0.14.2\n\n\nedit: download latest release from github\nhttps://github.com/ollama/ollama/releases\n\nunpack, sudo cp lib files to /usr/local/lib/ollama/ and bin file to /usr/local/bin/   \n\npulling glm4.7 now\n\nedit2: quite fast on laptop 3060 6GB VRAM + 32GB RAM\n\nNAME                    ID              SIZE     PROCESSOR          CONTEXT              \nglm-4.7-flash:latest    ff14144f31df    23 GB    77%/23% CPU/GPU    4096",
          "score": 1,
          "created_utc": "2026-01-20 15:07:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ojnqc",
          "author": "beefgroin",
          "text": "thanks, running glm-4.7-flash:q8\\_0, so far so good",
          "score": 1,
          "created_utc": "2026-01-20 15:33:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0pkcjg",
              "author": "thexdroid",
              "text": "What is your GPU?",
              "score": 1,
              "created_utc": "2026-01-20 18:22:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0plxu3",
                  "author": "beefgroin",
                  "text": "I have 4 5060 16gb, I moved to q4k\\_m because the model consumes a lot of vram when the context is increasing, with 32k context it's taking 50gb of vram.",
                  "score": 1,
                  "created_utc": "2026-01-20 18:30:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0pm8wh",
              "author": "beefgroin",
              "text": "I liked it, the thinking process is very structured and very different from other thinking models. I guess something is still buggy cause every long conversation ends up in an infinite thinking loop",
              "score": 1,
              "created_utc": "2026-01-20 18:31:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qgbl6g",
      "title": "(linux) i'm interested in historical roleplay (1600s)/early modern period), what would be your setup ?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qgbl6g/linux_im_interested_in_historical_roleplay/",
      "author": "Mid-Pri6170",
      "created_utc": "2026-01-18 15:33:57",
      "score": 6,
      "num_comments": 8,
      "upvote_ratio": 0.88,
      "text": "my longer term goal is to use gemini or other ai to make a little isometric world in Godot i can explore.\n\nyesterday gemini had me instal olama and lama3 on my pc. \n\ni only ran it in the terminal, but i am interested in what other things to consider to make it emersive.... considering cgpt etc are nerf'd\n\nGemini suggest Dolphin, Qwen and Nemo models too. however i was wondering if these models have a lot obscure trivia, knowledge of the period, language etc in them like the big llms do, otherwise they will quickly sound stale.\n\ni was thinking there might be a specially trained model on period language/literature?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qgbl6g/linux_im_interested_in_historical_roleplay/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0dhq2e",
          "author": "sinan_online",
          "text": "I have been testing this kind of stuff with self-hosted Ollama models as well, I the Eberron campaign setting. I also tested quite a bit of map generations as images.\n\nFor storytelling and role play, in the end, it has much more to do with RAG, or what you put in front of the model, in its context. At least, that has been my impression.\n\nI containerized much of what I did, PM me and I‚Äôll point you to the containers, if you are interested in using them.",
          "score": 1,
          "created_utc": "2026-01-18 22:52:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0dqek1",
              "author": "Mid-Pri6170",
              "text": "cause im using reddit in the UK i cant dm people as i refuse to verify my profile with a credit card! doh!",
              "score": 3,
              "created_utc": "2026-01-18 23:36:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0dqh2z",
                  "author": "Mid-Pri6170",
                  "text": "> Eberron campaign setting\n\nwhats that?",
                  "score": 2,
                  "created_utc": "2026-01-18 23:36:31",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qfauq9",
      "title": "Help a noob figure out how to achieve something in a game engine with Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qfauq9/help_a_noob_figure_out_how_to_achieve_something/",
      "author": "MountainPlantation",
      "created_utc": "2026-01-17 11:26:10",
      "score": 5,
      "num_comments": 10,
      "upvote_ratio": 0.86,
      "text": "Hey!\n\nI want to use Ollama to integrate it with a game engine. It's already in the engine and working, but I have some questions on what model I should use, and any tips in general for the experiments I want to do. I understand most LLMs running locally will take a while to think and generate a response, but for now let's ignore that.\n\n1. NPC Chat with commands: I know most people have tried doing NPC chatbots in engines, but I was thinking I could try to spice that up by integrating commands on it. Like the LLM would have a list of commands, given by me, that it could use contextually, like /laugh /cry /givePlayer(item), things like that. And I can make a system that parses the string and extracts/executes the commands. I attempted this one time, not in engine, just by using regular chat GPT and it would eventually come up with its own commands that were not stipulated by me. How to avoid that? Is there a model I should use for that?\n2. NPC consistency in character. I also tried one time to keep chat GPT in character, a peasant from the medieval ages, but I would ask about modern events like COVID and it would eventually break and talk about it as if he knew what it was.\n3. NPC Memory. What if I wanted to have NPCs remember things they have witnessed? I imagine I should make a log system that keeps every action done to that npc (NPC was hit by Player. NPC killed bandit. NPC found 1 gold etc) and then adding it to the beggining of the prompt as a little memory. Is that enough?\n4. Can I reliably limit the response length or is it finicky? Like, setting a limit of how many words per response \n5. Is there a way to guarantee responses are always in character? Because sometimes some of the LLMs will say \"I cannot answer to things related to that\" and that would be a big immersion breaker \n\nAnd another general question, is there a way to train certain models to get them used to a certani context? like i said, using commands I create in my game, or training them to act like a specific type of character etc.\n\nAgain, other than my experiments with just the chat GPT window, I am pretty new to this. If you have advice on what models to use or best practices, I'm listening.\n\nThank you!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qfauq9/help_a_noob_figure_out_how_to_achieve_something/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o040jq8",
          "author": "CooperDK",
          "text": "Just a comment, don't use ollama. Provide koboldcpp in a variant specific to the individual user's hardware. Ollama is too slow. In most cases, kobold is much faster.",
          "score": 1,
          "created_utc": "2026-01-17 14:39:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05cp8s",
              "author": "MountainPlantation",
              "text": "Can it do those things you mentioned?\n\n\nUpon testing something like deepseek takes roughly 16 seconds to respond¬†\n\n\nSomething like tinyllama which is supposed to be fast takes 3 which is still long for most applications\n\n\nIs koboldcpp JUST for character? Because I wanted to make it do other things like game direction and level generation¬†",
              "score": 1,
              "created_utc": "2026-01-17 18:29:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0hjsvy",
                  "author": "CooperDK",
                  "text": "It sure can. Koboldcpp is a CLI engine but can run silent without a console. It can do whatever you instruct the model to do. I am currently using it for AI dataset generation for an anthro AI model.",
                  "score": 1,
                  "created_utc": "2026-01-19 15:11:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o09219a",
                  "author": "alt-weeb",
                  "text": "respectfully nobody's gonna play your fucking game if the entire thing is ai generated, hoping that's not your intention bc if so you're wasting your time",
                  "score": -2,
                  "created_utc": "2026-01-18 06:48:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qeatiw",
      "title": "New version of Raspberry Pie Generative AI card (HAT+ 2)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qeatiw/new_version_of_raspberry_pie_generative_ai_card/",
      "author": "Unique_Winner_5927",
      "created_utc": "2026-01-16 08:43:42",
      "score": 5,
      "num_comments": 6,
      "upvote_ratio": 0.86,
      "text": "Perfect for private assistants, industrial equipment,¬†proof of concept, ...\n\n[https://www.raspberrypi.com/news/introducing-the-raspberry-pi-ai-hat-plus-2-generative-ai-on-raspberry-pi-5/](https://www.raspberrypi.com/news/introducing-the-raspberry-pi-ai-hat-plus-2-generative-ai-on-raspberry-pi-5/)\n\n\\#RaspberryPi #DataSovereignty #EmbeddedAI",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qeatiw/new_version_of_raspberry_pie_generative_ai_card/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nzw85q6",
          "author": "-Akos-",
          "text": "Jeffrey Geerling was less impressed: [https://youtu.be/jRQaur0LdLE](https://youtu.be/jRQaur0LdLE)\n\nBesides, all models I‚Äôve seen run on this board were 1.5B parameters or less, and older models at that.",
          "score": 2,
          "created_utc": "2026-01-16 10:01:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwoo6q",
              "author": "Comfortable_Ad_8117",
              "text": "Was that the original board or the +2 -The new +2 has 8gb of onboard ram and 2x speed",
              "score": 1,
              "created_utc": "2026-01-16 12:16:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzwu1yx",
                  "author": "-Akos-",
                  "text": "It's all in the video. In the end, you're better off buying a second Pi5 than this thing.",
                  "score": 3,
                  "created_utc": "2026-01-16 12:52:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzwp5t7",
                  "author": "danishkirel",
                  "text": "Still slower than the pi 5 cpu and there are models with more system memory too.",
                  "score": 1,
                  "created_utc": "2026-01-16 12:19:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzy284k",
          "author": "Unique_Winner_5927",
          "text": "Or we can directly install Ollama on Pi 5 : [https://monraspberry.com/installer-ollama-raspberry-pi/](https://monraspberry.com/installer-ollama-raspberry-pi/)",
          "score": 1,
          "created_utc": "2026-01-16 16:30:44",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o069278",
          "author": "tecneeq",
          "text": "The problem is the slow access to ram. You can fit up to 9b models i think, but they are all dense models, so everything is extremely slow.\n\nHowever, it has no load on the CPU, so that is great. It even works with a 1GB PI5.",
          "score": 1,
          "created_utc": "2026-01-17 21:07:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qd7dmx",
      "title": "Hey all- I built a self-hosted MCP server to run AI semantic search over your own databases, files, and codebases. Supports Ollama and cloud providers if you want. Thought you all might find a good use for it.",
      "subreddit": "ollama",
      "url": "/r/selfhosted/comments/1qbv3fm/ragtime_a_selfhosted_mcp_server_to_run_ai/",
      "author": "mattv8",
      "created_utc": "2026-01-15 02:33:30",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qd7dmx/hey_all_i_built_a_selfhosted_mcp_server_to_run_ai/",
      "domain": "",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qfzb7g",
      "title": "Ollama not detecting intel arc graphics",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qfzb7g/ollama_not_detecting_intel_arc_graphics/",
      "author": "Titanlucifer18",
      "created_utc": "2026-01-18 04:51:50",
      "score": 4,
      "num_comments": 5,
      "upvote_ratio": 0.84,
      "text": "I have Thinkpad E14 G7, with intel core ultra 5 225H processor, running Fedora 43. I tried to install Ollama, but it did not detect any GPU. I tried to search docs but couldn‚Äôt find anything, or maybe I weren‚Äôt looking at the right place.\n\nIf anyone can guide me would be helpful.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qfzb7g/ollama_not_detecting_intel_arc_graphics/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o09u793",
          "author": "DutchOfBurdock",
          "text": "Give this PyTorch a try: https://github.com/intel/ipex-llm anything built against this will get some assistance from the iGPU. Another option is Vulkan.\n\nTake into account the iGPU uses the host RAM for VRAM, so it'll be tiny (1GB likely). That'll prevent larger models from being ran purely in GPU (smollm2 (135m and 360m)  runs on my Vega 8 AMD just about without CPU+GPU shuffling). Larger models get a shake between GPU and CPU.\n\nMy Ollama (llama.cpp) is built with Vulkan support",
          "score": 3,
          "created_utc": "2026-01-18 11:05:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0bgocs",
              "author": "PossiblyTrolling",
              "text": "I've used this successfully with an Arc, it's a little setup but definitely works.",
              "score": 1,
              "created_utc": "2026-01-18 16:58:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0df41r",
              "author": "andy2na",
              "text": "Also keep in mind that Intel hasn't updated the ipex-llm in awhile, so the ollama build is old and you can't run newer models (like qwen3) on it.",
              "score": 1,
              "created_utc": "2026-01-18 22:39:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o09bktv",
          "author": "tecneeq",
          "text": "That's how it's supposed to be. The iGPU is intel and will not be used.\n\nMake sure you have both RAM sockets populated to get more throughput from CPU to RAM. It's as fast as it gets.\n\nI tried to use the NPU with llama.cpp, but it wasn't worth it at all.",
          "score": 2,
          "created_utc": "2026-01-18 08:13:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09ta51",
              "author": "DutchOfBurdock",
              "text": "ARCs can use Intel's Pytorch (IPEX-LLM). They can also use Vulkan acceleration. It's not entirely ideal, as the VRAM on iGPU's is shared with host RAM (UMA). But, some of the processing can be offloaded to GPU.\n\nOn a Vega 8 AMD APU, Vulkan accelerates llama.cpp to 6.7tps over 3.6 just on CPU (gemma3-4b)",
              "score": 1,
              "created_utc": "2026-01-18 10:56:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qgtc9i",
      "title": "Summary and Tagging",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qgtc9i/summary_and_tagging/",
      "author": "FlibblesHexEyes",
      "created_utc": "2026-01-19 03:39:53",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hi all; I don't usually use LLM's so thought I'd ask here if I'm doing this correctly - or if there is a better way to do it.\n\nI run the Hasheous project - the idea is that if you supply an MD5/SHA1 hash, Hasheous can respond with mappings to video game metadata suppliers such as IGDB and others.\n\nJust as a \"I thought this might be a cool addition\" feature, I wanted to add descriptions and tags to each record generated from the mapped metadata sources so that I could provide data to ROM management apps to provide similar games and the basis of a game recommendation engine.\n\nI don't have the budget for offloading this to commercial AI providers (this is a free open source project), so I'm going with a distributed model where anyone could download an agent to use their own installation of Ollama to generate the description and tags.\n\nWith the help of Copilot, I came up with the following:\n\n- pull the description for each mapped data source (IGDB, GiantBomb, Wikipedia, etc) and add them as embedded content. Copilot recommended the nomic-embed-text model to generate the vectors.\n\n- run a CosineSimularity over the response to extract the top `x` results (I won't pretend to understand how this function works!)\n\n- run this through a prompt generator which generates a string with the top `x` embeddings under the heading \"Context:\", and the the prompts below under the heading \"Instructions:\"\n\n- call the `/generate` endpoint with the RAG prompt generator to create the response\n\nFor the description model, I'm using Gemma3:12b, with the prompt:\n```\nGenerate a detailed description/synopsis for the game <DATA_OBJECT_NAME> for <DATA_OBJECT_PLATFORM>.\n\nIf present; use the Wikipedia source as context for the other provided sources.\n\nYou MUST respond only with the description/synopsis. Do not acknowledge you've received this request.\n\nThe description should be engaging and informative, highlighting plot, key features, and gameplay. Keep the description concise, ideally between 150 to 200 words, but no more than 250 words. The output should be in markdown format.\n```\n\nFor the tags model, I'm using qwen3:8b, with the prompt:\n```\nYou are an expert whose responsibility is to help with automatic tagging for a game recommendation engine.\n\nGenerate detailed tags for the game <DATA_OBJECT_NAME> for <DATA_OBJECT_PLATFORM>.\n\nIf present; use the Wikipedia source as context for the other provided sources.\n\nThe tags should accurately represent the game. Only generate tags in the following categories: Genre, Gameplay, Features, Theme, Perspective, and Art Style.\n\nEach tag should be no more than three words long. Ensure each tag is specific and commonly used within the gaming community, but avoid overly broad or generic terms. If you are unable to generate tags relevant to the category, leave it empty.\n\nGenerate a minimum of three tags and a maximum of ten tags per category.\n\nFormat the output as a raw JSON object, containing an array of tags for each category.\n\nMake sure the JSON is properly structured and valid.\n\nExample output:\n{\n    \"Genre\": [\n        \"Action\",\n        \"Adventure\"\n    ],\n    \"Gameplay\": [\n        \"Open World\",\n        \"Multiplayer\"\n    ],\n    \"Features\": [\n        \"Crafting\",\n        \"Character Customization\"\n    ],\n    \"Theme\": [\n        \"Sci-Fi\",\n        \"Fantasy\"\n    ],\n    \"Perspective\": [\n        \"First-Person\",\n        \"Third-Person\"\n    ],\n    \"Art Style\": [\n        \"Realistic\",\n        \"Pixel Art\"\n    ]\n}\n\nDo not include any additional text or content outside of the JSON object.\n```\n\nExample output here: https://beta.hasheous.org/index.html?page=dataobjectdetail&type=game&id=109\n\nI was wondering if anyone had any advice or suggestions to make this process faster or more accurate - or just better :)\n\nCurrently takes about 3 minutes per game on my GTX970 (my best GPU sadly) to generate the description and tags, so performance improvements would also be appreciated.\n\nThanks in advance!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qgtc9i/summary_and_tagging/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qef38r",
      "title": "Prompt tool I built/use with Ollama daily - render prompt variations without worrying about text files",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qef38r/prompt_tool_i_builtuse_with_ollama_daily_render/",
      "author": "springwasser",
      "created_utc": "2026-01-16 12:49:45",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.83,
      "text": "I posted this to another subreddit, but should have posted it here.. sorry if you've seen it.\n\nThis is a tool I built because I use it in local development. I know there are solutions for these things mixed into other software, but this is standalone and does just one thing really well for me.\n\n\\- create/version/store prompts.. don't have to worry about text files unless I want to  \n\\- runs from command line, can pipe stdout into anything.. eg Ollama, ci, git hooks  \n\\- easily render variations of prompts on the fly, inject {{variables}} or inject files.. e.g. git diffs or documents  \n\\- can store prompts globally or in projects, run anywhere\n\nBasic usage:\n\n    # Create a prompt.. paste in text\n    $ promptg prompt new my-prompt \n    \n    # -or-\n    $ echo \"Create a prompt with pipe\" | promptg prompt save hello\n    \n    # Then.. \n    $ promptg get my-prompt | ollama run deepseek-r1\n\nOr more advanced, render with dynamic variables and insert files..\n\n    # before..\n    cat prompt.txt | sed \"s/{{lang}}/Python/g; s/{{code}}/$(cat myfile.py)/g\" | ollama run mistral\n    \n    # now, replace dynamic {{templateValue}} and insert code/file.\n    promptg get code-review --var lang=Python --var code@myfile.py | ollama run mistral\n\nInstall:\n\n    npm install -g @promptg/cli",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qef38r/prompt_tool_i_builtuse_with_ollama_daily_render/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nzwtsz5",
          "author": "springwasser",
          "text": "Should have posted the Gitub:\n\ngithub dot com/promptg/cli",
          "score": 1,
          "created_utc": "2026-01-16 12:50:33",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcsn8m",
      "title": "We tried to automate product labeling in one prompt. It failed. 27 steps later, we've processed 10,000+ products.",
      "subreddit": "ollama",
      "url": "/r/LocalLLaMA/comments/1qcsmww/we_tried_to_automate_product_labeling_in_one/",
      "author": "No-Reindeer-9968",
      "created_utc": "2026-01-14 16:58:42",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qcsn8m/we_tried_to_automate_product_labeling_in_one/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "nzonye7",
          "author": "tom-mart",
          "text": "Sounds like a skill issue.",
          "score": 1,
          "created_utc": "2026-01-15 06:18:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcmwsr",
      "title": "New Ollama Desktop Client",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qcmwsr/new_ollama_desktop_client/",
      "author": "Odd-Feature-645",
      "created_utc": "2026-01-14 13:12:40",
      "score": 3,
      "num_comments": 13,
      "upvote_ratio": 0.56,
      "text": "[GitHub Ollama Desktop](https://github.com/barni007-pro/ollama_desktop_client)\n\nhttps://preview.redd.it/d8ikcl97ebdg1.png?width=706&format=png&auto=webp&s=1a1ca574611ed0ad2925680166b0a51fe62e7401\n\n    Hi everyone!\n    \n    I wanted to create a more powerful, native desktop experience for Ollama. Most clients are just simple chat wrappers, so I built \"Ollama Desktop\" in VB.NET 8 with a focus on advanced tools.\n    GitHub: https://github.com/barni007-pro/ollamGitHubGitHuba_desktop_client\n    \n    üöÄ High-Impact Features:\n     üß† Local RAG Tool: Chat with your large PDF and Text documents using local knowledge extraction.\n     üëÅÔ∏è Vision Support: Upload images or take screenshots directly to analyze them with multimodal models like gemma3.\n     üíª Code Interpreter: The model can execute Python, PowerShell, or Batch scripts locally. Great for task automation!\n     üìÑ Document Context: Easily import .pdf, .txt, or .json files directly into the chat context.\n     üß™ JSON Mode & Tools: Support for structured responses and function calling.\n     üìê LaTeX Support: Beautiful rendering of mathematical formulas.\n     üõ†Ô∏è Under the Hood: Built with .NET 8 and VB.NET.\n    \n    \n    Fast, lightweight, and specifically designed for Windows.\n    Model switching on-the-fly during conversations.\n    \n    \n    I‚Äôm looking for feedback and would love to hear which features you‚Äôd like to see next!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qcmwsr/new_ollama_desktop_client/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "nzjmhvt",
          "author": "naobebocafe",
          "text": "Na... thanks",
          "score": 17,
          "created_utc": "2026-01-14 14:35:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjw47o",
          "author": "alhinai_03",
          "text": "I don't use ollama anymore, but there are a bunch of good well made options out there like open webui, hollama..etc, no disrespect but yours looks vibe coded and very cluttered.",
          "score": 5,
          "created_utc": "2026-01-14 15:23:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzs11lc",
          "author": "danny_094",
          "text": "It reminds me of Windows XP. But it's good that you did something regardless of what others say. You made an effort despite everything.",
          "score": 1,
          "created_utc": "2026-01-15 18:56:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03583a",
          "author": "AwayLuck7875",
          "text": "–ß–µ–º –º–µ–Ω—å—à–µ —Ç—è–Ω–µ—Ç —Ä–µ—Å—É—Ä—Å–æ–≤ —Ç–µ–º –ª—É—Ç—à–µ",
          "score": 1,
          "created_utc": "2026-01-17 11:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hb83x",
          "author": "Available_Pressure47",
          "text": "Thank you for building and sharing this!",
          "score": 1,
          "created_utc": "2026-01-19 14:28:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hcrdo",
          "author": "Odd-Feature-645",
          "text": "https://preview.redd.it/96o6sx1ohbeg1.png?width=746&format=png&auto=webp&s=95c8fbad7436ec672a177dfec41d9be4c518990c\n\nI've continued working on the \"Ollama Desktop\" Tool.  \nVersion [1.1.0.11](http://1.1.0.11) has now been released.  \nYou're absolutely right that the user interface isn't particularly attractive yet.  \nTake a look and see if it looks better now.  \nV [1.1.0.11](http://1.1.0.11)  \n\\- new GUI design  \n\\- Prompt input and file content now include thumbnails",
          "score": 1,
          "created_utc": "2026-01-19 14:36:14",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nzk252k",
          "author": "HyperWinX",
          "text": "Useless AI slop, there are much better alternatives",
          "score": 1,
          "created_utc": "2026-01-14 15:51:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzkagtd",
              "author": "TheIncarnated",
              "text": "That's a weird opinion to have on an Ai subreddit...",
              "score": 3,
              "created_utc": "2026-01-14 16:29:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzkatwq",
                  "author": "HyperWinX",
                  "text": "Subreddit about cli tool for running LLMs doesnt mean that vibecoding random bs is good and everyone should immediately use it lmao, vibecoding is still vibecoding, and its still low effort",
                  "score": 2,
                  "created_utc": "2026-01-14 16:30:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzk4g0m",
          "author": "ai_hedge_fund",
          "text": "I appreciate that you saw something you wanted to improve and took the time to build something you like - good job!\n\nI‚Äôve commented before that having more options for ai clients is a good thing and it‚Äôs great to see new ideas. My preference is for lighter-weight clients that don‚Äôt try to do everything as Open WebUI kind of did\n\nAnyway, good job and will star on GitHub",
          "score": 0,
          "created_utc": "2026-01-14 16:02:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}