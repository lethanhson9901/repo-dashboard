{
  "metadata": {
    "last_updated": "2026-02-09 09:19:59",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 20,
    "total_comments": 132,
    "file_size_bytes": 146425
  },
  "items": [
    {
      "id": "1qyghp3",
      "title": "Ollie | A Friendly, Local-First AI Companion for Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/",
      "author": "MoonXPlayer",
      "created_utc": "2026-02-07 15:21:30",
      "score": 75,
      "num_comments": 33,
      "upvote_ratio": 0.95,
      "text": "Hi everyone,\n\nIâ€™m sharing **Ollie**, a Linux-native, local-first personal AI assistant built on top of **Ollama**.\n\nhttps://preview.redd.it/fh544zreb3ig1.png?width=1682&format=png&auto=webp&s=23c108dff77d288035dbc0d1dff64503bcd370dd\n\nOllie runs entirely on your machine â€” no cloud (I'm considering optional cloud APIs like Anthropic), no tracking, no CLI. It offers a polished desktop experience for chatting with local LLMs, managing models, analyzing files and images, and monitoring system usage in real time.\n\n**Highlights**\n\n* Clean chat UI with full Markdown, code, tables, and math\n* Built-in model management (pull / delete / switch)\n* Vision + PDF / text file analysis (drag & drop)\n* AppImage distribution (download & run)\n\nBuilt with **Tauri v2 (Rust) + React + TypeScript**.\n\nFeedback and technical criticism are very welcome.\n\nGitHub: [https://github.com/MedGm/Ollie](https://github.com/MedGm/Ollie)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o43q6zx",
          "author": "cuberhino",
          "text": "What are some use cases for this vs say just using lmstudio",
          "score": 5,
          "created_utc": "2026-02-07 16:33:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43uqz5",
              "author": "MoonXPlayer",
              "text": "In the base, they can look the same, even if itâ€™s already clear that LM Studio is better because:  \n\\- LM Studio is a full LLM platform with SDKs, CLI, engines, and dev tools.  \n\\- Ollie isâ€¦ a different, Linux-native, local-first personal AI companion.\n\nI cannot say there are things where Iâ€™m better yet, but Iâ€™m still trying to make things better, especially in terms of performance *â€” thatâ€™s why I chose Rust â€”* and Iâ€™m still brainstorming some unique features to integrate.",
              "score": 1,
              "created_utc": "2026-02-07 16:56:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o45ivgc",
                  "author": "cuberhino",
                  "text": "Yeah Iâ€™m really just trying to identify some things I would do with it before diving in with the play time on it",
                  "score": 1,
                  "created_utc": "2026-02-07 22:05:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o45lcji",
                  "author": "superdariom",
                  "text": "I don't really think rust is going to make things much faster as your bottleneck will be the inference?",
                  "score": 1,
                  "created_utc": "2026-02-07 22:19:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o44zdjz",
              "author": "germanpickles",
              "text": "The LM Studio desktop app is not open source",
              "score": 1,
              "created_utc": "2026-02-07 20:20:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o45is4v",
                  "author": "cuberhino",
                  "text": "Yeah but you can use open source models with it. Iâ€™m asking for specific use cases for this project",
                  "score": 2,
                  "created_utc": "2026-02-07 22:05:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o43c9md",
          "author": "Sad-Chard-9062",
          "text": "Wow looks great man!",
          "score": 4,
          "created_utc": "2026-02-07 15:25:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43cfou",
              "author": "MoonXPlayer",
              "text": "Thanks a lot, I really appreciate it!",
              "score": 1,
              "created_utc": "2026-02-07 15:26:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43jki2",
          "author": "valosius",
          "text": "Hi, sinple not found on github:  \n\nCan you send the exect link to the AppImage ?\n\nOlav\n\n\\#### snip ##\n\nwget -O Ollie.AppImage [https://github.com/MedGm/OllamaGUI/releases/latest/download/Ollie\\_\\*\\_amd64.AppImage](https://github.com/MedGm/OllamaGUI/releases/latest/download/Ollie_*_amd64.AppImage)\n\n...\n\nPlatz: [https://github.com/MedGm/Ollie/releases/download/v0.2.1/Ollie\\_\\*\\_amd64.AppImage](https://github.com/MedGm/Ollie/releases/download/v0.2.1/Ollie_*_amd64.AppImage) \\[folgend\n\nWiederverwendung der bestehenden Verbindung zu github.com:443.\n\nHTTP-Anforderung gesendet, auf Antwort wird gewartet â€¦ 404 Not Found\n\n2026-02-07 16:54:43 FEHLER 404: Not Found.\n\n\\## snip ##",
          "score": 3,
          "created_utc": "2026-02-07 16:01:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43kyk1",
              "author": "MoonXPlayer",
              "text": "Thank you for pointing that out\n\nthe wildcard `*` doesnâ€™t work in direct GitHub URLs. The exact AppImage link for the latest release is:  \n[https://github.com/MedGm/Ollie/releases/download/v0.2.1/Ollie\\_0.2.1\\_amd64.AppImage]()\n\nThen run:\n\n     chmod +x Ollie_0.2.1_amd64.AppImage\n    ./Ollie_0.2.1_amd64.AppImage\n\nIâ€™ll update the README to avoid the confusion. Thanks!!",
              "score": 1,
              "created_utc": "2026-02-07 16:07:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o49xhpu",
                  "author": "valosius",
                  "text": "Unfortunately, there are too many dependencies on the new glibc library, but I'm not going to update my Ubuntu system just for a single client. I can simply test it in a virtual machine.",
                  "score": 1,
                  "created_utc": "2026-02-08 16:36:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4590x7",
          "author": "Noiselexer",
          "text": "Tauri but no windows build?",
          "score": 3,
          "created_utc": "2026-02-07 21:13:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49yryd",
              "author": "MoonXPlayer",
              "text": "I will surely work on a Windows build version near in the future. I just wanted to start by focusing on a Linux native build first, since Linux is my main machine",
              "score": 2,
              "created_utc": "2026-02-08 16:42:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43jb6h",
          "author": "Prestigious_Ebb_5131",
          "text": "Looks perfekt! Does MCP integration support HTTP streaming and Bearer token authorization?",
          "score": 2,
          "created_utc": "2026-02-07 15:59:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43jyad",
              "author": "MoonXPlayer",
              "text": "Thanks!!  \nMCP support is planned but not there yet. my idea is to support HTTP-based MCP servers with streaming, and yes, Bearer token auth is part of that plan.",
              "score": 2,
              "created_utc": "2026-02-07 16:02:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o43l6yb",
                  "author": "Prestigious_Ebb_5131",
                  "text": "Thats great! MCP and building normal agentic loops (state Machines) -and it will be a unique and one-of-a-kind product. I'll do my best to help via pull requests, if you don't mind. \nGreat Job! ðŸ‘",
                  "score": 5,
                  "created_utc": "2026-02-07 16:09:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o44x7bt",
          "author": "PhilCoyo",
          "text": "I would immediately work with this if it had MCP Support. I have so many projects and admin work Where im using Mcps and I feel terrible always giving Claude Access instead of having a local Solution",
          "score": 2,
          "created_utc": "2026-02-07 20:09:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47i0n3",
          "author": "rb1811",
          "text": "Is there a way to integrate s3 client? Say Minio? For auto clean up of blobs uploaded ? \n\nAny benefits over OpenWebUI?",
          "score": 2,
          "created_utc": "2026-02-08 05:42:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49zfyd",
              "author": "MoonXPlayer",
              "text": "S3 / MinIO integration is not implemented yet, but itâ€™s a good idea. still thinking through the right abstraction to avoid adding unnecessary complexity.\n\nalso compared to OpenWebUI, Ollie main focus is local performance and OS-level integration.",
              "score": 1,
              "created_utc": "2026-02-08 16:45:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ee94u",
                  "author": "rb1811",
                  "text": "Ok looking forward for S3/ Minio integration. Thanks for the reply",
                  "score": 1,
                  "created_utc": "2026-02-09 07:54:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4a5tls",
          "author": "Professional_Ad5011",
          "text": "Fantastic. It's pretty much what I was looking for and now it's available to use.",
          "score": 1,
          "created_utc": "2026-02-08 17:16:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4c0ang",
          "author": "BidWestern1056",
          "text": "use npcpy to handle multi providers and a variety of other LLM capabilitiesÂ \nhttps://github.com/npc-worldwide/npcpy",
          "score": 1,
          "created_utc": "2026-02-08 22:42:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu4ka4",
      "title": "Recommendations for a good value machine to run LLMs locally?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qu4ka4/recommendations_for_a_good_value_machine_to_run/",
      "author": "onesemesterchinese",
      "created_utc": "2026-02-02 19:14:51",
      "score": 61,
      "num_comments": 64,
      "upvote_ratio": 0.96,
      "text": "Thinking of purchasing a machine in the few thousand $ range to work on some personal projects. Would like to hear if anyone has any thoughts or positive/negative experiences running inference with some of the bigger open models locally or with finetuning? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qu4ka4/recommendations_for_a_good_value_machine_to_run/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o37ioct",
          "author": "ZeroSkribe",
          "text": "Look at rtx 5060ti 16GB, get two if you can",
          "score": 24,
          "created_utc": "2026-02-02 19:23:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37lets",
              "author": "v01dm4n",
              "text": "Two would also need 2x expensive motherboard. Better to get a gpu with more vram with that budget. Say rtx pro 4000 with 24g vram.",
              "score": -2,
              "created_utc": "2026-02-02 19:35:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3aavni",
                  "author": "Big-Masterpiece-9581",
                  "text": "There is no reasonable priced new gpu with 24gb. New Intel b60 or old ass Nvidia RTX 3090 used for \nmining are both $800-900. For that price can get 32gb with two new 5060ti and for inference speeds will be fine.",
                  "score": 4,
                  "created_utc": "2026-02-03 04:14:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o38xglb",
                  "author": "ZeroSkribe",
                  "text": "no you wouldn't, even the cheapest motherboards come with 2 connectors, what you meant to say was you'll need a bigger power supply..duhhhhhh",
                  "score": 2,
                  "created_utc": "2026-02-02 23:30:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o38q7sl",
                  "author": "windumasta",
                  "text": "\"2x une carte mÃ¨re chÃ¨re\" ? si non une carte mÃ¨re avec 2 pcie 8x ?",
                  "score": 0,
                  "created_utc": "2026-02-02 22:51:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37mqsd",
          "author": "photobydanielr",
          "text": "The Ryzen ai max 395+ 128GB shared memory blah blah named machines come to mind. Good value if the models you want to use fit.",
          "score": 9,
          "created_utc": "2026-02-02 19:42:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37roo3",
              "author": "cjc4096",
              "text": "Rocm 7.2 fixed a lot of stability issues on 395+ too.",
              "score": 3,
              "created_utc": "2026-02-02 20:05:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37s30n",
                  "author": "photobydanielr",
                  "text": "I own the flow z13 128GB and boy has it been a wild ride",
                  "score": 5,
                  "created_utc": "2026-02-02 20:07:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3bsyf3",
              "author": "[deleted]",
              "text": "I have the framework desktop and it was the best purchase for me. 128GB ram can do wonders for huge context, but the prompt processing is slower than models that run on better GPUs. For me because i write code slower than these models, works perfectly for me",
              "score": 2,
              "created_utc": "2026-02-03 11:58:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37o98j",
          "author": "band-of-horses",
          "text": "You can spend thousands of dollars on a machine that can run a model that is still a fraction as capable as any of the cloud hosted models. If your motivation is to save money it's probably not worth it. If it's privacy there are cloud options for open models with no retention. If you just want to run small models locally for fun a cheap machine with 32 of ram or a 16gb c video card will do it.",
          "score": 16,
          "created_utc": "2026-02-02 19:49:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37wvto",
              "author": "mtbMo",
              "text": "Thatâ€™s right. Your local LLM instance will never ever compete with a trillion params models from cloud inference provider.\nYou can run local models and achieve good results.\n\nIâ€™m running a Dell workstation Xeon v4 256gb ram and three GPUs with 50GB VRAM all second hand\nItâ€™s up to 37t/s with drawing 500W from the wall",
              "score": 5,
              "created_utc": "2026-02-02 20:30:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37z6ji",
                  "author": "mon_key_house",
                  "text": "What GPUs do you have? I currently have 2x Rtx3060 and thinking about going 4x or getting 2x rtx 3090.",
                  "score": 1,
                  "created_utc": "2026-02-02 20:40:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o383h04",
              "author": "BoostedHemi73",
              "text": "This is so depressing. Weâ€™re just supposed to be beholden to cloud companies for this stuff? Thatâ€™s going to push everyone into like three places.\n\nComputers used to be so fun. This timeline sucks.",
              "score": 2,
              "created_utc": "2026-02-02 21:01:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3868lu",
                  "author": "goodguybane",
                  "text": "Running local LLMs IS fun, you just have to manage your expectations.",
                  "score": 8,
                  "created_utc": "2026-02-02 21:14:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3bxwhj",
                  "author": "SirWinstonSmith",
                  "text": "Plenty of technology has been very computationally expensive until it suddenly wasn't. What has changed is that you now get a sneak peek of the future through cloud services, but that future will eventually be optimized. Most likely there will be an innovation or new architecture that severely reduces the computational expense. And even if there isn't, current open source models are more than good enough to do most simple tasks on a limited context window.",
                  "score": 1,
                  "created_utc": "2026-02-03 12:34:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3cc928",
                  "author": "luncheroo",
                  "text": "Smaller local LLMs are currently are currently at GPT-3.5 or GPT-4, so a couple of years behind frontier. I'm not expecting them to surpass frontier in real time, but over time the trend is that they get better and better at a pretty fast clipÂ ",
                  "score": 1,
                  "created_utc": "2026-02-03 14:00:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3f0cjd",
                  "author": "mpw-linux",
                  "text": "They still are fun, local models give one a great learning experience that surprisingly powerful, give it a try. Most AI developers work on local machines before porting to the big iron.",
                  "score": 1,
                  "created_utc": "2026-02-03 21:33:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ezy6i",
              "author": "mpw-linux",
              "text": "Of course you are correct but for experimentation and getting the code right before using paid cloud services local development is fine. Apples's M chips are great for developing AI applications.",
              "score": 1,
              "created_utc": "2026-02-03 21:32:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37liwz",
          "author": "AmphibianFrog",
          "text": "I'm not convinced there is a good value machine to run LLMs locally! If I was on a budget I would build a pc with a single RTX3090.",
          "score": 6,
          "created_utc": "2026-02-02 19:36:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37kn8l",
          "author": "germanpickles",
          "text": "You can consider getting a Mac to run local LLMâ€™s. Macâ€™s have something called Unified Memory where the vRAM and system RAM are shared on the same bus. So if you have 64 GB of Unified Memory, you will be able to load a lot of different local models. While Ollama is a great tool, you can also look at LM Studio which has a MLX inference engine which allows you to run MLX LLMâ€™s very fast on a Mac.",
          "score": 17,
          "created_utc": "2026-02-02 19:32:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38rnlo",
              "author": "n2itus",
              "text": "This is really good advice.  I've just started down this journey and was surprised as how well my Macbook Air with 16gb ran local models.  I started looking at mac minis with 64 gb ... as memory is very important. You could not get 64 gb from video cards for as cheap as getting a 64 gb mac mini.",
              "score": 8,
              "created_utc": "2026-02-02 22:58:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o37mc0j",
              "author": "onesemesterchinese",
              "text": "Nice, I was looking at the mac mini -- You think that would work for finetuning some of the smaller models?",
              "score": 2,
              "created_utc": "2026-02-02 19:40:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o37rl6l",
                  "author": "st0ut717",
                  "text": "I am running Gemma 3b.  On my Mac air m4 with 16gb ram.",
                  "score": 4,
                  "created_utc": "2026-02-02 20:04:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3b4a3g",
                  "author": "arqn22",
                  "text": "I believe that fine tuning is especially slow on Mac chips, but you should verify that.  I have a 64gb MacBook pro and it can run a lot of models at decent speeds as long as context window isn't too high, not over 16k-32k (or an advanced attention mechanism is in place like in nemotron-nano-3 which easily handles 128 or probably more).  It's still significantly slower than GPUs with less RAM for models that fit in them though.  And I believe I've read that it's extra slow for training/fine-tuning.  But I don't have a source for that.",
                  "score": 3,
                  "created_utc": "2026-02-03 08:11:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o38c77n",
                  "author": "germanpickles",
                  "text": "Absolutely. Do check out Alex Ziskind on YouTube - https://youtube.com/@azisk?si=Q7cXou-ONnH0IP-P, he has so many videos on running local LLMâ€™s on Mac as well as DGX Spark etc",
                  "score": 2,
                  "created_utc": "2026-02-02 21:42:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o39s626",
              "author": "rorowhat",
              "text": "Lol don't. If anything get a strict halo",
              "score": -1,
              "created_utc": "2026-02-03 02:21:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o38xv46",
              "author": "ZeroSkribe",
              "text": "the mac mini slop is getting old, please research GPU memory speeds vs unified speeds. Its not worth it if every request takes forever.",
              "score": -6,
              "created_utc": "2026-02-02 23:32:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o39fhsr",
                  "author": "germanpickles",
                  "text": "Everyone's experience with speed on a Mac Mini vs GPU will really depend on the specs of the machine, the inference engine, quantization and the model. From my own real world experience, using local AI every day, I'm getting 104 tok/sec. Here are the specs:\n\n* Mac Mini with M4 Pro\n* 14 core CPU/20 core GPU\n* 64GB UMA (Unified Memory Architecture)\n* Llama 3.2 3B 4-bit MLX\n\nRunning the same model (Q4\\_K\\_M) on my RTX 5080, I get 276 tok/sec. Is it faster? Yes. But is the Mac still usable? Of course.\n\nNow of course, if someone is running the entry level Mac Mini and loading a 70 billion parameter model, it will be extremely slow.",
                  "score": 3,
                  "created_utc": "2026-02-03 01:09:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o39q42d",
          "author": "8ballfpv",
          "text": "I just built myself a machine to mess around with:\n\n\\- Chassis - supermicro SYS-7049GP-TRT\n\n\\- CPU - Dual Intel Xeon Gold 6240\n\n\\- Ram - 165GB DDr4\n\n\\- GPU - Dual rtx 3090\n\nplaying around with various models atm.  its nice to have the flexability with the 2 gpus. Means I can run a 70b size or 2 30b sizes ( 1 on each gpu) or many smaller ones depending what I want.  \nGot the hardware piping out to a grafana dashboard so I can keep an eye on the basics realtime, Run openwebui on another machine and Ollaman for quick maintenance of models etc etc. All up cost me about $5k aud.",
          "score": 3,
          "created_utc": "2026-02-03 02:09:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37hoew",
          "author": "DutchGM",
          "text": "I am currently testing the Dell GB10 spark. I see potential but Iâ€™m still benchmark testing it.",
          "score": 3,
          "created_utc": "2026-02-02 19:18:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37ws4q",
          "author": "st0ut717",
          "text": "Large models are not always the answer. \nWhat do you want to do?  \n\n\nMac mini if you want a general purpose pc that can also do llm \nOtherwise a dell gb10.  Or the like if you want a dedicated llm work station",
          "score": 3,
          "created_utc": "2026-02-02 20:29:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o382b5w",
          "author": "Possible-Algae4391",
          "text": "An older X99 workstation board, aÂ cheap Xeon CPU, 2x3060 GPU (2x12GB)\n\n\nIt was enough for me, I could run 30b models with reasonable speed (still far from what we're used with cloud models). It cost me about 1000$ with used parts.\n\n\nYou can go for beefier 3090s but then you might want a newer board with PCIe 4.",
          "score": 3,
          "created_utc": "2026-02-02 20:55:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38ybep",
          "author": "dipoots_",
          "text": "MBP Pro M5 at least 24 gb ram or Mac mini equivalent",
          "score": 2,
          "created_utc": "2026-02-02 23:34:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o394rv9",
          "author": "XdtTransform",
          "text": "These are all fine suggestions, but if you are planning to do something serious, you need, at the very least, a ~30b type model.  Something like Gemma3:27b.  This means at least a card with 24GB of VRAM.  \n\nThe other alternatives I've seen suggested like various 3b models fall apart at anything serious or with a larger context window.",
          "score": 2,
          "created_utc": "2026-02-03 00:10:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3a5rdi",
          "author": "recoverygarde",
          "text": "I would recommend get either a m5 14in MBP or wait for the rest of the lineup to get the M5 gen",
          "score": 1,
          "created_utc": "2026-02-03 03:41:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3blcrx",
          "author": "Financial-Source7453",
          "text": "If size & power consumption matter, go for Asus Aspire Gx10 (so far the cheapest clone of Nvidia Spark). It's a palm sized box filled with 128GB unified memory and 20x core ARM CPU. It's also Nvidia box, so you will be able to run almost everything from the local Ai world with acceptable speed. Macs suck at video/image generation, dedicated GPUs are noisy and eat a lot of space and electricity. AMD Strix Halo systems have lower performance and often come with HW issues.",
          "score": 1,
          "created_utc": "2026-02-03 10:55:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rd70w",
              "author": "drivebyposter2020",
              "text": "I would think or hope that the Strix Halo systems have hardware issues that depend upon how cheap a vendor you went with. there must be 50 of them on the market by now. back on Black Friday in the US last year I found I could get an HP maxed out for about $300 more than the mystery meat brands. so worth it. I'm not sure how the relative pricing is now, but watch for sales and HP coupon codes online. I managed to knock 10% off by just stumbling on to the right coupon code on one of those silly coupon code websites.\n\n\nI'm still just scratching the surface because my own first level interests actually lie in things like MCP and orchestration and the middleware of how you tie all this stuff together, but as I get more serious about the AI part, I suspect that this box will continue to do the heavy lifting for me in a way that I find more than acceptable, even though I know it's not exactly one of the big cloud systems.",
              "score": 1,
              "created_utc": "2026-02-05 18:24:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ez0s7",
          "author": "mpw-linux",
          "text": "Get one of those Macbook pro: M1,M2,M3,M4 with at least 32g ram. The Mlx framework is great designed for the Silicon chip. There are many models converted to Mlx on huggingface. I recently bought a Macbook pro 16 M1 with 32g to run lots local models.",
          "score": 1,
          "created_utc": "2026-02-03 21:27:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3a9y18",
          "author": "m-gethen",
          "text": "I just built a really solid budget-constrained PC for a friend to use for a) 3D CAD in Solidworks and Blender and b) so he could start down the local LLM path. Heâ€™s been very happy so far.\n\nhttps://preview.redd.it/h0fowxpfe7hg1.jpeg?width=4624&format=pjpg&auto=webp&s=09669791be012602f9e57f633cc67e4d5961a062\n\nKey specsâ€¦\n\n* Intel Core Ultra 5 245K\n* Gigabyte Z890M motherboard (automatically bifurcates and runs GPUs at PCIE 8)\n* 32Gb DDR5-6400 memory\n* 2x Intel Arc B580 12Gb (pooled for total 24Gb VRAM).\n\nOllama and LM Studio working smoothly with Vulcan and newest 32B MoE models like Granite 4 H in this set up.",
          "score": 1,
          "created_utc": "2026-02-03 04:07:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39e4p0",
          "author": "Seninut",
          "text": "Follow Jenson around with a pooper scooper, I am sure one will come out.",
          "score": -2,
          "created_utc": "2026-02-03 01:01:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvycjs",
      "title": "Just started using local LLMs, is this level of being wrong normal?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qvycjs/just_started_using_local_llms_is_this_level_of/",
      "author": "Cempres",
      "created_utc": "2026-02-04 19:31:52",
      "score": 46,
      "num_comments": 81,
      "upvote_ratio": 0.81,
      "text": "https://preview.redd.it/4swbpymp4jhg1.png?width=746&format=png&auto=webp&s=16d362a6bd68f24a9b349d8f0f21aec95376aa11\n\nTried using the glm4.7 flash version, and just randomly asking it stuff, and i got his with this trip, is that normal to expect and can someone explain why this even happens? lack of internet connection? Should I expect this from other models as well? \n\nMy regular usage wouldn't be this, but a tool to help me create D&D stuff, such as character ideas, helping me when i get stuck ecetera, any model to recommend for such? \n\nSpecs:  \nwin 11  \n32gb RAM  \nXTX 24gb VRAM",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qvycjs/just_started_using_local_llms_is_this_level_of/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3l3wsb",
          "author": "InfraScaler",
          "text": "Yup. Now online models have tools to fetch data from the web, but yeah hallucination is relatively common. The LLM tries to fill knowledge gaps and it just makes up stuff to accomplish that.",
          "score": 52,
          "created_utc": "2026-02-04 19:35:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l86nc",
              "author": "Cempres",
              "text": "Then i'm missing the point of local LLMs, kinda..",
              "score": 5,
              "created_utc": "2026-02-04 19:56:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3la92q",
                  "author": "Dubious-Decisions",
                  "text": "I think you are missing the point of LLMs in general. They are not definitive sources of fact. They are chat simulators. They are designed to predict the most reasonable, statistically likely series of tokens that should be emitted in response to all of their inputs. They have no ability to self-assess or fact check or even understand the semantics of what you are giving them as input.\n\nSo no, you shouldn't expect them to be accurate. The larger the model, the larger the training set and the more likely it is to match some piece of human-created information it was trained on. But as models shift to training with synthetic data and output from other models, you can imagine what happens to accuracy.\n\nWhat they ARE is reasonably inexpensive, simple to interact with natural language processors (NLPs) that can act as some reasonably reliable glue between users and tools. It's a gross oversimplification to say they can't produce useful results, but it's also incorrect to assume that they will ever be foolproof. There are certain types of interactions that they simply will never be able to perform.\n\nThere are many other types of AI and machine learning (ML) platforms besides LLMs, and the tragedy lately is that no one seems to remember that and instead are trying to build the entire AI house with nothing but a LLM hammer.\n\nOnce you realize that these are just tools for parsing human text inputs and producing some compelling, simulated text outputs, you will be better suited to figure out what they are and are not good for.",
                  "score": 92,
                  "created_utc": "2026-02-04 20:06:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3l8stl",
                  "author": "LePfeiff",
                  "text": "An LLM doesnt need to know alot of facts to be a DM assistant in DnD, but its also just a consequence of having to run smaller quantized models that fit on consumer hardware",
                  "score": 5,
                  "created_utc": "2026-02-04 19:58:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lbupi",
                  "author": "sinan_online",
                  "text": "I think they work best for fairly technical hobbyists. For instance, I am combining repeated LLM calls with some Python code to find a good way to get structured markdown out of PDF books. In other words, it's a specific use case of improving OCR quality.\n\nYou can also use them for classification tasks, etc... I personally do not have any cases that do not involve some programming or technical knowledge.\n\n",
                  "score": 4,
                  "created_utc": "2026-02-04 20:13:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lmray",
                  "author": "zenmatrix83",
                  "text": "if you want proper research you can likely try something like this [https://github.com/langchain-ai/local-deep-researcher](https://github.com/langchain-ai/local-deep-researcher) . a deep research look is doing a websearch, the ai gets a finding from the research report,  and then does a loop looking up answers trying to create a report.",
                  "score": 2,
                  "created_utc": "2026-02-04 21:05:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m8301",
                  "author": "Medium_Ordinary_2727",
                  "text": "To be clear all LLMs will do this to a certain extent. Some are better than others bit they can still hallucinate.\n\nFrame your prompt not as a question that you are asking the LLM, but as a research request, and give it the tools to do that research (such as web access).",
                  "score": 2,
                  "created_utc": "2026-02-04 22:50:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3mk2e4",
                  "author": "Western_Courage_6563",
                  "text": "Think about them as an engine, they need rest of the stuff to work (memory, Internet aces, local database, etc).",
                  "score": 2,
                  "created_utc": "2026-02-04 23:55:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3llgoe",
                  "author": "zenmatrix83",
                  "text": "to keep it simple llm means large language model or its a model of how a language is.  This is a decent video on how it works [https://www.youtube.com/watch?v=wjZofJX0v4M](https://www.youtube.com/watch?v=wjZofJX0v4M) .  Its doesn't know whats its saying, its a statistically correct response, which alot of time is correct, but not always. The lower power the model the more it will be statistically wrong.",
                  "score": 2,
                  "created_utc": "2026-02-04 20:59:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m16qs",
                  "author": "florinandrei",
                  "text": "More complex tasks - use bigger models.\n\nSimpler tasks - smaller models are okay.\n\nUnless you have your own dedicated inference hardware, you're not going to run big models at home.\n\nThe definitions of \"simpler\" and \"more complex\" will continue to evolve over time.",
                  "score": 1,
                  "created_utc": "2026-02-04 22:15:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3mv6sa",
                  "author": "oVerde",
                  "text": "If you use it in an agentic environment, like OpenCode, you then can provide tools to fetch from the internet",
                  "score": 1,
                  "created_utc": "2026-02-05 00:56:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3nzbs1",
                  "author": "fasti-au",
                  "text": "Cintext is a bucket of things it can juggle to find the 8/16/32k it can actually relate to your question thatâ€™s what is juggled by 1 shit then think deals with one shot output to decide if it was good. \n\nSo how you fill that first one shot is the key to getting this broken binary system to work.  Ternerary fixes it but itâ€™s not really hardware friendly yet.  Next round of llm with bee new chips if we are transforming but big fusion seems more in line with the in between steps",
                  "score": 1,
                  "created_utc": "2026-02-05 04:59:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3v5w92",
                  "author": "andrew_barratt",
                  "text": "Think of it less like a huge knowledgeable system, more like a software component that can read",
                  "score": 1,
                  "created_utc": "2026-02-06 07:39:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3m49wm",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-02-04 22:30:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3m91wk",
                  "author": "InfraScaler",
                  "text": "If you're offline you can't fetch data. Local does not mean offline. Thing is, if you want it to be online, you need to give it the tools to fetch data.",
                  "score": 2,
                  "created_utc": "2026-02-04 22:55:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3m36jm",
          "author": "themaskbehindtheman",
          "text": "That's why web search and tools are a thing, the tools are a big reason these companies still hire software engineers. \n\nMonitor chats > find commonly asked questions where an incorrect answer is given > write tool > claim sentience /s\n\nIf you whack openweb ui over the top of it, enable web search and ask about the weather it'll get it wrong, you have to write a tool to call a weather API, it can figure out it needs to use the tool and then to parse the JSON response.",
          "score": 10,
          "created_utc": "2026-02-04 22:25:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q37r3",
          "author": "Past-Grapefruit488",
          "text": "You need to enable Web Search so that App can use it to look up information. LLMs (Local or otherwise) are not encyclopedia. ChatGPT or Gemini will run a web search to answer such questions . Example : \n\nhttps://preview.redd.it/wvwdp3xdvohg1.png?width=1091&format=png&auto=webp&s=bb9ad3dbe1437c5f24e3a3e3f37e1bbf985dcf6f\n\n",
          "score": 5,
          "created_utc": "2026-02-05 14:48:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4cfhj1",
              "author": "FaithlessnessLife876",
              "text": "Yeah I think people don't realise how much [chatgpt.com](http://chatgpt.com) is actually a wrapper and the LLMs themselves how much pre-promting finetuning they have with the goal of mimic human behaviour",
              "score": 1,
              "created_utc": "2026-02-09 00:11:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ladtw",
          "author": "HonestoJago",
          "text": "GLM benefits from a low temp in situations like this, at least in my experience. But yeah, if youâ€™re expecting Claudeâ€¦..donâ€™t.",
          "score": 3,
          "created_utc": "2026-02-04 20:06:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lw42r",
              "author": "Cempres",
              "text": "was not expecting anything in particular, was just wondering. I'll test it with ideas for D&D, advices on how to connect different story arcs and such and i'll write a comment when i have the chance to test it for such use",
              "score": 1,
              "created_utc": "2026-02-04 21:50:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3qw8wy",
                  "author": "Dubious-Decisions",
                  "text": "Load a bunch of slightly structured (e.g. YAML) files into an IDE like VS Code and hook it up to your local LLM. It can crawl all over that document tree and make notes to itself. When you tell it you are in \"room x\", it can go read through the YAML file for room x at the direction of the IDE and switch its context to use the facts and descriptions you put in the room x file. llama3.2 is actually a competent little LLM for doing this locally. ",
                  "score": 1,
                  "created_utc": "2026-02-05 17:06:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3orp7j",
          "author": "cibernox",
          "text": "Yes. Small locals models are reasonably good at being smart, not at being knowledgeable about every niche. Whey will get most thing that widely known well (year of a mayor war, what is the biggest mammal, and wether it rains more in Seattle or in Los Angeles, but not how many mg of caffeine a drink has).",
          "score": 3,
          "created_utc": "2026-02-05 09:04:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l9bgg",
          "author": "Condomphobic",
          "text": "They generally suck unless they're the full-sized model, which normal consumers cannot afford",
          "score": 5,
          "created_utc": "2026-02-04 20:01:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l99z6",
          "author": "Witty_Mycologist_995",
          "text": "Thatâ€™s kind of strange. I use glm flash. What quantization is that?\nAsk it the capital of france",
          "score": 1,
          "created_utc": "2026-02-04 20:01:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lkd9i",
              "author": "gregusmeus",
              "text": "Berlin, lol. Sorry.",
              "score": 2,
              "created_utc": "2026-02-04 20:54:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ln11g",
                  "author": "mattv8",
                  "text": "ðŸª¦",
                  "score": 1,
                  "created_utc": "2026-02-04 21:07:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3pk783",
              "author": "Dzhmelyk135",
              "text": "Not even my Q3 version is that dumb",
              "score": 2,
              "created_utc": "2026-02-05 13:02:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3lvq1m",
              "author": "Cempres",
              "text": "https://preview.redd.it/bz8ghqbltjhg1.png?width=744&format=png&auto=webp&s=6e3477d638ab234ed58ad52dd52d3d6ab0b6c135\n\nI was trying out the glm-4.7-flash:q4\\_K\\_M",
              "score": 1,
              "created_utc": "2026-02-04 21:48:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3lzqy9",
                  "author": "Witty_Mycologist_995",
                  "text": "Send reasoning traces",
                  "score": 2,
                  "created_utc": "2026-02-04 22:07:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3lzww6",
          "author": "Ryanmonroe82",
          "text": "You need to use a smaller model with bf/fp16.  Using larger models that are quantized will always be less accurate especially a thinking/reasoning model thatâ€™s a MoE.  With 24gb VRAM look at dense 8b models that you can run in FP16/B16 with headroom for a decent context window.  RNJ-1-8b is excellent so is Gemma 2-7b.  They might not be as large as you want but the precision and accuracy is noticeably better especially on lower temps.",
          "score": 1,
          "created_utc": "2026-02-04 22:08:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mrp7q",
          "author": "superdariom",
          "text": "Isn't glm 4.7  a specialized coding model? Why not try deepseek instead?",
          "score": 1,
          "created_utc": "2026-02-05 00:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n4wj4",
          "author": "mpw-linux",
          "text": "Look at Liquid AI models. I use this one my Mac M1:\n\nmodel, tokenizer = load(\"mlx-community/LFM2.5-1.2B-Thinking-8bit\")\n\n It is quite good. I asked it create a 3 chord modern country song which i did with lyrics and chords. ",
          "score": 1,
          "created_utc": "2026-02-05 01:52:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ocqvk",
          "author": "tom-mart",
          "text": "There is nothing wrong about the example you showed. What is the problem?",
          "score": 1,
          "created_utc": "2026-02-05 06:45:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pc8xi",
              "author": "damhack",
              "text": "Itâ€™s low calorie, zero sugar but has caffeine in it.",
              "score": 1,
              "created_utc": "2026-02-05 12:08:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3pd024",
                  "author": "tom-mart",
                  "text": "I asked what's the problem in LLM response? It looks correct.",
                  "score": 1,
                  "created_utc": "2026-02-05 12:13:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pouht",
          "author": "bitmux",
          "text": "I find that most of the single-gpu local LLM's I've played with generate about 60% bs mostly regardless of settings and even including the websearch tool UNLESS they're hyper specialized in something and you use them right down the center of their specialization.  At that point its 40% bs :-P.",
          "score": 1,
          "created_utc": "2026-02-05 13:29:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rfwmp",
          "author": "generousone",
          "text": "With 24gb vram, go down to a smaller model like gpt-oss:20b and give it full 128k context. You'll see a vast improvement on capability and usefulness. GLM flash is 19gb. That's not a lot of room for context with 24gb vram.Â \n\n\n\nGenerally, it's also good to enable things like web search when asking fact specific questions.Â Â \n\n\nEdit: lol, tried the exact prompt in my gpt-oss:20b and it wouldn't complete the request because it was censored to providing proprietary information. So I omitted \"secret\" and just asked what the ingredient were and it worked.Â ",
          "score": 1,
          "created_utc": "2026-02-05 18:36:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vamc5",
          "author": "joshiegy",
          "text": "One of the best ways to use a local llm is to combine with an MCP that gives some sort of webacrape ability.\nOr to have one specific to a task, like storytelling or code review in a specific language.\n\nBut it's not a GPT, those are huuuuuge. Chatgpt, a while back at least, requires something like 256gb of vram. Maybe it's even more now.",
          "score": 1,
          "created_utc": "2026-02-06 08:23:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vislx",
          "author": "maxbiz",
          "text": "The problem you have run into is that LLMs are poor at saying, 'I don't know.'  Where the LLM does not know, it treats it as a multiple-choice question; a random pick of 4 possible answers is a 25% chance of being right rather than 0% for 'I do not know'.",
          "score": 1,
          "created_utc": "2026-02-06 09:42:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xrgvy",
          "author": "fasti-au",
          "text": "Local is smart but not wise. Was one comes with cintext filling with info not nearest guesses.   They can do anything but anything is anything you specifically are giving it information to work with not anything you need a nuclear plant Greenland all the metals from Canada and ukrain as well as cooling and also did we menthey they cheat lies and hide behind the government not fight court battles unlike anthropic whi get to turn around and destroy copyright completely and have to then compete as an underdog but actually are trying for good things in more ways\n\nOpenAI motto is all your shit belongs to us pay us for it back and do it while we destroy your jobs and say they are not money gobbling cunts with smart people staying alive and staying because they know not money or in club is bad",
          "score": 1,
          "created_utc": "2026-02-06 17:41:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42i8ux",
          "author": "dreamer2020-",
          "text": "Dont know what version of glm4.7 flash you are using. But the full variant is way above 250gb ram. Try to use gpt oss 120b if possible, it is really â€œwiseâ€",
          "score": 1,
          "created_utc": "2026-02-07 12:23:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n17yi",
          "author": "PrepperDisk",
          "text": "Yep! Â We had to give up on bringing one to Prepper Disk even with RAG. Â They are proofs of concept at best.",
          "score": 1,
          "created_utc": "2026-02-05 01:31:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nyx8l",
          "author": "fasti-au",
          "text": "Models are how they thing not what they think.   What they thing is parameters so the memory base inside a 1 trillion midel is about not looking.  Local models are all about the perfect prompt to avoid think having a different plan to you",
          "score": 0,
          "created_utc": "2026-02-05 04:56:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvj4la",
      "title": "Qwen3-Coder-Next",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qvj4la/qwen3codernext/",
      "author": "stailgot",
      "created_utc": "2026-02-04 08:29:24",
      "score": 38,
      "num_comments": 9,
      "upvote_ratio": 0.98,
      "text": "Qwen3-Coder-Next is a coding-focused language model from Alibaba's Qwen team, optimized for agentic coding workflows and local development. \n\n> ollama run qwen3-coder-next\n\nhttps://ollama.com/library/qwen3-coder-next\n\n> requires Ollama 0.15.5",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qvj4la/qwen3codernext/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3xt8lz",
          "author": "Mangostickyrice1999",
          "text": "Sighhh 64gb vram is 10k here in Europe. Just for de gpu power",
          "score": 1,
          "created_utc": "2026-02-06 17:49:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zoxer",
              "author": "Daepilin",
              "text": "You can run it on a 16gb card if you have enough System RAM (64 Gigs)Â \n\n\nIt's kinda slow though at thst point.Â ",
              "score": 1,
              "created_utc": "2026-02-06 23:28:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o42ughk",
              "author": "tecneeq",
              "text": "A 5090 and two 5070 Ti and a board can be had for 5 to 6kâ‚¬. Not sure what the Ryzen AI Max computers with 128GB cost, but i guess it's around 2.5kâ‚¬.",
              "score": 1,
              "created_utc": "2026-02-07 13:46:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3iq21p",
          "author": "WaitformeBumblebee",
          "text": "looking forward to test this and pit it against GLM4.7 flash which after minimal testing seems better than qwen3-A3B-2507",
          "score": 1,
          "created_utc": "2026-02-04 12:35:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ivvfi",
          "author": "Final-Watercress-253",
          "text": "What do you use these models with? I mean, with Claude I have the terminal, but if I want to use one of these models, what terminal software do I have to use?",
          "score": 1,
          "created_utc": "2026-02-04 13:12:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3j120b",
              "author": "JMowery",
              "text": "There are dozens of them nowadays. OpenCode seems to be one for the more popular.",
              "score": 6,
              "created_utc": "2026-02-04 13:41:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3u4pmm",
              "author": "coding9",
              "text": "I use it with Claude code still. Its just a couple env variables using LM Studio",
              "score": 2,
              "created_utc": "2026-02-06 03:03:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o42tu8p",
                  "author": "tecneeq",
                  "text": "For now.",
                  "score": 1,
                  "created_utc": "2026-02-07 13:42:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mx7ap",
              "author": "AmphibianFrog",
              "text": "I use the Cline plugin in VS Code and it works well with the previous coder model (haven't tested on this one)",
              "score": 1,
              "created_utc": "2026-02-05 01:08:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxmmio",
      "title": "Best models on your experience with 16gb VRAM? (7800xt)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qxmmio/best_models_on_your_experience_with_16gb_vram/",
      "author": "roshan231",
      "created_utc": "2026-02-06 16:40:26",
      "score": 38,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "Iâ€™m running a 7800 XT (16 GB VRAM) and looking to get the best balance of quality vs performance with Ollama.\n\nWhat models have you personally had good results with on 16 GB VRAM?\n\nReally I'm just curious about your use cases as well. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qxmmio/best_models_on_your_experience_with_16gb_vram/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3xh50w",
          "author": "tcarambat",
          "text": "For me, Qwen3 4B (Q4) @ 128K gives me excellent responses for speed quality without overmaxxing the card. If i need something multi-model (usually do) I just swap out for Qwen3-VL 4B Q4 @ 128k|256K.\n\nWorks excellent for me, no bs and if the thinking is bothering me I just turn it off. One cavet i have found is the qwen3 models you almost never want to send a simple unbounded prompt like \"Hello\" - it will easily think for 1k+ tokens just to say hello.\n\nIf you ask an actual prompt though or attach an image as context it thinks briefly and gives a great response.",
          "score": 11,
          "created_utc": "2026-02-06 16:51:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45aqod",
              "author": "No-Consequence-1779",
              "text": "4b is pretty good. I have 2 32gb gpus. Â I have been using smaller models more and more. Very little difference for simple coding tasks from a 30b to a 4b. Â Though for models coding is the easiest thing.Â ",
              "score": 2,
              "created_utc": "2026-02-07 21:22:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3yavqa",
          "author": "BGPchick",
          "text": "What are your goals? For coding, I really like gpt-oss:20b or qwen3-coder:30b. For general writing, I find llama3.2 or gemma3 a little better and faster.",
          "score": 6,
          "created_utc": "2026-02-06 19:13:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xm6r2",
          "author": "fasti-au",
          "text": "16 gb vram is qwen 14b territory Phi4 mini.   You might fit a devstral2small which is smaller that qwen coder 30b and codes",
          "score": 5,
          "created_utc": "2026-02-06 17:15:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41w28t",
          "author": "nycigo",
          "text": "Devstral 2 Small is unbeatable if you want to program.",
          "score": 4,
          "created_utc": "2026-02-07 08:54:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41ga9r",
          "author": "Remarkable_Stay_592",
          "text": "GPT-OSS 20B, QWEN 3 14B -> coding\nGemma 3 12B -> general chats",
          "score": 2,
          "created_utc": "2026-02-07 06:26:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43ogkv",
          "author": "_w0rm",
          "text": "Interesting. I have been just testing several models to be used with log analysis. So far I have not found reliable model for the task. Many models are successful in find the needle in haystack kind of tasks but fail when asked to retrieve multiple entries from context (kind of reasoning over haystack). In this kind scenarios models often just miss data or start to hallucinate especially in the middle of the context. Best results with reasonable speed I have gained with Ministral-3-14B-Reasoning with Q6_K quant (8B with Q8_0 provide almost equal results), Phi-4-Reasoning-Plus Q4_K_M and gpt-oss-20B Q4_K_M. Qwen-3-30B does good job but is really slow as it needs offloading and still with limited context.\n\nFor testing I wrote a small Python program which generates log file of specified size with known information spread in the logs. \n\nIf anyone have good suggestions, please let me know.",
          "score": 1,
          "created_utc": "2026-02-07 16:25:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y1tem",
          "author": "stonecannon",
          "text": "I like Gemma 3 on my 16gb laptop.",
          "score": 1,
          "created_utc": "2026-02-06 18:30:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40pw50",
          "author": "Old-Sherbert-4495",
          "text": "16gb here as well. tried glm 4.7 flash q2 and q4 they do a pretty good job. but didn't get to extensively test it. it's pretty good at coding agentic taks",
          "score": 0,
          "created_utc": "2026-02-07 03:11:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40xfg6",
              "author": "buttetsu",
              "text": "I second this. glm 4.7 flash w/ Zed has been the best quality I can get at reasonable speed on 16 GB VRAM. Miatral Small 2 is also great. Mistral has been more reliable,; glm has solved some more complex problems but can get stuck in repeat loops sometimes.",
              "score": 1,
              "created_utc": "2026-02-07 04:01:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qy77nm",
      "title": "Lorph: A Local AI Chat App with Advanced Web Search via Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/gallery/1qy77nm",
      "author": "Fantastic-Market-790",
      "created_utc": "2026-02-07 07:09:52",
      "score": 36,
      "num_comments": 9,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qy77nm/lorph_a_local_ai_chat_app_with_advanced_web/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o41okof",
          "author": "DanyShift",
          "text": "Nice! But how does it differ from Open Webui?",
          "score": 19,
          "created_utc": "2026-02-07 07:42:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41v1r6",
              "author": "__Maximum__",
              "text": "And 20 others",
              "score": 9,
              "created_utc": "2026-02-07 08:44:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o41orez",
              "author": "spacywave",
              "text": "Second. Any benefits compared to gpt researcher and similar?",
              "score": 2,
              "created_utc": "2026-02-07 07:44:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o41rcw0",
                  "author": "ExtentOdd",
                  "text": "It uses Ollama which means you can use your local model instead of GPT by openai",
                  "score": -9,
                  "created_utc": "2026-02-07 08:09:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4739gz",
              "author": "Witty_Mycologist_995",
              "text": "Open webui has the shittiest search system ever. Reason why I quit it",
              "score": 0,
              "created_utc": "2026-02-08 03:52:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o422nrk",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -13,
              "created_utc": "2026-02-07 10:00:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o426w4y",
                  "author": "SteveLorde",
                  "text": "holy AI slop",
                  "score": 14,
                  "created_utc": "2026-02-07 10:41:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4c7v2x",
          "author": "ScrapEngineer_",
          "text": "Great, more slop.",
          "score": 1,
          "created_utc": "2026-02-08 23:26:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qv1yey",
      "title": "Your thoughts on \"thinking\" LLMs?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qv1yey/your_thoughts_on_thinking_llms/",
      "author": "stonecannon",
      "created_utc": "2026-02-03 19:41:24",
      "score": 34,
      "num_comments": 43,
      "upvote_ratio": 0.75,
      "text": "almost all of the ollama-ready models released in recent months have been \"thinking\" or \"chain of thought\" or \"reasoning\" models -- you know, the ones that force you to watch the model's simulated thought process before it generates a final answer. \n\npersonally, i find this trend extremely annoying for a couple reasons:\n\n1). it's fake.  that's not how LLMs work.  it's a performance to make it look like the LLM has more consciousness than it does. \n\n2).  it's annoying.  i really don't want to sit through 18 seconds (actual example) of faux-thinking to get a reply to a prompt that just says \"good morning!\".\n\nThe worst example i've seen so far was with Olmo-3.1, which generated 1932 words of \"thinking\" to reply to \"good morning\" (i saved them if you're curious).\n\nin the Ollama CLI, some thinking models respond to the \"/set nothink\" command to turn off thinking mode, but not all do.  and there is no corresponding way to turn off thinking in the GUI.  same goes for the AnythingLLM, LM Studio, and GPT4All GUIs.\n\nso what do \\_you\\_ think?  do you enjoy seeing the simulated thought process in spite of the delays it causes?  if so, i'd love to know what it is that appeals to you... maybe you can help me understand this trend.\n\ni realize some people say this can actually improve results by forcing checkpoints into the inference process (or something like that), but to me it's still not worth it.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qv1yey/your_thoughts_on_thinking_llms/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3ew6mm",
          "author": "synth_mania",
          "text": "thinking models are created because they work. RL is used during post-training to train it to think in a way that increases performance.",
          "score": 29,
          "created_utc": "2026-02-03 21:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eqm1j",
          "author": "Wrapzii",
          "text": "Objectively for complex tasks itâ€™s much better. It should *if done correctly* ask the question you asked but in different ways resulting in slightly different responses which the model compacts into one response. It does the â€œwhat, how, whyâ€ to itself to improve responses. Obviously if you tell it â€œhiâ€ thats a stupid thought process. You shouldnâ€™t really be using thinking models unless you need them.",
          "score": 17,
          "created_utc": "2026-02-03 20:49:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eyywm",
          "author": "iam_maxinne",
          "text": "Look, \"thinking\" is the model generating more tokens to try (and often to succeed) to create more tokens about your prompt, so your desired output is more likely to be result. You exchange some tokens (and the time it takes to generate them) for precision.\n\nIf you are coding on a specific language, that have specific terms, jargoons, file formats, tools, and etc..., so adding them to the session may make the model find an answer more in line to what you need.\n\nI find thinking worse when doing generic work, as there is less data to it to \"think about\", while it improves when the scope is smaller and more specialized.",
          "score": 9,
          "created_utc": "2026-02-03 21:27:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3emb7s",
          "author": "msltoe",
          "text": "The smaller models thinking process does work for certain things, like solving puzzles, but it's generally much flimsier than a SotA model's thoughts.",
          "score": 5,
          "created_utc": "2026-02-03 20:28:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f6u4m",
          "author": "gamesta2",
          "text": "I have a great experience with devstral-small-2:24b even though its a coder, it does a fantastic job at tool use and general tasks. The thing with llm's, its not about paper performance, but your setup, prompt, and integration. For example, nemotron spends 1500 words to say good morning on the same setup and same prompt, but on paper, nemotron is supposed to be better and faster",
          "score": 6,
          "created_utc": "2026-02-03 22:04:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3q2xqn",
              "author": "Go_Fast_1993",
              "text": "I've found nemotron to be better than devstral at parameter extraction for tool calling. I only bring this up because I was specifically evaluating devstral-small against nemotron-3-nano this week. Response time is slower on nemotron, but that tradeoff works for my use case.",
              "score": 2,
              "created_utc": "2026-02-05 14:46:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3r0kxo",
                  "author": "gamesta2",
                  "text": "Yeah it all depends on setup. Another example for me is qwen3:30b model absolutely destroying gpt-oss. Everyone talks good about oss, but it just would not work for me. For now devstral performs the best",
                  "score": 1,
                  "created_utc": "2026-02-05 17:26:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o41sizy",
              "author": "tecneeq",
              "text": "That model has good paper performance. But you use it to make a point that it's not about paper performance.\n\nBold strategy. ;-)",
              "score": 1,
              "created_utc": "2026-02-07 08:20:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4447lg",
                  "author": "gamesta2",
                  "text": "At coding, or agentic tasks. Which yes, does suit tool calling - but not general chatbot tasks. Yet......... it does great overall. Not sure why but it does for me",
                  "score": 1,
                  "created_utc": "2026-02-07 17:42:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3etmc6",
          "author": "Savantskie1",
          "text": "In some tasks the thinking variants improve the model but the /no_think only works for Qwen models",
          "score": 4,
          "created_utc": "2026-02-03 21:02:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ezco4",
          "author": "funfun151",
          "text": "If youâ€™re saying â€œgood morningâ€ Iâ€™m guessing youâ€™re using it as a chatbot? Not really the use case for thinking models tbf, youâ€™d be better off with a high context standard model for that.",
          "score": 4,
          "created_utc": "2026-02-03 21:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gaj2h",
          "author": "eleqtriq",
          "text": "You want my thoughts on thinking?  Iâ€™d rather get straight to the answer.",
          "score": 3,
          "created_utc": "2026-02-04 01:36:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41sn1s",
              "author": "tecneeq",
              "text": "https://preview.redd.it/2ln3z9o681ig1.jpeg?width=754&format=pjpg&auto=webp&s=97a3bace9e8822a81e3e27ac1207f6b4afcb59c5\n\n",
              "score": 1,
              "created_utc": "2026-02-07 08:21:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o43eu1q",
                  "author": "Zealousideal-Pop-793",
                  "text": "Phenomenal ðŸ˜‰ðŸ‘‰",
                  "score": 1,
                  "created_utc": "2026-02-07 15:37:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3f0kuo",
          "author": "sinan_online",
          "text": "Itâ€™s not that they are â€œthinkingâ€, itâ€™s more that they are generating language that is a chin of thought. Since LLMs can generated human language, a straightforward application is to create one that is trained towards a chain-of-thought format.\n\nIt does look like it is wasted GPU hours and energy, I am not sure if measurable outcomes are much different.\n\nIâ€™d have to see something concrete. Claude 4.5 is good at programming, but I am not sure how much of that is related to how itâ€™s trained vs how it is implemented.",
          "score": 1,
          "created_utc": "2026-02-03 21:35:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fckmo",
              "author": "d_the_great",
              "text": "It make the model, well, model a solution to the user's problems better, resulting in a better answer. The only problem is small models don't have the ability to decide when or when not to enter thinking mode, which means they just default to thinking every time even when it's a waste of time and resources.",
              "score": 1,
              "created_utc": "2026-02-03 22:32:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3fcvfh",
          "author": "AmphibianFrog",
          "text": "Regarding point 1, I do agree it's kind of \"fake\" in that the chain of thought is not necessarily representative of what's going on in the network or how it came to a conclusion, but there's more to it than that. Because the chain of thought tokens get fed back in for subsequent token generation, it does provide the model with extra processing and those tokens do influence the final output. \n\nPersonally I normally pick instruct models or use whatever flag (i.e. /nothink) turns the thinking off. I'm not sure the benefits outweigh the extra processing time and energy for most local models. Often I just want it to be faster.\n\nIt can give a bit of extra insight too if you read the chain of thought but you have to take it with a pinch of salt!",
          "score": 1,
          "created_utc": "2026-02-03 22:33:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fdx7q",
          "author": "Frogy_mcfrogyface",
          "text": "It works better for some stuff, but ive noticed that if I ask the model something like \"Take this list and in front of each line add xx with a space before each line\" it will think about it, and it its thought process will actually create the list exactly how I want it, but then overthink it and its result would be something different.",
          "score": 1,
          "created_utc": "2026-02-03 22:38:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fnjus",
          "author": "duplicati83",
          "text": "Thereâ€™s a huge difference between my qwen3 30b thinking vs non thinking models. Itâ€™s the difference between right and wrong answers.",
          "score": 1,
          "created_utc": "2026-02-03 23:29:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fzb5d",
          "author": "JLeonsarmiento",
          "text": "I donâ€™t like â€œthinkingâ€ models either.  Hybrid or those with budget thinking, behaving like â€œinstructâ€ or â€œreasoningâ€ based on the complexity of the prompt are better.",
          "score": 1,
          "created_utc": "2026-02-04 00:33:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g0zu2",
          "author": "Medium_Ordinary_2727",
          "text": "Chain-of-thought reasoning definitely improves the results I get on certain types of questions.\n\nExample: until recently most LLMs couldn't correctly answer \"How many R's are in the word strawberry?\" But those same LLMs, prompted with my homemade janky chain-of-thought system instructions, would get the answer correct.\n\nI agree it's annoying to see the \"thinking\" process especially if you are just asking a quick question that doesn't require thought. Some frontends like OpenWebUI do have an [option to disable thinking mode](https://docs.openwebui.com/features/chat-features/reasoning-models/#think-ollama).",
          "score": 1,
          "created_utc": "2026-02-04 00:43:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g1gyl",
          "author": "sultan_papagani",
          "text": "yes its bad. gpt oss 20b creates fake policies while its \"thinking\" and cant even play a number guessing game.",
          "score": 1,
          "created_utc": "2026-02-04 00:45:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g6cl2",
          "author": "triynizzles1",
          "text": "Itâ€™s good for long prompts where youâ€™re asking the LLM to complete multiple tasks. The thinking tokens allow the model to work out the intricacies of what you are asking. Simple prompts like â€œhelloâ€ and â€œsummarize this:â€ isnt where these models excel. If the use cases correct, itâ€™s great!",
          "score": 1,
          "created_utc": "2026-02-04 01:12:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g80tt",
          "author": "sn0n",
          "text": "Ok, so the OP said thinking, so I should call <tool>â€¦.",
          "score": 1,
          "created_utc": "2026-02-04 01:22:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gkplk",
          "author": "fasti-au",
          "text": "One shots work in tokens in.  Reasoners work in token in and review which implies it has enough tokens to work with to one shot and the also to reason. \n\nIf one shotting is already there in tool calling then you are not really needing a reasoner but reasoners needed one shots to work to not think about code breaks and to think about it as a concept or design flaw.   Error then  fix error if no error look at results â€¦. Nope ðŸ‘Ž fixed error thatâ€™s what think decided was the task and you retry.     Think isnâ€™t your choice of workflow really.  If think is about why itâ€™s broken not how the task is broken in context then your creation token burn on retries",
          "score": 1,
          "created_utc": "2026-02-04 02:34:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hg736",
          "author": "DedsPhil",
          "text": "It's not fake, thinking tokens were created because in a lot of scenarios they help steering the model for a better answer without polluting the contex.\n\nThey were made because people figured that asking the model to \"think\" step by step made the answer better.",
          "score": 1,
          "created_utc": "2026-02-04 05:58:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hh41c",
          "author": "NoxinDev",
          "text": "People still believe that's anything other than the first tier of output for your input tokens, anything more than a performance?",
          "score": 1,
          "created_utc": "2026-02-04 06:06:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hrwb5",
          "author": "Ledeste",
          "text": "1) It's not \"fake,\" but it's clearly branding. But that's how words work. For example, words do not actually work, but when you read \"words work,\" you did not think it was fake.\n\n2) It's a way to improve the result; it's not made to be cool, it's made to make the response better. If it's taking too long, it's mostly because the LLM was not tuned enough and because local tools are basically non-existent (just forwarding user requests to an LLM with minimal wrapping). \n\nYou should learn a bit about how LLMs work; you'll quickly understand why these steps are becoming important.",
          "score": 1,
          "created_utc": "2026-02-04 07:38:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hxr5d",
          "author": "podgorniy",
          "text": "Before thinking was a thing one way to improve quality of LLM reply was to actually ask it to \"make a plan\" or \"think about the problem\" and only then get the final result. Idea of thinking is that this extra step should improve the output of the LLM. At least according to the benchmarks.",
          "score": 1,
          "created_utc": "2026-02-04 08:32:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ik5g4",
          "author": "StaysAwakeAllWeek",
          "text": "Beyond just the intelligence, performance, accuracy, etc benefits, thinking mode MASSIVELY reduces the hallucination rate because the model has ample chance to spot its own hallucinations and correct them\n\nIf you're ever having an LLM output content that cant/won't be fully manually verified it should be with thinking enabled. Always.",
          "score": 1,
          "created_utc": "2026-02-04 11:53:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3isteu",
          "author": "crone66",
          "text": "They're usage should be situational. E.g. in if this than that scenarios it might be able to solve things that are not directly in the training data e.g. simple math tasks (without using tools!).\n\n\nExample if the training data contains\n2+2=4 but not 20+20 it might reason ton calculate 2+2 and simply add a zero. Therefore it could solve it without having it in the trainingdata and without tool use.\n\n\nA non thinking model couldn't do that easily without explicitly prompting for calculation strategies first.\n\n\nThe down side is that think is in many scenarios completely useless and just a waste of time. Therefore, most providers automatically try to select the model based on the complexity of your prompt.\n\n\nTLDR: Use thinking models only when it makes sense because it has negative value for easy tasks/prompts.",
          "score": 1,
          "created_utc": "2026-02-04 12:53:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j3lc0",
          "author": "MrMakerWeTrust",
          "text": "In my Opinion LLMs have now reached a point of larger thinking memory. I donâ€™t think itâ€™s natural thinking I believe they use numbers to determine the quality of the response",
          "score": 1,
          "created_utc": "2026-02-04 13:55:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3r97y3",
          "author": "surfmaths",
          "text": "It is fake, and it wastes tokens/compute, but also... it works?! In the sense that the final answer is correct more often. \n\nI agree it's frustrating but the entirety of AI is made of fake layers that improve slightly and that all combined give something acceptable.",
          "score": 1,
          "created_utc": "2026-02-05 18:06:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rk68j",
          "author": "Jshdgensosnsiwbz",
          "text": "sure , you want to get into this....",
          "score": 1,
          "created_utc": "2026-02-05 18:56:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ufnea",
          "author": "RecalcitrantZak",
          "text": "After the anthropic research on model traces it was fairly obvious that most thinking models are just writing their own narrative fan fiction with nothing to do with how they are generating data. \n\nJust another way to burn tokens.",
          "score": 1,
          "created_utc": "2026-02-06 04:12:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o424h13",
          "author": "TedditBlatherflag",
          "text": "I think you have a fundamental misunderstanding of what â€œthinkingâ€ is doingâ€¦ these are effectively sub-agent patterns allowing the LLM to evaluate and summarize more without introducing context rot.Â ",
          "score": 1,
          "created_utc": "2026-02-07 10:18:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ehf0r",
          "author": "Dubious-Decisions",
          "text": "They are designed to encourage continued human engagement to drive token usage/sales. A \"thinking\" model that is free/open is just annoying because it's just wasting CPU resources and not even driving revenue for its creator. In all cases, they are a waste unless you are in it for the entertainment value alone.",
          "score": 0,
          "created_utc": "2026-02-03 20:05:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3elmts",
          "author": "Copakela",
          "text": "Not a fan. First model I used was one of the qwen family and I didnâ€™t know how to end the conversation after a few turns so I typed â€œexitâ€. Cue 30 seconds of unnecessary paranoia from the model wondering what it did wrong.",
          "score": 1,
          "created_utc": "2026-02-03 20:25:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3et9pw",
              "author": "Savantskie1",
              "text": "Didnâ€™t happen, if you type exit in cli it doesnâ€™t get sent to the model",
              "score": 2,
              "created_utc": "2026-02-03 21:01:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3eyaez",
                  "author": "Copakela",
                  "text": "Maybe it was quit or something like that. It was a good while ago. It wasnâ€™t /bye anyway, I only found that out after.",
                  "score": 1,
                  "created_utc": "2026-02-03 21:24:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3w0nz1",
              "author": "TeachNo196",
              "text": "I did something like that once when I first started trying out local llms.  Made me chuckle.",
              "score": 1,
              "created_utc": "2026-02-06 12:14:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3g8k5n",
          "author": "sn0n",
          "text": "Instant gratification entitlement manâ€¦ I tell yaâ€¦ back in my day we had to get in the car, goto the library, hunt down the librarian, then hunt down the right bookâ€¦. And spend hours pouring through pagesâ€¦. And you bitch about letting your computer do it for you slowly while you carry about other tasks? Get outta here",
          "score": -1,
          "created_utc": "2026-02-04 01:25:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3entor",
          "author": "BidWestern1056",
          "text": "can't really stand them, and build a lot of products that don't really emphasize them:\n\n[https://github.com/npc-worldwide/npcpy](https://github.com/npc-worldwide/npcpy)\n\n[https://github.com/npc-worldwide/incognide](https://github.com/npc-worldwide/incognide)\n\n[https://github.com/npc-worldwide/npcsh](https://github.com/npc-worldwide/npcsh)",
          "score": -1,
          "created_utc": "2026-02-03 20:36:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzh0op",
      "title": "I created a small AI Agent",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/",
      "author": "Rough_Philosopher877",
      "created_utc": "2026-02-08 18:43:20",
      "score": 29,
      "num_comments": 10,
      "upvote_ratio": 0.92,
      "text": "Hi guys.. I know it's not so big thing.. just for fun I created a Small AI Agent:\n\n[https://github.com/tysonchamp/Small-AI-Agent](https://github.com/tysonchamp/Small-AI-Agent)\n\nWould love the feedback of the community.. and any suggestions of new ideas.\n\nI created this for my day to day activities.. such as setup reminders, take notes, monitor all my client's website (if they are all ok or not).. monitor all my servers, connecting it to my custom erp for due invoice fetching, project management etc ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4ap7g5",
          "author": "RA2B_DIN",
          "text": "Sounds really nice, Iâ€™ll try it",
          "score": 1,
          "created_utc": "2026-02-08 18:47:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4arhru",
              "author": "Rough_Philosopher877",
              "text": "thanks mate.. let me know your feedback please..",
              "score": 1,
              "created_utc": "2026-02-08 18:58:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4cdwvf",
          "author": "Electronic_Fox594",
          "text": "I made one too but Iâ€™m not a real programmer so I wonâ€™t share it but very similar.",
          "score": 1,
          "created_utc": "2026-02-09 00:02:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4cpn6h",
          "author": "Civil_Tea_3250",
          "text": "Love it! I created something like this myself using Ollama, python scripts, n8n, .md files and all that. I've been adding to it when I get time and learn something new. I'll check it out when I have time. Would love to see what you do similar/different.\n\nI had the same idea. I hate all the AI down our throats and find most of it frustrating. Having something that does my regular tasks and is only focused on my home and server makes it much more trustworthy and useful.",
          "score": 1,
          "created_utc": "2026-02-09 01:07:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4dakcc",
              "author": "Rough_Philosopher877",
              "text": "I started with only website monitoring.. and now Iâ€™ve added four five skills.. thinking to add few more like notification sending to me and my team members based on pending task as reminder.. automated chat replies.. sent emails to my clients etc..",
              "score": 1,
              "created_utc": "2026-02-09 03:00:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4e75l0",
          "author": "Zyj",
          "text": "Telegram is not end-to-end encrypted most of the time",
          "score": 1,
          "created_utc": "2026-02-09 06:48:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4atim6",
          "author": "Acrobatic_Task_6573",
          "text": "This is cool. The fact that you built it around your actual daily workflow instead of making it generic is the right approach. Most AI agent projects try to do everything and end up doing nothing well.\n\nThe website monitoring and ERP integration pieces are especially interesting. Those are real problems that most people solve with 3 or 4 different SaaS tools. Having one agent that handles all of that is clean.\n\nA few questions/suggestions if you keep building on it:\n\n- How does it handle failures? Like if a website check times out, does it retry or just flag it?\n- For the reminder system, does it persist across restarts? That was one of the first things I had to solve with my own setup.\n- Have you thought about adding a simple web dashboard to see all your monitors at a glance?\n\nNice work for a personal project. The best tools are the ones built to scratch your own itch.",
          "score": 1,
          "created_utc": "2026-02-08 19:07:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d96mj",
              "author": "Rough_Philosopher877",
              "text": "Thanks for coming.. Right now it just send a notification to my telegram bot.. and i operate it via telegram bot only..\nIâ€™m using sqlite db to store everything..\nAbout the web didnâ€™t thought about it.. but I needed a way to see the db easy way..",
              "score": 1,
              "created_utc": "2026-02-09 02:53:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ayt2a",
          "author": "mosaad_gaber",
          "text": "I tried to clone and install and face this \ngit clone https://github.com/yourusername/ai-assistant-bot.git\ncd ai-assistant-bot\n\n# Create Virtual Environment\npython3 -m venv venv\nsource venv/bin/activate\n\n# Install Dependencies\npip install -r requirements.txt\nThe program git is not installed. Install it by executing:\n pkg install git\nbash: cd: ai-assistant-bot: No such file or directory\n\n[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n[notice] To update, run: pip install --upgrade pip\nERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'",
          "score": -2,
          "created_utc": "2026-02-08 19:33:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4czk7c",
              "author": "inteblio",
              "text": "Chat gpt",
              "score": 1,
              "created_utc": "2026-02-09 02:03:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qtqp9p",
      "title": "175k+ publicly exposed Ollama servers, so I built a tool",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/gallery/1qtqobb",
      "author": "truthfly",
      "created_utc": "2026-02-02 09:41:37",
      "score": 21,
      "num_comments": 9,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qtqp9p/175k_publicly_exposed_ollama_servers_so_i_built_a/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o34ugp2",
          "author": "HyperWinX",
          "text": "\"<literally any statement> so I built\" my ass",
          "score": 7,
          "created_utc": "2026-02-02 10:40:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ibiv7",
              "author": "ScrapEngineer_",
              "text": "It's vibe coded, and it's bad lol",
              "score": 1,
              "created_utc": "2026-02-04 10:41:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ihz3i",
                  "author": "truthfly",
                  "text": "Humm. No... Coded by hand.. readme was generated by LLM yeah but I coded each module, the orchestrator was made by LLM yeah but not the rest. Maybe it's bad, but it works. It's not at a scraper engineer level ðŸ¤—",
                  "score": 1,
                  "created_utc": "2026-02-04 11:36:55",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o35crn9",
              "author": "truthfly",
              "text": "Yeah, bad title maybe. I get the skepticism. Maybe the form is not good.. \n\nBut it's not a hype post. just releasing a tool I was already using once the article went public.",
              "score": -1,
              "created_utc": "2026-02-02 13:03:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o36xye5",
          "author": "sinan_online",
          "text": "Is it that interesting? I run stateless servers on the cloud from time to time, they are completely ephemeral.",
          "score": 2,
          "created_utc": "2026-02-02 17:49:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3cwqt1",
              "author": "truthfly",
              "text": "It highly depends on your plan, infrastructure and billing, but it could be a big issue with custom compagny model for example",
              "score": 3,
              "created_utc": "2026-02-03 15:44:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ib9vz",
          "author": "Adventurous-Hunter98",
          "text": "I dont understand, does running ollama expose my ip or something ?",
          "score": 1,
          "created_utc": "2026-02-04 10:39:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ibp9h",
              "author": "HyperWinX",
              "text": "Running ollama - no. Exposing API endpoint to public - yea. Bots will find that endpoint pretty quickly.",
              "score": 3,
              "created_utc": "2026-02-04 10:43:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o43ic88",
          "author": "desexmachina",
          "text": "Are these exposed API available for inference? I know, dumb question",
          "score": 1,
          "created_utc": "2026-02-07 15:55:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qv346v",
      "title": "Recommandation for a power and cost efficient local llm system",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qv346v/recommandation_for_a_power_and_cost_efficient/",
      "author": "Grummel78",
      "created_utc": "2026-02-03 20:23:53",
      "score": 18,
      "num_comments": 16,
      "upvote_ratio": 1.0,
      "text": "Hello everybody,\n\ni am looking power and cost efficient local llm system. especially when it is in idle. But i don't want wait minutes for reaction :-) ok ok i know that can not have everything :-)\n\nUse cases are the following:\n\n1. Using AI for Paperless-NGX (setting tags and ocr)\n\n2. Voice Assistant and automation in Home Assistant.\n\n3. Eventual Clawdbot\n\nAt the moment i tried Ai with the following setup:\n\nasrock n100m + RTX 3060 +32 GB Ram.\n\n  \nBut it use about 35 Watts in idle. I live in Germany with high energy cost. And for an 24/7 system it is too much for me. especially it will not be used every day. Paperless eventually every third day.  Voice Assistant and automation in Home Assistant 10-15 times per day.\n\nClawdbot i don't know.\n\nImportant for me is data stays at home (especially Paperless data).\n\nKnow i am thinking about a mac mini m4 base edition (16 gig unified ram and 256 ssd)\n\nHave somebody recommandations or experience with a mac mini and my use cases ?\n\nBest regards\n\nDirk\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qv346v/recommandation_for_a_power_and_cost_efficient/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3fdht4",
          "author": "prene1",
          "text": "Mac has the LLM low power in a chokehold",
          "score": 8,
          "created_utc": "2026-02-03 22:36:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fqo73",
          "author": "overand",
          "text": "In Germany, you probably pay \\~â‚¬0.40 per kilowatt hour. 35 watts, 24 hours a day , 30 days is a total of 25 kw/h.\n\nDo not spend â‚¬500 on a computer to lower your monthly electricity bill from â‚¬10 to â‚¬2.50. (Even if it cut your electricity bill to ZERO, it would take over 4 years to make back the cost. Realistically, at \\~8 watts vs \\~35 watts, you're saving, it's closer to 5 1/2 years)\n\nYou're spending less than â‚¬125 a *year* in electricity leaving your current setup on 24/7, if it's really 35 watts, and you're really at â‚¬0.40/kwh",
          "score": 6,
          "created_utc": "2026-02-03 23:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fvjzk",
          "author": "TheAussieWatchGuy",
          "text": "Wrong time ðŸ˜€ RAM prices have gone to the moon because of AI.\n\n\nThe most RAM and VRAM you can get is the answer. Mac M4 and Ryzen AI 395 both offer unified memory, so if you get 128gb of ddr5 ram you can allocate 112gb of it to the built in GPU.Â \n\n\nThat's pretty much the most cost effective way to run bigger models. 96gb in a pinch can be ok.Â \n\n\nOtherwise you can double down and get 64gb of ddr4 and a couple of 4090s, and a new PSU for your current PC, you'll spend more but Nvidia is still a bit easier to get local working well on.\n\n\nEither way you're spending a few grand.Â ",
          "score": 3,
          "created_utc": "2026-02-04 00:13:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3gkndk",
              "author": "ggone20",
              "text": "Not all unified memory is equal. \n\nOnly the Mac has true unified memory, all other systems just use it as marketing jargon with values needing to be set ahead of time performance hits for â€˜autoâ€™. \n\nSaid another way - Macs truly share the exact same memory, all other systems split a pool of memory.",
              "score": 2,
              "created_utc": "2026-02-04 02:33:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3h3ln9",
                  "author": "TheAussieWatchGuy",
                  "text": "Technically true. Mac is king here but with a price tag to match.\n\n\nFor the OP if budget is a concern the Ryzen AI platform is cost effective and fast enough.Â ",
                  "score": 1,
                  "created_utc": "2026-02-04 04:27:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3hcb56",
                  "author": "Zyj",
                  "text": "Have you looked at GTT memory on AMD AI Max+? I guess not.",
                  "score": 1,
                  "created_utc": "2026-02-04 05:29:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ywfkz",
                  "author": "GeroldM972",
                  "text": "What makes certain Macs better is the memory bandwidth with which the unified memory modules inside those chips can communicate. \n\nRegarding the assignment of RAM/VRAM in BIOS/UEFI on PC processors with Unified memory, I like that. That way I am sure my operating system can't overwrite something accidentally in the loaded LLM or the other way around.\n\nIf I had the money, the M3 Mac Studio from 10.000 USD (512 GB unified RAM) is the only true interesting Mac I would want to use for local LLMs. All the other M3 Mac Studios with less RAM have too much of a wrong price/performance ratio to me.\n\nThen I rather spend (way less) on those AMD Strix Halo processors with their Unified RAM.",
                  "score": 1,
                  "created_utc": "2026-02-06 21:00:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3gft6u",
          "author": "Royale_AJS",
          "text": "Youâ€™re looking for an AMD Strix Halo, a Mac, or a DGX Spark. The Mac is super efficient, my Strix Halo idles at 9 watts and I canâ€™t get it to pull more than 140 out of the wall.",
          "score": 2,
          "created_utc": "2026-02-04 02:06:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eymn9",
          "author": "GlassAd7618",
          "text": "Any specific model you have in mind for Paperless-ngx and voice automation? What runtime are you thinking about (e.g., Ollama or lmstudio)? I could run some tests on my Mac mini. (I'm in Germany too, so it would be interesting to see how smooth selected models run on a 16GB Mac mini to avoid a hefty electricity bill...)",
          "score": 1,
          "created_utc": "2026-02-03 21:26:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gnhi0",
          "author": "ServeAlone7622",
          "text": "LFM 2.5 seems optimized for this use case.",
          "score": 1,
          "created_utc": "2026-02-04 02:49:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hhpcx",
          "author": "gamesta2",
          "text": "4nm. Ryzen. my 9700x is pretty much identical to 13600k in performance, but draws half the power.",
          "score": 1,
          "created_utc": "2026-02-04 06:10:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxrl4o",
      "title": "Qwen3-ASR Swift: On-Device Speech Recognition for Apple Silicon",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qxrl4o/qwen3asr_swift_ondevice_speech_recognition_for/",
      "author": "ivan_digital",
      "created_utc": "2026-02-06 19:38:03",
      "score": 16,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I'm excited to releaseÂ [https://github.com/ivan-digital/qwen3-asr-swift](https://github.com/ivan-digital/qwen3-asr-swift), an open-source Swift implementation of Alibaba'sÂ   \nQwen3-ASR, optimized for Apple Silicon using MLX.Â \n\nWhy Qwen3-ASR? Exceptional noise robustness â€” 3.5x better than Whisper in noisy conditions (17.9% vs 63% CER).Â \n\nFeatures:Â   \n\\- 52 languages (30 major + 22 Chinese dialects)Â   \n\\- \\~600MB model (4-bit quantized)Â   \n\\- \\~100ms latency on M-series chipsÂ   \n\\- Fully local, no cloud APIÂ \n\n[https://github.com/ivan-digital/qwen3-asr-swift](https://github.com/ivan-digital/qwen3-asr-swift)Â | Apache 2.0",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qxrl4o/qwen3asr_swift_ondevice_speech_recognition_for/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qwwkip",
      "title": "Built a self-hosted execution control layer for local LLM workflows (works with Ollama)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qwwkip/built_a_selfhosted_execution_control_layer_for/",
      "author": "saurabhjain1592",
      "created_utc": "2026-02-05 20:28:01",
      "score": 14,
      "num_comments": 0,
      "upvote_ratio": 0.94,
      "text": "Hey folks. I am building AxonFlow, a self-hosted, source-available execution control layer for local LLM workflows once they move beyond single prompts and touch real systems.\n\nThe hard part was not model quality. It was making execution visible and controllable:\n\n* clear boundaries around what steps are allowed to run\n* logs tied to decisions and actions, not just model outputs\n* the ability to inspect and replay a run when something goes wrong\n\nRetries and partial failures still mattered, but only after we could see and control what happened in a run.\n\nAxonFlow sits inline between your workflow logic and LLM tool calls to make execution explicit. It is not an agent framework or UI platform. It is the runtime layer teams end up building underneath once local workflows get serious.\n\nWorks with Ollama by pointing the client to a local endpoint.  \nGitHub:Â [https://github.com/getaxonflow/axonflow](https://github.com/getaxonflow/axonflow)\n\nWould love feedback from folks running Ollama in real workflows.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qwwkip/built_a_selfhosted_execution_control_layer_for/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qzrd8g",
      "title": "DaveLovable is an open-source, AI-powered web UI/UX development platform",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/",
      "author": "LeadingFun1849",
      "created_utc": "2026-02-09 01:54:32",
      "score": 14,
      "num_comments": 0,
      "upvote_ratio": 0.99,
      "text": "I've been working on this project for a while.\n\nDaveLovable is an open-source, AI-powered web UI/UX development platform, inspired by Lovable, Vercel v0, and Google's Stitch. It combines cutting-edge AI orchestration with browser-based execution to offer the most advanced open-source alternative for rapid frontend prototyping.\n\nGithub :Â [https://github.com/davidmonterocrespo24/DaveLovable](https://github.com/davidmonterocrespo24/DaveLovable)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qvnpp7",
      "title": "Using Ollama for a real-time desktop assistant â€” latency vs usability tradeoffs?",
      "subreddit": "ollama",
      "url": "https://v.redd.it/n32j33l45hhg1",
      "author": "Ore_waa_luffy",
      "created_utc": "2026-02-04 12:50:21",
      "score": 13,
      "num_comments": 6,
      "upvote_ratio": 0.78,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qvnpp7/using_ollama_for_a_realtime_desktop_assistant/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3lsq4u",
          "author": "TheAndyGeorge",
          "text": "why you promoting [interview cheating](https://www.reddit.com/r/OpenAI/comments/1qvnrht/cheat_interviews_easily/) ?",
          "score": 3,
          "created_utc": "2026-02-04 21:34:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rupbm",
              "author": "Ore_waa_luffy",
              "text": "Im not promoting it , there are literally tools that people pay 30 dollars or so , I'm just making it free",
              "score": 1,
              "created_utc": "2026-02-05 19:45:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3o3enm",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-05 05:28:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3o88j3",
              "author": "Ore_waa_luffy",
              "text": "Oh Lemme try that . What i did was i used whisper small for ui transcription so the user can feel itâ€™s being transcribed and use whisper medium for actual transcription but the latency was like few seconds which i didnâ€™t like personally",
              "score": 1,
              "created_utc": "2026-02-05 06:06:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3khmh8",
          "author": "amgir1",
          "text": "I have 5070 ti, and my olama have been typing realy slow. It produces a lot of text but I don't trust it, too many words, too many time to type answer",
          "score": 0,
          "created_utc": "2026-02-04 17:54:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kln97",
              "author": "Ore_waa_luffy",
              "text": "Yes in my app i have given preference to google stt if anyone cant pay then can use dual pipeline  whisper quick and medium",
              "score": 0,
              "created_utc": "2026-02-04 18:13:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qyrotc",
      "title": "Releasing 1.22. 0 of Nanocoder - an update breakdown ðŸ”¥",
      "subreddit": "ollama",
      "url": "https://v.redd.it/t790s2gjg5ig1",
      "author": "willlamerton",
      "created_utc": "2026-02-07 22:37:49",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qyrotc/releasing_122_0_of_nanocoder_an_update_breakdown/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o47mn0y",
          "author": "drakgremlin",
          "text": "Does this support local models?Â  If so, what models do you recommend?",
          "score": 2,
          "created_utc": "2026-02-08 06:21:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49egew",
              "author": "willlamerton",
              "text": "Absolutely, it supports local models as a priority and thereâ€™s a big focus on improving the scaffolding around small models to make them better!\n\nWeâ€™re getting there but at the moment a big recommendation comes with the Mistral models. If you can run Devstral Small 2 then thatâ€™s great for 24B parameters. As is Nemotron Nano from Nvidia. Iâ€™m also a fan of the 8B and 14B flavours of the Ministral models. Set your expectations but they can certainly help with smaller coding tasks and codebase exploration.\n\nThey all work great through Ollama local and cloud :)",
              "score": 2,
              "created_utc": "2026-02-08 15:00:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4aqmjf",
                  "author": "drakgremlin",
                  "text": "Took it out for a whirl with mistral-3:3b !Â  Looks promising but stops every step to ask for confirmation.Â  Is this a feature or would this be alleviated by using 8b ?",
                  "score": 1,
                  "created_utc": "2026-02-08 18:54:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qv5cgs",
      "title": "AI Context as Code: Can structured docs improve AI resource usage and performance?",
      "subreddit": "ollama",
      "url": "https://github.com/eFAILution/AICaC",
      "author": "eFAILution",
      "created_utc": "2026-02-03 21:46:51",
      "score": 6,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qv5cgs/ai_context_as_code_can_structured_docs_improve_ai/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3urmlm",
          "author": "GlassAd7618",
          "text": "I think youâ€™re on to something. Thinking about the example in your Git repository (adding a new scanner), your idea probably works best for mature code bases which already have an established architecture and where there is some functionality that is repeatedly extended, such as adding a new scanner, filter, reader, â€¦",
          "score": 2,
          "created_utc": "2026-02-06 05:38:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w5yej",
              "author": "eFAILution",
              "text": "Totally agree! Also, donâ€™t sleep on what it can do for onboarding. Instead of new devs asking senior engineers or reading thousands of lines, their AI assistant already knows â€œhow we do things here.â€\nStructured context = their coding AI tool actually understands your conventions from day one.",
              "score": 2,
              "created_utc": "2026-02-06 12:50:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3np2b0",
          "author": "_RemyLeBeau_",
          "text": "You don't need AI, when you have a specification. You need a workflow engine.\n\nAI is great because it understands NLQ.",
          "score": 1,
          "created_utc": "2026-02-05 03:50:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nqoro",
              "author": "eFAILution",
              "text": "Good point. AICaC isnâ€™t for task execution as workflow engines are better there. Itâ€™s for reducing token waste when AI needs to understand your codebase before helping.\nThe NLQ strength of AI is why it needs efficient context. â€œHow do I add auth?â€ shouldnâ€™t cost 800 tokens parsing a README when 200 tokens of structured data would work.\nWhatâ€™s your approach for giving AI project context efficiently?â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹",
              "score": 1,
              "created_utc": "2026-02-05 04:01:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3o99lx",
                  "author": "_RemyLeBeau_",
                  "text": "Exactly what vercel is doing and has 100% evals. Do you have evals to share with your newly created DSL?",
                  "score": 1,
                  "created_utc": "2026-02-05 06:15:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qv4312",
      "title": "Ollama and Openclaw on separate dedicated, isolated, firewalled machines",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qv4312/ollama_and_openclaw_on_separate_dedicated/",
      "author": "timbo2m",
      "created_utc": "2026-02-03 21:00:04",
      "score": 6,
      "num_comments": 7,
      "upvote_ratio": 0.67,
      "text": "Anyone get this to work? \n\nMachine 1 is a dedicated Openclaw Mac laptop with outbound internet access.\n\nMachine 2 is a dedicated ollama server sharing various models.\n\nBoth machines are on the same subnet. A quick check shows Openclaw machine can see the models list on the ollama server. \n\nOnce Openclaw has been through onboarding it does not respond to any chat requests. I think maybe this could work with some extra messing around v\n\nSo while it should work, the test responses are all empty as if it's Openclaw can't communicate properly with ollama. \n\nEDIT: I found this https://github.com/openclaw/openclaw/issues/2838 so will see if some of those comments help",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qv4312/ollama_and_openclaw_on_separate_dedicated/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3ly4df",
          "author": "robonova-1",
          "text": "Yes, I have that working. If you are trying to use a local LLM as the main brain for OpenClaw you will not be happy because they simply don't have the advanced reasoning capabilities of the frontier models. You need to have the main brain using a large online model and then use a local LLM as a subagent doing grunt work and that keeps the costs down. There are TONS of YouTube videos floating around right now that show how to do this.",
          "score": 3,
          "created_utc": "2026-02-04 21:59:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lyx7k",
              "author": "timbo2m",
              "text": "Yeah the intent is to have opus as the brain to decide what to work on, but it delegates work to my local models.  I am using various local models and have qwen 3 next working at ~32 tokens per second with llama.cpp via llama-server.  The local coders will be checked by pr-agent hooked up to Kimi 2.5 and iterated before any code is committed.",
              "score": 2,
              "created_utc": "2026-02-04 22:03:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ifcxs",
          "author": "timbo2m",
          "text": "Ok so I dumped ollama for llama.cpp and it's working",
          "score": 2,
          "created_utc": "2026-02-04 11:15:07",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o3iq2o0",
          "author": "nosimsol",
          "text": "I couldnâ€™t even get open claw at work on the same machine with ollama. The set up process seem to want to force me to use an external provider which I didnâ€™t want to set up and I didnâ€™t see a way around it, and it just seemed to create problems.",
          "score": 2,
          "created_utc": "2026-02-04 12:36:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ja5ok",
              "author": "SaltyUncleMike",
              "text": "its a huge PITA.  I was able to get it work by starting clawdbot through ollama itself.\n\neg:\nConfig Only: Use ollama launch clawdbot --config if you want to set up the connection without starting the service immediately.",
              "score": 1,
              "created_utc": "2026-02-04 14:30:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lnurr",
          "author": "reddefcode",
          "text": "Given the GitHub issue you found, I'd recommend double-checking the Ollama server's API configuration and ensuring OpenClaw is pointing to the correct endpoint. Sometimes, firewall rules or app permissions on the Mac might be blocking the actual chat requests, even if listing works.",
          "score": 1,
          "created_utc": "2026-02-04 21:11:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4adqkx",
          "author": "Acrobatic_Task_6573",
          "text": "Yeah this works. Running a similar setup with a Mac mini for OpenClaw and Ollama on a separate Linux box.\n\nTwo things that tripped me up:\n\n1. Make sure you set OLLAMA_HOST=0.0.0.0 on the Ollama server. By default it only listens on localhost so even though you can see the models list, the actual inference calls might fail silently.\n\n2. In your openclaw.json, the Ollama provider URL needs to point to the full address including port. Something like http://192.168.1.x:11434. Do not forget the /v1 path if your config expects OpenAI compatible endpoints.\n\nThe empty responses thing is usually a context window issue. Smaller models choke on the system prompt if the context is too small. Try bumping it up in your modelfile or use a model that handles larger contexts natively like qwen2.5 14b.\n\nAlso worth checking: are you running the model with enough VRAM? If it is swapping to CPU mid inference, it can timeout and return empty.",
          "score": 1,
          "created_utc": "2026-02-08 17:54:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxvm16",
      "title": "Run Ollama on Legion 5.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qxvm16/run_ollama_on_legion_5/",
      "author": "iamoutofwords",
      "created_utc": "2026-02-06 22:11:40",
      "score": 5,
      "num_comments": 8,
      "upvote_ratio": 0.78,
      "text": "I want to run Ollama on Legion 5 and use Moltbot with it. Can it handle that?   \nSpecs are:  \n\\- 16gb RAM  \n\\- 512 GB SSD  \n\\- Ryzen 7 5800H 3.2GHz  \n\\- Rtx 3050 Ti 6GB",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qxvm16/run_ollama_on_legion_5/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3zpf2v",
          "author": "Fun_Librarian_7699",
          "text": "Yes it will work, but maybe very slow if you offload to RAM. Also be careful with moltbot (clawbot) bc you can give the agent full access to your computer",
          "score": 2,
          "created_utc": "2026-02-06 23:31:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zporh",
              "author": "iamoutofwords",
              "text": "iâ€™m using my old system just to test it out. any idea what it costs running on the cloud instead of locally?",
              "score": 1,
              "created_utc": "2026-02-06 23:32:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zq9hs",
                  "author": "Fun_Librarian_7699",
                  "text": "Depends on which provider you choose. Check the API prices per token on the website",
                  "score": 1,
                  "created_utc": "2026-02-06 23:36:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o40d00q",
          "author": "ZeroSkribe",
          "text": "I'm ollama's biggest support and you pushing it bro...lol, you should be able to get it working....very slowy..\n\nmaybe just use ollama for fun prompts,\n\nUse the free api key's from cerebras and groq for moltbot(you're welcome)",
          "score": 2,
          "created_utc": "2026-02-07 01:50:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43xtem",
              "author": "iamoutofwords",
              "text": "guess iâ€™ll just use cloud then. any idea on the average monthly price?",
              "score": 1,
              "created_utc": "2026-02-07 17:11:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43v8kf",
          "author": "Efficient_Loss_9928",
          "text": "It could run some small models, but would be slow and I can't imagine them be good at agentic tool use.",
          "score": 1,
          "created_utc": "2026-02-07 16:58:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4cfa3q",
          "author": "Key-Guitar4732",
          "text": "You can just use Ollama cloud too itâ€™s free for quite a bit",
          "score": 1,
          "created_utc": "2026-02-09 00:10:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zbv40",
          "author": "Hot-Cauliflower-1604",
          "text": "Yeah, your Legion 5 with the Ryzen 7 5800H, RTX 3050 Ti 6GB, and 16GB RAM can totally handle running **Ollama** + **Moltbot** locally. It's a solid mid-range gaming laptop from that era, and plenty of people run similar setups for local LLMs without issues.\n\nThe 3050 Ti (laptop variant, ~80-95W TGP) has decent CUDA support, so Ollama will offload to the GPU nicely. With 6GB VRAM, you're comfortably good for:\n\n- 7B-8B models (Llama 3.1 8B, Qwen 2.5 7B, Mistral/Nemo variants, Gemma 2 9B) at Q4_K_M or Q5 â€” these usually fit fully on GPU and spit out **25-40+ tokens/sec** on similar 3050 Ti laptops. That's snappy for chat/coding/everyday use.\n- You might squeeze in some 13B models at lower quants (Q3/Q4), but expect partial offload to system RAM â†’ slower (~10-20 t/s), still usable though.\n\n16GB system RAM is enough; Ollama can spill over if needed, and your Ryzen 7 is a beast for any CPU fallback or hybrid stuff.\n\nFor **Moltbot** (the open-source agent thing, ex-Clawdbot, that hooks into Ollama for tool-using/agent tasks like messaging, workflows, content gen, etc.) â€” yep, it works great with local Ollama. Just point it at `http://localhost:11434/v1` (OpenAI-compatible endpoint), no API key nonsense. People run this exact combo on laptops like yours for fully private, no-cloud agents. Start with a lighter 7-8B instruct model tuned for tool use (Qwen 2.5 or Llama 3.1 variants often recommended for agents) to keep response times reasonable during multi-step thinking.\n\nQuick tips from folks who've done it:\n- Update NVIDIA drivers (Studio or Game Ready, latest).\n- Plug in + crank Lenovo Vantage to Performance mode, good airflow â€” these get toasty under load.\n- If a model feels slow, drop to Q4 or try smaller ones first.\n- Test Ollama standalone (`ollama run llama3.1:8b`) before wiring up Moltbot.\n\nOverall, no, you didn't need a beefier rig â€” this setup will give you a capable private AI agent without paying per token. Way better than relying on cloud stuff. If you run into VRAM errors or whatever, just grab a lower quant or smaller model. Hit me up with logs if it acts up.\n\nGo for it, OP â€” you'll be golden. ðŸš€",
          "score": -9,
          "created_utc": "2026-02-06 22:17:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyqbpx",
      "title": "Advice for LLM choosing and configuration my setup",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qyqbpx/advice_for_llm_choosing_and_configuration_my_setup/",
      "author": "Particular-Idea805",
      "created_utc": "2026-02-07 21:41:47",
      "score": 5,
      "num_comments": 12,
      "upvote_ratio": 0.86,
      "text": "Hi guys,\n\nI am pretty new to the AI stuff. My wife uses gemini pro and thinking a lot, I sometimes use it for tutorials like setting up a proxmox host with some services like homeassistant, scrypted, jellyfin and so on...\n\n  \nI have a HP Z2 G9 with an Intel i9 and 96gb ram, rtx 4060 which I have installed proxmox and ollama on. Do you have some advice for a LLM model that fits for my setup? Is it possible to have a voice assistant like gemini?\n\n  \nThanks a lot for your help!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qyqbpx/advice_for_llm_choosing_and_configuration_my_setup/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o45s19s",
          "author": "timbo2m",
          "text": "First I would drop ollama for llama-cpp (llama-server) I found the output was way faster. Then I would try get qwen coder next 2 bit or 4 bit quant going, see https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF also look into nemotron, actually with that ram try gpt-oss 120B. Just load your spec into huggingface website and it will show you the models and the quantisation you can run",
          "score": 2,
          "created_utc": "2026-02-07 22:57:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o483k2n",
              "author": "Particular-Idea805",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-02-08 08:58:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o463eir",
          "author": "grudev",
          "text": "You can use this desktop app to compare several different models at once:\n\n\nhttps://github.com/dezoito/ollama-grid-search\n\n\nInstallers for all major platforms are in the \"Releases\" section.Â ",
          "score": 2,
          "created_utc": "2026-02-08 00:07:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o483jgq",
              "author": "Particular-Idea805",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-02-08 08:58:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o47vcm4",
          "author": "Odd-Aside456",
          "text": "Honestly, I just give my system specs to Gemini and Chatgpt and ask what the maximum parameters would be for running a model on my hardware, and start there. So, besides choosing models with specific capabilities, I know that i need to limit myself to an approximate max of 32b parameter models for my not very beefy system.\n\n\nI don't think ollama has voice assistant by default, every time I've set up a voice assistant I've either used Home Assistant connected to ollama and all the relevant plugins (like whisper) - Network Chuck has a great video for this - or manually set up a system with a TTS service (I personally pay for Elevenlabs) and STT (I self host whisper).",
          "score": 2,
          "created_utc": "2026-02-08 07:41:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o483n69",
              "author": "Particular-Idea805",
              "text": "Thank you!",
              "score": 2,
              "created_utc": "2026-02-08 08:59:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o45pv26",
          "author": "sn0n",
          "text": "Also, totally unrelated, you gotta watch out for those wives that are thinking a lotâ€¦. They are almost without a doubt up to no good!!!",
          "score": 1,
          "created_utc": "2026-02-07 22:44:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o483g64",
              "author": "Particular-Idea805",
              "text": "My wife uses it for recipes the most, so I think it is a good investment, don't you think too? ðŸ˜‰",
              "score": 1,
              "created_utc": "2026-02-08 08:57:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o483tgn",
                  "author": "sn0n",
                  "text": "I now outsource all my thinking, this timeline is entirely too effed up to make sense of any of it.",
                  "score": 1,
                  "created_utc": "2026-02-08 09:00:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o45pxdi",
          "author": "XxAnomo305",
          "text": "if your ready to tinker around and understand Decent stuff then yeah you could definitely make something work. for me I personally use python. I have input device - it will go from speech to text, send it to LLM with custom prompt + parse, then you can use a text to speech kokoro or pyper is decent in my opinion and then at the end just make python play the audio file. for my setup it takes around 15-20 seconds for all of that so I think it's pretty efficient in my opinion. I would recommend a 8b model of your choice each one has pros and cons, you have alot of ram but anything over 8gb vram (your GPU)it will slow down to a crawl making it very slow so wouldn't work for this. if you are just trying to test a large model then you could try 32b or above but for usable fast response 8b or under is your best bet.",
          "score": 1,
          "created_utc": "2026-02-07 22:45:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o483kwf",
              "author": "Particular-Idea805",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-02-08 08:58:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o45ppv8",
          "author": "sn0n",
          "text": "Write some tests, review results.",
          "score": -1,
          "created_utc": "2026-02-07 22:43:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzg2zw",
      "title": "What do we have for Ollama that works like Perplexityâ€™s Comet Browser?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qzg2zw/what_do_we_have_for_ollama_that_works_like/",
      "author": "Comfortable_Ad_8117",
      "created_utc": "2026-02-08 18:09:16",
      "score": 4,
      "num_comments": 4,
      "upvote_ratio": 0.76,
      "text": "Does something exist that will do a similar job as Perplexity Comet browser? Full interaction with web pages and tabs and run on my local Ollama server? I tried Ask Steve but could never get it to talk to my remote Ollama server (only an Ollama running on the same box as the plugin) I have a pair of 5060â€™s that are part of my dedicated Ai sever running Ollama and I want my workstation browser to be able to utilize the remote server. \n\nAnyone have a chrome plugin for this (other than Steve) or even an entire chromium browser like Comet that can plug into my local Ai? \n\nMy use case - I buy antiques at auction and I like to ask the Ai if it sees any lots on the page that may be worth my time to look at, then I want it to find comps that may have sold and give me an idea of what my ROI is going to be. I built an app to do this and it works, but having it right in the web browser side bar using perplexity is nice, but would like to give my hardware a chance at it too. \n\nThank you",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qzg2zw/what_do_we_have_for_ollama_that_works_like/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4ak2gx",
          "author": "Tim-Fra",
          "text": "It's called BrowserOS and it doesn't work very well compared to Comet.",
          "score": 2,
          "created_utc": "2026-02-08 18:24:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4bw7q3",
              "author": "Comfortable_Ad_8117",
              "text": "I just tried browserOS with the Ollama GTP-oss 20B and it actually worked I showed it an auction site, it opened a bunch of tabs and did some comparison shopping on ebay and wrote me a report.\nIt took a decent amount of time to get it done, but it looks like it could be a viable alternative to comet when I get tired of $20/mo \n- Thanks for the tip!",
              "score": 1,
              "created_utc": "2026-02-08 22:20:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4b1xy2",
          "author": "Comfortable_Ad_8117",
          "text": "And I just tried page assist and itâ€™s not that good either as compared to comet",
          "score": 1,
          "created_utc": "2026-02-08 19:49:00",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o4cc34w",
          "author": "Smart-Competition200",
          "text": "perplexica is the local alternative ",
          "score": 1,
          "created_utc": "2026-02-08 23:51:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}