{
  "metadata": {
    "last_updated": "2026-01-27 16:59:03",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 20,
    "total_comments": 114,
    "file_size_bytes": 136770
  },
  "items": [
    {
      "id": "1qm9cgp",
      "title": "Ollama Models Ranked by VRAM Requirements",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qm9cgp/ollama_models_ranked_by_vram_requirements/",
      "author": "AdventurousLion9548",
      "created_utc": "2026-01-25 04:36:31",
      "score": 433,
      "num_comments": 37,
      "upvote_ratio": 0.96,
      "text": "1250.08 GB  |  cogito-2.1:latest\n\n1250.08 GB  |  cogito-2.1:671b\n\n 376.71 GB  |  deepseek-v3.1:latest\n\n 376.71 GB  |  deepseek-v3.1:671b\n\n 376.65 GB  |  deepseek-r1:671b\n\n 376.65 GB  |  deepseek-v3:latest\n\n 376.65 GB  |  deepseek-v3:671b\n\n 376.65 GB  |  r1-1776:671b\n\n 270.14 GB  |  qwen3-coder:480b\n\n 226.38 GB  |  llama3.1:405b\n\n 213.14 GB  |  hermes3:405b\n\n 133.43 GB  |  qwen3-vl:235b\n\n 132.39 GB  |  qwen3:235b\n\n 123.78 GB  |  deepseek-coder-v2:236b\n\n 123.78 GB  |  deepseek-v2:236b\n\n 123.78 GB  |  deepseek-v2.5:latest\n\n 123.78 GB  |  deepseek-v2.5:236b\n\n  94.51 GB  |  falcon:180b\n\n  74.05 GB  |  zephyr:141b\n\n  69.75 GB  |  devstral-2:latest\n\n  69.75 GB  |  devstral-2:123b\n\n   69.1 GB  |  dbrx:latest\n\n   69.1 GB  |  dbrx:132b\n\n  68.19 GB  |  mistral-large:latest\n\n  68.19 GB  |  mistral-large:123b\n\n   63.1 GB  |  megadolphin:latest\n\n   63.1 GB  |  megadolphin:120b\n\n  62.81 GB  |  llama4:latest\n\n  62.52 GB  |  command-a:latest\n\n  62.52 GB  |  command-a:111b\n\n  60.88 GB  |  gpt-oss:120b\n\n  60.88 GB  |  gpt-oss-safeguard:120b\n\n  58.57 GB  |  qwen:110b\n\n  55.15 GB  |  command-r-plus:latest\n\n  55.15 GB  |  command-r-plus:104b\n\n  50.87 GB  |  llama3.2-vision:90b\n\n  46.89 GB  |  qwen3-next:latest\n\n  46.89 GB  |  qwen3-next:80b\n\n  45.36 GB  |  qwen2.5vl:72b\n\n  44.16 GB  |  athene-v2:latest\n\n  44.16 GB  |  athene-v2:72b\n\n  44.16 GB  |  qwen2.5:72b\n\n   39.6 GB  |  cogito:70b\n\n   39.6 GB  |  deepseek-r1:70b\n\n   39.6 GB  |  llama3.1:70b\n\n   39.6 GB  |  llama3.3:latest\n\n   39.6 GB  |  llama3.3:70b\n\n   39.6 GB  |  nemotron:latest\n\n   39.6 GB  |  nemotron:70b\n\n   39.6 GB  |  r1-1776:latest\n\n   39.6 GB  |  r1-1776:70b\n\n   39.6 GB  |  tulu3:70b\n\n   38.4 GB  |  qwen2:72b\n\n   38.4 GB  |  qwen2-math:72b\n\n  38.18 GB  |  qwen:72b\n\n  37.22 GB  |  dolphin-llama3:70b\n\n  37.22 GB  |  firefunction-v2:latest\n\n  37.22 GB  |  firefunction-v2:70b\n\n  37.22 GB  |  hermes3:70b\n\n  37.22 GB  |  llama3:70b\n\n  37.22 GB  |  llama3-chatqa:70b\n\n  37.22 GB  |  llama3-gradient:70b\n\n  37.22 GB  |  llama3-groq-tool-use:70b\n\n  37.22 GB  |  reflection:latest\n\n  37.22 GB  |  reflection:70b\n\n   36.2 GB  |  codellama:70b\n\n   36.2 GB  |  llama2:70b\n\n   36.2 GB  |  llama2-uncensored:70b\n\n   36.2 GB  |  meditron:70b\n\n   36.2 GB  |  orca-mini:70b\n\n   36.2 GB  |  stable-beluga:70b\n\n   36.2 GB  |  wizard-math:70b\n\n  35.53 GB  |  deepseek-llm:67b\n\n  24.63 GB  |  dolphin-mixtral:latest\n\n  24.63 GB  |  mixtral:latest\n\n  24.63 GB  |  notux:latest\n\n  24.63 GB  |  nous-hermes2-mixtral:latest\n\n   22.6 GB  |  nemotron-3-nano:latest\n\n   22.6 GB  |  nemotron-3-nano:30b\n\n  22.17 GB  |  alfred:latest\n\n  22.17 GB  |  alfred:40b\n\n  22.17 GB  |  falcon:40b\n\n  19.71 GB  |  qwen2.5vl:32b\n\n  19.47 GB  |  qwen3-vl:32b\n\n  18.84 GB  |  aya:35b\n\n  18.81 GB  |  qwen3:32b\n\n  18.78 GB  |  llava:34b\n\n  18.49 GB  |  cogito:32b\n\n  18.49 GB  |  deepseek-r1:32b\n\n  18.49 GB  |  openthinker:32b\n\n  18.49 GB  |  qwen2.5:32b\n\n  18.49 GB  |  qwen2.5-coder:32b\n\n  18.49 GB  |  qwq:latest\n\n  18.49 GB  |  qwq:32b\n\n  18.44 GB  |  aya-expanse:32b\n\n  18.25 GB  |  qwen3-vl:30b\n\n  18.14 GB  |  olmo-3:32b\n\n  18.14 GB  |  olmo-3.1:latest\n\n  18.14 GB  |  olmo-3.1:32b\n\n  18.13 GB  |  nous-hermes2:34b\n\n  18.13 GB  |  yi:34b\n\n  18.02 GB  |  exaone-deep:32b\n\n  18.02 GB  |  exaone3.5:32b\n\n  17.92 GB  |  granite-code:34b\n\n  17.74 GB  |  codebooga:latest\n\n  17.74 GB  |  codebooga:34b\n\n  17.74 GB  |  codellama:34b\n\n  17.74 GB  |  phind-codellama:latest\n\n  17.74 GB  |  phind-codellama:34b\n\n  17.53 GB  |  deepseek-coder:33b\n\n  17.53 GB  |  wizardcoder:33b\n\n  17.43 GB  |  command-r:latest\n\n  17.43 GB  |  command-r:35b\n\n  17.28 GB  |  qwen3:30b\n\n  17.28 GB  |  qwen3-coder:latest\n\n  17.28 GB  |  qwen3-coder:30b\n\n  17.23 GB  |  qwen:32b\n\n   17.1 GB  |  vicuna:33b\n\n   17.1 GB  |  wizard-vicuna-uncensored:30b\n\n   16.2 GB  |  gemma3:27b\n\n  16.17 GB  |  translategemma:27b\n\n   15.5 GB  |  shieldgemma:27b\n\n  14.56 GB  |  gemma2:27b\n\n  14.42 GB  |  mistral-small3.1:latest\n\n  14.42 GB  |  mistral-small3.1:24b\n\n  14.14 GB  |  devstral-small-2:latest\n\n  14.14 GB  |  devstral-small-2:24b\n\n  14.14 GB  |  mistral-small3.2:latest\n\n  14.14 GB  |  mistral-small3.2:24b\n\n  13.35 GB  |  devstral:latest\n\n  13.35 GB  |  devstral:24b\n\n  13.35 GB  |  magistral:latest\n\n  13.35 GB  |  magistral:24b\n\n  13.35 GB  |  mistral-small:latest\n\n  13.35 GB  |  mistral-small:24b\n\n  12.85 GB  |  gpt-oss:latest\n\n  12.85 GB  |  gpt-oss:20b\n\n  12.85 GB  |  gpt-oss-safeguard:latest\n\n  12.85 GB  |  gpt-oss-safeguard:20b\n\n   12.4 GB  |  solar-pro:latest\n\n   12.4 GB  |  solar-pro:22b\n\n  11.71 GB  |  codestral:latest\n\n  11.71 GB  |  codestral:22b\n\n  11.71 GB  |  mistral-small:22b\n\n  10.82 GB  |  sailor2:20b\n\n  10.76 GB  |  granite-code:20b\n\n  10.55 GB  |  internlm2:20b\n\n  10.35 GB  |  phi4-reasoning:latest\n\n  10.35 GB  |  phi4-reasoning:14b\n\n   8.64 GB  |  qwen3:14b\n\n   8.46 GB  |  ministral-3:14b\n\n   8.44 GB  |  dolphincoder:15b\n\n   8.44 GB  |  starcoder2:15b\n\n   8.43 GB  |  phi4:latest\n\n   8.43 GB  |  phi4:14b\n\n   8.37 GB  |  cogito:14b\n\n   8.37 GB  |  deepcoder:latest\n\n   8.37 GB  |  deepcoder:14b\n\n   8.37 GB  |  deepseek-r1:14b\n\n   8.37 GB  |  qwen2.5:14b\n\n   8.37 GB  |  qwen2.5-coder:14b\n\n   8.37 GB  |  sqlcoder:15b\n\n   8.37 GB  |  starcoder:15b\n\n   8.29 GB  |  deepseek-coder-v2:latest\n\n   8.29 GB  |  deepseek-coder-v2:16b\n\n   8.29 GB  |  deepseek-v2:latest\n\n   8.29 GB  |  deepseek-v2:16b\n\n   7.78 GB  |  olmo2:13b\n\n   7.62 GB  |  qwen:14b\n\n   7.59 GB  |  gemma3:12b\n\n   7.55 GB  |  translategemma:12b\n\n   7.46 GB  |  llava:13b\n\n   7.35 GB  |  phi3:14b\n\n   7.28 GB  |  llama3.2-vision:latest\n\n   7.28 GB  |  llama3.2-vision:11b\n\n   7.03 GB  |  gemma3n:latest\n\n   6.86 GB  |  codellama:13b\n\n   6.86 GB  |  codeup:latest\n\n   6.86 GB  |  codeup:13b\n\n   6.86 GB  |  everythinglm:latest\n\n   6.86 GB  |  everythinglm:13b\n\n   6.86 GB  |  llama2:13b\n\n   6.86 GB  |  llama2-chinese:13b\n\n   6.86 GB  |  nexusraven:latest\n\n   6.86 GB  |  nexusraven:13b\n\n   6.86 GB  |  nous-hermes:13b\n\n   6.86 GB  |  open-orca-platypus2:latest\n\n   6.86 GB  |  open-orca-platypus2:13b\n\n   6.86 GB  |  orca-mini:13b\n\n   6.86 GB  |  orca2:13b\n\n   6.86 GB  |  stable-beluga:13b\n\n   6.86 GB  |  vicuna:13b\n\n   6.86 GB  |  wizard-math:13b\n\n   6.86 GB  |  wizard-vicuna:latest\n\n   6.86 GB  |  wizard-vicuna:13b\n\n   6.86 GB  |  wizard-vicuna-uncensored:13b\n\n   6.86 GB  |  wizardlm-uncensored:latest\n\n   6.86 GB  |  wizardlm-uncensored:13b\n\n   6.86 GB  |  xwinlm:13b\n\n   6.86 GB  |  yarn-llama2:13b\n\n   6.59 GB  |  mistral-nemo:latest\n\n   6.59 GB  |  mistral-nemo:12b\n\n   6.49 GB  |  stablelm2:12b\n\n   6.23 GB  |  deepseek-ocr:latest\n\n   6.23 GB  |  deepseek-ocr:3b\n\n   5.94 GB  |  falcon2:latest\n\n   5.94 GB  |  falcon2:11b\n\n   5.86 GB  |  falcon3:10b\n\n   5.72 GB  |  qwen3-vl:latest\n\n   5.72 GB  |  qwen3-vl:8b\n\n   5.66 GB  |  nous-hermes2:latest\n\n   5.66 GB  |  nous-hermes2:10.7b\n\n   5.66 GB  |  solar:latest\n\n   5.66 GB  |  solar:10.7b\n\n   5.61 GB  |  ministral-3:latest\n\n   5.61 GB  |  ministral-3:8b\n\n   5.56 GB  |  qwen2.5vl:latest\n\n   5.56 GB  |  qwen2.5vl:7b\n\n5.4 GB  |  granite3-guardian:8b\n\n   5.37 GB  |  shieldgemma:latest\n\n   5.37 GB  |  shieldgemma:9b\n\n   5.16 GB  |  llava-llama3:latest\n\n   5.16 GB  |  llava-llama3:8b\n\n5.1 GB  |  minicpm-v:latest\n\n5.1 GB  |  minicpm-v:8b\n\n   5.08 GB  |  codegeex4:latest\n\n   5.08 GB  |  codegeex4:9b\n\n   5.08 GB  |  glm4:latest\n\n   5.08 GB  |  glm4:9b\n\n   5.07 GB  |  gemma2:latest\n\n   5.07 GB  |  gemma2:9b\n\n   4.88 GB  |  sailor2:latest\n\n   4.88 GB  |  sailor2:8b\n\n   4.87 GB  |  deepseek-r1:latest\n\n   4.87 GB  |  deepseek-r1:8b\n\n   4.87 GB  |  qwen3:latest\n\n   4.87 GB  |  qwen3:8b\n\n   4.76 GB  |  rnj-1:latest\n\n   4.76 GB  |  rnj-1:8b\n\n   4.71 GB  |  aya-expanse:latest\n\n   4.71 GB  |  aya-expanse:8b\n\n   4.71 GB  |  command-r7b:latest\n\n   4.71 GB  |  command-r7b:7b\n\n   4.71 GB  |  command-r7b-arabic:latest\n\n   4.71 GB  |  command-r7b-arabic:7b\n\n   4.69 GB  |  yi:9b\n\n   4.69 GB  |  yi-coder:latest\n\n   4.69 GB  |  yi-coder:9b\n\n   4.67 GB  |  codegemma:latest\n\n   4.67 GB  |  codegemma:7b\n\n   4.67 GB  |  gemma:latest\n\n   4.67 GB  |  gemma:7b\n\n   4.65 GB  |  granite3.1-dense:latest\n\n   4.65 GB  |  granite3.1-dense:8b\n\n4.6 GB  |  granite3-dense:8b\n\n4.6 GB  |  granite3.2:latest\n\n4.6 GB  |  granite3.2:8b\n\n4.6 GB  |  granite3.3:latest\n\n4.6 GB  |  granite3.3:8b\n\n   4.58 GB  |  cogito:latest\n\n   4.58 GB  |  cogito:8b\n\n   4.58 GB  |  dolphin3:latest\n\n   4.58 GB  |  dolphin3:8b\n\n   4.58 GB  |  llama-guard3:latest\n\n   4.58 GB  |  llama-guard3:8b\n\n   4.58 GB  |  llama3.1:latest\n\n   4.58 GB  |  llama3.1:8b\n\n   4.58 GB  |  tulu3:latest\n\n   4.58 GB  |  tulu3:8b\n\n   4.47 GB  |  aya:latest\n\n   4.47 GB  |  aya:8b\n\n   4.44 GB  |  exaone-deep:latest\n\n   4.44 GB  |  exaone-deep:7.8b\n\n   4.44 GB  |  exaone3.5:latest\n\n   4.44 GB  |  exaone3.5:7.8b\n\n   4.41 GB  |  bakllava:latest\n\n   4.41 GB  |  bakllava:7b\n\n   4.41 GB  |  llama-pro:latest\n\n   4.41 GB  |  llava:latest\n\n   4.41 GB  |  llava:7b\n\n   4.41 GB  |  opencoder:latest\n\n   4.41 GB  |  opencoder:8b\n\n   4.39 GB  |  bespoke-minicheck:latest\n\n   4.39 GB  |  bespoke-minicheck:7b\n\n   4.36 GB  |  deepseek-r1:7b\n\n   4.36 GB  |  marco-o1:latest\n\n   4.36 GB  |  marco-o1:7b\n\n   4.36 GB  |  openthinker:latest\n\n   4.36 GB  |  openthinker:7b\n\n   4.36 GB  |  qwen2.5:latest\n\n   4.36 GB  |  qwen2.5:7b\n\n   4.36 GB  |  qwen2.5-coder:latest\n\n   4.36 GB  |  qwen2.5-coder:7b\n\n   4.36 GB  |  qwen3-embedding:latest\n\n   4.36 GB  |  qwen3-embedding:8b\n\n   4.34 GB  |  dolphin-llama3:latest\n\n   4.34 GB  |  dolphin-llama3:8b\n\n   4.34 GB  |  hermes3:latest\n\n   4.34 GB  |  hermes3:8b\n\n   4.34 GB  |  llama3:latest\n\n   4.34 GB  |  llama3:8b\n\n   4.34 GB  |  llama3-chatqa:latest\n\n   4.34 GB  |  llama3-chatqa:8b\n\n   4.34 GB  |  llama3-gradient:latest\n\n   4.34 GB  |  llama3-gradient:8b\n\n   4.34 GB  |  llama3-groq-tool-use:latest\n\n   4.34 GB  |  llama3-groq-tool-use:8b\n\n   4.28 GB  |  granite-code:8b\n\n   4.26 GB  |  falcon3:latest\n\n   4.26 GB  |  falcon3:7b\n\n4.2 GB  |  qwen:7b\n\n   4.16 GB  |  olmo-3:latest\n\n   4.16 GB  |  olmo-3:7b\n\n   4.16 GB  |  olmo2:latest\n\n   4.16 GB  |  olmo2:7b\n\n   4.15 GB  |  internlm2:latest\n\n   4.15 GB  |  internlm2:7b\n\n   4.13 GB  |  qwen2:latest\n\n   4.13 GB  |  qwen2:7b\n\n   4.13 GB  |  qwen2-math:latest\n\n   4.13 GB  |  qwen2-math:7b\n\n   4.07 GB  |  mistral:latest\n\n   4.07 GB  |  mistral:7b\n\n4.0 GB  |  starcoder:7b\n\n   3.94 GB  |  dolphincoder:latest\n\n   3.94 GB  |  dolphincoder:7b\n\n   3.92 GB  |  falcon:latest\n\n   3.92 GB  |  falcon:7b\n\n   3.89 GB  |  codeqwen:latest\n\n   3.89 GB  |  codeqwen:7b\n\n   3.83 GB  |  dolphin-mistral:latest\n\n   3.83 GB  |  dolphin-mistral:7b\n\n   3.83 GB  |  mathstral:latest\n\n   3.83 GB  |  mathstral:7b\n\n   3.83 GB  |  mistral-openorca:latest\n\n   3.83 GB  |  mistral-openorca:7b\n\n   3.83 GB  |  mistrallite:latest\n\n   3.83 GB  |  mistrallite:7b\n\n   3.83 GB  |  neural-chat:latest\n\n   3.83 GB  |  neural-chat:7b\n\n   3.83 GB  |  notus:latest\n\n   3.83 GB  |  notus:7b\n\n   3.83 GB  |  openchat:latest\n\n   3.83 GB  |  openchat:7b\n\n   3.83 GB  |  openhermes:latest\n\n   3.83 GB  |  samantha-mistral:latest\n\n   3.83 GB  |  samantha-mistral:7b\n\n   3.83 GB  |  sqlcoder:latest\n\n   3.83 GB  |  sqlcoder:7b\n\n   3.83 GB  |  starling-lm:latest\n\n   3.83 GB  |  starling-lm:7b\n\n   3.83 GB  |  wizard-math:latest\n\n   3.83 GB  |  wizard-math:7b\n\n   3.83 GB  |  wizardlm2:latest\n\n   3.83 GB  |  wizardlm2:7b\n\n   3.83 GB  |  yarn-mistral:latest\n\n   3.83 GB  |  yarn-mistral:7b\n\n   3.83 GB  |  zephyr:latest\n\n   3.83 GB  |  zephyr:7b\n\n   3.77 GB  |  starcoder2:7b\n\n   3.73 GB  |  deepseek-llm:latest\n\n   3.73 GB  |  deepseek-llm:7b\n\n   3.56 GB  |  codellama:latest\n\n   3.56 GB  |  codellama:7b\n\n   3.56 GB  |  deepseek-coder:6.7b\n\n   3.56 GB  |  duckdb-nsql:latest\n\n   3.56 GB  |  duckdb-nsql:7b\n\n   3.56 GB  |  llama2:latest\n\n   3.56 GB  |  llama2:7b\n\n   3.56 GB  |  llama2-chinese:latest\n\n   3.56 GB  |  llama2-chinese:7b\n\n   3.56 GB  |  llama2-uncensored:latest\n\n   3.56 GB  |  llama2-uncensored:7b\n\n   3.56 GB  |  magicoder:latest\n\n   3.56 GB  |  magicoder:7b\n\n   3.56 GB  |  meditron:latest\n\n   3.56 GB  |  meditron:7b\n\n   3.56 GB  |  medllama2:latest\n\n   3.56 GB  |  medllama2:7b\n\n   3.56 GB  |  nous-hermes:latest\n\n   3.56 GB  |  nous-hermes:7b\n\n   3.56 GB  |  orca-mini:7b\n\n   3.56 GB  |  orca2:latest\n\n   3.56 GB  |  orca2:7b\n\n   3.56 GB  |  stable-beluga:latest\n\n   3.56 GB  |  stable-beluga:7b\n\n   3.56 GB  |  vicuna:latest\n\n   3.56 GB  |  vicuna:7b\n\n   3.56 GB  |  wizard-vicuna-uncensored:latest\n\n   3.56 GB  |  wizard-vicuna-uncensored:7b\n\n   3.56 GB  |  xwinlm:latest\n\n   3.56 GB  |  xwinlm:7b\n\n   3.56 GB  |  yarn-llama2:latest\n\n   3.56 GB  |  yarn-llama2:7b\n\n   3.37 GB  |  smallthinker:latest\n\n   3.37 GB  |  smallthinker:3b\n\n   3.32 GB  |  deepscaler:latest\n\n   3.32 GB  |  deepscaler:1.5b\n\n   3.24 GB  |  yi:latest\n\n   3.24 GB  |  yi:6b\n\n   3.11 GB  |  gemma3:latest\n\n   3.11 GB  |  gemma3:4b\n\n   3.07 GB  |  qwen3-vl:4b\n\n   3.07 GB  |  translategemma:latest\n\n   3.07 GB  |  translategemma:4b\n\n   3.04 GB  |  granite4:1b\n\n   2.98 GB  |  qwen2.5vl:3b\n\n   2.94 GB  |  phi4-mini-reasoning:latest\n\n   2.94 GB  |  phi4-mini-reasoning:3.8b\n\n   2.75 GB  |  ministral-3:3b\n\n   2.73 GB  |  llava-phi3:latest\n\n   2.73 GB  |  llava-phi3:3.8b\n\n   2.51 GB  |  granite3-guardian:latest\n\n   2.51 GB  |  granite3-guardian:2b\n\n   2.51 GB  |  nemotron-mini:latest\n\n   2.51 GB  |  nemotron-mini:4b\n\n   2.33 GB  |  qwen3:4b\n\n   2.33 GB  |  qwen3-embedding:4b\n\n   2.32 GB  |  phi4-mini:latest\n\n   2.32 GB  |  phi4-mini:3.8b\n\n   2.27 GB  |  granite3.2-vision:latest\n\n   2.27 GB  |  granite3.2-vision:2b\n\n   2.17 GB  |  qwen:latest\n\n   2.17 GB  |  qwen:4b\n\n   2.09 GB  |  cogito:3b\n\n   2.03 GB  |  nuextract:latest\n\n   2.03 GB  |  nuextract:3.8b\n\n   2.03 GB  |  phi3:latest\n\n   2.03 GB  |  phi3:3.8b\n\n   2.03 GB  |  phi3.5:latest\n\n   2.03 GB  |  phi3.5:3.8b\n\n   1.96 GB  |  granite4:3b\n\n   1.92 GB  |  granite3-moe:3b\n\n1.9 GB  |  granite3.1-moe:latest\n\n1.9 GB  |  granite3.1-moe:3b\n\n   1.88 GB  |  hermes3:3b\n\n   1.88 GB  |  llama3.2:latest\n\n   1.88 GB  |  llama3.2:3b\n\n   1.87 GB  |  falcon3:3b\n\n   1.86 GB  |  granite-code:latest\n\n   1.86 GB  |  granite-code:3b\n\n   1.84 GB  |  orca-mini:latest\n\n   1.84 GB  |  orca-mini:3b\n\n1.8 GB  |  qwen2.5:3b\n\n1.8 GB  |  qwen2.5-coder:3b\n\n   1.76 GB  |  qwen3-vl:2b\n\n   1.71 GB  |  starcoder:latest\n\n   1.71 GB  |  starcoder:3b\n\n1.7 GB  |  smollm2:latest\n\n1.7 GB  |  smollm2:1.7b\n\n   1.66 GB  |  falcon3:1b\n\n   1.62 GB  |  moondream:latest\n\n   1.62 GB  |  moondream:1.8b\n\n   1.59 GB  |  shieldgemma:2b\n\n   1.59 GB  |  starcoder2:latest\n\n   1.59 GB  |  starcoder2:3b\n\n   1.56 GB  |  gemma:2b\n\n   1.53 GB  |  exaone-deep:2.4b\n\n   1.53 GB  |  exaone3.5:2.4b\n\n   1.52 GB  |  gemma2:2b\n\n1.5 GB  |  stable-code:latest\n\n1.5 GB  |  stable-code:3b\n\n1.5 GB  |  stablelm-zephyr:latest\n\n1.5 GB  |  stablelm-zephyr:3b\n\n   1.49 GB  |  dolphin-phi:latest\n\n   1.49 GB  |  dolphin-phi:2.7b\n\n   1.49 GB  |  granite3-dense:latest\n\n   1.49 GB  |  granite3-dense:2b\n\n   1.49 GB  |  llama-guard3:1b\n\n   1.49 GB  |  phi:latest\n\n   1.49 GB  |  phi:2.7b\n\n   1.46 GB  |  granite3.1-dense:2b\n\n   1.44 GB  |  codegemma:2b\n\n   1.44 GB  |  granite3.2:2b\n\n   1.44 GB  |  granite3.3:2b\n\n   1.32 GB  |  granite3.1-moe:1b\n\n   1.32 GB  |  opencoder:1.5b\n\n   1.27 GB  |  qwen3:1.7b\n\n   1.23 GB  |  llama3.2:1b\n\n   1.08 GB  |  bge-m3:latest\n\n   1.08 GB  |  snowflake-arctic-embed2:latest\n\n   1.04 GB  |  deepcoder:1.5b\n\n   1.04 GB  |  deepseek-r1:1.5b\n\n   1.04 GB  |  internlm2:1.8b\n\n   1.04 GB  |  qwen:1.8b\n\n   0.98 GB  |  sailor2:1b\n\n   0.92 GB  |  qwen2.5:1.5b\n\n   0.92 GB  |  qwen2.5-coder:1.5b\n\n   0.92 GB  |  smollm:latest\n\n   0.92 GB  |  smollm:1.7b\n\n   0.92 GB  |  stablelm2:latest\n\n   0.92 GB  |  stablelm2:1.6b\n\n   0.87 GB  |  qwen2:1.5b\n\n   0.87 GB  |  qwen2-math:1.5b\n\n   0.87 GB  |  reader-lm:latest\n\n   0.87 GB  |  reader-lm:1.5b\n\n   0.81 GB  |  yi-coder:1.5b\n\n   0.77 GB  |  granite3-moe:latest\n\n   0.77 GB  |  granite3-moe:1b\n\n   0.76 GB  |  gemma3:1b\n\n   0.72 GB  |  deepseek-coder:latest\n\n   0.72 GB  |  deepseek-coder:1.3b\n\n   0.68 GB  |  lfm2.5-thinking:latest\n\n   0.68 GB  |  lfm2.5-thinking:1.2b\n\n   0.68 GB  |  starcoder:1b\n\n   0.62 GB  |  bge-large:latest\n\n   0.62 GB  |  mxbai-embed-large:latest\n\n   0.62 GB  |  snowflake-arctic-embed:latest\n\n0.6 GB  |  qwen3-embedding:0.6b\n\n   0.59 GB  |  tinydolphin:latest\n\n   0.59 GB  |  tinydolphin:1.1b\n\n   0.59 GB  |  tinyllama:latest\n\n   0.59 GB  |  tinyllama:1.1b\n\n   0.58 GB  |  embeddinggemma:latest\n\n   0.52 GB  |  paraphrase-multilingual:latest\n\n   0.49 GB  |  qwen3:0.6b\n\n   0.37 GB  |  qwen:0.5b\n\n   0.37 GB  |  qwen2.5:0.5b\n\n   0.37 GB  |  qwen2.5-coder:0.5b\n\n   0.33 GB  |  qwen2:0.5b\n\n   0.33 GB  |  reader-lm:0.5b\n\n   0.28 GB  |  functiongemma:latest\n\n   0.26 GB  |  nomic-embed-text:latest\n\n   0.06 GB  |  granite-embedding:latest\n\n   0.04 GB  |  all-minilm:latest",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qm9cgp/ollama_models_ranked_by_vram_requirements/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1kljht",
          "author": "merica420_69",
          "text": "At what quant?",
          "score": 19,
          "created_utc": "2026-01-25 05:59:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kxbyk",
              "author": "triynizzles1",
              "text": "q4 since that is what ollama defaults to. This is a list of models so if you wanted to download and run any of them you would enter ‚Äúollama run‚Äù before the name of the model. Then the file it downloads will be the size listed in the chart.",
              "score": 9,
              "created_utc": "2026-01-25 07:33:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ked3h",
          "author": "triynizzles1",
          "text": "Good post üëç",
          "score": 27,
          "created_utc": "2026-01-25 05:10:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1km6wl",
              "author": "Tall_Instance9797",
              "text": "Looks like a copy and paste from this website: [https://llm-explorer.com/](https://llm-explorer.com/)",
              "score": 32,
              "created_utc": "2026-01-25 06:03:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1l5bs0",
          "author": "arm2armreddit",
          "text": "Ollama defaults to a 4k context length; unfortunately, this is realistically unuseful for real tasks. It would be good to see the true memory usage with the 100% supported context length by the given model.",
          "score": 11,
          "created_utc": "2026-01-25 08:42:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kctku",
          "author": "Gombaoxo",
          "text": "Where are all huihui models? LLM is not fun without abliterated models.",
          "score": 18,
          "created_utc": "2026-01-25 05:00:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kyshj",
              "author": "darkpigvirus",
              "text": "Back then I liked huihui models but intelligence suffers when abliterating a model unless you really want those censored topics. Also there is a technique better than abliteration that lessen the intelligence suffering I just forgot it.",
              "score": 2,
              "created_utc": "2026-01-25 07:45:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1lkfnt",
                  "author": "alhinai_03",
                  "text": "You probably mean heretic",
                  "score": 1,
                  "created_utc": "2026-01-25 10:56:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1p2ssh",
                  "author": "According-Delivery44",
                  "text": "What technique? Prompyt injection?",
                  "score": 0,
                  "created_utc": "2026-01-25 21:29:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1l7c0n",
          "author": "leo-the-great",
          "text": "üòÖ damn I have to scroll way way down to check what I can run.",
          "score": 10,
          "created_utc": "2026-01-25 09:00:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1lk729",
              "author": "AdventurousLion9548",
              "text": "I feel ya!",
              "score": 1,
              "created_utc": "2026-01-25 10:54:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1kmcg7",
          "author": "Tall_Instance9797",
          "text": "Is this post just a copy and paste from this website: [https://llm-explorer.com/](https://llm-explorer.com/) ?",
          "score": 11,
          "created_utc": "2026-01-25 06:05:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kp0eu",
              "author": "AdventurousLion9548",
              "text": "Actually, I copied the code that ChatGPT generated to scrape and rank from ollama/models and got the output. üòÇüòÇüòÇ import re\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\nLIBRARY_URL = \"https://ollama.com/library/\"\nMANIFEST_URL = \"https://registry.ollama.ai/v2/library/{name}/manifests/{tag}\"\nOUTPUT_FILE = \"ollama_models_ranked_by_vram.txt\"\n\nHEADERS = {\n    \"User-Agent\": \"ollama-vram-ranker/1.1\"\n}\n\nWEIGHT_MEDIA_TYPES = {\n    \"application/vnd.ollama.image.model\",\n    \"application/vnd.ollama.image.projector\",  # vision models\n}\n\nsession = requests.Session()\nsession.headers.update(HEADERS)\n\n\n# -------------------- helpers --------------------\n\ndef get_html(url):\n    r = session.get(url, timeout=30)\n    r.raise_for_status()\n    return r.text\n\n\ndef bytes_to_gb(b):\n    return b / (1024 ** 3)\n\n\n# -------------------- scraping --------------------\n\ndef get_all_models():\n    html = get_html(LIBRARY_URL)\n    soup = BeautifulSoup(html, \"html.parser\")\n\n    models = set()\n    for a in soup.select('a[href^=\"/library/\"]'):\n        href = a.get(\"href\", \"\")\n        m = re.fullmatch(r\"/library/([^/]+)\", href)\n        if m:\n            models.add(m.group(1))\n\n    return sorted(models)\n\n\ndef get_model_tags(model):\n    url = urljoin(LIBRARY_URL, model)\n    html = get_html(url)\n    soup = BeautifulSoup(html, \"html.parser\")\n\n    text = soup.get_text(\" \", strip=True).lower()\n    tags = set()\n\n    for token in text.split():\n        if re.fullmatch(r\"\\d+(\\.\\d+)?b\", token):\n            tags.add(token)\n        elif token == \"latest\":\n            tags.add(token)\n\n    if not tags:\n        tags.add(\"latest\")\n\n    # latest first, then numeric\n    def sort_key(t):\n        if t == \"latest\":\n            return (0, 0)\n        m = re.match(r\"(\\d+(\\.\\d+)?)b\", t)\n        return (1, float(m.group(1)) if m else 999)\n\n    return sorted(tags, key=sort_key)\n\n\n# -------------------- registry --------------------\n\ndef get_manifest_weight_bytes(model, tag):\n    url = MANIFEST_URL.format(name=model, tag=tag)\n\n    try:\n        r = session.get(url, timeout=30)\n        if r.status_code != 200:\n            return None\n\n        manifest = r.json()\n\n        # √∞¬ü¬î¬¥ CRITICAL FIX: validate manifest\n        if not isinstance(manifest, dict):\n            return None\n\n        layers = manifest.get(\"layers\")\n        if not isinstance(layers, list):\n            return None\n\n        total = 0\n        for layer in layers:\n            if (\n                isinstance(layer, dict)\n                and layer.get(\"mediaType\") in WEIGHT_MEDIA_TYPES\n                and isinstance(layer.get(\"size\"), int)\n            ):\n                total += layer[\"size\"]\n\n        return total if total > 0 else None\n\n    except Exception:\n        return None\n\n\n# -------------------- main --------------------\n\ndef main():\n    print(\"Fetching Ollama model list...\")\n    models = get_all_models()\n\n    results = []\n\n    for idx, model in enumerate(models, 1):\n        print(f\"[{idx}/{len(models)}] {model}\")\n\n        try:\n            tags = get_model_tags(model)\n        except Exception:\n            continue\n\n        for tag in tags:\n            size_bytes = get_manifest_weight_bytes(model, tag)\n            if size_bytes:\n                results.append({\n                    \"model\": model,\n                    \"tag\": tag,\n                    \"gb\": round(bytes_to_gb(size_bytes), 2)\n                })\n\n    results.sort(key=lambda x: x[\"gb\"], reverse=True)\n\n    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"Ollama Models Ranked by VRAM Requirement (Weights Only)\\n\")\n        f.write(\"=\" * 60 + \"\\n\\n\")\n\n        for r in results:\n            f.write(f\"{r['gb']:>7} GB  |  {r['model']}:{r['tag']}\\n\")\n\n    print(f\"\\n√¢¬ú¬î Output written to: {OUTPUT_FILE}\")\n    print(f\"√¢¬ú¬î {len(results)} valid model entries written\")\n\n\nif __name__ == \"__main__\":\n    main()",
              "score": 8,
              "created_utc": "2026-01-25 06:25:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1klbna",
          "author": "nitinmms1",
          "text": "This is very useful",
          "score": 9,
          "created_utc": "2026-01-25 05:57:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1km1wn",
              "author": "Tall_Instance9797",
              "text": "Looks like a copy and paste from this website: [https://llm-explorer.com/](https://llm-explorer.com/) \\- which honestly is far more useful.",
              "score": 9,
              "created_utc": "2026-01-25 06:02:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1o1du4",
                  "author": "Puzzled_Surprise_383",
                  "text": "Amigo √© o seu terceiro coment√°rio querendo acabar com o OP, j√° entendemos sua revolta. \nAgrade√ßo a ele por postar pois n√£o conhecia o site. \nObrigado, OP, post muito √∫til",
                  "score": 5,
                  "created_utc": "2026-01-25 18:46:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1utz62",
                  "author": "jamalakj",
                  "text": "We don't care",
                  "score": 2,
                  "created_utc": "2026-01-26 17:38:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1kj8bj",
          "author": "slow-fast-person",
          "text": "How to calculate this for corresponding requirements on apple metal?",
          "score": 3,
          "created_utc": "2026-01-25 05:43:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rhqcx",
          "author": "MLExpert000",
          "text": "Who is actually serving a 1250GB model locally? To run that cogito model, you‚Äôd need about 53 RTX 3090s daisy-chained together. at that point, it‚Äôs not a home lab, it‚Äôs a fire hazard.",
          "score": 3,
          "created_utc": "2026-01-26 04:46:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yvtrx",
              "author": "TedditBlatherflag",
              "text": "You don‚Äôt have an HGX H200 rack running in your HVAC, heating your home?",
              "score": 1,
              "created_utc": "2026-01-27 05:55:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1lhfit",
          "author": "austrobergbauernbua",
          "text": "I can just highly recommend the granite 4 models (hybrid architecture). For small tasks like simple Q&A or rewriting they are extremely efficient (fast memory loading - extremely fast inference).",
          "score": 2,
          "created_utc": "2026-01-25 10:30:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1m4rez",
          "author": "Electronic-Set-2413",
          "text": "Nice one",
          "score": 2,
          "created_utc": "2026-01-25 13:31:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1md7hd",
          "author": "Bonzupii",
          "text": "Really dude",
          "score": 2,
          "created_utc": "2026-01-25 14:19:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nxtyg",
          "author": "gamesta2",
          "text": "Looks like 4k context length for all. I can confirm that most of these take as much as double the vram once you increase context to 48k+. For example, my 4b model that supports my home assistant takes 16gb vram at 128k context.",
          "score": 2,
          "created_utc": "2026-01-25 18:31:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1out0a",
              "author": "SundayButtermilk",
              "text": "Are you using this for home assistant voice? What hardware are you running it on?",
              "score": 2,
              "created_utc": "2026-01-25 20:55:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1povhv",
                  "author": "gamesta2",
                  "text": "I use it for tool calling. The model i run is qwen3:4b-instruct-q_8 (for better precision). I lied about context, mine is set to 96k. It has to be this high for hass because for some reason each call is 50k+ tokens, depending on how many devices you expose.\n\nThe hardware is a separate server on the same local network. nothing fancy. Ryzen 7 9700x, 64gb ddr5, and dual rtx 3060 do the hard lifting. Llms are hosted by ollama. \n\nIm not too worried about offloading into ram for most Ai usage when utilizing other models for things like image analysis (qwen3-vl:30b) but for hass tool calling I had to go with a model small enough to fit into 24gb vram to maximize response time, and at the same time handle 96k context.\nBiggest ragret is not getting additional 64gb of ram back when it was 240$.",
                  "score": 2,
                  "created_utc": "2026-01-25 23:07:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1pqxos",
                  "author": "gamesta2",
                  "text": "As for voice, yes. I use hass assistant on my phone and my watch to give it voice commands. Nabu transcribes it to text (or you can use the stock stt that comes with hass) and the assistant either takes own action if its something simple, or makes a call to llm if its something complex like if-then or multiple actions.",
                  "score": 2,
                  "created_utc": "2026-01-25 23:16:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1l35ym",
          "author": "vir_db",
          "text": "With maximum num_ctx?",
          "score": 1,
          "created_utc": "2026-01-25 08:23:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l6hnx",
          "author": "pmv143",
          "text": "We run all of these models on the same 16√óH100 pool. Big models take the whole beam, smaller ones take slices. Fast snapshot restore lets us swap between them in ~1‚Äì2s instead of pinning GPUs.",
          "score": 1,
          "created_utc": "2026-01-25 08:52:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ldnpu",
          "author": "WildDogOne",
          "text": "and now, if someone could do that, but with maxed out context window",
          "score": 1,
          "created_utc": "2026-01-25 09:56:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ldpyp",
          "author": "zenmatrix83",
          "text": "Context length matters and the default is too small for anything not 100 chat based, my current use case is using 32k but I can get 64k worth from qwen3 from my 4090 at decent speeds",
          "score": 1,
          "created_utc": "2026-01-25 09:57:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1mg8vl",
          "author": "AUFairhope1104",
          "text": "Per",
          "score": 1,
          "created_utc": "2026-01-25 14:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nujze",
          "author": "Mangostickyrice1999",
          "text": "Source?",
          "score": 1,
          "created_utc": "2026-01-25 18:18:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ocmn9",
              "author": "AdventurousLion9548",
              "text": "From ollama/models site. Also here https://www.reddit.com/r/ollama/s/HaUw1tIu6W",
              "score": 2,
              "created_utc": "2026-01-25 19:34:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rmwlq",
          "author": "KiranjotSingh",
          "text": "Incorrect. Most of these are MoE and vram requirement differs a lot in that case",
          "score": 1,
          "created_utc": "2026-01-26 05:21:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rpzdb",
          "author": "eagledoto",
          "text": "Can you please let me know What are the best models for image captioning or turning image to prompt for flux2/z image? I've got rtx 2060 with 12vram and 32gigs of system ram",
          "score": 1,
          "created_utc": "2026-01-26 05:42:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o203g46",
          "author": "Informal-Victory8655",
          "text": "Any model better than qwen2.5:14b in agentic tool calling capabilities? Anyone has any experience?",
          "score": 1,
          "created_utc": "2026-01-27 12:09:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qiug46",
      "title": "Fine-tuned Qwen3 0.6B for Text2SQL using a claude skill. The result tiny model matches a Deepseek 3.1 and runs locally on CPU.",
      "subreddit": "ollama",
      "url": "https://i.redd.it/fy13421qjoeg1.png",
      "author": "party-horse",
      "created_utc": "2026-01-21 10:30:24",
      "score": 192,
      "num_comments": 13,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qiug46/finetuned_qwen3_06b_for_text2sql_using_a_claude/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0up0ey",
          "author": "cirejr",
          "text": "This is great, I've been trying to make this text2sql happen for couple of weeks now using lightweight models. And I have to say without fine tuning them it's really something üòÖ. I tried couple of ways, giving functionGemma bunch of tools. Using some 3b models and giving and creating a Neon mcp client but yeah I guess fine tuning is all that's left.",
          "score": 9,
          "created_utc": "2026-01-21 13:37:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0utcy8",
              "author": "party-horse",
              "text": "Awsome, feel free to use the claude skill to train a model for your specific domain/dialect!",
              "score": 1,
              "created_utc": "2026-01-21 14:00:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0wnup2",
          "author": "jlugao",
          "text": "How did you come up with the datasets for training and evaluating? I am thinking of doing a similar project for evaluating execution plans and coming up with recommendations",
          "score": 3,
          "created_utc": "2026-01-21 19:06:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0x6gmz",
              "author": "party-horse",
              "text": "I chatted with a few LLMs to get example conversations. Fortunately you only need approx 20 to get started so its pretty easy",
              "score": 4,
              "created_utc": "2026-01-21 20:30:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ip6g9",
          "author": "Sairefer",
          "text": "    4. Upload data\n    ...\n    All local.\n    ...\n    Hmmm...",
          "score": 2,
          "created_utc": "2026-01-24 23:26:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tg0cq",
              "author": "party-horse",
              "text": "The trained model is all local, the training itself is in the cloud since its hard to get large enough instances locally.",
              "score": 1,
              "created_utc": "2026-01-26 13:50:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zezv0",
          "author": "Puzzled_Fisherman_94",
          "text": "Thx for the tutorial",
          "score": 1,
          "created_utc": "2026-01-22 03:30:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14ig60",
          "author": "_RemyLeBeau_",
          "text": "\"all local\"\n\nIf this were true, you could just share the skill and not have to¬†distil login",
          "score": 1,
          "created_utc": "2026-01-22 21:46:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tg1dq",
              "author": "party-horse",
              "text": "The trained model is all local, the training itself is in the cloud since its hard to get large enough instances locally.",
              "score": 1,
              "created_utc": "2026-01-26 13:50:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1aakp1",
          "author": "Odd-Photojournalist8",
          "text": "Would be cool to do one that could integrate `ctibutler` and a few KEV reputable sources. Then a bigger model ask detailed queries asking small fine tuned model(cheap) to extract correlated set of data. Cyber security basics using AI",
          "score": 1,
          "created_utc": "2026-01-23 18:41:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tg31d",
              "author": "party-horse",
              "text": "Makes sense, will take a look into this :)",
              "score": 2,
              "created_utc": "2026-01-26 13:51:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1qgzdk",
          "author": "ComedianObjective572",
          "text": "TBH if you have a background prompt on an LLM I think the output you will get will still be correct without training the model. It would need more inference but either way you don‚Äôt need to fine tune the model for it to be correct you might just need 1 shot prompts etc.",
          "score": 1,
          "created_utc": "2026-01-26 01:24:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tg842",
              "author": "party-horse",
              "text": "I agree you can go very far with prompt engineering. I am trying to showcase that training the models is also a very powerful technique and can let you go beyond jutst prompting!",
              "score": 1,
              "created_utc": "2026-01-26 13:51:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qj3b01",
      "title": "[Open Sourse] I built a tool that forces 5 AIs to debate and cross-check facts before answering you",
      "subreddit": "ollama",
      "url": "https://i.redd.it/ja8et3degqeg1.jpeg",
      "author": "S_Anv",
      "created_utc": "2026-01-21 16:55:27",
      "score": 56,
      "num_comments": 15,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qj3b01/open_sourse_i_built_a_tool_that_forces_5_ais_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0wv1t2",
          "author": "atika",
          "text": "Wasn‚Äôt enough to ‚Äúmake‚Äù them debate, you had to ‚Äúforce‚Äù them against their wishes?",
          "score": 8,
          "created_utc": "2026-01-21 19:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0wvpic",
              "author": "Nerdinat0r",
              "text": "Not to mention the overhead. How much more RAM and electricity and GPU Time this uses‚Ä¶",
              "score": 1,
              "created_utc": "2026-01-21 19:41:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0x5eoh",
          "author": "Basic_Young538",
          "text": "This could be really funny...",
          "score": 6,
          "created_utc": "2026-01-21 20:25:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14nk2z",
              "author": "EXPATasap",
              "text": "It can be that and you can get them to be SO DARK and twisted to, think Dark Eldar maxed out lol! I‚Äôve had a pyqt app I built six months ago that does this with local/external models, I use Jinja templates which gives me all the control I‚Äôd ever want and need with per turn reinforcement system messages, custom to each turn/model. lol. It‚Äôs crazy effective.",
              "score": 1,
              "created_utc": "2026-01-22 22:11:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o11ec2n",
          "author": "butterninja",
          "text": "You gave me an idea. I will waterboard the crap out of 5 AIs to debate and cross-check facts before answering you. Give me a bit of time.",
          "score": 3,
          "created_utc": "2026-01-22 12:55:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0x81b9",
          "author": "Faisal_Biyari",
          "text": "If I use this between the same model, does that mean it is effectively converted into a \"thinking\" model?",
          "score": 1,
          "created_utc": "2026-01-21 20:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0y1t4a",
          "author": "ServeAlone7622",
          "text": "This is the way!\n\nI love that you built an ensemble setup too!\nI could never get this to work for me in a real sense. Accuracy and a lack of ability or possibly desire to critique one another leading into a giant circle jerk of AI handshaking.\n\nHow‚Äôs your accuracy? Do they argue back and forth or do they mostly just circle jerk each other?",
          "score": 1,
          "created_utc": "2026-01-21 22:57:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o106gle",
          "author": "upboat_allgoals",
          "text": "multi turn zero shot ensembling? \n\nnice tool. worth running it through some benchmarks.",
          "score": 1,
          "created_utc": "2026-01-22 06:44:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17o1qa",
          "author": "Fabulous-Speech6593",
          "text": "This is good because Chat-GPT is lying its ass off too often, it should be standard to regulate these lying ass Ai confuse models üòÅ!",
          "score": 1,
          "created_utc": "2026-01-23 09:59:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17rmoo",
          "author": "usercantollie",
          "text": "You're a genius, OP! This is exactly what the AI world needs right now, forcing multiple models to debate and cross-check facts is the only way to actually maximize productivity without drowning in hallucinations.\n\nMost people are just blindly trusting single-model outputs, but you're actually solving the real problem.",
          "score": 1,
          "created_utc": "2026-01-23 10:31:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17sweo",
          "author": "milli_xoxxy",
          "text": "Honestly, this is brilliant idea! Making five Als argue with each other to find the truth is a game-changer. Also running it locally with Ollama is the real win. This is the future of reliable Al, not just another API wrapper.",
          "score": 1,
          "created_utc": "2026-01-23 10:43:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17xw2s",
          "author": "Eugene_sh",
          "text": "I used \"chorus\" app just for that but they smh abandoned the project it seems",
          "score": 1,
          "created_utc": "2026-01-23 11:25:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15f7m5",
          "author": "NoxinDev",
          "text": "I love it, waste 5 times as much money from the AI speculation companies - way to help burst this bubble, just fantastic.",
          "score": 1,
          "created_utc": "2026-01-23 00:37:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10v8di",
          "author": "390adv",
          "text": "Awesome. Post the results of the Holocaust question",
          "score": 0,
          "created_utc": "2026-01-22 10:30:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1b981e",
          "author": "Low-Coconut5857",
          "text": "‚Äù5 person that debates and cross-checks facts before answering you.‚Äù Sounds like the weekly meeting I have with my team‚Ä¶  except the part about providing an answer.\nI think I skip.",
          "score": 0,
          "created_utc": "2026-01-23 21:23:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnersl",
      "title": "I built a \"Spatial\" website for Ollama because I hate linear chats. (Local-first, no DB)",
      "subreddit": "ollama",
      "url": "https://v.redd.it/utanig3onofg1",
      "author": "yibie",
      "created_utc": "2026-01-26 12:31:18",
      "score": 56,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qnersl/i_built_a_spatial_website_for_ollama_because_i/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1tr7v8",
          "author": "General_Sandwich_353",
          "text": "what's the license? this is an interesting concept and I'd like to know if it's alright to reuse the code and under what terms",
          "score": 2,
          "created_utc": "2026-01-26 14:47:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ttx56",
              "author": "yibie",
              "text": "GPLv3.",
              "score": 4,
              "created_utc": "2026-01-26 15:00:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1tkbuu",
          "author": "zelkovamoon",
          "text": "This is a really cool idea I think. Nice work, I'll be trying it.",
          "score": 1,
          "created_utc": "2026-01-26 14:13:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ttulo",
              "author": "yibie",
              "text": "Thank you. LOL",
              "score": 1,
              "created_utc": "2026-01-26 15:00:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vieiy",
          "author": "ddoice",
          "text": "Great idea!",
          "score": 1,
          "created_utc": "2026-01-26 19:21:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xoqge",
              "author": "yibie",
              "text": "Thank you\n\n![gif](giphy|pAHAgWYYjWIE9DNLcC)",
              "score": 1,
              "created_utc": "2026-01-27 01:33:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vmav4",
          "author": "austrobergbauernbua",
          "text": "I really like it! It would be cool if it would arrange the notes automatically based on a similarity -> embedding clustering for example -> add a label. Only as an additional view.\n\n  \nCould also be used for writing Business Model Canvas or Lean Canvas for programming projects.\n\nKeep up the good work! Maybe I am motivated to contribute :)",
          "score": 1,
          "created_utc": "2026-01-26 19:38:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1za668",
              "author": "yibie",
              "text": "https://i.redd.it/3gcuy2s8lufg1.gif\n\nHi, do you mean this?",
              "score": 2,
              "created_utc": "2026-01-27 07:53:55",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1wvlj5",
              "author": "yibie",
              "text": "Thank you. This project was indeed designed from the beginning to allow users to talk directly to AI in an environment similar to Business Model Canvas.\n\nI find it interesting about the auto-arrange feature you mentioned, but I don't quite understand what this addtional view is, thanks for your attention.",
              "score": 1,
              "created_utc": "2026-01-26 23:02:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qj3qkv",
      "title": "Hi folks, I‚Äôve built an open‚Äësource project that could be useful to some of you",
      "subreddit": "ollama",
      "url": "https://i.redd.it/4plhjok3jqeg1.png",
      "author": "panos_s_",
      "created_utc": "2026-01-21 17:10:42",
      "score": 48,
      "num_comments": 11,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qj3qkv/hi_folks_ive_built_an_opensource_project_that/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0x1sha",
          "author": "jovn1234567890",
          "text": "Very useful thank you",
          "score": 2,
          "created_utc": "2026-01-21 20:09:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0xt8z7",
              "author": "panos_s_",
              "text": "thanks mate!",
              "score": 1,
              "created_utc": "2026-01-21 22:14:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0xgfv7",
          "author": "mofa1",
          "text": "I just want to say that I like the project and have been using it for some time in my Unraid system!",
          "score": 2,
          "created_utc": "2026-01-21 21:15:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0xsofs",
              "author": "panos_s_",
              "text": "thanks a lot :)",
              "score": 1,
              "created_utc": "2026-01-21 22:12:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ybfg1",
          "author": "selfdestroyer",
          "text": "I also have been running this for a month or so and it‚Äôs been great to monitor while using OpenwebUI and ConfyUI.",
          "score": 1,
          "created_utc": "2026-01-21 23:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15c5xk",
              "author": "panos_s_",
              "text": "thanks a lot :)",
              "score": 2,
              "created_utc": "2026-01-23 00:21:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o10jxgg",
          "author": "UseHopeful8146",
          "text": "Stumbling blind into the world of GPU compute and am very appreciative",
          "score": 1,
          "created_utc": "2026-01-22 08:44:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15e99z",
          "author": "IllustriousMessage79",
          "text": "I love you",
          "score": 1,
          "created_utc": "2026-01-23 00:32:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15ejne",
              "author": "panos_s_",
              "text": ":)",
              "score": 2,
              "created_utc": "2026-01-23 00:34:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o15yotd",
          "author": "960be6dde311",
          "text": "Nice design. Cool little project",
          "score": 1,
          "created_utc": "2026-01-23 02:26:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gtxd0",
          "author": "Old-Wolverine-4134",
          "text": "You mean AI made it for you",
          "score": 1,
          "created_utc": "2026-01-24 18:12:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkokhv",
      "title": "Ollama Image Generator",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qkokhv/ollama_image_generator/",
      "author": "nickinnov",
      "created_utc": "2026-01-23 11:40:26",
      "score": 47,
      "num_comments": 21,
      "upvote_ratio": 0.93,
      "text": "Hey fellow Ollama fans, I'm delighted that image generation is available so I have written a web app you can run on your own computer (alongside Ollama) to make it easier to generate, save and delete images.\n\nOK it ain't no ComfyUI but makes things tidier and, unlike using the terminal Ollama CLI, images don't clutter up your home folder!  \nRepo at [https://github.com/nicklansley/OllamaImageGenerator](https://github.com/nicklansley/OllamaImageGenerator)\n\n... but all you really need to download is:\n\n* [**server.py**](https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/server.py) \\- small proxying server which you can run without extra packages on your machine as long as you have python 3.9 or higher. No need for a venv as I've just used built-in packages like *http.server* \\- run from terminal as ***python3*** [***server.py***](https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/server.py)\n* [index.html](https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/index.html) \\- does all the grunt work in your web browser on port 8080. Generated images are saved to local storage along with generation settings, and can be deleted individually. Once [server.py](https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/server.py) is up and running, take your web browser to [http://localhost:8080](http://localhost:8080)\n\n[README.md](https://github.com/nicklansley/OllamaImageGenerator/blob/main/README.md) gives more info but quick instructions:\n\n* Download an image generation model in terminal - currently '***ollama pull x/z-image-turbo:bf16***' and '***ollama pull x/flux2-klein:latest***' are supported.\n* Type a prompt, set image width and height, choose a seed and the number of steps, then click 'Generate Image'.\n* During image progression, a 'step N of X' message appears to denote progress.\n* Images are saved to a side panel (actually they are in localStorage so they survive from one session to the next).\n* Save an image onto your machine with right-click 'Save Image As..' or drag if out of the main window and into a folder.\n* Double click a saved image to move it to the main window along with the settings that created it (the image can be recreated as long as seed 0 (Ollama internal random seed) was not used.\n* Single click an image then click 'x' to delete it. The image is removed from the image list saved in localStorage.\n\nWhen more image models become available:\n\n* Download a model with 'image pull image\\_gen\\_model\\_name:tag'\n* Update image list variable IMAGE\\_GEN\\_MODEL\\_LIST with the same model name and tag at the top of index.html\n\n[Screenshot of Ollama Image Generator](https://preview.redd.it/n8kap3a823fg1.png?width=1369&format=png&auto=webp&s=accb2258e20e62c356823ad8b85439aab8424f4a)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qkokhv/ollama_image_generator/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1867zs",
          "author": "vini_stoffel",
          "text": "Only in Mac?",
          "score": 3,
          "created_utc": "2026-01-23 12:26:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o187csu",
              "author": "simplir",
              "text": "Image generation on Ollama is still only Mac at the moment",
              "score": 3,
              "created_utc": "2026-01-23 12:34:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1cna9u",
                  "author": "FlyByPC",
                  "text": "Are there plans to add this to the Windows client anytime soon?",
                  "score": 1,
                  "created_utc": "2026-01-24 01:46:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o18aaup",
          "author": "planetearth80",
          "text": "Doesn‚Äôt the Ollama GUI do the same thing?",
          "score": 3,
          "created_utc": "2026-01-23 12:53:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18y01i",
              "author": "nickinnov",
              "text": "Not (yet) - it only understands /api/chat",
              "score": 6,
              "created_utc": "2026-01-23 15:00:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o182vmk",
          "author": "Narrow-Impress-2238",
          "text": "Is that work for windows too already?",
          "score": 1,
          "created_utc": "2026-01-23 12:03:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18yd0f",
              "author": "nickinnov",
              "text": "BY all means give it a go -  [server.py](http://server.py) will work fine as long as you have Python 3.9 or later, but you may need to check the Ollama app for Windows. At its heart [server.py](http://server.py) simply proxies the Ollama API (avoids CORS) plus has its own history saving of images capability.",
              "score": 2,
              "created_utc": "2026-01-23 15:01:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o18atuv",
          "author": "Birdinhandandbush",
          "text": "So why no quantized Z-Image models? that one is like 32gb. At least Klien will work for me",
          "score": 1,
          "created_utc": "2026-01-23 12:56:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18ykmq",
              "author": "nickinnov",
              "text": "Out of my control I'm afraid! Klein is very fast and works well (I am using Apple Metal GPU).",
              "score": 2,
              "created_utc": "2026-01-23 15:03:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o18zacd",
          "author": "nickinnov",
          "text": "UPDATE: Yes so soon used up local storage! Now saves to a history folder that [server.py](http://server.py) creates in its own folder instead. Who knew that web browser local storage can only save about 10MB per web site. Not me it seemed... üòë",
          "score": 1,
          "created_utc": "2026-01-23 15:06:31",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o190nlk",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-01-23 15:13:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o198s10",
              "author": "nickinnov",
              "text": "I agree - like much of Ollama it is simplified to work cleanly without any complexity. Comfy is absolutely at the opposite spectrum. However, I think it's useful to have a simple tool that can be used with little setup. And I think the images are pretty good for all that lack of ComfyUI fine tuning.",
              "score": 1,
              "created_utc": "2026-01-23 15:50:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o19g4tv",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-01-23 16:23:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o19dd35",
          "author": "Total-Context64",
          "text": "You should take a look at [ALICE](https://github.com/SyntheticAutonomicMind/ALICE), it's pretty advanced and can already do this without Ollama.  I'm curious though, what does Ollama add over just using the diffusion models directly?",
          "score": 1,
          "created_utc": "2026-01-23 16:11:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a8y11",
              "author": "nickinnov",
              "text": "For me it's about Ollama's simplicity (with this and all its models). It abstracts away model loading and inference for local 'offline' use. If anyone wants to be more serious about image generation then really Comfy-UI is really the best way to go.",
              "score": 2,
              "created_utc": "2026-01-23 18:34:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1abbw7",
                  "author": "Total-Context64",
                  "text": "Ahh, fair enough.  With ALICE you do need to set up the backend, ROCm or whatever but that's mostly automated.  I created it to interface with SAM so I can use AI assistants to generate images using old hardware that I just had sitting around.  It works very well, and it integrates with HuggingFace and CivitAI directly so you can download models right from the web interface.  It even has LoRA support.",
                  "score": 1,
                  "created_utc": "2026-01-23 18:44:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ytr0z",
                  "author": "ZeroSkribe",
                  "text": "This",
                  "score": 1,
                  "created_utc": "2026-01-27 05:39:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o19z2wl",
          "author": "Euphoric-Tank-6791",
          "text": "what OS? it does not seem to run on w11 wsl2 as of last night",
          "score": 1,
          "created_utc": "2026-01-23 17:50:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a9brw",
              "author": "nickinnov",
              "text": "MacOS only at present for image generation - a limitation imposed by Ollama itself. I'm sure they are working on Windows and Linux though. I suspect they are developing Ollama on Macs üòÅ",
              "score": 1,
              "created_utc": "2026-01-23 18:35:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1bti1g",
          "author": "CoDMplayer_",
          "text": "Any image+prompt->image capability?",
          "score": 1,
          "created_utc": "2026-01-23 23:02:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e4utp",
              "author": "nickinnov",
              "text": "Yes I'd like that too - plus text encoders + loras! I suspect their curren work will be to make image gen work on Windows and Linux (Mac only right now). But hey they read this r/ollama stuff so hopefully they are creating a backlog that includes image+prompt->image too (please!).",
              "score": 1,
              "created_utc": "2026-01-24 08:01:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qkrpnx",
      "title": "I gave my local LLM pipeline  a brain - now it thinks before it speaks",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qkrpnx/i_gave_my_local_llm_pipeline_a_brain_now_it/",
      "author": "danny_094",
      "created_utc": "2026-01-23 14:06:41",
      "score": 39,
      "num_comments": 7,
      "upvote_ratio": 0.85,
      "text": "Jarvis/TRION has received a major update after weeks of implementation. Jarvis (soon to be TRION) has now been provided with a self-developed SEQUENTIAL THINKING MCP.\n\nI would love to explain everything it can do in this Reddit post. But I don't have the space, and neither do you have the patience. u/frank_brsrk Provided a self-developed CIM framework That's hard twisted with Sequential Thinking. So Claude help for the answer:\n\nüß† Gave my local Ollama setup \"extended thinking\" - like Claude, but 100% local\n\nTL;DR: Built a Sequential Thinking system that lets DeepSeek-R1\n\n\"think out loud\" step-by-step before answering. All local, all Ollama.\n\nWhat it does:\n\n\\- Complex questions ‚Üí AI breaks them into steps\n\n\\- You SEE the reasoning live (not just the answer)\n\n\\- Reduces hallucinations significantly\n\nThe cool part: The AI decides WHEN to use deep thinking.\n\nSimple questions ‚Üí instant answer.\n\nComplex questions ‚Üí step-by-step reasoning first.\n\nBuilt with: Ollama + DeepSeek-R1 + custom MCP servers\n\nShoutout to u/frank_brsrk for the CIM framework that makes\n\nthe reasoning actually make sense.\n\nGitHub: [https://github.com/danny094/Jarvis/tree/main](https://github.com/danny094/Jarvis/tree/main)\n\nHappy to answer questions! This took weeks to build üòÖ\n\nOther known issues:\n\n\\- excessively long texts, skipping the control layer - Solution in progress\n\n\\- The side panel is still being edited and will be integrated as a canvas with MCP support.\n\n  \n\n\nhttps://reddit.com/link/1qkrpnx/video/zb6z5muax3fg1/player\n\nhttps://preview.redd.it/el6uhfy6q4fg1.png?width=1147&format=png&auto=webp&s=a16a9525fc50ba59b710f6932cdb3626c2562074\n\nhttps://preview.redd.it/j1ol6fy6q4fg1.png?width=863&format=png&auto=webp&s=f7726aee3e5079419dc665959fc0b779b6d37571\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qkrpnx/i_gave_my_local_llm_pipeline_a_brain_now_it/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o18q7ov",
          "author": "frank_brsrk",
          "text": "Here above is the architecture of the causal intelligence module that decouples thought and augments reasoning on demand. Thanks u/danny094 for the trust. \" Everyday is the day that the project should be finished\" :D\n\nhttps://preview.redd.it/cr7y0qimy3fg1.png?width=2800&format=png&auto=webp&s=3fa05b50978d7f2ea14c2ae7854c682db888331d\n\ncompliments and from the bottom of my heart's void all the best!!! :",
          "score": 5,
          "created_utc": "2026-01-23 14:20:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eoboz",
              "author": "Batinium",
              "text": "Pic Quality too low can't read. On what tool did you prepare them?",
              "score": 2,
              "created_utc": "2026-01-24 10:59:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1eurxj",
                  "author": "frank_brsrk",
                  "text": "On draw.io, I will pubblish soon the open source with polished version for n8n template. Otherwise send dm ur email and i will send u the diagram in higher quality",
                  "score": 1,
                  "created_utc": "2026-01-24 11:56:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1evb3j",
                  "author": "Awkward--Panda",
                  "text": "I guess it's readable here: https://www.reddit.com/r/LocalLLM/s/Hf6UO4Laeo\n\n(via Google picture search. I hope it reflects the actual content)",
                  "score": 1,
                  "created_utc": "2026-01-24 12:00:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1q603c",
          "author": "dropswisdom",
          "text": "I'm building the docker image locally now. However, I'd love a online maintained docker image. and please fix your discord server. it is invalid (from the git repo page).",
          "score": 1,
          "created_utc": "2026-01-26 00:29:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qa2v6",
          "author": "dropswisdom",
          "text": "Getting bad request from fastmcp (192.168.128.9:56052 - \"POST /mcp HTTP/1.1\" 400 Bad Request), communicates in german without an option to change interface language, and goes into a loop. oh, and can't use system prompt.. there's some (A LOT) of work to do.",
          "score": 1,
          "created_utc": "2026-01-26 00:49:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qzos9",
              "author": "danny_094",
              "text": "**In short, to avoid any further misunderstandings:** Jarvis is¬†**not**¬†intended to replace existing web UIs. Jarvis is a¬†**pipeline / bridge layer**¬†that sits between¬†**Ollama and any web UI**, allowing advanced routing, memory, agents, and processing. The¬†**Jarvis Web UI**¬†itself is an¬†**administration and control panel**, not a full chat UI, and will be expanded gradually over time. There will definitely be more bugs  this is an evolving system. I‚Äôm grateful to every user who runs into them and takes the time to report them. :)",
              "score": 1,
              "created_utc": "2026-01-26 02:58:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjqcqv",
      "title": "Built an open-source, self-hosted AI agent automation platform ‚Äî feedback welcome",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qjqcqv/built_an_opensource_selfhosted_ai_agent/",
      "author": "Feathered-Beast",
      "created_utc": "2026-01-22 09:44:30",
      "score": 36,
      "num_comments": 18,
      "upvote_ratio": 0.95,
      "text": "Hey folks üëã\n\nI‚Äôve been building an open-source, self-hosted AI agent automation platform that runs locally and keeps all data under your control. It‚Äôs focused on agent workflows, scheduling, execution logs, and document chat (RAG) without relying on hosted SaaS tools.\n\nI recently put together a small website with docs and a project overview.\n\nLinks to the website and GitHub are in the comments.\n\nWould really appreciate feedback from people building or experimenting with open-source AI systems üôå",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qjqcqv/built_an_opensource_selfhosted_ai_agent/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o10q88s",
          "author": "Feathered-Beast",
          "text": "Github:- https://github.com/vmDeshpande/ai-agent-automation\n\nWebsite:- https://vmdeshpande.github.io/ai-automation-platform-website/",
          "score": 9,
          "created_utc": "2026-01-22 09:44:43",
          "is_submitter": true,
          "replies": [
            {
              "id": "o11tpn1",
              "author": "cirejr",
              "text": "Curious to know, did you build the website and docs with Gemini-3 ?",
              "score": 2,
              "created_utc": "2026-01-22 14:20:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o11uotu",
                  "author": "Feathered-Beast",
                  "text": "Nope. I built the website myself and wrote the docs manually.\nI did my own research, took feedback from other developers, and documented each feature step by step.\nThis is my first open-source project, so I wanted to do it properly.",
                  "score": 2,
                  "created_utc": "2026-01-22 14:25:17",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o10uvij",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 2,
          "created_utc": "2026-01-22 10:27:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10wmtx",
              "author": "Feathered-Beast",
              "text": "Yes i do have screenshot\n\nhttps://preview.redd.it/ds7l369wqveg1.jpeg?width=831&format=pjpg&auto=webp&s=19ed72d8ced71008629fab414d5a61d9fcdf859f",
              "score": 2,
              "created_utc": "2026-01-22 10:43:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1112bx",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-01-22 11:20:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21hn4k",
          "author": "SpkyBdgr",
          "text": "This is... exactly what I was looking for. You came up in the top results on google for \"ai and automation\"! Can't believe you only posted 5 days ago.. I'm new to local AI stuff but I've been putting it off. This is EXACTLY the type of platform I was dreaming of! I will report back and hopefully find ways to contribute. Thanks OP!",
          "score": 2,
          "created_utc": "2026-01-27 16:28:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21in4z",
              "author": "Feathered-Beast",
              "text": "Wow, that honestly made my day - thank you!  The project is still very early, so feedback like this really helps shape where it goes next. Take your time exploring, and if you run into anything confusing or have ideas, I‚Äôd love to hear them. Contributions are very welcome!",
              "score": 1,
              "created_utc": "2026-01-27 16:32:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o127gra",
          "author": "JakeMascaOfficial",
          "text": "This looks amazing",
          "score": 1,
          "created_utc": "2026-01-22 15:28:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o129bd8",
              "author": "Feathered-Beast",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-01-22 15:37:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o14hprs",
          "author": "Known-Maintenance-83",
          "text": "Will test it tomorrow can we contribute?",
          "score": 1,
          "created_utc": "2026-01-22 21:42:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16cqra",
              "author": "Feathered-Beast",
              "text": "Yes, contributions are welcome!",
              "score": 1,
              "created_utc": "2026-01-23 03:45:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o15a6wc",
          "author": "looktwise",
          "text": "it builts a RAG locally?? would it be possible to switch RAGs if I want to separate distinguish content? ",
          "score": 1,
          "created_utc": "2026-01-23 00:11:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16d10g",
              "author": "Feathered-Beast",
              "text": "Yes, the RAG runs fully locally.\nRight now it uses a single vector store, but switching between multiple RAGs (or separate knowledge bases per domain) is definitely possible and planned.\n\nThe architecture already supports isolating content; it‚Äôs mostly a config / routing layer on top.",
              "score": 2,
              "created_utc": "2026-01-23 03:47:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o17dox7",
                  "author": "looktwise",
                  "text": "Yeah, separate knowledge domains could be a term for that. I would be very interested if the chosen option of a 'RAG-file' would allow to chat with content 1 in RAG-file 1 and then or in a parallel session would allow a separated chat in RAG-file 2. (think in terms of 2 different schools of thought in philosophy for example, RAG-files filled by several book-PDFs, OCR ready).",
                  "score": 1,
                  "created_utc": "2026-01-23 08:22:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ai5o7",
          "author": "dephraiiim",
          "text": "That's awesome you're building this! For scheduling and coordinating agent workflows, you might want to check out [weekday.so](http://weekday.so) ;  it's an open-source calendar alternative with AI capabilities that keeps everything self-hosted and under your control. Could be a solid complement to your platform.",
          "score": 1,
          "created_utc": "2026-01-23 19:15:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnjk4p",
      "title": "SHELLper üêö: Qwen3 0.6B for More Reliable Multi-Turn Function Calling",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qnjk4p/shellper_qwen3_06b_for_more_reliable_multiturn/",
      "author": "gabucz",
      "created_utc": "2026-01-26 15:44:43",
      "score": 30,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "We fine-tuned a 0.6B model for converting English to executable bash commands. It's small enough to run locally on your laptop, giving you full data privacy.\n\nMulti-turn tool calling is incredibly challenging for small models - before fine-tuning, Qwen3-0.6B had 84% single-call accuracy, which collapses to **only 42% across 5 turns**! After our tuning, it reaches 100% on our test set, providing dependable multi-turn capabilities.\n\n|Model|Parameters|Tool call accuracy (test set)|=> 5-turn tool call accuracy|\n|:-|:-|:-|:-|\n|Qwen3 235B Instruct (teacher)|235B|99%|95%|\n|Qwen3 0.6B (base)|0.6B|84%|42%|\n|**Qwen3 0.6B (tuned)**|**0.6B**|**100%**|**100%**|\n\nRepo: [https://github.com/distil-labs/distil-SHELLper](https://github.com/distil-labs/distil-SHELLper)\n\nHuggingface model: [https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper](https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper)\n\n# Quick Start\n\nSet up the environment:\n\n    # Set up environment\n    python -m venv .venv\n    . .venv/bin/activate\n    pip install openai huggingface_hub\n    \n\nDowload the model:\n\n    hf download distil-labs/distil-qwen3-0.6b-SHELLper --local-dir distil_model\n    cd distil_model\n    ollama create distil_model -f Modelfile\n    cd ..\n    \n\nRun the assistant:\n\n    python filesystem_demo.py\n    \n\nThe demo asks for permission before executing commands (for safety) and restricts dangerous operations (like `rm -r /`), so don't hesitate to try it!\n\n# How We Trained SHELLper\n\n# The Problem\n\nMulti-turn tool calling is exceptionally hard for small models - performance breaks down as tool calls chain, degrading with each turn. If prediction errors are independent (e.g. due to bad parameter values), an 80% accurate model has just a 33% chance of succeeding across 5 turns.\n\n|Single tool call accuracy|5-turn tool call accuracy|\n|:-|:-|\n|80%|33%|\n|90%|59%|\n|95%|77%|\n|99%|95%|\n\nIn this demo, we explored whether we could substantially boost a small model's multi-turn performance. We picked a task from the [Berkeley function calling leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html) \\- the [gorilla file system tool calling task](https://github.com/ShishirPatil/gorilla/blob/main/berkeley-function-call-leaderboard/bfcl_eval/data/BFCL_v4_multi_turn_base.json). Our modifications were:\n\n* The original allows multiple tool calls per assistant turn ‚Üí we permit only one\n* Maximum 5 turns\n* Commands map to real bash (not gorilla filesystem functions)\n* Tool call outputs aren't added to the conversation history\n\nIn short, an identical tool set, but simpler [train/test data.](https://github.com/distil-labs/distil-SHELLper/tree/main/data)\n\n# Training Pipeline\n\n1. **Seed Data:** We wrote 20 simplified training conversations that span the available tools while remaining reasonably realistic.\n2. **Synthetic Expansion:** Through our [data synthesis pipeline](https://www.distillabs.ai/blog/small-expert-agents-from-10-examples/?utm_source=github&utm_medium=referral&utm_campaign=shellper), we scaled to thousands of examples.\n\nFor handling variable conversation lengths, we split each conversation into subsets ending with a tool call. For instance:\n\n    [Input] User: List all files => Model: ls -al => User: go to directory models\n    [Output] Model: cd models\n    \n\n... becomes 2 training points:\n\n    [Input] User: List all files\n    [Output] Model: ls -al\n    \n\n    [Input] User: List all files => Model: ls -al => User: go to directory models\n    [Output] Model: cd models`\n    \n\n1. **Fine-tuning:** We went with **Qwen3-0.6B** as the [best fine-tunable sub-1B](https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning) model on our platform that handles tool calling.\n\n# Usage Examples\n\nThe assistant takes natural language input, generates bash commands, and optionally runs them (after Y/N confirmation).\n\n**Basic filesystem operations**\n\n    > python filesystem_demo.py\n    \n    USER: List all files in the current directory\n    COMMAND: ls\n    USER: Create a new directory called test_folder\n    COMMAND: mkdir test_folder\n    USER: Navigate to test_folder COMMAND: cd test_folder\n    \n\n# Limitations and Next Steps\n\nRight now, we're limited to a basic bash tool set:\n\n* no pipes, compound commands, or multiple tool calls per turn\n* no validation of invalid commands/parameters\n* 5-turn conversation maximum\n\nWe prioritized getting the simple case right before expanding to more complex scenarios. Up next: support for multiple tool calls (enabling more sophisticated agent workflows) and benchmarking on [BFCL](https://gorilla.cs.berkeley.edu/leaderboard.html).\n\nIf you're using this in your bash workflows, track failing commands, append them to `data/train.jsonl`, and retrain with the updated dataset (or experiment with a larger student model!).\n\n# Discussion\n\nInterested to hear from the community:\n\n* Is anyone else working on fine-tuning small models for multi-turn tool calling?\n* What other \"narrow but valuable\" tasks could benefit from local, privacy-preserving models?\n\nLet us know your thoughts!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qnjk4p/shellper_qwen3_06b_for_more_reliable_multiturn/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1ufkep",
          "author": "sinan_online",
          "text": "Oh wow, I think I‚Äôll write a wrapper for this! Unless you did so already.",
          "score": 3,
          "created_utc": "2026-01-26 16:36:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ugi9e",
          "author": "sinan_online",
          "text": "I just want to add two things: I definitely believe that narrow-but-valuable is the way to go. Niche needs create businesses. The VC world has gone too far into this pointless philosophical chase into AGI.\n\nI also use Gemma3 270M for local testing, it works surprisingly well, so far. I haven‚Äôt gone into tool calling yet. It runs sufficiently fast without a GPU on the MacBook. Qwen3 1.7B is great on a years-old 6GB machine. So there is actually quite a bit of ‚Äúnarrow-but-valuable‚Äù possibilities for people who own a regular piece of equipment, if the implementation is done.",
          "score": 3,
          "created_utc": "2026-01-26 16:40:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1up3j9",
              "author": "gabucz",
              "text": "Yeah, I think local setup has a huge potential for small projects especially :) and not only that, I feel like you don't need a huge LLM for most narrow tasks\n\n\nGemma wasn't that great for function calling last time we tried it. But recently they released function gemma, we plan to try it out",
              "score": 2,
              "created_utc": "2026-01-26 17:17:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uxcin",
          "author": "mangoskive",
          "text": "I'm a total beginner and amateur, but I'm currently about to dive into fine-tuning Gemma 3 1b for exactly this, in a swarm of gemma 1b models. I've been thinking about maybe running three in parallel with temp variations, prompt variations or some other parameter tweaking for variation, with validation and judging by the same model. I was considering fine-tuning functiongemma, running it on the CPU, but I decided to work with only the 1b for easy vLLM setup and less complexity. I am pondering about how to fine tune, have been thinking about self-play maybe? I'm primarily just learning and trying to just have some fun with it. See the little agents at work.",
          "score": 2,
          "created_utc": "2026-01-26 17:53:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1za8ua",
              "author": "gabucz",
              "text": "Self-play is an interesting suggestion - we generated data via a much stronger model, but maybe you could make the small model progressively refine itself if the task is limited enough! I'd be curious to know how it went :) \n\nTool calling itself is a bit tough - earlier models (including gemma) weren't really pre-trained to do that, and so you mostly do it via instructions in the prompt. This could limit how good the model is for you, because it needs to learn both the tool calling format AND the task itself. That's why we went with the Qwen family, because these had tool calling in pre-training (and Llama 3.2 can also do it, although I'm not sure how well/if it does multi-turn tools).\n\nI'm not saying you can't do gemma, but if your performance isn't great, this could be one of the reasons. Qwen3 and Llama3.2 have vLLM integration too in case you want to try multiple families (or any particular reason why you want gemma? :) )",
              "score": 1,
              "created_utc": "2026-01-27 07:54:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1xmnm9",
          "author": "sinan_online",
          "text": "I want to try. Is there an easy way to get it running with Ollama? \n\nIf you say yes, I will proceed to use ChatGPT to do that.\n\nIf you say no, I will turn your venv into a container, expose an endpoint, and try it that way‚Ä¶",
          "score": 1,
          "created_utc": "2026-01-27 01:22:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xmtal",
              "author": "sinan_online",
              "text": "Never mind, I see it in the instructions!",
              "score": 1,
              "created_utc": "2026-01-27 01:23:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1z9l3e",
                  "author": "gabucz",
                  "text": "Nice! Reach out in case you need help or if anything's unclear in the instructions :)",
                  "score": 1,
                  "created_utc": "2026-01-27 07:48:38",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1z8mrg",
          "author": "BombardierComfy",
          "text": "Really clear and much needed!\nGreat post thx!",
          "score": 1,
          "created_utc": "2026-01-27 07:40:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21j2vi",
          "author": "cirejr",
          "text": "Hi, is it better than a fine-tuned functiongemma ?",
          "score": 1,
          "created_utc": "2026-01-27 16:34:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qiv7v8",
      "title": "I built a CLI tool using Ollama (nomic-embed-text) to replace grep with Semantic Code Search",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qiv7v8/i_built_a_cli_tool_using_ollama_nomicembedtext_to/",
      "author": "Technical_Meeting_81",
      "created_utc": "2026-01-21 11:16:14",
      "score": 25,
      "num_comments": 2,
      "upvote_ratio": 0.96,
      "text": "Hi r/ollama,\n\nI've been working on an open-source tool called **GrepAI**, and I wanted to share it here because it relies heavily on **Ollama** to function.\n\n**What is it?** GrepAI is a CLI tool (written in Go) designed to help AI agents (like Claude Code, Cursor, or local agents) understand your codebase better.\n\nInstead of using standard regex `grep` to find code‚Äîwhich often misses the context‚ÄîGrepAI uses **Ollama** to generate local embeddings of your code. This allows you to perform **semantic searches** directly from the terminal.\n\n**The Stack:**\n\n* **Core:** Written in Go.\n* **Embeddings:** Connects to your local Ollama instance (defaults to `nomic-embed-text`).\n* **Vector Store:** In-memory / Local (fast and private).\n\n**Why use Ollama for this?** I wanted a solution that respects privacy and doesn't cost a fortune in API credits just to index a repo. By using Ollama locally, GrepAI builds an index of your project (respecting `.gitignore`) without your code leaving your machine.\n\n**Real-world Impact (Benchmark)** I tested this setup by using GrepAI as a filter for Claude Code (instead of the default grep). The idea was to let Ollama decide what files were relevant before sending them to the cloud. The results were huge:\n\n* **-97% Input Tokens** sent to the LLM (because Ollama filtered the noise).\n* **-27.5% Cost reduction** on the task.\n\nEven if you don't use Claude, this demonstrates how effective local embeddings (via Ollama) are at retrieving the right context for RAG applications.\n\nüëâ **Benchmark details:**[https://yoanbernabeu.github.io/grepai/blog/benchmark-grepai-vs-grep-claude-code/](https://yoanbernabeu.github.io/grepai/blog/benchmark-grepai-vs-grep-claude-code/)\n\n**Links:**\n\n* üì¶ **GitHub:**[https://github.com/yoanbernabeu/grepai](https://github.com/yoanbernabeu/grepai)\n* üìö **Docs:**[https://yoanbernabeu.github.io/grepai/](https://yoanbernabeu.github.io/grepai/)\n\nI'd love to know what other embedding models you guys are running with Ollama. Currently, `nomic-embed-text` gives me the best results for code, but I'm open to suggestions!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qiv7v8/i_built_a_cli_tool_using_ollama_nomicembedtext_to/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0vbgvm",
          "author": "Ok-District-1756",
          "text": "I'm going to try Qwen3-Embedding-4B-GGUF:Q5\\_K\\_M. Honestly, I have no idea how it will perform in real-world conditions, but I'll test it for a week and report back if anyone is interested",
          "score": 4,
          "created_utc": "2026-01-21 15:30:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ybqej",
          "author": "yesbee-yesbee",
          "text": "Will it work for opencode?¬†",
          "score": 1,
          "created_utc": "2026-01-21 23:50:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlvud9",
      "title": "Mac Studio as host for Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qlvud9/mac_studio_as_host_for_ollama/",
      "author": "amgsus",
      "created_utc": "2026-01-24 19:10:24",
      "score": 24,
      "num_comments": 33,
      "upvote_ratio": 0.85,
      "text": "Hello,\n\nI‚Äôm wondering if it worth buying Mac Studio M4 Max (64 GB) for hosting Ollama. Does anyone have experience with this box? Or better to build a cluster of GPUs like RTX 3090, etc.?\n\nPrimarily, I will be using LLMs for coding. Rarely, for media content generation.\n\nKind regards",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qlvud9/mac_studio_as_host_for_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1h9z86",
          "author": "tom-mart",
          "text": "Define \"better\". What do you carr about? Raw power? 2 z 3090 will blow 64GB Mac out of the water. Ease of use? Mac will be miles easier to set up than multiple GPU's. Running cost? Mac wins again. Ability to fine tune, RTX takes the lead.",
          "score": 10,
          "created_utc": "2026-01-24 19:21:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hdbat",
              "author": "ZeroSkribe",
              "text": "There is no setup for multiple GPU's, it just works. Ollama has had this working out of the box for a while.",
              "score": 4,
              "created_utc": "2026-01-24 19:36:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1s4mpk",
                  "author": "cmk1523",
                  "text": "Power can get complicated unless you just go super big",
                  "score": 1,
                  "created_utc": "2026-01-26 07:38:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1hc986",
          "author": "Whospakka",
          "text": "If I say I‚Äôm running an Ollama on a Mac Mini M4 base config and it‚Äôs working awesome for an 8B model (plus a small embedding model at the same time), what do you think? üòÅ",
          "score": 3,
          "created_utc": "2026-01-24 19:31:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hewvv",
              "author": "CMPUTX486",
              "text": "Same here.. I did a bit more with Gemma 3 12b",
              "score": 4,
              "created_utc": "2026-01-24 19:43:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1kqfd0",
              "author": "rorowhat",
              "text": "even my phone can run a 8B models these days at good speeds, its not impressive.",
              "score": 3,
              "created_utc": "2026-01-25 06:36:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1jl52l",
          "author": "Popular-Diamond-9928",
          "text": "For folks using the Mac studios what is your time to first token? I‚Äôm curious at the optimizations you all are making. I am using Ollama and I‚Äôve seen posts below about llama.cpp which I‚Äôm seriously considering now lol. \n\nFor me, I have a classifier that classifies the query, determines if rag is needed, conversation is needed, and even additional context (I don‚Äôt use tool calling because it‚Äôs just faster for the routing to identify entities needed and then using sql to fetch)\n\nBut even when I‚Äôm controlling the token consumption I‚Äôm getting about 30seconds for my time to first token. \n\nI‚Äôm using Qwen2.5 8B or 7B I don‚Äôt remember \nAnd a small embedding model.",
          "score": 1,
          "created_utc": "2026-01-25 02:17:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jl7pl",
              "author": "Popular-Diamond-9928",
              "text": "I‚Äôm running my setup on a Mac mini base which has been amazing to setup.",
              "score": 1,
              "created_utc": "2026-01-25 02:17:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1kon7l",
          "author": "mhjor70",
          "text": "Yes i use it with a 32GB M3 and it works. It really depends on what you want to do with it and what models you want to load.",
          "score": 1,
          "created_utc": "2026-01-25 06:22:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kpcsx",
          "author": "Odd_Butterfly6003",
          "text": "In my case, i used 256gb ram m3 ultra.\nQwen3 30B took about 5 second per one request, but Qwen3 VL 235B model took 20 seconds per one request.",
          "score": 1,
          "created_utc": "2026-01-25 06:28:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kq8yl",
          "author": "rorowhat",
          "text": "Stick with Nvidia, way higher memory BW and compute power. Your TTFT and TS will be 2x of the Mac.\n\nM4 Max: Up to \\~546 GB/s  \nRTX 3090: \\~936 GB/s",
          "score": 1,
          "created_utc": "2026-01-25 06:35:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lyjy1",
          "author": "Weak_Ad9730",
          "text": "Have a m3u using Mlx-vllm and reality impressed of the Performance the Switch from lmstudio to vllm was a Hugh Jump in Processing time and Speed. I use my Studio in an Agent Zero setup. Realy recommend those Apple Silikon for llm work. My Go to Models are qwen3-vl-32b , got-oss-120b and minimax-m2.1",
          "score": 1,
          "created_utc": "2026-01-25 12:50:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nyyxf",
          "author": "bishopLucas",
          "text": "the best mac studio you can get is \"Apple M4¬†Max chip with 16‚Äëcore CPU, 40‚Äëcore GPU, 16‚Äëcore Neural¬†Engine\", mine has 64GB/1TB.\n\nIt is like MAGIC.  I came from a surface book laptop and didn't really appreciate how contained I was.\n\nI may create a cluster when ever the Ultra chip is released.\n\nEven at 64GB/1TB so many more possibilities are open to you.",
          "score": 1,
          "created_utc": "2026-01-25 18:36:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rreg3",
          "author": "Glum_Mistake1933",
          "text": "I run a 64 GB M4 Pro with 10 cores. Totally worth it. Models usually communicate using LM-Studio, works fine. If you have continued workload, the mac studio might be your choice, since it has better cooling.",
          "score": 1,
          "created_utc": "2026-01-26 05:53:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1shd2r",
          "author": "bharattrader",
          "text": "On my Mac Mini M4 Pro, (64 GB) I can able to run GLM4.7-Flash at Q4 using llama.cpp backend. Since I run parallel = 1 I can get 32K context for one client. I can run other models too, LM Stuido also and MLX is a little faster. I do not use Ollama.",
          "score": 1,
          "created_utc": "2026-01-26 09:32:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tilyi",
          "author": "Affectionate-Hat-536",
          "text": "Not Mac Studio, but Macbook pro max 64gb.\nI got both ollama and LM studio working and found GLM models quite useful for coding.\nIf you are in tech, you will move to llama.cpp easily which has got a UI as well.\nLlama.cpp will have better inference performance as compared to Ollama.",
          "score": 1,
          "created_utc": "2026-01-26 14:04:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1hgpqy",
          "author": "Le_mele",
          "text": "I highly recommend it, it works great even for models up to 32b",
          "score": 1,
          "created_utc": "2026-01-24 19:51:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1hd3cr",
          "author": "ZeroSkribe",
          "text": "Naw, you really need to get full GPU vram coverage, a cluster of 3050's would be way faster",
          "score": -1,
          "created_utc": "2026-01-24 19:35:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1hlc92",
          "author": "g_rich",
          "text": "I have a Mac Studio M4 Max, 64GB, 1TB which can easily run Devstral-Small-2-24B, Qwen3-Coder-30B and GLM-4.7-Flash; performance is acceptable but by no means fast. \n\nA cluster of RTX 3090‚Äôs will be faster and a better option if you intend on doing anything requiring CUDA. However that doesn‚Äôt mean it‚Äôs the better option for everyone, heat, power and likely cost will be higher made especially worse with the current GPU and RAM prices. \n\nAlso consider that you‚Äôll need at least 256-512GB of memory to get anywhere near Claude, Gemini or ChatGPT. So while you will get useful performance from the local models with 64GB and learn a lot you‚Äôll still be dependent on cloud hosted foundation models to do any serious work. \n\nOne other thing to consider the Mac Studio supports RDMA over Thunderbolt 5 which offers an interesting option to cluster Mac‚Äôs to run larger models; so it‚Äôs possible to start with one Mac Studio and add another down the road. There are a few videos on YouTube with Jeff Geerling doing a great job of demoing this capability.\n\nLastly don‚Äôt use Ollama, llama.cpp and AI Studio are both much better options.",
          "score": 0,
          "created_utc": "2026-01-24 20:13:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hybyd",
              "author": "BoostedHemi73",
              "text": "\\> Lastly don‚Äôt use Ollama, llama.cpp and AI Studio are both much better options.\n\nCan you say more on this? I've been so impressed with ollama on my M4 Max.",
              "score": 1,
              "created_utc": "2026-01-24 21:14:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1i5bwx",
                  "author": "g_rich",
                  "text": "The software itself is a ripoff of llama.cpp, they are not good stewards of open source and they are trying to lock users into their ecosystem. They have also had a few security vulnerabilities over the last few years and have shown little alarm or urgency to address them when they are discovered. \n\nllama.cpp is better but more complex, although not difficult to get up and running; especially when peered with llama-swap. It will also give you better performance and is overall a much better open source project. \n\nLM Studio is the easy choice if you are just looking for a quick and easy way to run local models.",
                  "score": 0,
                  "created_utc": "2026-01-24 21:47:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1i0ofj",
          "author": "Pale_Reputation_511",
          "text": "For Mac use mlx better performance and less ram usage",
          "score": 0,
          "created_utc": "2026-01-24 21:25:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ie2hn",
          "author": "No_Entertainer6253",
          "text": "Just dont",
          "score": 0,
          "created_utc": "2026-01-24 22:29:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ieggf",
              "author": "No_Entertainer6253",
              "text": "I bought 192gb studio, still paying 200$/ month to use cc",
              "score": 3,
              "created_utc": "2026-01-24 22:31:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1irn5z",
                  "author": "not-really-adam",
                  "text": "Have the 256GB M3 Ultra, and still pay $200/mo to claude. The local stuff just isn‚Äôt as good or as fast as the cloud. For privacy of personal transcriptions and other things you don‚Äôt want to leak to the cloud, it is really nice though. And, I think we are getting closer to being able to use Opus for planning and thinking and local models for research and execution which means either lower Claude bill, or ability to do loops for bug finding and/or optimizations.",
                  "score": 3,
                  "created_utc": "2026-01-24 23:39:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1iouh2",
                  "author": "AstroZombie138",
                  "text": "Have you tried GLM 4.7?  It works well for me.",
                  "score": 1,
                  "created_utc": "2026-01-24 23:24:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1iz9vz",
          "author": "rockinyp",
          "text": "Memory is more important than the CPU. Find a used M1 Mac Studio with more memory so you can run larger models. Running larger models means you'll often get better responses, which means you're more likely to use it. I have an M1 Ultra with 128 GB of memory running 30b+ models just fine for multiple users via Open WebUI.",
          "score": 0,
          "created_utc": "2026-01-25 00:18:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjtnqm",
      "title": "How to implement a RAG (Retrieval Augmented Generation) on your laptop",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qjtnqm/how_to_implement_a_rag_retrieval_augmented/",
      "author": "Unique_Winner_5927",
      "created_utc": "2026-01-22 12:48:36",
      "score": 23,
      "num_comments": 0,
      "upvote_ratio": 0.88,
      "text": "This guide explains how to implement a RAG (Retrieval Augmented Generation) on your laptop.\n\nhttps://preview.redd.it/ftsddeqtcweg1.png?width=2184&format=png&auto=webp&s=640e3013e9113c3c7780a88b39d6992cd34b8d6f\n\nWith n8n, Ollama and Qdrant (with Docker).\n\n[https://github.com/ThomasPlantain/n8n](https://github.com/ThomasPlantain/n8n)\n\nI put a lot of screenshots to explain how to configure each component.\n\n\\#Ollama #n8n #Qdrant #dataSovereignty #embeddedAI",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qjtnqm/how_to_implement_a_rag_retrieval_augmented/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qijhth",
      "title": "Weekend Project: An Open-Source Claude Cowork That Can Handle Skills",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qijhth/weekend_project_an_opensource_claude_cowork_that/",
      "author": "Frequent_Cash2598",
      "created_utc": "2026-01-21 00:59:50",
      "score": 20,
      "num_comments": 5,
      "upvote_ratio": 0.96,
      "text": "I spent last weekend building something I had been thinking about for a while. Claude Cowork is great, but I wanted an open-source, lightweight version that could run with any model, so I created Open Cowork.\n\nIt's written entirely in Rust, which I had never used before. Starting from scratch meant no heavy dependencies, no Python bloat, and no reliance on existing agent SDKs. Just a tiny, fast binary that works anywhere.\n\nSecurity was a big concern since the agents can execute code. Open Cowork handles this by running tasks inside temporary Docker containers. Everything stays isolated, but you can still experiment freely.\n\nYou can plug in any model you want. OpenAI, Anthropic, or even fully offline LLMs through Ollama are all supported. You keep full control over your API keys and your data.\n\nIt already comes with built-in skills for handling documents like PDFs and Excel files. I was surprised by how useful it became right away.\n\nThe development experience was wild. An AI agent helped me build a secure, open-source version of itself, and I learned Rust along the way. It was one of those projects where everything just clicked together in a weekend.\n\nThe code is live on GitHub: [https://github.com/kuse-ai/kuse\\_cowork](https://github.com/kuse-ai/kuse_cowork) . It's still early, but I'd love to hear feedback from anyone who wants to try it out or contribute.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qijhth/weekend_project_an_opensource_claude_cowork_that/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0rvzc8",
          "author": "Available-Craft-5795",
          "text": "Im pretty sure this is AI.   \nLoads of emojis in readme  \nLoads of commments in code that no sane dev would add  \nDone in a weekend? How?",
          "score": 2,
          "created_utc": "2026-01-21 01:13:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ss3zn",
              "author": "Frequent_Cash2598",
              "text": "Yes, it is Claude Code making a Rust alternative of itself. \n\nSo you are right. :)",
              "score": 4,
              "created_utc": "2026-01-21 04:23:15",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0s2p5t",
              "author": "gingeropolous",
              "text": "Sounds like they used Claude code.",
              "score": 3,
              "created_utc": "2026-01-21 01:51:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0sepez",
          "author": "Efficient_Click_2689",
          "text": "cool project bro, would love to try out!",
          "score": 1,
          "created_utc": "2026-01-21 03:00:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0w48w9",
          "author": "wombweed",
          "text": "OS compatibility? If it runs on Linux can you include a Nix flake?",
          "score": 1,
          "created_utc": "2026-01-21 17:40:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo9tsi",
      "title": "NotebookLM For Teams",
      "subreddit": "ollama",
      "url": "https://v.redd.it/zxqevbwh8vfg1",
      "author": "Uiqueblhats",
      "created_utc": "2026-01-27 10:04:14",
      "score": 17,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qo9tsi/notebooklm_for_teams/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o20lca9",
          "author": "Ok_Swing9407",
          "text": "if you're exploring self-hosted options for team knowledge management and automation, needle.app has been solid for me. it's like vibe automation meets RAG, so workflows actually understand your docs instead of just moving data around. way less setup than wiring langchain or n8n for every new project.",
          "score": 1,
          "created_utc": "2026-01-27 13:56:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkcg6a",
      "title": "Boom, the greatest repo yet from Lucas Valbuena ( x1xhlol)",
      "subreddit": "ollama",
      "url": "https://i.redd.it/dspr44juxzeg1.jpeg",
      "author": "OriginalZebraPoo",
      "created_utc": "2026-01-23 00:57:26",
      "score": 16,
      "num_comments": 0,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qkcg6a/boom_the_greatest_repo_yet_from_lucas_valbuena/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qlssc0",
      "title": "HashIndex: An alternative to a page that doesn't require RAG but can still perform indexing well.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qlssc0/hashindex_an_alternative_to_a_page_that_doesnt/",
      "author": "jasonhon2013",
      "created_utc": "2026-01-24 17:19:08",
      "score": 14,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "The Pardus AI team has decided to open source our memory system, which is similar to PageIndex. However, instead of using a B+ tree, we use a hash map to handle data. This feature allows you to parse the document only once, while achieving retrieval performance on par with PageIndex and significantly better than embedding vector search. It also supports Ollama. Give it a try and consider implementing it in your system ‚Äî you might like it! Give us a star maybe hahahaha\n\n[ https://github.com/JasonHonKL/HashIndex/tree/main ](https://github.com/JasonHonKL/HashIndex/tree/main)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qlssc0/hashindex_an_alternative_to_a_page_that_doesnt/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1gncij",
          "author": "immediate_a982",
          "text": "Your sample code does to show Ollama usage",
          "score": 2,
          "created_utc": "2026-01-24 17:44:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1gnq4w",
              "author": "jasonhon2013",
              "text": "Hi all you need is to change the base url in the .env file ! Hope you enjoy it",
              "score": 2,
              "created_utc": "2026-01-24 17:46:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ha2to",
          "author": "zair",
          "text": "Does this have value for other structured document types, eg spreadsheets? And is the benefit mainly for very long documents or also large numbers of short documents?",
          "score": 1,
          "created_utc": "2026-01-24 19:22:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jv624",
              "author": "jasonhon2013",
              "text": "Ah it‚Äôs mainly focusing on long context documents",
              "score": 2,
              "created_utc": "2026-01-25 03:12:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sjsnf",
          "author": "numbworks",
          "text": "The idea is very interesting, but I have two questions:\n\n1. Does it support PDFs with tables inside?\n2. Does it support other file formats such as Markdown or Asciidoc?",
          "score": 1,
          "created_utc": "2026-01-26 09:55:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmmp22",
      "title": "Claude Code stuck on <function=TaskList> when using Ollama + Qwen3-Coder",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qmmp22/claude_code_stuck_on_functiontasklist_when_using/",
      "author": "Healthy-Laugh-6745",
      "created_utc": "2026-01-25 15:52:04",
      "score": 14,
      "num_comments": 16,
      "upvote_ratio": 0.94,
      "text": "Hi everyone,\n\nI'm struggling to get Claude Code working with Ollama on my Mac M4 Max (48GB RAM). I strictly followed the official Ollama integration guide (https://docs.ollama.com/integrations/claude-code), but I'm stuck in a loop.\n\nEvery time I ask the model to perform a file-based task (e.g., \"create a txt file\"), the process hangs indefinitely.\n\nThe model acknowledges the request.\n\nIt outputs: ‚ùØ <function=TaskList> ‚è∫\n\nNothing happens after that. No file is created, and the terminal just sits there with the \"active\" dot.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qmmp22/claude_code_stuck_on_functiontasklist_when_using/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1ngiam",
          "author": "Outrageous_Rub_6527",
          "text": "Hey! Bump up your context length before running claude code with Ollama - https://docs.ollama.com/context-length",
          "score": 4,
          "created_utc": "2026-01-25 17:19:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1oy32w",
              "author": "jmorganca",
              "text": "This. Sorry it's not obvious right now - we're working on improving this so context length size automatically grows (up to an acceptable amount on your hardware)",
              "score": 1,
              "created_utc": "2026-01-25 21:09:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1nnebn",
              "author": "Healthy-Laugh-6745",
              "text": "correct! thanks a lot at least 64k. sorry i'm a newbie :)",
              "score": 1,
              "created_utc": "2026-01-25 17:49:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1n4fds",
          "author": "paq85",
          "text": "I would be really surprised to see Claude Code work with model like Qwen3 Coder 30B... but, please let me know if you manage to make it work... from my experience those local models are way too limited.",
          "score": 3,
          "created_utc": "2026-01-25 16:27:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1p4kil",
              "author": "nunodonato",
              "text": "I've used it with a 4B model :D",
              "score": 2,
              "created_utc": "2026-01-25 21:37:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1vtedu",
                  "author": "paq85",
                  "text": "I really doubt it could do anything, even at a level of tic tac toe game.\n\nWhat model was that?",
                  "score": 0,
                  "created_utc": "2026-01-26 20:09:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1n62m3",
              "author": "Healthy-Laugh-6745",
              "text": "Thanks! According to the Ollama docs, open models can be used with Claude Code via Ollama‚Äôs Anthropic-compatible API (for example, glm-4.7, qwen3-coder, or gpt-oss).\nAre you saying I should try a different model?",
              "score": 1,
              "created_utc": "2026-01-25 16:34:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1npe3p",
                  "author": "paq85",
                  "text": "I'm afraid there's no local model up to 30B that's good enought for tools like Claude Code, Cline etc... [Has anyone got GLM 4.7 flash to not be shit? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/)\n\nHere's GLM 4.7 Flash with Cline (VS Code) struggling to provide a simple answer... \n\nhttps://preview.redd.it/wb9yr8j3bjfg1.png?width=667&format=png&auto=webp&s=0195981d970e6235018e7efe9ee4028f89c0718e",
                  "score": 1,
                  "created_utc": "2026-01-25 17:57:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1n6qk2",
          "author": "960be6dde311",
          "text": "Have you tried using OpenCode CLI instead? I love the concept of running AI models locally, and have done a fair amount of experimentation, but they often have \"issues\" ranging from:\n\n* Hanging on generating responses\n* Infinite MCP tool calling loops\n* Spending a disproportionate amount of time \"thinking\" (reasoning)\n* Garbling tool calls or responses\n\nYou really have to find a client tool that has been **thoroughly tested** with the specific model you're using. Without adequate testing and bug fixes, there's a reasonably high probability you'll run into some kind of issue.\n\nI'm sure we will see these kinds of issues get worked out over time, but for now, you'll need to spend time with trial and error to see what works and what doesn't.",
          "score": 1,
          "created_utc": "2026-01-25 16:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1pgtqb",
          "author": "Savantskie1",
          "text": "This is a bit of a noobish question but we have to establish competency. Have you given it the tools you‚Äôre trying to get it to use? Ollama doesn‚Äôt automatically make it an AI that can do everything",
          "score": 1,
          "created_utc": "2026-01-25 22:31:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s66yz",
              "author": "Healthy-Laugh-6745",
              "text": "correct it was a newbie question, i'm learning the ollama and Ai. the problem was the context",
              "score": 1,
              "created_utc": "2026-01-26 07:52:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1yma2u",
                  "author": "Savantskie1",
                  "text": "Ok, yeah you need to build the tools or download tools and set them up with an internal API that the model can call. Or use a platform like lm studio or OpenWebUi or a frontend that can import the tools. Then the model can use tool calling if it‚Äôs built in with the model and it can call those tools so long as they‚Äôre broadcast to the model that they‚Äôre there. But until then all you can do with ollama is chat with them",
                  "score": 1,
                  "created_utc": "2026-01-27 04:47:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1qdsfu",
          "author": "Embarrassed_Sun_7807",
          "text": "Post settings or we can't help really.¬†",
          "score": 1,
          "created_utc": "2026-01-26 01:07:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s5ym1",
              "author": "Healthy-Laugh-6745",
              "text": "the problem was the context, thanks bro",
              "score": 1,
              "created_utc": "2026-01-26 07:50:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qmteov",
      "title": "Claraverse is not dead, now you can use AI with more fun, more productivity, and more PRIVACY.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/gallery/1qmteov",
      "author": "aruntemme",
      "created_utc": "2026-01-25 19:52:48",
      "score": 14,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qmteov/claraverse_is_not_dead_now_you_can_use_ai_with/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1ol7id",
          "author": "Available-Craft-5795",
          "text": "TL;DR:  \nVibe coded N8N clone but you dont do any of the work",
          "score": 1,
          "created_utc": "2026-01-25 20:13:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1p8q3t",
          "author": "immediate_a982",
          "text": "Has it been cyber attack pentested",
          "score": 1,
          "created_utc": "2026-01-25 21:55:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmlc8p",
      "title": "My AI Open Source Workflow",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qmlc8p/my_ai_open_source_workflow/",
      "author": "candidosales",
      "created_utc": "2026-01-25 15:01:05",
      "score": 13,
      "num_comments": 1,
      "upvote_ratio": 0.93,
      "text": "Lately, I have been studying AI and Open Source Workflows. I thought it would be interesting to share a bit of what I'm learning: [https://www.candidosales.me/blog/my-ai-open-source-workflow](https://www.candidosales.me/blog/my-ai-open-source-workflow)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qmlc8p/my_ai_open_source_workflow/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1y2pz1",
          "author": "PropertyLoover",
          "text": "Good job!",
          "score": 1,
          "created_utc": "2026-01-27 02:50:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj02gw",
      "title": "New Rules for ollama cloud",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qj02gw/new_rules_for_ollama_cloud/",
      "author": "killing_daisy",
      "created_utc": "2026-01-21 14:57:44",
      "score": 12,
      "num_comments": 7,
      "upvote_ratio": 0.81,
      "text": "so i've just seen this:\n\nPro:  \nEverything in Free, plus:\n\n* Run 3 cloud models at a time\n* Faster responses from cloud hardware\n* Larger models for challenging tasks\n* 3 private models\n* 3 collaborators per model\n\nits been a lot slower for usage within zed for me the last hours - does anyone have more information whats happening to the pro subscription? it seems like the changes in the subscription are random and without any notice to users? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qj02gw/new_rules_for_ollama_cloud/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0vg4mk",
          "author": "jmorganca",
          "text": "Hi there. I work on Ollama. No new restrictions on the cloud model usage with this change. We actually increased usage amounts on each plan on Monday and will share more about that this week. Our goal is to make this a the best subscription for using open models with your favorite tools as we add more model support, better performance and reliability.\n\nHappy to answer any questions and if you hit any issues or limits with the plans let me know (email is jeff at ollama.com)",
          "score": 13,
          "created_utc": "2026-01-21 15:52:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vraab",
              "author": "jmorganca",
              "text": "Also, OP, let me know if you're still seeing any slowdown (and for which models). We've been working on improving performance and capacity a lot in the last few weeks and will keep doing so. (Feel free to DM/email me)",
              "score": 7,
              "created_utc": "2026-01-21 16:42:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0w2o4h",
              "author": "killing_daisy",
              "text": "hi, it a bit worrying, that the contracts are sortof changing without notice to active users, i've only seen this because i was investigating the slow down - i'll have a try today at home, maybe it was only our network at the company.  \nedit: forgot to thank you for a response :) - so thanks :D",
              "score": 5,
              "created_utc": "2026-01-21 17:33:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o10on2k",
                  "author": "Ryanmonroe82",
                  "text": "This isn't the first time",
                  "score": 1,
                  "created_utc": "2026-01-22 09:29:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o10ol2i",
              "author": "Ryanmonroe82",
              "text": "Why does the APi no longer work when using synthetic data gen tools? Kiln AI and Easy Dataset both return errors now using cloud API and it was not like this when I first subscribed. All cloud models worked for this now none do.",
              "score": 1,
              "created_utc": "2026-01-22 09:28:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1hgeju",
              "author": "Mulan20",
              "text": "Can you share how many tokens per day can generate with big models and what is the limit? \nLet's say 10 milion tokens per day.",
              "score": 1,
              "created_utc": "2026-01-24 19:50:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zf3bo",
          "author": "MLExpert000",
          "text": "exactly why a lot of people still prefer local runtimes. Once inference becomes part of a workflow (agents, tools, multi model setups), predictability matters more than raw scale. Random slowdowns, model eviction, or behavior changes break trust fast.\n\nLot of teams use local setups not because they‚Äôre cheaper, but because they behave the same every time and let you debug real lifecycle issues like model load, memory pressure, and switching between models. Cloud is definitely great for scale, but local is still hard to beat for stability and development parity.",
          "score": 1,
          "created_utc": "2026-01-27 08:39:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}