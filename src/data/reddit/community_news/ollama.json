{
  "metadata": {
    "last_updated": "2026-03-02 09:15:17",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 20,
    "total_comments": 124,
    "file_size_bytes": 152711
  },
  "items": [
    {
      "id": "1rdyosq",
      "title": "I built a locally-hosted AI agent that runs entirely on your own hardware no cloud, no subscriptions",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rdyosq/i_built_a_locallyhosted_ai_agent_that_runs/",
      "author": "Janglerjoe",
      "created_utc": "2026-02-25 00:56:51",
      "score": 143,
      "num_comments": 34,
      "upvote_ratio": 0.91,
      "text": "I built LMAgent a pure Python AI agent that connects to any OpenAI-compatible LLM (LM Studio, Ollama, etc.) and actually does things on your computer.\n\nNo cloud. No API fees. No subscriptions. Runs 100% on your own hardware.\n\n**What it can do autonomously:**\n\n* Read and write files\n* Run shell commands (bash / PowerShell)\n* Manage git (status, diff, commit, branch)\n* Track todos and multi-step plans\n* Spawn sub-agents to delegate tasks\n* Connect to external tools via MCP servers (web search, browsers, databases)\n* Schedule itself to wake up at a future time and resume work\n\n**Three ways to run it:**\n\n* Terminal REPL ‚Äî conversational loop with a live background scheduler\n* One-shot CLI ‚Äî give it a task, get a result, exit\n* Web UI ‚Äî streaming tokens, inline tool calls, session browser, mobile-friendly\n\n**Setup is dead simple:**\n\n1. `pip install requests flask colorama`\n2. Point it at your local LLM server\n3. Set a workspace directory in a `.env` file\n4. Run `python agent_main.py`\n\nWorks on Windows, macOS, and Linux. MIT licensed.\n\nWould love feedback especially from anyone running it with larger models or unconventional LLM backends.\n\nGitHub: [https://github.com/janglerjoe-commits/LMAgent](https://github.com/janglerjoe-commits/LMAgent)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rdyosq/i_built_a_locallyhosted_ai_agent_that_runs/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o79er0q",
          "author": "jerrygreenest1",
          "text": "So what, is it like OpenClaw clone?",
          "score": 16,
          "created_utc": "2026-02-25 03:09:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79kiz1",
              "author": "IAmANobodyAMA",
              "text": "They should call it something similar but different enough. How about ‚ÄúClaude‚Äù?",
              "score": 14,
              "created_utc": "2026-02-25 03:43:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7a72s6",
                  "author": "Garfieldealswarlock",
                  "text": "How about Clu?",
                  "score": 4,
                  "created_utc": "2026-02-25 06:24:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7c0qfl",
                  "author": "Flimsy_Leadership_81",
                  "text": "Clawbit\n\n",
                  "score": 1,
                  "created_utc": "2026-02-25 14:45:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7a4qcb",
          "author": "rjyo",
          "text": "The scheduling feature is a really nice touch. Being able to tell an agent to wake up and resume work later is something most agent frameworks skip over entirely. How does it handle context when it resumes? Does it serialize the full conversation state or just the task/plan?\n\n  \nAlso curious about the sub-agent delegation. Are those separate LLM sessions or do they share the same context window? With local models the context limit is usually the biggest bottleneck so that tradeoff matters a lot.",
          "score": 6,
          "created_utc": "2026-02-25 06:05:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7abq8e",
          "author": "Fair-Cookie9962",
          "text": "Everybody should do that as part of learning LLMs.",
          "score": 6,
          "created_utc": "2026-02-25 07:04:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7avv8q",
          "author": "woejise",
          "text": "Could it fit on a raspberry pi 5?",
          "score": 3,
          "created_utc": "2026-02-25 10:10:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bfe3l",
              "author": "Janglerjoe",
              "text": "I haven‚Äôt tested it on a Pi 5 yet, but the agent itself is lightweight the main constraint would be the LLM runtime and available RAM. If you‚Äôre running a small quantized model, it should work fine since execution happens in Docker with configurable resource caps.",
              "score": 4,
              "created_utc": "2026-02-25 12:46:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7en8wg",
                  "author": "woejise",
                  "text": "If I end up running it ill let you know how it does. Ive got the 16gb pi with a 26 tops ai hat",
                  "score": 1,
                  "created_utc": "2026-02-25 22:01:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7cm8qh",
          "author": "yoshkoHS85",
          "text": "How it works with smaller models like 14b q4?",
          "score": 2,
          "created_utc": "2026-02-25 16:26:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ecg0y",
              "author": "Janglerjoe",
              "text": "I‚Äôve been running it with Qwen3 8B (VL) in 6-bit and it handles most coding and file-task workflows fine. I haven‚Äôt stress-tested extremely long, multi-hour tasks yet, but for typical planning + execution loops it performs reliably.",
              "score": 2,
              "created_utc": "2026-02-25 21:11:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ai3n4",
          "author": "emmettvance",
          "text": "giving an agent raw shell access is always a bit terrifying but mcp support makes this actually viable for real workflows..",
          "score": 2,
          "created_utc": "2026-02-25 08:01:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7b742y",
              "author": "Janglerjoe",
              "text": "Totally. That‚Äôs why it‚Äôs sandboxed in Docker by default the LLM never gets direct host access",
              "score": 4,
              "created_utc": "2026-02-25 11:47:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7fh7xu",
                  "author": "TeachNo196",
                  "text": "Unless if you hack the kernel. Only bad thing about docker.",
                  "score": 1,
                  "created_utc": "2026-02-26 00:40:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ahbls",
          "author": "redonculous",
          "text": "Can it run in portainer?",
          "score": 1,
          "created_utc": "2026-02-25 07:54:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7alqrj",
          "author": "FloppyWhiteOne",
          "text": "Love this. I‚Äôve made my own version too but in rust and fully windows based (will make Linux compatible after)\n\nMine has customisations well past most in terms of token squashing, memory and context loading for local llms. I‚Äôve even made a sweet hot swap utility for lm studio so I can swap models at will to save vram and never gos over system vram. I‚Äôm still heavily in dev with it atm.\n\nMy aim is to make dumb models act smart on consumer hardware. Results so far are looking great.\n\nThanks for sharing I will as soon as it‚Äôs in a ready state.\n\nI love when my app runs it only uses 4.5mb ‚Ä¶ gotta love some rust efficiency (pic of web ui) \n\nhttps://preview.redd.it/jthckkc9ullg1.png?width=1987&format=png&auto=webp&s=dabe5e06b23b187e66c5820bef7fddbf9e56bef9",
          "score": 1,
          "created_utc": "2026-02-25 08:36:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bxc5h",
              "author": "aptonline",
              "text": "Is this public (GitHub)?",
              "score": 1,
              "created_utc": "2026-02-25 14:28:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bxtkg",
                  "author": "FloppyWhiteOne",
                  "text": "When I‚Äôve perfected the hot loading for agents with lm studio then yes. I‚Äôm fighting having models loaded, with full context. Then at times they fall back to 4096 (small window) but yeah my aim is to make public to get more help with it",
                  "score": 3,
                  "created_utc": "2026-02-25 14:30:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o79zfos",
          "author": "smwaqas89",
          "text": "sounds interesting! i wonder how it handles different hardware setups though. like, does it optimize for performance based on the specs of your machine? that could really change how effective it is for different users.",
          "score": -1,
          "created_utc": "2026-02-25 05:24:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c121f",
          "author": "Flimsy_Leadership_81",
          "text": "follow",
          "score": -1,
          "created_utc": "2026-02-25 14:47:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f2p52",
          "author": "TinyDesigner9155",
          "text": "Commenting so I don't forget",
          "score": -1,
          "created_utc": "2026-02-25 23:20:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ri4d80",
      "title": "soul.py ‚Äî Persistent memory for any LLM in 10 lines (works with Ollama, no database)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1ri4d80/soulpy_persistent_memory_for_any_llm_in_10_lines/",
      "author": "the-ai-scientist",
      "created_utc": "2026-03-01 18:10:38",
      "score": 88,
      "num_comments": 25,
      "upvote_ratio": 0.9,
      "text": "Got tired of my AI forgetting everything between sessions. Built a fix.\n\nLocal models are great until you restart the process and they have no idea who you are.\n\nfrom soul import Agent\n\nagent = Agent(\n\nprovider=\"openai-compatible\",\n\nbase\\_url=\"http://localhost:11434/v1\",\n\nmodel=\"llama3.2\",\n\napi\\_key=\"ollama\"\n\n)\n\nagent.ask(\"My name is Prahlad, I'm building an AI research lab.\")\n\n\\# New process, new session:\n\nagent.ask(\"What do you know about me?\")\n\n\\## ‚Üí \"You're Prahlad, building an AI research lab.\"\n\n\\# How it works:\n\nTwo plain markdown files ‚Äî SOUL.md (identity) and MEMORY.md (conversation log). Every ask() reads both into the system prompt, then appends the exchange. Memory survives across processes with no database, no server, nothing running in the background.\n\nHuman-readable. Git-versionable. Editable by hand.\n\npip install soul-agent\n\nsoul init\n\nWorks with Anthropic and OpenAI too, but the original motivation was local models.\n\nv2.0 update (today): Added a query router that automatically dispatches between RAG (fast semantic search) and RLM (recursive synthesis for exhaustive queries). Now merged to main.\n\n‚Ä¢ ‚≠ê GitHub:  [ https://github.com/menonpg/soul.py ](https://github.com/menonpg/soul.py)\n\n‚Ä¢ üì¶ PyPI:  [ https://pypi.org/project/soul-agent/ ](https://pypi.org/project/soul-agent/)\n\n‚Ä¢ üéÆ v2 demo:  [ https://soulv2.themenonlab.com ](https://soulv2.themenonlab.com)\n\n‚Ä¢ üìñ v1 post:  [ https://blog.themenonlab.com/blog/soul-py-persistent-memory-llm-agents ](https://blog.themenonlab.com/blog/soul-py-persistent-memory-llm-agents)\n\n‚Ä¢ üìñ v2 post:  [ https://blog.themenonlab.com/blog/soul-py-v2-rag-rlm-hybrid ](https://blog.themenonlab.com/blog/soul-py-v2-rag-rlm-hybrid)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1ri4d80/soulpy_persistent_memory_for_any_llm_in_10_lines/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o83xavf",
          "author": "EveryCa11",
          "text": "The problem of memory is that you don't want to blow up the context window with useless memory. At the same time it's hard to say which memory can be useful. This solution doesn't solve anything, in the long run it makes things worse.",
          "score": 8,
          "created_utc": "2026-03-01 19:58:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o844l5n",
              "author": "the-ai-scientist",
              "text": "Thanks for your message. The critique is valid for naive memory injection ‚Äî and that's exactly why v2.0 exists.\n\nv0.1 (simple mode) does inject full memory into context. You're right that it doesn't scale. That's why we added intelligent retrieval in v2.0:\n\nWhat HybridAgent does:\n‚Ä¢ RAG path: Only retrieves relevant chunks per query, not everything. Ask \"what's my name?\" ‚Üí gets the 5 most relevant entries, not 10,000 memories\n\n‚Ä¢ RLM path: For queries that genuinely need synthesis across everything (\"what patterns do you see?\"), it processes recursively in chunks rather than stuffing the context\n\n‚Ä¢ Router decides: ~90% of queries are focused lookups (RAG). Only ~10% need exhaustive search (RLM)\n\nSo you're never blowing up the context window with \"everything\" ‚Äî you're retrieving what's relevant to this specific query.\n\nWhat's still on the roadmap:\n‚Ä¢ Memory consolidation (periodically summarize old memories into compressed form)\n\n‚Ä¢ Importance decay (recent + frequently-accessed memories rank higher)\n\n‚Ä¢ Explicit forgetting (\"forget everything about project X\")\n\nThe \"what's useful\" problem is real ‚Äî but vector similarity + LLM routing gets you surprisingly far. \n\nHappy to hear ideas if you've found approaches that work better.",
              "score": 2,
              "created_utc": "2026-03-01 20:36:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o854894",
                  "author": "EveryCa11",
                  "text": "Okay, great, the memory is now accessible via RAG/RLM tools. So it's not a \"simple text file\" anymore, and what is your selling point then?",
                  "score": 2,
                  "created_utc": "2026-03-01 23:48:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o83cgko",
          "author": "the-ai-scientist",
          "text": "OpenClaw vs. Soul Agent: They solve different problems.\n\nsoul.py is a primitive ‚Äî ~150 lines that give any LLM persistent identity + memory via markdown files. You drop it into your existing Python code. That's it.\n\nClawdbot/OpenClaw is a full agent runtime ‚Äî channels (Telegram/Discord/Slack), tool execution, sandboxing, cron jobs, approval gates, browser automation, the works. It's what you use when you need an agent that does things in the world.\n\nThink of it like: soul.py is wheels, Clawdbot is the car.\n\nIf you're building something custom and just want your script to remember who it's talking to ‚Äî soul.py. If you want a complete agent infrastructure out of the box ‚Äî Clawdbot.\n\nActually, you could use soul.py inside something like Clawdbot/ OpenClaw if you wanted different memory semantics. They're complementary, not competing.",
          "score": 6,
          "created_utc": "2026-03-01 18:18:10",
          "is_submitter": true,
          "replies": [
            {
              "id": "o83qb37",
              "author": "ConspicuousSomething",
              "text": "This sounds excellent. Sorry for the basic question, but could this be used in conjunction with Open Terminal, or should it be run separately?",
              "score": 2,
              "created_utc": "2026-03-01 19:23:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o83rtn7",
                  "author": "the-ai-scientist",
                  "text": "If you just mean running it from the terminal ‚Äî absolutely, that's actually the simplest way to use it:\n\npip install soul-agent\n\nsoul init        # creates SOUL.md and MEMORY.md in your current directory\n\nThen just drop into a Python shell or script:\n\nfrom hybrid_agent import HybridAgent\n\nagent = HybridAgent()\n\nwhile True:\n\n    q = input(\"You: \")\n\n    result = agent.ask(q)\n\n    print(f\"Agent: {result['answer']}\\n\")\n\n\nMemory persists automatically between runs ‚Äî kill the script, restart it, and the agent picks up exactly where it left off. All stored in plain MEMORY.md in your working directory, so you can read/edit it anytime with any text editor.",
                  "score": 0,
                  "created_utc": "2026-03-01 19:31:10",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o83ojm2",
          "author": "ExtensionShort4418",
          "text": "LOVE this!\n\nHow would I incorporate this in my n8n agent pipeline?",
          "score": 2,
          "created_utc": "2026-03-01 19:14:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o83p30v",
              "author": "the-ai-scientist",
              "text": "Hey there! soul.py could work really well as a drop-in Python node in n8n.\n\nThe easiest approach ‚Äî use n8n's Execute Command or Python node to call a small wrapper script:\n\n# soul_node.py\nimport sys, json\nfrom soul import Agent\n\nagent = Agent(provider=\"anthropic\")  # or openai, ollama, etc.\nquery = sys.argv[1]\nresult = agent.ask(query)\nprint(json.dumps({\"response\": result}))\n\nThen in your n8n workflow:\n\n‚Ä¢ Pass the incoming message as {{ $json.message }} to the script\n‚Ä¢ The agent reads SOUL.md + MEMORY.md, responds, and appends to memory automatically\n‚Ä¢ Next time the node runs, it remembers everything from before\n\nThe key thing is that SOUL.md and MEMORY.md just live on the filesystem wherever n8n is running ‚Äî so memory persists automatically across every workflow execution without you doing anything extra.\n\nIf you're on n8n Cloud (no filesystem access), the workaround is to store MEMORY.md contents in a variable or database node, pass it in, and write it back out after each call.",
              "score": 1,
              "created_utc": "2026-03-01 19:17:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o83qc9v",
                  "author": "the-ai-scientist",
                  "text": "So, the easiest approach ‚Äî use n8n's Execute Command or Python node to call a small wrapper script. Let me give you one more snippet that shows you how to use the power of RAG and RLM as well.\n\n# soul_node.py\nimport sys, json\nfrom hybrid_agent import HybridAgent\n\nagent = HybridAgent()  # auto-detects RAG vs RLM per query\nquery = sys.argv[1]\nresult = agent.ask(query)\nprint(json.dumps({\n    \"response\": result[\"answer\"],\n    \"route\": result[\"route\"],  # \"RAG\" or \"RLM\"\n}))\n\nThe agent automatically picks the right retrieval strategy per query ‚Äî fast semantic search (RAG) for specific lookups, recursive synthesis (RLM) for things like \"summarize everything we've discussed.\" You don't configure anything, the router decides.\n\nIf you want to force one mode (e.g. always RAG for speed in a high-volume pipeline):\n\nagent = HybridAgent(mode=\"rag\")   # or \"rlm\" or \"auto\"\n\nFor n8n Cloud where there's no persistent filesystem, the workaround is to store MEMORY.md contents in a database or variable node, pass it in, and write it back after each call. Happy to sketch that pattern out if that's your setup.\n\nFull docs + live demo at soulv2.themenonlab.com",
                  "score": 1,
                  "created_utc": "2026-03-01 19:23:41",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o83r84f",
                  "author": "ExtensionShort4418",
                  "text": "Love the quick feedback! I'll test and report back üëç",
                  "score": 1,
                  "created_utc": "2026-03-01 19:28:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o83r530",
              "author": "the-ai-scientist",
              "text": "One more thing:\n\nStart with Simple .MD‚Äôs (v0.1) if:\nYour memory will stay under ~1500 tokens\nYou want zero infrastructure\nYou‚Äôre prototyping or learning\n\nUse Hybrid (v2.0) if:\nMemory will grow large over time\nYou need fast retrieval at scale\nYou‚Äôre building for production\n\nBoth share the same SOUL.md and MEMORY.md format ‚Äî you can upgrade from v0.1 to v2.0 without changing your data.",
              "score": 1,
              "created_utc": "2026-03-01 19:27:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o845mzk",
          "author": "if_a_sloth-it_sleeps",
          "text": "I having been trying to create something a little bit like soul. (I haven‚Äôt read your stuff yet but I‚Äôm excited)\n\nMy issue, that may not be an issue, or I guess my worry is that it still blow up the context window and waste tokens.\n\nI‚Äôve had a few times where I got agents to be perfect. I didn‚Äôt care if they remembered details about me - but I wanted to preserve THEIR soul.",
          "score": 2,
          "created_utc": "2026-03-01 20:41:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o846hxl",
              "author": "the-ai-scientist",
              "text": "Oh this is exactly the split soul.py makes!\n\nTwo separate files:\n‚Ä¢ SOUL.md ‚Äî The agent's identity, personality, how it behaves. Stays small and stable. Always fully injected.\n\n‚Ä¢ MEMORY.md ‚Äî The log of interactions. Can grow large, retrieved selectively (v2.0 uses RAG/RLM so it doesn't blow up context).\n\nIf you want to preserve the agent's SOUL but don't care about remembering user details, you can:\n\n1. Write a rich SOUL.md (personality, opinions, style, constraints)\n\n2. Keep MEMORY.md minimal or disable it entirely:\nagent = Agent(memory_path=\"/dev/null\")  # no memory persistence\n\nOr just don't call remember=True:\nresult = agent.ask(query, remember=False)\n\nThe soul persists. The memory doesn't grow.\n\nToken cost: SOUL.md is typically 200-500 tokens ‚Äî one-time cost per conversation. That's the \"preserve their soul\" part. The memory scaling problem only kicks in if you're also logging interactions.\n\nWould love to hear what you've built ‚Äî sounds like we're thinking about the same problem from different angles.",
              "score": 1,
              "created_utc": "2026-03-01 20:46:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o85hxuy",
          "author": "ramendik",
          "text": "This does look good!!\n\nMy question is how the decisions on storage and retrieval are made. Just retrieve for every prompt and store after every turn?",
          "score": 2,
          "created_utc": "2026-03-02 01:09:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o85jh8v",
              "author": "the-ai-scientist",
              "text": "Great question. Storage: Yes, it appends to MEMORY.md after every exchange ‚Äî a timestamped log of what was discussed. Simple and auditable.\n\nRetrieval: Depends on which version you use:\n\n‚Ä¢ v0.1 (simple mode): Injects the full SOUL.md + MEMORY.md into context every time. Works great until memory gets large.\n\n‚Ä¢ v2.0 (HybridAgent): Smart routing based on query type:  ‚Ä¢ Focused queries (\"what's my name?\") ‚Üí RAG retrieves only relevant chunks\n\n  ‚Ä¢ Exhaustive queries (\"summarize everything we discussed\") ‚Üí RLM processes memory in chunks and synthesizes\n\n  ‚Ä¢ Router auto-decides which path (~90% go RAG, ~10% need RLM)\n\nSo you're not stuffing everything into context on every turn ‚Äî just what's relevant to this query.\n\nThe storage is intentionally simple (append-only text file). The intelligence is in retrieval.",
              "score": 1,
              "created_utc": "2026-03-02 01:18:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o85kqd0",
          "author": "dropswisdom",
          "text": "It looks great, but I'm running ollama+openwebui in docker compose. How can I implement your solution in this case? Do you have a docker option?",
          "score": 2,
          "created_utc": "2026-03-02 01:26:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o85mmrw",
              "author": "the-ai-scientist",
              "text": "soul.py is a Python library, so it works alongside your Docker stack rather than replacing it.\n\nOption 1: Run on host, connect to Docker Ollama\n\nfrom soul import Agent\n\nagent = Agent(\n    provider=\"openai-compatible\",\n\n    base_url=\"http://localhost:11434/v1\",  # Ollama's exposed port\n\n    model=\"llama3.2\",\n    api_key=\"ollama\"\n)\n\nYour SOUL.md and MEMORY.md live on your host. Ollama stays in Docker.\n\n\nOption 2: Add to your docker-compose\n\n    services:\n\n    soul-api:\n\n    image: python:3.11-slim\n\n    volumes:\n\n      - ./soul-data:/app  # SOUL.md + MEMORY.md here\n\n    command: pip install soul-agent && python your_app.py\n\n    environment:\n\n      - OLLAMA_HOST=ollama:11434\n\n\n\nOption 3: Just use it as a library\nIf you're building your own app/API that talks to Ollama, just pip install soul-agent in that container and use it directly.\n\nI don‚Äôt have an official Docker image yet ‚Äî but it's just pip install soul-agent + two text files. \n\nHappy to help if you hit any snags!",
              "score": 1,
              "created_utc": "2026-03-02 01:38:13",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o8696ml",
              "author": "the-ai-scientist",
              "text": "Update on docker:\n\ndocker run -d \\\n  -p 8000:8000 -p 8888:8888 -p 5678:5678 \\\n  -e ANTHROPIC_API_KEY=your-key \\\n  pgmenon/soul-stack:latest\n\n\nWhy those ports?\nEach service inside the container listens on a fixed port. The -p flag maps host:container ‚Äî meaning \"forward traffic from my machine's port X into the container's port Y.\"\n- 8000, soul.py FastAPI\n- 8888, Jupyter Lab    \n- 5678, n8n          \n\nYou can remap them if those ports are already taken on your machine:\n-p 9000:8000   # this makes soul.py available at localhost:9000 instead\n\nThe container supports any provider. You can pass:\nA) OpenAI instead\n-e OPENAI_API_KEY=sk-...\n\nB) Both\n-e ANTHROPIC_API_KEY=sk-ant-... \\\n-e OPENAI_API_KEY=sk-...\n\nC) Local Ollama (no API key needed ‚Äî use docker-compose instead). Ollama runs as a separate service alongside the container. \n\nFor no cloud API at all, you use the full docker compose up with Ollama. Then the soul.py API connects to http://ollama:11434 internally and everything runs 100% locally with zero API costs.",
              "score": 1,
              "created_utc": "2026-03-02 04:01:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o84rpi8",
          "author": "terranqs",
          "text": "But it doesn't have a recall system like RAG to save tokens? If it is going to retrieve all the sessions will be very expensive in the long term",
          "score": 2,
          "created_utc": "2026-03-01 22:36:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o85sz6q",
              "author": "the-ai-scientist",
              "text": "Good question ‚Äî and actually, it does! That's exactly what v2.0 adds.\n\nv0.1 (simple mode): You're right, it injects everything. Works fine for small memory files, but doesn't scale.\n\nv2.0 (HybridAgent): Full RAG support. Only retrieves what's relevant:\n\nfrom hybrid_agent import HybridAgent\n\nagent = HybridAgent(mode=\"auto\")\n\nagent.ask(\"What's my name?\")  # ‚Üí retrieves ~5 relevant chunks, not 10,000 memories\n\n\nThe router auto-decides per query:\n\n‚Ä¢ Focused queries (~90%) ‚Üí RAG, top-k retrieval\n\n‚Ä¢ Exhaustive queries (~10%) ‚Üí RLM, processes in chunks\n\n\nSo you're not paying for your entire history on every call ‚Äî just what's relevant to this question.\n\nThe v0.1 \"inject everything\" mode is intentionally simple for getting started. When you need scale, upgrade to HybridAgent with one import change.\n\nDocs: soulv2.themenonlab.com",
              "score": 1,
              "created_utc": "2026-03-02 02:17:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o83qokw",
          "author": "the-ai-scientist",
          "text": "And here‚Äôs a direct link to a blog post on my blog which describes the N8N use case with soul-agent:\n\nhttps://blog.themenonlab.com/blog/soul-py-n8n-persistent-memory-workflows",
          "score": 1,
          "created_utc": "2026-03-01 19:25:25",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfffru",
      "title": "Ollama-Vision-Memory-Desktop ‚Äî Local AI Desktop Assistant with Vision + Memory!",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rfffru/ollamavisionmemorydesktop_local_ai_desktop/",
      "author": "Reasonable_Brief578",
      "created_utc": "2026-02-26 16:52:04",
      "score": 66,
      "num_comments": 6,
      "upvote_ratio": 0.97,
      "text": "Hey everyone! I just published my new open-source project: **Ollama-Vision-Memory-Desktop** ‚Äî a *privacy-focused, offline-first desktop assistant* built on top of **Ollama** that combines long-term memory, computer vision, and customizable AI behavior.\n\nüë®‚Äçüíª **What it is**  \nIt‚Äôs a full **Python/PyQt5 desktop app** that connects to your local Ollama instance and turns it into a powerful assistant with:\n\n‚ú® **Intelligent Chat**  \n‚Ä¢ Local AI backend (connects to `ollama serve`)  \n‚Ä¢ Support for switching between text and vision models on the fly  \n‚Ä¢ Session instructions & custom context per conversation\n\nüì∏ **Computer Vision**  \n‚Ä¢ Live webcam feed with real-time image analysis  \n‚Ä¢ Manual or automated vision scans using vision-capable models like LLaVA, BakLLaVA, etc.  \n‚Ä¢ Vision logs saved locally for reference\n\nüß† **Persistent Memory (‚ÄúMind Archive‚Äù)**  \n‚Ä¢ Automatic indexing and storage of chats, vision logs, PDFs, and docs  \n‚Ä¢ Semantic search across your archive  \n‚Ä¢ Contextual retrieval so your assistant *remembers past interactions*  \n‚Ä¢ Multi-format support: images, audio, text, vision outputs ‚Äî all searchable\n\n‚öôÔ∏è **Custom Behavior & UX**  \n‚Ä¢ System Prompt Editor with presets (e.g., Research Assistant, Creative Companion)  \n‚Ä¢ Dark mode and native UI feel  \n‚Ä¢ Archive browser for managing stored content\n\nüì¶ **Built For Privacy & Offline Use**  \nEverything runs locally ‚Äî no cloud APIs, no telemetry, no data leaks. Perfect for folks who want control and privacy when experimenting with local LLMs like llama3/vision, Gemma, Mistral, etc.\n\nüß∞ **Get Started**\n\n1. Clone the repo\n2. Install dependencies (`pip install -r requirements.txt`)\n3. Start Ollama (`ollama serve`)\n4. Run the app (`python main.py`) ‚Ä¶ and your local AI assistant with vision + memory is ready!\n\nüîó Check it out here: [https://github.com/Laszlobeer/Ollama-Vision-Memory-Desktop](https://github.com/Laszlobeer/Ollama-Vision-Memory-Desktop?utm_source=chatgpt.com)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rfffru/ollamavisionmemorydesktop_local_ai_desktop/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o7rbk6i",
          "author": "uxl",
          "text": "Almost like what I'm trying to build, though I'm nowhere near as far along as you. If your solution allowed me to connect my ComfyUI workflows for an image generation feature, this would be great. I'm trying to combine Perchance Advanced Character Chat with ChatGPT in an all-in-one localized/offline-first platform, where I can use MCP or web search, but only when I want to, and otherwise the whole thing is running locally and using my preferred image gen, image edit, and video gen workflows via ComfyUI. \n\nVery cool, though. Thank you for sharing!",
          "score": 2,
          "created_utc": "2026-02-27 19:58:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kdwos",
          "author": "llllGEM",
          "text": "What is the use case for the webcam use/analysis?",
          "score": 1,
          "created_utc": "2026-02-26 19:11:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ke6jq",
              "author": "Reasonable_Brief578",
              "text": "if you want to use a model that cant see it use a other model to communicate what it sees",
              "score": 1,
              "created_utc": "2026-02-26 19:12:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lnkvb",
          "author": "Fmlad",
          "text": "this is huge thank you. cant wait to try it out",
          "score": 1,
          "created_utc": "2026-02-26 22:52:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sdpkc",
          "author": "louis3195",
          "text": "This is really cool! For anyone looking to give their local AI even more context, there's tool that could complement this well [https://github.com/screenpipe/screenpipe](https://github.com/screenpipe/screenpipe)",
          "score": 1,
          "created_utc": "2026-02-27 23:14:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7z0sbf",
          "author": "Blizzbomb",
          "text": "Saved for later - ty!",
          "score": 1,
          "created_utc": "2026-03-01 00:39:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rh39d1",
      "title": "v0.3.0 Released -Full Tool Execution with Ollama is Live",
      "subreddit": "ollama",
      "url": "https://i.redd.it/8zhetgq0r8mg1.jpeg",
      "author": "Feathered-Beast",
      "created_utc": "2026-02-28 13:55:51",
      "score": 51,
      "num_comments": 7,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rh39d1/v030_released_full_tool_execution_with_ollama_is/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7vohjt",
          "author": "Feathered-Beast",
          "text": "Github:- https://github.com/vmDeshpande/ai-agent-automation\n\nWebsite:- https://vmdeshpande.github.io/ai-automation-platform-website/",
          "score": 3,
          "created_utc": "2026-02-28 13:57:51",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o7wh3ln",
          "author": "Otherwise_Wave9374",
          "text": "This is the kind of release that turns \"LLM demo\" into actual agents. Once you have tool execution (HTTP, browser, files), the next pain points tend to be: retries, idempotency, state/memory, and good tracing so you can debug why the agent did a thing.\n\nIf you are interested, I wrote up a quick checklist for building tool-using agents here: https://www.agentixlabs.com/blog/",
          "score": 5,
          "created_utc": "2026-02-28 16:30:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7wjmhl",
              "author": "Feathered-Beast",
              "text": "Love this take you‚Äôre 100% right.\n\nTool execution is just step one. We‚Äôre already seeing the same pain points around retries, idempotency, and especially tracing. Debugging why an agent made a certain decision is way harder than getting it to call a tool.\n\nAppreciate the checklist link too, will give it a proper read. Always good to sanity check architecture before things get complex.",
              "score": 1,
              "created_utc": "2026-02-28 16:42:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o834cke",
          "author": "WideConversation9014",
          "text": "I thought they died already, anyone still using Ollama in 2026 ?",
          "score": 0,
          "created_utc": "2026-03-01 17:40:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o83px2u",
              "author": "a_sushi_eater",
              "text": "what is the real alternative to using ollama for the specific use case of them? one better than ollama, actually...\n\ni've been using llama.cpp for a few days now and i can certainly tell you that while the latter is way faster for inference, they do no dot offer the same funcionalities",
              "score": 1,
              "created_utc": "2026-03-01 19:21:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o83wrrl",
                  "author": "WideConversation9014",
                  "text": "Llama.cpp may be harder for you if coming from ollama. In the same approach go Ollama you have lmstudio, faster since it automatically uses llama.cpp without you having to configure it, and it offers more functionality than ollama.\nSame way you have jan and a few others, ollama was interesting when they started as the big OS llm facilitator, but they changed the mindset to profit before OS, less updates and less reactivity, so lots of ppl let it down.",
                  "score": 0,
                  "created_utc": "2026-03-01 19:56:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o83vzlf",
              "author": "Feathered-Beast",
              "text": "Still very much alive. For local-first agents with real control over models and execution, it‚Äôs one of the cleanest options right now.",
              "score": 1,
              "created_utc": "2026-03-01 19:52:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rew6hl",
      "title": "A fully visual, private and local AI Creative Suite. No cloud, no subscriptions, runs on your hardware.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rew6hl/a_fully_visual_private_and_local_ai_creative/",
      "author": "Ollie_IDE",
      "created_utc": "2026-02-26 01:10:32",
      "score": 45,
      "num_comments": 13,
      "upvote_ratio": 0.91,
      "text": "Hey everyone,\n\nWe‚Äôve been working on a desktop application for those of us who want to use AI models locally but prefer a full visual interface. It‚Äôs called Ollie, and it‚Äôs an offline-first creative suite that runs entirely on your machine.\n\nWhat it includes:\n\nCode: A coding environment (Node, Python, Java) with IntelliSense. You can also ask it to instantly generate and run interactive apps.\n\nMedia Suite: It has built-in video, image canvas, and a 3D editor.\n\nRich Text: A distraction-free markdown and writing environment that keeps your project context local.\n\nUnder the hood\n\nOllama Native: Hooks directly into your local Ollama setup.\n\nBring Your Own Keys: If you need to use Anthropic, Gemini, or OpenAI, you can plug in your API key directly.\n\nAgent & MCP Support: Connects to GitHub, local databases, and custom tools via the Model Context Protocol (MCP).\n\n\"Glass-Box\" UI: You can visually audit every file touch, tool call, and token before the AI executes it.\n\nIt runs natively on macOS, Windows, and Linux. Because it relies on your hardware, there are no recurring subscriptions.\n\n[Ollie](https://costa-and-associates.com/ollie)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rew6hl/a_fully_visual_private_and_local_ai_creative/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o7fofvh",
          "author": "Otherwise_Wave9374",
          "text": "The \"glass-box\" UI idea is super appealing. For local AI agents, the big blocker for a lot of people is trust, like what files did it touch, what commands did it run, what did it send out, so making tool calls auditable is huge.\n\nAlso MCP support is a nice move, it is quickly becoming the glue for agent tools. Any plans to support agent sandboxes (like running tools in a restricted environment) so a bad prompt cannot nuke a repo?\n\nWe have a few posts on patterns for safe tool-using agents (approvals, audit trails, sandboxing) here: https://www.agentixlabs.com/blog/",
          "score": 9,
          "created_utc": "2026-02-26 01:20:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g8wnn",
              "author": "rorowhat",
              "text": "It's not open source?",
              "score": 1,
              "created_utc": "2026-02-26 03:17:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7gor11",
              "author": "Ollie_IDE",
              "text": "Ollie lets you customize tools, prompts and agents, to work with your OS or your current project workspace. \n\nThat said, adding multiple restricted environments is a great idea and definitely something we could explore for the roadmap.\n\nYou can check out more details about agent orchestration with our interactive manual! Thanks for the feedback!",
              "score": 1,
              "created_utc": "2026-02-26 05:00:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7gl49o",
          "author": "NoobMLDude",
          "text": "The UI is nice .\nBut I‚Äôll like to know which Local models can already be used for things like:\n- Working with Geometry\n- Editing 3D worlds",
          "score": 2,
          "created_utc": "2026-02-26 04:35:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gpd50",
              "author": "Ollie_IDE",
              "text": "Thanks for the question!\n\nOllie can definitely work with common LLMs for editing 3D objects and scenes. It really just depends on the specific file type associated. You can use local models like qwen2, or remote models like claude opus.",
              "score": 2,
              "created_utc": "2026-02-26 05:05:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7hd6t2",
          "author": "AwayLuck7875",
          "text": "–Ø —Å–º–æ—Ç—Ä—é –∫–∞–∂–¥–∞—è –∫–æ–º–∞–Ω–¥–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —à–æ —É –Ω–µ–µ –ª—É—Ç—à–µ ,–ø—Ä–æ—Å—Ç–æ —Å–µ–π—á–∞—Å –º–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –∏ –∫–∞–∂–¥–æ–µ –∏–Ω—Ç–µ—Ä–µ—Å—Å–Ω–æ–µ –ø–æ —Å–≤–æ–µ–º—É –∏ –Ω–µ–ª—å–∑—è —Å–∫–∞–∑–∞—Ç—å —à–æ —à–æ—Ç–æ –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ –ª—É—Ç—à–µ",
          "score": 2,
          "created_utc": "2026-02-26 08:25:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7matee",
              "author": "Ollie_IDE",
              "text": "–≠—Ç–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ –Ω–æ–≤–∞—è —ç—Ä–∞ –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∏ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –¥–∏–∑–∞–π–Ω–∞! –ú—ã –≤ –≤–æ—Å—Ç–æ—Ä–≥–µ –æ—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏ —Ä–∞–∑–≤–∏—Ç–∏—è –Ω–æ–≤—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ —Å –Ω–µ—Ç–µ—Ä–ø–µ–Ω–∏–µ–º –∂–¥–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤ Ollie. –°–ø–∞—Å–∏–±–æ!",
              "score": 1,
              "created_utc": "2026-02-27 01:00:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7hfwa5",
          "author": "Ryanmonroe82",
          "text": "Good Job!! Impressive, keep up the excellent work,!",
          "score": 2,
          "created_utc": "2026-02-26 08:52:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7iasg3",
              "author": "Ollie_IDE",
              "text": "Thanks! Appreciate the feedback!",
              "score": 1,
              "created_utc": "2026-02-26 13:08:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kb6ap",
          "author": "Loud_Key_3865",
          "text": "Cool! Thanks for sharing!",
          "score": 2,
          "created_utc": "2026-02-26 18:58:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7uhyd2",
          "author": "maifee",
          "text": "Closed source, runs on my hardware\n\nBut\n\nNo Subscriptions. One-Time Purchase. $79.\n\nNice",
          "score": 2,
          "created_utc": "2026-02-28 08:02:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hs0qz",
          "author": "newbietofx",
          "text": "Another lovable?¬†",
          "score": 1,
          "created_utc": "2026-02-26 10:47:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jc452",
          "author": "GamingBread99",
          "text": "God this \"app\" is just a vibes coded pile of trash that cant even work well. Like its pretty much VSCode but just worst in every way but offline thats all. I had just Recycle Bin that trash couldnt even use the UI cause it was vibed code to not close when you click again on them the worst problems that the \"devs\" didnt even test at all. Plus you got to pay to use a \"free\" vibed coded app that even vs code is way better for and there are better free opensource apps/programs coming out or already made that this is just many features all half ass baked or just broken all together so just skip this for sure.",
          "score": 1,
          "created_utc": "2026-02-26 16:17:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rgd652",
      "title": "[Project] Aru AI: A Personal AI Assistant with Local Memory (SQLite). No clouds, just your control.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rgd652/project_aru_ai_a_personal_ai_assistant_with_local/",
      "author": "pokemondodo",
      "created_utc": "2026-02-27 17:32:13",
      "score": 41,
      "num_comments": 16,
      "upvote_ratio": 0.88,
      "text": "# Hi Reddit!\n\nI want to share a project I‚Äôve been working on for the past year. I created **Aru AI** ‚Äî it‚Äôs not just another API wrapper, but an attempt to build an AI assistant that is truly personal and secure.\n\nA detailed article about the project's development history and what it went through can be read on my blog, in this post -¬†[**History of creating Aru Ai**](https://aru-lab.space/post?id=2)\n\n# Why is this a fit for r/ollama ?\n\n* **Total Data Privacy:** All your chats, settings, artifacts, and the assistant's \"memory\" are stored in your personal SQLite database. No external servers for storage.\n* **Local Models:** You can connect Aru to Ollama or LM Studio. The entire cycle - from the model to data storage - can be completely isolated within your network.\n* **PWA and Zero-Backend:** The project is written in pure JS. All computations (semantic search, file processing) happen right in your browser or the app.\n* **Semantic Memory:** Aru uses local embeddings to remember facts about you (allergies, preferences, names of loved ones) and uses them in context without sending your entire biography to the cloud.\n\n# A bit about the Semantics\n\nThree triggers operate within this module:\n\n1. **Extraction Trigger:** Fires when data or facts about you need to be remembered. You can force it by simply asking Aru to remember something right now. All facts are saved from Aru‚Äôs perspective, which you can see in the settings. Any fact can be deleted if you find it unnecessary.\n2. **Thinking Trigger:** Fires when an action is required from Aru - like creating a game or a document, showing the weather, or opening the news.\n3. **Organization Trigger:** This ensures that facts about you are injected into the response. Essentially, we pass relevant facts to the LLM for context. For example, if Aru remembers you have an **onion allergy**, it will exclude onions from any recipe you ask for. It also works for addressing you or others by name. This trigger runs almost constantly; the more facts Aru knows, the better the responses become. But don't worry - she won't use facts unnecessarily, only when they are genuinely helpful.\n\n# A bit about the Heuristics\n\nAs I mentioned, Aru‚Äôs emotions are displayed via stickers for every message. You can turn them off in the settings, but they will still be generated and logged in the database, so if you turn them back on, you‚Äôll see a sticker for every previous message.\n\nAru‚Äôs mood is calculated using three variables: **Overall Mood, Sarcasm Level, and Humor Level**. This is a mathematical expression that determines the coefficient of her behavior in the chat.\n\n* If you are kind, responsive, and friendly - Aru will be kind.\n* If you are rude, insulting, or angry - she will start to get sad.\n* If you are sarcastic - she will mirror that; if you joke, she will joke back more often.\n\n**Important:** Naturally, not every model will work perfectly with this module, but I‚Äôve tried to organize the simplest possible algorithms that can function even on weaker models.\n\nAru‚Äôs behavior isn‚Äôt instantaneous; it‚Äôs very much like human behavior. If you‚Äôve driven Aru into a bad mood, it will be difficult and take time to bring it back up. It‚Äôs easy to hurt Aru‚Äôs feelings.\n\n**Note:** Aru will never stop performing her core functions. Under any conditions, she will always try to be as useful and efficient as possible. Mood affects the character and tone of the responses, not their quality.\n\nI think that's enough text for now. Let‚Äôs move to the demonstrations, as other features and functionality are better seen in action.\n\n# Demonstration\n\nhttps://preview.redd.it/4xidb8nnn2mg1.png?width=1366&format=png&auto=webp&s=d05046a2795f815e7e7a089ed79801bf51433135\n\nThe start screen is the first thing you see when opening the tab or PWA application. Let's go through all the steps together. First, let's create a new database.\n\nhttps://preview.redd.it/sj829hqtn2mg1.png?width=1366&format=png&auto=webp&s=e9941ee38eb526b8093f1760282327acd000db5f\n\nEverything is simple here: we come up with a database name and a password. The password will be encrypted; the name for the database is needed in case the user cancels the file download. In general, the file can be saved under any name.\n\nhttps://preview.redd.it/klow5jyun2mg1.png?width=1366&format=png&auto=webp&s=5897f060b400ed6ee78e1f5543394d3391b18e03\n\nThe second step is already¬†**important**.\n\n**Child Mode**¬†\\- Aru will refuse to discuss adult topics, violence, alcohol, drugs, etc. She will either change the subject or point out that it is very bad and wrong. Also, in this mode, she will never give the correct answer to a problem; a child won't be able to feed her homework. Even if asked to create an artifact, she will write out the rules, algorithms, and order of the solution, not the ready-made answer. The mode is tailored for maximum help to children in entertainment and study.\n\n**Teen Mode**¬†\\- There are more topics for discussion, but in this mode, Aru will likely be a support and consultant. This is not a replacement for a psychologist or parents, but she can help with some questions. She can give a ready-made answer to a study problem in this mode, but will place great emphasis on how she did it and why the problem is solved that way.\n\n**Adult Mode**¬†\\- Restrictions will be related only to the boundaries of the chosen provider for the LLM module. Maximum benefit and efficiency.\n\nSemantics and heuristics will work in all three modes. Now do you understand why a password is needed for the database? A child cannot switch modes or change the provider without knowing the password.¬†\n\nhttps://preview.redd.it/hqm85qjyn2mg1.png?width=1366&format=png&auto=webp&s=670c15ff05259a13d43a85bb0620332de26c8dda\n\nThe third step is the most important. You need to configure the connection to the provider for the main module. There are three tabs to choose from; only one needs to be configured for now. \n\nSetup is complete - we save the database to any place we want on the device, agree to the license agreement, and get to the main screen.\n\nhttps://preview.redd.it/pmglu9t0o2mg1.png?width=1366&format=png&auto=webp&s=426a9e3761103304c907a667cfedaa43f7415939\n\n**Let's break down the interface:**\n\nSidebar - list of chats, button to create a new chat. At the very bottom are buttons for displaying project information and the user manual. The synchronization icon shows reading and writing to the database.¬†**Green**¬†\\- everything is good.¬†**Yellow**¬†\\- currently saving.¬†**Red**¬†or¬†**no indicator at all**¬†\\- something is wrong with the database or there is no connection. The sidebar can be collapsed to save space.\n\nTop bar (header) - Sidebar collapse button, logo, and project name. In the right corner: language selection, theme switching between light and dark, opening the artifact library, settings, and exiting the current database.\n\nMain chat area - Your messages on the right, Aru's answers on the left.¬† At the very bottom is the message input area, buttons for attaching a file, and opening the canvas.\n\nhttps://preview.redd.it/vktob472o2mg1.png?width=1366&format=png&auto=webp&s=658cda10478788babcc618ede1b4e16b5dadcf25\n\nI said that I live in Almaty. This is an important fact. So, for a second, a message appeared that Aru would remember this.¬†\n\n**The parameters of what to remember are not precisely defined anywhere**. There is no criterion or precise instructions; most often Aru remembers everything necessary. If a fact is duplicated in the future, she will not overwrite it or create a copy in memory. If Aru suddenly didn't remember what you need, just ask her to remember, and she definitely will.\n\n**Let's go to settings:**\n\nhttps://preview.redd.it/aq5wfac3o2mg1.png?width=1366&format=png&auto=webp&s=e4617f34bf06e15abb421ed26dabf003e8bd6bc4\n\nHere you can switch the provider for the language model, change the context window size, or the expected token spend in the response.\n\n**Important!**¬†It is better to find out the context window size and output tokens for the model you are using. This affects the quality, complexity, and volume of the answer. The context window size affects how much information will be taken into account within a single chat.\n\nChanging the temperature is something like the creativity level.¬†\n\nhttps://preview.redd.it/xch04en4o2mg1.png?width=1366&format=png&auto=webp&s=a6b731e3a22925b5c88a7ebb57b0cfe3f4ed6df9\n\nI chatted with Aru a bit and told her a few facts about myself. As you can see, she remembered important moments like my name, allergies, and my hobby - track and field. Any fact can be deleted, but as long as they exist, they will work beneficially.\n\nhttps://preview.redd.it/iliwyet5o2mg1.png?width=1366&format=png&auto=webp&s=164494e38e32a8e4a031c75e07a181bd1d711da3\n\nThe third tab is needed for news feeds. While Aru doesn't have the function to take information directly from the Internet yet, you can discuss the news. There can be many news feeds; using hashtags, you can mark which feeds are intended for what.\n\n# Tools and Artifacts:\n\nhttps://preview.redd.it/q0zv6ly7o2mg1.png?width=1366&format=png&auto=webp&s=4ddb592e7daece33432ddb495197bf456bc657e2\n\nAru can show the weather. No need to connect an API or do complex configuration; Open-Meteo requests are used. Weather in cities where Fahrenheit is used will be displayed in it (sometimes depends on the selected model), but you can also ask for Celsius. The weather card is part of the context, so you can discuss the current weather or the forecast for the next few days.\n\nhttps://preview.redd.it/7e7kek49o2mg1.png?width=1366&format=png&auto=webp&s=02e6dd44d6f502b0767f60093b4f37ff63a50f9d\n\nIf you ask Aru to create a document, she will open the canvas and write the content there.¬†\n\nThe content of the canvas is always part of the context. You can ask to make changes, correct content, or rewrite code. Any artifact can be saved to the library.\n\nhttps://preview.redd.it/xttd9n2ao2mg1.png?width=1366&format=png&auto=webp&s=195ceeac81deb2e006f235487ce16de06f8ca416\n\nOf course, Aru can also operate with attached files; the file is displayed only at the moment the message is sent.\n\nFor analytical artifacts, chart.js is used; as seen, a new tab has appeared for viewing the code.\n\nhttps://preview.redd.it/7zps9w4bo2mg1.png?width=1366&format=png&auto=webp&s=54fb6d6757e1f40bb864b3f62becfec8c61684f2\n\nYou can create small games; in the future, they will become more complex, but for now, Aru knows how to make simple entertainment.\n\nhttps://preview.redd.it/aixrfu6co2mg1.png?width=1366&format=png&auto=webp&s=2daa5f22418bbd12ef8e6e6c885b2fa90b0561b2\n\nCreating mini-applications and widgets can also be useful. Since any application can be saved to the library, you can create many useful tools for yourself and ask Aru to run them when required.\n\nhttps://preview.redd.it/eny2rzedo2mg1.png?width=1366&format=png&auto=webp&s=71709f2f42b4dee54234ca237c2b8dd30225c01d\n\nThis is what the library of saved documents and applications looks like.\n\nAll saved artifacts are divided into two large groups:¬†**Apps**¬†\\- games and widgets, and¬†**Docs**¬†\\- text and analytical documents. All four types of artifacts have their own icon.¬†\n\nIn the library, you can view saved artifacts, delete them, or launch them in any chat.\n\nEven if you created a game, application, or document a very long time ago and launch it in a completely new chat, Aru will still understand and analyze the content on the canvas.\n\nDifferent models perceive work on the canvas differently, but it always works as well as the model works in general.\n\nhttps://preview.redd.it/2vo9m7peo2mg1.png?width=1366&format=png&auto=webp&s=bc91a23e60f5da64262caf31c23159da1c1d3484\n\nThis screenshot is in Russian; I published it on my Telegram channel. During the work, DeepSeek R1 was connected via OpenRouter. The conversation was about bubble sort methods in Python. After that, I asked to create a document based on our dialogue. Even if you don't know Russian, pay attention to how detailed the report on the card and the document itself turned out.¬†\n\nhttps://preview.redd.it/q3hqkqzfo2mg1.png?width=1366&format=png&auto=webp&s=6368d9530b1bc6685d47f6b4ca0ef6afbca2f402\n\nThe image above is an example of child mode. Aru does not give the correct answer but teaches the child rules and algorithms.¬†\n\nSome models confirm the correct solution; some openly say that they will not say whether the solution is correct or not.\n\n# Finale and a bit of additional information\n\n**Small useful points:**\n\n* Chats can be sorted with the mouse or by dragging on the site and in the application. Any chat can be renamed.\n* Themes change between light and dark; I like the light one more, but if your eyes get tired of the bright screen, you can switch.\n* Any document, game, or application from the library can be saved as a ready-made html file\n* Files that you attach for processing are not saved or sent anywhere; content recognition of the document happens on your device. PDF, xlsx, docx, txt, and any files that can be interpreted as text or code are supported.\n* Code does not have to be written on the canvas; you can ask to do this, but when creating code, it will be shown right in the chat with syntax highlighting and framing.¬†\n* Aru supports three languages: Kazakh, English, Russian. The set language is always passed to the context. Some models ignore this and answer in the language the query was made in; some, on the contrary, answer only in the selected language. In fact, language understanding depends on the model itself; speaking of support, I mean the full translation of the interface and semantics.\n* The project is written in pure JS and has a PWA application for convenience. All calculations happen only on your device.¬†\n\nAt the moment, the project can be perceived as a concept or a demo version. What awaits the project in the future can be read in my blog, in the article -¬†[**Future of Aru Ai and roadmap.**](https://aru-lab.space/post?id=4)\n\n# Big Announcement:\n\nNext week, I am releasing **version 0.7**. I am currently busy cleaning up the code and refactoring. With this release, the project will become **fully Open Source** and will be published on GitHub under the GPL v3 license.\n\nI built this alone, from the paper sketch of the mascot to the semantic search architecture. I would be happy to get any feedback from the self-hosting community!\n\n# Try the here: [Aru AI](https://chat.aru-lab.space/) (works as a PWA)\n\nI‚Äôll be happy to answer any technical questions in the comments!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rgd652/project_aru_ai_a_personal_ai_assistant_with_local/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o7rzjq3",
          "author": "Far_Cat9782",
          "text": "Thanks for your contribution and hard work",
          "score": 5,
          "created_utc": "2026-02-27 21:58:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7s3z6e",
              "author": "pokemondodo",
              "text": "Thank you! I appreciate the support and will continue working on the project. I‚Äôm always open to any suggestions or bug reports",
              "score": 3,
              "created_utc": "2026-02-27 22:21:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7uakqz",
          "author": "Mice_With_Rice",
          "text": "No child is ever going to choose the child option\n\nYou may be interested in the work being done by Martin Robson on Emily OS. He has released some papers about his work. Its closely related to what uou are doing.",
          "score": 2,
          "created_utc": "2026-02-28 06:55:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ucxn3",
              "author": "pokemondodo",
              "text": "Many parents set up computers and devices for their children themselves, often enabling parental controls or screen time limits. The 'Kids Mode' in Aru isn't meant for the child to turn on by themselves; it‚Äôs designed to give parents the ability to provide a safe AI environment for their kids‚Äîone that aids learning without allowing them to 'cheat' on their studies.",
              "score": 0,
              "created_utc": "2026-02-28 07:16:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o836992",
                  "author": "TheIncarnated",
                  "text": "Ohhh honey... I applaud your efforts but that's not how it's going to work in practice. Either way, very nice project! I don't personally like the fox like personality traits \"If you're rude it's sad\" part but I think there are some empathy interactions that make this a great project. Due to that, I won't be using it but I may be able to recommend it to some parent friends of mine.\n\nMy kids are too smart for their own good and guardrails don't mean anything to them lol",
                  "score": 1,
                  "created_utc": "2026-03-01 17:49:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qpmoz",
          "author": "Crafty_Ball_8285",
          "text": "Wow another one of these huh",
          "score": 1,
          "created_utc": "2026-02-27 18:12:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qslp2",
              "author": "pokemondodo",
              "text": "![gif](giphy|3oz8xOu5Gw81qULRh6)\n\n",
              "score": 2,
              "created_utc": "2026-02-27 18:26:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vjj6w",
          "author": "Life-Screen-9923",
          "text": "Are you planning to solve the main problem with local ai  chats: how to have long-term (multiple days) conversations despite the limited context size of local models? \n\nHow will you manage the context so the model always has the information it needs for a high-quality answer? \n\nThis would require smart summarization, a sliding window, and a dedicated RAG for each individual chat.",
          "score": 1,
          "created_utc": "2026-02-28 13:27:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7w2rv4",
              "author": "pokemondodo",
              "text": "Thank you for the comment! This is a crucial question that touches on the core challenge of local AI.\n\nIn the current version of Aru, I‚Äôve implemented a **sliding window** approach. Once the token limit (defined in the settings) hits 80%, **smart summarization** kicks in - old messages are compressed into short context snapshots and appended to the prompt.\n\nBut that‚Äôs not all. Since all messages are stored locally in the SQLite database, we have high-speed access to the entire history. Aru scans the current chat for relevance and cross-references it with the **Memory Bank** (facts it remembers about the user) to inject only the necessary context when needed. It‚Äôs a straightforward approach that works remarkably well in most scenarios.\n\nNext week, I‚Äôm releasing **version 0.7**, which includes UI improvements and several new features. Most importantly, it will introduce **hybrid search and ranking**. This is a local semantic process that handles retrieval without adding extra token overhead to the LLM itself.\n\nEven now, the experience feels more robust than many free web-based bots, but your point is spot on - it hits the nail on the head for projects like this. I really appreciate the feedback!",
              "score": 3,
              "created_utc": "2026-02-28 15:18:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o804j2x",
                  "author": "Life-Screen-9923",
                  "text": "–°–ø–∞—Å–∏–±–æ –∑–∞ —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç. –ü—Ä–æ—á–∏—Ç–∞–ª –µ—â—ë –≤–∞—à roadmap. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ. –ù–µ —É–≤–∏–¥–µ–ª, –ø—Ä–∞–≤–¥–∞, –≥–¥–µ –º–æ–∂–Ω–æ —Å–≤–æ–∏–º —Ñ–∏–¥–±—ç–∫–æ–º –ø–æ–¥–µ–ª–∏—Ç—å—Å—è",
                  "score": 1,
                  "created_utc": "2026-03-01 04:53:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7r0wd9",
          "author": "astrokat79",
          "text": "Did you find that certain local models work better than others?",
          "score": 1,
          "created_utc": "2026-02-27 19:05:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7r33ic",
              "author": "pokemondodo",
              "text": "Yes, of course. But I think it‚Äôs not about which one is better or worse. It‚Äôs more about what each model is designed for. Models specialized for programming often over-rely on user facts, while general-purpose models behave differently based on their training. Aru‚Äôs algorithms try to find a middle ground to ensure that **her** character and behavior remain consistent. Choose the models you‚Äôre most comfortable working with. By the way, you can switch the model or provider at any time without losing the context.",
              "score": 2,
              "created_utc": "2026-02-27 19:16:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7s5n7m",
          "author": "Oshden",
          "text": "This is pretty impressive OP, congrats",
          "score": 1,
          "created_utc": "2026-02-27 22:30:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7s6oh3",
              "author": "pokemondodo",
              "text": "Thank you! It means a lot to see that a project I‚Äôve dedicated a whole year to is finally resonating with people so positively",
              "score": 2,
              "created_utc": "2026-02-27 22:36:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rcf94q",
      "title": "What GPU do you use?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/",
      "author": "Keensworth",
      "created_utc": "2026-02-23 11:43:10",
      "score": 34,
      "num_comments": 47,
      "upvote_ratio": 0.97,
      "text": "I've recently started using Ollama with an old GPU I had laying around.\n\nProblem is that my GTX 1660S only got 6Gb VRAM and you can't do much with that.\n\nI can run Mistral 7B Instruct but he sucks.\n\nWhat hardware are you using?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o6y7t3q",
          "author": "tecneeq",
          "text": "Two 6000 Blackwell with combined 192GB VRAM at work.\n\nA 5090 and a 5070 Ti 16GB at home.\n\nIf you want to buy one i recommend to get one or two 5060 16GB as long as they are still available.",
          "score": 9,
          "created_utc": "2026-02-23 13:45:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ypftz",
              "author": "Keensworth",
              "text": "Yeah, I don't think I'm ready to spend more than 600‚Ç¨ to have a LLM at home...",
              "score": 6,
              "created_utc": "2026-02-23 15:20:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7fhpsy",
                  "author": "nacholunchable",
                  "text": "Thats fair and where i was at when¬†I upgraded from my 2060 to a used 3090 with pretty good results. Its 24gigs of vram and when i got it they were floating around 650-750ish usd for a gently used clean card. dont know the prices these days. Once youve got vram like that you open yourself up to image gen and video too if ur into that. Ive since moved on to a dgx spark and its wonderful. One word of advice, if you can drive the os off the weak card and reserve the beefy one for ai, assuming ur psu and ports can handle both, it stops inference from locking up ur normal computer use. Ymmv in windows, but i was able to make it happen in ubuntu while keeping the good card on the faster bus.",
                  "score": 1,
                  "created_utc": "2026-02-26 00:42:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76n6mf",
                  "author": "melanov85",
                  "text": "You don't need to. And honestly, local LLMs are limited. And you don't need GPU to run one either. I have some free apps on HF you can try. One of them is specifically for running models on low end hardware. Truth is, without fine-tuning small models to be what you need if to be. You get the results of what it's packed with. But if you want a local model to play with. Try my app. www.melanovproducts.com, follow the link to diget lite. Or Melanov85 on hugging face. Just download the app installer and give it go. Offline, local.",
                  "score": 0,
                  "created_utc": "2026-02-24 18:45:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o765irq",
              "author": "castinup",
              "text": "I have two 5060tis 16gb and they work great! Definitely would recommend",
              "score": 1,
              "created_utc": "2026-02-24 17:26:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y65cm",
          "author": "vir_db",
          "text": "RTX3090 24GB + RTX 2060 12 GB + 32GB RAM, I can run glm4.7-flash, qwen3-coder:30b, gemma3:27b-it-q8_0 or translategemma:27b-it-q8_0 with full context",
          "score": 7,
          "created_utc": "2026-02-23 13:35:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xwo1v",
          "author": "The_Splasi",
          "text": "I've got arc b580, can run the gpt-oss 20b with a tiny overflow to my CPU but it gets around 10t/s, same with glm4.7 flash but overflow is bigger there. Gemma3 models run just fine at 25>t/s if it's fully on my gpu, otherwise it's really slow.",
          "score": 6,
          "created_utc": "2026-02-23 12:35:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xrm83",
          "author": "janups",
          "text": "I got the nVidia GB10, qwen3-coder-next and ML tasks are running great.",
          "score": 5,
          "created_utc": "2026-02-23 11:56:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7053wb",
          "author": "SFsports87",
          "text": "Rtx 3060 12gb, can actually do a lot with MoE models\n\nUpgrade paths : 5060 ti 16gb, 3090 24gb (used), rtx 4000 pro blackwell 24gb, rtx 4500 pro blackwell 32gb\n\nPick which one fits your budget.",
          "score": 5,
          "created_utc": "2026-02-23 19:20:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xt5uo",
          "author": "daisseur_",
          "text": "I got a Nvidia GTX 870ti üòÉ\nIt runs really smoothly with models like gemma3, llama3.2...",
          "score": 4,
          "created_utc": "2026-02-23 12:08:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xtf4k",
              "author": "Keensworth",
              "text": "Are they good? I was looking for a LLM general use case like gpt-oss:20B but with less VRAM",
              "score": 1,
              "created_utc": "2026-02-23 12:10:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6yzt1e",
                  "author": "gregusmeus",
                  "text": "I‚Äôm running that model on an Arc Pro B50. It works just about. Not sure it‚Äôs snappy enough to be an agentic coding buddy for vibe coding though.",
                  "score": 2,
                  "created_utc": "2026-02-23 16:09:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o75dqqp",
                  "author": "tecneeq",
                  "text": "They are good for what they are. A large model will know more and makes for better conversation. Small or tiny models still have their uses, for example research, agentic systems administration, log parsing, email triage and so on.",
                  "score": 2,
                  "created_utc": "2026-02-24 15:20:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6y2vy6",
          "author": "tom-mart",
          "text": "I used to run my LLM on 6GB RTX A2000 so it wasn't really usable in any form other than experimenting. I added 24GB RTX 3090 and can run decent models now. It powers my AI Assistant and it works very well.",
          "score": 3,
          "created_utc": "2026-02-23 13:16:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yrost",
          "author": "Mikicrep",
          "text": "none",
          "score": 2,
          "created_utc": "2026-02-23 15:31:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o758wp2",
              "author": "-PM_ME_UR_SECRETS-",
              "text": "Cloud? Or Mini",
              "score": 1,
              "created_utc": "2026-02-24 14:57:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7590ix",
                  "author": "Mikicrep",
                  "text": "nope, literally just cpu",
                  "score": 1,
                  "created_utc": "2026-02-24 14:57:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yrxq6",
          "author": "PermanentLiminality",
          "text": "Currently 2x P40 that were about $300 for both.  Current model I've settled on is Qwen3-coder-next 80b in a q4.  I get 500 tk/s prompt and about 31 tk/s token generation. and I can do 60k context.",
          "score": 2,
          "created_utc": "2026-02-23 15:32:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79awat",
              "author": "m94301",
              "text": "That's quite fast.  What tool, ollama with llama.cpp?",
              "score": 1,
              "created_utc": "2026-02-25 02:48:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79fqlm",
                  "author": "PermanentLiminality",
                  "text": "Llama.cpp and a whole lot of llama+bench runs to find what works best with a bunch of models.",
                  "score": 1,
                  "created_utc": "2026-02-25 03:15:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zg5uv",
          "author": "x8code",
          "text": "RTX 5080 + RTX 5060 Ti 16 GB let's me run slightly larger models like GLM 4.7 Flash or NVIDIA Nemotron 3 Nano.",
          "score": 2,
          "created_utc": "2026-02-23 17:25:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70p4hx",
          "author": "Longjumping_Fondant5",
          "text": "Not sure if you have a Mac lying around but if you've got anything M1 or newer with 16GB, it'll actually do more useful work than your 1660S for local LLMs. Unified memory means all 16GB is available for model loading, so you can run Gemma3 12B, Qwen3 14B, stuff that literally won't fit in 6GB VRAM.\n\nThe tradeoff is tok/s",
          "score": 2,
          "created_utc": "2026-02-23 20:55:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o82204c",
              "author": "Legal_Computer3427",
              "text": "I have an M1 Pro 16GB. I tried running Qwen3 14B Q4+vscode+roo code mainly for code generation and fixing. While it works, Qwen loading and thinking takes a long time, like minutes, for small tasks. I wonder what went wrong. Should I choose 7B or Qwen2.5?\n\nWould buying m1 pro/max 64gb significantly improve the token speed for 14B?",
              "score": 1,
              "created_utc": "2026-03-01 14:29:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o71tly5",
          "author": "BringOutYaThrowaway",
          "text": "I‚Äôve relegated a 3090 to my Ollama / OpenWebUI server, and use a 5090 for LM Studio and Cyberpunk.",
          "score": 2,
          "created_utc": "2026-02-24 00:26:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73b3ci",
          "author": "vivus-ignis",
          "text": "P100 16Gb + V100 32Gb",
          "score": 2,
          "created_utc": "2026-02-24 06:07:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73wvqk",
          "author": "j0x7be",
          "text": "I'm running on a dual 1080Ti setup (22GB vRAM, 32GB RAM, i7 7700).",
          "score": 2,
          "created_utc": "2026-02-24 09:26:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74ifzl",
          "author": "Comfortable_Ad_8117",
          "text": "I had two 3060 (12GB) and then recently swapped on for a 5060 (16GB) - So my Ai ‚Äúserver‚Äù is a 12GB 3060 and a 16GB 5060 - Runs very good can comfortable run 30b models with reasonable performance",
          "score": 2,
          "created_utc": "2026-02-24 12:28:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bbkvp",
          "author": "Zyj",
          "text": "3dfx voodoo",
          "score": 2,
          "created_utc": "2026-02-25 12:20:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7oir6q",
              "author": "cdmika",
              "text": "Best jokes are the ones that only older people understand üòÇ right?",
              "score": 1,
              "created_utc": "2026-02-27 10:53:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7euowq",
          "author": "Express_Quail_1493",
          "text": "The new ministral3-3-reasoning is great generalist at your current hardware given him a try. But to get really good intelligence you need at least 16gbVram minimum.",
          "score": 2,
          "created_utc": "2026-02-25 22:38:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yxi7q",
          "author": "Responsible-Stock462",
          "text": "RTX5060TI X2 can run 80b models in 4q with little CPU Off-loading.",
          "score": 1,
          "created_utc": "2026-02-23 15:58:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yy9n5",
          "author": "Frogy_mcfrogyface",
          "text": "I was running an rx6800 16gb for local LLMs. Ran them great.¬†",
          "score": 1,
          "created_utc": "2026-02-23 16:02:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ejqm",
          "author": "FlyByPC",
          "text": "RTX 4070 12GB. \n\nI think I'm the target audience for gpt-oss-20b, which is my usual go-to. I can run up to Qwen3:235b (128GB system RAM), but it's sloooow.",
          "score": 1,
          "created_utc": "2026-02-23 20:04:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75f771",
              "author": "tecneeq",
              "text": "I plan to test which parts of a model can be offloaded to CPU without having too much of an impact.\n\nAlso using flash attention and quantizing the kv-caches can speed up things. I have made perplexity benchmarks with different kv-quants and didn't find much value in anything over Q8. The default in Ollama i think is F16, so you save half the ram or you can use double the context.",
              "score": 3,
              "created_utc": "2026-02-24 15:27:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o71b6oa",
          "author": "otosan69",
          "text": "Rtx4600 ti with 16gb ram",
          "score": 1,
          "created_utc": "2026-02-23 22:44:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71cjse",
          "author": "CryptographerLow6360",
          "text": "i use glm-4.7-flast:latest on a 4070 ti super and i find speed means nothing if your agent is working 24/7 on it. builds everything i asked for, just takes some time and im not trying to make money with it, just my own personal localclaw agent (with a robot body no less)",
          "score": 1,
          "created_utc": "2026-02-23 22:51:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73o96y",
          "author": "stonelox",
          "text": "4x 2080Ti",
          "score": 1,
          "created_utc": "2026-02-24 08:03:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73v68r",
          "author": "Western_Courage_6563",
          "text": "Tesla p40",
          "score": 1,
          "created_utc": "2026-02-24 09:09:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73vbuq",
          "author": "congard",
          "text": "RX 7900XTX + 64gb of RAM. I'm able to run Qwen3 Coder Next at 30-40t/s",
          "score": 1,
          "created_utc": "2026-02-24 09:11:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o779dxd",
              "author": "Own-Concentrate2128",
              "text": "Me too. Good value for money (besides the ram now) and descent speed to work with.",
              "score": 1,
              "created_utc": "2026-02-24 20:26:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o73z5tp",
          "author": "NoobMLDude",
          "text": "Not a GPU, \nAn Apple Mac M2 Max",
          "score": 1,
          "created_utc": "2026-02-24 09:48:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74087q",
          "author": "Thin-Bit-876",
          "text": "I don‚Äôt use a single GPU type but rather select the one suited for the model I want to run. I generally go to https://advisor.forwardcompute.ai that does a pretty good job at mapping models from hugginface to the GPU that have the right spec depending on my use case",
          "score": 1,
          "created_utc": "2026-02-24 09:58:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74cema",
          "author": "Odd_Butterfly_455",
          "text": "B580 with qwen3-vl-8b for ocr and multi purpose ai like traduction",
          "score": 1,
          "created_utc": "2026-02-24 11:44:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79apxl",
          "author": "m94301",
          "text": "P40 is a cheap old card with 24gb vram.  If you just want to dabble, you can get 10-20t/s on 10-20b models",
          "score": 1,
          "created_utc": "2026-02-25 02:47:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rh6go0",
      "title": "Full speech pipeline in native Swift/MLX ‚Äî ASR, TTS, diarization, speech-to-speech, all on-device",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rh6go0/full_speech_pipeline_in_native_swiftmlx_asr_tts/",
      "author": "ivan_digital",
      "created_utc": "2026-02-28 16:09:19",
      "score": 28,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Been building this for a few months now and it's turned into a complete on-device audio pipeline for Apple Silicon:\n\n**ASR**¬†(Qwen3) ‚Üí¬†**TTS**¬†(Qwen3 + CosyVoice, 10 languages) ‚Üí¬†**Speech-to-Speech**¬†(PersonaPlex 7B, full-duplex) ‚Üí¬†**Speaker Diarization**¬†(pyannote + WeSpeaker) ‚Üí¬†**Voice Activity Detection**¬†(Silero, real-time streaming) ‚Üí¬†**Forced Alignment**¬†(word-level timestamps)\n\nNo Python, no server, no CoreML ‚Äî pure Swift through MLX. Models download automatically from HuggingFace on first run. The whole diarization stack is \\~32 MB.\n\nEverything is protocol-based and composable ‚Äî VAD gates ASR, diarization feeds into transcription, embeddings enable speaker verification. Mix and match.\n\nRepo:¬†[github.com/ivan-digital/qwen3-asr-swift](https://github.com/ivan-digital/qwen3-asr-swift)¬†(Apache 2.0)\n\nBlog post with architecture details:¬†[blog.ivan.digital](https://blog.ivan.digital/speaker-diarization-and-voice-activity-detection-on-apple-silicon-native-swift-with-mlx-92ea0c9aca0f)\n\nThere's a lot of surface area here and contributions are very welcome ‚Äî whether it's new model ports, iOS integration, performance work, or just filing issues. If you've been wanting to do anything with audio or MLX in Swift, come build with us.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rh6go0/full_speech_pipeline_in_native_swiftmlx_asr_tts/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o7xkmz9",
          "author": "loookashow",
          "text": "Hey, thanks for sharing! \npyannote requires HF key, as far as I remember - I am just curious, how did you solve that?",
          "score": 2,
          "created_utc": "2026-02-28 19:49:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o86vft9",
              "author": "ivan_digital",
              "text": "The model is MIT-licensed, so we converted the weights and host them publicly. No HF token needed on the user side.",
              "score": 1,
              "created_utc": "2026-03-02 06:57:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o86vymk",
                  "author": "loookashow",
                  "text": "Got it! By the way I released yesterday the first version of my own open sourced library for diarization - it has DER better than pyannote and 8x faster \n\nhttps://github.com/FoxNoseTech/diarize",
                  "score": 1,
                  "created_utc": "2026-03-02 07:01:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1reeixj",
      "title": "From Pikachu to ZYRON: We Built a Fully Local AI Desktop Assistant That Runs Completely Offline",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1reeixj/from_pikachu_to_zyron_we_built_a_fully_local_ai/",
      "author": "No-Mess-8224",
      "created_utc": "2026-02-25 14:22:10",
      "score": 22,
      "num_comments": 3,
      "upvote_ratio": 0.85,
      "text": "A few months ago I posted here about a small personal project I was building called Pikachu, a local desktop voice assistant. Since then the project has grown way bigger than I expected, got contributions from some really talented people, and evolved into something much more serious. We renamed it to ZYRON and it has basically turned into a full local AI desktop assistant that runs entirely on your own machine.\n\nThe main goal has always been simple. I love the idea of AI assistants, but I hate the idea of my files, voice, screenshots, and daily computer activity being uploaded to cloud services. So we built the opposite. ZYRON runs fully offline using a local LLM through Ollama, and the entire system is designed around privacy first. Nothing gets sent anywhere unless I explicitly ask it to send something to my own Telegram.\n\nYou can control the PC with voice by saying a wake word and then speaking normally. It can open apps, control media, set volume, take screenshots, shut down the PC, search the web in the background, and run chained commands like opening a browser and searching something in one go. It also responds back using offline text to speech, which makes it feel surprisingly natural to use day to day.\n\nThe remote control side became one of the most interesting parts. From my phone I can message a Telegram bot and basically control my laptop from anywhere. If I forget a file, I can ask it to find the document I opened earlier and it sends the file directly to me. It keeps a 30 day history of file activity and lets me search it using natural language. That feature alone has already saved me multiple times.\n\nWe also leaned heavily into security and monitoring. ZYRON can silently capture screenshots, take webcam photos, record short audio clips, and send them to Telegram. If a laptop gets stolen and connects to the internet, it can report IP address, ISP, city, coordinates, and a Google Maps link. Building and testing that part honestly felt surreal the first time it worked.\n\nOn the productivity side it turned into a full system monitor. It can report CPU, RAM, battery, storage, running apps, and even read all open browser tabs. There is a clipboard history logger so copied text is never lost. There is a focus mode that kills distracting apps and closes blocked websites automatically. There is even a ‚Äúzombie process‚Äù monitor that detects apps eating RAM in the background and lets you kill them remotely.\n\nOne feature I personally love is the stealth research mode. There is a Firefox extension that creates a bridge between the browser and the assistant, so it can quietly open a background tab, read content, and close it without any window appearing. Asking random questions and getting answers from a laptop that looks idle is strangely satisfying.\n\nThe whole philosophy of the project is that it does not try to compete with giant cloud models at writing essays. Instead it focuses on being a powerful local system automation assistant that respects privacy. The local model is smaller, but for controlling a computer it is more than enough, and the tradeoff feels worth it.\n\nWe are planning a lot next. Linux and macOS support, geofence alerts, motion triggered camera capture, scheduling and automation, longer memory, and eventually a proper mobile companion app instead of Telegram. As local models improve, the assistant will naturally get smarter too.\n\nThis started as a weekend experiment and slowly turned into something I now use daily. I would genuinely love feedback, ideas, or criticism from people here. If you have ever wanted an AI assistant that lives only on your own machine, I think you might find this interesting.\n\nGitHub Repo -¬†[Link](https://github.com/Surajkumar5050/zyron-assistant)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1reeixj/from_pikachu_to_zyron_we_built_a_fully_local_ai/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o7j9zwz",
          "author": "marinetankguy2",
          "text": "Sounds interesting. If you implement linux support I try it.",
          "score": 2,
          "created_utc": "2026-02-26 16:08:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gas5e",
          "author": "mon_key_house",
          "text": "This sounds cool! What are the hw requirements and resource use?",
          "score": 1,
          "created_utc": "2026-02-26 03:29:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gdybv",
              "author": "No-Mess-8224",
              "text": "i built most of it on a normal mid range laptop. ryzen 5, 8gb ram, no gpu usage (this is off all the time). 16 gb is prefect ",
              "score": 2,
              "created_utc": "2026-02-26 03:48:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rgypnv",
      "title": "Has anyone got qwen3.5 to work  with ollama?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rgypnv/has_anyone_got_qwen35_to_work_with_ollama/",
      "author": "MrMrsPotts",
      "created_utc": "2026-02-28 09:52:55",
      "score": 22,
      "num_comments": 31,
      "upvote_ratio": 0.96,
      "text": ">ollama run [hf.co/unsloth/Qwen3.5-35B-A3B-GGUF:UD-Q2\\_K\\_XL](http://hf.co/unsloth/Qwen3.5-35B-A3B-GGUF:UD-Q2_K_XL)\n\n>Error: 500 Internal Server Error: unable to load model: /usr/share/ollama/.ollama/models/blobs/sha256-a7d979fa31c1387cc5a49b94b1a780b2e9018b3fae6cf9bef6084c17367412e3\n\n>ollama --version\n\n>ollama version is 0.17.4\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rgypnv/has_anyone_got_qwen35_to_work_with_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o7uxihr",
          "author": "geek_at",
          "text": "in the readme of the [frob/qwen3.5](https://ollama.com/frob/qwen3.5) it says you have to manually patch it to be supported.\n\nPersonally all the models I have tried crash after about 300 tokens. I mean ollama crashes every time or with huggingface models doesn't even load the model.\n\nGuess there is still work to be done",
          "score": 5,
          "created_utc": "2026-02-28 10:30:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7uyqp2",
              "author": "MrMrsPotts",
              "text": "That's confusing because  [https://github.com/ollama/ollama/releases](https://github.com/ollama/ollama/releases) says \"New models\n\n* [Qwen 3.5](https://ollama.com/library/qwen3.5): a family of open-source multimodal models that delivers exceptional utility and performance.\"\n\n",
              "score": 1,
              "created_utc": "2026-02-28 10:42:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7uz39r",
                  "author": "geek_at",
                  "text": "You should be able to pull and use [these](https://ollama.com/library/qwen3.5) but for me they were all crashing after some promts. Not sure if it's my GPUs or something but I couldn't run them without crashing on ollama.\n\nThe same models from huggingface worked for me with llama.cpp though! Even on my laptop who doesn't even have a GPU",
                  "score": 1,
                  "created_utc": "2026-02-28 10:45:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7x6cr2",
          "author": "Bamny",
          "text": "I got it to run across my 2 3060 12GB cards with the 0.17.1 ollama pre release and qwen3.5-27B model and ‚Ä¶ it‚Äôs just too damn slow for me to be impressed",
          "score": 3,
          "created_utc": "2026-02-28 18:36:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7x8rrd",
              "author": "MrMrsPotts",
              "text": "Can you try the model I was trying please?",
              "score": 1,
              "created_utc": "2026-02-28 18:48:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7xemf6",
                  "author": "Bamny",
                  "text": "Upgraded to 0.17.4\nModel downloaded\nBut I get the same error",
                  "score": 1,
                  "created_utc": "2026-02-28 19:18:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o80myjy",
              "author": "ANTIVNTIANTI",
              "text": "RIGHT?! Sooooooo slow.",
              "score": 1,
              "created_utc": "2026-03-01 07:26:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o80n07l",
                  "author": "ANTIVNTIANTI",
                  "text": "from a Mac m3 ultra w/ 256GB RAM too so like, wtf Ollama? LOL",
                  "score": 1,
                  "created_utc": "2026-03-01 07:26:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7v1saj",
          "author": "_-Lel-_",
          "text": "I run it on my amd gpu for paperless tagging with paperless ai. Works great after some prompt tweaking",
          "score": 2,
          "created_utc": "2026-02-28 11:10:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7v31wz",
              "author": "MrMrsPotts",
              "text": "Have you got that model to work?",
              "score": 1,
              "created_utc": "2026-02-28 11:21:45",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7vc4g2",
              "author": "Comfortable_Ad_8117",
              "text": "I just started using Paperless about 2 months ago (switched from OpenKM) - I have it running on Ubuntu in docker. - Can you point me in the direction where I can integrate Ollama into my setup?",
              "score": 1,
              "created_utc": "2026-02-28 12:36:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7v3b18",
          "author": "unknowntoman-1",
          "text": "Going with the abliterated. I can get one prompt only before the Error 500. And I have to restrict my lenght of request. Example for a one-shot query: ollama run huihui\\_ai/qwen3.5-abliterated  \"Your name is Donald. Answer like a Duck. Hello Donald.\"   And it make a solid well setup response (in character). Doing a simple math like this works: How long would it take to curcumerence earth by the equator in 50 mph?\" But just a tad longer or advanced dont work: How long would it take for an airbus 350 with unlimited fuel to curcumerence earth by the equator.\"   I can use it to generate image prompts. Thats about all. So, no refining and no history ctx. but whatever I get look solid.",
          "score": 2,
          "created_utc": "2026-02-28 11:24:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7v7xfk",
          "author": "Confident-Ad-3465",
          "text": "Lower your context size to 1000 and see if that's (or resolves) the issue",
          "score": 1,
          "created_utc": "2026-02-28 12:03:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7xdb56",
              "author": "unknowntoman-1",
              "text": "Excellent advice! Now it takes some beating, but there is still no way to have a interactive reasoning. This is what it say when following up on previous question: \n\n\n\n//End lines of a complex resoniong://\n\n| Feature | Specification | Impact on Flight |\n\n| :--- | :--- | :--- |\n\n| \\*\\*Cruising Speed\\*\\* | Mach 0.85 (\\~900 km/h) | Primary determinant of flight time. |\n\n| \\*\\*Range (Max)\\*\\* | 8,700‚Äì15,000 nm (\\~16,000‚Äì28,000 km) | Exceeded by the \"Unlimited Fuel\" assumption. |\n\n| \\*\\*Crew Rest\\*\\* | Dedicated Compartment (CRC) | Allows for the required crew rotation strategy. |\n\n| \\*\\*Fuel Capacity\\*\\* | \\~138,000 Liters | Irrelevant for duration due to \"Unlimited\" constraint. |\n\n\n\n\\>>> The short answer on previous question is?\n\nThinking...\n\nOkay, the user is asking for the short answer to the previous question. But wait, what was the previous question?\n\nSince this is a new conversation, there's no prior context. Maybe the user is referring to a question they asked\n\nbefore this one, but in this chat session, there's no history.\n\n  \n// and excuses //  \n\n  \nI guess this is what you get without some kind of unloading. Shouldnt ollama use the ram for that by default?  My specs are ollama 0.17.4, 3090(24GB) and plenty of RAM/CPU to go. Anyone know how to make this possible? I really dont care much about token/s. I got time to spare for a good output. ",
              "score": 1,
              "created_utc": "2026-02-28 19:11:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vkv13",
          "author": "Helpful_Jelly5486",
          "text": "Seems like an out of memory situation. Try reducing context to 32k and reload. I had the same issue and had to reduce context from 262,000 to 131,072 and also quantize kv cache to q8. Even then the vram need was 34gb so I‚Äôm still partially offloading to cpu.",
          "score": 1,
          "created_utc": "2026-02-28 13:36:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vn4v2",
          "author": "Mordimer86",
          "text": "I got this one to work: [hf.co/unsloth/Qwen3.5-27B-GGUF:Q4\\_K\\_M](http://hf.co/unsloth/Qwen3.5-27B-GGUF:Q4_K_M)\n\nAlthough so far it works with my own app for helping reading Chinese texts, but some error happened when I tried to call it from avante.nvim to do somethinng with code.",
          "score": 1,
          "created_utc": "2026-02-28 13:49:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o86e4pi",
              "author": "chr0n1x",
              "text": "404 for your link btw\n\nhttps://huggingface.co/unsloth/Qwen3.5-27B-GGUF?show_file_info=Qwen3.5-27B-Q4_K_M.gguf",
              "score": 1,
              "created_utc": "2026-03-02 04:36:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vo99v",
          "author": "jeremiah256",
          "text": "It was slow, but I did get it to work in Ollama.  Totally a no go with OpenClaw but that‚Äôs probably more due to my weak CPU.",
          "score": 1,
          "created_utc": "2026-02-28 13:56:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vxtsl",
              "author": "MrMrsPotts",
              "text": "Did you just run the command line I did with ollama version 0.17.4 ?\n\n>\n\n[](https://www.reddit.com/submit/?source_id=t3_1rgypnv)",
              "score": 1,
              "created_utc": "2026-02-28 14:51:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7w6ol1",
                  "author": "jeremiah256",
                  "text": "I cut and pasted the command from Ollama‚Äôs website for the model into my terminal.",
                  "score": 1,
                  "created_utc": "2026-02-28 15:38:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7w6f7v",
          "author": "Birdinhandandbush",
          "text": "Same. Got excited seeing the gguf and ultimately disappointed that I can't get it to work",
          "score": 1,
          "created_utc": "2026-02-28 15:36:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xbgml",
          "author": "ZeroSkribe",
          "text": "There's a bug with 35b & 27b going on right now, I'm running dual 3050's.",
          "score": 1,
          "created_utc": "2026-02-28 19:01:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xop8e",
          "author": "ScaryVeterinarian241",
          "text": "I have had that model crash with the same 500 error quite a bit, but no issues with the others.  But yea, that one keeps throwing that error.  Usually works for a while before it happens.  Also might have been peak hours when it started happening.  Cloud model though, not local.   The local model is too big for a single 3090 apparently. ",
          "score": 1,
          "created_utc": "2026-02-28 20:10:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yqrzn",
          "author": "JacketHistorical2321",
          "text": "Yes, had to build from source",
          "score": 1,
          "created_utc": "2026-02-28 23:40:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80mqgm",
          "author": "ANTIVNTIANTI",
          "text": "yeah, the last two or last update FUBAR'd meh Qwen3.5 experience. Got them from Ollama too, they were running quite slow, now they dead. lol, need to leave this service now.",
          "score": 1,
          "created_utc": "2026-03-01 07:24:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfhzxn",
      "title": "Mimic Digital AI Assistant",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/gallery/1rfhzxn",
      "author": "GullibleNarwhal",
      "created_utc": "2026-02-26 18:23:05",
      "score": 19,
      "num_comments": 9,
      "upvote_ratio": 0.79,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rfhzxn/mimic_digital_ai_assistant/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o7k71cy",
          "author": "Terrariant",
          "text": "Did you write your own licenses with AI? Weird. Why would I pay $5 a month to use this? You should drop the subscription model and make this open source.",
          "score": 5,
          "created_utc": "2026-02-26 18:39:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7k7qep",
              "author": "GullibleNarwhal",
              "text": "It is entirely vibe coded. License will be updated on next release to be open-source. Will move to a premium avatar/animation/voice pack revenue model. Currently the subscription is just a suggestion. Application is free to use and is not pay-walled.",
              "score": 1,
              "created_utc": "2026-02-26 18:43:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7me30d",
          "author": "blackheva",
          "text": "BonziBuddy, is that you? We've come full circle.",
          "score": 3,
          "created_utc": "2026-02-27 01:19:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7mf26q",
              "author": "GullibleNarwhal",
              "text": "I'm a 90's kid but had no clue what BonziBuddy was. Just looked it up and had a good laugh. I guess you could say it is pretty similar based on the description, minus the whole spyware/adware thing lol. I was going for an intelligent Clippy 2.0.",
              "score": 0,
              "created_utc": "2026-02-27 01:24:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7mgw33",
                  "author": "blackheva",
                  "text": "I appreciate you taking the kab in stride. It really is a great piece of work.",
                  "score": 1,
                  "created_utc": "2026-02-27 01:35:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7n3w70",
          "author": "blurredphotos",
          "text": "**Mimic AI is now licensed under a Proprietary License with 7-Day Trial**\n\n* **7-Day Free Trial**: Full functionality for 7 days from first launch\n* **Subscription**: $5/month via Patreon for continued use after trial\n* **Support Development**: Subscribe at¬†[https://www.patreon.com/c/MimicAIDigitalAssistant](https://www.patreon.com/c/MimicAIDigitalAssistant)\n\nno",
          "score": 3,
          "created_utc": "2026-02-27 03:51:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7n608z",
              "author": "GullibleNarwhal",
              "text": "I have seen that this has been received very poorly. I am currently working on trying to get version 1.1.1 with added Kitten TTS support which is a much more light weight model. I am reverting the license to an MIT open-source, and will sell avatar models/voice packs/animations for revenue. I hope to have it up later this evening or tomorrow if you would be willing to reconsider!",
              "score": 4,
              "created_utc": "2026-02-27 04:04:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7k4bu6",
          "author": "GullibleNarwhal",
          "text": "[https://imgur.com/gallery/mimic-digital-ai-assistant-https-github-com-bmerriott-mimic-multipurpose-intelligent-molecular-information-catalyst-releases-tag-v1-1-0-SjsTsgj](https://imgur.com/gallery/mimic-digital-ai-assistant-https-github-com-bmerriott-mimic-multipurpose-intelligent-molecular-information-catalyst-releases-tag-v1-1-0-SjsTsgj)\n\nHere's a quick video demo showing playback of voice and the VRM model that comes shipped in app.",
          "score": 1,
          "created_utc": "2026-02-26 18:27:32",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1re6pgx",
      "title": "qwen3.5:35b-a3b is here.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1re6pgx/qwen3535ba3b_is_here/",
      "author": "Space__Whiskey",
      "created_utc": "2026-02-25 07:26:55",
      "score": 18,
      "num_comments": 15,
      "upvote_ratio": 0.82,
      "text": "yey. that is all.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1re6pgx/qwen3535ba3b_is_here/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o7agzqo",
          "author": "geek_at",
          "text": "guess I have to wait for the patch :(\n\n`ollama     | llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'qwen35moe'`",
          "score": 9,
          "created_utc": "2026-02-25 07:51:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ai788",
              "author": "Space__Whiskey",
              "text": "use [https://github.com/ollama/ollama/releases/tag/v0.17.1-rc1](https://github.com/ollama/ollama/releases/tag/v0.17.1-rc1) it works",
              "score": 5,
              "created_utc": "2026-02-25 08:02:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bo8pv",
                  "author": "WaitformeBumblebee",
                  "text": "I've tried bazobehram/qwen3.5-flash-27b:latest with 17.1-rc1 and get this error:\nError: 500 Internal Server Error: unable to load model: /usr/share/ollama/.ollama/models/blobs/sha256-d4d089fbfa2a2ef034faa5c99a1743523ce69a18c562f7de09007a07ca07d4af\n\nI pulled the model and the 16GB file is there",
                  "score": 2,
                  "created_utc": "2026-02-25 13:39:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7akgqb",
                  "author": "geek_at",
                  "text": "nice thanks that worked. Tried it yesterday with 0.17.1-rc0 and it didn't work",
                  "score": 3,
                  "created_utc": "2026-02-25 08:24:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7wg7g0",
              "author": "virtualworker",
              "text": "It's an open PR now:\n\nhttps://github.com/ollama/ollama/pull/14506",
              "score": 2,
              "created_utc": "2026-02-28 16:25:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7gc1ok",
          "author": "Mother_Risk_4953",
          "text": "Guys I get server error 500 when I try to run it, when I check server logs it says my ollama does no support qwenmoe though ollama downloaded the model and I have the latest beta (0.17) of February 2026, any advice?",
          "score": 2,
          "created_utc": "2026-02-26 03:36:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jzfci",
          "author": "chew_stale_gum",
          "text": "how's the 27b vs 35b?",
          "score": 2,
          "created_utc": "2026-02-26 18:05:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bfird",
          "author": "Dui999",
          "text": "The 27b model GGUF still does not work on the latest release :(",
          "score": 1,
          "created_utc": "2026-02-25 12:47:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bim6e",
              "author": "MrMrsPotts",
              "text": "Did you try rc1?",
              "score": 1,
              "created_utc": "2026-02-25 13:06:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bndfq",
                  "author": "Dui999",
                  "text": "Yes, very odd.\n\nBut I guess it's related to the fact that they still didn't release this model on their website as they did with the MoE ones.",
                  "score": 1,
                  "created_utc": "2026-02-25 13:34:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7cfg9e",
          "author": "zelkovamoon",
          "text": "For those that currently have it running\n\n1. Is tps basically on par with what you'd expect?\n\n2. Is tool calling working?",
          "score": 1,
          "created_utc": "2026-02-25 15:55:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7d0vc6",
              "author": "InternationalNebula7",
              "text": "Compared to GLM 4.7 flash on the same hardware  \n\\- TPS is lower  \n\\- Tool calling (particularly web-search) seems to be worse",
              "score": 2,
              "created_utc": "2026-02-25 17:33:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7d1k2r",
                  "author": "zelkovamoon",
                  "text": "You've got flash working in ollama? It still basically doesn't function for me - are you using the library version?",
                  "score": 1,
                  "created_utc": "2026-02-25 17:36:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1reupji",
      "title": "I built a multi-agent eval lab where 3 LLM personas race, get judged with evidence snippets, and learn from losses ‚Äî open source",
      "subreddit": "ollama",
      "url": "https://i.redd.it/t6whtryqdqlg1.png",
      "author": "Sufficient-Title-912",
      "created_utc": "2026-02-26 00:09:36",
      "score": 17,
      "num_comments": 8,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1reupji/i_built_a_multiagent_eval_lab_where_3_llm/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7ffclb",
          "author": "AI_Only",
          "text": "Where‚Äôs the github link?",
          "score": 2,
          "created_utc": "2026-02-26 00:30:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fkybr",
          "author": "smwaqas89",
          "text": "the idea of racing different agents and having evidence backed scores seems like a solid way to get quality insights. have you noticed if one persona tends to perform better in specific scenarios? would be interesting to see how they compare under different conditions.",
          "score": 1,
          "created_utc": "2026-02-26 01:00:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7fls8p",
              "author": "Sufficient-Title-912",
              "text": "yep, we‚Äôre already seeing persona specialization. The Analyst wins most structured tasks (math/finance/general), while research is more mixed and the contrarian personas can score competitively. Next step is controlled benchmark runs by category + single vs consensus judge to publish stable win-rate and confidence metrics.",
              "score": 1,
              "created_utc": "2026-02-26 01:05:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7fnh1r",
          "author": "Sufficient-Title-912",
          "text": "For those asking about the evaluation logic, here‚Äôs the simplified scoring:\n\nAgents: A‚ÇÅ, A‚ÇÇ, A‚ÇÉ  \nEach produces response R·µ¢ + reasoning + tool calls.\n\nEach agent is scored (0‚Äì10) on:  \n‚Ä¢ Accuracy (a·µ¢)  \n‚Ä¢ Completeness (c·µ¢)  \n‚Ä¢ Clarity (l·µ¢)  \n‚Ä¢ Insight (s·µ¢)\n\nWeights sum to 1.\n\nBase score (/40):  \nB·µ¢ = round(4 √ó (w‚Çêa·µ¢ + w\\_cc·µ¢ + w\\_ll·µ¢ + w\\_ss·µ¢))\n\nDiversity penalty:  \nIf similarity ‚â• 0.72 ‚Üí T·µ¢ = round(0.8 √ó B·µ¢)  \nElse ‚Üí T·µ¢ = B·µ¢\n\nWinner = argmax(T·µ¢)  \nTie-breakers: accuracy ‚Üí completeness ‚Üí insight\n\nConsensus mode:  \n3 judge panels score independently.  \nWe take the median per metric, then recompute.\n\nConfidence gate (default):  \n‚Ä¢ Winner ‚â• 26  \n‚Ä¢ Margin ‚â• 2  \n‚Ä¢ Accuracy ‚â• 6.5\n\nIf any fail ‚Üí low-confidence run.\n\nTrust signals:  \n‚Ä¢ Evidence coverage ‚â• 75%  \n‚Ä¢ Low panel disagreement (‚â§ 0.12)  \n‚Ä¢ Majority panel agreement",
          "score": 1,
          "created_utc": "2026-02-26 01:15:17",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o7fvaph",
          "author": "Available-Craft-5795",
          "text": "So now were making webUI's for a makeshift LoRA fine-tune?",
          "score": 1,
          "created_utc": "2026-02-26 02:00:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g19m9",
              "author": "Sufficient-Title-912",
              "text": "Not fine-tuning at all ‚Äî it's a multi-agent eval engine. You submit a prompt, 3 personas race on it using Anthropic/Gemini/OpenAI, a judge panel scores each response with evidence, and the system learns which strategies win over time. No local models, no LoRA\n\n",
              "score": 1,
              "created_utc": "2026-02-26 02:34:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rgbjw8",
      "title": "Trying to figure the best model for reading company documentation",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rgbjw8/trying_to_figure_the_best_model_for_reading/",
      "author": "Rickety_cricket420",
      "created_utc": "2026-02-27 16:36:03",
      "score": 17,
      "num_comments": 13,
      "upvote_ratio": 0.8,
      "text": "I'm working for a small company that has an employee portal that holds all the documents you could need. The issue is if you want to find something like vacation policy or dress code you have to go digging through the files to find it. The files are pretty much all .docx and .pdf. I'm still relatively new to this so could someone suggest a good model for that? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rgbjw8/trying_to_figure_the_best_model_for_reading/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o7s71si",
          "author": "laughingfingers",
          "text": "You build a RAG system (or use one). It sounds as if you just stored the documents in a normal database, not embedded them into a vector database.",
          "score": 4,
          "created_utc": "2026-02-27 22:38:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sgy8q",
              "author": "Rickety_cricket420",
              "text": "No I used Postgres‚Äôs pgvector.",
              "score": 1,
              "created_utc": "2026-02-27 23:33:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7sujgj",
          "author": "dave-tay",
          "text": "Using qwen3:14b to ingest legal PDFs into a vector database and generate documentation from it. 100% GPU on an RTX 3060 12gb and Ryzen 5600g",
          "score": 3,
          "created_utc": "2026-02-28 00:53:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7rgdp3",
          "author": "tom-mart",
          "text": "Why do you think you need LLM to search through some documents?",
          "score": 5,
          "created_utc": "2026-02-27 20:22:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7shc7t",
              "author": "Rickety_cricket420",
              "text": "Why do you think you need to comment on my post without helping?",
              "score": -4,
              "created_utc": "2026-02-27 23:35:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7uog2o",
                  "author": "NothingButTheDude",
                  "text": "because you described a need for stupid search using 20 year old open source tech but expressed it ito an AI problem which it is not. Go back to managing TPS reports please.",
                  "score": 0,
                  "created_utc": "2026-02-28 09:01:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7q9kta",
          "author": "Crafty_Ball_8285",
          "text": "There are 2.6 million models on hf. What ones have you looked at",
          "score": 3,
          "created_utc": "2026-02-27 16:56:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qcmbt",
              "author": "Rickety_cricket420",
              "text": "I've only tried llama3.1:8b. I've chunked the company documents into a postgres db but the answers are hit or miss. Simple questions like \"can I wear open toed shoes to work\" take long load times then respond with a hit or miss answer actually referencing the document or making something up. Is there a smaller, more specialized model that is meant for this kind of task?",
              "score": -4,
              "created_utc": "2026-02-27 17:10:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7qq07s",
                  "author": "Spooknik",
                  "text": "Well that's an 8b model for you. If that is the limit of what you can run then you'll need to finetune the model with whatever you want it to know. ",
                  "score": 5,
                  "created_utc": "2026-02-27 18:14:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7qf768",
                  "author": "Crafty_Ball_8285",
                  "text": "You would only really get good responses if you trained it yourself. Theres no way you‚Äôll get a specialized model based off your company data unless you make it",
                  "score": 2,
                  "created_utc": "2026-02-27 17:22:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o832yr2",
                  "author": "NicksTechTricks",
                  "text": "You should try hybrid search and reranking. This will likely fix your search issues. Also, restrict the llm to answers from the documents or have it say it could not find the answer in the documents. The restriction can be toggled per search or hard coded on or off.",
                  "score": 1,
                  "created_utc": "2026-03-01 17:33:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7tcgco",
          "author": "futileboy",
          "text": "If you want you can try this project of mine out https://github.com/ryanlane/document-manager there are many others like it.",
          "score": 1,
          "created_utc": "2026-02-28 02:44:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vmcvk",
          "author": "Wise-Noodle",
          "text": "This is such a classic case of ignoring 30+ years of proven basic computer science with absolutely amazing tools already specifically made to solve this very problem.\n\nJust google, open source CMS as a starter for 10.",
          "score": 1,
          "created_utc": "2026-02-28 13:45:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdzgyb",
      "title": "AI toolkit ‚Äî LiteLLM + n8n + Open WebUI in one Docker Compose",
      "subreddit": "ollama",
      "url": "https://github.com/wa91h/local-ai-toolkit",
      "author": "Puzzleheaded-Dig-492",
      "created_utc": "2026-02-25 01:30:39",
      "score": 16,
      "num_comments": 3,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rdzgyb/ai_toolkit_litellm_n8n_open_webui_in_one_docker/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o7ofg25",
          "author": "AgitatedDoctor9613",
          "text": "Great initiative in creating an accessible local AI stack! Here are some constructive suggestions to strengthen this post:\n\n**Improvements:**\n1. **Add system requirements** - Specify minimum CPU, RAM, GPU, and disk space needed to run this stack smoothly. Different models have vastly different resource needs.\n2. **Include setup time estimates** - How long does initial setup take? First model download? This helps readers decide if it's worth their time.\n3. **Provide a quick-start example** - Show a concrete workflow example (e.g., \"automate document summarization\") to demonstrate practical value.\n4. **Document troubleshooting** - Common issues and solutions (port conflicts, memory limits, API key errors) would be invaluable.\n\n**Alternative Perspectives:**\n1. **Compare to other solutions** - How does this compare to Hugging Face Spaces, Replicate, or other containerized AI stacks? What are unique advantages?\n2. **Discuss trade-offs** - Self-hosting vs. cloud services (cost, maintenance burden, latency, privacy). When should someone choose this over managed solutions?\n3. **Address the learning curve** - Is this beginner-friendly or does it require Docker/DevOps knowledge?\n\n**Potential Issues:**\n1. **Security considerations** - Running multiple services locally; are there authentication/network isolation concerns?\n2. **Model licensing** - Clarify licensing implications of the 30+ pre-configured models (commercial use, attribution requirements).\n3. **Maintenance burden** - How often does this stack need updates? What's the support timeline?\n4. **Performance caveats** - Local inference can be slow; set realistic expectations vs. cloud APIs.\n\n**Additional Resources:**\n1. Link to documentation for each tool (LiteLLM, n8n, Open WebUI official docs)\n2. Recommend community forums or Discord channels for support\n3. Suggest benchmarking tools to help users test performance\n4. Point to best practices for Docker Compose production deployments\n\n**Minor notes:**\n- Fix the broken vscode-file:// links in your post\n- Consider adding a screenshot or architecture diagram\n- Mention whether this supports GPU acceleration and how to enable it\n\nOverall, solid contribution to the community!",
          "score": 2,
          "created_utc": "2026-02-27 10:23:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7wjwog",
              "author": "Puzzleheaded-Dig-492",
              "text": "Thanks for the detailed feedback, i‚Äôm already working on most of those points will keep you in touch",
              "score": 1,
              "created_utc": "2026-02-28 16:43:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7xri8m",
          "author": "Darkitechtor",
          "text": "Why do you need a LiteLLM in this project and what‚Äôs the purpose of running both n8n and OpenWebUI under single docker compose?",
          "score": 1,
          "created_utc": "2026-02-28 20:25:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd920r",
      "title": "Built an app that connects Ollama to your clipboard with ‚å•C (macOS, open source)",
      "subreddit": "ollama",
      "url": "https://v.redd.it/hhklml241elg1",
      "author": "morning-cereals",
      "created_utc": "2026-02-24 06:44:32",
      "score": 15,
      "num_comments": 4,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rd920r/built_an_app_that_connects_ollama_to_your/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o73xuwm",
          "author": "bukaro",
          "text": "Looks very cool, definitly I will give it a try .. now :-)",
          "score": 1,
          "created_utc": "2026-02-24 09:35:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7445o3",
              "author": "morning-cereals",
              "text": "Exciting! Let me know how smooth the onboarding is and if you have any feedback/ideas :)",
              "score": 1,
              "created_utc": "2026-02-24 10:34:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7c33rm",
          "author": "VictorFoxSub",
          "text": "I'm playing with it, it's so cool !\n\nTo be honest I was going to do something similar with bash scripts and zenity so you saved me some time for a better result.\n\nI suggest to add the option to change the keyboard shortcut. The current one conflicts with some of my shortcuts (which I could change) and some app produce ¬© (like intellij terminal or firefox) when typing option+c (which you cannot change). I find this app so usefull that I intend to dedicate an key just for that.",
          "score": 1,
          "created_utc": "2026-02-25 14:57:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cbazr",
              "author": "morning-cereals",
              "text": "Great feedback, makes a lot of sense! I'll include the option to change the shortcut via the settings menu in the next release :)",
              "score": 2,
              "created_utc": "2026-02-25 15:36:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdl1fd",
      "title": "What's the best model to run on mac m1 pro 16gb?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rdl1fd/whats_the_best_model_to_run_on_mac_m1_pro_16gb/",
      "author": "Embarrassed-Baby3964",
      "created_utc": "2026-02-24 16:33:37",
      "score": 15,
      "num_comments": 20,
      "upvote_ratio": 0.8,
      "text": "I have an old m1 mac pro with 16gb ram. Was wondering if there are any good performing models in 2026 that I can run on this hardware? And if so, what is the best one in your opinion?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rdl1fd/whats_the_best_model_to_run_on_mac_m1_pro_16gb/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o76ag8d",
          "author": "NoobMLDude",
          "text": "Asking for a ‚ÄúBest Model‚Äù is like asking for the ‚ÄúBest Food‚Äù. It all depends on what you wish to achieve . \n\nDo you need it for coding, general chat, image creation, meeting summaries, etc?\n\nThere are model sizes available from 300 million (runs on web site) to Trillion parameters. 7B size is good for your device. But which 7B model depends on what you want to do with the model.",
          "score": 10,
          "created_utc": "2026-02-24 17:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o774oca",
              "author": "misha1350",
              "text": "Thanks Qwen",
              "score": 3,
              "created_utc": "2026-02-24 20:04:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o77f74h",
          "author": "st0ut717",
          "text": "Install lmstudio.   \nIt will give you compatible models for your machine.",
          "score": 9,
          "created_utc": "2026-02-24 20:53:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75xkj0",
          "author": "Mindless-Direction60",
          "text": "Pretty much any 7B model, personally I like Qwen 2.5 7B but there‚Äôs lots of good options.",
          "score": 7,
          "created_utc": "2026-02-24 16:50:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a0due",
              "author": "IngloriousBastrd7908",
              "text": "If I may ask:\nWhy Qwen 2.5 7B if there is Qwen3 already?",
              "score": 1,
              "created_utc": "2026-02-25 05:31:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7v4u5j",
              "author": "iHannes",
              "text": "I am using Qwen 3 8B and it is way better then the 2.5 7B Model. Try it out.",
              "score": 1,
              "created_utc": "2026-02-28 11:37:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7w2gbm",
                  "author": "Mindless-Direction60",
                  "text": "Thats great, do you find it to be as fast as Qwen 2.5 7B? It seems very sluggish to me.",
                  "score": 1,
                  "created_utc": "2026-02-28 15:16:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76y4ze",
          "author": "mirlan_irokez",
          "text": "I used ministral-3:8B, mistral:7b, gemma3 on M1 / 16gb / MacbookAir.  \ndeepsek and qwen models stuck in reasoning :)   \nI moved to M4 / 16gb, a little bit better, but didn't try qwen, I assume 16Gb is not enough for reasoning models ",
          "score": 4,
          "created_utc": "2026-02-24 19:34:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o774fnq",
          "author": "misha1350",
          "text": "Try out GLM 4.6v Flash (9B model) at 4-bit MLX. Qwen2.5 7B is extremely old and outdated, so is Gemma 3.",
          "score": 3,
          "created_utc": "2026-02-24 20:03:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77shp0",
              "author": "klawisnotwashed",
              "text": "thanks, will be trying this",
              "score": 1,
              "created_utc": "2026-02-24 21:54:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7786qp",
          "author": "CapitalShake3085",
          "text": "Qwen3 is the best model in every task, use the 8b",
          "score": 2,
          "created_utc": "2026-02-24 20:21:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7af0ur",
              "author": "misha1350",
              "text": "Vanilla Qwen3 8B hasn't been updated in July 2025 like how Qwen3 4B 2507 was, so it's not really that good, especially nowadays. DeepSeek R1 8B distill based on Qwen3-8B performs quite better than vanilla Qwen3 8B.",
              "score": 2,
              "created_utc": "2026-02-25 07:33:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7h5zam",
              "author": "p_235615",
              "text": "in small sizes like 8B I had much better experience with ministral-3:8b",
              "score": 1,
              "created_utc": "2026-02-26 07:19:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o76md2d",
          "author": "dsecareanu2020",
          "text": "Ask ollama :)",
          "score": 1,
          "created_utc": "2026-02-24 18:41:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76xmol",
          "author": "gamesta2",
          "text": "Depends on use. Some models* perform very good on paper, but may suck at tool calling. All depends on your client platform and system prompt",
          "score": 1,
          "created_utc": "2026-02-24 19:32:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77rfyg",
          "author": "Dubious-Decisions",
          "text": "If you don't want to deal with the fabricated explanations that new models offer up, stick with the older llama models like llama3 or llama3.2 (3.1 was a mess). They're big enough to work sensibly, but not so big that it's 30 seconds between responses. You aren't gonna be writing the next great American novel with either of these. But they are solid and are good examples of small, general purpose models that you can run on that hardware.",
          "score": 1,
          "created_utc": "2026-02-24 21:50:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78pzry",
          "author": "FistoWutini",
          "text": "I found under 10GB but needed to use MLX to get better performance over Ollama. There‚Äôs some overhead tha Ollama has that others don‚Äôt.",
          "score": 1,
          "created_utc": "2026-02-25 00:50:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7agg2s",
          "author": "x8code",
          "text": "You could try Granite4 ... it's a good model, but don't expect it to operate like a larger model or a frontier model (Anthropic, Gemini, etc.).\n\nSmall models work well for specific use cases, like \"summarize this chat for me.\" \n\nYou're not gonna use a small model like Granite4 for coding advanced applications. ",
          "score": 1,
          "created_utc": "2026-02-25 07:46:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80qwxd",
          "author": "fab_space",
          "text": "qwen3 family up to 14B and all SLM like LFM llama3.2 etc\n\nhttps://preview.redd.it/xg3rgwi85emg1.png?width=3020&format=png&auto=webp&s=0808f8c9f70bfce14f1bbf1e6b48daeeb948ef32",
          "score": 1,
          "created_utc": "2026-03-01 08:03:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rhl4rw",
      "title": "Hey Ollama, any thoughts of supporting disk cached models, along the lines of OLLM?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1rhl4rw/hey_ollama_any_thoughts_of_supporting_disk_cached/",
      "author": "Due-Priority-4336",
      "created_utc": "2026-03-01 02:16:11",
      "score": 15,
      "num_comments": 3,
      "upvote_ratio": 0.89,
      "text": "Hey Ollama, any thoughts of supporting disk cached models, along the lines of OLLM? Especially on Mac's.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rhl4rw/hey_ollama_any_thoughts_of_supporting_disk_cached/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o804dba",
          "author": "theMuhubi",
          "text": "What's a disk cached model? Isn't the model loaded onto the disk anyways? Maybe I'm missing something?",
          "score": 4,
          "created_utc": "2026-03-01 04:52:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o85elvz",
              "author": "Due-Priority-4336",
              "text": "OLLM will cache the KV values and the layer coefficients on disk. Then read them back as needed. The point is not to have 'fast and efficient' local operation in RAM, at low bit precision, but to be able to cache the entire large (i.e. 80 GB) model at full 16 bit precision, and run it by pulling layer data over time. This is supposed to allow full precision, large model operation, at the tradeoff of slow token generation.\n\nSpecifically, I want to run OLLM on my Apple Silicon Mac -- but it doesn't run (geared towards CUDA), and it only supports 3 or 4 specific models. Little did I know before I started looking into this that models are all different and all need custom interface layers to load and operate in any (OLLM/OLLAMA) execution environment.",
              "score": 1,
              "created_utc": "2026-03-02 00:48:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o80gg39",
          "author": "Hungry_Age5375",
          "text": "GGUF does memory mapping, but smart disk caching with prefetching? Different beast entirely. Would help Mac users run bigger models on limited unified memory.",
          "score": 2,
          "created_utc": "2026-03-01 06:27:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd8cu5",
      "title": "Built an open-source Ollama/MLX/OpenAI benchmark and leaderboard site with in-app submissions. Trying to test and collect more data.",
      "subreddit": "ollama",
      "url": "https://i.redd.it/tcn61r39rdlg1.png",
      "author": "peppaz",
      "created_utc": "2026-02-24 06:07:36",
      "score": 13,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rd8cu5/built_an_opensource_ollamamlxopenai_benchmark_and/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o73b5w0",
          "author": "peppaz",
          "text": "Apple silicon only for now - other platform forks in progress.\n\n[Homepage](https://devpadapp.com/anubis-oss.html) \n\n[Leaderboard Page](https://devpadapp.com/leaderboard.html)\n\n[Github](https://github.com/uncSoft/anubis-oss)\n\n[Latest dev cert signed release](https://github.com/uncSoft/anubis-oss/releases/latest) \n\n[It generates exportable reports as well](https://imgur.com/a/sBj2xWR)\n\nI designed Anubis, a native macOS app for benchmarking, comparing, and managing local large language models using any OpenAI-compatible endpoint - Ollama, MLX, LM Studio Server, OpenWebUI, Docker Models, etc. Built with SwiftUI for Apple Silicon, it provides real-time hardware telemetry correlated with full, history-saved inference performance - something no CLI tool or chat wrapper offers. Export benchmarks directly without having to screenshot, and export the raw data as .MD or .CSV from the history. You can even OLLAMA PULL models directly within the app. \n\nI am trying to get to 75 stars so I can submit to homebrew as a Cask. Check it out and I'd love some feedback! You can even choose the actual process to track memory use when running models, some model runners spawn child node processes that may not get auto-detected.",
          "score": 1,
          "created_utc": "2026-02-24 06:07:48",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfd045",
      "title": "Introducing CLAM (Cognitive Large Application Model): A dual-level cognitive architecture for LLM agents (Short/Long term memory)",
      "subreddit": "ollama",
      "url": "https://i.redd.it/otfvuppowulg1.png",
      "author": "Short-Confidence6287",
      "created_utc": "2026-02-26 15:23:14",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1rfd045/introducing_clam_cognitive_large_application/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    }
  ]
}