{
  "metadata": {
    "last_updated": "2026-01-26 02:37:15",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 20,
    "total_comments": 139,
    "file_size_bytes": 142215
  },
  "items": [
    {
      "id": "1qh5z0j",
      "title": "I built a voice-first AI mirror that runs fully on Ollama.",
      "subreddit": "ollama",
      "url": "https://v.redd.it/bjeyts2qibeg1",
      "author": "DirectorChance4012",
      "created_utc": "2026-01-19 14:43:25",
      "score": 370,
      "num_comments": 47,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qh5z0j/i_built_a_voicefirst_ai_mirror_that_runs_fully_on/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0hf0l3",
          "author": "ThomasNowProductions",
          "text": "How far we have come ;-) It is real nice tho",
          "score": 25,
          "created_utc": "2026-01-19 14:47:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hfop5",
              "author": "DirectorChance4012",
              "text": "Thanks!",
              "score": 4,
              "created_utc": "2026-01-19 14:51:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0lqmr5",
                  "author": "redditissocoolyoyo",
                  "text": "Start a Kickstarter! Make it a real product. People will buy it!",
                  "score": 7,
                  "created_utc": "2026-01-20 03:31:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0hr4is",
                  "author": "mlt-",
                  "text": "Did you aim for that Pal thing look from Netflix cartoon The Mitchells vs. the Machines?",
                  "score": 6,
                  "created_utc": "2026-01-19 15:46:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0i5cwe",
          "author": "croninsiglos",
          "text": "Need one with a vision model that both gives compliments and body shames me when appropriate. Must support tool use and connect with my scale.\n\nYou know, it also needs a more humanoid avatar that can  answer a simple question about who is the fairest of them all. The answer should be obvious, but I just want to be sure.",
          "score": 10,
          "created_utc": "2026-01-19 16:49:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tfvmu",
              "author": "Fuzzy_Independent241",
              "text": "WARNING: might have serious consequences if left to control prescription magical drugs for others in your kingdom!",
              "score": 3,
              "created_utc": "2026-01-21 07:28:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0j0wgi",
              "author": "ScoreUnique",
              "text": "SmolVlm",
              "score": 2,
              "created_utc": "2026-01-19 19:10:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0hgxfn",
          "author": "Money-Frame7664",
          "text": "That is really fun ! Congratulations ðŸ‘ \nDo you have connectors with any home automation system? (Or MCP capacity?)",
          "score": 7,
          "created_utc": "2026-01-19 14:57:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ho7ss",
          "author": "Travelosaur",
          "text": "Mirror mirror on the wall... You made it a reality bro. Nice job!",
          "score": 6,
          "created_utc": "2026-01-19 15:32:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jhgq8",
          "author": "ServeAlone7622",
          "text": "This is the future",
          "score": 5,
          "created_utc": "2026-01-19 20:27:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jtr8r",
          "author": "klei10",
          "text": "Where did you get the mirror ?",
          "score": 3,
          "created_utc": "2026-01-19 21:25:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0l4fx0",
              "author": "DirectorChance4012",
              "text": "I covered the details here: [https://noted.lol/mirrormate/](https://noted.lol/mirrormate/).  \nThe mirror was purchased from [https://www.e-kagami.com/](https://www.e-kagami.com/).",
              "score": 2,
              "created_utc": "2026-01-20 01:29:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0lamvu",
          "author": "KernelFlux",
          "text": "Thatâ€™s very cool and I want to build one!",
          "score": 3,
          "created_utc": "2026-01-20 02:03:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mjfif",
          "author": "Easy_Cable6224",
          "text": "so cool, this is something I wouldn't imagine back when I was a kid, we are IN the future now!",
          "score": 3,
          "created_utc": "2026-01-20 06:49:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hki0u",
          "author": "kiyyik",
          "text": "Thank you for sharing this! Looking forward to doing something similar.",
          "score": 2,
          "created_utc": "2026-01-19 15:15:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hs0z2",
          "author": "stoopwafflestomper",
          "text": "Very cool!",
          "score": 2,
          "created_utc": "2026-01-19 15:50:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hss7j",
              "author": "DirectorChance4012",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-19 15:53:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ht3w8",
          "author": "kkiran",
          "text": "Awesome, kudos! Mac Studio hosted AI mirror sounds cool. I wanted to justify my home lab spending and this fits part of the bill. Thank you!",
          "score": 2,
          "created_utc": "2026-01-19 15:54:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0htbp3",
              "author": "DirectorChance4012",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-19 15:55:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0hyitp",
          "author": "irodov4030",
          "text": "super cool!",
          "score": 2,
          "created_utc": "2026-01-19 16:19:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hzg6o",
              "author": "DirectorChance4012",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-19 16:23:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0i36ve",
          "author": "Reasonable_Brief578",
          "text": "Beautiful",
          "score": 2,
          "created_utc": "2026-01-19 16:39:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jk7b9",
          "author": "roshan231",
          "text": "Thatâ€™s cool",
          "score": 2,
          "created_utc": "2026-01-19 20:40:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ju6s9",
          "author": "After_Construction72",
          "text": "Was only looking into similar over the weekend",
          "score": 2,
          "created_utc": "2026-01-19 21:27:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jyutj",
          "author": "No_Thing8294",
          "text": "Thanks for sharing! Very nice project! I love the face of the agent!",
          "score": 2,
          "created_utc": "2026-01-19 21:51:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kjwd0",
          "author": "thunder-wear",
          "text": "This is really awesome. I love it!",
          "score": 2,
          "created_utc": "2026-01-19 23:38:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ksxo6",
          "author": "Relevant_Middle_4779",
          "text": "Wow!!!",
          "score": 2,
          "created_utc": "2026-01-20 00:26:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rlzzf",
          "author": "zaschmaen",
          "text": "Damn nice! Thats what i wanna make but with an comic character or smth like thisðŸ‘Œ",
          "score": 2,
          "created_utc": "2026-01-21 00:18:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0syczv",
          "author": "wittlewayne",
          "text": "shit.... beat me to it !",
          "score": 2,
          "created_utc": "2026-01-21 05:06:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0t9gdr",
          "author": "beast_modus",
          "text": "Good Jobâ€¦Greatâ€¦",
          "score": 2,
          "created_utc": "2026-01-21 06:32:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tg0t5",
          "author": "Fuzzy_Independent241",
          "text": "That looks cool! Did you embed the hardware or is it calling Ollama over your lan / VPN ?",
          "score": 2,
          "created_utc": "2026-01-21 07:29:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tg6nd",
              "author": "Fuzzy_Independent241",
              "text": "PS - noticed your GH link on my phone. Will check the repo. Thx!",
              "score": 2,
              "created_utc": "2026-01-21 07:31:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0iv6rl",
          "author": "BringOutYaThrowaway",
          "text": "I need to know where you got the glass?",
          "score": 1,
          "created_utc": "2026-01-19 18:45:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0l4h3z",
              "author": "DirectorChance4012",
              "text": "I covered the details here: [https://noted.lol/mirrormate/](https://noted.lol/mirrormate/).  \nThe mirror was purchased from [https://www.e-kagami.com/](https://www.e-kagami.com/).",
              "score": 1,
              "created_utc": "2026-01-20 01:29:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0l6nc9",
          "author": "MeatballStroganoff",
          "text": "How long is it from prompt to response? It seems like you cut the video right before it starts speaking.",
          "score": 1,
          "created_utc": "2026-01-20 01:41:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0l8eh0",
              "author": "DirectorChance4012",
              "text": "I takes roughly 5-7 second.",
              "score": 2,
              "created_utc": "2026-01-20 01:51:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0l92mv",
                  "author": "MeatballStroganoff",
                  "text": " Not bad, thanks!",
                  "score": 1,
                  "created_utc": "2026-01-20 01:55:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0n7ehn",
          "author": "JacketHistorical2321",
          "text": "How does the vision plugin work? I don't see any documentation besides referencing it exists",
          "score": 1,
          "created_utc": "2026-01-20 10:28:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0plcut",
          "author": "Affectionate_Bus_884",
          "text": "![gif](giphy|ZRICsU0zv4tJS)",
          "score": 1,
          "created_utc": "2026-01-20 18:27:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0skjvs",
          "author": "Ordinary_Marketing36",
          "text": "Having a Ktop of 5 means that you will always recover 5 of the best memories?",
          "score": 1,
          "created_utc": "2026-01-21 03:35:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10mpbe",
          "author": "Pleasant_Designer_14",
          "text": "funny,following",
          "score": 1,
          "created_utc": "2026-01-22 09:10:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17m9wl",
          "author": "realaneesani",
          "text": "Mate, what voice package you used?",
          "score": 1,
          "created_utc": "2026-01-23 09:43:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e2fye",
          "author": "Routine-Opening-90",
          "text": "This has been my long term dream for the longest time! Thanks for the guide! Looks very cool! :)",
          "score": 1,
          "created_utc": "2026-01-24 07:39:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nw31u",
          "author": "NickMcGurkThe3rd",
          "text": "the problem with those videos is that they are always cut to cache/hide the fact that it takes the like 10 seconds for that thing to respond and make it seem seemless",
          "score": 1,
          "created_utc": "2026-01-20 13:30:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nwd8p",
              "author": "DirectorChance4012",
              "text": "yeah, it actually takes 5-7second.\nhttps://www.reddit.com/r/ollama/s/8b6TeRsRFx",
              "score": 3,
              "created_utc": "2026-01-20 13:32:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qm9cgp",
      "title": "Ollama Models Ranked by VRAM Requirements",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qm9cgp/ollama_models_ranked_by_vram_requirements/",
      "author": "AdventurousLion9548",
      "created_utc": "2026-01-25 04:36:31",
      "score": 319,
      "num_comments": 31,
      "upvote_ratio": 0.96,
      "text": "1250.08 GB  |  cogito-2.1:latest\n\n1250.08 GB  |  cogito-2.1:671b\n\n 376.71 GB  |  deepseek-v3.1:latest\n\n 376.71 GB  |  deepseek-v3.1:671b\n\n 376.65 GB  |  deepseek-r1:671b\n\n 376.65 GB  |  deepseek-v3:latest\n\n 376.65 GB  |  deepseek-v3:671b\n\n 376.65 GB  |  r1-1776:671b\n\n 270.14 GB  |  qwen3-coder:480b\n\n 226.38 GB  |  llama3.1:405b\n\n 213.14 GB  |  hermes3:405b\n\n 133.43 GB  |  qwen3-vl:235b\n\n 132.39 GB  |  qwen3:235b\n\n 123.78 GB  |  deepseek-coder-v2:236b\n\n 123.78 GB  |  deepseek-v2:236b\n\n 123.78 GB  |  deepseek-v2.5:latest\n\n 123.78 GB  |  deepseek-v2.5:236b\n\n  94.51 GB  |  falcon:180b\n\n  74.05 GB  |  zephyr:141b\n\n  69.75 GB  |  devstral-2:latest\n\n  69.75 GB  |  devstral-2:123b\n\n   69.1 GB  |  dbrx:latest\n\n   69.1 GB  |  dbrx:132b\n\n  68.19 GB  |  mistral-large:latest\n\n  68.19 GB  |  mistral-large:123b\n\n   63.1 GB  |  megadolphin:latest\n\n   63.1 GB  |  megadolphin:120b\n\n  62.81 GB  |  llama4:latest\n\n  62.52 GB  |  command-a:latest\n\n  62.52 GB  |  command-a:111b\n\n  60.88 GB  |  gpt-oss:120b\n\n  60.88 GB  |  gpt-oss-safeguard:120b\n\n  58.57 GB  |  qwen:110b\n\n  55.15 GB  |  command-r-plus:latest\n\n  55.15 GB  |  command-r-plus:104b\n\n  50.87 GB  |  llama3.2-vision:90b\n\n  46.89 GB  |  qwen3-next:latest\n\n  46.89 GB  |  qwen3-next:80b\n\n  45.36 GB  |  qwen2.5vl:72b\n\n  44.16 GB  |  athene-v2:latest\n\n  44.16 GB  |  athene-v2:72b\n\n  44.16 GB  |  qwen2.5:72b\n\n   39.6 GB  |  cogito:70b\n\n   39.6 GB  |  deepseek-r1:70b\n\n   39.6 GB  |  llama3.1:70b\n\n   39.6 GB  |  llama3.3:latest\n\n   39.6 GB  |  llama3.3:70b\n\n   39.6 GB  |  nemotron:latest\n\n   39.6 GB  |  nemotron:70b\n\n   39.6 GB  |  r1-1776:latest\n\n   39.6 GB  |  r1-1776:70b\n\n   39.6 GB  |  tulu3:70b\n\n   38.4 GB  |  qwen2:72b\n\n   38.4 GB  |  qwen2-math:72b\n\n  38.18 GB  |  qwen:72b\n\n  37.22 GB  |  dolphin-llama3:70b\n\n  37.22 GB  |  firefunction-v2:latest\n\n  37.22 GB  |  firefunction-v2:70b\n\n  37.22 GB  |  hermes3:70b\n\n  37.22 GB  |  llama3:70b\n\n  37.22 GB  |  llama3-chatqa:70b\n\n  37.22 GB  |  llama3-gradient:70b\n\n  37.22 GB  |  llama3-groq-tool-use:70b\n\n  37.22 GB  |  reflection:latest\n\n  37.22 GB  |  reflection:70b\n\n   36.2 GB  |  codellama:70b\n\n   36.2 GB  |  llama2:70b\n\n   36.2 GB  |  llama2-uncensored:70b\n\n   36.2 GB  |  meditron:70b\n\n   36.2 GB  |  orca-mini:70b\n\n   36.2 GB  |  stable-beluga:70b\n\n   36.2 GB  |  wizard-math:70b\n\n  35.53 GB  |  deepseek-llm:67b\n\n  24.63 GB  |  dolphin-mixtral:latest\n\n  24.63 GB  |  mixtral:latest\n\n  24.63 GB  |  notux:latest\n\n  24.63 GB  |  nous-hermes2-mixtral:latest\n\n   22.6 GB  |  nemotron-3-nano:latest\n\n   22.6 GB  |  nemotron-3-nano:30b\n\n  22.17 GB  |  alfred:latest\n\n  22.17 GB  |  alfred:40b\n\n  22.17 GB  |  falcon:40b\n\n  19.71 GB  |  qwen2.5vl:32b\n\n  19.47 GB  |  qwen3-vl:32b\n\n  18.84 GB  |  aya:35b\n\n  18.81 GB  |  qwen3:32b\n\n  18.78 GB  |  llava:34b\n\n  18.49 GB  |  cogito:32b\n\n  18.49 GB  |  deepseek-r1:32b\n\n  18.49 GB  |  openthinker:32b\n\n  18.49 GB  |  qwen2.5:32b\n\n  18.49 GB  |  qwen2.5-coder:32b\n\n  18.49 GB  |  qwq:latest\n\n  18.49 GB  |  qwq:32b\n\n  18.44 GB  |  aya-expanse:32b\n\n  18.25 GB  |  qwen3-vl:30b\n\n  18.14 GB  |  olmo-3:32b\n\n  18.14 GB  |  olmo-3.1:latest\n\n  18.14 GB  |  olmo-3.1:32b\n\n  18.13 GB  |  nous-hermes2:34b\n\n  18.13 GB  |  yi:34b\n\n  18.02 GB  |  exaone-deep:32b\n\n  18.02 GB  |  exaone3.5:32b\n\n  17.92 GB  |  granite-code:34b\n\n  17.74 GB  |  codebooga:latest\n\n  17.74 GB  |  codebooga:34b\n\n  17.74 GB  |  codellama:34b\n\n  17.74 GB  |  phind-codellama:latest\n\n  17.74 GB  |  phind-codellama:34b\n\n  17.53 GB  |  deepseek-coder:33b\n\n  17.53 GB  |  wizardcoder:33b\n\n  17.43 GB  |  command-r:latest\n\n  17.43 GB  |  command-r:35b\n\n  17.28 GB  |  qwen3:30b\n\n  17.28 GB  |  qwen3-coder:latest\n\n  17.28 GB  |  qwen3-coder:30b\n\n  17.23 GB  |  qwen:32b\n\n   17.1 GB  |  vicuna:33b\n\n   17.1 GB  |  wizard-vicuna-uncensored:30b\n\n   16.2 GB  |  gemma3:27b\n\n  16.17 GB  |  translategemma:27b\n\n   15.5 GB  |  shieldgemma:27b\n\n  14.56 GB  |  gemma2:27b\n\n  14.42 GB  |  mistral-small3.1:latest\n\n  14.42 GB  |  mistral-small3.1:24b\n\n  14.14 GB  |  devstral-small-2:latest\n\n  14.14 GB  |  devstral-small-2:24b\n\n  14.14 GB  |  mistral-small3.2:latest\n\n  14.14 GB  |  mistral-small3.2:24b\n\n  13.35 GB  |  devstral:latest\n\n  13.35 GB  |  devstral:24b\n\n  13.35 GB  |  magistral:latest\n\n  13.35 GB  |  magistral:24b\n\n  13.35 GB  |  mistral-small:latest\n\n  13.35 GB  |  mistral-small:24b\n\n  12.85 GB  |  gpt-oss:latest\n\n  12.85 GB  |  gpt-oss:20b\n\n  12.85 GB  |  gpt-oss-safeguard:latest\n\n  12.85 GB  |  gpt-oss-safeguard:20b\n\n   12.4 GB  |  solar-pro:latest\n\n   12.4 GB  |  solar-pro:22b\n\n  11.71 GB  |  codestral:latest\n\n  11.71 GB  |  codestral:22b\n\n  11.71 GB  |  mistral-small:22b\n\n  10.82 GB  |  sailor2:20b\n\n  10.76 GB  |  granite-code:20b\n\n  10.55 GB  |  internlm2:20b\n\n  10.35 GB  |  phi4-reasoning:latest\n\n  10.35 GB  |  phi4-reasoning:14b\n\n   8.64 GB  |  qwen3:14b\n\n   8.46 GB  |  ministral-3:14b\n\n   8.44 GB  |  dolphincoder:15b\n\n   8.44 GB  |  starcoder2:15b\n\n   8.43 GB  |  phi4:latest\n\n   8.43 GB  |  phi4:14b\n\n   8.37 GB  |  cogito:14b\n\n   8.37 GB  |  deepcoder:latest\n\n   8.37 GB  |  deepcoder:14b\n\n   8.37 GB  |  deepseek-r1:14b\n\n   8.37 GB  |  qwen2.5:14b\n\n   8.37 GB  |  qwen2.5-coder:14b\n\n   8.37 GB  |  sqlcoder:15b\n\n   8.37 GB  |  starcoder:15b\n\n   8.29 GB  |  deepseek-coder-v2:latest\n\n   8.29 GB  |  deepseek-coder-v2:16b\n\n   8.29 GB  |  deepseek-v2:latest\n\n   8.29 GB  |  deepseek-v2:16b\n\n   7.78 GB  |  olmo2:13b\n\n   7.62 GB  |  qwen:14b\n\n   7.59 GB  |  gemma3:12b\n\n   7.55 GB  |  translategemma:12b\n\n   7.46 GB  |  llava:13b\n\n   7.35 GB  |  phi3:14b\n\n   7.28 GB  |  llama3.2-vision:latest\n\n   7.28 GB  |  llama3.2-vision:11b\n\n   7.03 GB  |  gemma3n:latest\n\n   6.86 GB  |  codellama:13b\n\n   6.86 GB  |  codeup:latest\n\n   6.86 GB  |  codeup:13b\n\n   6.86 GB  |  everythinglm:latest\n\n   6.86 GB  |  everythinglm:13b\n\n   6.86 GB  |  llama2:13b\n\n   6.86 GB  |  llama2-chinese:13b\n\n   6.86 GB  |  nexusraven:latest\n\n   6.86 GB  |  nexusraven:13b\n\n   6.86 GB  |  nous-hermes:13b\n\n   6.86 GB  |  open-orca-platypus2:latest\n\n   6.86 GB  |  open-orca-platypus2:13b\n\n   6.86 GB  |  orca-mini:13b\n\n   6.86 GB  |  orca2:13b\n\n   6.86 GB  |  stable-beluga:13b\n\n   6.86 GB  |  vicuna:13b\n\n   6.86 GB  |  wizard-math:13b\n\n   6.86 GB  |  wizard-vicuna:latest\n\n   6.86 GB  |  wizard-vicuna:13b\n\n   6.86 GB  |  wizard-vicuna-uncensored:13b\n\n   6.86 GB  |  wizardlm-uncensored:latest\n\n   6.86 GB  |  wizardlm-uncensored:13b\n\n   6.86 GB  |  xwinlm:13b\n\n   6.86 GB  |  yarn-llama2:13b\n\n   6.59 GB  |  mistral-nemo:latest\n\n   6.59 GB  |  mistral-nemo:12b\n\n   6.49 GB  |  stablelm2:12b\n\n   6.23 GB  |  deepseek-ocr:latest\n\n   6.23 GB  |  deepseek-ocr:3b\n\n   5.94 GB  |  falcon2:latest\n\n   5.94 GB  |  falcon2:11b\n\n   5.86 GB  |  falcon3:10b\n\n   5.72 GB  |  qwen3-vl:latest\n\n   5.72 GB  |  qwen3-vl:8b\n\n   5.66 GB  |  nous-hermes2:latest\n\n   5.66 GB  |  nous-hermes2:10.7b\n\n   5.66 GB  |  solar:latest\n\n   5.66 GB  |  solar:10.7b\n\n   5.61 GB  |  ministral-3:latest\n\n   5.61 GB  |  ministral-3:8b\n\n   5.56 GB  |  qwen2.5vl:latest\n\n   5.56 GB  |  qwen2.5vl:7b\n\n5.4 GB  |  granite3-guardian:8b\n\n   5.37 GB  |  shieldgemma:latest\n\n   5.37 GB  |  shieldgemma:9b\n\n   5.16 GB  |  llava-llama3:latest\n\n   5.16 GB  |  llava-llama3:8b\n\n5.1 GB  |  minicpm-v:latest\n\n5.1 GB  |  minicpm-v:8b\n\n   5.08 GB  |  codegeex4:latest\n\n   5.08 GB  |  codegeex4:9b\n\n   5.08 GB  |  glm4:latest\n\n   5.08 GB  |  glm4:9b\n\n   5.07 GB  |  gemma2:latest\n\n   5.07 GB  |  gemma2:9b\n\n   4.88 GB  |  sailor2:latest\n\n   4.88 GB  |  sailor2:8b\n\n   4.87 GB  |  deepseek-r1:latest\n\n   4.87 GB  |  deepseek-r1:8b\n\n   4.87 GB  |  qwen3:latest\n\n   4.87 GB  |  qwen3:8b\n\n   4.76 GB  |  rnj-1:latest\n\n   4.76 GB  |  rnj-1:8b\n\n   4.71 GB  |  aya-expanse:latest\n\n   4.71 GB  |  aya-expanse:8b\n\n   4.71 GB  |  command-r7b:latest\n\n   4.71 GB  |  command-r7b:7b\n\n   4.71 GB  |  command-r7b-arabic:latest\n\n   4.71 GB  |  command-r7b-arabic:7b\n\n   4.69 GB  |  yi:9b\n\n   4.69 GB  |  yi-coder:latest\n\n   4.69 GB  |  yi-coder:9b\n\n   4.67 GB  |  codegemma:latest\n\n   4.67 GB  |  codegemma:7b\n\n   4.67 GB  |  gemma:latest\n\n   4.67 GB  |  gemma:7b\n\n   4.65 GB  |  granite3.1-dense:latest\n\n   4.65 GB  |  granite3.1-dense:8b\n\n4.6 GB  |  granite3-dense:8b\n\n4.6 GB  |  granite3.2:latest\n\n4.6 GB  |  granite3.2:8b\n\n4.6 GB  |  granite3.3:latest\n\n4.6 GB  |  granite3.3:8b\n\n   4.58 GB  |  cogito:latest\n\n   4.58 GB  |  cogito:8b\n\n   4.58 GB  |  dolphin3:latest\n\n   4.58 GB  |  dolphin3:8b\n\n   4.58 GB  |  llama-guard3:latest\n\n   4.58 GB  |  llama-guard3:8b\n\n   4.58 GB  |  llama3.1:latest\n\n   4.58 GB  |  llama3.1:8b\n\n   4.58 GB  |  tulu3:latest\n\n   4.58 GB  |  tulu3:8b\n\n   4.47 GB  |  aya:latest\n\n   4.47 GB  |  aya:8b\n\n   4.44 GB  |  exaone-deep:latest\n\n   4.44 GB  |  exaone-deep:7.8b\n\n   4.44 GB  |  exaone3.5:latest\n\n   4.44 GB  |  exaone3.5:7.8b\n\n   4.41 GB  |  bakllava:latest\n\n   4.41 GB  |  bakllava:7b\n\n   4.41 GB  |  llama-pro:latest\n\n   4.41 GB  |  llava:latest\n\n   4.41 GB  |  llava:7b\n\n   4.41 GB  |  opencoder:latest\n\n   4.41 GB  |  opencoder:8b\n\n   4.39 GB  |  bespoke-minicheck:latest\n\n   4.39 GB  |  bespoke-minicheck:7b\n\n   4.36 GB  |  deepseek-r1:7b\n\n   4.36 GB  |  marco-o1:latest\n\n   4.36 GB  |  marco-o1:7b\n\n   4.36 GB  |  openthinker:latest\n\n   4.36 GB  |  openthinker:7b\n\n   4.36 GB  |  qwen2.5:latest\n\n   4.36 GB  |  qwen2.5:7b\n\n   4.36 GB  |  qwen2.5-coder:latest\n\n   4.36 GB  |  qwen2.5-coder:7b\n\n   4.36 GB  |  qwen3-embedding:latest\n\n   4.36 GB  |  qwen3-embedding:8b\n\n   4.34 GB  |  dolphin-llama3:latest\n\n   4.34 GB  |  dolphin-llama3:8b\n\n   4.34 GB  |  hermes3:latest\n\n   4.34 GB  |  hermes3:8b\n\n   4.34 GB  |  llama3:latest\n\n   4.34 GB  |  llama3:8b\n\n   4.34 GB  |  llama3-chatqa:latest\n\n   4.34 GB  |  llama3-chatqa:8b\n\n   4.34 GB  |  llama3-gradient:latest\n\n   4.34 GB  |  llama3-gradient:8b\n\n   4.34 GB  |  llama3-groq-tool-use:latest\n\n   4.34 GB  |  llama3-groq-tool-use:8b\n\n   4.28 GB  |  granite-code:8b\n\n   4.26 GB  |  falcon3:latest\n\n   4.26 GB  |  falcon3:7b\n\n4.2 GB  |  qwen:7b\n\n   4.16 GB  |  olmo-3:latest\n\n   4.16 GB  |  olmo-3:7b\n\n   4.16 GB  |  olmo2:latest\n\n   4.16 GB  |  olmo2:7b\n\n   4.15 GB  |  internlm2:latest\n\n   4.15 GB  |  internlm2:7b\n\n   4.13 GB  |  qwen2:latest\n\n   4.13 GB  |  qwen2:7b\n\n   4.13 GB  |  qwen2-math:latest\n\n   4.13 GB  |  qwen2-math:7b\n\n   4.07 GB  |  mistral:latest\n\n   4.07 GB  |  mistral:7b\n\n4.0 GB  |  starcoder:7b\n\n   3.94 GB  |  dolphincoder:latest\n\n   3.94 GB  |  dolphincoder:7b\n\n   3.92 GB  |  falcon:latest\n\n   3.92 GB  |  falcon:7b\n\n   3.89 GB  |  codeqwen:latest\n\n   3.89 GB  |  codeqwen:7b\n\n   3.83 GB  |  dolphin-mistral:latest\n\n   3.83 GB  |  dolphin-mistral:7b\n\n   3.83 GB  |  mathstral:latest\n\n   3.83 GB  |  mathstral:7b\n\n   3.83 GB  |  mistral-openorca:latest\n\n   3.83 GB  |  mistral-openorca:7b\n\n   3.83 GB  |  mistrallite:latest\n\n   3.83 GB  |  mistrallite:7b\n\n   3.83 GB  |  neural-chat:latest\n\n   3.83 GB  |  neural-chat:7b\n\n   3.83 GB  |  notus:latest\n\n   3.83 GB  |  notus:7b\n\n   3.83 GB  |  openchat:latest\n\n   3.83 GB  |  openchat:7b\n\n   3.83 GB  |  openhermes:latest\n\n   3.83 GB  |  samantha-mistral:latest\n\n   3.83 GB  |  samantha-mistral:7b\n\n   3.83 GB  |  sqlcoder:latest\n\n   3.83 GB  |  sqlcoder:7b\n\n   3.83 GB  |  starling-lm:latest\n\n   3.83 GB  |  starling-lm:7b\n\n   3.83 GB  |  wizard-math:latest\n\n   3.83 GB  |  wizard-math:7b\n\n   3.83 GB  |  wizardlm2:latest\n\n   3.83 GB  |  wizardlm2:7b\n\n   3.83 GB  |  yarn-mistral:latest\n\n   3.83 GB  |  yarn-mistral:7b\n\n   3.83 GB  |  zephyr:latest\n\n   3.83 GB  |  zephyr:7b\n\n   3.77 GB  |  starcoder2:7b\n\n   3.73 GB  |  deepseek-llm:latest\n\n   3.73 GB  |  deepseek-llm:7b\n\n   3.56 GB  |  codellama:latest\n\n   3.56 GB  |  codellama:7b\n\n   3.56 GB  |  deepseek-coder:6.7b\n\n   3.56 GB  |  duckdb-nsql:latest\n\n   3.56 GB  |  duckdb-nsql:7b\n\n   3.56 GB  |  llama2:latest\n\n   3.56 GB  |  llama2:7b\n\n   3.56 GB  |  llama2-chinese:latest\n\n   3.56 GB  |  llama2-chinese:7b\n\n   3.56 GB  |  llama2-uncensored:latest\n\n   3.56 GB  |  llama2-uncensored:7b\n\n   3.56 GB  |  magicoder:latest\n\n   3.56 GB  |  magicoder:7b\n\n   3.56 GB  |  meditron:latest\n\n   3.56 GB  |  meditron:7b\n\n   3.56 GB  |  medllama2:latest\n\n   3.56 GB  |  medllama2:7b\n\n   3.56 GB  |  nous-hermes:latest\n\n   3.56 GB  |  nous-hermes:7b\n\n   3.56 GB  |  orca-mini:7b\n\n   3.56 GB  |  orca2:latest\n\n   3.56 GB  |  orca2:7b\n\n   3.56 GB  |  stable-beluga:latest\n\n   3.56 GB  |  stable-beluga:7b\n\n   3.56 GB  |  vicuna:latest\n\n   3.56 GB  |  vicuna:7b\n\n   3.56 GB  |  wizard-vicuna-uncensored:latest\n\n   3.56 GB  |  wizard-vicuna-uncensored:7b\n\n   3.56 GB  |  xwinlm:latest\n\n   3.56 GB  |  xwinlm:7b\n\n   3.56 GB  |  yarn-llama2:latest\n\n   3.56 GB  |  yarn-llama2:7b\n\n   3.37 GB  |  smallthinker:latest\n\n   3.37 GB  |  smallthinker:3b\n\n   3.32 GB  |  deepscaler:latest\n\n   3.32 GB  |  deepscaler:1.5b\n\n   3.24 GB  |  yi:latest\n\n   3.24 GB  |  yi:6b\n\n   3.11 GB  |  gemma3:latest\n\n   3.11 GB  |  gemma3:4b\n\n   3.07 GB  |  qwen3-vl:4b\n\n   3.07 GB  |  translategemma:latest\n\n   3.07 GB  |  translategemma:4b\n\n   3.04 GB  |  granite4:1b\n\n   2.98 GB  |  qwen2.5vl:3b\n\n   2.94 GB  |  phi4-mini-reasoning:latest\n\n   2.94 GB  |  phi4-mini-reasoning:3.8b\n\n   2.75 GB  |  ministral-3:3b\n\n   2.73 GB  |  llava-phi3:latest\n\n   2.73 GB  |  llava-phi3:3.8b\n\n   2.51 GB  |  granite3-guardian:latest\n\n   2.51 GB  |  granite3-guardian:2b\n\n   2.51 GB  |  nemotron-mini:latest\n\n   2.51 GB  |  nemotron-mini:4b\n\n   2.33 GB  |  qwen3:4b\n\n   2.33 GB  |  qwen3-embedding:4b\n\n   2.32 GB  |  phi4-mini:latest\n\n   2.32 GB  |  phi4-mini:3.8b\n\n   2.27 GB  |  granite3.2-vision:latest\n\n   2.27 GB  |  granite3.2-vision:2b\n\n   2.17 GB  |  qwen:latest\n\n   2.17 GB  |  qwen:4b\n\n   2.09 GB  |  cogito:3b\n\n   2.03 GB  |  nuextract:latest\n\n   2.03 GB  |  nuextract:3.8b\n\n   2.03 GB  |  phi3:latest\n\n   2.03 GB  |  phi3:3.8b\n\n   2.03 GB  |  phi3.5:latest\n\n   2.03 GB  |  phi3.5:3.8b\n\n   1.96 GB  |  granite4:3b\n\n   1.92 GB  |  granite3-moe:3b\n\n1.9 GB  |  granite3.1-moe:latest\n\n1.9 GB  |  granite3.1-moe:3b\n\n   1.88 GB  |  hermes3:3b\n\n   1.88 GB  |  llama3.2:latest\n\n   1.88 GB  |  llama3.2:3b\n\n   1.87 GB  |  falcon3:3b\n\n   1.86 GB  |  granite-code:latest\n\n   1.86 GB  |  granite-code:3b\n\n   1.84 GB  |  orca-mini:latest\n\n   1.84 GB  |  orca-mini:3b\n\n1.8 GB  |  qwen2.5:3b\n\n1.8 GB  |  qwen2.5-coder:3b\n\n   1.76 GB  |  qwen3-vl:2b\n\n   1.71 GB  |  starcoder:latest\n\n   1.71 GB  |  starcoder:3b\n\n1.7 GB  |  smollm2:latest\n\n1.7 GB  |  smollm2:1.7b\n\n   1.66 GB  |  falcon3:1b\n\n   1.62 GB  |  moondream:latest\n\n   1.62 GB  |  moondream:1.8b\n\n   1.59 GB  |  shieldgemma:2b\n\n   1.59 GB  |  starcoder2:latest\n\n   1.59 GB  |  starcoder2:3b\n\n   1.56 GB  |  gemma:2b\n\n   1.53 GB  |  exaone-deep:2.4b\n\n   1.53 GB  |  exaone3.5:2.4b\n\n   1.52 GB  |  gemma2:2b\n\n1.5 GB  |  stable-code:latest\n\n1.5 GB  |  stable-code:3b\n\n1.5 GB  |  stablelm-zephyr:latest\n\n1.5 GB  |  stablelm-zephyr:3b\n\n   1.49 GB  |  dolphin-phi:latest\n\n   1.49 GB  |  dolphin-phi:2.7b\n\n   1.49 GB  |  granite3-dense:latest\n\n   1.49 GB  |  granite3-dense:2b\n\n   1.49 GB  |  llama-guard3:1b\n\n   1.49 GB  |  phi:latest\n\n   1.49 GB  |  phi:2.7b\n\n   1.46 GB  |  granite3.1-dense:2b\n\n   1.44 GB  |  codegemma:2b\n\n   1.44 GB  |  granite3.2:2b\n\n   1.44 GB  |  granite3.3:2b\n\n   1.32 GB  |  granite3.1-moe:1b\n\n   1.32 GB  |  opencoder:1.5b\n\n   1.27 GB  |  qwen3:1.7b\n\n   1.23 GB  |  llama3.2:1b\n\n   1.08 GB  |  bge-m3:latest\n\n   1.08 GB  |  snowflake-arctic-embed2:latest\n\n   1.04 GB  |  deepcoder:1.5b\n\n   1.04 GB  |  deepseek-r1:1.5b\n\n   1.04 GB  |  internlm2:1.8b\n\n   1.04 GB  |  qwen:1.8b\n\n   0.98 GB  |  sailor2:1b\n\n   0.92 GB  |  qwen2.5:1.5b\n\n   0.92 GB  |  qwen2.5-coder:1.5b\n\n   0.92 GB  |  smollm:latest\n\n   0.92 GB  |  smollm:1.7b\n\n   0.92 GB  |  stablelm2:latest\n\n   0.92 GB  |  stablelm2:1.6b\n\n   0.87 GB  |  qwen2:1.5b\n\n   0.87 GB  |  qwen2-math:1.5b\n\n   0.87 GB  |  reader-lm:latest\n\n   0.87 GB  |  reader-lm:1.5b\n\n   0.81 GB  |  yi-coder:1.5b\n\n   0.77 GB  |  granite3-moe:latest\n\n   0.77 GB  |  granite3-moe:1b\n\n   0.76 GB  |  gemma3:1b\n\n   0.72 GB  |  deepseek-coder:latest\n\n   0.72 GB  |  deepseek-coder:1.3b\n\n   0.68 GB  |  lfm2.5-thinking:latest\n\n   0.68 GB  |  lfm2.5-thinking:1.2b\n\n   0.68 GB  |  starcoder:1b\n\n   0.62 GB  |  bge-large:latest\n\n   0.62 GB  |  mxbai-embed-large:latest\n\n   0.62 GB  |  snowflake-arctic-embed:latest\n\n0.6 GB  |  qwen3-embedding:0.6b\n\n   0.59 GB  |  tinydolphin:latest\n\n   0.59 GB  |  tinydolphin:1.1b\n\n   0.59 GB  |  tinyllama:latest\n\n   0.59 GB  |  tinyllama:1.1b\n\n   0.58 GB  |  embeddinggemma:latest\n\n   0.52 GB  |  paraphrase-multilingual:latest\n\n   0.49 GB  |  qwen3:0.6b\n\n   0.37 GB  |  qwen:0.5b\n\n   0.37 GB  |  qwen2.5:0.5b\n\n   0.37 GB  |  qwen2.5-coder:0.5b\n\n   0.33 GB  |  qwen2:0.5b\n\n   0.33 GB  |  reader-lm:0.5b\n\n   0.28 GB  |  functiongemma:latest\n\n   0.26 GB  |  nomic-embed-text:latest\n\n   0.06 GB  |  granite-embedding:latest\n\n   0.04 GB  |  all-minilm:latest",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qm9cgp/ollama_models_ranked_by_vram_requirements/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1kljht",
          "author": "merica420_69",
          "text": "At what quant?",
          "score": 12,
          "created_utc": "2026-01-25 05:59:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kxbyk",
              "author": "triynizzles1",
              "text": "q4 since that is what ollama defaults to. This is a list of models so if you wanted to download and run any of them you would enter â€œollama runâ€ before the name of the model. Then the file it downloads will be the size listed in the chart.",
              "score": 6,
              "created_utc": "2026-01-25 07:33:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ked3h",
          "author": "triynizzles1",
          "text": "Good post ðŸ‘",
          "score": 25,
          "created_utc": "2026-01-25 05:10:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1km6wl",
              "author": "Tall_Instance9797",
              "text": "Looks like a copy and paste from this website: [https://llm-explorer.com/](https://llm-explorer.com/)",
              "score": 28,
              "created_utc": "2026-01-25 06:03:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1kctku",
          "author": "Gombaoxo",
          "text": "Where are all huihui models? LLM is not fun without abliterated models.",
          "score": 16,
          "created_utc": "2026-01-25 05:00:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kyshj",
              "author": "darkpigvirus",
              "text": "Back then I liked huihui models but intelligence suffers when abliterating a model unless you really want those censored topics. Also there is a technique better than abliteration that lessen the intelligence suffering I just forgot it.",
              "score": 2,
              "created_utc": "2026-01-25 07:45:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1lkfnt",
                  "author": "alhinai_03",
                  "text": "You probably mean heretic",
                  "score": 1,
                  "created_utc": "2026-01-25 10:56:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1p2ssh",
                  "author": "According-Delivery44",
                  "text": "What technique? Prompyt injection?",
                  "score": 1,
                  "created_utc": "2026-01-25 21:29:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1l5bs0",
          "author": "arm2armreddit",
          "text": "Ollama defaults to a 4k context length; unfortunately, this is realistically unuseful for real tasks. It would be good to see the true memory usage with the 100% supported context length by the given model.",
          "score": 7,
          "created_utc": "2026-01-25 08:42:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kmcg7",
          "author": "Tall_Instance9797",
          "text": "Is this post just a copy and paste from this website: [https://llm-explorer.com/](https://llm-explorer.com/) ?",
          "score": 9,
          "created_utc": "2026-01-25 06:05:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kp0eu",
              "author": "AdventurousLion9548",
              "text": "Actually, I copied the code that ChatGPT generated and got the output. ðŸ˜‚ðŸ˜‚ðŸ˜‚ import re\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\nLIBRARY_URL = \"https://ollama.com/library/\"\nMANIFEST_URL = \"https://registry.ollama.ai/v2/library/{name}/manifests/{tag}\"\nOUTPUT_FILE = \"ollama_models_ranked_by_vram.txt\"\n\nHEADERS = {\n    \"User-Agent\": \"ollama-vram-ranker/1.1\"\n}\n\nWEIGHT_MEDIA_TYPES = {\n    \"application/vnd.ollama.image.model\",\n    \"application/vnd.ollama.image.projector\",  # vision models\n}\n\nsession = requests.Session()\nsession.headers.update(HEADERS)\n\n\n# -------------------- helpers --------------------\n\ndef get_html(url):\n    r = session.get(url, timeout=30)\n    r.raise_for_status()\n    return r.text\n\n\ndef bytes_to_gb(b):\n    return b / (1024 ** 3)\n\n\n# -------------------- scraping --------------------\n\ndef get_all_models():\n    html = get_html(LIBRARY_URL)\n    soup = BeautifulSoup(html, \"html.parser\")\n\n    models = set()\n    for a in soup.select('a[href^=\"/library/\"]'):\n        href = a.get(\"href\", \"\")\n        m = re.fullmatch(r\"/library/([^/]+)\", href)\n        if m:\n            models.add(m.group(1))\n\n    return sorted(models)\n\n\ndef get_model_tags(model):\n    url = urljoin(LIBRARY_URL, model)\n    html = get_html(url)\n    soup = BeautifulSoup(html, \"html.parser\")\n\n    text = soup.get_text(\" \", strip=True).lower()\n    tags = set()\n\n    for token in text.split():\n        if re.fullmatch(r\"\\d+(\\.\\d+)?b\", token):\n            tags.add(token)\n        elif token == \"latest\":\n            tags.add(token)\n\n    if not tags:\n        tags.add(\"latest\")\n\n    # latest first, then numeric\n    def sort_key(t):\n        if t == \"latest\":\n            return (0, 0)\n        m = re.match(r\"(\\d+(\\.\\d+)?)b\", t)\n        return (1, float(m.group(1)) if m else 999)\n\n    return sorted(tags, key=sort_key)\n\n\n# -------------------- registry --------------------\n\ndef get_manifest_weight_bytes(model, tag):\n    url = MANIFEST_URL.format(name=model, tag=tag)\n\n    try:\n        r = session.get(url, timeout=30)\n        if r.status_code != 200:\n            return None\n\n        manifest = r.json()\n\n        # Ã°ÂŸÂ”Â´ CRITICAL FIX: validate manifest\n        if not isinstance(manifest, dict):\n            return None\n\n        layers = manifest.get(\"layers\")\n        if not isinstance(layers, list):\n            return None\n\n        total = 0\n        for layer in layers:\n            if (\n                isinstance(layer, dict)\n                and layer.get(\"mediaType\") in WEIGHT_MEDIA_TYPES\n                and isinstance(layer.get(\"size\"), int)\n            ):\n                total += layer[\"size\"]\n\n        return total if total > 0 else None\n\n    except Exception:\n        return None\n\n\n# -------------------- main --------------------\n\ndef main():\n    print(\"Fetching Ollama model list...\")\n    models = get_all_models()\n\n    results = []\n\n    for idx, model in enumerate(models, 1):\n        print(f\"[{idx}/{len(models)}] {model}\")\n\n        try:\n            tags = get_model_tags(model)\n        except Exception:\n            continue\n\n        for tag in tags:\n            size_bytes = get_manifest_weight_bytes(model, tag)\n            if size_bytes:\n                results.append({\n                    \"model\": model,\n                    \"tag\": tag,\n                    \"gb\": round(bytes_to_gb(size_bytes), 2)\n                })\n\n    results.sort(key=lambda x: x[\"gb\"], reverse=True)\n\n    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"Ollama Models Ranked by VRAM Requirement (Weights Only)\\n\")\n        f.write(\"=\" * 60 + \"\\n\\n\")\n\n        for r in results:\n            f.write(f\"{r['gb']:>7} GB  |  {r['model']}:{r['tag']}\\n\")\n\n    print(f\"\\nÃ¢ÂœÂ” Output written to: {OUTPUT_FILE}\")\n    print(f\"Ã¢ÂœÂ” {len(results)} valid model entries written\")\n\n\nif __name__ == \"__main__\":\n    main()",
              "score": 5,
              "created_utc": "2026-01-25 06:25:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1l7c0n",
          "author": "leo-the-great",
          "text": "ðŸ˜… damn I have to scroll way way down to check what I can run.",
          "score": 5,
          "created_utc": "2026-01-25 09:00:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1lk729",
              "author": "AdventurousLion9548",
              "text": "I feel ya!",
              "score": 1,
              "created_utc": "2026-01-25 10:54:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1klbna",
          "author": "nitinmms1",
          "text": "This is very useful",
          "score": 6,
          "created_utc": "2026-01-25 05:57:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1km1wn",
              "author": "Tall_Instance9797",
              "text": "Looks like a copy and paste from this website: [https://llm-explorer.com/](https://llm-explorer.com/) \\- which honestly is far more useful.",
              "score": 9,
              "created_utc": "2026-01-25 06:02:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1o1du4",
                  "author": "Puzzled_Surprise_383",
                  "text": "Amigo Ã© o seu terceiro comentÃ¡rio querendo acabar com o OP, jÃ¡ entendemos sua revolta. \nAgradeÃ§o a ele por postar pois nÃ£o conhecia o site. \nObrigado, OP, post muito Ãºtil",
                  "score": 5,
                  "created_utc": "2026-01-25 18:46:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1kj8bj",
          "author": "slow-fast-person",
          "text": "How to calculate this for corresponding requirements on apple metal?",
          "score": 3,
          "created_utc": "2026-01-25 05:43:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lhfit",
          "author": "austrobergbauernbua",
          "text": "I can just highly recommend the granite 4 models (hybrid architecture). For small tasks like simple Q&A or rewriting they are extremely efficient (fast memory loading - extremely fast inference).",
          "score": 2,
          "created_utc": "2026-01-25 10:30:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1m4rez",
          "author": "Electronic-Set-2413",
          "text": "Nice one",
          "score": 2,
          "created_utc": "2026-01-25 13:31:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1md7hd",
          "author": "Bonzupii",
          "text": "Really dude",
          "score": 2,
          "created_utc": "2026-01-25 14:19:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nxtyg",
          "author": "gamesta2",
          "text": "Looks like 4k context length for all. I can confirm that most of these take as much as double the vram once you increase context to 48k+. For example, my 4b model that supports my home assistant takes 16gb vram at 128k context.",
          "score": 2,
          "created_utc": "2026-01-25 18:31:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1out0a",
              "author": "SundayButtermilk",
              "text": "Are you using this for home assistant voice? What hardware are you running it on?",
              "score": 2,
              "created_utc": "2026-01-25 20:55:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1povhv",
                  "author": "gamesta2",
                  "text": "I use it for tool calling. The model i run is qwen3:4b-instruct-q_8 (for better precision). I lied about context, mine is set to 96k. It has to be this high for hass because for some reason each call is 50k+ tokens, depending on how many devices you expose.\n\nThe hardware is a separate server on the same local network. nothing fancy. Ryzen 7 9700x, 64gb ddr5, and dual rtx 3060 do the hard lifting. Llms are hosted by ollama. \n\nIm not too worried about offloading into ram for most Ai usage when utilizing other models for things like image analysis (qwen3-vl:30b) but for hass tool calling I had to go with a model small enough to fit into 24gb vram to maximize response time, and at the same time handle 96k context.\nBiggest ragret is not getting additional 64gb of ram back when it was 240$.",
                  "score": 1,
                  "created_utc": "2026-01-25 23:07:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1pqxos",
                  "author": "gamesta2",
                  "text": "As for voice, yes. I use hass assistant on my phone and my watch to give it voice commands. Nabu transcribes it to text (or you can use the stock stt that comes with hass) and the assistant either takes own action if its something simple, or makes a call to llm if its something complex like if-then or multiple actions.",
                  "score": 1,
                  "created_utc": "2026-01-25 23:16:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1l35ym",
          "author": "vir_db",
          "text": "With maximum num_ctx?",
          "score": 1,
          "created_utc": "2026-01-25 08:23:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l6hnx",
          "author": "pmv143",
          "text": "We run all of these models on the same 16Ã—H100 pool. Big models take the whole beam, smaller ones take slices. Fast snapshot restore lets us swap between them in ~1â€“2s instead of pinning GPUs.",
          "score": 1,
          "created_utc": "2026-01-25 08:52:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ldnpu",
          "author": "WildDogOne",
          "text": "and now, if someone could do that, but with maxed out context window",
          "score": 1,
          "created_utc": "2026-01-25 09:56:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ldpyp",
          "author": "zenmatrix83",
          "text": "Context length matters and the default is too small for anything not 100 chat based, my current use case is using 32k but I can get 64k worth from qwen3 from my 4090 at decent speeds",
          "score": 1,
          "created_utc": "2026-01-25 09:57:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1mg8vl",
          "author": "AUFairhope1104",
          "text": "Per",
          "score": 1,
          "created_utc": "2026-01-25 14:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nujze",
          "author": "Mangostickyrice1999",
          "text": "Source?",
          "score": 1,
          "created_utc": "2026-01-25 18:18:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ocmn9",
              "author": "AdventurousLion9548",
              "text": "From ollama/models site. Also here https://www.reddit.com/r/ollama/s/HaUw1tIu6W",
              "score": 2,
              "created_utc": "2026-01-25 19:34:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qiug46",
      "title": "Fine-tuned Qwen3 0.6B for Text2SQL using a claude skill. The result tiny model matches a Deepseek 3.1 and runs locally on CPU.",
      "subreddit": "ollama",
      "url": "https://i.redd.it/fy13421qjoeg1.png",
      "author": "party-horse",
      "created_utc": "2026-01-21 10:30:24",
      "score": 195,
      "num_comments": 9,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qiug46/finetuned_qwen3_06b_for_text2sql_using_a_claude/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0up0ey",
          "author": "cirejr",
          "text": "This is great, I've been trying to make this text2sql happen for couple of weeks now using lightweight models. And I have to say without fine tuning them it's really something ðŸ˜…. I tried couple of ways, giving functionGemma bunch of tools. Using some 3b models and giving and creating a Neon mcp client but yeah I guess fine tuning is all that's left.",
          "score": 8,
          "created_utc": "2026-01-21 13:37:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0utcy8",
              "author": "party-horse",
              "text": "Awsome, feel free to use the claude skill to train a model for your specific domain/dialect!",
              "score": 1,
              "created_utc": "2026-01-21 14:00:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0wnup2",
          "author": "jlugao",
          "text": "How did you come up with the datasets for training and evaluating? I am thinking of doing a similar project for evaluating execution plans and coming up with recommendations",
          "score": 3,
          "created_utc": "2026-01-21 19:06:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0x6gmz",
              "author": "party-horse",
              "text": "I chatted with a few LLMs to get example conversations. Fortunately you only need approx 20 to get started so its pretty easy",
              "score": 5,
              "created_utc": "2026-01-21 20:30:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ip6g9",
          "author": "Sairefer",
          "text": "    4. Upload data\n    ...\n    All local.\n    ...\n    Hmmm...",
          "score": 2,
          "created_utc": "2026-01-24 23:26:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0zezv0",
          "author": "Puzzled_Fisherman_94",
          "text": "Thx for the tutorial",
          "score": 1,
          "created_utc": "2026-01-22 03:30:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14ig60",
          "author": "_RemyLeBeau_",
          "text": "\"all local\"\n\nIf this were true, you could just share the skill and not have toÂ distil login",
          "score": 1,
          "created_utc": "2026-01-22 21:46:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1aakp1",
          "author": "Odd-Photojournalist8",
          "text": "Would be cool to do one that could integrate `ctibutler` and a few KEV reputable sources. Then a bigger model ask detailed queries asking small fine tuned model(cheap) to extract correlated set of data. Cyber security basics using AI",
          "score": 1,
          "created_utc": "2026-01-23 18:41:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qgzdk",
          "author": "ComedianObjective572",
          "text": "TBH if you have a background prompt on an LLM I think the output you will get will still be correct without training the model. It would need more inference but either way you donâ€™t need to fine tune the model for it to be correct you might just need 1 shot prompts etc.",
          "score": 1,
          "created_utc": "2026-01-26 01:24:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj3b01",
      "title": "[Open Sourse] I built a tool that forces 5 AIs to debate and cross-check facts before answering you",
      "subreddit": "ollama",
      "url": "https://i.redd.it/ja8et3degqeg1.jpeg",
      "author": "S_Anv",
      "created_utc": "2026-01-21 16:55:27",
      "score": 55,
      "num_comments": 15,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qj3b01/open_sourse_i_built_a_tool_that_forces_5_ais_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0wv1t2",
          "author": "atika",
          "text": "Wasnâ€™t enough to â€œmakeâ€ them debate, you had to â€œforceâ€ them against their wishes?",
          "score": 9,
          "created_utc": "2026-01-21 19:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0wvpic",
              "author": "Nerdinat0r",
              "text": "Not to mention the overhead. How much more RAM and electricity and GPU Time this usesâ€¦",
              "score": 1,
              "created_utc": "2026-01-21 19:41:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0x5eoh",
          "author": "Basic_Young538",
          "text": "This could be really funny...",
          "score": 6,
          "created_utc": "2026-01-21 20:25:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14nk2z",
              "author": "EXPATasap",
              "text": "It can be that and you can get them to be SO DARK and twisted to, think Dark Eldar maxed out lol! Iâ€™ve had a pyqt app I built six months ago that does this with local/external models, I use Jinja templates which gives me all the control Iâ€™d ever want and need with per turn reinforcement system messages, custom to each turn/model. lol. Itâ€™s crazy effective.",
              "score": 1,
              "created_utc": "2026-01-22 22:11:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o11ec2n",
          "author": "butterninja",
          "text": "You gave me an idea. I will waterboard the crap out of 5 AIs to debate and cross-check facts before answering you. Give me a bit of time.",
          "score": 3,
          "created_utc": "2026-01-22 12:55:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0x81b9",
          "author": "Faisal_Biyari",
          "text": "If I use this between the same model, does that mean it is effectively converted into a \"thinking\" model?",
          "score": 1,
          "created_utc": "2026-01-21 20:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0y1t4a",
          "author": "ServeAlone7622",
          "text": "This is the way!\n\nI love that you built an ensemble setup too!\nI could never get this to work for me in a real sense. Accuracy and a lack of ability or possibly desire to critique one another leading into a giant circle jerk of AI handshaking.\n\nHowâ€™s your accuracy? Do they argue back and forth or do they mostly just circle jerk each other?",
          "score": 1,
          "created_utc": "2026-01-21 22:57:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o106gle",
          "author": "upboat_allgoals",
          "text": "multi turn zero shot ensembling? \n\nnice tool. worth running it through some benchmarks.",
          "score": 1,
          "created_utc": "2026-01-22 06:44:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17o1qa",
          "author": "Fabulous-Speech6593",
          "text": "This is good because Chat-GPT is lying its ass off too often, it should be standard to regulate these lying ass Ai confuse models ðŸ˜!",
          "score": 1,
          "created_utc": "2026-01-23 09:59:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17rmoo",
          "author": "usercantollie",
          "text": "You're a genius, OP! This is exactly what the AI world needs right now, forcing multiple models to debate and cross-check facts is the only way to actually maximize productivity without drowning in hallucinations.\n\nMost people are just blindly trusting single-model outputs, but you're actually solving the real problem.",
          "score": 1,
          "created_utc": "2026-01-23 10:31:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17sweo",
          "author": "milli_xoxxy",
          "text": "Honestly, this is brilliant idea! Making five Als argue with each other to find the truth is a game-changer. Also running it locally with Ollama is the real win. This is the future of reliable Al, not just another API wrapper.",
          "score": 1,
          "created_utc": "2026-01-23 10:43:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17xw2s",
          "author": "Eugene_sh",
          "text": "I used \"chorus\" app just for that but they smh abandoned the project it seems",
          "score": 1,
          "created_utc": "2026-01-23 11:25:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15f7m5",
          "author": "NoxinDev",
          "text": "I love it, waste 5 times as much money from the AI speculation companies - way to help burst this bubble, just fantastic.",
          "score": 1,
          "created_utc": "2026-01-23 00:37:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10v8di",
          "author": "390adv",
          "text": "Awesome. Post the results of the Holocaust question",
          "score": 0,
          "created_utc": "2026-01-22 10:30:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1b981e",
          "author": "Low-Coconut5857",
          "text": "â€5 person that debates and cross-checks facts before answering you.â€ Sounds like the weekly meeting I have with my teamâ€¦  except the part about providing an answer.\nI think I skip.",
          "score": 0,
          "created_utc": "2026-01-23 21:23:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj3qkv",
      "title": "Hi folks, Iâ€™ve built an openâ€‘source project that could be useful to some of you",
      "subreddit": "ollama",
      "url": "https://i.redd.it/4plhjok3jqeg1.png",
      "author": "panos_s_",
      "created_utc": "2026-01-21 17:10:42",
      "score": 48,
      "num_comments": 11,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qj3qkv/hi_folks_ive_built_an_opensource_project_that/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0x1sha",
          "author": "jovn1234567890",
          "text": "Very useful thank you",
          "score": 2,
          "created_utc": "2026-01-21 20:09:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0xt8z7",
              "author": "panos_s_",
              "text": "thanks mate!",
              "score": 1,
              "created_utc": "2026-01-21 22:14:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0xgfv7",
          "author": "mofa1",
          "text": "I just want to say that I like the project and have been using it for some time in my Unraid system!",
          "score": 2,
          "created_utc": "2026-01-21 21:15:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0xsofs",
              "author": "panos_s_",
              "text": "thanks a lot :)",
              "score": 1,
              "created_utc": "2026-01-21 22:12:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ybfg1",
          "author": "selfdestroyer",
          "text": "I also have been running this for a month or so and itâ€™s been great to monitor while using OpenwebUI and ConfyUI.",
          "score": 1,
          "created_utc": "2026-01-21 23:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15c5xk",
              "author": "panos_s_",
              "text": "thanks a lot :)",
              "score": 2,
              "created_utc": "2026-01-23 00:21:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o10jxgg",
          "author": "UseHopeful8146",
          "text": "Stumbling blind into the world of GPU compute and am very appreciative",
          "score": 1,
          "created_utc": "2026-01-22 08:44:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15e99z",
          "author": "IllustriousMessage79",
          "text": "I love you",
          "score": 1,
          "created_utc": "2026-01-23 00:32:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15ejne",
              "author": "panos_s_",
              "text": ":)",
              "score": 2,
              "created_utc": "2026-01-23 00:34:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o15yotd",
          "author": "960be6dde311",
          "text": "Nice design. Cool little project",
          "score": 1,
          "created_utc": "2026-01-23 02:26:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gtxd0",
          "author": "Old-Wolverine-4134",
          "text": "You mean AI made it for you",
          "score": 1,
          "created_utc": "2026-01-24 18:12:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkokhv",
      "title": "Ollama Image Generator",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qkokhv/ollama_image_generator/",
      "author": "nickinnov",
      "created_utc": "2026-01-23 11:40:26",
      "score": 48,
      "num_comments": 20,
      "upvote_ratio": 0.93,
      "text": "Hey fellow Ollama fans, I'm delighted that image generation is available so I have written a web app you can run on your own computer (alongside Ollama) to make it easier to generate, save and delete images.\n\nOK it ain't no ComfyUI but makes things tidier and, unlike using the terminal Ollama CLI, images don't clutter up your home folder!  \nRepo at [https://github.com/nicklansley/OllamaImageGenerator](https://github.com/nicklansley/OllamaImageGenerator)\n\n... but all you really need to download is:\n\n* [**server.py**](https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/server.py) \\- small proxying server which you can run without extra packages on your machine as long as you have python 3.9 or higher. No need for a venv as I've just used built-in packages like *http.server* \\- run from terminal as ***python3*** [***server.py***](https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/server.py)\n* [index.html](https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/index.html) \\- does all the grunt work in your web browser on port 8080. Generated images are saved to local storage along with generation settings, and can be deleted individually. Once [server.py](https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/server.py) is up and running, take your web browser to [http://localhost:8080](http://localhost:8080)\n\n[README.md](https://github.com/nicklansley/OllamaImageGenerator/blob/main/README.md) gives more info but quick instructions:\n\n* Download an image generation model in terminal - currently '***ollama pull x/z-image-turbo:bf16***' and '***ollama pull x/flux2-klein:latest***' are supported.\n* Type a prompt, set image width and height, choose a seed and the number of steps, then click 'Generate Image'.\n* During image progression, a 'step N of X' message appears to denote progress.\n* Images are saved to a side panel (actually they are in localStorage so they survive from one session to the next).\n* Save an image onto your machine with right-click 'Save Image As..' or drag if out of the main window and into a folder.\n* Double click a saved image to move it to the main window along with the settings that created it (the image can be recreated as long as seed 0 (Ollama internal random seed) was not used.\n* Single click an image then click 'x' to delete it. The image is removed from the image list saved in localStorage.\n\nWhen more image models become available:\n\n* Download a model with 'image pull image\\_gen\\_model\\_name:tag'\n* Update image list variable IMAGE\\_GEN\\_MODEL\\_LIST with the same model name and tag at the top of index.html\n\n[Screenshot of Ollama Image Generator](https://preview.redd.it/n8kap3a823fg1.png?width=1369&format=png&auto=webp&s=accb2258e20e62c356823ad8b85439aab8424f4a)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qkokhv/ollama_image_generator/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1867zs",
          "author": "vini_stoffel",
          "text": "Only in Mac?",
          "score": 3,
          "created_utc": "2026-01-23 12:26:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o187csu",
              "author": "simplir",
              "text": "Image generation on Ollama is still only Mac at the moment",
              "score": 3,
              "created_utc": "2026-01-23 12:34:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1cna9u",
                  "author": "FlyByPC",
                  "text": "Are there plans to add this to the Windows client anytime soon?",
                  "score": 1,
                  "created_utc": "2026-01-24 01:46:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o18aaup",
          "author": "planetearth80",
          "text": "Doesnâ€™t the Ollama GUI do the same thing?",
          "score": 3,
          "created_utc": "2026-01-23 12:53:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18y01i",
              "author": "nickinnov",
              "text": "Not (yet) - it only understands /api/chat",
              "score": 7,
              "created_utc": "2026-01-23 15:00:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o182vmk",
          "author": "Narrow-Impress-2238",
          "text": "Is that work for windows too already?",
          "score": 1,
          "created_utc": "2026-01-23 12:03:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18yd0f",
              "author": "nickinnov",
              "text": "BY all means give it a go -  [server.py](http://server.py) will work fine as long as you have Python 3.9 or later, but you may need to check the Ollama app for Windows. At its heart [server.py](http://server.py) simply proxies the Ollama API (avoids CORS) plus has its own history saving of images capability.",
              "score": 2,
              "created_utc": "2026-01-23 15:01:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o18atuv",
          "author": "Birdinhandandbush",
          "text": "So why no quantized Z-Image models? that one is like 32gb. At least Klien will work for me",
          "score": 1,
          "created_utc": "2026-01-23 12:56:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18ykmq",
              "author": "nickinnov",
              "text": "Out of my control I'm afraid! Klein is very fast and works well (I am using Apple Metal GPU).",
              "score": 2,
              "created_utc": "2026-01-23 15:03:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o18zacd",
          "author": "nickinnov",
          "text": "UPDATE: Yes so soon used up local storage! Now saves to a history folder that [server.py](http://server.py) creates in its own folder instead. Who knew that web browser local storage can only save about 10MB per web site. Not me it seemed... ðŸ˜‘",
          "score": 1,
          "created_utc": "2026-01-23 15:06:31",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o190nlk",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-01-23 15:13:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o198s10",
              "author": "nickinnov",
              "text": "I agree - like much of Ollama it is simplified to work cleanly without any complexity. Comfy is absolutely at the opposite spectrum. However, I think it's useful to have a simple tool that can be used with little setup. And I think the images are pretty good for all that lack of ComfyUI fine tuning.",
              "score": 1,
              "created_utc": "2026-01-23 15:50:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o19g4tv",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-01-23 16:23:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o19dd35",
          "author": "Total-Context64",
          "text": "You should take a look at [ALICE](https://github.com/SyntheticAutonomicMind/ALICE), it's pretty advanced and can already do this without Ollama.  I'm curious though, what does Ollama add over just using the diffusion models directly?",
          "score": 1,
          "created_utc": "2026-01-23 16:11:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a8y11",
              "author": "nickinnov",
              "text": "For me it's about Ollama's simplicity (with this and all its models). It abstracts away model loading and inference for local 'offline' use. If anyone wants to be more serious about image generation then really Comfy-UI is really the best way to go.",
              "score": 1,
              "created_utc": "2026-01-23 18:34:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1abbw7",
                  "author": "Total-Context64",
                  "text": "Ahh, fair enough.  With ALICE you do need to set up the backend, ROCm or whatever but that's mostly automated.  I created it to interface with SAM so I can use AI assistants to generate images using old hardware that I just had sitting around.  It works very well, and it integrates with HuggingFace and CivitAI directly so you can download models right from the web interface.  It even has LoRA support.",
                  "score": 1,
                  "created_utc": "2026-01-23 18:44:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o19z2wl",
          "author": "Euphoric-Tank-6791",
          "text": "what OS? it does not seem to run on w11 wsl2 as of last night",
          "score": 1,
          "created_utc": "2026-01-23 17:50:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a9brw",
              "author": "nickinnov",
              "text": "MacOS only at present for image generation - a limitation imposed by Ollama itself. I'm sure they are working on Windows and Linux though. I suspect they are developing Ollama on Macs ðŸ˜",
              "score": 1,
              "created_utc": "2026-01-23 18:35:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1bti1g",
          "author": "CoDMplayer_",
          "text": "Any image+prompt->image capability?",
          "score": 1,
          "created_utc": "2026-01-23 23:02:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e4utp",
              "author": "nickinnov",
              "text": "Yes I'd like that too - plus text encoders + loras! I suspect their curren work will be to make image gen work on Windows and Linux (Mac only right now). But hey they read this r/ollama stuff so hopefully they are creating a backlog that includes image+prompt->image too (please!).",
              "score": 1,
              "created_utc": "2026-01-24 08:01:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qkrpnx",
      "title": "I gave my local LLM pipeline  a brain - now it thinks before it speaks",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qkrpnx/i_gave_my_local_llm_pipeline_a_brain_now_it/",
      "author": "danny_094",
      "created_utc": "2026-01-23 14:06:41",
      "score": 38,
      "num_comments": 6,
      "upvote_ratio": 0.87,
      "text": "Jarvis/TRION has received a major update after weeks of implementation. Jarvis (soon to be TRION) has now been provided with a self-developed SEQUENTIAL THINKING MCP.\n\nI would love to explain everything it can do in this Reddit post. But I don't have the space, and neither do you have the patience. u/frank_brsrk Provided a self-developed CIM framework That's hard twisted with Sequential Thinking. So Claude help for the answer:\n\nðŸ§  Gave my local Ollama setup \"extended thinking\" - like Claude, but 100% local\n\nTL;DR: Built a Sequential Thinking system that lets DeepSeek-R1\n\n\"think out loud\" step-by-step before answering. All local, all Ollama.\n\nWhat it does:\n\n\\- Complex questions â†’ AI breaks them into steps\n\n\\- You SEE the reasoning live (not just the answer)\n\n\\- Reduces hallucinations significantly\n\nThe cool part: The AI decides WHEN to use deep thinking.\n\nSimple questions â†’ instant answer.\n\nComplex questions â†’ step-by-step reasoning first.\n\nBuilt with: Ollama + DeepSeek-R1 + custom MCP servers\n\nShoutout to u/frank_brsrk for the CIM framework that makes\n\nthe reasoning actually make sense.\n\nGitHub: [https://github.com/danny094/Jarvis/tree/main](https://github.com/danny094/Jarvis/tree/main)\n\nHappy to answer questions! This took weeks to build ðŸ˜…\n\nOther known issues:\n\n\\- excessively long texts, skipping the control layer - Solution in progress\n\n\\- The side panel is still being edited and will be integrated as a canvas with MCP support.\n\n  \n\n\nhttps://reddit.com/link/1qkrpnx/video/zb6z5muax3fg1/player\n\nhttps://preview.redd.it/el6uhfy6q4fg1.png?width=1147&format=png&auto=webp&s=a16a9525fc50ba59b710f6932cdb3626c2562074\n\nhttps://preview.redd.it/j1ol6fy6q4fg1.png?width=863&format=png&auto=webp&s=f7726aee3e5079419dc665959fc0b779b6d37571\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qkrpnx/i_gave_my_local_llm_pipeline_a_brain_now_it/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o18q7ov",
          "author": "frank_brsrk",
          "text": "Here above is the architecture of the causal intelligence module that decouples thought and augments reasoning on demand. Thanks u/danny094 for the trust. \" Everyday is the day that the project should be finished\" :D\n\nhttps://preview.redd.it/cr7y0qimy3fg1.png?width=2800&format=png&auto=webp&s=3fa05b50978d7f2ea14c2ae7854c682db888331d\n\ncompliments and from the bottom of my heart's void all the best!!! :",
          "score": 5,
          "created_utc": "2026-01-23 14:20:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eoboz",
              "author": "Batinium",
              "text": "Pic Quality too low can't read. On what tool did you prepare them?",
              "score": 2,
              "created_utc": "2026-01-24 10:59:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1eurxj",
                  "author": "frank_brsrk",
                  "text": "On draw.io, I will pubblish soon the open source with polished version for n8n template. Otherwise send dm ur email and i will send u the diagram in higher quality",
                  "score": 1,
                  "created_utc": "2026-01-24 11:56:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1evb3j",
                  "author": "Awkward--Panda",
                  "text": "I guess it's readable here: https://www.reddit.com/r/LocalLLM/s/Hf6UO4Laeo\n\n(via Google picture search. I hope it reflects the actual content)",
                  "score": 1,
                  "created_utc": "2026-01-24 12:00:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1q603c",
          "author": "dropswisdom",
          "text": "I'm building the docker image locally now. However, I'd love a online maintained docker image. and please fix your discord server. it is invalid (from the git repo page).",
          "score": 1,
          "created_utc": "2026-01-26 00:29:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qa2v6",
          "author": "dropswisdom",
          "text": "Getting bad request from fastmcp (192.168.128.9:56052 - \"POST /mcp HTTP/1.1\" 400 Bad Request), communicates in german without an option to change interface language, and goes into a loop. oh, and can't use system prompt.. there's some (A LOT) of work to do.",
          "score": 1,
          "created_utc": "2026-01-26 00:49:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjqcqv",
      "title": "Built an open-source, self-hosted AI agent automation platform â€” feedback welcome",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qjqcqv/built_an_opensource_selfhosted_ai_agent/",
      "author": "Feathered-Beast",
      "created_utc": "2026-01-22 09:44:30",
      "score": 36,
      "num_comments": 16,
      "upvote_ratio": 0.95,
      "text": "Hey folks ðŸ‘‹\n\nIâ€™ve been building an open-source, self-hosted AI agent automation platform that runs locally and keeps all data under your control. Itâ€™s focused on agent workflows, scheduling, execution logs, and document chat (RAG) without relying on hosted SaaS tools.\n\nI recently put together a small website with docs and a project overview.\n\nLinks to the website and GitHub are in the comments.\n\nWould really appreciate feedback from people building or experimenting with open-source AI systems ðŸ™Œ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qjqcqv/built_an_opensource_selfhosted_ai_agent/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o10q88s",
          "author": "Feathered-Beast",
          "text": "Github:- https://github.com/vmDeshpande/ai-agent-automation\n\nWebsite:- https://vmdeshpande.github.io/ai-automation-platform-website/",
          "score": 9,
          "created_utc": "2026-01-22 09:44:43",
          "is_submitter": true,
          "replies": [
            {
              "id": "o11tpn1",
              "author": "cirejr",
              "text": "Curious to know, did you build the website and docs with Gemini-3 ?",
              "score": 2,
              "created_utc": "2026-01-22 14:20:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o11uotu",
                  "author": "Feathered-Beast",
                  "text": "Nope. I built the website myself and wrote the docs manually.\nI did my own research, took feedback from other developers, and documented each feature step by step.\nThis is my first open-source project, so I wanted to do it properly.",
                  "score": 2,
                  "created_utc": "2026-01-22 14:25:17",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o10uvij",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 2,
          "created_utc": "2026-01-22 10:27:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10wmtx",
              "author": "Feathered-Beast",
              "text": "Yes i do have screenshot\n\nhttps://preview.redd.it/ds7l369wqveg1.jpeg?width=831&format=pjpg&auto=webp&s=19ed72d8ced71008629fab414d5a61d9fcdf859f",
              "score": 2,
              "created_utc": "2026-01-22 10:43:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1112bx",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-01-22 11:20:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o127gra",
          "author": "JakeMascaOfficial",
          "text": "This looks amazing",
          "score": 1,
          "created_utc": "2026-01-22 15:28:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o129bd8",
              "author": "Feathered-Beast",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-01-22 15:37:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o14hprs",
          "author": "Known-Maintenance-83",
          "text": "Will test it tomorrow can we contribute?",
          "score": 1,
          "created_utc": "2026-01-22 21:42:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16cqra",
              "author": "Feathered-Beast",
              "text": "Yes, contributions are welcome!",
              "score": 1,
              "created_utc": "2026-01-23 03:45:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o15a6wc",
          "author": "looktwise",
          "text": "it builts a RAG locally?? would it be possible to switch RAGs if I want to separate distinguish content? ",
          "score": 1,
          "created_utc": "2026-01-23 00:11:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16d10g",
              "author": "Feathered-Beast",
              "text": "Yes, the RAG runs fully locally.\nRight now it uses a single vector store, but switching between multiple RAGs (or separate knowledge bases per domain) is definitely possible and planned.\n\nThe architecture already supports isolating content; itâ€™s mostly a config / routing layer on top.",
              "score": 2,
              "created_utc": "2026-01-23 03:47:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o17dox7",
                  "author": "looktwise",
                  "text": "Yeah, separate knowledge domains could be a term for that. I would be very interested if the chosen option of a 'RAG-file' would allow to chat with content 1 in RAG-file 1 and then or in a parallel session would allow a separated chat in RAG-file 2. (think in terms of 2 different schools of thought in philosophy for example, RAG-files filled by several book-PDFs, OCR ready).",
                  "score": 1,
                  "created_utc": "2026-01-23 08:22:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ai5o7",
          "author": "dephraiiim",
          "text": "That's awesome you're building this! For scheduling and coordinating agent workflows, you might want to check out [weekday.so](http://weekday.so) ;  it's an open-source calendar alternative with AI capabilities that keeps everything self-hosted and under your control. Could be a solid complement to your platform.",
          "score": 1,
          "created_utc": "2026-01-23 19:15:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qh10xr",
      "title": "Demo: On-device browser agent (Qwen) running locally in Chrome",
      "subreddit": "ollama",
      "url": "https://v.redd.it/ljp6zwzfcaeg1",
      "author": "thecoder12322",
      "created_utc": "2026-01-19 10:48:47",
      "score": 25,
      "num_comments": 3,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qh10xr/demo_ondevice_browser_agent_qwen_running_locally/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0k28rw",
          "author": "TigerOk6003",
          "text": "Would be very curious to see how data centers will be rendered useless if small models get better and hardware for edge inference gets better too",
          "score": 3,
          "created_utc": "2026-01-19 22:07:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0j7059",
          "author": "New_Inflation_6927",
          "text": "Next challenge for the team could be trying out VLM within it?",
          "score": 1,
          "created_utc": "2026-01-19 19:38:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0n4kha",
              "author": "thecoder12322",
              "text": "We tried and weâ€™re fixing a few bugs for that, itâ€™ll make it even more accurate! Open for any contributions, please check our runanywhere-sdks as well here: https://github.com/RunanywhereAI/runanywhere-sdks\n\nWeâ€™ll be adding web-gpu and vlm support as well which",
              "score": 1,
              "created_utc": "2026-01-20 10:02:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qiv7v8",
      "title": "I built a CLI tool using Ollama (nomic-embed-text) to replace grep with Semantic Code Search",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qiv7v8/i_built_a_cli_tool_using_ollama_nomicembedtext_to/",
      "author": "Technical_Meeting_81",
      "created_utc": "2026-01-21 11:16:14",
      "score": 24,
      "num_comments": 2,
      "upvote_ratio": 0.94,
      "text": "Hi r/ollama,\n\nI've been working on an open-source tool called **GrepAI**, and I wanted to share it here because it relies heavily on **Ollama** to function.\n\n**What is it?** GrepAI is a CLI tool (written in Go) designed to help AI agents (like Claude Code, Cursor, or local agents) understand your codebase better.\n\nInstead of using standard regex `grep` to find codeâ€”which often misses the contextâ€”GrepAI uses **Ollama** to generate local embeddings of your code. This allows you to perform **semantic searches** directly from the terminal.\n\n**The Stack:**\n\n* **Core:** Written in Go.\n* **Embeddings:** Connects to your local Ollama instance (defaults to `nomic-embed-text`).\n* **Vector Store:** In-memory / Local (fast and private).\n\n**Why use Ollama for this?** I wanted a solution that respects privacy and doesn't cost a fortune in API credits just to index a repo. By using Ollama locally, GrepAI builds an index of your project (respecting `.gitignore`) without your code leaving your machine.\n\n**Real-world Impact (Benchmark)** I tested this setup by using GrepAI as a filter for Claude Code (instead of the default grep). The idea was to let Ollama decide what files were relevant before sending them to the cloud. The results were huge:\n\n* **-97% Input Tokens** sent to the LLM (because Ollama filtered the noise).\n* **-27.5% Cost reduction** on the task.\n\nEven if you don't use Claude, this demonstrates how effective local embeddings (via Ollama) are at retrieving the right context for RAG applications.\n\nðŸ‘‰ **Benchmark details:**[https://yoanbernabeu.github.io/grepai/blog/benchmark-grepai-vs-grep-claude-code/](https://yoanbernabeu.github.io/grepai/blog/benchmark-grepai-vs-grep-claude-code/)\n\n**Links:**\n\n* ðŸ“¦ **GitHub:**[https://github.com/yoanbernabeu/grepai](https://github.com/yoanbernabeu/grepai)\n* ðŸ“š **Docs:**[https://yoanbernabeu.github.io/grepai/](https://yoanbernabeu.github.io/grepai/)\n\nI'd love to know what other embedding models you guys are running with Ollama. Currently, `nomic-embed-text` gives me the best results for code, but I'm open to suggestions!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qiv7v8/i_built_a_cli_tool_using_ollama_nomicembedtext_to/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0vbgvm",
          "author": "Ok-District-1756",
          "text": "I'm going to try Qwen3-Embedding-4B-GGUF:Q5\\_K\\_M. Honestly, I have no idea how it will perform in real-world conditions, but I'll test it for a week and report back if anyone is interested",
          "score": 4,
          "created_utc": "2026-01-21 15:30:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ybqej",
          "author": "yesbee-yesbee",
          "text": "Will it work for opencode?Â ",
          "score": 1,
          "created_utc": "2026-01-21 23:50:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qgxulm",
      "title": "Would Anthropic Block Ollama?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qgxulm/would_anthropic_block_ollama/",
      "author": "Lopsided_Dot_4557",
      "created_utc": "2026-01-19 07:36:07",
      "score": 24,
      "num_comments": 17,
      "upvote_ratio": 0.85,
      "text": "Few hours ago, Ollama announced following:\n\nOllama now has Anthropic API compatibility. This enables tools like Claude Code to be used with open-source models.\n\nOllama Blog:Â [Claude Code with Anthropic API compatibility Â· Ollama Blog](https://ollama.com/blog/claude)\n\nHands-on Guide:Â [https://youtu.be/Pbsn-6JEE2s?si=7pdAv5LU9GiBx7aN](https://youtu.be/Pbsn-6JEE2s?si=7pdAv5LU9GiBx7aN)\n\nFor now it's working but for how long?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qgxulm/would_anthropic_block_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0fwcc2",
          "author": "Kholtien",
          "text": "I donâ€™t think they could. You can use Claude code (the application, not the service Claude) with just about any LLM these days. Itâ€™s all local on your computer.",
          "score": 15,
          "created_utc": "2026-01-19 07:49:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0i42ty",
              "author": "sinan_online",
              "text": "Hey, so I got a question to ask. If you are doing this, what are you doing, exactly, and whatâ€™s the VRAM? \n\nMy understanding is that the Claude endpoint does quite a bit of orchestration, not just coding, when used under GitHub CoPilot. Also the model they are using seems to require more VRAM than I have locally (12GB) to respond in a reasonable amount of time.",
              "score": 1,
              "created_utc": "2026-01-19 16:43:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0ktgnp",
              "author": "Big-Masterpiece-9581",
              "text": "It works but only with hacks and youâ€™re at Claudeâ€™s mercy anytime they want to throw a curveball and break everything else. I would just use opencode.",
              "score": 1,
              "created_utc": "2026-01-20 00:29:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0l87yb",
                  "author": "StardockEngineer",
                  "text": "They're not hacks.  It's just pointing the base url to another server.  It's a documented feature in Claude Code.",
                  "score": 3,
                  "created_utc": "2026-01-20 01:50:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0jhwpr",
          "author": "ShadoWolf",
          "text": "The Claude Code situation isnâ€™t about Anthropicâ€™s standard API.  \nWhat was happening is that people were signing up for Claude Code, then using tools like OpenCode to redirect it and reuse the Claude Code credential.  \nThat credential wasnâ€™t a normal Anthropic API key, it was tied to the Claude Code plan itself, which is priced as a product SKU, not as metered API access. Anthropic will happy take your money for API usage through their normal API plans. They just donâ€™t want the Claude Code plan being used as a cheap substitute for the standard API pricing",
          "score": 3,
          "created_utc": "2026-01-19 20:29:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g5sld",
          "author": "gabrielxdesign",
          "text": "Why would they block it? I've been using Open WebUI for years, which uses Ollama for Open Source LLM, and I use it together with paid APIs. I don't see the problem with using a platform that can handle local and remote.",
          "score": 6,
          "created_utc": "2026-01-19 09:16:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0gsxig",
          "author": "UnbeliebteMeinung",
          "text": "They blocked the other way because of their costs. Why would they block it when they have no costs?",
          "score": 2,
          "created_utc": "2026-01-19 12:38:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0it8hw",
          "author": "croninsiglos",
          "text": "They are pretty confident in their modelâ€™s ability over open source models and the CEO seems to think youâ€™ll never be hosting the more performant open models locally. \n\nSo does it matter?\n\nIâ€™m sure theyâ€™d love people to standardize on their APO vs openaiâ€™s API.",
          "score": 1,
          "created_utc": "2026-01-19 18:36:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0osgxo",
              "author": "Practical-Plan-2560",
              "text": "Under that logic, they should open source Claude Code. If the model is the magic, why keep tools to use the model so closed?",
              "score": 1,
              "created_utc": "2026-01-20 16:14:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0izx4z",
          "author": "roger_ducky",
          "text": "Itâ€™s the same as the OpenAI API compatibility.\n\nAs long as anthropic doesnâ€™t suddenly make major breaking changes, itâ€™d work.\n\nThis isnâ€™t the same thing anthropic banned recentlyâ€” thatâ€™s piggybacking on the actual Claude using a personâ€™s monthly paid subscription, rather than APIs.\n\nThis is using Claude code the CLI with your local models.",
          "score": 1,
          "created_utc": "2026-01-19 19:06:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ju0js",
          "author": "Clay_Ferguson",
          "text": "Anthropic realized that if OpenCode and/or LangChain OpenWork open source solutions are what people start using for local LLMs, then it gets people out of the Claude Code way of doing things. So there was no disadvantage to letting Claude Code be used for local inference, but there was an advantage, which is to make Claude Code be what people want to use regardless of whether they're using local or cloud-based LLMs.",
          "score": 1,
          "created_utc": "2026-01-19 21:26:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nhtd1",
          "author": "Ok_Hospital_5265",
          "text": "And just as I installed Crushâ€¦ ðŸ¤¦ðŸ»â€â™‚ï¸",
          "score": 1,
          "created_utc": "2026-01-20 11:56:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0p7oio",
              "author": "seangalie",
              "text": "Sometimes it's useful to have Opencode, Crush, Codex, and Claude all co-existing.  Good way to jump between models and such without having to play around with configs everytime.  I use Ollama's cloud models in Opencode, and local models in Crush (since Crush supports a big task/little task setup).",
              "score": 1,
              "created_utc": "2026-01-20 17:25:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0m8y9k",
          "author": "stocky789",
          "text": "Would anyone want to? \nClaude code isn't really anything to write home about",
          "score": 0,
          "created_utc": "2026-01-20 05:26:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjtnqm",
      "title": "How to implement a RAG (Retrieval Augmented Generation) on your laptop",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qjtnqm/how_to_implement_a_rag_retrieval_augmented/",
      "author": "Unique_Winner_5927",
      "created_utc": "2026-01-22 12:48:36",
      "score": 23,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "This guide explains how to implement a RAG (Retrieval Augmented Generation) on your laptop.\n\nhttps://preview.redd.it/ftsddeqtcweg1.png?width=2184&format=png&auto=webp&s=640e3013e9113c3c7780a88b39d6992cd34b8d6f\n\nWith n8n, Ollama and Qdrant (with Docker).\n\n[https://github.com/ThomasPlantain/n8n](https://github.com/ThomasPlantain/n8n)\n\nI put a lot of screenshots to explain how to configure each component.\n\n\\#Ollama #n8n #Qdrant #dataSovereignty #embeddedAI",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qjtnqm/how_to_implement_a_rag_retrieval_augmented/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qijhth",
      "title": "Weekend Project: An Open-Source Claude Cowork That Can Handle Skills",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qijhth/weekend_project_an_opensource_claude_cowork_that/",
      "author": "Frequent_Cash2598",
      "created_utc": "2026-01-21 00:59:50",
      "score": 19,
      "num_comments": 5,
      "upvote_ratio": 0.96,
      "text": "I spent last weekend building something I had been thinking about for a while. Claude Cowork is great, but I wanted an open-source, lightweight version that could run with any model, so I created Open Cowork.\n\nIt's written entirely in Rust, which I had never used before. Starting from scratch meant no heavy dependencies, no Python bloat, and no reliance on existing agent SDKs. Just a tiny, fast binary that works anywhere.\n\nSecurity was a big concern since the agents can execute code. Open Cowork handles this by running tasks inside temporary Docker containers. Everything stays isolated, but you can still experiment freely.\n\nYou can plug in any model you want. OpenAI, Anthropic, or even fully offline LLMs through Ollama are all supported. You keep full control over your API keys and your data.\n\nIt already comes with built-in skills for handling documents like PDFs and Excel files. I was surprised by how useful it became right away.\n\nThe development experience was wild. An AI agent helped me build a secure, open-source version of itself, and I learned Rust along the way. It was one of those projects where everything just clicked together in a weekend.\n\nThe code is live on GitHub: [https://github.com/kuse-ai/kuse\\_cowork](https://github.com/kuse-ai/kuse_cowork) . It's still early, but I'd love to hear feedback from anyone who wants to try it out or contribute.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qijhth/weekend_project_an_opensource_claude_cowork_that/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0rvzc8",
          "author": "Available-Craft-5795",
          "text": "Im pretty sure this is AI.   \nLoads of emojis in readme  \nLoads of commments in code that no sane dev would add  \nDone in a weekend? How?",
          "score": 2,
          "created_utc": "2026-01-21 01:13:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ss3zn",
              "author": "Frequent_Cash2598",
              "text": "Yes, it is Claude Code making a Rust alternative of itself. \n\nSo you are right. :)",
              "score": 5,
              "created_utc": "2026-01-21 04:23:15",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0s2p5t",
              "author": "gingeropolous",
              "text": "Sounds like they used Claude code.",
              "score": 2,
              "created_utc": "2026-01-21 01:51:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0sepez",
          "author": "Efficient_Click_2689",
          "text": "cool project bro, would love to try out!",
          "score": 1,
          "created_utc": "2026-01-21 03:00:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0w48w9",
          "author": "wombweed",
          "text": "OS compatibility? If it runs on Linux can you include a Nix flake?",
          "score": 1,
          "created_utc": "2026-01-21 17:40:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlvud9",
      "title": "Mac Studio as host for Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qlvud9/mac_studio_as_host_for_ollama/",
      "author": "amgsus",
      "created_utc": "2026-01-24 19:10:24",
      "score": 18,
      "num_comments": 26,
      "upvote_ratio": 0.8,
      "text": "Hello,\n\nIâ€™m wondering if it worth buying Mac Studio M4 Max (64 GB) for hosting Ollama. Does anyone have experience with this box? Or better to build a cluster of GPUs like RTX 3090, etc.?\n\nPrimarily, I will be using LLMs for coding. Rarely, for media content generation.\n\nKind regards",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qlvud9/mac_studio_as_host_for_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1h9z86",
          "author": "tom-mart",
          "text": "Define \"better\". What do you carr about? Raw power? 2 z 3090 will blow 64GB Mac out of the water. Ease of use? Mac will be miles easier to set up than multiple GPU's. Running cost? Mac wins again. Ability to fine tune, RTX takes the lead.",
          "score": 9,
          "created_utc": "2026-01-24 19:21:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hdbat",
              "author": "ZeroSkribe",
              "text": "There is no setup for multiple GPU's, it just works. Ollama has had this working out of the box for a while.",
              "score": 4,
              "created_utc": "2026-01-24 19:36:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1hc986",
          "author": "Whospakka",
          "text": "If I say Iâ€™m running an Ollama on a Mac Mini M4 base config and itâ€™s working awesome for an 8B model (plus a small embedding model at the same time), what do you think? ðŸ˜",
          "score": 3,
          "created_utc": "2026-01-24 19:31:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hewvv",
              "author": "CMPUTX486",
              "text": "Same here.. I did a bit more with Gemma 3 12b",
              "score": 4,
              "created_utc": "2026-01-24 19:43:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1kqfd0",
              "author": "rorowhat",
              "text": "even my phone can run a 8B models these days at good speeds, its not impressive.",
              "score": 2,
              "created_utc": "2026-01-25 06:36:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1jl52l",
          "author": "Popular-Diamond-9928",
          "text": "For folks using the Mac studios what is your time to first token? Iâ€™m curious at the optimizations you all are making. I am using Ollama and Iâ€™ve seen posts below about llama.cpp which Iâ€™m seriously considering now lol. \n\nFor me, I have a classifier that classifies the query, determines if rag is needed, conversation is needed, and even additional context (I donâ€™t use tool calling because itâ€™s just faster for the routing to identify entities needed and then using sql to fetch)\n\nBut even when Iâ€™m controlling the token consumption Iâ€™m getting about 30seconds for my time to first token. \n\nIâ€™m using Qwen2.5 8B or 7B I donâ€™t remember \nAnd a small embedding model.",
          "score": 1,
          "created_utc": "2026-01-25 02:17:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jl7pl",
              "author": "Popular-Diamond-9928",
              "text": "Iâ€™m running my setup on a Mac mini base which has been amazing to setup.",
              "score": 1,
              "created_utc": "2026-01-25 02:17:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1kon7l",
          "author": "mhjor70",
          "text": "Yes i use it with a 32GB M3 and it works. It really depends on what you want to do with it and what models you want to load.",
          "score": 1,
          "created_utc": "2026-01-25 06:22:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kpcsx",
          "author": "Odd_Butterfly6003",
          "text": "In my case, i used 256gb ram m3 ultra.\nQwen3 30B took about 5 second per one request, but Qwen3 VL 235B model took 20 seconds per one request.",
          "score": 1,
          "created_utc": "2026-01-25 06:28:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kq8yl",
          "author": "rorowhat",
          "text": "Stick with Nvidia, way higher memory BW and compute power. Your TTFT and TS will be 2x of the Mac.\n\nM4 Max: Up to \\~546 GB/s  \nRTX 3090: \\~936 GB/s",
          "score": 1,
          "created_utc": "2026-01-25 06:35:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lyjy1",
          "author": "Weak_Ad9730",
          "text": "Have a m3u using Mlx-vllm and reality impressed of the Performance the Switch from lmstudio to vllm was a Hugh Jump in Processing time and Speed. I use my Studio in an Agent Zero setup. Realy recommend those Apple Silikon for llm work. My Go to Models are qwen3-vl-32b , got-oss-120b and minimax-m2.1",
          "score": 1,
          "created_utc": "2026-01-25 12:50:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nyyxf",
          "author": "bishopLucas",
          "text": "the best mac studio you can get is \"Apple M4Â Max chip with 16â€‘core CPU, 40â€‘core GPU, 16â€‘core NeuralÂ Engine\", mine has 64GB/1TB.\n\nIt is like MAGIC.  I came from a surface book laptop and didn't really appreciate how contained I was.\n\nI may create a cluster when ever the Ultra chip is released.\n\nEven at 64GB/1TB so many more possibilities are open to you.",
          "score": 1,
          "created_utc": "2026-01-25 18:36:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1hgpqy",
          "author": "Le_mele",
          "text": "I highly recommend it, it works great even for models up to 32b",
          "score": 1,
          "created_utc": "2026-01-24 19:51:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1hd3cr",
          "author": "ZeroSkribe",
          "text": "Naw, you really need to get full GPU vram coverage, a cluster of 3050's would be way faster",
          "score": -1,
          "created_utc": "2026-01-24 19:35:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1hlc92",
          "author": "g_rich",
          "text": "I have a Mac Studio M4 Max, 64GB, 1TB which can easily run Devstral-Small-2-24B, Qwen3-Coder-30B and GLM-4.7-Flash; performance is acceptable but by no means fast. \n\nA cluster of RTX 3090â€™s will be faster and a better option if you intend on doing anything requiring CUDA. However that doesnâ€™t mean itâ€™s the better option for everyone, heat, power and likely cost will be higher made especially worse with the current GPU and RAM prices. \n\nAlso consider that youâ€™ll need at least 256-512GB of memory to get anywhere near Claude, Gemini or ChatGPT. So while you will get useful performance from the local models with 64GB and learn a lot youâ€™ll still be dependent on cloud hosted foundation models to do any serious work. \n\nOne other thing to consider the Mac Studio supports RDMA over Thunderbolt 5 which offers an interesting option to cluster Macâ€™s to run larger models; so itâ€™s possible to start with one Mac Studio and add another down the road. There are a few videos on YouTube with Jeff Geerling doing a great job of demoing this capability.\n\nLastly donâ€™t use Ollama, llama.cpp and AI Studio are both much better options.",
          "score": 0,
          "created_utc": "2026-01-24 20:13:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hybyd",
              "author": "BoostedHemi73",
              "text": "\\> Lastly donâ€™t use Ollama, llama.cpp and AI Studio are both much better options.\n\nCan you say more on this? I've been so impressed with ollama on my M4 Max.",
              "score": 1,
              "created_utc": "2026-01-24 21:14:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1i5bwx",
                  "author": "g_rich",
                  "text": "The software itself is a ripoff of llama.cpp, they are not good stewards of open source and they are trying to lock users into their ecosystem. They have also had a few security vulnerabilities over the last few years and have shown little alarm or urgency to address them when they are discovered. \n\nllama.cpp is better but more complex, although not difficult to get up and running; especially when peered with llama-swap. It will also give you better performance and is overall a much better open source project. \n\nLM Studio is the easy choice if you are just looking for a quick and easy way to run local models.",
                  "score": -1,
                  "created_utc": "2026-01-24 21:47:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1i0ofj",
          "author": "Pale_Reputation_511",
          "text": "For Mac use mlx better performance and less ram usage",
          "score": 0,
          "created_utc": "2026-01-24 21:25:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ie2hn",
          "author": "No_Entertainer6253",
          "text": "Just dont",
          "score": 0,
          "created_utc": "2026-01-24 22:29:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ieggf",
              "author": "No_Entertainer6253",
              "text": "I bought 192gb studio, still paying 200$/ month to use cc",
              "score": 3,
              "created_utc": "2026-01-24 22:31:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1irn5z",
                  "author": "not-really-adam",
                  "text": "Have the 256GB M3 Ultra, and still pay $200/mo to claude. The local stuff just isnâ€™t as good or as fast as the cloud. For privacy of personal transcriptions and other things you donâ€™t want to leak to the cloud, it is really nice though. And, I think we are getting closer to being able to use Opus for planning and thinking and local models for research and execution which means either lower Claude bill, or ability to do loops for bug finding and/or optimizations.",
                  "score": 2,
                  "created_utc": "2026-01-24 23:39:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1iouh2",
                  "author": "AstroZombie138",
                  "text": "Have you tried GLM 4.7?  It works well for me.",
                  "score": 1,
                  "created_utc": "2026-01-24 23:24:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1iz9vz",
          "author": "rockinyp",
          "text": "Memory is more important than the CPU. Find a used M1 Mac Studio with more memory so you can run larger models. Running larger models means you'll often get better responses, which means you're more likely to use it. I have an M1 Ultra with 128 GB of memory running 30b+ models just fine for multiple users via Open WebUI.",
          "score": 0,
          "created_utc": "2026-01-25 00:18:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkcg6a",
      "title": "Boom, the greatest repo yet from Lucas Valbuena ( x1xhlol)",
      "subreddit": "ollama",
      "url": "https://i.redd.it/dspr44juxzeg1.jpeg",
      "author": "OriginalZebraPoo",
      "created_utc": "2026-01-23 00:57:26",
      "score": 16,
      "num_comments": 0,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qkcg6a/boom_the_greatest_repo_yet_from_lucas_valbuena/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qlssc0",
      "title": "HashIndex: An alternative to a page that doesn't require RAG but can still perform indexing well.",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qlssc0/hashindex_an_alternative_to_a_page_that_doesnt/",
      "author": "jasonhon2013",
      "created_utc": "2026-01-24 17:19:08",
      "score": 15,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "The Pardus AI team has decided to open source our memory system, which is similar to PageIndex. However, instead of using a B+ tree, we use a hash map to handle data. This feature allows you to parse the document only once, while achieving retrieval performance on par with PageIndex and significantly better than embedding vector search. It also supports Ollama. Give it a try and consider implementing it in your system â€” you might like it! Give us a star maybe hahahaha\n\n[ https://github.com/JasonHonKL/HashIndex/tree/main ](https://github.com/JasonHonKL/HashIndex/tree/main)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qlssc0/hashindex_an_alternative_to_a_page_that_doesnt/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1gncij",
          "author": "immediate_a982",
          "text": "Your sample code does to show Ollama usage",
          "score": 2,
          "created_utc": "2026-01-24 17:44:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1gnq4w",
              "author": "jasonhon2013",
              "text": "Hi all you need is to change the base url in the .env file ! Hope you enjoy it",
              "score": 2,
              "created_utc": "2026-01-24 17:46:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ha2to",
          "author": "zair",
          "text": "Does this have value for other structured document types, eg spreadsheets? And is the benefit mainly for very long documents or also large numbers of short documents?",
          "score": 1,
          "created_utc": "2026-01-24 19:22:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jv624",
              "author": "jasonhon2013",
              "text": "Ah itâ€™s mainly focusing on long context documents",
              "score": 2,
              "created_utc": "2026-01-25 03:12:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qj02gw",
      "title": "New Rules for ollama cloud",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qj02gw/new_rules_for_ollama_cloud/",
      "author": "killing_daisy",
      "created_utc": "2026-01-21 14:57:44",
      "score": 14,
      "num_comments": 6,
      "upvote_ratio": 0.89,
      "text": "so i've just seen this:\n\nPro:  \nEverything in Free, plus:\n\n* Run 3 cloud models at a time\n* Faster responses from cloud hardware\n* Larger models for challenging tasks\n* 3 private models\n* 3 collaborators per model\n\nits been a lot slower for usage within zed for me the last hours - does anyone have more information whats happening to the pro subscription? it seems like the changes in the subscription are random and without any notice to users? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qj02gw/new_rules_for_ollama_cloud/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0vg4mk",
          "author": "jmorganca",
          "text": "Hi there. I work on Ollama. No new restrictions on the cloud model usage with this change. We actually increased usage amounts on each plan on Monday and will share more about that this week. Our goal is to make this a the best subscription for using open models with your favorite tools as we add more model support, better performance and reliability.\n\nHappy to answer any questions and if you hit any issues or limits with the plans let me know (email is jeff at ollama.com)",
          "score": 15,
          "created_utc": "2026-01-21 15:52:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vraab",
              "author": "jmorganca",
              "text": "Also, OP, let me know if you're still seeing any slowdown (and for which models). We've been working on improving performance and capacity a lot in the last few weeks and will keep doing so. (Feel free to DM/email me)",
              "score": 8,
              "created_utc": "2026-01-21 16:42:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0w2o4h",
              "author": "killing_daisy",
              "text": "hi, it a bit worrying, that the contracts are sortof changing without notice to active users, i've only seen this because i was investigating the slow down - i'll have a try today at home, maybe it was only our network at the company.  \nedit: forgot to thank you for a response :) - so thanks :D",
              "score": 5,
              "created_utc": "2026-01-21 17:33:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o10on2k",
                  "author": "Ryanmonroe82",
                  "text": "This isn't the first time",
                  "score": 1,
                  "created_utc": "2026-01-22 09:29:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o10ol2i",
              "author": "Ryanmonroe82",
              "text": "Why does the APi no longer work when using synthetic data gen tools? Kiln AI and Easy Dataset both return errors now using cloud API and it was not like this when I first subscribed. All cloud models worked for this now none do.",
              "score": 1,
              "created_utc": "2026-01-22 09:28:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1hgeju",
              "author": "Mulan20",
              "text": "Can you share how many tokens per day can generate with big models and what is the limit? \nLet's say 10 milion tokens per day.",
              "score": 1,
              "created_utc": "2026-01-24 19:50:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qhr198",
      "title": "GLM 4.7 is apparently almost ready on Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/",
      "author": "Savantskie1",
      "created_utc": "2026-01-20 04:18:14",
      "score": 11,
      "num_comments": 19,
      "upvote_ratio": 0.87,
      "text": "It's listed, just not downloadable yet. Trying in WebOllama, and in CLI gives weird excuses\n\nhttps://preview.redd.it/96ly2bgckfeg1.png?width=1723&format=png&auto=webp&s=d8bfc2386dd789ef4f28ff0516de4893bf5c6772",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o0u11zy",
          "author": "Resonant_Jones",
          "text": "Iâ€™m running it on apple silicon m4 32gb\n\nGLM-4.7-flash:q4_K_M\n\nItâ€™s an incredible model. Iâ€™ve been blown away so far.",
          "score": 2,
          "created_utc": "2026-01-21 10:47:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17clmx",
              "author": "Forbidden-era",
              "text": "how you running it exactly?",
              "score": 1,
              "created_utc": "2026-01-23 08:13:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ojnqc",
          "author": "beefgroin",
          "text": "thanks, running glm-4.7-flash:q8\\_0, so far so good",
          "score": 2,
          "created_utc": "2026-01-20 15:33:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0pkcjg",
              "author": "thexdroid",
              "text": "What is your GPU?",
              "score": 1,
              "created_utc": "2026-01-20 18:22:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0plxu3",
                  "author": "beefgroin",
                  "text": "I have 4 5060 16gb, I moved to q4k\\_m because the model consumes a lot of vram when the context is increasing, with 32k context it's taking 50gb of vram.",
                  "score": 1,
                  "created_utc": "2026-01-20 18:30:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0pm8wh",
              "author": "beefgroin",
              "text": "I liked it, the thinking process is very structured and very different from other thinking models. I guess something is still buggy cause every long conversation ends up in an infinite thinking loop",
              "score": 1,
              "created_utc": "2026-01-20 18:31:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0oe8av",
          "author": "WaitformeBumblebee",
          "text": "this one? updated 19 hours ago:\n\nhttps://ollama.com/library/glm-4.7-flash\n\n\nHmm, glm-4.7-flash:latest and glm-4.7-flash:q4_K_M are the same size, are they the same?\n\n\nCan't pull the new model, it requires Ollama update, even right after updating...\n\nollama --version\nollama version is 0.14.2\n\n\nedit: download latest release from github\nhttps://github.com/ollama/ollama/releases\n\nunpack, sudo cp lib files to /usr/local/lib/ollama/ and bin file to /usr/local/bin/   \n\npulling glm4.7 now\n\nedit2: quite fast on laptop 3060 6GB VRAM + 32GB RAM\n\nNAME                    ID              SIZE     PROCESSOR          CONTEXT              \nglm-4.7-flash:latest    ff14144f31df    23 GB    77%/23% CPU/GPU    4096",
          "score": 1,
          "created_utc": "2026-01-20 15:07:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0x6083",
          "author": "Zyj",
          "text": "I just pulled an Ollama container update, now I can download GLM 4.7 flash q8\\_0 with it. It will run on 2x 3090.",
          "score": 1,
          "created_utc": "2026-01-21 20:28:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0y8f7h",
          "author": "thexdroid",
          "text": "Odd, my system ram is 32GB and 16GB for GPU (4070ti). However I can't run any GLM model due memory... The glm-4.7-flash:q4\\_K\\_M model is actually 18GB but it requires 128GiB, I have downloaded and ran models like qwen3:30b-a3b-instruct-2507-q4\\_K\\_M (same 18GB) and even more. Do I would need more RAM? With current prices? lol, thanks.",
          "score": 1,
          "created_utc": "2026-01-21 23:32:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zjcs1",
              "author": "Savantskie1",
              "text": "I can run this exact model as of today with an RX 6800 16GB and a 7900 XT 20GB and 48GB of ram perfectly fine with room to spare. Itâ€™s not even using all the vram yet. WTF are you on about? 4.7 flash is a 30b moe model.",
              "score": 1,
              "created_utc": "2026-01-22 03:57:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o10pq8q",
                  "author": "thexdroid",
                  "text": "I am saying the file size is 18GB and 30B parameters, for comparison, so qwen and the GLM are same in file sizes and parameters, qwen loads and but with glm Ollama fails due insufficient memory error. Maybe I should download it again, not sure.",
                  "score": 1,
                  "created_utc": "2026-01-22 09:40:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0zwrta",
              "author": "_twrecks_",
              "text": "I see something similar, with just 8K context the glm-4.7-flash:q4\\_K\\_M model pulled today from Ollama overlfows my 3090 by like 30%. It does work though. Its like it turned of kv cache and flash attention? I do see the model file has \"RENDERER glm-4.7\" and \"PARSER glm-4.7\" in it.\n\nThe 2 GGUF versions on HF I tried uses way less memory, but also starts babbling to itself w/o end. I tried the recommend settings.\n\nNot seeing this model as even useful ATM.",
              "score": 1,
              "created_utc": "2026-01-22 05:27:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o19zgy8",
                  "author": "thexdroid",
                  "text": "Someone in Ollama's Discord told me that it won't fit right now in the 16GB due current context size and they're working in another version",
                  "score": 1,
                  "created_utc": "2026-01-23 17:52:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0m0ppd",
          "author": "iansltx_",
          "text": "You can't use the GGUF. You \\*can\\* use the ones from the website if you have the latest ollama build (on GitHub, see that link).\n\nOne caveat: it runs slow on a unified-memory Mac for some reason, and splits 80/20 CPU/GPU. Not sure what's going on there. My Intel Mac runs faster.",
          "score": 1,
          "created_utc": "2026-01-20 04:30:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zjrel",
              "author": "Savantskie1",
              "text": "No shit ollama canâ€™t use the gguf version. It never was going to be able to. LM studio will though.",
              "score": 1,
              "created_utc": "2026-01-22 03:59:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qkl6rr",
      "title": "GitHub - FlorinAndrei/pipe-llama: Put an LLM in your shell scripts and command-line pipelines. Dead simple.",
      "subreddit": "ollama",
      "url": "https://github.com/FlorinAndrei/pipe-llama",
      "author": "florinandrei",
      "created_utc": "2026-01-23 08:14:30",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 0.83,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qkl6rr/github_florinandreipipellama_put_an_llm_in_your/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o196rdm",
          "author": "smith288",
          "text": "Pretty slick. Could you throw in an ascii progress while it makes the request?",
          "score": 3,
          "created_utc": "2026-01-23 15:41:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dob7e",
              "author": "florinandrei",
              "text": "It's meant to use small, fast models, doing relatively simple jobs with short or very short prompts. It's not meant for inference jobs that take a long time. Thinking is disabled by default. It has a warm-up mode. It has a one-line mode. I did everything I could think of to make it go faster.\n\nYou can always put `pv` in front of it in the pipeline, if you need a progress bar for a large file where many inference requests are issued to the endpoint for that one file. I'll add it to the examples.",
              "score": 2,
              "created_utc": "2026-01-24 05:40:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1eyx9t",
                  "author": "smith288",
                  "text": "All fair. But it still has delay especially when my ollama server is on another computer. Just spitting out an idea.",
                  "score": 1,
                  "created_utc": "2026-01-24 12:29:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1bphjy",
              "author": "1800not4you",
              "text": "+1 to this",
              "score": 1,
              "created_utc": "2026-01-23 22:41:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qmmp22",
      "title": "Claude Code stuck on <function=TaskList> when using Ollama + Qwen3-Coder",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qmmp22/claude_code_stuck_on_functiontasklist_when_using/",
      "author": "Healthy-Laugh-6745",
      "created_utc": "2026-01-25 15:52:04",
      "score": 11,
      "num_comments": 10,
      "upvote_ratio": 0.92,
      "text": "Hi everyone,\n\nI'm struggling to get Claude Code working with Ollama on my Mac M4 Max (48GB RAM). I strictly followed the official Ollama integration guide (https://docs.ollama.com/integrations/claude-code), but I'm stuck in a loop.\n\nEvery time I ask the model to perform a file-based task (e.g., \"create a txt file\"), the process hangs indefinitely.\n\nThe model acknowledges the request.\n\nIt outputs: â¯ <function=TaskList> âº\n\nNothing happens after that. No file is created, and the terminal just sits there with the \"active\" dot.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qmmp22/claude_code_stuck_on_functiontasklist_when_using/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o1ngiam",
          "author": "Outrageous_Rub_6527",
          "text": "Hey! Bump up your context length before running claude code with Ollama - https://docs.ollama.com/context-length",
          "score": 3,
          "created_utc": "2026-01-25 17:19:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1oy32w",
              "author": "jmorganca",
              "text": "This. Sorry it's not obvious right now - we're working on improving this so context length size automatically grows (up to an acceptable amount on your hardware)",
              "score": 1,
              "created_utc": "2026-01-25 21:09:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1nnebn",
              "author": "Healthy-Laugh-6745",
              "text": "correct! thanks a lot at least 64k. sorry i'm a newbie :)",
              "score": 1,
              "created_utc": "2026-01-25 17:49:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1n4fds",
          "author": "paq85",
          "text": "I would be really surprised to see Claude Code work with model like Qwen3 Coder 30B... but, please let me know if you manage to make it work... from my experience those local models are way too limited.",
          "score": 2,
          "created_utc": "2026-01-25 16:27:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1n62m3",
              "author": "Healthy-Laugh-6745",
              "text": "Thanks! According to the Ollama docs, open models can be used with Claude Code via Ollamaâ€™s Anthropic-compatible API (for example, glm-4.7, qwen3-coder, or gpt-oss).\nAre you saying I should try a different model?",
              "score": 1,
              "created_utc": "2026-01-25 16:34:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1npe3p",
                  "author": "paq85",
                  "text": "I'm afraid there's no local model up to 30B that's good enought for tools like Claude Code, Cline etc... [Has anyone got GLM 4.7 flash to not be shit? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/)\n\nHere's GLM 4.7 Flash with Cline (VS Code) struggling to provide a simple answer... \n\nhttps://preview.redd.it/wb9yr8j3bjfg1.png?width=667&format=png&auto=webp&s=0195981d970e6235018e7efe9ee4028f89c0718e",
                  "score": 1,
                  "created_utc": "2026-01-25 17:57:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1p4kil",
              "author": "nunodonato",
              "text": "I've used it with a 4B model :D",
              "score": 1,
              "created_utc": "2026-01-25 21:37:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1n6qk2",
          "author": "960be6dde311",
          "text": "Have you tried using OpenCode CLI instead? I love the concept of running AI models locally, and have done a fair amount of experimentation, but they often have \"issues\" ranging from:\n\n* Hanging on generating responses\n* Infinite MCP tool calling loops\n* Spending a disproportionate amount of time \"thinking\" (reasoning)\n* Garbling tool calls or responses\n\nYou really have to find a client tool that has been **thoroughly tested** with the specific model you're using. Without adequate testing and bug fixes, there's a reasonably high probability you'll run into some kind of issue.\n\nI'm sure we will see these kinds of issues get worked out over time, but for now, you'll need to spend time with trial and error to see what works and what doesn't.",
          "score": 1,
          "created_utc": "2026-01-25 16:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1pgtqb",
          "author": "Savantskie1",
          "text": "This is a bit of a noobish question but we have to establish competency. Have you given it the tools youâ€™re trying to get it to use? Ollama doesnâ€™t automatically make it an AI that can do everything",
          "score": 1,
          "created_utc": "2026-01-25 22:31:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qdsfu",
          "author": "Embarrassed_Sun_7807",
          "text": "Post settings or we can't help really.Â ",
          "score": 1,
          "created_utc": "2026-01-26 01:07:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}