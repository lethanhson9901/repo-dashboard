{
  "metadata": {
    "last_updated": "2026-02-10 17:16:23",
    "time_filter": "week",
    "subreddit": "ollama",
    "total_items": 20,
    "total_comments": 124,
    "file_size_bytes": 138004
  },
  "items": [
    {
      "id": "1qyghp3",
      "title": "Ollie | A Friendly, Local-First AI Companion for Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/",
      "author": "MoonXPlayer",
      "created_utc": "2026-02-07 15:21:30",
      "score": 82,
      "num_comments": 35,
      "upvote_ratio": 0.95,
      "text": "Hi everyone,\n\nIâ€™m sharing **Ollie**, a Linux-native, local-first personal AI assistant built on top of **Ollama**.\n\nhttps://preview.redd.it/fh544zreb3ig1.png?width=1682&format=png&auto=webp&s=23c108dff77d288035dbc0d1dff64503bcd370dd\n\nOllie runs entirely on your machine â€” no cloud (I'm considering optional cloud APIs like Anthropic), no tracking, no CLI. It offers a polished desktop experience for chatting with local LLMs, managing models, analyzing files and images, and monitoring system usage in real time.\n\n**Highlights**\n\n* Clean chat UI with full Markdown, code, tables, and math\n* Built-in model management (pull / delete / switch)\n* Vision + PDF / text file analysis (drag & drop)\n* AppImage distribution (download & run)\n\nBuilt with **Tauri v2 (Rust) + React + TypeScript**.\n\nFeedback and technical criticism are very welcome.\n\nGitHub: [https://github.com/MedGm/Ollie](https://github.com/MedGm/Ollie)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o43q6zx",
          "author": "cuberhino",
          "text": "What are some use cases for this vs say just using lmstudio",
          "score": 6,
          "created_utc": "2026-02-07 16:33:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43uqz5",
              "author": "MoonXPlayer",
              "text": "In the base, they can look the same, even if itâ€™s already clear that LM Studio is better because:  \n\\- LM Studio is a full LLM platform with SDKs, CLI, engines, and dev tools.  \n\\- Ollie isâ€¦ a different, Linux-native, local-first personal AI companion.\n\nI cannot say there are things where Iâ€™m better yet, but Iâ€™m still trying to make things better, especially in terms of performance *â€” thatâ€™s why I chose Rust â€”* and Iâ€™m still brainstorming some unique features to integrate.",
              "score": 2,
              "created_utc": "2026-02-07 16:56:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o45ivgc",
                  "author": "cuberhino",
                  "text": "Yeah Iâ€™m really just trying to identify some things I would do with it before diving in with the play time on it",
                  "score": 1,
                  "created_utc": "2026-02-07 22:05:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o45lcji",
                  "author": "superdariom",
                  "text": "I don't really think rust is going to make things much faster as your bottleneck will be the inference?",
                  "score": 1,
                  "created_utc": "2026-02-07 22:19:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o44zdjz",
              "author": "germanpickles",
              "text": "The LM Studio desktop app is not open source",
              "score": 1,
              "created_utc": "2026-02-07 20:20:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o45is4v",
                  "author": "cuberhino",
                  "text": "Yeah but you can use open source models with it. Iâ€™m asking for specific use cases for this project",
                  "score": 2,
                  "created_utc": "2026-02-07 22:05:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o43c9md",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 5,
          "created_utc": "2026-02-07 15:25:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43cfou",
              "author": "MoonXPlayer",
              "text": "Thanks a lot, I really appreciate it!",
              "score": 1,
              "created_utc": "2026-02-07 15:26:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43jki2",
          "author": "valosius",
          "text": "Hi, sinple not found on github:  \n\nCan you send the exect link to the AppImage ?\n\nOlav\n\n\\#### snip ##\n\nwget -O Ollie.AppImage [https://github.com/MedGm/OllamaGUI/releases/latest/download/Ollie\\_\\*\\_amd64.AppImage](https://github.com/MedGm/OllamaGUI/releases/latest/download/Ollie_*_amd64.AppImage)\n\n...\n\nPlatz: [https://github.com/MedGm/Ollie/releases/download/v0.2.1/Ollie\\_\\*\\_amd64.AppImage](https://github.com/MedGm/Ollie/releases/download/v0.2.1/Ollie_*_amd64.AppImage) \\[folgend\n\nWiederverwendung der bestehenden Verbindung zu github.com:443.\n\nHTTP-Anforderung gesendet, auf Antwort wird gewartet â€¦ 404 Not Found\n\n2026-02-07 16:54:43 FEHLER 404: Not Found.\n\n\\## snip ##",
          "score": 3,
          "created_utc": "2026-02-07 16:01:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43kyk1",
              "author": "MoonXPlayer",
              "text": "Thank you for pointing that out\n\nthe wildcard `*` doesnâ€™t work in direct GitHub URLs. The exact AppImage link for the latest release is:  \n[https://github.com/MedGm/Ollie/releases/download/v0.2.1/Ollie\\_0.2.1\\_amd64.AppImage]()\n\nThen run:\n\n     chmod +x Ollie_0.2.1_amd64.AppImage\n    ./Ollie_0.2.1_amd64.AppImage\n\nIâ€™ll update the README to avoid the confusion. Thanks!!",
              "score": 1,
              "created_utc": "2026-02-07 16:07:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o49xhpu",
                  "author": "valosius",
                  "text": "Unfortunately, there are too many dependencies on the new glibc library, but I'm not going to update my Ubuntu system just for a single client. I can simply test it in a virtual machine.",
                  "score": 1,
                  "created_utc": "2026-02-08 16:36:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4590x7",
          "author": "Noiselexer",
          "text": "Tauri but no windows build?",
          "score": 3,
          "created_utc": "2026-02-07 21:13:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49yryd",
              "author": "MoonXPlayer",
              "text": "I will surely work on a Windows build version near in the future. I just wanted to start by focusing on a Linux native build first, since Linux is my main machine",
              "score": 2,
              "created_utc": "2026-02-08 16:42:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43jb6h",
          "author": "Prestigious_Ebb_5131",
          "text": "Looks perfekt! Does MCP integration support HTTP streaming and Bearer token authorization?",
          "score": 2,
          "created_utc": "2026-02-07 15:59:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43jyad",
              "author": "MoonXPlayer",
              "text": "Thanks!!  \nMCP support is planned but not there yet. my idea is to support HTTP-based MCP servers with streaming, and yes, Bearer token auth is part of that plan.",
              "score": 2,
              "created_utc": "2026-02-07 16:02:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o43l6yb",
                  "author": "Prestigious_Ebb_5131",
                  "text": "Thats great! MCP and building normal agentic loops (state Machines) -and it will be a unique and one-of-a-kind product. I'll do my best to help via pull requests, if you don't mind. \nGreat Job! ðŸ‘",
                  "score": 4,
                  "created_utc": "2026-02-07 16:09:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o44x7bt",
          "author": "PhilCoyo",
          "text": "I would immediately work with this if it had MCP Support. I have so many projects and admin work Where im using Mcps and I feel terrible always giving Claude Access instead of having a local Solution",
          "score": 2,
          "created_utc": "2026-02-07 20:09:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47i0n3",
          "author": "rb1811",
          "text": "Is there a way to integrate s3 client? Say Minio? For auto clean up of blobs uploaded ? \n\nAny benefits over OpenWebUI?",
          "score": 2,
          "created_utc": "2026-02-08 05:42:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49zfyd",
              "author": "MoonXPlayer",
              "text": "S3 / MinIO integration is not implemented yet, but itâ€™s a good idea. still thinking through the right abstraction to avoid adding unnecessary complexity.\n\nalso compared to OpenWebUI, Ollie main focus is local performance and OS-level integration.",
              "score": 1,
              "created_utc": "2026-02-08 16:45:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ee94u",
                  "author": "rb1811",
                  "text": "Ok looking forward for S3/ Minio integration. Thanks for the reply",
                  "score": 2,
                  "created_utc": "2026-02-09 07:54:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4a5tls",
          "author": "Professional_Ad5011",
          "text": "Fantastic. It's pretty much what I was looking for and now it's available to use.",
          "score": 2,
          "created_utc": "2026-02-08 17:16:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4eo9hx",
          "author": "Professional_Ad5011",
          "text": "I have an issue where the answer is above and I cannot see it as Message section has taken up the whole window! anyone else face similar issue?\n\nhttps://preview.redd.it/tn2yq72vufig1.jpeg?width=1024&format=pjpg&auto=webp&s=aed58c0d7a265fbaf1f3668aa433096cb82fe0dc",
          "score": 2,
          "created_utc": "2026-02-09 09:33:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4frqej",
              "author": "MoonXPlayer",
              "text": "Thank you for pointing this out. Iâ€™ve fixed several UI/UX issues in the latest version. Feel free to try it out and let me know if the problem still persists!!",
              "score": 1,
              "created_utc": "2026-02-09 14:30:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4c0ang",
          "author": "BidWestern1056",
          "text": "use npcpy to handle multi providers and a variety of other LLM capabilitiesÂ \nhttps://github.com/npc-worldwide/npcpy",
          "score": 1,
          "created_utc": "2026-02-08 22:42:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvycjs",
      "title": "Just started using local LLMs, is this level of being wrong normal?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qvycjs/just_started_using_local_llms_is_this_level_of/",
      "author": "Cempres",
      "created_utc": "2026-02-04 19:31:52",
      "score": 49,
      "num_comments": 81,
      "upvote_ratio": 0.82,
      "text": "https://preview.redd.it/4swbpymp4jhg1.png?width=746&format=png&auto=webp&s=16d362a6bd68f24a9b349d8f0f21aec95376aa11\n\nTried using the glm4.7 flash version, and just randomly asking it stuff, and i got his with this trip, is that normal to expect and can someone explain why this even happens? lack of internet connection? Should I expect this from other models as well? \n\nMy regular usage wouldn't be this, but a tool to help me create D&D stuff, such as character ideas, helping me when i get stuck ecetera, any model to recommend for such? \n\nSpecs:  \nwin 11  \n32gb RAM  \nXTX 24gb VRAM",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qvycjs/just_started_using_local_llms_is_this_level_of/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3l3wsb",
          "author": "InfraScaler",
          "text": "Yup. Now online models have tools to fetch data from the web, but yeah hallucination is relatively common. The LLM tries to fill knowledge gaps and it just makes up stuff to accomplish that.",
          "score": 52,
          "created_utc": "2026-02-04 19:35:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l86nc",
              "author": "Cempres",
              "text": "Then i'm missing the point of local LLMs, kinda..",
              "score": 5,
              "created_utc": "2026-02-04 19:56:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3la92q",
                  "author": "Dubious-Decisions",
                  "text": "I think you are missing the point of LLMs in general. They are not definitive sources of fact. They are chat simulators. They are designed to predict the most reasonable, statistically likely series of tokens that should be emitted in response to all of their inputs. They have no ability to self-assess or fact check or even understand the semantics of what you are giving them as input.\n\nSo no, you shouldn't expect them to be accurate. The larger the model, the larger the training set and the more likely it is to match some piece of human-created information it was trained on. But as models shift to training with synthetic data and output from other models, you can imagine what happens to accuracy.\n\nWhat they ARE is reasonably inexpensive, simple to interact with natural language processors (NLPs) that can act as some reasonably reliable glue between users and tools. It's a gross oversimplification to say they can't produce useful results, but it's also incorrect to assume that they will ever be foolproof. There are certain types of interactions that they simply will never be able to perform.\n\nThere are many other types of AI and machine learning (ML) platforms besides LLMs, and the tragedy lately is that no one seems to remember that and instead are trying to build the entire AI house with nothing but a LLM hammer.\n\nOnce you realize that these are just tools for parsing human text inputs and producing some compelling, simulated text outputs, you will be better suited to figure out what they are and are not good for.",
                  "score": 92,
                  "created_utc": "2026-02-04 20:06:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3l8stl",
                  "author": "LePfeiff",
                  "text": "An LLM doesnt need to know alot of facts to be a DM assistant in DnD, but its also just a consequence of having to run smaller quantized models that fit on consumer hardware",
                  "score": 6,
                  "created_utc": "2026-02-04 19:58:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lbupi",
                  "author": "sinan_online",
                  "text": "I think they work best for fairly technical hobbyists. For instance, I am combining repeated LLM calls with some Python code to find a good way to get structured markdown out of PDF books. In other words, it's a specific use case of improving OCR quality.\n\nYou can also use them for classification tasks, etc... I personally do not have any cases that do not involve some programming or technical knowledge.\n\n",
                  "score": 4,
                  "created_utc": "2026-02-04 20:13:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lmray",
                  "author": "zenmatrix83",
                  "text": "if you want proper research you can likely try something like this [https://github.com/langchain-ai/local-deep-researcher](https://github.com/langchain-ai/local-deep-researcher) . a deep research look is doing a websearch, the ai gets a finding from the research report,  and then does a loop looking up answers trying to create a report.",
                  "score": 2,
                  "created_utc": "2026-02-04 21:05:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m8301",
                  "author": "Medium_Ordinary_2727",
                  "text": "To be clear all LLMs will do this to a certain extent. Some are better than others bit they can still hallucinate.\n\nFrame your prompt not as a question that you are asking the LLM, but as a research request, and give it the tools to do that research (such as web access).",
                  "score": 2,
                  "created_utc": "2026-02-04 22:50:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3mk2e4",
                  "author": "Western_Courage_6563",
                  "text": "Think about them as an engine, they need rest of the stuff to work (memory, Internet aces, local database, etc).",
                  "score": 2,
                  "created_utc": "2026-02-04 23:55:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3llgoe",
                  "author": "zenmatrix83",
                  "text": "to keep it simple llm means large language model or its a model of how a language is.  This is a decent video on how it works [https://www.youtube.com/watch?v=wjZofJX0v4M](https://www.youtube.com/watch?v=wjZofJX0v4M) .  Its doesn't know whats its saying, its a statistically correct response, which alot of time is correct, but not always. The lower power the model the more it will be statistically wrong.",
                  "score": 2,
                  "created_utc": "2026-02-04 20:59:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m16qs",
                  "author": "florinandrei",
                  "text": "More complex tasks - use bigger models.\n\nSimpler tasks - smaller models are okay.\n\nUnless you have your own dedicated inference hardware, you're not going to run big models at home.\n\nThe definitions of \"simpler\" and \"more complex\" will continue to evolve over time.",
                  "score": 1,
                  "created_utc": "2026-02-04 22:15:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3mv6sa",
                  "author": "oVerde",
                  "text": "If you use it in an agentic environment, like OpenCode, you then can provide tools to fetch from the internet",
                  "score": 1,
                  "created_utc": "2026-02-05 00:56:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3nzbs1",
                  "author": "fasti-au",
                  "text": "Cintext is a bucket of things it can juggle to find the 8/16/32k it can actually relate to your question thatâ€™s what is juggled by 1 shit then think deals with one shot output to decide if it was good. \n\nSo how you fill that first one shot is the key to getting this broken binary system to work.  Ternerary fixes it but itâ€™s not really hardware friendly yet.  Next round of llm with bee new chips if we are transforming but big fusion seems more in line with the in between steps",
                  "score": 1,
                  "created_utc": "2026-02-05 04:59:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3v5w92",
                  "author": "andrew_barratt",
                  "text": "Think of it less like a huge knowledgeable system, more like a software component that can read",
                  "score": 1,
                  "created_utc": "2026-02-06 07:39:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3m49wm",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-02-04 22:30:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3m91wk",
                  "author": "InfraScaler",
                  "text": "If you're offline you can't fetch data. Local does not mean offline. Thing is, if you want it to be online, you need to give it the tools to fetch data.",
                  "score": 2,
                  "created_utc": "2026-02-04 22:55:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3m36jm",
          "author": "themaskbehindtheman",
          "text": "That's why web search and tools are a thing, the tools are a big reason these companies still hire software engineers. \n\nMonitor chats > find commonly asked questions where an incorrect answer is given > write tool > claim sentience /s\n\nIf you whack openweb ui over the top of it, enable web search and ask about the weather it'll get it wrong, you have to write a tool to call a weather API, it can figure out it needs to use the tool and then to parse the JSON response.",
          "score": 10,
          "created_utc": "2026-02-04 22:25:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q37r3",
          "author": "Past-Grapefruit488",
          "text": "You need to enable Web Search so that App can use it to look up information. LLMs (Local or otherwise) are not encyclopedia. ChatGPT or Gemini will run a web search to answer such questions . Example : \n\nhttps://preview.redd.it/wvwdp3xdvohg1.png?width=1091&format=png&auto=webp&s=bb9ad3dbe1437c5f24e3a3e3f37e1bbf985dcf6f\n\n",
          "score": 4,
          "created_utc": "2026-02-05 14:48:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4cfhj1",
              "author": "FaithlessnessLife876",
              "text": "Yeah I think people don't realise how much [chatgpt.com](http://chatgpt.com) is actually a wrapper and the LLMs themselves how much pre-promting finetuning they have with the goal of mimic human behaviour",
              "score": 1,
              "created_utc": "2026-02-09 00:11:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ladtw",
          "author": "HonestoJago",
          "text": "GLM benefits from a low temp in situations like this, at least in my experience. But yeah, if youâ€™re expecting Claudeâ€¦..donâ€™t.",
          "score": 3,
          "created_utc": "2026-02-04 20:06:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lw42r",
              "author": "Cempres",
              "text": "was not expecting anything in particular, was just wondering. I'll test it with ideas for D&D, advices on how to connect different story arcs and such and i'll write a comment when i have the chance to test it for such use",
              "score": 1,
              "created_utc": "2026-02-04 21:50:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3qw8wy",
                  "author": "Dubious-Decisions",
                  "text": "Load a bunch of slightly structured (e.g. YAML) files into an IDE like VS Code and hook it up to your local LLM. It can crawl all over that document tree and make notes to itself. When you tell it you are in \"room x\", it can go read through the YAML file for room x at the direction of the IDE and switch its context to use the facts and descriptions you put in the room x file. llama3.2 is actually a competent little LLM for doing this locally. ",
                  "score": 1,
                  "created_utc": "2026-02-05 17:06:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3orp7j",
          "author": "cibernox",
          "text": "Yes. Small locals models are reasonably good at being smart, not at being knowledgeable about every niche. Whey will get most thing that widely known well (year of a mayor war, what is the biggest mammal, and wether it rains more in Seattle or in Los Angeles, but not how many mg of caffeine a drink has).",
          "score": 3,
          "created_utc": "2026-02-05 09:04:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l9bgg",
          "author": "Condomphobic",
          "text": "They generally suck unless they're the full-sized model, which normal consumers cannot afford",
          "score": 5,
          "created_utc": "2026-02-04 20:01:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l99z6",
          "author": "Witty_Mycologist_995",
          "text": "Thatâ€™s kind of strange. I use glm flash. What quantization is that?\nAsk it the capital of france",
          "score": 1,
          "created_utc": "2026-02-04 20:01:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lkd9i",
              "author": "gregusmeus",
              "text": "Berlin, lol. Sorry.",
              "score": 2,
              "created_utc": "2026-02-04 20:54:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ln11g",
                  "author": "mattv8",
                  "text": "ðŸª¦",
                  "score": 1,
                  "created_utc": "2026-02-04 21:07:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3pk783",
              "author": "Dzhmelyk135",
              "text": "Not even my Q3 version is that dumb",
              "score": 2,
              "created_utc": "2026-02-05 13:02:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3lvq1m",
              "author": "Cempres",
              "text": "https://preview.redd.it/bz8ghqbltjhg1.png?width=744&format=png&auto=webp&s=6e3477d638ab234ed58ad52dd52d3d6ab0b6c135\n\nI was trying out the glm-4.7-flash:q4\\_K\\_M",
              "score": 1,
              "created_utc": "2026-02-04 21:48:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3lzqy9",
                  "author": "Witty_Mycologist_995",
                  "text": "Send reasoning traces",
                  "score": 2,
                  "created_utc": "2026-02-04 22:07:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3lzww6",
          "author": "Ryanmonroe82",
          "text": "You need to use a smaller model with bf/fp16.  Using larger models that are quantized will always be less accurate especially a thinking/reasoning model thatâ€™s a MoE.  With 24gb VRAM look at dense 8b models that you can run in FP16/B16 with headroom for a decent context window.  RNJ-1-8b is excellent so is Gemma 2-7b.  They might not be as large as you want but the precision and accuracy is noticeably better especially on lower temps.",
          "score": 1,
          "created_utc": "2026-02-04 22:08:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mrp7q",
          "author": "superdariom",
          "text": "Isn't glm 4.7  a specialized coding model? Why not try deepseek instead?",
          "score": 1,
          "created_utc": "2026-02-05 00:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n4wj4",
          "author": "mpw-linux",
          "text": "Look at Liquid AI models. I use this one my Mac M1:\n\nmodel, tokenizer = load(\"mlx-community/LFM2.5-1.2B-Thinking-8bit\")\n\n It is quite good. I asked it create a 3 chord modern country song which i did with lyrics and chords. ",
          "score": 1,
          "created_utc": "2026-02-05 01:52:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ocqvk",
          "author": "tom-mart",
          "text": "There is nothing wrong about the example you showed. What is the problem?",
          "score": 1,
          "created_utc": "2026-02-05 06:45:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pc8xi",
              "author": "damhack",
              "text": "Itâ€™s low calorie, zero sugar but has caffeine in it.",
              "score": 1,
              "created_utc": "2026-02-05 12:08:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3pd024",
                  "author": "tom-mart",
                  "text": "I asked what's the problem in LLM response? It looks correct.",
                  "score": 1,
                  "created_utc": "2026-02-05 12:13:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pouht",
          "author": "bitmux",
          "text": "I find that most of the single-gpu local LLM's I've played with generate about 60% bs mostly regardless of settings and even including the websearch tool UNLESS they're hyper specialized in something and you use them right down the center of their specialization.  At that point its 40% bs :-P.",
          "score": 1,
          "created_utc": "2026-02-05 13:29:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rfwmp",
          "author": "generousone",
          "text": "With 24gb vram, go down to a smaller model like gpt-oss:20b and give it full 128k context. You'll see a vast improvement on capability and usefulness. GLM flash is 19gb. That's not a lot of room for context with 24gb vram.Â \n\n\n\nGenerally, it's also good to enable things like web search when asking fact specific questions.Â Â \n\n\nEdit: lol, tried the exact prompt in my gpt-oss:20b and it wouldn't complete the request because it was censored to providing proprietary information. So I omitted \"secret\" and just asked what the ingredient were and it worked.Â ",
          "score": 1,
          "created_utc": "2026-02-05 18:36:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vamc5",
          "author": "joshiegy",
          "text": "One of the best ways to use a local llm is to combine with an MCP that gives some sort of webacrape ability.\nOr to have one specific to a task, like storytelling or code review in a specific language.\n\nBut it's not a GPT, those are huuuuuge. Chatgpt, a while back at least, requires something like 256gb of vram. Maybe it's even more now.",
          "score": 1,
          "created_utc": "2026-02-06 08:23:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vislx",
          "author": "maxbiz",
          "text": "The problem you have run into is that LLMs are poor at saying, 'I don't know.'  Where the LLM does not know, it treats it as a multiple-choice question; a random pick of 4 possible answers is a 25% chance of being right rather than 0% for 'I do not know'.",
          "score": 1,
          "created_utc": "2026-02-06 09:42:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xrgvy",
          "author": "fasti-au",
          "text": "Local is smart but not wise. Was one comes with cintext filling with info not nearest guesses.   They can do anything but anything is anything you specifically are giving it information to work with not anything you need a nuclear plant Greenland all the metals from Canada and ukrain as well as cooling and also did we menthey they cheat lies and hide behind the government not fight court battles unlike anthropic whi get to turn around and destroy copyright completely and have to then compete as an underdog but actually are trying for good things in more ways\n\nOpenAI motto is all your shit belongs to us pay us for it back and do it while we destroy your jobs and say they are not money gobbling cunts with smart people staying alive and staying because they know not money or in club is bad",
          "score": 1,
          "created_utc": "2026-02-06 17:41:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42i8ux",
          "author": "dreamer2020-",
          "text": "Dont know what version of glm4.7 flash you are using. But the full variant is way above 250gb ram. Try to use gpt oss 120b if possible, it is really â€œwiseâ€",
          "score": 1,
          "created_utc": "2026-02-07 12:23:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n17yi",
          "author": "PrepperDisk",
          "text": "Yep! Â We had to give up on bringing one to Prepper Disk even with RAG. Â They are proofs of concept at best.",
          "score": 1,
          "created_utc": "2026-02-05 01:31:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nyx8l",
          "author": "fasti-au",
          "text": "Models are how they thing not what they think.   What they thing is parameters so the memory base inside a 1 trillion midel is about not looking.  Local models are all about the perfect prompt to avoid think having a different plan to you",
          "score": 0,
          "created_utc": "2026-02-05 04:56:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzh0op",
      "title": "I created a small AI Agent",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/",
      "author": "Rough_Philosopher877",
      "created_utc": "2026-02-08 18:43:20",
      "score": 48,
      "num_comments": 13,
      "upvote_ratio": 0.96,
      "text": "Hi guys.. I know it's not so big thing.. just for fun I created a Small AI Agent:\n\n[https://github.com/tysonchamp/Small-AI-Agent](https://github.com/tysonchamp/Small-AI-Agent)\n\nWould love the feedback of the community.. and any suggestions of new ideas.\n\nI created this for my day to day activities.. such as setup reminders, take notes, monitor all my client's website (if they are all ok or not).. monitor all my servers, connecting it to my custom erp for due invoice fetching, project management etc ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4ap7g5",
          "author": "RA2B_DIN",
          "text": "Sounds really nice, Iâ€™ll try it",
          "score": 1,
          "created_utc": "2026-02-08 18:47:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4arhru",
              "author": "Rough_Philosopher877",
              "text": "thanks mate.. let me know your feedback please..",
              "score": 1,
              "created_utc": "2026-02-08 18:58:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4cdwvf",
          "author": "Electronic_Fox594",
          "text": "I made one too but Iâ€™m not a real programmer so I wonâ€™t share it but very similar.",
          "score": 1,
          "created_utc": "2026-02-09 00:02:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4cpn6h",
          "author": "Civil_Tea_3250",
          "text": "Love it! I created something like this myself using Ollama, python scripts, n8n, .md files and all that. I've been adding to it when I get time and learn something new. I'll check it out when I have time. Would love to see what you do similar/different.\n\nI had the same idea. I hate all the AI down our throats and find most of it frustrating. Having something that does my regular tasks and is only focused on my home and server makes it much more trustworthy and useful.",
          "score": 1,
          "created_utc": "2026-02-09 01:07:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4dakcc",
              "author": "Rough_Philosopher877",
              "text": "I started with only website monitoring.. and now Iâ€™ve added four five skills.. thinking to add few more like notification sending to me and my team members based on pending task as reminder.. automated chat replies.. sent emails to my clients etc..",
              "score": 1,
              "created_utc": "2026-02-09 03:00:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4e75l0",
          "author": "Zyj",
          "text": "Telegram is not end-to-end encrypted most of the time",
          "score": 1,
          "created_utc": "2026-02-09 06:48:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hvdtk",
          "author": "Consistent-Signal373",
          "text": "I started with a fairly simple telegram also.\n\nI will suggest considering using a Matrix server, if you value fully local and private data.\n\nAlso Matrix can be end-to-end encrypted, and you have full control of everything.\n\nI also dropped using Ollama and switched over to LM Studio, as the speeds are much better, at least with Nvidia GPU's.\n\nAt some points my my goal became to create a complete AI operation system.\n\nSince then I added 15 dashboards, different modes e.g. work, chat, code and swarm with 14 custom agents, a personal code assistant and tons more. Last check it was 120k+ lines of code.\n\nSo yeah just keep going",
          "score": 1,
          "created_utc": "2026-02-09 20:38:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4atim6",
          "author": "Acrobatic_Task_6573",
          "text": "This is cool. The fact that you built it around your actual daily workflow instead of making it generic is the right approach. Most AI agent projects try to do everything and end up doing nothing well.\n\nThe website monitoring and ERP integration pieces are especially interesting. Those are real problems that most people solve with 3 or 4 different SaaS tools. Having one agent that handles all of that is clean.\n\nA few questions/suggestions if you keep building on it:\n\n- How does it handle failures? Like if a website check times out, does it retry or just flag it?\n- For the reminder system, does it persist across restarts? That was one of the first things I had to solve with my own setup.\n- Have you thought about adding a simple web dashboard to see all your monitors at a glance?\n\nNice work for a personal project. The best tools are the ones built to scratch your own itch.",
          "score": 1,
          "created_utc": "2026-02-08 19:07:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d96mj",
              "author": "Rough_Philosopher877",
              "text": "Thanks for coming.. Right now it just send a notification to my telegram bot.. and i operate it via telegram bot only..\nIâ€™m using sqlite db to store everything..\nAbout the web didnâ€™t thought about it.. but I needed a way to see the db easy way..",
              "score": 1,
              "created_utc": "2026-02-09 02:53:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ayt2a",
          "author": "mosaad_gaber",
          "text": "I tried to clone and install and face this \ngit clone https://github.com/yourusername/ai-assistant-bot.git\ncd ai-assistant-bot\n\n# Create Virtual Environment\npython3 -m venv venv\nsource venv/bin/activate\n\n# Install Dependencies\npip install -r requirements.txt\nThe program git is not installed. Install it by executing:\n pkg install git\nbash: cd: ai-assistant-bot: No such file or directory\n\n[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n[notice] To update, run: pip install --upgrade pip\nERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'",
          "score": -3,
          "created_utc": "2026-02-08 19:33:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4czk7c",
              "author": "inteblio",
              "text": "Chat gpt",
              "score": 1,
              "created_utc": "2026-02-09 02:03:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4eskly",
                  "author": "mosaad_gaber",
                  "text": "I relace ollama with Gemini and works like acharm because gemma3 it's eat ram and make my device lag",
                  "score": 0,
                  "created_utc": "2026-02-09 10:15:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4gs2zc",
              "author": "Rough_Philosopher877",
              "text": "Updated the readme.. btw you need git installed first or download the zip from github",
              "score": 1,
              "created_utc": "2026-02-09 17:29:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxmmio",
      "title": "Best models on your experience with 16gb VRAM? (7800xt)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qxmmio/best_models_on_your_experience_with_16gb_vram/",
      "author": "roshan231",
      "created_utc": "2026-02-06 16:40:26",
      "score": 39,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "Iâ€™m running a 7800 XT (16 GB VRAM) and looking to get the best balance of quality vs performance with Ollama.\n\nWhat models have you personally had good results with on 16 GB VRAM?\n\nReally I'm just curious about your use cases as well. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qxmmio/best_models_on_your_experience_with_16gb_vram/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3xh50w",
          "author": "tcarambat",
          "text": "For me, Qwen3 4B (Q4) @ 128K gives me excellent responses for speed quality without overmaxxing the card. If i need something multi-model (usually do) I just swap out for Qwen3-VL 4B Q4 @ 128k|256K.\n\nWorks excellent for me, no bs and if the thinking is bothering me I just turn it off. One cavet i have found is the qwen3 models you almost never want to send a simple unbounded prompt like \"Hello\" - it will easily think for 1k+ tokens just to say hello.\n\nIf you ask an actual prompt though or attach an image as context it thinks briefly and gives a great response.",
          "score": 10,
          "created_utc": "2026-02-06 16:51:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45aqod",
              "author": "No-Consequence-1779",
              "text": "4b is pretty good. I have 2 32gb gpus. Â I have been using smaller models more and more. Very little difference for simple coding tasks from a 30b to a 4b. Â Though for models coding is the easiest thing.Â ",
              "score": 2,
              "created_utc": "2026-02-07 21:22:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3yavqa",
          "author": "BGPchick",
          "text": "What are your goals? For coding, I really like gpt-oss:20b or qwen3-coder:30b. For general writing, I find llama3.2 or gemma3 a little better and faster.",
          "score": 6,
          "created_utc": "2026-02-06 19:13:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xm6r2",
          "author": "fasti-au",
          "text": "16 gb vram is qwen 14b territory Phi4 mini.   You might fit a devstral2small which is smaller that qwen coder 30b and codes",
          "score": 5,
          "created_utc": "2026-02-06 17:15:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41w28t",
          "author": "nycigo",
          "text": "Devstral 2 Small is unbeatable if you want to program.",
          "score": 3,
          "created_utc": "2026-02-07 08:54:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41ga9r",
          "author": "Remarkable_Stay_592",
          "text": "GPT-OSS 20B, QWEN 3 14B -> coding\nGemma 3 12B -> general chats",
          "score": 2,
          "created_utc": "2026-02-07 06:26:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43ogkv",
          "author": "_w0rm",
          "text": "Interesting. I have been just testing several models to be used with log analysis. So far I have not found reliable model for the task. Many models are successful in find the needle in haystack kind of tasks but fail when asked to retrieve multiple entries from context (kind of reasoning over haystack). In this kind scenarios models often just miss data or start to hallucinate especially in the middle of the context. Best results with reasonable speed I have gained with Ministral-3-14B-Reasoning with Q6_K quant (8B with Q8_0 provide almost equal results), Phi-4-Reasoning-Plus Q4_K_M and gpt-oss-20B Q4_K_M. Qwen-3-30B does good job but is really slow as it needs offloading and still with limited context.\n\nFor testing I wrote a small Python program which generates log file of specified size with known information spread in the logs. \n\nIf anyone have good suggestions, please let me know.",
          "score": 1,
          "created_utc": "2026-02-07 16:25:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y1tem",
          "author": "stonecannon",
          "text": "I like Gemma 3 on my 16gb laptop.",
          "score": 1,
          "created_utc": "2026-02-06 18:30:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40pw50",
          "author": "Old-Sherbert-4495",
          "text": "16gb here as well. tried glm 4.7 flash q2 and q4 they do a pretty good job. but didn't get to extensively test it. it's pretty good at coding agentic taks",
          "score": 0,
          "created_utc": "2026-02-07 03:11:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40xfg6",
              "author": "buttetsu",
              "text": "I second this. glm 4.7 flash w/ Zed has been the best quality I can get at reasonable speed on 16 GB VRAM. Miatral Small 2 is also great. Mistral has been more reliable,; glm has solved some more complex problems but can get stuck in repeat loops sometimes.",
              "score": 1,
              "created_utc": "2026-02-07 04:01:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qvj4la",
      "title": "Qwen3-Coder-Next",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qvj4la/qwen3codernext/",
      "author": "stailgot",
      "created_utc": "2026-02-04 08:29:24",
      "score": 38,
      "num_comments": 9,
      "upvote_ratio": 0.96,
      "text": "Qwen3-Coder-Next is a coding-focused language model from Alibaba's Qwen team, optimized for agentic coding workflows and local development. \n\n> ollama run qwen3-coder-next\n\nhttps://ollama.com/library/qwen3-coder-next\n\n> requires Ollama 0.15.5",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qvj4la/qwen3codernext/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3xt8lz",
          "author": "Mangostickyrice1999",
          "text": "Sighhh 64gb vram is 10k here in Europe. Just for de gpu power",
          "score": 1,
          "created_utc": "2026-02-06 17:49:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zoxer",
              "author": "Daepilin",
              "text": "You can run it on a 16gb card if you have enough System RAM (64 Gigs)Â \n\n\nIt's kinda slow though at thst point.Â ",
              "score": 1,
              "created_utc": "2026-02-06 23:28:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o42ughk",
              "author": "tecneeq",
              "text": "A 5090 and two 5070 Ti and a board can be had for 5 to 6kâ‚¬. Not sure what the Ryzen AI Max computers with 128GB cost, but i guess it's around 2.5kâ‚¬.",
              "score": 1,
              "created_utc": "2026-02-07 13:46:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3iq21p",
          "author": "WaitformeBumblebee",
          "text": "looking forward to test this and pit it against GLM4.7 flash which after minimal testing seems better than qwen3-A3B-2507",
          "score": 1,
          "created_utc": "2026-02-04 12:35:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ivvfi",
          "author": "Final-Watercress-253",
          "text": "What do you use these models with? I mean, with Claude I have the terminal, but if I want to use one of these models, what terminal software do I have to use?",
          "score": 1,
          "created_utc": "2026-02-04 13:12:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3j120b",
              "author": "JMowery",
              "text": "There are dozens of them nowadays. OpenCode seems to be one for the more popular.",
              "score": 8,
              "created_utc": "2026-02-04 13:41:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3u4pmm",
              "author": "coding9",
              "text": "I use it with Claude code still. Its just a couple env variables using LM Studio",
              "score": 2,
              "created_utc": "2026-02-06 03:03:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o42tu8p",
                  "author": "tecneeq",
                  "text": "For now.",
                  "score": 1,
                  "created_utc": "2026-02-07 13:42:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mx7ap",
              "author": "AmphibianFrog",
              "text": "I use the Cline plugin in VS Code and it works well with the previous coder model (haven't tested on this one)",
              "score": 1,
              "created_utc": "2026-02-05 01:08:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qy77nm",
      "title": "Lorph: A Local AI Chat App with Advanced Web Search via Ollama",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/gallery/1qy77nm",
      "author": "Fantastic-Market-790",
      "created_utc": "2026-02-07 07:09:52",
      "score": 36,
      "num_comments": 9,
      "upvote_ratio": 0.83,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qy77nm/lorph_a_local_ai_chat_app_with_advanced_web/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o41okof",
          "author": "DanyShift",
          "text": "Nice! But how does it differ from Open Webui?",
          "score": 19,
          "created_utc": "2026-02-07 07:42:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41v1r6",
              "author": "__Maximum__",
              "text": "And 20 others",
              "score": 9,
              "created_utc": "2026-02-07 08:44:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o41orez",
              "author": "spacywave",
              "text": "Second. Any benefits compared to gpt researcher and similar?",
              "score": 2,
              "created_utc": "2026-02-07 07:44:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o41rcw0",
                  "author": "ExtentOdd",
                  "text": "It uses Ollama which means you can use your local model instead of GPT by openai",
                  "score": -7,
                  "created_utc": "2026-02-07 08:09:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4739gz",
              "author": "Witty_Mycologist_995",
              "text": "Open webui has the shittiest search system ever. Reason why I quit it",
              "score": 0,
              "created_utc": "2026-02-08 03:52:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o422nrk",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -13,
              "created_utc": "2026-02-07 10:00:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o426w4y",
                  "author": "SteveLorde",
                  "text": "holy AI slop",
                  "score": 14,
                  "created_utc": "2026-02-07 10:41:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4c7v2x",
          "author": "ScrapEngineer_",
          "text": "Great, more slop.",
          "score": 2,
          "created_utc": "2026-02-08 23:26:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qv1yey",
      "title": "Your thoughts on \"thinking\" LLMs?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qv1yey/your_thoughts_on_thinking_llms/",
      "author": "stonecannon",
      "created_utc": "2026-02-03 19:41:24",
      "score": 30,
      "num_comments": 43,
      "upvote_ratio": 0.73,
      "text": "almost all of the ollama-ready models released in recent months have been \"thinking\" or \"chain of thought\" or \"reasoning\" models -- you know, the ones that force you to watch the model's simulated thought process before it generates a final answer. \n\npersonally, i find this trend extremely annoying for a couple reasons:\n\n1). it's fake.  that's not how LLMs work.  it's a performance to make it look like the LLM has more consciousness than it does. \n\n2).  it's annoying.  i really don't want to sit through 18 seconds (actual example) of faux-thinking to get a reply to a prompt that just says \"good morning!\".\n\nThe worst example i've seen so far was with Olmo-3.1, which generated 1932 words of \"thinking\" to reply to \"good morning\" (i saved them if you're curious).\n\nin the Ollama CLI, some thinking models respond to the \"/set nothink\" command to turn off thinking mode, but not all do.  and there is no corresponding way to turn off thinking in the GUI.  same goes for the AnythingLLM, LM Studio, and GPT4All GUIs.\n\nso what do \\_you\\_ think?  do you enjoy seeing the simulated thought process in spite of the delays it causes?  if so, i'd love to know what it is that appeals to you... maybe you can help me understand this trend.\n\ni realize some people say this can actually improve results by forcing checkpoints into the inference process (or something like that), but to me it's still not worth it.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qv1yey/your_thoughts_on_thinking_llms/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3ew6mm",
          "author": "synth_mania",
          "text": "thinking models are created because they work. RL is used during post-training to train it to think in a way that increases performance.",
          "score": 29,
          "created_utc": "2026-02-03 21:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eqm1j",
          "author": "Wrapzii",
          "text": "Objectively for complex tasks itâ€™s much better. It should *if done correctly* ask the question you asked but in different ways resulting in slightly different responses which the model compacts into one response. It does the â€œwhat, how, whyâ€ to itself to improve responses. Obviously if you tell it â€œhiâ€ thats a stupid thought process. You shouldnâ€™t really be using thinking models unless you need them.",
          "score": 14,
          "created_utc": "2026-02-03 20:49:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eyywm",
          "author": "iam_maxinne",
          "text": "Look, \"thinking\" is the model generating more tokens to try (and often to succeed) to create more tokens about your prompt, so your desired output is more likely to be result. You exchange some tokens (and the time it takes to generate them) for precision.\n\nIf you are coding on a specific language, that have specific terms, jargoons, file formats, tools, and etc..., so adding them to the session may make the model find an answer more in line to what you need.\n\nI find thinking worse when doing generic work, as there is less data to it to \"think about\", while it improves when the scope is smaller and more specialized.",
          "score": 8,
          "created_utc": "2026-02-03 21:27:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3emb7s",
          "author": "msltoe",
          "text": "The smaller models thinking process does work for certain things, like solving puzzles, but it's generally much flimsier than a SotA model's thoughts.",
          "score": 5,
          "created_utc": "2026-02-03 20:28:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f6u4m",
          "author": "gamesta2",
          "text": "I have a great experience with devstral-small-2:24b even though its a coder, it does a fantastic job at tool use and general tasks. The thing with llm's, its not about paper performance, but your setup, prompt, and integration. For example, nemotron spends 1500 words to say good morning on the same setup and same prompt, but on paper, nemotron is supposed to be better and faster",
          "score": 7,
          "created_utc": "2026-02-03 22:04:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3q2xqn",
              "author": "Go_Fast_1993",
              "text": "I've found nemotron to be better than devstral at parameter extraction for tool calling. I only bring this up because I was specifically evaluating devstral-small against nemotron-3-nano this week. Response time is slower on nemotron, but that tradeoff works for my use case.",
              "score": 2,
              "created_utc": "2026-02-05 14:46:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3r0kxo",
                  "author": "gamesta2",
                  "text": "Yeah it all depends on setup. Another example for me is qwen3:30b model absolutely destroying gpt-oss. Everyone talks good about oss, but it just would not work for me. For now devstral performs the best",
                  "score": 1,
                  "created_utc": "2026-02-05 17:26:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o41sizy",
              "author": "tecneeq",
              "text": "That model has good paper performance. But you use it to make a point that it's not about paper performance.\n\nBold strategy. ;-)",
              "score": 1,
              "created_utc": "2026-02-07 08:20:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4447lg",
                  "author": "gamesta2",
                  "text": "At coding, or agentic tasks. Which yes, does suit tool calling - but not general chatbot tasks. Yet......... it does great overall. Not sure why but it does for me",
                  "score": 1,
                  "created_utc": "2026-02-07 17:42:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3etmc6",
          "author": "Savantskie1",
          "text": "In some tasks the thinking variants improve the model but the /no_think only works for Qwen models",
          "score": 4,
          "created_utc": "2026-02-03 21:02:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ezco4",
          "author": "funfun151",
          "text": "If youâ€™re saying â€œgood morningâ€ Iâ€™m guessing youâ€™re using it as a chatbot? Not really the use case for thinking models tbf, youâ€™d be better off with a high context standard model for that.",
          "score": 4,
          "created_utc": "2026-02-03 21:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gaj2h",
          "author": "eleqtriq",
          "text": "You want my thoughts on thinking?  Iâ€™d rather get straight to the answer.",
          "score": 3,
          "created_utc": "2026-02-04 01:36:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41sn1s",
              "author": "tecneeq",
              "text": "https://preview.redd.it/2ln3z9o681ig1.jpeg?width=754&format=pjpg&auto=webp&s=97a3bace9e8822a81e3e27ac1207f6b4afcb59c5\n\n",
              "score": 1,
              "created_utc": "2026-02-07 08:21:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o43eu1q",
                  "author": "Zealousideal-Pop-793",
                  "text": "Phenomenal ðŸ˜‰ðŸ‘‰",
                  "score": 1,
                  "created_utc": "2026-02-07 15:37:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3f0kuo",
          "author": "sinan_online",
          "text": "Itâ€™s not that they are â€œthinkingâ€, itâ€™s more that they are generating language that is a chin of thought. Since LLMs can generated human language, a straightforward application is to create one that is trained towards a chain-of-thought format.\n\nIt does look like it is wasted GPU hours and energy, I am not sure if measurable outcomes are much different.\n\nIâ€™d have to see something concrete. Claude 4.5 is good at programming, but I am not sure how much of that is related to how itâ€™s trained vs how it is implemented.",
          "score": 1,
          "created_utc": "2026-02-03 21:35:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fckmo",
              "author": "d_the_great",
              "text": "It make the model, well, model a solution to the user's problems better, resulting in a better answer. The only problem is small models don't have the ability to decide when or when not to enter thinking mode, which means they just default to thinking every time even when it's a waste of time and resources.",
              "score": 1,
              "created_utc": "2026-02-03 22:32:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3fcvfh",
          "author": "AmphibianFrog",
          "text": "Regarding point 1, I do agree it's kind of \"fake\" in that the chain of thought is not necessarily representative of what's going on in the network or how it came to a conclusion, but there's more to it than that. Because the chain of thought tokens get fed back in for subsequent token generation, it does provide the model with extra processing and those tokens do influence the final output. \n\nPersonally I normally pick instruct models or use whatever flag (i.e. /nothink) turns the thinking off. I'm not sure the benefits outweigh the extra processing time and energy for most local models. Often I just want it to be faster.\n\nIt can give a bit of extra insight too if you read the chain of thought but you have to take it with a pinch of salt!",
          "score": 1,
          "created_utc": "2026-02-03 22:33:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fdx7q",
          "author": "Frogy_mcfrogyface",
          "text": "It works better for some stuff, but ive noticed that if I ask the model something like \"Take this list and in front of each line add xx with a space before each line\" it will think about it, and it its thought process will actually create the list exactly how I want it, but then overthink it and its result would be something different.",
          "score": 1,
          "created_utc": "2026-02-03 22:38:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fnjus",
          "author": "duplicati83",
          "text": "Thereâ€™s a huge difference between my qwen3 30b thinking vs non thinking models. Itâ€™s the difference between right and wrong answers.",
          "score": 1,
          "created_utc": "2026-02-03 23:29:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fzb5d",
          "author": "JLeonsarmiento",
          "text": "I donâ€™t like â€œthinkingâ€ models either.  Hybrid or those with budget thinking, behaving like â€œinstructâ€ or â€œreasoningâ€ based on the complexity of the prompt are better.",
          "score": 1,
          "created_utc": "2026-02-04 00:33:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g0zu2",
          "author": "Medium_Ordinary_2727",
          "text": "Chain-of-thought reasoning definitely improves the results I get on certain types of questions.\n\nExample: until recently most LLMs couldn't correctly answer \"How many R's are in the word strawberry?\" But those same LLMs, prompted with my homemade janky chain-of-thought system instructions, would get the answer correct.\n\nI agree it's annoying to see the \"thinking\" process especially if you are just asking a quick question that doesn't require thought. Some frontends like OpenWebUI do have an [option to disable thinking mode](https://docs.openwebui.com/features/chat-features/reasoning-models/#think-ollama).",
          "score": 1,
          "created_utc": "2026-02-04 00:43:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g1gyl",
          "author": "sultan_papagani",
          "text": "yes its bad. gpt oss 20b creates fake policies while its \"thinking\" and cant even play a number guessing game.",
          "score": 1,
          "created_utc": "2026-02-04 00:45:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g6cl2",
          "author": "triynizzles1",
          "text": "Itâ€™s good for long prompts where youâ€™re asking the LLM to complete multiple tasks. The thinking tokens allow the model to work out the intricacies of what you are asking. Simple prompts like â€œhelloâ€ and â€œsummarize this:â€ isnt where these models excel. If the use cases correct, itâ€™s great!",
          "score": 1,
          "created_utc": "2026-02-04 01:12:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g80tt",
          "author": "sn0n",
          "text": "Ok, so the OP said thinking, so I should call <tool>â€¦.",
          "score": 1,
          "created_utc": "2026-02-04 01:22:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gkplk",
          "author": "fasti-au",
          "text": "One shots work in tokens in.  Reasoners work in token in and review which implies it has enough tokens to work with to one shot and the also to reason. \n\nIf one shotting is already there in tool calling then you are not really needing a reasoner but reasoners needed one shots to work to not think about code breaks and to think about it as a concept or design flaw.   Error then  fix error if no error look at results â€¦. Nope ðŸ‘Ž fixed error thatâ€™s what think decided was the task and you retry.     Think isnâ€™t your choice of workflow really.  If think is about why itâ€™s broken not how the task is broken in context then your creation token burn on retries",
          "score": 1,
          "created_utc": "2026-02-04 02:34:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hg736",
          "author": "DedsPhil",
          "text": "It's not fake, thinking tokens were created because in a lot of scenarios they help steering the model for a better answer without polluting the contex.\n\nThey were made because people figured that asking the model to \"think\" step by step made the answer better.",
          "score": 1,
          "created_utc": "2026-02-04 05:58:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hh41c",
          "author": "NoxinDev",
          "text": "People still believe that's anything other than the first tier of output for your input tokens, anything more than a performance?",
          "score": 1,
          "created_utc": "2026-02-04 06:06:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hrwb5",
          "author": "Ledeste",
          "text": "1) It's not \"fake,\" but it's clearly branding. But that's how words work. For example, words do not actually work, but when you read \"words work,\" you did not think it was fake.\n\n2) It's a way to improve the result; it's not made to be cool, it's made to make the response better. If it's taking too long, it's mostly because the LLM was not tuned enough and because local tools are basically non-existent (just forwarding user requests to an LLM with minimal wrapping). \n\nYou should learn a bit about how LLMs work; you'll quickly understand why these steps are becoming important.",
          "score": 1,
          "created_utc": "2026-02-04 07:38:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hxr5d",
          "author": "podgorniy",
          "text": "Before thinking was a thing one way to improve quality of LLM reply was to actually ask it to \"make a plan\" or \"think about the problem\" and only then get the final result. Idea of thinking is that this extra step should improve the output of the LLM. At least according to the benchmarks.",
          "score": 1,
          "created_utc": "2026-02-04 08:32:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ik5g4",
          "author": "StaysAwakeAllWeek",
          "text": "Beyond just the intelligence, performance, accuracy, etc benefits, thinking mode MASSIVELY reduces the hallucination rate because the model has ample chance to spot its own hallucinations and correct them\n\nIf you're ever having an LLM output content that cant/won't be fully manually verified it should be with thinking enabled. Always.",
          "score": 1,
          "created_utc": "2026-02-04 11:53:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3isteu",
          "author": "crone66",
          "text": "They're usage should be situational. E.g. in if this than that scenarios it might be able to solve things that are not directly in the training data e.g. simple math tasks (without using tools!).\n\n\nExample if the training data contains\n2+2=4 but not 20+20 it might reason ton calculate 2+2 and simply add a zero. Therefore it could solve it without having it in the trainingdata and without tool use.\n\n\nA non thinking model couldn't do that easily without explicitly prompting for calculation strategies first.\n\n\nThe down side is that think is in many scenarios completely useless and just a waste of time. Therefore, most providers automatically try to select the model based on the complexity of your prompt.\n\n\nTLDR: Use thinking models only when it makes sense because it has negative value for easy tasks/prompts.",
          "score": 1,
          "created_utc": "2026-02-04 12:53:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j3lc0",
          "author": "MrMakerWeTrust",
          "text": "In my Opinion LLMs have now reached a point of larger thinking memory. I donâ€™t think itâ€™s natural thinking I believe they use numbers to determine the quality of the response",
          "score": 1,
          "created_utc": "2026-02-04 13:55:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3r97y3",
          "author": "surfmaths",
          "text": "It is fake, and it wastes tokens/compute, but also... it works?! In the sense that the final answer is correct more often. \n\nI agree it's frustrating but the entirety of AI is made of fake layers that improve slightly and that all combined give something acceptable.",
          "score": 1,
          "created_utc": "2026-02-05 18:06:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rk68j",
          "author": "Jshdgensosnsiwbz",
          "text": "sure , you want to get into this....",
          "score": 1,
          "created_utc": "2026-02-05 18:56:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ufnea",
          "author": "RecalcitrantZak",
          "text": "After the anthropic research on model traces it was fairly obvious that most thinking models are just writing their own narrative fan fiction with nothing to do with how they are generating data. \n\nJust another way to burn tokens.",
          "score": 1,
          "created_utc": "2026-02-06 04:12:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o424h13",
          "author": "TedditBlatherflag",
          "text": "I think you have a fundamental misunderstanding of what â€œthinkingâ€ is doingâ€¦ these are effectively sub-agent patterns allowing the LLM to evaluate and summarize more without introducing context rot.Â ",
          "score": 1,
          "created_utc": "2026-02-07 10:18:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ehf0r",
          "author": "Dubious-Decisions",
          "text": "They are designed to encourage continued human engagement to drive token usage/sales. A \"thinking\" model that is free/open is just annoying because it's just wasting CPU resources and not even driving revenue for its creator. In all cases, they are a waste unless you are in it for the entertainment value alone.",
          "score": 1,
          "created_utc": "2026-02-03 20:05:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3elmts",
          "author": "Copakela",
          "text": "Not a fan. First model I used was one of the qwen family and I didnâ€™t know how to end the conversation after a few turns so I typed â€œexitâ€. Cue 30 seconds of unnecessary paranoia from the model wondering what it did wrong.",
          "score": 1,
          "created_utc": "2026-02-03 20:25:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3et9pw",
              "author": "Savantskie1",
              "text": "Didnâ€™t happen, if you type exit in cli it doesnâ€™t get sent to the model",
              "score": 2,
              "created_utc": "2026-02-03 21:01:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3eyaez",
                  "author": "Copakela",
                  "text": "Maybe it was quit or something like that. It was a good while ago. It wasnâ€™t /bye anyway, I only found that out after.",
                  "score": 1,
                  "created_utc": "2026-02-03 21:24:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3w0nz1",
              "author": "TeachNo196",
              "text": "I did something like that once when I first started trying out local llms.  Made me chuckle.",
              "score": 1,
              "created_utc": "2026-02-06 12:14:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3g8k5n",
          "author": "sn0n",
          "text": "Instant gratification entitlement manâ€¦ I tell yaâ€¦ back in my day we had to get in the car, goto the library, hunt down the librarian, then hunt down the right bookâ€¦. And spend hours pouring through pagesâ€¦. And you bitch about letting your computer do it for you slowly while you carry about other tasks? Get outta here",
          "score": -1,
          "created_utc": "2026-02-04 01:25:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3entor",
          "author": "BidWestern1056",
          "text": "can't really stand them, and build a lot of products that don't really emphasize them:\n\n[https://github.com/npc-worldwide/npcpy](https://github.com/npc-worldwide/npcpy)\n\n[https://github.com/npc-worldwide/incognide](https://github.com/npc-worldwide/incognide)\n\n[https://github.com/npc-worldwide/npcsh](https://github.com/npc-worldwide/npcsh)",
          "score": -1,
          "created_utc": "2026-02-03 20:36:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0d432",
      "title": "Local AI for small company",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r0d432/local_ai_for_small_company/",
      "author": "LiteLive",
      "created_utc": "2026-02-09 19:02:02",
      "score": 19,
      "num_comments": 29,
      "upvote_ratio": 0.84,
      "text": "Hey guys,\n\nIâ€˜m looking into options to get local AI running for my company.\n\nWe do technical consulting and love to use AI for skimming through technical documents and pinpointing information down.\n\nWe are burning through Tokens and Iâ€˜m trying to save us some money but having local AI would actually allow us to use it on sensible data. Not all our customers allow cloud based AI assistance, because even when the providers say they donâ€™t train / store data, we cannot be certain.\n\nWhat do we want to do?\n\nI envision a paperless-ngx instance where we can upload a shitton of unsorted / unknown data. We have a solid promt\n\nThat categorizes the data and indexes the files. Allocates it to the right customer / project. And makes it accessible, searchable and tags them according to our need.\n\nRight now we use cloud providers to do this, but as I mentioned before we are burning through tokens. Especially in the beginning of projects when we digitalize a wheelbarrow full of hard copies folders.\n\nMy colleague said we should just buy a Mac mini and use that as an Ollama host, but I hate Apple with a passion (while writing this on an iPhoneâ€¦).\n\nI was looking at the Minisforum MS-S1 Max, hardware looks promising. I want to run Proxmox PVE 9 on it, then pass the GPU to the LXC where Ollama will reside.\n\nIs this a viable path? \n\nMy calculation is, if we spent 500â‚¬ on Tokens per month, and we can save half of that with this device, it would basically pay itself off within a year. And looking back at the last 12 months, I can see a steady increase in tokens for us. While enabling us to also process highly sensible data with AI.\n\nWhat models can I realistically run on this hardware? I was thinking something like Llama4:Maverik will probably work for us.\n\nWould you guys maybe recommend a different model for our â€žbackgroundâ€œ usecase? Are there other ways to streamline our workflow maybe?\n\nTo be fair I donâ€™t want to get rid of all cloud AI, as I fully understand that their models will always be more sophisticated and faster.\n\nLooking forward for you comments!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r0d432/local_ai_for_small_company/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4hfudy",
          "author": "ZeroSkribe",
          "text": "Why are you even mentioning proxmox... anyway you need a rtx 5090 or two, or the RTX 6000. Nvidia nemotron nano is a good option, but you can easily experiment. Use ollama.",
          "score": 9,
          "created_utc": "2026-02-09 19:21:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ibqit",
              "author": "LiteLive",
              "text": "What is the reason no to use Proxmox?\n\nI wanted to add it to our existing infrastructure and have a single pane for management.",
              "score": 3,
              "created_utc": "2026-02-09 22:00:06",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4i657c",
              "author": "StunningMouse1965",
              "text": "What is wrong with doing this with proxmox?",
              "score": 2,
              "created_utc": "2026-02-09 21:31:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ia473",
                  "author": "trolololster",
                  "text": "everything\n\nwhat he wants to do is much easier to host on a linux bare-metal server and a container stack for his project, and container stacks for other projects.",
                  "score": 4,
                  "created_utc": "2026-02-09 21:51:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hed9c",
          "author": "DieHard028",
          "text": "The size of your context will matter in deciding the best model for you.\n\nTry out IBM Granite and let me know if it helps.",
          "score": 5,
          "created_utc": "2026-02-09 19:14:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4id4vy",
              "author": "LiteLive",
              "text": "Can you elaborate please?\n\nLetâ€™s say I give 112GB to the LLM, keeping 16GB to the system.\n\nLlama4 takes 67GB leaving ~40GB to context.\n\nThe largest PDFâ€˜s we have are like 400-500 pages.\nEven if I load several of those â€žfolder scansâ€œ to the LLM, will I exceed the context?",
              "score": 1,
              "created_utc": "2026-02-09 22:07:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4iu0cp",
                  "author": "WolpertingerRumo",
                  "text": "No, that seems pretty good. Youâ€™ll have plenty of room for context with that.  Looking at your use case still try granite. Itâ€™s pretty small, but specifically trained on PDFs. The biggest is granite4:32b-a9b-h. Itâ€™ll be lightning fast on your set up.\n\nBut Iâ€™d still look into a good vector database framework. Iâ€˜ve been using openwebui together with Ollama, with great success. Itâ€™s a ChatGPT-like frontend with knowledge bases integrated. Basically it will scan your documents, cut them into chunks, size at your leisure, run a search which ones apply to your question, and only put those into context. With such large PDFs youâ€™d have to play around with a little setting called TopK inside the â€ždocumentsâ€œ settings, setting it very high. It sets how many of those chunks are loaded each time, depending on relevance.\n\nIâ€™m pretty sure openwebui is not state of the art anymore, but itâ€™s been working well and is quite flexible.",
                  "score": 3,
                  "created_utc": "2026-02-09 23:35:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4jgweh",
                  "author": "DieHard028",
                  "text": "That should be fine",
                  "score": 1,
                  "created_utc": "2026-02-10 01:46:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4i3lwh",
          "author": "st0ut717",
          "text": "Get over your hated of Apple and learn to think.\n\nThe man mini is the perfect tool for this job.    Other wise get a Dell gb10 or nvidia digx\n\nThe fact you want run it under a hypervisor tell me you really donâ€™t understand ai models.  A GPU with enough vram for production wil cost 3x a Mac mini",
          "score": 2,
          "created_utc": "2026-02-09 21:19:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ibbxf",
              "author": "LiteLive",
              "text": "Iâ€˜ll look into the Mac mini / studio option then.\n\nLooking at the GB10, I personally would think that \n\nI wanted to run it under Proxmox because thatâ€™s something 8â€˜m used to. We have a Proxmox Cluster for the remaining infrastructure and I was thinking to just add it in there. Not into the cluster but into the management backend.\n\nBut of the Mac requires little to no maintenance then it will be fine. itâ€˜s just not something Iâ€˜m used to and my previous Mac experience is, well letâ€™s say it was not pleasant for dem.",
              "score": 2,
              "created_utc": "2026-02-09 21:58:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4hkm92",
          "author": "Ryanmonroe82",
          "text": "You don't need a model that large to do what you are doing. Also your iPhone is extremely capable especially iPhone 17. \nCheck out RNJ-1 8b. The biggest think you need to get right is text extraction, chunking, and embeddings.  \nCheck out KilnAI and Easy Dataset on GitHub to start. \nTransformer Lab is another great one",
          "score": 1,
          "created_utc": "2026-02-09 19:44:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hn1ci",
          "author": "Alexious_sh",
          "text": "Consider that any user-grade GPU setup would get you about a single user concurrency usage. So, you'll have to either share one server and wait for any concurrent queries to complete or multiply your setups by the number of users. Or you could end up wasting time in a queue instead of burning tokens.",
          "score": 1,
          "created_utc": "2026-02-09 19:56:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4iew0t",
              "author": "LiteLive",
              "text": "The regular user content would still go through cloud providers, itâ€™s just super easy. I mainly want to cut down token costs for background tasks like I depicted. As it is a background task, we donâ€™t even care if it takes longer and or documents being queued.",
              "score": 2,
              "created_utc": "2026-02-09 22:16:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4hqv9h",
          "author": "AstroZombie138",
          "text": "I'd recommend testing the model you intend to run on something like ollama cloud or openrouter first and then deciding if it works well enough for your use cases.",
          "score": 1,
          "created_utc": "2026-02-09 20:16:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4if6c8",
              "author": "LiteLive",
              "text": "I just took a look into OpenRouter. Weâ€˜ll try the models there. We used ChatGPT and Anthropic Keys before.",
              "score": 1,
              "created_utc": "2026-02-09 22:17:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4hsmzb",
          "author": "DeepInEvil",
          "text": "Try things in hugging face spaces and see what works the best for you and then try to optimize and think about hardware etc",
          "score": 1,
          "created_utc": "2026-02-09 20:25:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4huoh2",
          "author": "Ok_Pizza_9352",
          "text": "You can host paperless on any old computer you want, and for AI node - either mac mini or minisforum. In case of minisforum - I'd recommend adding a GPU. I am using minisforum n5 pro with intel arc pro B50. For my needs more than enough. \nYou can selfhost n8n along with paperless, and build an automation in n8n (triggered by workflow in paperless) to do whatever it is you need AI to do automatically",
          "score": 1,
          "created_utc": "2026-02-09 20:35:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ig6s4",
              "author": "LiteLive",
              "text": "The workflow you mentioned with n8n is what we have in mind.\n\nThe minisforum I mentioned has a GPU that is tailored for AI use with UMD, like a Mac mini.\n\nPaperless and n8n will be hosted in dedicated VMâ€˜s on our Proxmox cluster.",
              "score": 1,
              "created_utc": "2026-02-09 22:22:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4iksml",
                  "author": "Ok_Pizza_9352",
                  "text": "Why not just containers in docker? Sounds like extra overhead for VMs \n\nI know it's got integrated GPU (which is usable with ollama, I guess up to 32gb ram can be assigned to gpu), but allegedly it's not as good as dedicated gpu. And the NPU - well that's currently only compatible with Windows 11 copilot. Guess will take another year or two till it's widely supported in linux\n\nAs for n8n workflows with selfhosted AI - imo best practice is to give AI narrow specific tasks. Selfhosted AI has way less parameters than cloud vendors. Better not to overwhelm it, and use smaller model, and have larger context window..",
                  "score": 0,
                  "created_utc": "2026-02-09 22:46:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4k4pfw",
          "author": "bourbonandpistons",
          "text": "What are the PDFs?\n\nDo you just need OCR on them cuz you can do a really lightweight OCR model and store everything to a database? Even vector for ai searches?",
          "score": 1,
          "created_utc": "2026-02-10 04:11:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kdr6a",
          "author": "PermanentLiminality",
          "text": "Do not buy hardware at this time.  Get an OpenRouter account and figure out which model will do what you need.  Once you have that settled, you can design your hardware that will run that model.",
          "score": 1,
          "created_utc": "2026-02-10 05:15:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ktjwo",
          "author": "BackUpBiii",
          "text": "You donâ€™t need anything. Download my ide from master and read itsmehrawrxd repo is RawrXD",
          "score": 1,
          "created_utc": "2026-02-10 07:29:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kvcic",
          "author": "AICodeSmith",
          "text": "This is a solid approach and something weâ€™ve seen work well. Weâ€™ve built similar local setups for document understanding and indexing to cut token usage and keep sensitive data on-prem, then selectively use cloud models only when needed. A hybrid local + cloud workflow usually gives the best balance. ",
          "score": 1,
          "created_utc": "2026-02-10 07:46:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hgnba",
          "author": "BisonMysterious8902",
          "text": "I think you may want to give your colleague's idea more credit. Apple is hard to beat when it comes to price and performance for local LLM's. \n\nA Mac Studio would likely server you better than a mini. Run it headless. Once you go through the initial OSX setup, install Ollama or LM Studio, and then its essentially running in the background. ",
          "score": 1,
          "created_utc": "2026-02-09 19:25:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jzio9",
              "author": "p_235615",
              "text": "I think those mini PCs with top Ryzen AI chips are really great sweet spot. They can run up to 120B models with decent speeds and cost less than half of a Mac Studio. \n\nLinux runs great on them, so it can be easily managed remotely via SSH. \n\nBut if inference speed is important for OP, there is no beating discrete GPUs...",
              "score": 3,
              "created_utc": "2026-02-10 03:38:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4hkstu",
              "author": "Responsible-Shake112",
              "text": "Mac mini or Mac Studio. The max you can afford to spend on it",
              "score": 0,
              "created_utc": "2026-02-09 19:45:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4hvlx8",
          "author": "BidWestern1056",
          "text": "you can do a good bit and use tools like npcpy\n\n[https://github.com/npc-worldwide/npcpy](https://github.com/npc-worldwide/npcpy)\n\n",
          "score": 0,
          "created_utc": "2026-02-09 20:40:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4i9usv",
          "author": "trolololster",
          "text": "i would probably recommend any other hypervisor than proxmox.\n\nyou have a lot of caveats running lxc/lxd on a proxmox - do also realise the difference between a fat and a slim container, proxmox exclusively uses fat containers...",
          "score": 0,
          "created_utc": "2026-02-09 21:50:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qv346v",
      "title": "Recommandation for a power and cost efficient local llm system",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qv346v/recommandation_for_a_power_and_cost_efficient/",
      "author": "Grummel78",
      "created_utc": "2026-02-03 20:23:53",
      "score": 18,
      "num_comments": 16,
      "upvote_ratio": 1.0,
      "text": "Hello everybody,\n\ni am looking power and cost efficient local llm system. especially when it is in idle. But i don't want wait minutes for reaction :-) ok ok i know that can not have everything :-)\n\nUse cases are the following:\n\n1. Using AI for Paperless-NGX (setting tags and ocr)\n\n2. Voice Assistant and automation in Home Assistant.\n\n3. Eventual Clawdbot\n\nAt the moment i tried Ai with the following setup:\n\nasrock n100m + RTX 3060 +32 GB Ram.\n\n  \nBut it use about 35 Watts in idle. I live in Germany with high energy cost. And for an 24/7 system it is too much for me. especially it will not be used every day. Paperless eventually every third day.  Voice Assistant and automation in Home Assistant 10-15 times per day.\n\nClawdbot i don't know.\n\nImportant for me is data stays at home (especially Paperless data).\n\nKnow i am thinking about a mac mini m4 base edition (16 gig unified ram and 256 ssd)\n\nHave somebody recommandations or experience with a mac mini and my use cases ?\n\nBest regards\n\nDirk\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qv346v/recommandation_for_a_power_and_cost_efficient/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3fdht4",
          "author": "prene1",
          "text": "Mac has the LLM low power in a chokehold",
          "score": 7,
          "created_utc": "2026-02-03 22:36:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fqo73",
          "author": "overand",
          "text": "In Germany, you probably pay \\~â‚¬0.40 per kilowatt hour. 35 watts, 24 hours a day , 30 days is a total of 25 kw/h.\n\nDo not spend â‚¬500 on a computer to lower your monthly electricity bill from â‚¬10 to â‚¬2.50. (Even if it cut your electricity bill to ZERO, it would take over 4 years to make back the cost. Realistically, at \\~8 watts vs \\~35 watts, you're saving, it's closer to 5 1/2 years)\n\nYou're spending less than â‚¬125 a *year* in electricity leaving your current setup on 24/7, if it's really 35 watts, and you're really at â‚¬0.40/kwh",
          "score": 5,
          "created_utc": "2026-02-03 23:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fvjzk",
          "author": "TheAussieWatchGuy",
          "text": "Wrong time ðŸ˜€ RAM prices have gone to the moon because of AI.\n\n\nThe most RAM and VRAM you can get is the answer. Mac M4 and Ryzen AI 395 both offer unified memory, so if you get 128gb of ddr5 ram you can allocate 112gb of it to the built in GPU.Â \n\n\nThat's pretty much the most cost effective way to run bigger models. 96gb in a pinch can be ok.Â \n\n\nOtherwise you can double down and get 64gb of ddr4 and a couple of 4090s, and a new PSU for your current PC, you'll spend more but Nvidia is still a bit easier to get local working well on.\n\n\nEither way you're spending a few grand.Â ",
          "score": 3,
          "created_utc": "2026-02-04 00:13:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3gkndk",
              "author": "ggone20",
              "text": "Not all unified memory is equal. \n\nOnly the Mac has true unified memory, all other systems just use it as marketing jargon with values needing to be set ahead of time performance hits for â€˜autoâ€™. \n\nSaid another way - Macs truly share the exact same memory, all other systems split a pool of memory.",
              "score": 2,
              "created_utc": "2026-02-04 02:33:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3h3ln9",
                  "author": "TheAussieWatchGuy",
                  "text": "Technically true. Mac is king here but with a price tag to match.\n\n\nFor the OP if budget is a concern the Ryzen AI platform is cost effective and fast enough.Â ",
                  "score": 1,
                  "created_utc": "2026-02-04 04:27:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3hcb56",
                  "author": "Zyj",
                  "text": "Have you looked at GTT memory on AMD AI Max+? I guess not.",
                  "score": 1,
                  "created_utc": "2026-02-04 05:29:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ywfkz",
                  "author": "GeroldM972",
                  "text": "What makes certain Macs better is the memory bandwidth with which the unified memory modules inside those chips can communicate. \n\nRegarding the assignment of RAM/VRAM in BIOS/UEFI on PC processors with Unified memory, I like that. That way I am sure my operating system can't overwrite something accidentally in the loaded LLM or the other way around.\n\nIf I had the money, the M3 Mac Studio from 10.000 USD (512 GB unified RAM) is the only true interesting Mac I would want to use for local LLMs. All the other M3 Mac Studios with less RAM have too much of a wrong price/performance ratio to me.\n\nThen I rather spend (way less) on those AMD Strix Halo processors with their Unified RAM.",
                  "score": 1,
                  "created_utc": "2026-02-06 21:00:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3gft6u",
          "author": "Royale_AJS",
          "text": "Youâ€™re looking for an AMD Strix Halo, a Mac, or a DGX Spark. The Mac is super efficient, my Strix Halo idles at 9 watts and I canâ€™t get it to pull more than 140 out of the wall.",
          "score": 2,
          "created_utc": "2026-02-04 02:06:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eymn9",
          "author": "GlassAd7618",
          "text": "Any specific model you have in mind for Paperless-ngx and voice automation? What runtime are you thinking about (e.g., Ollama or lmstudio)? I could run some tests on my Mac mini. (I'm in Germany too, so it would be interesting to see how smooth selected models run on a 16GB Mac mini to avoid a hefty electricity bill...)",
          "score": 1,
          "created_utc": "2026-02-03 21:26:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gnhi0",
          "author": "ServeAlone7622",
          "text": "LFM 2.5 seems optimized for this use case.",
          "score": 1,
          "created_utc": "2026-02-04 02:49:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hhpcx",
          "author": "gamesta2",
          "text": "4nm. Ryzen. my 9700x is pretty much identical to 13600k in performance, but draws half the power.",
          "score": 1,
          "created_utc": "2026-02-04 06:10:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxrl4o",
      "title": "Qwen3-ASR Swift: On-Device Speech Recognition for Apple Silicon",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qxrl4o/qwen3asr_swift_ondevice_speech_recognition_for/",
      "author": "ivan_digital",
      "created_utc": "2026-02-06 19:38:03",
      "score": 15,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I'm excited to releaseÂ [https://github.com/ivan-digital/qwen3-asr-swift](https://github.com/ivan-digital/qwen3-asr-swift), an open-source Swift implementation of Alibaba'sÂ   \nQwen3-ASR, optimized for Apple Silicon using MLX.Â \n\nWhy Qwen3-ASR? Exceptional noise robustness â€” 3.5x better than Whisper in noisy conditions (17.9% vs 63% CER).Â \n\nFeatures:Â   \n\\- 52 languages (30 major + 22 Chinese dialects)Â   \n\\- \\~600MB model (4-bit quantized)Â   \n\\- \\~100ms latency on M-series chipsÂ   \n\\- Fully local, no cloud APIÂ \n\n[https://github.com/ivan-digital/qwen3-asr-swift](https://github.com/ivan-digital/qwen3-asr-swift)Â | Apache 2.0",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qxrl4o/qwen3asr_swift_ondevice_speech_recognition_for/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qzrd8g",
      "title": "DaveLovable is an open-source, AI-powered web UI/UX development platform",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/",
      "author": "LeadingFun1849",
      "created_utc": "2026-02-09 01:54:32",
      "score": 15,
      "num_comments": 5,
      "upvote_ratio": 0.9,
      "text": "I've been working on this project for a while.\n\nDaveLovable is an open-source, AI-powered web UI/UX development platform, inspired by Lovable, Vercel v0, and Google's Stitch. It combines cutting-edge AI orchestration with browser-based execution to offer the most advanced open-source alternative for rapid frontend prototyping.\n\nGithub :Â [https://github.com/davidmonterocrespo24/DaveLovable](https://github.com/davidmonterocrespo24/DaveLovable)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4f3bxm",
          "author": "newbietofx",
          "text": "Nice. Can I fork it? What do you hope to achieve?Â ",
          "score": 1,
          "created_utc": "2026-02-09 11:52:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4f81w9",
              "author": "LeadingFun1849",
              "text": "Yes, you can fork it. The next step is for the system to also create a backend with Firebase or some open-source alternative.",
              "score": 1,
              "created_utc": "2026-02-09 12:29:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4f9q0a",
                  "author": "newbietofx",
                  "text": "I have a youtube channel. Only 107 subscribers. I see if ii have a topic for this. It will be great if there is an sdk to integrate to aws. Aws has cdk.Â ",
                  "score": 1,
                  "created_utc": "2026-02-09 12:41:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4m9t1o",
          "author": "LeadingFun1849",
          "text": "https://preview.redd.it/b96pijjyeoig1.png?width=1388&format=png&auto=webp&s=005e80c537454ff882c508381c957b2679011f44\n\nIf you find it interesting, Iâ€™d really appreciate it if you could check out the GitHub repo and give it aItâ€™s free and would help me a lot.",
          "score": 1,
          "created_utc": "2026-02-10 14:20:35",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyrotc",
      "title": "Releasing 1.22. 0 of Nanocoder - an update breakdown ðŸ”¥",
      "subreddit": "ollama",
      "url": "https://v.redd.it/t790s2gjg5ig1",
      "author": "willlamerton",
      "created_utc": "2026-02-07 22:37:49",
      "score": 14,
      "num_comments": 4,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qyrotc/releasing_122_0_of_nanocoder_an_update_breakdown/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o47mn0y",
          "author": "drakgremlin",
          "text": "Does this support local models?Â  If so, what models do you recommend?",
          "score": 2,
          "created_utc": "2026-02-08 06:21:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49egew",
              "author": "willlamerton",
              "text": "Absolutely, it supports local models as a priority and thereâ€™s a big focus on improving the scaffolding around small models to make them better!\n\nWeâ€™re getting there but at the moment a big recommendation comes with the Mistral models. If you can run Devstral Small 2 then thatâ€™s great for 24B parameters. As is Nemotron Nano from Nvidia. Iâ€™m also a fan of the 8B and 14B flavours of the Ministral models. Set your expectations but they can certainly help with smaller coding tasks and codebase exploration.\n\nThey all work great through Ollama local and cloud :)",
              "score": 2,
              "created_utc": "2026-02-08 15:00:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4aqmjf",
                  "author": "drakgremlin",
                  "text": "Took it out for a whirl with mistral-3:3b !Â  Looks promising but stops every step to ask for confirmation.Â  Is this a feature or would this be alleviated by using 8b ?",
                  "score": 1,
                  "created_utc": "2026-02-08 18:54:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qwwkip",
      "title": "Built a self-hosted execution control layer for local LLM workflows (works with Ollama)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qwwkip/built_a_selfhosted_execution_control_layer_for/",
      "author": "saurabhjain1592",
      "created_utc": "2026-02-05 20:28:01",
      "score": 13,
      "num_comments": 0,
      "upvote_ratio": 0.93,
      "text": "Hey folks. I am building AxonFlow, a self-hosted, source-available execution control layer for local LLM workflows once they move beyond single prompts and touch real systems.\n\nThe hard part was not model quality. It was making execution visible and controllable:\n\n* clear boundaries around what steps are allowed to run\n* logs tied to decisions and actions, not just model outputs\n* the ability to inspect and replay a run when something goes wrong\n\nRetries and partial failures still mattered, but only after we could see and control what happened in a run.\n\nAxonFlow sits inline between your workflow logic and LLM tool calls to make execution explicit. It is not an agent framework or UI platform. It is the runtime layer teams end up building underneath once local workflows get serious.\n\nWorks with Ollama by pointing the client to a local endpoint.  \nGitHub:Â [https://github.com/getaxonflow/axonflow](https://github.com/getaxonflow/axonflow)\n\nWould love feedback from folks running Ollama in real workflows.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qwwkip/built_a_selfhosted_execution_control_layer_for/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qvnpp7",
      "title": "Using Ollama for a real-time desktop assistant â€” latency vs usability tradeoffs?",
      "subreddit": "ollama",
      "url": "https://v.redd.it/n32j33l45hhg1",
      "author": "Ore_waa_luffy",
      "created_utc": "2026-02-04 12:50:21",
      "score": 13,
      "num_comments": 6,
      "upvote_ratio": 0.76,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qvnpp7/using_ollama_for_a_realtime_desktop_assistant/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3lsq4u",
          "author": "TheAndyGeorge",
          "text": "why you promoting [interview cheating](https://www.reddit.com/r/OpenAI/comments/1qvnrht/cheat_interviews_easily/) ?",
          "score": 3,
          "created_utc": "2026-02-04 21:34:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rupbm",
              "author": "Ore_waa_luffy",
              "text": "Im not promoting it , there are literally tools that people pay 30 dollars or so , I'm just making it free",
              "score": 1,
              "created_utc": "2026-02-05 19:45:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3o3enm",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-05 05:28:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3o88j3",
              "author": "Ore_waa_luffy",
              "text": "Oh Lemme try that . What i did was i used whisper small for ui transcription so the user can feel itâ€™s being transcribed and use whisper medium for actual transcription but the latency was like few seconds which i didnâ€™t like personally",
              "score": 1,
              "created_utc": "2026-02-05 06:06:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3khmh8",
          "author": "amgir1",
          "text": "I have 5070 ti, and my olama have been typing realy slow. It produces a lot of text but I don't trust it, too many words, too many time to type answer",
          "score": 0,
          "created_utc": "2026-02-04 17:54:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kln97",
              "author": "Ore_waa_luffy",
              "text": "Yes in my app i have given preference to google stt if anyone cant pay then can use dual pipeline  whisper quick and medium",
              "score": 0,
              "created_utc": "2026-02-04 18:13:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r0n1sg",
      "title": "We won a hackathon with this project using Ollama. But is it actually useful?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r0n1sg/we_won_a_hackathon_with_this_project_using_ollama/",
      "author": "BriefAd2120",
      "created_utc": "2026-02-10 01:25:10",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 0.91,
      "text": "TLDR: I built a 3d memory layer to visualize your chats with a custom MCP server to inject relevant context, Looking for feedback!\n\nCortex turns raw chat history into reusable context using hybrid retrieval (about 65% keyword, 35% semantic), local summaries with Qwen 2.5 8B, and auto system prompts so setup goes from minutes to seconds.\n\nIt also runs through a custom MCP server with search + fetch tools, so external LLMs like Claude can pull the right memory at inference time.\n\nAnd because scrolling is pain, I added a 3D brain-style map built with UMAP, K-Means, and Three.js so you can explore conversations like a network instead of a timeline.\n\nWe won the hackathon with it, but I want a reality check: is this actually useful, or just a cool demo?\n\nYouTube demo: [https://www.youtube.com/watch?v=SC\\_lDydnCF4](https://www.youtube.com/watch?v=SC_lDydnCF4)\n\nLinkedIn post: [https://www.linkedin.com/feed/update/urn:li:activity:7426518101162205184/](https://www.linkedin.com/feed/update/urn:li:activity:7426518101162205184/)\n\nGithub Link (pls star itðŸ¥º): [https://github.com/Vibhor7-7/Cortex-CxC](https://github.com/Vibhor7-7/Cortex-CxC)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r0n1sg/we_won_a_hackathon_with_this_project_using_ollama/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qzzd5x",
      "title": "Qwen 3 coder next for R coding (academic)",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qzzd5x/qwen_3_coder_next_for_r_coding_academic/",
      "author": "Bahaal_1981",
      "created_utc": "2026-02-09 09:01:29",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 0.91,
      "text": "I am an academic. I have claude via work and it excels at R coding and building Shiny apps with little prompting (Opus 4.5 but Sonnet does fairly well also). This is both for teaching / research. But I also want local models (for various reasons, privacy, reproducibility, etc). I have ollama with cohere / Mistral Large / phi reasoning, running on an M4 Max with 128 gb ram. Reading up I think qwen coder next might do better:\n\n[https://ollama.com/library/qwen3-coder-next](https://ollama.com/library/qwen3-coder-next) \\--> 85GB model -- additional settings needed?\n\n  \nI also looked for Kimi but could only find the cloud version. Any advice? Many thanks",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qzzd5x/qwen_3_coder_next_for_r_coding_academic/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r0q55x",
      "title": "any good models?",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1r0q55x/any_good_models/",
      "author": "No-Mortgage4154",
      "created_utc": "2026-02-10 03:43:05",
      "score": 7,
      "num_comments": 14,
      "upvote_ratio": 0.71,
      "text": "So i recently found out about ollama and how its like a local ai and was wondering what are some good models out there my pc specs are: ryzen 7 7800x3d, 4070ti super nvidia, and ddr5 32gb ram.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r0q55x/any_good_models/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o4kafsm",
          "author": "p_235615",
          "text": "I quite like ministral-3:14b from dense models (or its thinking or instruct variants) - really great all around model, also supports vision. \nBut if you prefer a bit more speed, then gpt-oss:20b should also fit in 16GB. I use those two probably the most. \n\nIf speed is not a concern, you can also run some of the ~30B models like qwen3-coder, glm-4.7-flash, nemotron-3-nano. Those will be partially offloaded to RAM, but will probably still do 20-30 tokens/s on your system.",
          "score": 5,
          "created_utc": "2026-02-10 04:51:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kpyrg",
              "author": "StvDblTrbl",
              "text": "I support this. Even ministral-3:8b is surprisingly good for my case. Plus really good with tools. Really unexpected. ",
              "score": 3,
              "created_utc": "2026-02-10 06:56:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n0qvj",
                  "author": "p_235615",
                  "text": "Yes, I use it on my isolated work macbook, and its surprisingly good for python coding with cline in VScode. But right now also testing huihui-moe-abliterated:12b, which seems to be really great so far, but only tested for few hours.",
                  "score": 1,
                  "created_utc": "2026-02-10 16:32:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4mj0mw",
              "author": "XxCotHGxX",
              "text": "Have you tried any with OpenClaw? I haven't had good success with local models. Always tool call errors. I tried Devstral, but not ministral",
              "score": 1,
              "created_utc": "2026-02-10 15:08:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n0b24",
                  "author": "p_235615",
                  "text": "I use ministral-3:8b-instruct on my work macbook with VScode (due to strict policies, you cant send stuff out to internet) and tried the ministral models with opencode, of course not large context, but for the size they work surprisingly well for tool calling and python coding.",
                  "score": 1,
                  "created_utc": "2026-02-10 16:30:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lj135",
          "author": "tim610",
          "text": "Hi, I created [WhatModelsCanIRun.com](https://whatmodelscanirun.com) where you can plug in your GPU, and see what models will fit in your VRAM with estimates of token generation speed. I'm continuing to work on it so it will improve over time!",
          "score": 3,
          "created_utc": "2026-02-10 11:29:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4k2tzv",
          "author": "XxAnomo305",
          "text": "12b models for fastest output will fit in gpu, max you can run around 32b (slow). and for \"good\" models there is hundreds for different models. pick what you want it for and I can give you suggestions for models.",
          "score": 1,
          "created_utc": "2026-02-10 03:59:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4k4gpj",
              "author": "No-Mortgage4154",
              "text": "I want a model for like coding and just chatting about stuff",
              "score": 1,
              "created_utc": "2026-02-10 04:10:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4k4z8l",
                  "author": "XxAnomo305",
                  "text": "qwen2.5 14b would be great for coding, and for chatting lamma 3.1 or phi3.",
                  "score": 2,
                  "created_utc": "2026-02-10 04:13:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4kylqw",
          "author": "Mount_Gamer",
          "text": "I have the 5060ti, 5650g pro, 32GB ddr4 ecc 2666 ram (slow by today's standards...). I only give this VM 8 threads, 20GB RAM and the 5060ti.\n\nI get about 55t/s with llama.cpp using qwen3 coder 30B A3 and nemotron nano 30B A3, both Quant are Q4, and both context I've given 50k.\n\nI have not tried running it through ollama yet, but thought I'd share since these models are pretty good for their size.\n\nHowever, when things get a bit complicated I end up model swapping, and even the bigger models don't always get it right, but since ollama's subscription offers gemini flash and pro, I seem to notice these models handling more complex tasks better, but there are so many models and another might work better for your use case.",
          "score": 1,
          "created_utc": "2026-02-10 08:16:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l086z",
          "author": "Fiskepudding",
          "text": "gpt-oss, qwen3, glm-4.7-flash, gemma3.\n\n\nMake sure you enable flash attention, quantized kv cache, and set a decent context size.",
          "score": 1,
          "created_utc": "2026-02-10 08:32:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l0gif",
              "author": "Fiskepudding",
              "text": "qwen3-coder-next is hype right now, but I havent been able to test it. it requires heavy quantization because it is a bit big",
              "score": 1,
              "created_utc": "2026-02-10 08:35:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4n28do",
              "author": "Civil_Breakfast9998",
              "text": "How do you enable flash attention in ollama?Â ",
              "score": 1,
              "created_utc": "2026-02-10 16:39:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n8t2z",
          "author": "Mustard_Popsicles",
          "text": "Iâ€™m a fan of Gemma3:12b, gpt-oss 20b is ok but it basically maxes out my 16gb of vram. Plus itâ€™s a thinking model and it takes forever to answer any basic prompt x",
          "score": 1,
          "created_utc": "2026-02-10 17:09:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r07fwp",
      "title": "Izwi - A local audio inference engine written in Rust",
      "subreddit": "ollama",
      "url": "https://github.com/agentem-ai/izwi",
      "author": "zinyando",
      "created_utc": "2026-02-09 15:39:06",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1r07fwp/izwi_a_local_audio_inference_engine_written_in/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o4jlotl",
          "author": "tedstr1ker",
          "text": "Whatâ€™s the use case?",
          "score": 1,
          "created_utc": "2026-02-10 02:14:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qv5cgs",
      "title": "AI Context as Code: Can structured docs improve AI resource usage and performance?",
      "subreddit": "ollama",
      "url": "https://github.com/eFAILution/AICaC",
      "author": "eFAILution",
      "created_utc": "2026-02-03 21:46:51",
      "score": 6,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qv5cgs/ai_context_as_code_can_structured_docs_improve_ai/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3urmlm",
          "author": "GlassAd7618",
          "text": "I think youâ€™re on to something. Thinking about the example in your Git repository (adding a new scanner), your idea probably works best for mature code bases which already have an established architecture and where there is some functionality that is repeatedly extended, such as adding a new scanner, filter, reader, â€¦",
          "score": 2,
          "created_utc": "2026-02-06 05:38:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w5yej",
              "author": "eFAILution",
              "text": "Totally agree! Also, donâ€™t sleep on what it can do for onboarding. Instead of new devs asking senior engineers or reading thousands of lines, their AI assistant already knows â€œhow we do things here.â€\nStructured context = their coding AI tool actually understands your conventions from day one.",
              "score": 2,
              "created_utc": "2026-02-06 12:50:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3np2b0",
          "author": "_RemyLeBeau_",
          "text": "You don't need AI, when you have a specification. You need a workflow engine.\n\nAI is great because it understands NLQ.",
          "score": 1,
          "created_utc": "2026-02-05 03:50:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nqoro",
              "author": "eFAILution",
              "text": "Good point. AICaC isnâ€™t for task execution as workflow engines are better there. Itâ€™s for reducing token waste when AI needs to understand your codebase before helping.\nThe NLQ strength of AI is why it needs efficient context. â€œHow do I add auth?â€ shouldnâ€™t cost 800 tokens parsing a README when 200 tokens of structured data would work.\nWhatâ€™s your approach for giving AI project context efficiently?â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹",
              "score": 1,
              "created_utc": "2026-02-05 04:01:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3o99lx",
                  "author": "_RemyLeBeau_",
                  "text": "Exactly what vercel is doing and has 100% evals. Do you have evals to share with your newly created DSL?",
                  "score": 1,
                  "created_utc": "2026-02-05 06:15:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qv4312",
      "title": "Ollama and Openclaw on separate dedicated, isolated, firewalled machines",
      "subreddit": "ollama",
      "url": "https://www.reddit.com/r/ollama/comments/1qv4312/ollama_and_openclaw_on_separate_dedicated/",
      "author": "timbo2m",
      "created_utc": "2026-02-03 21:00:04",
      "score": 6,
      "num_comments": 7,
      "upvote_ratio": 0.67,
      "text": "Anyone get this to work? \n\nMachine 1 is a dedicated Openclaw Mac laptop with outbound internet access.\n\nMachine 2 is a dedicated ollama server sharing various models.\n\nBoth machines are on the same subnet. A quick check shows Openclaw machine can see the models list on the ollama server. \n\nOnce Openclaw has been through onboarding it does not respond to any chat requests. I think maybe this could work with some extra messing around v\n\nSo while it should work, the test responses are all empty as if it's Openclaw can't communicate properly with ollama. \n\nEDIT: I found this https://github.com/openclaw/openclaw/issues/2838 so will see if some of those comments help",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/ollama/comments/1qv4312/ollama_and_openclaw_on_separate_dedicated/",
      "domain": "self.ollama",
      "is_self": true,
      "comments": [
        {
          "id": "o3ly4df",
          "author": "robonova-1",
          "text": "Yes, I have that working. If you are trying to use a local LLM as the main brain for OpenClaw you will not be happy because they simply don't have the advanced reasoning capabilities of the frontier models. You need to have the main brain using a large online model and then use a local LLM as a subagent doing grunt work and that keeps the costs down. There are TONS of YouTube videos floating around right now that show how to do this.",
          "score": 3,
          "created_utc": "2026-02-04 21:59:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lyx7k",
              "author": "timbo2m",
              "text": "Yeah the intent is to have opus as the brain to decide what to work on, but it delegates work to my local models.  I am using various local models and have qwen 3 next working at ~32 tokens per second with llama.cpp via llama-server.  The local coders will be checked by pr-agent hooked up to Kimi 2.5 and iterated before any code is committed.",
              "score": 2,
              "created_utc": "2026-02-04 22:03:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ifcxs",
          "author": "timbo2m",
          "text": "Ok so I dumped ollama for llama.cpp and it's working",
          "score": 2,
          "created_utc": "2026-02-04 11:15:07",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o3iq2o0",
          "author": "nosimsol",
          "text": "I couldnâ€™t even get open claw at work on the same machine with ollama. The set up process seem to want to force me to use an external provider which I didnâ€™t want to set up and I didnâ€™t see a way around it, and it just seemed to create problems.",
          "score": 2,
          "created_utc": "2026-02-04 12:36:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ja5ok",
              "author": "SaltyUncleMike",
              "text": "its a huge PITA.  I was able to get it work by starting clawdbot through ollama itself.\n\neg:\nConfig Only: Use ollama launch clawdbot --config if you want to set up the connection without starting the service immediately.",
              "score": 1,
              "created_utc": "2026-02-04 14:30:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lnurr",
          "author": "reddefcode",
          "text": "Given the GitHub issue you found, I'd recommend double-checking the Ollama server's API configuration and ensuring OpenClaw is pointing to the correct endpoint. Sometimes, firewall rules or app permissions on the Mac might be blocking the actual chat requests, even if listing works.",
          "score": 1,
          "created_utc": "2026-02-04 21:11:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4adqkx",
          "author": "Acrobatic_Task_6573",
          "text": "Yeah this works. Running a similar setup with a Mac mini for OpenClaw and Ollama on a separate Linux box.\n\nTwo things that tripped me up:\n\n1. Make sure you set OLLAMA_HOST=0.0.0.0 on the Ollama server. By default it only listens on localhost so even though you can see the models list, the actual inference calls might fail silently.\n\n2. In your openclaw.json, the Ollama provider URL needs to point to the full address including port. Something like http://192.168.1.x:11434. Do not forget the /v1 path if your config expects OpenAI compatible endpoints.\n\nThe empty responses thing is usually a context window issue. Smaller models choke on the system prompt if the context is too small. Try bumping it up in your modelfile or use a model that handles larger contexts natively like qwen2.5 14b.\n\nAlso worth checking: are you running the model with enough VRAM? If it is swapping to CPU mid inference, it can timeout and return empty.",
          "score": 1,
          "created_utc": "2026-02-08 17:54:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}