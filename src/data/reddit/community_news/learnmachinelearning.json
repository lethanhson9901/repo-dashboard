{
  "metadata": {
    "last_updated": "2026-01-19 02:39:57",
    "time_filter": "week",
    "subreddit": "learnmachinelearning",
    "total_items": 20,
    "total_comments": 194,
    "file_size_bytes": 191066
  },
  "items": [
    {
      "id": "1qb9jpa",
      "title": "convolutional neural network from scratch in js",
      "subreddit": "learnmachinelearning",
      "url": "https://v.redd.it/6h1g7zzovzcg1",
      "author": "Ok-Statement-3244",
      "created_utc": "2026-01-12 22:31:58",
      "score": 825,
      "num_comments": 24,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qb9jpa/convolutional_neural_network_from_scratch_in_js/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nz9tup2",
          "author": "singinggiraffe",
          "text": "This is amazing!\n1. What motivated you to do this?\n2. How did you learn about all these layers in such detail?\n3. What do you do as a job? How did you become so fluent in WebGL?",
          "score": 49,
          "created_utc": "2026-01-13 01:32:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzadsf4",
              "author": "Ok-Statement-3244",
              "text": "1. Satanism  \n2. Internet  \n3. Unemployed. Practice.",
              "score": 106,
              "created_utc": "2026-01-13 03:18:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzbqfg7",
                  "author": "singinggiraffe",
                  "text": "Ofc, satanism! C'mon, I was actually curious about your main sources, especially the WebGL part, but alright. I really enjoyed the project.",
                  "score": 6,
                  "created_utc": "2026-01-13 09:34:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzb8wk1",
                  "author": "paul_tu",
                  "text": "Nice looking demo btw",
                  "score": 3,
                  "created_utc": "2026-01-13 06:49:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzg0zdd",
                  "author": "Palmquistador",
                  "text": "This should take care of number 3.",
                  "score": 1,
                  "created_utc": "2026-01-13 23:34:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzirn9n",
                  "author": "InterenetExplorer",
                  "text": "Any sources you followed not just for training but also architecture setup and for visualizations?",
                  "score": 1,
                  "created_utc": "2026-01-14 11:19:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz9wf6a",
              "author": "modcowboy",
              "text": "That WebGL question is the big one. \n\nThat skill is worth way more than training a CNN by hand.",
              "score": 14,
              "created_utc": "2026-01-13 01:46:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz99bzw",
          "author": "Fluffy_Garlic_6759",
          "text": "Now thatâ€™s cool as fuck",
          "score": 23,
          "created_utc": "2026-01-12 23:40:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbbc3d",
          "author": "Shekher_05",
          "text": "How did you make a visual representation of it ?",
          "score": 11,
          "created_utc": "2026-01-13 07:10:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbc4tg",
              "author": "appy_j",
              "text": "Exactly my question too â€¦",
              "score": 5,
              "created_utc": "2026-01-13 07:17:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzbxfwx",
              "author": "Former-House-8382",
              "text": "From my understanding :the visual representation is just an animation showing the process but it goes way faster to run and execute the model",
              "score": 5,
              "created_utc": "2026-01-13 10:40:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzhxi41",
              "author": "HasFiveVowels",
              "text": "One voxel at a time.",
              "score": 1,
              "created_utc": "2026-01-14 06:40:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz8wrfo",
          "author": "Cybyss",
          "text": "CNNs are easy. \n\nI'm thoroughly impressed, however, by that neat UI you created to visualize everything. Nicely done!",
          "score": 52,
          "created_utc": "2026-01-12 22:35:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz9qqnr",
          "author": "homezlice",
          "text": "very nice, +1",
          "score": 3,
          "created_utc": "2026-01-13 01:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzc1dbd",
          "author": "Better_Pair_4608",
          "text": "I saw the similar post here about a month ago. Was that you?",
          "score": 3,
          "created_utc": "2026-01-13 11:14:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzc9t8p",
          "author": "yaksnowball",
          "text": "Beautiful UI, well done",
          "score": 1,
          "created_utc": "2026-01-13 12:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfhfi0",
          "author": "Wroisu",
          "text": "If I wanted to learn how to do this, where would I start ?",
          "score": 1,
          "created_utc": "2026-01-13 21:56:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhxnxl",
              "author": "HasFiveVowels",
              "text": "I'd start by learning how to position a cube in 3-space. Then learn (if you don't already know) how to make a convolutional neural network. Then associate each node with a cube and draw lines etc.",
              "score": 2,
              "created_utc": "2026-01-14 06:42:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzfy8p3",
          "author": "Context_Core",
          "text": "lol $300 lappy wappy\n\nThis is awesome",
          "score": 1,
          "created_utc": "2026-01-13 23:20:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzio4gb",
          "author": "Mjrem",
          "text": "The UI is impressive, the CNN identify 9 as 3 sometimes as 2 , also 7 and 0",
          "score": 1,
          "created_utc": "2026-01-14 10:49:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqpdes",
          "author": "Navyoki",
          "text": "thats so coool!",
          "score": 1,
          "created_utc": "2026-01-15 15:22:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqpewn",
          "author": "Navyoki",
          "text": "that's so coool!",
          "score": 1,
          "created_utc": "2026-01-15 15:23:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzylwzz",
          "author": "Grestige",
          "text": "wow. Saw a YT video of a guy do something similar in scratch. But this is way cooler",
          "score": 1,
          "created_utc": "2026-01-16 17:57:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04ihi6",
          "author": "giadev",
          "text": "Are you crazy or what? how long you made this project?",
          "score": 1,
          "created_utc": "2026-01-17 16:08:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qclstb",
      "title": "TensorFlow isn't dead. Itâ€™s just becoming the COBOL of Machine Learning.",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qclstb/tensorflow_isnt_dead_its_just_becoming_the_cobol/",
      "author": "IT_Certguru",
      "created_utc": "2026-01-14 12:17:24",
      "score": 407,
      "num_comments": 41,
      "upvote_ratio": 0.93,
      "text": "I keep seeing \"Should I learn TensorFlow in 2026?\" posts, and the answers are always \"No, PyTorch won.\"\n\nBut looking at the actual enterprise landscape, I think we're missing the point.\n\n1. Research is over: If you look at , PyTorch has essentially flatlined TensorFlow in academia. If you are writing a paper in TF today, you are actively hurting your citation count.\n2. The \"Zombie\" Enterprise: Despite this, 40% of the Fortune 500 job listings I see still demand TensorFlow. Why? Because banks and insurance giants built massive TFX pipelines in 2019 that they refuse to rewrite.\n\nMy theory: TensorFlow is no longer a tool for innovation; itâ€™s a tool for maintenance. If you want to build cool generative AI, learn PyTorch. If you want a stable, boring paycheck maintaining legacy fraud detection models, learn TensorFlow.\n\nIf anyoneâ€™s trying to make sense of this choice from a practical, enterprise point of view, this breakdown is genuinely helpful: [**PyTorch vs TensorFlow**](https://www.netcomlearning.com/blog/pytorch-vs-tensorflow-enterprise-guide)\n\nAm I wrong? Is anyone actually starting a greenfield GenAI project in raw TensorFlow today?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qclstb/tensorflow_isnt_dead_its_just_becoming_the_cobol/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "nzj1gyv",
          "author": "Mithrandir2k16",
          "text": "Tensorflows tooling for deployments has been, and still in small parts is, more mature than PyTorch. At some point you'll want to switch from rapid innovation (which PyTorch excels at) to production, and Tensorflow has traditionally beaten PyTorch in that regard. The gap has closed significantly thanks to e.g. ONNX, but it still exists.",
          "score": 154,
          "created_utc": "2026-01-14 12:34:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjdkg2",
              "author": "pm_me_your_smth",
              "text": "Could you expand more on which aspects of deployment tensorflow does better?",
              "score": 29,
              "created_utc": "2026-01-14 13:47:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjhd53",
                  "author": "Mithrandir2k16",
                  "text": "TFLite, TF Serving, TFX, TFDV, etc. For a lot of MLOps stuff, PyTorch needs external tooling, which is an advantage for moving rapidly, but also more brittle in my experience.\n\nAnd then there's some edge-computing or integrated hardware stuff, like nVidia specifically supporting Tensorflow for their Jetson plattforms.",
                  "score": 47,
                  "created_utc": "2026-01-14 14:08:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzk2v5j",
          "author": "WearMoreHats",
          "text": "> 40% of the Fortune 500 job listings I see still demand TensorFlow\n\nOf that 40%, what percentage *also* mention PyTorch in the listing? Anecdotally, when I see TF on a job listing it's normally part of a list of deep learning terms/tech and is being used as shorthand for \"do you have experience working with neural networks\". I'm pretty skeptical of the idea that there's a huge market for maintaining old TF models.\n\nAnyway, this is spam and probably a bot. OP posts loads of these \"thoughts\" each day, all with a handy link to the same website (which happens to sell very expensive courses).",
          "score": 35,
          "created_utc": "2026-01-14 15:54:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzj4wq1",
          "author": "suedepaid",
          "text": "The reason you see it in job listings still is because hiring managers havenâ€™t caught on that tensorflow is dead.\n\nItâ€™s not the cobol of ML because ML systems donâ€™t have a particularly long shelf-life. People are actively ripping out their TF stuff right now.\n\nMore and more of the â€œTF has better productionâ€ stuff is getting eaten by ONNX bindings from traditional backend languages.\n\nPlus googleâ€™s gonna end-of-life TF, and then what do you do.",
          "score": 80,
          "created_utc": "2026-01-14 12:56:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjappy",
              "author": "_bez_os",
              "text": "True. These hiring managers are dumbass.",
              "score": 16,
              "created_utc": "2026-01-14 13:32:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlob19",
                  "author": "Exotic-Tooth8166",
                  "text": "Hiring managers should have a hiring manager for hiring hiring managers who hire managers with hiring management experience in a modern hiring management technical environment for hiring managers",
                  "score": 4,
                  "created_utc": "2026-01-14 20:13:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzj4xig",
          "author": "darklinux1977",
          "text": "PyTorch also has the advantage of an integrated end-to-end ecosystem, and all this in a short amount of time.",
          "score": 8,
          "created_utc": "2026-01-14 12:57:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzj3q18",
          "author": "CuriousAIVillager",
          "text": " What about Jax?",
          "score": 13,
          "created_utc": "2026-01-14 12:49:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzj6r79",
              "author": "CircularPR",
              "text": "Jax is the succesor of TF. I think in the long run it will beat PyTorch because it can be used for other things as well and its \"accelator first\" approach is great to run things on GPUs and TPUs",
              "score": 24,
              "created_utc": "2026-01-14 13:08:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjkjdh",
                  "author": "RobbinDeBank",
                  "text": "Donâ€™t think it will beat PyTorch because it is significantly harder to approach. Ease of use is a major contributor to popularity of a tool/framework.",
                  "score": 11,
                  "created_utc": "2026-01-14 14:25:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzjf5n4",
                  "author": "nickpsecurity",
                  "text": "Open-source tooling that can convert between the two would be ideal. Then, one can use all the PyTorch prototypes in researchers' papers on Jax-supported accelerators.",
                  "score": 6,
                  "created_utc": "2026-01-14 13:56:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzodx7t",
              "author": "DigThatData",
              "text": "honestly jax is lit. wanna max out your MFU? throw the XLA compiler at it.",
              "score": 1,
              "created_utc": "2026-01-15 05:01:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzjbi9s",
          "author": "SnoWayKnown",
          "text": "I would definitely NOT recommend starting a greenfield project in Tensor flow unless you explicitly need it for something like Edge deployment with TF-lite. Maintenance on it is clearly slowing down, performance isn't competitive anymore and the whole ecosystem is working against you. HuggingFace has also dropped their Tensor flow support too.",
          "score": 6,
          "created_utc": "2026-01-14 13:36:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjcrk5",
              "author": "gajop",
              "text": "tensorflow-lite-micro comes to mind; but the stuff you'll do with tensorflow there are probably not very complex - they can't be, because of the HW restrictions",
              "score": 1,
              "created_utc": "2026-01-14 13:43:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzm5ecd",
          "author": "No_Indication_1238",
          "text": "Damn. TensorFlow just sounds cooler. Should have won based on that alone...",
          "score": 4,
          "created_utc": "2026-01-14 21:30:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjepe1",
          "author": "Old-School8916",
          "text": "tflite/litert is where TF shines atm",
          "score": 3,
          "created_utc": "2026-01-14 13:53:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjibpb",
          "author": "CrawlerVolteeg",
          "text": "Oh so it's going to take over the industry, become everything and no one's going to ever figure out a way to replace it?Â ",
          "score": 2,
          "created_utc": "2026-01-14 14:13:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzk8fly",
          "author": "arcandor",
          "text": "Ugh those runtime errors are so annoying! Half my inexperience at the time, but I don't seem to have that problem nearly as much with other frameworks.",
          "score": 2,
          "created_utc": "2026-01-14 16:20:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzo07yr",
          "author": "Spy_Fox64",
          "text": "I genuinely think it shows up on hiring posts because the people making those posts don't actually keep up with what's happening in ML, they just use buzz words and whatever comes up on google when you search ML libraries.",
          "score": 2,
          "created_utc": "2026-01-15 03:30:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzj74i1",
          "author": "Lower_Improvement763",
          "text": "Idk, but saying GenAI >> lower-dim models isnâ€™t true in the slightest bc one costs 1000x more and they solve different classes of problems.",
          "score": 4,
          "created_utc": "2026-01-14 13:10:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjek6o",
              "author": "Lower_Improvement763",
              "text": "Something like â€œKeys to the White Houseâ€ model can be broken down into empirical factors + expert  opinion.",
              "score": 1,
              "created_utc": "2026-01-14 13:53:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjjxti",
                  "author": "hurhurdedur",
                  "text": "Unfortunately thatâ€™s not a great example as itâ€™s not really a model: itâ€™s really 100% just one guyâ€™s opinion. Allan Lichtman highlights or downplays the relevant pieces of empirical data as needed to fit his general opinion. The â€œmodelâ€ is just a list of which empirical data or vibes heâ€™ll focus on when justifying his opinion to others.",
                  "score": 1,
                  "created_utc": "2026-01-14 14:22:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzjj7b7",
              "author": "NightmareLogic420",
              "text": "I honestly hate the current trend of \"well just shoehorn an LLM into it and call it good\"",
              "score": 1,
              "created_utc": "2026-01-14 14:18:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzjd4mp",
          "author": "Helios",
          "text": "And there are lots of TensorFlow modules that are quite relevant today: Tensorboard, LiteRT (Former TFLite), TF.js, tf.data, TFX, TF Probability,  TF Transform... so no, it is definitely not dead.",
          "score": 4,
          "created_utc": "2026-01-14 13:45:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzr0vnd",
              "author": "fustercluck6000",
              "text": "TF Data especially, pretty hard to beat if want to build crazy efficient, hardware-accelerated data pipelines with as much built-in optimization",
              "score": 2,
              "created_utc": "2026-01-15 16:15:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzjhz45",
          "author": "outerproduct",
          "text": "Tech debt from corporations will be the death of us all.  We can't afford to fix our outdated processes!  Now be a good lad and get me a bump and some cash for this prostitute.",
          "score": 1,
          "created_utc": "2026-01-14 14:11:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkxfim",
          "author": "BarfingOnMyFace",
          "text": "Hahahahaâ€¦ COBOL as a euphemismâ€¦ lol",
          "score": 1,
          "created_utc": "2026-01-14 18:12:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmenb0",
          "author": "DazzedXI",
          "text": "As some one who has had exposure to writing a little bit of AI related stuff in NLP class in PyTorch and pretty good familiarity with agentic frameworks such as lang graph and langchain how would I go about realearning PyTorch / tensor flow with applications particularly in ai related projects.",
          "score": 1,
          "created_utc": "2026-01-14 22:12:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzodr29",
          "author": "DigThatData",
          "text": "not my domain, but my impression is that tensorflow is still popular for mobile deployments and other hardware-constrained environments.",
          "score": 1,
          "created_utc": "2026-01-15 05:00:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzowshw",
          "author": "Zomunieo",
          "text": "Tensorflow. Now thereâ€™s a name I have not heard in a long time. A long time.",
          "score": 1,
          "created_utc": "2026-01-15 07:35:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjaovo",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -10,
          "created_utc": "2026-01-14 13:31:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qf6l1j",
      "title": "Iâ€™m working on an animated series to visualize the math behind Machine Learning (Manim)",
      "subreddit": "learnmachinelearning",
      "url": "https://v.redd.it/cnezsdqyzudg1",
      "author": "No_Skill_8393",
      "created_utc": "2026-01-17 07:08:36",
      "score": 245,
      "num_comments": 22,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qf6l1j/im_working_on_an_animated_series_to_visualize_the/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o02kh04",
          "author": "314159267",
          "text": "Very cool. Like the visual. Nicely done, looking forward for more.",
          "score": 9,
          "created_utc": "2026-01-17 07:48:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02mp88",
              "author": "No_Skill_8393",
              "text": "Thank you. The math animations and video editting is quite new to me and I'm learning and as I do. Hope you guys have patience with me as I improve.",
              "score": 3,
              "created_utc": "2026-01-17 08:08:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o02rool",
          "author": "Dramatic_Yam8355",
          "text": "I hope you donâ€™t stop uploading videosâ€”please keep going.",
          "score": 5,
          "created_utc": "2026-01-17 08:55:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02y50b",
              "author": "No_Skill_8393",
              "text": "I'm already working on Episode 1 and It's receiving more effort and animation budget than Episode 0.\n\nStay in touch :D",
              "score": 3,
              "created_utc": "2026-01-17 09:56:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o02o1tm",
          "author": "tandir_boy",
          "text": "Cool idea. Also chech out Karpathy's [visualization](https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)",
          "score": 4,
          "created_utc": "2026-01-17 08:21:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02ochr",
              "author": "No_Skill_8393",
              "text": "Thank you. I'm also learning from Karpathy [https://karpathy.ai/zero-to-hero.html](https://karpathy.ai/zero-to-hero.html) \n\nGreat course. I learned alot from building micrograd. Excellent stuff to learn ML from scratch",
              "score": 3,
              "created_utc": "2026-01-17 08:24:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o031t4c",
          "author": "Image_Similar",
          "text": "manim is a boon to all of science",
          "score": 2,
          "created_utc": "2026-01-17 10:31:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03392n",
              "author": "No_Skill_8393",
              "text": "It is. God bless 3B1B and his splendid works. Really introduced me into Manim.",
              "score": 2,
              "created_utc": "2026-01-17 10:44:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o03p7z8",
          "author": "kindr_7000",
          "text": "Congrats, keep going.",
          "score": 2,
          "created_utc": "2026-01-17 13:36:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o053w7x",
          "author": "Leading_Pay4635",
          "text": "If this is an AI voiceover a disclaimer is appreciated",
          "score": 2,
          "created_utc": "2026-01-17 17:48:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o033lla",
          "author": "Obvious-Shine-3573",
          "text": "hey, this is really interesting! I've tried some stuff with manim myself, could you share where you're learning from?",
          "score": 1,
          "created_utc": "2026-01-17 10:47:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0342a0",
              "author": "No_Skill_8393",
              "text": "Hi, I use https://docs.manim.community/en/stable/examples.html\n\nr/manim and alot of slamming my head against chatgpt lol",
              "score": 2,
              "created_utc": "2026-01-17 10:51:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o04hjqb",
          "author": "NightmareLogic420",
          "text": "Content seems great! The audio voiceover has something weird and off about it though.",
          "score": 1,
          "created_utc": "2026-01-17 16:04:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o04hqzk",
              "author": "No_Skill_8393",
              "text": "Im considering moving to elevenlabs for better audio :)\n\nSorry for using AI voice im really not confident with my voice.",
              "score": 2,
              "created_utc": "2026-01-17 16:05:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o04irxw",
                  "author": "NightmareLogic420",
                  "text": "Understandable choice, but just one man's opinion, human voice goes a long way to keep people locked in for longer, even if you aren't the best orator in the world, AI voice fatigues the ear faster. Especially if you're just worried about accent, accent is no big deal imo.",
                  "score": 3,
                  "created_utc": "2026-01-17 16:09:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o05k4q8",
          "author": "herooffjustice",
          "text": "Nice, keep going.  \nI'm doing something similar btw: [Link](https://www.reddit.com/r/learnmachinelearning/comments/1q3y6m5/ml_intuition_004_multilinear_regression/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",
          "score": 1,
          "created_utc": "2026-01-17 19:03:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05skzk",
              "author": "No_Skill_8393",
              "text": "Awesome. More Manim enthusiast!",
              "score": 2,
              "created_utc": "2026-01-17 19:44:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o05x8p7",
          "author": "Sea-Lettuce-9635",
          "text": "ðŸ”¥ðŸ”¥",
          "score": 1,
          "created_utc": "2026-01-17 20:07:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06xzwq",
          "author": "Naive-Extension7953",
          "text": "is this your voice?",
          "score": 1,
          "created_utc": "2026-01-17 23:13:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o08hbnr",
          "author": "2hands10fingers",
          "text": "instant subscribe",
          "score": 1,
          "created_utc": "2026-01-18 04:14:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qg6q1w",
      "title": "I implemented a VAE in Pure C for Minecraft Items",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/gallery/1qg6q1w",
      "author": "Boliye",
      "created_utc": "2026-01-18 11:53:54",
      "score": 199,
      "num_comments": 23,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qg6q1w/i_implemented_a_vae_in_pure_c_for_minecraft_items/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0a2iu3",
          "author": "Cybyss",
          "text": "Damn. That is really cool! \n\nYou basically reinvented your own PyTorch from scratch in plain C and used that to create your own variational autoencoder? Ambitious. I also love the creativity of training on Minecraft images instead of the usual MNIST or CIFAR. \n\nWell done!",
          "score": 25,
          "created_utc": "2026-01-18 12:17:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a36ye",
              "author": "Boliye",
              "text": "Thank you!",
              "score": 4,
              "created_utc": "2026-01-18 12:22:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0a1xnh",
          "author": "Palmquistador",
          "text": "I donâ€™t think I understand. You created your own image generator specific to Minecraft images?",
          "score": 19,
          "created_utc": "2026-01-18 12:12:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a35l0",
              "author": "Boliye",
              "text": "Yeah, sort of! A VAE is a type of network that can be used for image generation. And I created and trained one of these with Minecraft images. But as a VAE is also an autoencoder, something you can also do is play with the embeddings and ask the network stuff like \"what's at the middle point between this and that?\" \"What would happen if you took this and subtracted that?\". If the network was successful in learning meaningful concepts, these averages won't be nonsense, and stuff like what I show in the images will happen.",
              "score": 23,
              "created_utc": "2026-01-18 12:22:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0acap7",
                  "author": "LumpyWelds",
                  "text": "Is this like word2vec?\n\n\"dog\" - \"puppy\" + \"kitten\" = \"cat\"",
                  "score": 15,
                  "created_utc": "2026-01-18 13:28:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0a6v1n",
          "author": "gocurl",
          "text": "Doing the \"- concept 1 + concept 2\" and having a relevant result is a very cool way to confirm your model understood key concept. Very well done!\nOut of curiositÃ©, in that \"- x + x\" step, what is the input you provides? Do you start from the middle layer?",
          "score": 8,
          "created_utc": "2026-01-18 12:50:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a95uo",
              "author": "Boliye",
              "text": "Yeah I use the encoder and decoder parts separately:  \n\nEncode iron\\_chestplate -> Latent for iron\\_chestplate  \n\nEncode all items that contain 'iron' in their name and average them out -> Latent for iron concept  \n\nEncode all items that contain 'diamond' in their name and average them out -> Latent for diamond concept  \n\nLiterally do the operation \"Latent for iron\\_chestplate\" - \"Latent for iron concept\" + \"Latent for diamond concept\"  \n\nFinally, pass this result to the decoder (second half of the network).",
              "score": 9,
              "created_utc": "2026-01-18 13:07:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0arh78",
                  "author": "gocurl",
                  "text": "It seems so clear now youâ€™ve said it, but I've never thought about it this way, thanks!",
                  "score": 4,
                  "created_utc": "2026-01-18 14:55:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0a81f9",
          "author": "JanBitesTheDust",
          "text": "Very cool idea! How did you find the latent vector for the concept of iron? Is it just averaging latent vectors for all iron related minecraft textures?",
          "score": 6,
          "created_utc": "2026-01-18 12:59:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a8q6v",
              "author": "Boliye",
              "text": "Yes! it is just the average",
              "score": 3,
              "created_utc": "2026-01-18 13:04:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0dtqkf",
                  "author": "possiblyquestionabl3",
                  "text": "This is a pretty cool trick!\n\nA couple of assumptions:\n\n1. your set of iron related items form some concept vector space like `v_{iron_sword} \\approx v_{iron} + v_{sword}`\n2. it's sufficiently large in dimension, so that different concepts tend to be naturally orthogonal (e.g. v\\_{sword} and v\\_{bar} are nearly orthogonal to each other)\n\nlet `z_{iron}` be the mean vector over all iron related vectors, then\n\n    z_{iron} = 1/N \\sum_{type} v_{iron type} = v_{iron} + 1/N \\sum_{type} v_{type}\n\nif N, the number of distinct iron X items, is large enough, and if we assume v_{type} are generally orthogonal to each other, then `\\sum_{type} v_{type}` can be seen as an isotropic ball of noise with a spherical radius of `||v_{type}|| * sqrt(N)`. As a result, you're essentially computing\n\n    z_{iron} = v_{iron} + noise ||v_{type}||/sqrt(N)\n\nnormalizing v, you're basically computing the actual concept vector for `v_{iron}`, plus noise of O(n^(-1/2)). E.g. if you can provide 100 `iron X` samples, your noise goes down to ~10%.\n\nIf structurally, iron and diamond items have the exact same types (e.g. swords, bars, etc), then the noise term will be structurally similar (same general direction), so `- z_{iron} + z_{diamond}` should be able to cancel the noise out almost perfectly. However, adding/subtracting concepts together without cancellation will amplify the noise, and categories with low # of types to average will have proportionally sqrt(N) times more noise.\n\nIt might be worthwhile to do a few power-iterations on the 2nd-moment operator `M = 1/N \\sum_{type} v_{iron type} v_{iron type}^T` using the mean `z_{iron}` direction to get to the real `v_{iron}` direction (probably converges in just 1-2 steps since the mean `z_{iron}` is already dominated in the `v_{iron}` direction). It's fairly cheap and would allow you to avoid the sqrt(N) noise term.",
                  "score": 1,
                  "created_utc": "2026-01-18 23:53:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0aus8u",
          "author": "ToSAhri",
          "text": "To what extent did this latent arithmetic operation rely on the convenience of the iron and diamond items being very similar (if not identical save for the color)?\n\nI guess Iâ€™m confused on how the latent arithmetic has inherent use of that method. If we only had a random sample of half of the iron items and half of the diamond items would it still work well? Cause then we could use it to generate diamond versions of iron pieces we donâ€™t have and vice versa.\n\nItâ€™s possible I just donâ€™t grasp the use case of VAEs in general and thatâ€™s where my confusion comes from.",
          "score": 3,
          "created_utc": "2026-01-18 15:12:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0b06wb",
              "author": "Boliye",
              "text": "You are right this is just a toy project, and the model is not that powerful to do anything really useful. If I ask it to generate new items, the quality is meh and I wouldn't be surprised if we did that experiment and asked it to generate something that did not exist in its training data, the VAE would struggle to generate something out of distribution like that. I am definetly making the task easier by trying to generate something that I know was present in the training data.\n\nTake the second example (the one where we turn a gold horse armor to a golden shovel). The fact that the generated shovel is yellow, shows that somewhere in the latent space, color of the object is one of the compressed features it learned to be useful for reconstructing.\n\nUltimately, the essence of the latents is that they are just 32 numbers trying to compress the information of a 3\\*16\\*16 = 768 pixel image. So the VAE has to find the most useful high-level features that characterize each image.",
              "score": 3,
              "created_utc": "2026-01-18 15:39:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ahb3j",
          "author": "Anas0101",
          "text": "so cool",
          "score": 2,
          "created_utc": "2026-01-18 13:59:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0al3sb",
              "author": "Boliye",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-18 14:20:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0bkd64",
          "author": "Poleski69",
          "text": "Thats really cool! \n\nIt's a variational autoencoder, so what happens if you sample the latent space with a normal distribution? I know var autoencoders aren't the best at generation but I'm really curious to see what your implementation thinks the 'average' minecraft item is!",
          "score": 2,
          "created_utc": "2026-01-18 17:15:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0cl97x",
              "author": "Boliye",
              "text": "Here are you go:  \n[https://imgur.com/a/TNYgQ32](https://imgur.com/a/TNYgQ32)\n\nFor convinience, for these extra generations I didn't use the C VAE, but the proof of concept in Python that I also created (it can be found in the folder poc\\_python in the git repo). It implements the exact same architecture and achieves the same loss.\n\nIn addition to \"the average item\", and some generations, I also added interpolations between a few items like a diamond chestplate turning into a diamond sword.",
              "score": 1,
              "created_utc": "2026-01-18 20:09:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0dszkw",
          "author": "ami98",
          "text": "Super cool! Your code is very easy to understand, and it's neat that your python proof of concept matches your C results!\n\nBy any chance, do you know of a good textbook that discusses the mathematics behind these algorithms? Thanks:)",
          "score": 1,
          "created_utc": "2026-01-18 23:49:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0e7fqu",
          "author": "Fickle_Lettuce_2547",
          "text": "How long have you used C to be able to make something like this??",
          "score": 1,
          "created_utc": "2026-01-19 01:06:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbr1ds",
      "title": "Built a RAG app to explore Tokyo land prices on an interactive map",
      "subreddit": "learnmachinelearning",
      "url": "https://i.redd.it/zszt9ocy94dg1.png",
      "author": "itsspiderhand",
      "created_utc": "2026-01-13 13:16:53",
      "score": 176,
      "num_comments": 16,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qbr1ds/built_a_rag_app_to_explore_tokyo_land_prices_on/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzdo6bw",
          "author": "BackyardAnarchist",
          "text": "Cool! I've been wanting to do something similar.Â  Where did you get the data?",
          "score": 10,
          "created_utc": "2026-01-13 16:46:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdpkt7",
              "author": "itsspiderhand",
              "text": "Thank you! Itâ€™s from one of Japanâ€™s government ministries. They provide various types of data, some of which are allowed for commercial use as well.",
              "score": 7,
              "created_utc": "2026-01-13 16:53:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzer9qd",
          "author": "astarjack",
          "text": "Kudos! It's not easy to find a unique and intelligent idea to apply a RAG to.",
          "score": 9,
          "created_utc": "2026-01-13 19:54:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzg3110",
              "author": "itsspiderhand",
              "text": "Thank you!",
              "score": 3,
              "created_utc": "2026-01-13 23:45:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzhphtm",
          "author": "Pinoy-Cya1234",
          "text": "Want to do the same project for Metro Manila. Am not sure if land prices in Metro Manila is publicly available.",
          "score": 2,
          "created_utc": "2026-01-14 05:35:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzij6hb",
              "author": "itsspiderhand",
              "text": "Thanks for your interest. Land price data is actually interesting to work with especially for big cities.",
              "score": 1,
              "created_utc": "2026-01-14 10:04:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzi64fb",
          "author": "CompetitiveMobile179",
          "text": "Really interesting project OP",
          "score": 1,
          "created_utc": "2026-01-14 07:58:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nziig8a",
              "author": "itsspiderhand",
              "text": "Thank you!",
              "score": 2,
              "created_utc": "2026-01-14 09:57:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzmclwc",
          "author": "shadowylurking",
          "text": "checking out!",
          "score": 1,
          "created_utc": "2026-01-14 22:03:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzo8pk6",
              "author": "itsspiderhand",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-01-15 04:25:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzo9a8w",
          "author": "Shekher_05",
          "text": "Mind telling me how to learn RAG",
          "score": 1,
          "created_utc": "2026-01-15 04:28:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpgzlc",
              "author": "itsspiderhand",
              "text": "Thanks for your comment. I am a software engineer but pretty new to RAG. This project is really small and the data is just well-defined so just knowing the basic RAG flow was just enough. I am not really doing anything theoretical for this project. I currently just tweak the prompt based on my observation and want to learn how to evaluate the results in a more formal way.",
              "score": 1,
              "created_utc": "2026-01-15 10:47:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzpn8ci",
                  "author": "Shekher_05",
                  "text": "Thanks man, I am an undergraduate student and thinking of getting into machine learning. I have gained all the fundamental knowledge I need but I don't know how to move forward from here on out . \nYour project caught my attention that's why I thought maybe RAG must be an essential skill so i wanted to know how it functions.",
                  "score": 1,
                  "created_utc": "2026-01-15 11:40:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzclfkx",
          "author": "Samiul-Nahiyan6714",
          "text": "I thought u made it from scratch then i saw OPENAI, elbozo mate",
          "score": -16,
          "created_utc": "2026-01-13 13:35:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzcznfv",
              "author": "Cipher_01",
              "text": "why re-invent the wheel?",
              "score": 14,
              "created_utc": "2026-01-13 14:50:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qamb60",
      "title": "Best way to learn Machine Learning in 2â€“3 months (strong math background, looking for practical advice)",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qamb60/best_way_to_learn_machine_learning_in_23_months/",
      "author": "Substantial-Key-1363",
      "created_utc": "2026-01-12 05:18:10",
      "score": 127,
      "num_comments": 22,
      "upvote_ratio": 0.95,
      "text": "Iâ€™m planning to learn **machine learning in 2â€“3 months** and would appreciate practical advice. I have a **strong math background** (linear algebra, calculus, probability) and an engineering/technical background, so Iâ€™m comfortable with programming.\n\nMy goal is **hands-on, applied ML**: understanding core concepts, using Python libraries (NumPy, pandas, scikit-learn, possibly PyTorch/TensorFlow), and building a few meaningful beginner projects.\n\nIâ€™d love advice on:\n\n* The **best learning strategy** for a short timeline\n* **Recommended resources** (courses, books, YouTube, GitHub)\n* A **simple roadmap**: what to focus on first vs what can wait\n* **Project ideas** and common **mistakes to avoid**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qamb60/best_way_to_learn_machine_learning_in_23_months/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "nz471tj",
          "author": "TheAgaveFairy",
          "text": "Fast AI course and many things from Jeremy Howard are all a great place to start for practical, hands-dirtying. The math there is mild. If you can already program Python or something in the C family, great, otherwise it's worth learning basic python, use LLMs to explain code, and just get a breadth of the field into you know where you want to focus more.\n\nCNNs are usually taught early because they're easy to teach and understand, but Andrej Kaparthy's course on building GPT from scratch will catch you up to speed on the basics of modem transformer based LLMs. It's pretty simple math, honestly, even for my dumb brain",
          "score": 37,
          "created_utc": "2026-01-12 05:49:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz59p7w",
              "author": "liberty_me",
              "text": "Great list. Which course from Andrej? Zero to Hero?",
              "score": 1,
              "created_utc": "2026-01-12 11:38:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz5eelu",
                  "author": "sen_rengi",
                  "text": "Yes, Zero to Hero it is. If you understand the \"makemore\" initially, you will be fine.",
                  "score": 2,
                  "created_utc": "2026-01-12 12:14:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz52yq3",
          "author": "Radiant-Rain2636",
          "text": "https://www.reddit.com/r/learnmachinelearning/s/F8LFUXCbbz\nThis open roadmap.\nThen thereâ€™s Fast AI, campusX, this one by offchan (https://github.com/offchan42/machine-learning-curriculum) even Udemy has this guy Henrik something",
          "score": 7,
          "created_utc": "2026-01-12 10:40:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4sbna",
          "author": "itexamples",
          "text": "* The Hundred Page Machine Learning Book\n* Designing Machine Learning Systems\n* Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n* Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python",
          "score": 12,
          "created_utc": "2026-01-12 08:58:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4qg8g",
          "author": "Fragrant-Strike4783",
          "text": "I'd recommend \"Hands-On Machine Learning with Scikit-Learn and PyTorch\"",
          "score": 4,
          "created_utc": "2026-01-12 08:40:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz650mq",
          "author": "DataCamp",
          "text": "Start by moving fast through the applied basics. You already know the math, so focus on how it shows up in practice: data loading, cleaning, feature engineering, train/validation splits, and evaluation. NumPy â†’ pandas â†’ scikit-learn should be your core stack at first.\n\nFor learning structure, it helps to follow one coherent path instead of mixing random videos. A hands-on ML path that walks through classical models end-to-end (regression, trees, ensembles, clustering) will get you productive quickly. You donâ€™t need deep learning immediately to be effective.\n\nProjects are where things actually click. Aim for 2â€“3 solid ones, not many:\n\n* one regression problem with messy real-world data\n* one classification problem where you explain model choice and tradeoffs\n* optionally one small NLP or tabular ML project if time allows\n\nWhat can wait: heavy DL, complex frameworks, and chasing SOTA models. Those add very little value this early.\n\nBiggest mistakes to avoid:\n\n* spending weeks on theory you already understand\n* jumping to PyTorch before youâ€™re fluent in evaluation and data issues\n* copying Kaggle notebooks without explaining why things work\n\nIf you can explain your projects clearly, debug models, and talk about failure modes, youâ€™ll be in a strong position after those 2â€“3 months.",
          "score": 3,
          "created_utc": "2026-01-12 14:52:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4t9iw",
          "author": "xtermin8r2",
          "text": "https://ubc-cs.github.io/cpsc330-2024W1/README.html\n\nThis is a pretty useful resource if you're gonna go into applied ml. This doesn't teach a lot of theory though (take a look at cpsc340 at the same uni if ur interested in that).",
          "score": 2,
          "created_utc": "2026-01-12 09:07:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzr8msl",
              "author": "Altair-01",
              "text": "Great learning resource",
              "score": 1,
              "created_utc": "2026-01-15 16:49:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz6srfo",
          "author": "Electric-Sun88",
          "text": "There are a ton of free resources for learning machine learning. There are [online machine learning courses](https://www.nobledesktop.com/topics/machine-learning-training-nyc) with live instructors, and I would recommend that you consider taking one of those. If you want to learn fast and feel confident about your grasp of the material, then nothing beats a structured class with a live instructor who you can ask questions and will keep you motivated while preventing you from getting stuck",
          "score": 2,
          "created_utc": "2026-01-12 16:44:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nziv9kn",
          "author": "fzngagan",
          "text": "Do [fast.ai](http://fast.ai) course. If you already know how to write training loops, then go for p2 of the course otherwise do p1 and p2. Link: [course.fast.ai](http://course.fast.ai)\n\nI followed it sequentially finished 23/25 lessons.",
          "score": 2,
          "created_utc": "2026-01-14 11:49:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz43ubr",
          "author": "Standard_Iron6393",
          "text": "start first python   \nand when you have grip on python , start doing basic ml tasks from youtube and chat gpt  \nand then make production grade projects",
          "score": 3,
          "created_utc": "2026-01-12 05:25:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5ldah",
          "author": "Ty4Readin",
          "text": "In my opinion, it's extremely difficult and unlikely that someone can \"learn ML\" to any significant degree in 2-3 months.\n\nHowever, ignoring that, I see a lot of common mistakes when people work on side projects to learn.\n\nThe two biggest mistakes that people make are:\n\n1. They don't actually solve any real problems. They build a \"house prediction\" model that is completely useless and that nobody would ever ever actually use for many reasons.\n\n2. They don't actually scrape any data themselves, they just look for pre-existing datasets and train a model on it.",
          "score": 4,
          "created_utc": "2026-01-12 13:02:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz63tui",
              "author": "pm_me_github_repos",
              "text": "I agree 1-2 months is pretty unrealistic. That said, solving toy problems and using preexisting datasets is totally fine for learning. It DOES become a problem when you have a 1-2 month timeline and it suddenly becomes easier to just copy/paste one of the 2000 kaggle notebooks for a given toy problem without actually understanding what is happening",
              "score": 3,
              "created_utc": "2026-01-12 14:46:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzg15vd",
                  "author": "Ty4Readin",
                  "text": ">I agree 1-2 months is pretty unrealistic. That said, solving toy problems and using preexisting datasets is totally fine for learning.\n\nI would say its fine up until a point, but the most important skills are learned in actually formulating problems, collecting data, and building a real solution that leverages the models & problem formulation.\n\nBut most people will literally never even complete one side project where they do any of those. Many people start with toy datasets and end there, which is a huge disservice to actually learning imo.",
                  "score": 1,
                  "created_utc": "2026-01-13 23:35:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz4umc6",
          "author": "Boom_Boom_Kids",
          "text": "With your background, focus on doing, not over studying theory.\n\nStart with Python ML basics.. NumPy, pandas, matplotlib, then move quickly to scikit learn. Learn core models only.. linear/logistic regression, trees, random forest, basic clustering. Understand data cleaning, feature selection, train/test split, and evaluation.\n\nFor resources\nâ— Andrew Ngâ€™s ML course (fast forward math parts)\nâ— Hands-On Machine Learning with Scikit Learn book\nâ— Kaggle notebooks for real examples\n\nProjects matter most. Do 2â€“3 solid ones like\nâ— House price or sales prediction\nâ— Classification on a messy real dataset\nâ— Simple recommendation or text classification\n\nAvoid spending too much time on deep learning at first. Donâ€™t jump tools without understanding results. Focus on why models fail and how to improve them.",
          "score": 2,
          "created_utc": "2026-01-12 09:21:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz76m8y",
          "author": "Flimsy_Challenge4683",
          "text": "Go ahead with basics",
          "score": 1,
          "created_utc": "2026-01-12 17:48:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbpk4k",
          "author": "natika1",
          "text": "Courses on hugging face, courses from google. There is plenty. The best way to learn is to create something.",
          "score": 1,
          "created_utc": "2026-01-13 09:25:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzh09iq",
          "author": "Straight-Ad5757",
          "text": "If you are good with maths then the good way to learn machine learning is by start implementing projects around the basic algorithms. Thatâ€™s something that worked for me as I used to take a single dataset then try solving the classification problem with different algorithms and then reading more about the algorithm and how they work! That kinda gave me the way to implement first and then understand how things and why things worked like that. Understanding the results and correlating them back to working of algos. Thus helping in understanding the algo as well but at the saltine keeping up with the implementation and new things.",
          "score": 1,
          "created_utc": "2026-01-14 02:50:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o08qqqw",
          "author": "Joe_Hart99",
          "text": "Given your background, Iâ€™d avoid anything too theory-heavy and go straight into applied workflows. A structured path like Udacity worked well for me because it focuses on implementing models, using scikit-learn/PyTorch, and building end to end projects instead of just watching lectures.",
          "score": 1,
          "created_utc": "2026-01-18 05:18:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz54c99",
          "author": "aaaannuuj",
          "text": "I can teach live one to one if you pay.",
          "score": 0,
          "created_utc": "2026-01-12 10:52:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdbghw",
      "title": "For people learning ML how are you thinking about long-term career direction right now?",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qdbghw/for_people_learning_ml_how_are_you_thinking_about/",
      "author": "RepairActual9047",
      "created_utc": "2026-01-15 05:51:30",
      "score": 99,
      "num_comments": 19,
      "upvote_ratio": 0.98,
      "text": "Iâ€™m currently learning machine learning and trying to be more intentional about where this path leads. With how fast models tooling and automation are evolving Iâ€™m finding it harder to answer questions like:\n\n* What kinds of ML-related roles are likely to grow vs get compressed?\n* Which skills actually compound over time instead of becoming quickly abstracted away?\n* How much should learners focus on theory vs applied vs domain depth?\n\nFor those already working in or around ML:  \nHow are you personally thinking about long-term career direction in this field?  \nWhat would you prioritize if you were starting again today?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qdbghw/for_people_learning_ml_how_are_you_thinking_about/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "nzp0u82",
          "author": "DataCamp",
          "text": "From what we're seeing and hearing from our learners, tooling will change fast, but some problems stay stubborn.\n\nWhat seems to keep growing: people who can take a model from â€œworks in a notebookâ€ to â€œworks in productionâ€ (deployment, monitoring, versioning), and people who pair ML with real domain knowledge (finance/health/ops etc.).\n\nWhat compounds over time: data work (cleaning + feature thinking), evaluation (metrics, leakage, drift), and solid software habits (Git, tests, APIs, basic cloud/containers). Also: being able to explain tradeoffs to non-ML folks.\n\nTheory vs applied: learn enough theory to not cargo-cult, then spend most time shipping small end-to-end projects on real datasets. Add one â€œproduction muscleâ€ each time (e.g., simple API, logging, monitoring metric).\n\nIf youâ€™re starting again: foundations first (stats + Python + data), then projects, then specialize once youâ€™ve built a few things you can show.",
          "score": 84,
          "created_utc": "2026-01-15 08:12:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzp4b9j",
          "author": "Gullible_Eggplant120",
          "text": "I am learning Machine Learning for the fun of it myself, so I wont be the best in telling where the careers go. \n\nHowever, I work in consulting and see tons of companies. When it comes to in-house data teams, I am still staggered by how most data people are removed from business common sense and vice versa. I think there is huge potential in being able to speak both langauges. It practically probably means learning Finance and spending some time on the front lines (Sales, Marketing, Ops). But yeah, take this advice with a degree of salt, as these are only my observations.",
          "score": 18,
          "created_utc": "2026-01-15 08:46:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpi6fy",
              "author": "Tender_Figs",
              "text": "The potential is dangerous though - I'm one of these people who can speak both languages and I get taken advantage of on a daily basis. Just be careful about thinking about becoming an \"in-between\", it sounds more fun and in demand, but there's no leverage.",
              "score": 5,
              "created_utc": "2026-01-15 10:58:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzpwbke",
                  "author": "BraindeadCelery",
                  "text": "I have Similar Experiences. I was a process consultant and learned enough SWE/ML to switch to the ML Engineering team. But because my slides were the best, it was always me who was set aside for non-technical / communications / meeting work which impeded my growth as an engineer cause i was also not getting shiny projects to have more time for comms work. \n\nI chaged companies (partly because of that) after a year or so.",
                  "score": 3,
                  "created_utc": "2026-01-15 12:46:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nztknze",
                  "author": "Mammoth_Visit_9044",
                  "text": "Could you elaborate on what you mean by being taken advantage of?",
                  "score": 2,
                  "created_utc": "2026-01-15 23:19:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzqubxd",
          "author": "Jaded_Individual_630",
          "text": "Mathematics will always outlive tool stacks.\n\n\nIt's a great time to be versed with the real fundamentals while tech bros chase the tool of the week.",
          "score": 7,
          "created_utc": "2026-01-15 15:45:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqwn82",
              "author": "smuhamm4",
              "text": "Which mathematics do you recommend besides stats",
              "score": 2,
              "created_utc": "2026-01-15 15:56:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzqy956",
                  "author": "Jaded_Individual_630",
                  "text": "Like learning many instruments or languages, all would prove useful, but some highlights:\n\n\nStatistical Learning Theory (which isn't \"stats\", colloquially), probability and measure theory, functional analysis, historical understanding of the development of ML, lin alg, matrix calculus,Â  operator theory, numerical analysis, actual computer science that isn't just tooling, CUDA....",
                  "score": 6,
                  "created_utc": "2026-01-15 16:03:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzt2i5b",
              "author": "apexvice88",
              "text": "We should also take the power back from tech bros, the way Zuckerberg took power away from the winklevoss twins, not sure if that story is 100% true but it needs to happen more often. Iâ€™m not for gate keeping, however there are certain people that it needs to be kept away from.",
              "score": 1,
              "created_utc": "2026-01-15 21:49:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzt8gw1",
                  "author": "smuhamm4",
                  "text": "What do you mean?",
                  "score": 1,
                  "created_utc": "2026-01-15 22:17:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzqjx2x",
          "author": "Maneisthebeat",
          "text": "Well it's quite obvious that data engineering will remain. It's possible there will be a contraction in budgets for creating systems, but the consulting mindset and requirements to set up a functional system that communicates with other systems will always be needed.\n\nAnd then it depends what field you are going in. Some can be simplified more easily and with less risk than others. If you are working with delicate personally identifiable information, strictness on processes and requirements go up and the consequences of failure to adhere to those standards along with it, meaning the budget to adhere to certain standards is more easily acquired.",
          "score": 2,
          "created_utc": "2026-01-15 14:56:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuu93n",
          "author": "argvalue",
          "text": "I really want to step foot into the healthcare/biotech domain and see what kind of application AI has in it. This has been my motivation to learn ML since the past few months. It's gonna take a lot of time for me, not only because I'm a very slow learner, but also the amount of material required to study is insane.\n\nI want to join one of the companies focusing on this domain (in a year's time mostly). I also have plans of becoming an entrepreneur, which I want to pursue maybe 4-5 years later, mostly into the domain I mentioned above",
          "score": 2,
          "created_utc": "2026-01-16 03:31:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzq4akt",
          "author": "zehen5",
          "text": "Hey, are you taking a university course for ML or something else?",
          "score": 1,
          "created_utc": "2026-01-15 13:35:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qe7si1",
      "title": "RNNs are the most challenging thing to understand in ML",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qe7si1/rnns_are_the_most_challenging_thing_to_understand/",
      "author": "radjeep",
      "created_utc": "2026-01-16 05:48:12",
      "score": 69,
      "num_comments": 34,
      "upvote_ratio": 0.88,
      "text": "Iâ€™ve been thinking about this for a while, and Iâ€™m curious if others feel the same.\n\nIâ€™ve been reasonably comfortable building intuition around most ML concepts Iâ€™ve touched so far. CNNs made sense once I understood basic image processing ideas. Autoencoders clicked as compression + reconstruction. Even time series models felt intuitive once I framed them as structured sequences with locality and dependency over time.\n\nBut RNNs? Theyâ€™ve been uniquely hard in a way nothing else has been.\n\nItâ€™s not that the math is incomprehensible, or that I donâ€™t understand sequences. I *do*. I understand sliding windows, autoregressive models, sequence-to-sequence setups, and Iâ€™ve even built LSTM-based projects before without fully â€œgettingâ€ what was going on internally.\n\nWhat trips me up is that RNNs donâ€™t give me a stable mental model. The hidden state feels fundamentally opaque i.e. it's not like a feature map or a signal transformation, but a compressed, evolving internal memory whose semantics I canâ€™t easily reason about. Every explanation feels syntactically different, but conceptually slippery in the same way.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qe7si1/rnns_are_the_most_challenging_thing_to_understand/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "nzvkpsj",
          "author": "ds_account_",
          "text": "How do you feel about SVM, VAE and latent diffusion.\n\nBut I agree RNN can be tough without first grasping time series analysis.",
          "score": 45,
          "created_utc": "2026-01-16 06:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvhgkn",
          "author": "newrockstyle",
          "text": "RNNs are tough because their hidden state is hard to visualise and reason about.",
          "score": 28,
          "created_utc": "2026-01-16 06:05:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw0tsq",
              "author": "narasadow",
              "text": "TBH I felt the same for Kalman Filters",
              "score": 10,
              "created_utc": "2026-01-16 08:52:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzw2yfr",
          "author": "UnusualClimberBear",
          "text": "I think the best explanation is this famous yet old blog post by C. Olah : [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)",
          "score": 14,
          "created_utc": "2026-01-16 09:12:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzynlep",
              "author": "CasulaScience",
              "text": "Agreed this is what made it click for me. OP, they're actually pretty simple and make a lot of sense if you come from a comp sci background (which I don't). It's very reminiscent of a turing machine, you have some hidden state (instead of \"tape\") that you write to and read from at every step. What you write/delete and what you read at each step is controlled by little neural nets. \n\nThat's really it.",
              "score": 3,
              "created_utc": "2026-01-16 18:05:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzvw5g8",
          "author": "Expensive_Fun4346",
          "text": "if you think that's opaque, wait until you look at reinforcement learning",
          "score": 11,
          "created_utc": "2026-01-16 08:09:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01o5vw",
              "author": "jsh_",
              "text": "which aspects of reinforcement learning do you find opaque? I feel like once you understand MDPs it's relatively straightforward",
              "score": 1,
              "created_utc": "2026-01-17 03:35:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o09t62o",
                  "author": "Organic_botulism",
                  "text": "MDPs are the absolute basics for understanding tabular RL but once you leave that setting and start relaxing assumptions things get hairy very quickly as convergence proofs (e.g. for Q-learning) rely on a tabular setting among other requirements.\n\nThe Bellman error not being learnable is quite opaque IMO, as is the intuition for why bootstrapping, off-policy learning and function approximation can lead to divergence and instability during training (e.g. one-step semi-gradient Q-learning will diverge in certain settings).\n\nThese are all opaque ideas. If youâ€™re using any type of function approximation what is the requirement needed to guarantee stability? Why does linear function approximation with dynamic programming sometimes fail even if the least squares solution is found at each step?",
                  "score": 1,
                  "created_utc": "2026-01-18 10:55:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvn4mu",
          "author": "Silly_Guidance_8871",
          "text": "I agree, for reasons similar to why reasoning about loops and recursion are more difficult than non-branching code paths: There's a lot more implied state that's not easily managed",
          "score": 10,
          "created_utc": "2026-01-16 06:51:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvj3s2",
          "author": "TBSchemer",
          "text": "Then how do you feel about transformers?",
          "score": 17,
          "created_utc": "2026-01-16 06:18:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwuta8",
              "author": "quadrillio",
              "text": "I think self attention is actually easier to grasp. And 3blue1brown has a set of excellent videos explaining them. There arenâ€™t very many accessible resources for things like LSTM and many of the diagrams found online are over simplified or badly explained.",
              "score": 2,
              "created_utc": "2026-01-16 12:56:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzz650x",
                  "author": "TBSchemer",
                  "text": "Transformers take the same mechanisms used in LSTMs and add more complexity to it. If you can understand LLMs, you can understand RNNs.",
                  "score": 2,
                  "created_utc": "2026-01-16 19:27:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvhkfl",
          "author": "d0r1h",
          "text": "On the same boat, I've been revising ML concepts and got stuck at RNN currently. I've opened 10s of blogs and two books, Just to grasp the RNN. Gonna give few more hrs before moving forward.",
          "score": 5,
          "created_utc": "2026-01-16 06:06:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwqjy1",
              "author": "CuriousAIVillager",
              "text": "What did you use as a checklist?",
              "score": 1,
              "created_utc": "2026-01-16 12:29:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o01aubi",
                  "author": "d0r1h",
                  "text": "checklist as in ?",
                  "score": 1,
                  "created_utc": "2026-01-17 02:11:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvyleu",
          "author": "DrXaos",
          "text": "RNNs are dynamical systems in the 'chaos theory' sense.  Their original difficulties in training were because they had in effect strong Lyapunov exponents in either forward or backward directions resulting in exponential decay or explosion in state or gradients. \n\nFunny that until 8 years ago, RNNs of various forms were the state of the art as the most complex and interesting machine learning architecture.",
          "score": 5,
          "created_utc": "2026-01-16 08:32:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzs1ck",
          "author": "s-jb-s",
          "text": "I come from a probability theory background, so my intuition for \"ML\" came from outside the way CS students tend to think about things, which I generally don't understand.\n\nIf you're struggling with the intuition, I'd advise taking a step back from RNNs and looking at the evolution of the problems they solve. I think a natural progression to build up your intuition regarding latent-state models is to start with Discrete-time Hidden Markov Models, which are very easy to intuit.\n\nThe problem is that HMMs are inefficient in high dimensions. Factorial HMMs improve this by distributing the state into multiple binary variables, but this makes inference much more expensive to calculate. Intuitively, the fix is to move to Linear Dynamical Systems, where the state vectors are now continuous rather than discrete (think Kalman Filters, if you're familiar). This solves the representation problem, but now you have a linearity issue because you can only model simple curves. How do we fix that? We take the Linear Dynamical System and wrap the transition in a non-linear activation function. That is effectively an RNN.\n\nThere's a lot of nuance missing here and I've been a bit handwavey (it's not intended to fix your intuition) but I think there's value in studying what came before RNN's and building your intuition up from there, particularly in relation to what actually is actually going on in latent space, and what that represents. I'm unsure how helpful this is if you're not particularly interested in the theory, and just want some intuition. But I think the intuition comes from the theory, and seeing how it progresses.",
          "score": 3,
          "created_utc": "2026-01-16 21:10:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwckfh",
          "author": "divided_capture_bro",
          "text": "Wait, you can't reason about evolving internal states? It's almost like it is a black box of some sorts...",
          "score": 2,
          "created_utc": "2026-01-16 10:40:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzyuqec",
              "author": "bythenumbers10",
              "text": "Meanwhile, statistical methods await the disillusioned deep learning practitioners with open arms, ready to soothe their crisis of blind faith unrewarded with transparency and explicable tuning methods.",
              "score": 2,
              "created_utc": "2026-01-16 18:36:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzz0hr6",
                  "author": "Fair_Treacle4112",
                  "text": "compare normal ad hoc arrest scary practice coordinated absorbed doll employ\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*",
                  "score": 0,
                  "created_utc": "2026-01-16 19:01:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzyg735",
          "author": "Longjumping_Echo486",
          "text": "Personally I feel lstm math is tougher due to multiple gates  and it's beautiful when u see mathematically how the gates are deleting and updating info using pointwise operations and create a refined long term memory",
          "score": 2,
          "created_utc": "2026-01-16 17:32:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzyiikb",
          "author": "dankwartrustow",
          "text": "It only really made sense to me when I learned about vanishing and exploding gradients",
          "score": 2,
          "created_utc": "2026-01-16 17:42:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzypcox",
          "author": "Mr_iCanDoItAll",
          "text": "Try learning HMMs if you haven't already. Might help intuit hidden states.",
          "score": 1,
          "created_utc": "2026-01-16 18:13:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o01qdmt",
          "author": "pool007",
          "text": "OP, I think you already understand the main concept. It's really not a curated memory structure or features but computation and flow of data that's supposed to become stuff to memorize and that is all it does, imo. Flow should be fast to compute but still have expression power while being stable for training.",
          "score": 1,
          "created_utc": "2026-01-17 03:50:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o064q4r",
          "author": "arsenale",
          "text": "Each RNN variant is a standalone architecture.\n\nVec2Seq\n\nSeq2Seq\n\n...\n\nEach one is a totally different model, they are called RNN just because yes, they share an equation which uses the same Wxh Whh etc, but those matrices are multiplied by different values making the architecture completely different. Just stick with one architecture until you get it?",
          "score": 1,
          "created_utc": "2026-01-17 20:45:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzz09km",
          "author": "HumbleJiraiya",
          "text": "I dont know man. I understood RNNs immediately. It was probably the most intuitive concept for me in DL ðŸ˜…. \n\nAutoencoders were harder to understand than RNNs. \n\nWeâ€™re all different I guess.",
          "score": 0,
          "created_utc": "2026-01-16 19:00:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qb8te7",
      "title": "Just finished my first End-to-End ML Project (XGBoost + FastAPI + Docker + Streamlit). Looking for feedback.",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qb8te7/just_finished_my_first_endtoend_ml_project/",
      "author": "Present-Respect3405",
      "created_utc": "2026-01-12 22:04:05",
      "score": 67,
      "num_comments": 13,
      "upvote_ratio": 0.99,
      "text": "Hi everyone,\n\nI built a Car Price Predictor with sklearn and XGBoost but I realized it felt kinda \"meaningless\" to do everything in a jupyter notebook.\n\nSo I decided to use FastAPI to create a backend, Streamlit to create a frontend and used docker so anyone can run it. I did it so my project would feel more \"touchable\" and because I thought it would be good to learn important technologies like docker and FastAPI before going deeper in machine learning.\n\nThe Tech Stack:\n\nModel: XGBoost Regressor (Optimized to avoid overfitting, \\~15% MAPE).\n\nBackend: FastAPI (for serving predictions).\n\nFrontend: Streamlit (for user interaction).\n\nInfrastructure:  Docker & Docker Compose (separated services).\n\nI would love some feedback on the project structure. Any kind of feedback is welcomed, it can be about the model, architecture or literally anything\n\nRepo: [https://github.com/hvbridi/XGBRegressor-on-car-prices/tree/main](https://github.com/hvbridi/XGBRegressor-on-car-prices/tree/main)\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qb8te7/just_finished_my_first_endtoend_ml_project/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "nzat1ph",
          "author": "chrisvdweth",
          "text": "Since I actually set this task in some semesters :), here are some very quick comments:\n\n* I'm not sure about using ordinal encoding for \"model\" as you imply a natural order/ranking. Yes, car model may have some ranking but only within the same brand.\n* Feature importance analysis: did you perform some ablation studies or \"asked\" your model which are the features that affect the predictions most. If nothing else, it's a basic sanity check, but generally provides useful insights\n* Error analysis: apart from the raw errors, can you tell me which cases your model gets particularly wrong (e.g., maybe exotic and expensive cars underrepresented in your training data)?\n\nIn short, in practice it's not just about error but also about understanding the model including its limitation.",
          "score": 17,
          "created_utc": "2026-01-13 04:48:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbrxod",
              "author": "Straight_Emphasis635",
              "text": "Great feedback. Are you a ml engineer",
              "score": 3,
              "created_utc": "2026-01-13 09:48:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzbyafk",
                  "author": "chrisvdweth",
                  "text": "Nope, just a university lecturer teaching courses around AI/ML/NLP and text/data mining.",
                  "score": 7,
                  "created_utc": "2026-01-13 10:47:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzcmz5l",
              "author": "Present-Respect3405",
              "text": "Thank you for the amazing feedback! Do you have any recommendations of tools/approaches for the models?",
              "score": 1,
              "created_utc": "2026-01-13 13:44:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzbjq40",
          "author": "1010111000z",
          "text": "Great work ...\n\nI worked on the same project a month ago and used a random forest model.\n\nHere is my project repo: https://github.com/Zaid-Al-Habbal/car-price-predictor",
          "score": 4,
          "created_utc": "2026-01-13 08:28:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzccaqi",
          "author": "Bobsthejob",
          "text": "great job. i had a project where a took a /predict model api and turned it into a more prod ready app with linting, tests, documentation, and a pipeline with github actionsÂ https://github.com/divakaivan/model-api-oip. i also turned it into a follow-along repo (you can see if you check the different branches)",
          "score": 2,
          "created_utc": "2026-01-13 12:39:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzcfvpo",
              "author": "burntoutdev8291",
              "text": "Link doesn't work",
              "score": 1,
              "created_utc": "2026-01-13 13:02:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nze22o5",
                  "author": "Bobsthejob",
                  "text": "fixed. for some reason an extra \" i\" was included in the url",
                  "score": 1,
                  "created_utc": "2026-01-13 18:01:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzd96e5",
          "author": "MathProfGeneva",
          "text": "I haven't looked too deeply, but I noticed in the readme you tell people to use a notebook to generate your model pkl files. \n\nIt would definitely be better to have Python scripts for that.",
          "score": 1,
          "created_utc": "2026-01-13 15:37:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzeofia",
          "author": "Just-Signal2379",
          "text": "just want to ask, were you a complete beginner before starting on this? if so, what courses did you take",
          "score": 1,
          "created_utc": "2026-01-13 19:41:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzf88jg",
              "author": "Present-Respect3405",
              "text": "In machine learning yes, in python no. For machine learning I did the beginner and intermediate courses on Kaggle",
              "score": 1,
              "created_utc": "2026-01-13 21:14:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzbrupq",
          "author": "Straight_Emphasis635",
          "text": "Was there a course that required this exercise? Please post a link",
          "score": 0,
          "created_utc": "2026-01-13 09:47:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzcnc84",
              "author": "Present-Respect3405",
              "text": "No, I just did it as a way to learn more about sklearn/xgboost and other technologies.",
              "score": 1,
              "created_utc": "2026-01-13 13:46:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qf2q85",
      "title": "decision tree from scratch in js. no libraries.",
      "subreddit": "learnmachinelearning",
      "url": "https://v.redd.it/alufthat0udg1",
      "author": "Ok-Statement-3244",
      "created_utc": "2026-01-17 03:51:32",
      "score": 65,
      "num_comments": 2,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qf2q85/decision_tree_from_scratch_in_js_no_libraries/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o01ud0b",
          "author": "disquieter",
          "text": "Wow, like, wow!",
          "score": 1,
          "created_utc": "2026-01-17 04:17:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04gwgm",
          "author": "giadev",
          "text": "thats so cool",
          "score": 1,
          "created_utc": "2026-01-17 16:01:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfcotf",
      "title": "An introduction to Physics Informed Neural Networks (PINNs): Teach your neural network to â€œrespectâ€ Physics",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qfcotf/an_introduction_to_physics_informed_neural/",
      "author": "omunaman",
      "created_utc": "2026-01-17 13:03:11",
      "score": 57,
      "num_comments": 2,
      "upvote_ratio": 0.9,
      "text": "https://preview.redd.it/ll4z0ewvqwdg1.png?width=1100&format=png&auto=webp&s=e6a375679fb5575866953109c00e86d8eb31523a\n\nAs universal function approximators, neural networks can learn to fit any dataset produced by complex functions. With deep neural networks, overfitting is not a feature. It is a bug.\n\nMedium Link for better readability: [https://vizuara.medium.com/an-introduction-to-physics-informed-neural-networks-pinns-teach-your-neural-network-to-respect-af484ac650fc](https://vizuara.medium.com/an-introduction-to-physics-informed-neural-networks-pinns-teach-your-neural-network-to-respect-af484ac650fc)\n\nLet us consider a hypothetical set of experiments. You throw a ball up (or at an angle), and note down the height of the ball at different points of time.\n\nWhen you plot the height v/s time, you will see something like this.\n\nhttps://preview.redd.it/b9byjx62pwdg1.png?width=1100&format=png&auto=webp&s=22aebc098ad30d2b18505fcaa3d80cf61777f2b5\n\nIt is easy to train a neural network on this dataset so that you can predict the height of the ball even at time points where you did not note down the height in your experiments.\n\nFirst, let us discuss how this training is done.\n\n# Training a regular neural network\n\n\n\nhttps://preview.redd.it/732wrp23pwdg1.png?width=1100&format=png&auto=webp&s=5c65e4fc46e3a8fd8fcac281361ece4328932f2b\n\nYou can construct a neural network with few or multiple hidden layers. The input is time (t) and the output predicted by the neural network is height of the ball (h).\n\nThe neural network will be initialized with random weights. This means the predictions of h(t) made by the neural network will be very bad initially as shown in the image below.\n\nhttps://preview.redd.it/xdgeu9s4pwdg1.png?width=1100&format=png&auto=webp&s=2e97b932fe7bef937f45716295435c7d50c0212f\n\nWe need to penalize the neural network for making these bad predictions right? How do we do that? In the form of loss functions.\n\nLoss of a neural network is a measure of how bad its predictions are compared the real data. The close the predictions and data, the lower the loss.\n\nA singular goal of neural network training is to minimize the loss.\n\nSo how can we define the loss here? Consider the 3 options below.\n\nhttps://preview.redd.it/slcx6y27pwdg1.png?width=1100&format=png&auto=webp&s=fcccb9ec6c9aac8b976b71ae5a7f7f6dfd481c24\n\nIn all the 3 options, you are finding the average of some kind of loss.\n\n* **Option 1 is not good**Â because positive and negative errors will cancel each other.\n* **Option 2 is okay**Â because we are taking the absolute value of errors, but the problem is modulus function is not differentiable at x=0.\n* **Option 3 is the best**. It is a square function which means individual errors are converted to positive numbers and the function is differentiable. This is the famous Mean Squared Error (MSE). You are taking the mean value of the square of all individual errors.\n\nHere error means the difference between actual value and predicted value.\n\nMean Squared Error is minimum when the predictions are very close to the experimental data as shown in the figure below.\n\nhttps://preview.redd.it/vwm6mxq8pwdg1.png?width=1100&format=png&auto=webp&s=33983e165ecec1efca3a973e97b3d28aa2a89782\n\nBut there is a problem with this approach. What if your experimental data was not good? In the image below you can see that one of the data points is not following the trend shown by the rest of the dataset.\n\nhttps://preview.redd.it/mswknvl9pwdg1.png?width=1100&format=png&auto=webp&s=71546cc05f741175a11e486ae3fe6a77c44b82e7\n\nThere can be multiple reasons due to which such data points show up in the data.\n\n1. You did not perform the experiments well. You made a manual mistake while noting the height.\n2. The sensor or instrument using which you were making the height measurement was faulty.\n3. A sudden gush of wind caused a sudden jump in the height of the ball.\n\nThere could be many possibilities that results in outliers and noise in a dataset.\n\nKnowing that real life data may have noise and outliers, it will not be wise if we train a neural network to exactly mimic this dataset. It results in something called as overfitting.\n\nhttps://preview.redd.it/1e7r509apwdg1.png?width=1100&format=png&auto=webp&s=e3269c58b8ca9e873945ca9970aafac78bc53279\n\nhttps://preview.redd.it/l0fgrzrapwdg1.png?width=1100&format=png&auto=webp&s=28acb46d2af8e6398876ee107b7900e860061904\n\nIn the figure above, mean squared error will be low in both cases. However in one case neural network is fitting on outlier also, which is not good. So what should we do?\n\n# Bring physics into the picture\n\nIf you are throwing a ball and observing its physics, then you already have some knowledge about the trajectory of the ball, based on Newtonâ€™s laws of motion.\n\nSure, you may be making simplifications by assuming that the effect of wind or air drag or buoyancy are negligible. But that does not take away from the fact that you already have decent knowledge about this system even in the absence of a trained neural network.\n\nhttps://preview.redd.it/8cudgx0epwdg1.png?width=1100&format=png&auto=webp&s=9efaf22e50525030c0ceaa9995b0afe96a26c79d\n\nThe physics you assume may not be in perfect agreement with the experimental data as shown above, but it makes sense to think that the experiments will not deviate too much from physics.  \n\n\nhttps://preview.redd.it/fpy7q3oepwdg1.png?width=1100&format=png&auto=webp&s=dc5ff5cacaf8b8d2895139589897c6dd3d670be9\n\nSo if one of your experimental data points deviate too much from what physics says, there is probably something wrong with that data point. So how can you let you neural network take care of this?\n\n# How can you teach physics to neural networks?\n\nIf you want to teach physics to neural network, then you have to somehow incentivize neural network to make predictions closer to what is suggested by physics.\n\nIf the neural network makes a prediction where the height of the ball is far away from the purple dotted line, then loss should increase.\n\nIf the predictions are closer to the dotted line, then the loss should be minimum.\n\nWhat does this mean? Modify the loss function.\n\nHow can you modify the loss function such that the loss is high when predictions deviate from physics? And how does this enable the neural network make more physically sensible predictions?Â **Enter PINN Physics Informed Neural Network.**  \n  \nPhysics Informed Neural Network (PINN)\n\nThe goal of PINNs is to solve (*or learn solutions to*) differential equations by embedding the known physics (or governing differential equations) directly into the neural networkâ€™s training objective (loss function).\n\nThe idea of PINNs were introduced in this seminal paper by Maziar Raissi et. al.:Â [https://maziarraissi.github.io/PINNs/](https://maziarraissi.github.io/PINNs/)\n\nThe basic idea in PINN is to have a neural network is trained to minimize a loss function that includes:\n\n1. AÂ **data mismatch**Â term (*if observational data are available*).\n2. AÂ **physics loss**Â term enforcing the differential equation itself (and initial/boundary conditions).\n\n# Let us implement PINN on our example\n\nLet us look at what we know about our example. When a ball is thrown up, it trajectory h(t) varies according to the following ordinary differential equation (ODE).\n\nhttps://preview.redd.it/vacsz6dlpwdg1.png?width=1100&format=png&auto=webp&s=14111c810dba1e861fbcc71a1bf8d920e479448c\n\nHowever this ODE alone cannot fully describe h(t) uniquely. You also need an initial condition. Mathematically this is because to solve a first-order differential equation in time, you need 1 initial condition.\n\nLogically, to know height as a function of time, you need to know the starting height from which the ball was thrown. Look at the image below. In both cases, the balls are thrown at the exact same time with the exact same initial velocity component in the vertical direction. But the h(t) depends on the initial height. So you need to know h(t=0) for fully describing the height of the ball as a function of time.\n\nhttps://preview.redd.it/eobv9u1mpwdg1.png?width=1100&format=png&auto=webp&s=a28a6c8584f37683f703b4c72a5a8f436353dedc\n\nThis means it is not enough to make the neural network make accurate predictions on dh/dt, the neural network should also make accurate prediction on h(t=0) for fully matching the physics in this case.\n\n# Loss due to dh/dt (ODE loss)\n\nWe know the expected dh/dt because we know the initial velocity and acceleration due to gravity.\n\nHow do we get the dh/dt predicted by the neural network? After all it is predicting height h, not velocity v or dh/dt. The answer isÂ **Automatic differentiation (AD).**\n\nBecause most machineâ€learning frameworks (e.g., TensorFlow, PyTorch, JAX) support automatic differentiation, you can compute dh/dt by differentiating the neural network.\n\nThus, we have a predicted dh/dt (from the neural network differentiation) for every experimental time points, and we have an actual dh/dt based on the physics.\n\nhttps://preview.redd.it/msf6gyunpwdg1.png?width=1100&format=png&auto=webp&s=1392d9e60f5ee011a480392af07e05bc5d094492\n\nNow we can define a loss due to the difference between predicted and physics-based dh/dt.\n\nhttps://preview.redd.it/68xl4xpopwdg1.png?width=1100&format=png&auto=webp&s=5b9a727be489bd8736e8ffc235f49fca5dc25b9a\n\nMinimizing this loss (which I prefer to call ODE loss) is a good thing to ensure that neural network learns the ODE. But that is not enough. We need to make the neural network follow the initial condition also. That brings us to the next loss term.Initial condition loss\n\n# Initial condition loss\n\nThis is easy. You know the initial condition. You make the neural network make a prediction of height for t=0. See how far off the prediction is from the reality. You can construct a squared error which can be called as theÂ *Initial Condition Loss.*\n\nhttps://preview.redd.it/4u4syj1qpwdg1.png?width=1100&format=png&auto=webp&s=591b7e0f46ebf32024533c9d727042a889c3007d\n\nSo is that it? You have ODE loss and Initial condition loss. Is it enough that the neural network tries to minimize these 2 losses? What about the experimental data? There are 3 things to consider.\n\n1. You cannot throw away the experimental data.\n2. You cannot neglect the physics described by the ODEs or PDEs.\n3. You cannot neglect the initial and/or boundary conditions.\n\nThus you have to also consider the data-based mean squared error loss along with ODE loss and Initial condition loss.\n\n# The modified loss term\n\nThe simple mean squared error based loss term can now be modified like below.\n\nhttps://preview.redd.it/n2xc18prpwdg1.png?width=1100&format=png&auto=webp&s=95fabc8b54b2b291292d6ab2c15f5810c13379ce\n\nIf there are boundary conditions in addition to initial conditions, you can add an additional term based on the difference between predicted boundary conditions and actual boundary conditions.\n\nhttps://preview.redd.it/ezh3in7spwdg1.png?width=1100&format=png&auto=webp&s=70367e6fbb1aa6e7924d93da8ff3b0ce8898419d\n\nHere the Data loss term ensures that the predictions are not too far from the experimental data points.\n\nTheÂ *ODE loss term*Â \\+ theÂ *initial condition loss term*Â ensures that the predictions are not too far from what described by the physics.\n\nIf you are pretty sure about the physics the you can set Î»1 to zero. In the ball throwing experiment, you will be sure about the physics described by our ODE if air drag, wind, buoyancy and any other factors are ignored. Only gravity is present. And in such cases, the PINN effectively becomes an ODE solver.\n\nHowever, for real life cases where only part of the physics is known or if you are not fully sure of the ODE, then you retain Î»1 and other Î» terms in the net loss term. That way you force the neural network to respect physics as well as the experimental data. This also suppress the effects of experimental noise and outliers.\n\n>",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qfcotf/an_introduction_to_physics_informed_neural/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o03l86k",
          "author": "n0obmaster699",
          "text": "So you just add a lagrange multiplier which follows the eom?",
          "score": 5,
          "created_utc": "2026-01-17 13:11:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o08qh47",
          "author": "inmadisonforabit",
          "text": "Look into regularization. Also, if you need your models to respect physics, it would be best to avoid using a NN to begin with and instead directly model it via the ODEs (in reality, likely PDEs) you're already using.",
          "score": 4,
          "created_utc": "2026-01-18 05:16:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qao4vr",
      "title": "Machine learning peer group",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qao4vr/machine_learning_peer_group/",
      "author": "Several-Praline-5160",
      "created_utc": "2026-01-12 06:59:50",
      "score": 52,
      "num_comments": 50,
      "upvote_ratio": 0.98,
      "text": "We have created a ml peer group to build projects together learn new stuff solving doubts etc if anyone is interested dm me and upvote this",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qao4vr/machine_learning_peer_group/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "nz4yhdo",
          "author": "ImaginaryGold6836",
          "text": "interested",
          "score": 1,
          "created_utc": "2026-01-12 09:58:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5a4sy",
              "author": "Several-Praline-5160",
              "text": "Dm",
              "score": 1,
              "created_utc": "2026-01-12 11:42:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz58g49",
          "author": "Initial_Fig_1101",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-12 11:28:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5a4fg",
              "author": "Several-Praline-5160",
              "text": "Dm",
              "score": 1,
              "created_utc": "2026-01-12 11:42:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz5p0so",
          "author": "Big-Touch-5408",
          "text": "interested",
          "score": 1,
          "created_utc": "2026-01-12 13:25:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5q88d",
              "author": "Several-Praline-5160",
              "text": "Dm",
              "score": 1,
              "created_utc": "2026-01-12 13:32:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz5qasr",
                  "author": "Big-Touch-5408",
                  "text": "dmed u",
                  "score": 1,
                  "created_utc": "2026-01-12 13:32:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz5qzou",
          "author": "Hopeful_Pay_1615",
          "text": "Are you targeting people who already have experience building ML systems or you're open to beginners as well?",
          "score": 1,
          "created_utc": "2026-01-12 13:36:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5rt97",
              "author": "Several-Praline-5160",
              "text": "Both experienced persons can give guidance to begineers and begineers will learn from them and grow",
              "score": 1,
              "created_utc": "2026-01-12 13:41:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz5s0jm",
                  "author": "Hopeful_Pay_1615",
                  "text": "Alright, I'm interested then",
                  "score": 1,
                  "created_utc": "2026-01-12 13:42:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz5r2ux",
          "author": "No-Butterscotch9679",
          "text": "intrested",
          "score": 1,
          "created_utc": "2026-01-12 13:37:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5tr1d",
          "author": "infinty1729",
          "text": "interested",
          "score": 1,
          "created_utc": "2026-01-12 13:52:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz67vgb",
          "author": "Pure_Play_5650",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-12 15:06:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6b086",
          "author": "InternalQuestion898",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-12 15:22:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6bpwk",
          "author": "arsenic-ofc",
          "text": "hmu",
          "score": 1,
          "created_utc": "2026-01-12 15:25:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6jx4g",
          "author": "hulk1432",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-12 16:04:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz73qye",
          "author": "alter_Dex",
          "text": "InterestedÂ ",
          "score": 1,
          "created_utc": "2026-01-12 17:35:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7cgfp",
          "author": "onepiece_luffy-",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-12 18:14:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7doh0",
          "author": "yournext78",
          "text": "Share the group link that",
          "score": 1,
          "created_utc": "2026-01-12 18:19:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7g4wc",
          "author": "Appropriate_Try_5567",
          "text": "interested",
          "score": 1,
          "created_utc": "2026-01-12 18:30:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7gcxm",
          "author": "Taboosh321",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-12 18:31:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7jpcf",
          "author": "Rockwith_You",
          "text": "I'm in",
          "score": 1,
          "created_utc": "2026-01-12 18:46:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7rmcc",
          "author": "bibbletrash",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-12 19:22:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7t1yq",
          "author": "NeverAgainEverPls",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-12 19:29:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz88dfo",
          "author": "Lost-Machine-5395",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-12 20:40:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8eaw6",
          "author": "Specialist-Till-637",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-12 21:08:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8p3bh",
          "author": "szalonymjut",
          "text": "interested",
          "score": 1,
          "created_utc": "2026-01-12 21:58:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8p7lg",
          "author": "Similar-Stop7772",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-12 21:58:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8tr6c",
          "author": "SteakStrict1737",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-12 22:20:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8u6x6",
          "author": "AppointmentWest7876",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-12 22:22:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8vimz",
          "author": "Illustrious_Code_419",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-12 22:29:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz9tck1",
          "author": "Dear_Delivery533",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-13 01:29:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzae4nr",
          "author": "Acceptable-Leopard58",
          "text": "Dm",
          "score": 1,
          "created_utc": "2026-01-13 03:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzappyy",
          "author": "Glittering-Dress-681",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-13 04:26:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzb1k9j",
          "author": "Nuclear_fusion1784",
          "text": "Hey",
          "score": 1,
          "created_utc": "2026-01-13 05:49:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzcfau0",
          "author": "Admirable-Action-153",
          "text": "Yeah, leat do it",
          "score": 1,
          "created_utc": "2026-01-13 12:58:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nze6mo8",
          "author": "Amazing_Swing1357",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-13 18:21:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfdfwd",
          "author": "gocurl",
          "text": "That immediately feels like a scam",
          "score": 1,
          "created_utc": "2026-01-13 21:37:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhdxsi",
          "author": "Hot-Common-7617",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-14 04:13:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhtdlw",
          "author": "lunasoulshine",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-14 06:06:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzi4tbw",
          "author": "geekyayaz",
          "text": "Is it for beginners? If yes then please add me",
          "score": 1,
          "created_utc": "2026-01-14 07:46:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzimflc",
          "author": "Double-Bunch-71",
          "text": "interested",
          "score": 1,
          "created_utc": "2026-01-14 10:34:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvfhkp",
          "author": "ReputationNo1600",
          "text": "Interested",
          "score": 1,
          "created_utc": "2026-01-16 05:50:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o09hope",
          "author": "mathematical_retard",
          "text": "Am interested can I dm you?",
          "score": 1,
          "created_utc": "2026-01-18 09:09:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09iahk",
              "author": "Several-Praline-5160",
              "text": "Yes why not",
              "score": 1,
              "created_utc": "2026-01-18 09:15:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz6czi0",
          "author": "Several-Praline-5160",
          "text": "Dm me so I can share the link of gc and upvote",
          "score": 0,
          "created_utc": "2026-01-12 15:32:01",
          "is_submitter": true,
          "replies": [
            {
              "id": "nzac4ou",
              "author": "bensunnnn",
              "text": "Interested",
              "score": 1,
              "created_utc": "2026-01-13 03:09:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzb7llj",
              "author": "PSRebel512",
              "text": "Interested",
              "score": 1,
              "created_utc": "2026-01-13 06:38:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qf92tn",
      "title": "I implemented a GPT-style model from scratch using PyTorch to understand the math behind Attention & Fine-tuning (following Sebastian Raschka's book)",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qf92tn/i_implemented_a_gptstyle_model_from_scratch_using/",
      "author": "Bthreethree",
      "created_utc": "2026-01-17 09:39:19",
      "score": 40,
      "num_comments": 6,
      "upvote_ratio": 0.96,
      "text": "I've spent the last few weeks building a GPT-style LLM entirely from scratch in PyTorch to understand the architecture. This isn't just a wrapper; it's a full implementation covering the entire lifecycle from tokenization to instruction fine-tuning.\n\nI have followed Sebastian Raschka's 'Build a LLM from Scratch' book for the implementation, here is the breakdown of the repo:\n\n**1. Data & Tokenization (**`src/data.py`**)** Instead of using pre-built tokenizers, I implemented:\n\n* `SimpleTokenizerV2`: Handles regex-based splitting and special tokens (`<|endoftext|>`, `<|unk|>`).\n* `GPTDatasetV1`: A sliding-window dataset implementation for efficient autoregressive training.\n\n**2. The Attention Mechanism (**`src/attention.py`**)**\n\nI manually implemented `MultiHeadAttention` to understand the tensor math:\n\n* Handles the query/key/value projections and splitting heads.\n* Implements the **Causal Mask** (using `register_buffer`) to prevent the model from \"cheating\" by seeing future tokens.\n* Includes `SpatialDropout` and scaled dot-product attention.\n\n**3. The GPT Architecture (**`src/model.py`**)** A complete 124M parameter model assembly:\n\n* Combines `TransformerBlock`, `LayerNorm`, and `GELU` activations.\n* Features positional embeddings and residual connections exactly matching the GPT-2 spec.\n\n**4. Training & Generation (**`src/train.py`**)**\n\n* Custom training loop with loss visualization.\n* Implements `generate()` with **Top-K sampling** and **Temperature scaling** to control output creativity.\n\n**5. Fine-tuning:**\n\n* **Classification (**`src/finetune_classification.py`**):** Adapted the backbone to detect Spam/Ham messages (90%+ accuracy on the test set).\n* **Instruction Tuning (**`src/finetune_instructions.py`**):** Implemented an Alpaca-style training loop. The model can now handle instruction-response pairs rather than just completing text.\n\n**Repo:** [https://github.com/Nikshaan/llm-from-scratch](https://github.com/Nikshaan/llm-from-scratch)\n\nIâ€™ve tried to comment every shape transformation in the code. If you are learning this stuff too, I hope this reference helps!",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qf92tn/i_implemented_a_gptstyle_model_from_scratch_using/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o02wnfv",
          "author": "Bthreethree",
          "text": "This is the code snippet of the most interesting part - building Multi-head attention from scratch instead of using nn.MultiheadAttention.\n\n[https://github.com/Nikshaan/llm-from-scratch](https://github.com/Nikshaan/llm-from-scratch)\n\nclass MultiHeadAttention(nn.Module):  \ndef \\_\\_init\\_\\_(self, d\\_in, d\\_out, context\\_length, dropout, num\\_heads, qkv\\_bias=False): # context length is max sequence length for the mask  \nsuper().\\_\\_init\\_\\_()  \nassert d\\_out % num\\_heads == 0, \"d\\_out must be divisible by num\\_heads\"  \nself.d\\_out = d\\_out  \nself.num\\_heads = num\\_heads  \nself.head\\_dim = d\\_out // num\\_heads # dimension per head  \nself.W\\_query = nn.Linear(d\\_in, d\\_out, bias=qkv\\_bias)  \nself.W\\_key = nn.Linear(d\\_in, d\\_out, bias=qkv\\_bias)  \nself.W\\_value = nn.Linear(d\\_in, d\\_out, bias=qkv\\_bias)  \nself.out\\_proj = nn.Linear(d\\_out, d\\_out)  \nself.dropout = nn.Dropout(dropout)  \nself.register\\_buffer(\"mask\", torch.triu(torch.ones((context\\_length, context\\_length)) \\* float('-inf'), diagonal=1))  \n  \ndef forward(self, x):  \nb, num\\_tokens, d\\_in = x.shape  \nkeys = self.W\\_key(x)  \nqueries = self.W\\_query(x)  \nvalues = self.W\\_value(x)  \n  \nkeys = keys.view(b, num\\_tokens, self.num\\_heads, self.head\\_dim) # reshape for multi-head  \nqueries = queries.view(b, num\\_tokens, self.num\\_heads, self.head\\_dim)  \nvalues = values.view(b, num\\_tokens, self.num\\_heads, self.head\\_dim)  \n  \nkeys.transpose\\_(1, 2) # move head dimension to the front so that it is treated as batch dimension  \nqueries.transpose\\_(1, 2)  \nvalues.transpose\\_(1, 2)  \n  \nattn\\_scores = queries @ keys.transpose(2, 3) # flip last two dimensions for dot product  \nmask\\_bool = self.mask.bool()\\[:num\\_tokens, :num\\_tokens\\]  \nattn\\_scores.masked\\_fill\\_(mask\\_bool, -torch.inf)  \nattn\\_weights = torch.softmax(attn\\_scores / self.head\\_dim\\*\\*0.5, dim=-1)  \nattn\\_weights = self.dropout(attn\\_weights)  \ncontext\\_vec = (attn\\_weights @ values).transpose(1, 2).contiguous().view(b, num\\_tokens, self.d\\_out) # reshape back to original  \ncontext\\_vec = self.out\\_proj(context\\_vec) # final linear layer to mix heads  \nreturn context\\_vec",
          "score": 3,
          "created_utc": "2026-01-17 09:42:31",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o09fznv",
          "author": "chrisvdweth",
          "text": "Thanks for sharing. I have a few comments/questions:\n\n* Why does \\`attention.py\\` have so many classes? Some of them don't seem to be used meaningfully?\n* Not sure why you organized the code like that. For example, when importing \\`attention.py\\`, all the code gets executed. Why not do it cleaner cleaner with \\`if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":\\`?\n* Well, your \\`create\\_dataloader\\_v1\\` method seems to use a pretrained BPE tokenizer after all :). At least, I couldn't spot any code that trains a BPE tokenizer from scratch.",
          "score": 2,
          "created_utc": "2026-01-18 08:53:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0bjpd4",
              "author": "Bthreethree",
              "text": "Hey, the repo is an implementation of Sebastian Roschka's Build a LLM from scratch book, thus while learning and implementing from the book, I have made many classes which have improved further down in the file.\n\nYup I did forget to add \\`if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":\\` and will do that for sure! Thanks! :)",
              "score": 1,
              "created_utc": "2026-01-18 17:12:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o07h4dk",
          "author": "Bthreethree",
          "text": "I have added a colab notebook link in the readme of the repo on github to show the final results! The accuracy can be made better with experimentation of hyperparamaters & further fine-tuning.\n\n[https://github.com/Nikshaan/llm-from-scratch](https://github.com/Nikshaan/llm-from-scratch)",
          "score": 1,
          "created_utc": "2026-01-18 00:54:27",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o0b48se",
          "author": "Better_Pair_4608",
          "text": "How many parameters does your model have?",
          "score": 1,
          "created_utc": "2026-01-18 15:59:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0bjueb",
              "author": "Bthreethree",
              "text": "The model is trained over 124M parameters (inspired by GPT-2 architecture)",
              "score": 1,
              "created_utc": "2026-01-18 17:13:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qfkal8",
      "title": "Which subfields of ML can I realistically achieve PhD level mastery of by self study at home with limited budget?",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qfkal8/which_subfields_of_ml_can_i_realistically_achieve/",
      "author": "Proof-Bed-6928",
      "created_utc": "2026-01-17 18:08:35",
      "score": 36,
      "num_comments": 36,
      "upvote_ratio": 0.73,
      "text": "Suppose you have somehow managed to generate 25k disposable income and only work 20hours a week so you have plenty of free time. You want to dedicate the remaining time to the mastery of one small but important ML niche just for the sake of it. To the level where you can theoretically waltz into a room full of FAANG level ML engineers and impress them with your contributions.\n\nIt will have to be a subfields where your competitive advantage plateaus with capital after some number (so not some compute arms race like LLM). \n\nWhich subfields in ML is this possible? What kind of benchmarks can you use to validate? How do you know youâ€™ve learned something without being in a university surrounded by academics?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qfkal8/which_subfields_of_ml_can_i_realistically_achieve/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o07bjun",
          "author": "notsofastaicoder",
          "text": "I think you are over estimating FAANG level ML engineers, and you don't need to a Phd to impress people either.\n\nSince most of these engineers do applied engineering, they are looking for solutions to current problems.\n\nMy experience has been in applied software engineering, worked in FAANG and AI startups. So advice from a non PhD guy:\n\nAfter you establish the base skills for ML, focus on an area that truly excites you, then make a meaningful contribution. Either by proving a hypothesis, or making an incremental improvement to existing solutions.\n\nBeing able to do that, in your own time, is really impressive.",
          "score": 58,
          "created_utc": "2026-01-18 00:24:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09evq5",
              "author": "belabacsijolvan",
              "text": "also [https://www.kaggle.com/competitions](https://www.kaggle.com/competitions)",
              "score": 5,
              "created_utc": "2026-01-18 08:43:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0dqr71",
              "author": "ChillmanITB",
              "text": "read an academic paper everyday, lots of books on theory, do all the free courses on calculus and linear algebra, probability and statistics (This is where you will develop an actual understanding), learn to code if you have not already and try and come up with new methods of improving current algorithms or create new ones all together.",
              "score": 1,
              "created_utc": "2026-01-18 23:38:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o06dtfy",
          "author": "williamkotoco_",
          "text": "anything related to efficient ml techniques such as pruning, quantization,  nas, efficient architectures and edge ai in general",
          "score": 17,
          "created_utc": "2026-01-17 21:32:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06slmz",
              "author": "Altruistic_Basis_69",
              "text": "My PhD was in DL optimization and I agree with this (though I am a little biased tbf). Compared to other niches, Iâ€™d argue that architecture optimization is one of the more intuitive areas to look into as opposed to some of the faster growing fields where you wonâ€™t even get a chance to catch-up.",
              "score": 8,
              "created_utc": "2026-01-17 22:45:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o06gpd8",
              "author": "Annual-Salamander-85",
              "text": "These are all pretty deep, why do you say that?",
              "score": 3,
              "created_utc": "2026-01-17 21:46:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o077s6k",
          "author": "Playful-Score-67",
          "text": "PhD level? It takes 5+ years of intense studying, independent research, mentoring from someone who is an expert on the field and contact with an academic community that can support and answer questions. It's not only about \"self study\".\nSo, realistically? Nah.",
          "score": 39,
          "created_utc": "2026-01-18 00:04:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o08wtuv",
              "author": "BreadBrowser",
              "text": "Wellâ€¦ I donâ€™t think I learned a single god damned thing from my advisor. Fuck that guy. Why I didnâ€™t leave Iâ€™ll never know. But yeah, you need a good five intense years.",
              "score": 2,
              "created_utc": "2026-01-18 06:04:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o09d6yl",
                  "author": "digiorno",
                  "text": "Thatâ€™s a tragedy, if someone wants to be an advisor then they should take that role seriously and genuinely try to improve the next generation of researchers.",
                  "score": 3,
                  "created_utc": "2026-01-18 08:27:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o08nsh8",
              "author": "arihoenig",
              "text": "Nonsense, Faraday was self taught as was Galileo and Joule. All of their contributions are PhD level in the fields in which they contributed.",
              "score": -3,
              "created_utc": "2026-01-18 04:57:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o08tzoo",
                  "author": "RatKnees",
                  "text": "Ah yes, people born 200+ years ago. \n\nI don't disagree that anyone can make a contribution, but listing people from pre-electricity feels disingenuous",
                  "score": 19,
                  "created_utc": "2026-01-18 05:42:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o07b6gd",
          "author": "nickpsecurity",
          "text": "Any of the older techniques that are still widely used in industry. Then, smaller NN's, GA's, and LLM's. Also, techniques for things like time series and tabukar data which are highly important in industry but get little press or investment like GPT. That's the answer to your title.\n\nFar as FAANG, this [person](https://www.trybackprop.com/blog/top_ml_learning_resources) shares what they studied to do something like that.",
          "score": 7,
          "created_utc": "2026-01-18 00:22:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0awr63",
              "author": "iamevpo",
              "text": "What is a GA here?",
              "score": 1,
              "created_utc": "2026-01-18 15:22:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0bde7w",
                  "author": "NightmareLogic420",
                  "text": "Genetic Algorithms, I believe",
                  "score": 3,
                  "created_utc": "2026-01-18 16:42:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0btu0u",
                  "author": "nickpsecurity",
                  "text": "Genetic algorithms. They call the broader field evolutionary algorithms because they claim to use a highly-simplifies version of its principles. It's an optimization technique capable of working with non-differentiable data without getting stuck in local minima or maxima. It has heavier computation, though.\n\nIIRC, the top methods are differentiable evolution (in SciPy, too) and evolution strategies. If you use GA's, try tournament selection with tournament size of 7. Coevolution, or fitness tests evolving with the population, historically outperformed static, fitness measures. Such methods can be combined with NN's (neuroevolution) to find architectures, weights, hyperparameters, etc.",
                  "score": 1,
                  "created_utc": "2026-01-18 18:00:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o06rxw3",
          "author": "seriousgourmetshit",
          "text": "I don't think its possible to reach the level you are thinking of tbh",
          "score": 27,
          "created_utc": "2026-01-17 22:42:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o08n6md",
              "author": "arihoenig",
              "text": "George Boole was an autodidact. LLMs are just large assemblies of the logic he proposed.",
              "score": 5,
              "created_utc": "2026-01-18 04:53:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0b1060",
                  "author": "jonsca",
                  "text": "Found the guy who knows nothing about nothingÂ ",
                  "score": 4,
                  "created_utc": "2026-01-18 15:43:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o08ks4e",
          "author": "Complex_Medium_7125",
          "text": "a PhD teaches you how to do novel research, is that what you want to do?  \nif not, taking some grad courses in ml from stanford/berkeley/cmu gets you the foundations of solid ml engineer level\n\ncheck out [https://stanford-cs336.github.io/spring2025/](https://stanford-cs336.github.io/spring2025/) and do the homeworks",
          "score": 5,
          "created_utc": "2026-01-18 04:36:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06x83c",
          "author": "UngratefulSheeple",
          "text": "Thatâ€™s not gonna happen bro ðŸ˜‚ðŸ˜‚\n\nSincerely, someone doing a phd in ai right now.",
          "score": 8,
          "created_utc": "2026-01-17 23:09:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o09cfu5",
          "author": "digiorno",
          "text": "For actual ML research and not just application, you need math skills. And sure those can be self taught but it could be incredibly difficult to get good at without the guidance of professors. \n\nSecondly for cutting edge stuff, a lot of that so done via collaborative efforts with other theorists in an academic setting.\n\nIf you get good enough at the math and have some novel ideas then a school or research would likely give you admittance to a PhD program so that you could work with them, learn from them and join their ranks. And they would pay you a bit more than the amount you mentioned.",
          "score": 2,
          "created_utc": "2026-01-18 08:20:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0e9h02",
          "author": "mpaes98",
          "text": "Iâ€™ll be real with you, a PhD is basically self studying while also having to juggle 1-2 exploitative jobs where you report to different tiers of narcissistic individuals who have no proper management experience, and donâ€™t even get me started on how bad teaching duties have gotten (overbloated classes, unprepared undergrads/grads).\n\nI had a decent experience, but itâ€™s a rough time and now weâ€™re in a terrible market.",
          "score": 2,
          "created_utc": "2026-01-19 01:18:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o07fihm",
          "author": "strangeanswers",
          "text": "Iâ€™d argue that this is possible in the operationalization of LLMs to address business problems such as automated customer support, sentiment scraping or any other forms of information retrieval and programmatic decision-making. \n\nyou need a firm grasp of how LLMs work, strong software engineering technicals, LLMOps/DevOps, prompt & context engineering skills and good product reasoning. each of these skills is difficult to develop but Iâ€™d argue that itâ€™s feasible to become elite in this problem space through lots of self study and practice and very solid applied projects where you actually develop and deploy cloud infrastructure that can handle high throughput.",
          "score": 2,
          "created_utc": "2026-01-18 00:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05bvvp",
          "author": "tewmuchdrama",
          "text": "Reinforcement Learning",
          "score": -8,
          "created_utc": "2026-01-17 18:25:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o080svj",
              "author": "jsh_",
              "text": "?",
              "score": 1,
              "created_utc": "2026-01-18 02:39:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o05udrd",
              "author": "pb_syr",
              "text": "Why",
              "score": 0,
              "created_utc": "2026-01-17 19:52:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qg28lm",
      "title": "Built a Multi-Source Knowledge Discovery API (arXiv, GitHub, YouTube, Kaggle) â€” looking for feedback",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qg28lm/built_a_multisource_knowledge_discovery_api_arxiv/",
      "author": "Appropriate_West_879",
      "created_utc": "2026-01-18 07:29:50",
      "score": 25,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nIâ€™ve just finished building an open-source project called Knowledge Universe API and pushed it to GitHub.\n\nItâ€™s a FastAPI-based backend that discovers and ranks educational / technical resources from multiple sources in a single request.\n\nWhat it does (purely technical)\n\nParallel crawling from:\n\narXiv\n\nGitHub\n\nYouTube\n\nKaggle (API-based)\n\nUnified response schema across all sources\n\nQuality scoring pipeline (difficulty alignment, freshness, accessibility, social signals)\n\nRedis-based caching with TTL + background refresh\n\nAsync orchestration with timeout isolation per crawler\n\nDeduplication + diversity filtering\n\nDocker + local Redis support\n\nWhy I built it\n\nI wanted a single API that returns ranked, clean, structured learning resources instead of manually searching each platform.\n\nThis was mostly a backend / systems exercise:\n\nasync pipelines\n\ncrawling reliability\n\nscoring consistency\n\ncache correctness\n\nStack\n\nPython 3.11\n\nFastAPI\n\nhttpx / asyncio\n\nRedis\n\nDocker\n\nPydantic v2\n\nRepo\n\nðŸ‘‰ GitHub: https://github.com/VLSiddarth/Knowledge-Universe.git\n\nWhat Iâ€™m looking for Open contribution to add new source collection from Internet to Create \"Knowledge Universe API\",\n\nCode review (especially async orchestration & scoring)\n\nArchitecture feedback\n\nAny obvious mistakes / improvements\n\nNot promoting anything â€” just sharing what I built and learning from feedback.\n\nThanks ðŸ™",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qg28lm/built_a_multisource_knowledge_discovery_api_arxiv/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qbodtd",
      "title": "Looking for project ideas in ML",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qbodtd/looking_for_project_ideas_in_ml/",
      "author": "chiken-dinner458",
      "created_utc": "2026-01-13 10:56:25",
      "score": 24,
      "num_comments": 13,
      "upvote_ratio": 0.89,
      "text": "I have a project going on and have been looking for some projects for some time. my initial project idea got regected. Can anyone suggest some ML project ideas .. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qbodtd/looking_for_project_ideas_in_ml/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "nzc0rym",
          "author": "Decent-Pool4058",
          "text": "1. Implement a research paper by yourself\n\n2. Build something that is relat ed to your perssonal life or hobby. ie AI Agent to file taxes.",
          "score": 9,
          "created_utc": "2026-01-13 11:09:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzc7o74",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 9,
          "created_utc": "2026-01-13 12:06:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzc9i4n",
              "author": "chiken-dinner458",
              "text": "Ok",
              "score": 1,
              "created_utc": "2026-01-13 12:19:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzekhop",
          "author": "ExampleSweet3152",
          "text": "It would help if you could narrow down what domain of Machine Learning you wish to work in the first place. ML is a huge domain from traditional statistical models to transformers and LLM's. You need to narrow it down to a domain and then proceed with an idea in the same, preferably start by reading and gradually implementing a simple research paper in that domain to get the gist of it followed by something a bit more novel. This is typically the \"right\" way to do something like this. There is a lot going on in the ML space and I am certain you will discover something novel or at the very least acceptable if you are vigilant enough.",
          "score": 2,
          "created_utc": "2026-01-13 19:23:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzes7n7",
          "author": "Lside0",
          "text": "Check out kaggle. There dozens of datasets with clean nice data. Start drom them until youn get comfortable with ml models. \n\nFrom there i would suggest to pick a topic you like, study the state of the art and create a new model tonsolve a real problem.",
          "score": 2,
          "created_utc": "2026-01-13 19:58:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzew33p",
          "author": "Apart_Situation972",
          "text": "Hi make an AI judge. \n\nBased on the defendant's case + historic offences, determine the correct sentencing (by comparing it to other cases of similar nature).",
          "score": 1,
          "created_utc": "2026-01-13 20:16:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzf5c98",
              "author": "Lost-Machine-5395",
              "text": "Interesting idea",
              "score": 1,
              "created_utc": "2026-01-13 21:00:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzffctw",
          "author": "Azaze-",
          "text": "Pipeline data validation and error handling, create a model that uses anomaly detection + LLM to detect anomalies and errors during the data processing, and the LLM part will be to explain the problem or even take actions ðŸ¤·â€â™‚ï¸ really cool project",
          "score": 1,
          "created_utc": "2026-01-13 21:46:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgehmd",
          "author": "Ty4Readin",
          "text": "What are you interested in?\n\nDo you like Minecraft? Why not build an ML agent to play? Or any other game?\n\nDo you like a specific sport? Why not build prediction models for it.\n\nDo you like to cook? Maybe build an ML model to identify high quality recipes.\n\nDo you like to make films? Why not build an ML model to help with some part of the process whether its cutting, composting, visual effects work, etc.\n\nDo you like music? What about an automated ML model that can predict interesting characteristics of a song that you might find useful?\n\nComing up with an idea for a project that you are passionate about and that might be useful is the most important part imo. There are so many benefits to learning and is much better than choosing some random dataset on kaggle and training a model.",
          "score": 1,
          "created_utc": "2026-01-14 00:47:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgqb8u",
          "author": "chrisvdweth",
          "text": "Rejected by whom and why?",
          "score": 1,
          "created_utc": "2026-01-14 01:54:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgzmri",
          "author": "Straight-Ad5757",
          "text": "Try working on active kaggle problems and experimenting around it. Itâ€™s like start with kaggle competitions, try one approach get a score now try to read some more research on the same topic and try a different approach. Itâ€™s simply exploring more and new things but ina guided competition way.",
          "score": 1,
          "created_utc": "2026-01-14 02:47:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmuyi3",
          "author": "asshat0064",
          "text": "There are open source projects that use ML. You could find one that you find interesting and then look at issues that are easy to contribute to. And obviously look at kaggle",
          "score": 1,
          "created_utc": "2026-01-14 23:35:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzcylbh",
          "author": "DataCamp",
          "text": "* House price or rent prediction (city-specific)\n* Customer churn prediction\n* Sales or demand forecasting\n* Credit card fraud detection\n* Student performance prediction\n* Loan approval prediction\n* Employee attrition prediction\n* E-commerce product recommendation\n* Energy consumption prediction\n* Traffic or accident risk prediction\n* Customer segmentation (clustering)\n* Movie or music rating prediction\n* Inventory stock prediction\n* Insurance claim risk prediction\n* Bike sharing usage prediction",
          "score": -1,
          "created_utc": "2026-01-13 14:45:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzd8rjk",
              "author": "TraditionalNumber353",
              "text": "This is the most common thing I've read this year.",
              "score": 7,
              "created_utc": "2026-01-13 15:35:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qchxs4",
      "title": "Anyone using AI just for productivity (not side hustles)?",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qchxs4/anyone_using_ai_just_for_productivity_not_side/",
      "author": "Coffee_Talkerr",
      "created_utc": "2026-01-14 08:23:12",
      "score": 21,
      "num_comments": 13,
      "upvote_ratio": 0.82,
      "text": "Most AI content online is about making money or side hustles.\n\nI attended a Be10X workshop that focused more on:\n\nSaving time\n\nWorking smarter\n\nReducing mental load\n\nThat angle felt refreshing. Not everything needs to be monetized.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qchxs4/anyone_using_ai_just_for_productivity_not_side/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "nziae1a",
          "author": "Top_Bicycle_2430",
          "text": "Using AI is effective for coding.",
          "score": 6,
          "created_utc": "2026-01-14 08:39:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjfhq6",
              "author": "hc_fella",
              "text": "Only as a bumped up autocomplete or StackOverflow alternative though. Using LLMs for larger tasks has churned out some aweful stuff in my experience.",
              "score": 8,
              "created_utc": "2026-01-14 13:58:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzu3d0c",
                  "author": "xxtherealgbhxx",
                  "text": "I'm curious. Define \"larger stuff\"? I'm broadly clueless and just use LLMs to code everything as without it I code nothing. I know what I do is ridiculous small stuff just for me but I do wonder what is classed as a large task. Thanks",
                  "score": 1,
                  "created_utc": "2026-01-16 01:00:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzu3v59",
          "author": "Icy_Computer2309",
          "text": "This is exactly how I see it, a tool that allows you to 10x your output. Unemployment wonâ€™t happen because AI can replace people. The unemployment issue will happened because people wonâ€™t use it as a productivity tool.  They will make themselves unemployable by trying to use AI for tasks they have no expertise or knowledge. They will 10x their own stupidity. Ai will make them more efficient at being stupid, like trying 30 dumb fuck money making side hustles, as seen on social media, all at the same time.",
          "score": 2,
          "created_utc": "2026-01-16 01:03:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzj4xvq",
          "author": "Adarshsgh",
          "text": "It has access to all my personal and professional notes, I use it to identify gaps in my learnings and ask questions on top of all notes I have collected. Pretty helpful so far.  \nI use plain text notes + antigravity (free version)",
          "score": 1,
          "created_utc": "2026-01-14 12:57:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkbh52",
          "author": "Specialist-Till-637",
          "text": "My AI does a good job help me remember things",
          "score": 1,
          "created_utc": "2026-01-14 16:33:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvc4zn",
          "author": "letsbreakstuff",
          "text": "Sheeple, this is an ad",
          "score": 1,
          "created_utc": "2026-01-16 05:26:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzib2b3",
          "author": "pauliusztin",
          "text": "I am using Obsidian + Claude Code/Cursor to manage my Second Brain (notes, sources, projects), but regardless of the tooling you use, it's more about your \"productivity\" processes",
          "score": 1,
          "created_utc": "2026-01-14 08:45:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzikeq2",
          "author": "Smergmerg432",
          "text": "I used to do that, but itâ€™s not helpful anymore; they redesigned ChatGPT, and Iâ€™ve noticed Qwen tends to then follow suit. Gemini, Grok, and Claude have never been as good at subdividing work or making practical suggestions as ChatGPT4.1. I agree, though, that to me was the biggest draw!",
          "score": 0,
          "created_utc": "2026-01-14 10:16:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdp54a",
      "title": "From Notebook to Production: A 3-Month Data Engineering Roadmap for ML Engineers on GCP",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qdp54a/from_notebook_to_production_a_3month_data/",
      "author": "IT_Certguru",
      "created_utc": "2026-01-15 17:03:59",
      "score": 21,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "I spent the last 6 months learning how to productionize ML models on Google Cloud. I realized many of us (myself included) get stuck in \"Jupyter Notebook Purgatory.\" Here is the complete roadmap I used to learn Data Engineering specifically for ML.\n\nPhase 1: The Foundation (Weeks 1-4)\n\n* Identity & Access (IAM): Why your permissions always fail and how to fix them.\n* Compute Engine vs. Cloud Run: When to use which for serving models.\n\nPhase 2: The Data Pipeline (Weeks 5-8)\n\n* BigQuery: It's not just for SQL. Using BQML (BigQuery ML) to train models without moving data.\n* Dataflow (Apache Beam): Real-time data processing.\n* Project Idea: Build a pipeline that ingests live crypto/stock data -> Pub/Sub -> Dataflow -> BigQuery.\n\nPhase 3: Orchestration & MLOps (Weeks 9-12)\n\n* Cloud Composer (Airflow): Scheduling your retraining jobs.\n* Vertex AI: The holy grail. Managing feature stores and model registry.\n\nIf anyone wants a more structured path for the data engineering side, this course helped me connect a lot of the dots from notebooks to production: [Data Engineering on Google Cloud](https://www.netcomlearning.com/course/data-engineering-on-google-cloud)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qdp54a/from_notebook_to_production_a_3month_data/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qdi7zm",
      "title": "Confused between Data Engineering and Machine Learning as a beginner",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qdi7zm/confused_between_data_engineering_and_machine/",
      "author": "FyodorAgape",
      "created_utc": "2026-01-15 12:29:22",
      "score": 19,
      "num_comments": 19,
      "upvote_ratio": 0.88,
      "text": "Hi everyone,\n\nI have done a few small projects and mostly learn by Googling things and trying stuff out. Sometimes I feel like I still do not know much, which is probably normal at this stage.\n\nI have been stuck trying to choose between Data Engineering and Machine Learning as a career path. Every time I read Reddit or Twitter, I see totally different opinions. Some people say DE is more stable and practical, others say ML is more interesting but very competitive. Honestly it is making me more confused than helping.\n\nA bit about me:\n\n* Still early in coding, no real industry experience yet\n* I enjoy understanding concepts and the â€œwhyâ€ behind things\n* I get overwhelmed when there are too many tools and technologies at once\n* I would rather build and learn gradually instead of jumping into heavy cloud and infra immediately\n* Long term I care about enjoying the work and not burning out\n* money\n\nMy questions:\n\n1. For someone like me, which path makes more sense long term, DE or ML?\n2. How much cloud, system design, or MLOps is actually expected for entry level roles in each?\n3. If you were starting today from scratch, what would you focus on first?\n4. Any lessons or regrets from people who picked one over the other?\n\nI am not looking for hype or trends, just honest advice from people who are actually working in these roles.\n\nThanks in advance.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qdi7zm/confused_between_data_engineering_and_machine/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "nzq0fjy",
          "author": "redrosa1312",
          "text": ">\n\nYou haven't said anything about *why* you are stuck trying to choose between either path. What drew you to either concept in the first place? Why are you trying to learn? All we know is that you've done a few small projects and that you're confused, but knowing nothing about your motivations or interests, it's impossible to say.\n\n>2. How much cloud, system design, or MLOps is actually expected for entry level roles in each?\n\nVery little is expected in entry-level roles, but you will almost certainly not find entry-level roles in either field. Data engineering and ML Engineering (which is what I'm assuming you mean when you say Machine Learning, as being a Machine Learning Scientist is a different role altogether) are extensions of software engineering, and by their nature involve a lot of interdisciplinary skills. While it's the company-specific role that dictates how much cloud or MLOps is involved, you will need a solid foundation in software engineering principles, as well as (at a minimum) basic exposure to building data pipelines, managing deployments, and system design.\n\n>3. If you were starting today from scratch, what would you focus on first?\n\nThe fundamentals of software development, starting with proficiency in Python, SQL, an RDBMS like Postgres, git, and basic tooling (e.g., using uv to manage your dependencies for a project.) Focus on building projects using software best practices (modular design, loose coupling, tests, thoughtful relational design, etc).\n\nThe majority of this experience will come from building things. Start with small scripts, and as your projects grow, google around for how to keep your projects maintainable. Books like *A Philosophy of Software Design* and *The Mythical Man-Month* are great, but they won't replace hands-on time struggling organizing the code you write.\n\n>4. Any lessons or regrets from people who picked one over the other?\n\nEither side is hard. You'd be making a mistake by going into this thinking you'll be working in either role within a couple of years, especially with no coding or industry experience. Focus on the fundamentals of building software and becoming a good coder first, as that on its own will take you a while, but there aren't any shortcuts.",
          "score": 12,
          "created_utc": "2026-01-15 13:13:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzq2qqj",
              "author": "FyodorAgape",
              "text": "Hello, thanks for the in depth reply.\n\n>You haven't said anything about *why* you are stuck trying to choose between either path. What drew you to either concept in the first place? Why are you trying to learn? All we know is that you've done a few small projects and that you're confused, but knowing nothing about your motivations or interests,\n\nTo be honest, I am an undergraduate, and I first learned about machine learning through the hype surrounding it. If I am being honest, I was more interested in traditional machine learning predictions rather than just NLP and LLMs. However, over time, I realized how high the entry-level barrier is and how difficult it is to get into the field.\n\nRecently, I learned about data engineering, and based on my research, many of the concepts overlap, and it does not seem as oversaturated as machine learning, although I could be wrong.\n\nThis has made me confused.\n\nIt is embarrassing to say, but one of my main motivations is money, especially since I do not come from a strong economic background.\n\nEdit: so far I have done Andrew Ng course and Solving problems in Hackerrank to improve my programming skills",
              "score": 2,
              "created_utc": "2026-01-15 13:26:33",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzwbof5",
              "author": "FyodorAgape",
              "text": "Hey, u/redrosa1312  \n\nJust to make sure I understand, are you suggesting that I shouldnâ€™t be too picky or specialize yet, and instead let my specialization develop based on the job I get?\n\n  \np.s. also sorry for tagging you",
              "score": 1,
              "created_utc": "2026-01-16 10:32:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzxcryx",
                  "author": "redrosa1312",
                  "text": "I think it's fine to specialize. If you know you likely want to go into ML, it can only help you to take the appropriate math/CS/cogsci courses to start down that path. As long as you're working on the other CS and programming fundamentals I mentioned, I would say it's even preferred to start to specialize in school. \n\nBut don't get so focused on one path that you forego others that might just as or even more interesting to you. In my opinion, you'll never have as much freedom to explore your intellectual curiosity as you will while you're still in school, so make sure you're taking advantage of that. \n\n>let my specialization develop based on the job I get?\n\nIf you have no particular pull toward one field over another, then sure, letting your job dictate what you want to specialize in is totally fine. Many people go that route. The problem is that for DE and ML in particular, especially for entry-level jobs, having some additional experience via coursework and projects in those areas can give you a big leg up in landing those jobs. So if you're sure you want to pursue those, it'll help you to start adding a bit of specificity into your coursework and project work in anticipation.\n\nLike I mentioned above, DE and ML are extensions of software engineering, and it's easier to go from a specific field to a more general programming job than the other way around.",
                  "score": 1,
                  "created_utc": "2026-01-16 14:34:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o07aphi",
              "author": "FinalRide7181",
              "text": "Not OP, but i find your comment very helpful. I wanted to ask: should one focus on design patterns/lld (like parking lot) in python or is it better to study leetcode + ml + ml system design?",
              "score": 1,
              "created_utc": "2026-01-18 00:20:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzq63g7",
          "author": "redrosa1312",
          "text": "Thanks for the context! You shouldnâ€™t be embarrassed about being in it for the money; thatâ€™s a big driver for many people. Just keep in mind that being in it ONLY for the money might lead to burnout if it doesnâ€™t actually align with your interests.   Data engineering is probably a little more accessible than ML and very in-demand, and a lot of data engineering skills will def help if you ever decide to pivot to ML. So starting there isnâ€™t a bad idea.  Are you studying CS?",
          "score": 3,
          "created_utc": "2026-01-15 13:45:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzq7rn7",
              "author": "FyodorAgape",
              "text": "Actually, Iâ€™m currently an undergraduate pursuing Electronics and Communication, and Iâ€™ll graduate next year. However, our subjects have been very theoretical, mathematical, and outdated compared to industry standards, in my opinion. There also arenâ€™t many well-paying opportunities in my major in my country, and I would have to pursue a masterâ€™s degree.\n\nSo far, Iâ€™ve completed Andrew Ngâ€™s course and have been improving my coding skills through HackerRank, but I havenâ€™t really done any major end-to-end projects yet.\n\nIâ€™m confused about what to pursue next. I need to get a job and be employed by the time I graduate next year. Maybe after getting a job and saving some money, I could pursue a masterâ€™s degree later.\n\nIf thereâ€™s any advice that could help me, Iâ€™d really appreciate it.",
              "score": 2,
              "created_utc": "2026-01-15 13:54:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzqiw79",
          "author": "randomseedfarmer",
          "text": "I've worked in both fields. Yes the base skills overlap but what you actually do is very different. DE tasks focus more on the data infrastructure and management whereas ML is more focused on modeling building and inference. I find ML much more interesting and for me DE is boring and repetitive. But that's me.",
          "score": 3,
          "created_utc": "2026-01-15 14:51:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqj6ak",
              "author": "FyodorAgape",
              "text": "If it's possible, could you look into other comments and advise me something.\n\nI would really appreciate the help.",
              "score": 1,
              "created_utc": "2026-01-15 14:53:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzqoy7g",
                  "author": "randomseedfarmer",
                  "text": "I agree with everything redrosa1312 said. Also, at this stage in your career it's really too early to choose one over the other. Get internships in each area. That's a good way to see what they are really like and which one appeals to you the most.",
                  "score": 1,
                  "created_utc": "2026-01-15 15:20:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzr0fcq",
          "author": "DataPastor",
          "text": "A Data Engineer is a programmer. The best education for this job is a computer science degree, and their core skills are programming at least in Python, but potentially also in Java, Scala or Go (depending on the company); and very good database skills (SQL, DB configuration etc.), and also the knowledge of orchestration tools like Apache Airflow, Dagster etc.; cloud services (Google, AWS or Azure) and virtualization technologies so docker, kubernetes, kubeflow, cloudrun, vertex ai etc. etc.\n\nA Data Scientist is a statistical programmer. The best education for this job is any numerate undergrad plus some statistics-heavy postgrad, like statistics, data analytics, data science, biostatistics / bioinformatics, econometrics etc. etc. Data scientists train (and/or code) models, and also program a full solution, but they are definitely more on the modeling / statistical side of the story.\n\nI don't exactly know what MLEs are doing, because here in Europe this role is not very wide spread (Data Scientists are doing their job), but I think that MLE is a new name for Data Scientist, to distinguish themselves from those Data Scientists who couldn't really program, rather just develop models in jupyter notebook.\n\nBut the bottom line is -- if you are a programmer, you might want to focus on the data engineer, devops/mlops, backend engineer side. I wouldn't try to get into ML without considerable statistical education.\n\n\"AI Engineers\" is a new category, I think they are also rather programmers than data scientists.",
          "score": 1,
          "created_utc": "2026-01-15 16:13:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzr2tgs",
              "author": "FyodorAgape",
              "text": ">But the bottom line is -- if you are a programmer, you might want to focus on the data engineer, devops/mlops, backend engineer side. I wouldn't try to get into ML without considerable statistical education\n\nActually, Iâ€™m currently an undergraduate pursuing Electronics and Communication, and Iâ€™ll graduate next year. However, our subjects have been very theoretical, mathematical, and outdated compared to industry standards, in my opinion. There also arenâ€™t many well-paying opportunities in my major in my country, and I would have to pursue a masterâ€™s degree.\n\nSo far, Iâ€™ve completed Andrew Ngâ€™s course and have been improving my coding skills through HackerRank, but I havenâ€™t really done any major end-to-end projects yet.\n\nIâ€™m confused about what to actually pursue considering my options, since afaik both aren't entry level jobs and which is easier to get into.",
              "score": 1,
              "created_utc": "2026-01-15 16:23:48",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzr7xqc",
              "author": "randomseedfarmer",
              "text": "I guess the definitions differ depending on where you live. Here are examples of tasks I've done for these roles in the US:\n\nDE/DS: run SQL queries in Python and display plots of data patterns\n\nMLE: build XGBoost model in Python, train it on data, perform inference, write report about analysis with recommendations",
              "score": 1,
              "created_utc": "2026-01-15 16:46:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzs2d4a",
          "author": "salorozco23",
          "text": "Just read hands on machine learning book. Data engineering is part of machine learning. As you have to manipulate data to be able to train some models.\n\nI took a 8 month deep dive course on machine learning and AI. This book covers most of it.  Data engineering is part of it. \n\n[https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646)\n\nOnce you learn machine learning.  Gen AI is easy to learn.",
          "score": 1,
          "created_utc": "2026-01-15 19:02:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzscl5n",
          "author": "acana95",
          "text": "DE is about moving data from point A to B as efficiently as possible with minimal errors. MLE is about building models at point B.",
          "score": 1,
          "created_utc": "2026-01-15 19:49:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzsrfwk",
          "author": "another_summer",
          "text": "DE: move data from A to B. Build pipelines that do just that, and maintain them.\nDS/MLE: understand data, build models, maintain models.\nAI engineer: new breed, mostly centered around repackaging LLMs and deploying them.\nDS/MLE/AIE tend to overlap. \nAs a DS, I do a bit of DE too, but the reverse is not expected.",
          "score": 1,
          "created_utc": "2026-01-15 20:58:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdihqv",
      "title": "LLMs: Just a Next Token Predictor",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qdihqv/llms_just_a_next_token_predictor/",
      "author": "Gradient_descent1",
      "created_utc": "2026-01-15 12:42:40",
      "score": 19,
      "num_comments": 17,
      "upvote_ratio": 0.66,
      "text": "https://reddit.com/link/1qdihqv/video/x4745amkbidg1/player\n\n  \n**Process behind LLMs:**\n\n1. **Tokenization:** Your text is split into sub-word units (tokens) using a learned vocabulary. Each token becomes an integer ID the model can process. See it here: [https://tiktokenizer.vercel.app/](https://tiktokenizer.vercel.app/)\n2. **Embedding:** Each token ID is mapped to a dense vector representing semantic meaning. Similar meanings produce vectors close in mathematical space.\n3. **Positional Encoding:** Position information is added so word order is known. This allows the model to distinguish â€œdog bites manâ€ from â€œman bites dogâ€.\n4. **Transformer Encoding (Self-Attention):** Every token attends to every other token to understand context. Relationships like subject, object, tense, and intent are computed.\\[See the process here: [https://www.youtube.com/watch?v=wjZofJX0v4M&t=183s](https://www.youtube.com/watch?v=wjZofJX0v4M&t=183s) \\]\n5. **Deep Layer Processing:** The network passes information through many layers to refine understanding. Meaning becomes more abstract and context-aware at each layer.\n6. **Logit Generation:** The model computes scores for all possible next tokens. These scores represent likelihood before normalization.\n7. **Probability Normalization (Softmax):** Scores are converted into probabilities between 0 and 1. Higher probability means the token is more likely to be chosen.\n8. **Decoding / Sampling:** A strategy (greedy, top-k, top-p, temperature) selects one token. This balances coherence and creativity.\n9. **Autoregressive Feedback:** The chosen token is appended to the input sequence. The process repeats to generate the next token.\n10. **Detokenization:** Token IDs are converted back into readable text. Sub-words are merged to form the final response.\n\nThat is the full internal generation loop behind an LLM response.",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qdihqv/llms_just_a_next_token_predictor/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "nzt7m7b",
          "author": "IDefendWaffles",
          "text": "This is not the whole story.  Initial layers in transformers essentially attend across words, but subsequent layers attend across latent vectors that represent ideas.  While the output is the next token, this token is essentially obtained from decoding a latent vector which represents a \"thought\".  It is this thought that is decoded one token at a time.  Much like humans  who hold a thought in their head and then as they communicate it they say one word at a time.",
          "score": 17,
          "created_utc": "2026-01-15 22:13:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqnogy",
          "author": "modcowboy",
          "text": "Anyone without schizophrenia already knows itâ€™s just a next token generator.",
          "score": 24,
          "created_utc": "2026-01-15 15:14:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzte8h7",
              "author": "Busy-Vet1697",
              "text": "When you see the word -just- you know rationalization and \"in group\" signalling is hard at work. **ã…‹ã…‹ã…‹**",
              "score": 8,
              "created_utc": "2026-01-15 22:45:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzvreoc",
              "author": "MartinMystikJonas",
              "text": "Any intelligent system (including humans) can be seen as \"just next action predictor\" when you look only at outputs and ignore everything else.",
              "score": 2,
              "created_utc": "2026-01-16 07:27:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzwqe5o",
                  "author": "modcowboy",
                  "text": "Yes, but you see, we harness quantum physics.",
                  "score": -1,
                  "created_utc": "2026-01-16 12:28:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0c5k2k",
              "author": "yannbouteiller",
              "text": "\"Generator\" and \"predictor\" are not the same thing at all.\n\nLLMs are clearly not predictors anymore, they are generators.\n\nAs for \"just\" and \"schizophrenia\", not sure what this means in this context. Perhaps you are saying that anyone who is not religious and understands deep learning already believes that they are themselves some kind of next-token generator?",
              "score": 1,
              "created_utc": "2026-01-18 18:53:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzsmidk",
              "author": "Possible_Let1964",
              "text": "There is a hypothesis that this is partly how our brain works, for example, when you came up with the string of words in your sentence.",
              "score": 2,
              "created_utc": "2026-01-15 20:35:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzteigz",
          "author": "Busy-Vet1697",
          "text": "\"Just\" posters constantly trying to remind their bosses, and themselves that they're special princesses",
          "score": 11,
          "created_utc": "2026-01-15 22:47:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvlfyf",
              "author": "digiorno",
              "text": "Just a next token predictor which can help me develop a highly functional script, containing thousands of lines,  in a single afternoon instead of over the course of several months. The productivity benefits to coding are massive and I donâ€™t even use agentic services which would likely be better.",
              "score": 2,
              "created_utc": "2026-01-16 06:37:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzvq1al",
                  "author": "Thick-Protection-458",
                  "text": "Funny to see that solving complicated (not rocket science, but just somehow complicated) task may be just a matter of autocomplete, or at the worst case (althrough I am not aware of such systems) - autocomplete-driven monte-carlo search (which is arguably how we work. Throwing somewhere-plausible-looking hypothesis at the wall unless something sticks and can't be rejected at practice, while being rejectable in theory).\n\nIsn't it?",
                  "score": 3,
                  "created_utc": "2026-01-16 07:15:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzvom1y",
              "author": "Thick-Protection-458",
              "text": "Nah, that's technically correct. Best kind of correct.\n\nJust suddenly we arrived at the point where autocomplete is good enough to do many tasks, should that tasks be describable in the language autocomplete was trained on. Which, if you think about it - totally makes sense, especially with certain neural language models qualities making them capable of remembering more generic patterns than just \"strictly word 1 - strictly word 2 - ... - strictly word N\" N-grams. Because once task and decision can be described in language - it is only matter of probability to solve it via autocomplete (and so - boosting that probability).",
              "score": 1,
              "created_utc": "2026-01-16 07:03:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o02y5js",
          "author": "mave_ad",
          "text": "My opinions: yes LLM predict next tokens. However, to predict next tokens you need to learn the latent representation and build a probabilistic internal model of the information it's been exposed to. \n\nFoundational models are very general systems. They try to generalise very heavily since they are trying to match a probabilistic state of getting the least loss on cross entropy loss or so. Human intelligence is a lot like how next token prediction work. Not on fundamental working but analogically as a llm converts language into their internal representation and produce output just like humans convert language into internal representation to understand words and meanings and then respond.",
          "score": 1,
          "created_utc": "2026-01-17 09:56:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztexiy",
          "author": "IKerimI",
          "text": "There are also diffusion text generation models (though not the norm for foundation models)",
          "score": 0,
          "created_utc": "2026-01-15 22:49:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztqkjz",
          "author": "unlikely_ending",
          "text": "Great, accurate summary",
          "score": -1,
          "created_utc": "2026-01-15 23:51:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvxcjm",
              "author": "MatteyRitch",
              "text": "It is not complete and just an odd post in general.",
              "score": 3,
              "created_utc": "2026-01-16 08:20:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}