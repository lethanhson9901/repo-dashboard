{
  "metadata": {
    "last_updated": "2026-01-21 17:20:00",
    "time_filter": "week",
    "subreddit": "learnmachinelearning",
    "total_items": 20,
    "total_comments": 134,
    "file_size_bytes": 162915
  },
  "items": [
    {
      "id": "1qg6q1w",
      "title": "I implemented a VAE in Pure C for Minecraft Items",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/gallery/1qg6q1w",
      "author": "Boliye",
      "created_utc": "2026-01-18 11:53:54",
      "score": 282,
      "num_comments": 34,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qg6q1w/i_implemented_a_vae_in_pure_c_for_minecraft_items/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0a2iu3",
          "author": "Cybyss",
          "text": "Damn. That is really cool! \n\nYou basically reinvented your own PyTorch from scratch in plain C and used that to create your own variational autoencoder? Ambitious. I also love the creativity of training on Minecraft images instead of the usual MNIST or CIFAR. \n\nWell done!",
          "score": 34,
          "created_utc": "2026-01-18 12:17:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a36ye",
              "author": "Boliye",
              "text": "Thank you!",
              "score": 6,
              "created_utc": "2026-01-18 12:22:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0a1xnh",
          "author": "Palmquistador",
          "text": "I don‚Äôt think I understand. You created your own image generator specific to Minecraft images?",
          "score": 25,
          "created_utc": "2026-01-18 12:12:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a35l0",
              "author": "Boliye",
              "text": "Yeah, sort of! A VAE is a type of network that can be used for image generation. And I created and trained one of these with Minecraft images. But as a VAE is also an autoencoder, something you can also do is play with the embeddings and ask the network stuff like \"what's at the middle point between this and that?\" \"What would happen if you took this and subtracted that?\". If the network was successful in learning meaningful concepts, these averages won't be nonsense, and stuff like what I show in the images will happen.",
              "score": 32,
              "created_utc": "2026-01-18 12:22:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0acap7",
                  "author": "LumpyWelds",
                  "text": "Is this like word2vec?\n\n\"dog\" - \"puppy\" + \"kitten\" = \"cat\"",
                  "score": 20,
                  "created_utc": "2026-01-18 13:28:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0f0ys1",
              "author": "irekit_",
              "text": "A variational auto-encoder is something that encodes data like images into the latent space, it uses probabilities and outputs a gaussian distribution of where the data is likely to be in the latent space.",
              "score": 3,
              "created_utc": "2026-01-19 03:50:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0f29ye",
                  "author": "Palmquistador",
                  "text": "So like RAG? But the VAE is just one step in the image generation process, right? That‚Äôs what ComfyUI implies with their connectors at least.",
                  "score": 1,
                  "created_utc": "2026-01-19 03:59:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0a6v1n",
          "author": "gocurl",
          "text": "Doing the \"- concept 1 + concept 2\" and having a relevant result is a very cool way to confirm your model understood key concept. Very well done!\nOut of curiosit√©, in that \"- x + x\" step, what is the input you provides? Do you start from the middle layer?",
          "score": 11,
          "created_utc": "2026-01-18 12:50:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a95uo",
              "author": "Boliye",
              "text": "Yeah I use the encoder and decoder parts separately:  \n\nEncode iron\\_chestplate -> Latent for iron\\_chestplate  \n\nEncode all items that contain 'iron' in their name and average them out -> Latent for iron concept  \n\nEncode all items that contain 'diamond' in their name and average them out -> Latent for diamond concept  \n\nLiterally do the operation \"Latent for iron\\_chestplate\" - \"Latent for iron concept\" + \"Latent for diamond concept\"  \n\nFinally, pass this result to the decoder (second half of the network).",
              "score": 12,
              "created_utc": "2026-01-18 13:07:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0arh78",
                  "author": "gocurl",
                  "text": "It seems so clear now you‚Äôve said it, but I've never thought about it this way, thanks!",
                  "score": 4,
                  "created_utc": "2026-01-18 14:55:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0a81f9",
          "author": "JanBitesTheDust",
          "text": "Very cool idea! How did you find the latent vector for the concept of iron? Is it just averaging latent vectors for all iron related minecraft textures?",
          "score": 9,
          "created_utc": "2026-01-18 12:59:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a8q6v",
              "author": "Boliye",
              "text": "Yes! it is just the average",
              "score": 6,
              "created_utc": "2026-01-18 13:04:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0dtqkf",
                  "author": "possiblyquestionabl3",
                  "text": "This is a pretty cool trick!\n\nA couple of assumptions:\n\n1. your set of iron related items form some concept vector space like `v_{iron_sword} \\approx v_{iron} + v_{sword}`\n2. it's sufficiently large in dimension, so that different concepts tend to be naturally orthogonal (e.g. v\\_{sword} and v\\_{bar} are nearly orthogonal to each other)\n\nlet `z_{iron}` be the mean vector over all iron related vectors, then\n\n    z_{iron} = 1/N \\sum_{type} v_{iron type} = v_{iron} + 1/N \\sum_{type} v_{type}\n\nif N, the number of distinct iron X items, is large enough, and if we assume v_{type} are generally orthogonal to each other, then `\\sum_{type} v_{type}` can be seen as an isotropic ball of noise with a spherical radius of `||v_{type}|| * sqrt(N)`. As a result, you're essentially computing\n\n    z_{iron} = v_{iron} + noise ||v_{type}||/sqrt(N)\n\nnormalizing v, you're basically computing the actual concept vector for `v_{iron}`, plus noise of O(n^(-1/2)). E.g. if you can provide 100 `iron X` samples, your noise goes down to ~10%.\n\nIf structurally, iron and diamond items have the exact same types (e.g. swords, bars, etc), then the noise term will be structurally similar (same general direction), so `- z_{iron} + z_{diamond}` should be able to cancel the noise out almost perfectly. However, adding/subtracting concepts together without cancellation will amplify the noise, and categories with low # of types to average will have proportionally sqrt(N) times more noise.\n\nIt might be worthwhile to do a few power-iterations on the 2nd-moment operator `M = 1/N \\sum_{type} v_{iron type} v_{iron type}^T` using the mean `z_{iron}` direction to get to the real `v_{iron}` direction (probably converges in just 1-2 steps since the mean `z_{iron}` is already dominated in the `v_{iron}` direction). It's fairly cheap and would allow you to avoid the sqrt(N) noise term.",
                  "score": 3,
                  "created_utc": "2026-01-18 23:53:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0aus8u",
          "author": "ToSAhri",
          "text": "To what extent did this latent arithmetic operation rely on the convenience of the iron and diamond items being very similar (if not identical save for the color)?\n\nI guess I‚Äôm confused on how the latent arithmetic has inherent use of that method. If we only had a random sample of half of the iron items and half of the diamond items would it still work well? Cause then we could use it to generate diamond versions of iron pieces we don‚Äôt have and vice versa.\n\nIt‚Äôs possible I just don‚Äôt grasp the use case of VAEs in general and that‚Äôs where my confusion comes from.",
          "score": 5,
          "created_utc": "2026-01-18 15:12:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0b06wb",
              "author": "Boliye",
              "text": "You are right this is just a toy project, and the model is not that powerful to do anything really useful. If I ask it to generate new items, the quality is meh and I wouldn't be surprised if we did that experiment and asked it to generate something that did not exist in its training data, the VAE would struggle to generate something out of distribution like that. I am definetly making the task easier by trying to generate something that I know was present in the training data.\n\nTake the second example (the one where we turn a gold horse armor to a golden shovel). The fact that the generated shovel is yellow, shows that somewhere in the latent space, color of the object is one of the compressed features it learned to be useful for reconstructing.\n\nUltimately, the essence of the latents is that they are just 32 numbers trying to compress the information of a 3\\*16\\*16 = 768 pixel image. So the VAE has to find the most useful high-level features that characterize each image.",
              "score": 3,
              "created_utc": "2026-01-18 15:39:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ahb3j",
          "author": "Anas0101",
          "text": "so cool",
          "score": 2,
          "created_utc": "2026-01-18 13:59:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0al3sb",
              "author": "Boliye",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-18 14:20:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0bkd64",
          "author": "Poleski69",
          "text": "Thats really cool! \n\nIt's a variational autoencoder, so what happens if you sample the latent space with a normal distribution? I know var autoencoders aren't the best at generation but I'm really curious to see what your implementation thinks the 'average' minecraft item is!",
          "score": 2,
          "created_utc": "2026-01-18 17:15:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0cl97x",
              "author": "Boliye",
              "text": "Here are you go:  \n[https://imgur.com/a/TNYgQ32](https://imgur.com/a/TNYgQ32)\n\nFor convinience, for these extra generations I didn't use the C VAE, but the proof of concept in Python that I also created (it can be found in the folder poc\\_python in the git repo). It implements the exact same architecture and achieves the same loss.\n\nIn addition to \"the average item\", and some generations, I also added interpolations between a few items like a diamond chestplate turning into a diamond sword.",
              "score": 1,
              "created_utc": "2026-01-18 20:09:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0kqbj5",
                  "author": "Poleski69",
                  "text": "Super cool!\n\nAm I understanding correctly that the top image is the output of a latent tensor filled with zeros, and the ones under are from normally distributed \"noise\" latents (torch.randn ..)?\n\nInterpolations are really cool too, makes me wonder how hard it would be to implement latent ddpm in pure c and fully make use of the autoencoder.",
                  "score": 1,
                  "created_utc": "2026-01-20 00:13:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0dszkw",
          "author": "ami98",
          "text": "Super cool! Your code is very easy to understand, and it's neat that your python proof of concept matches your C results!\n\nBy any chance, do you know of a good textbook that discusses the mathematics behind these algorithms? Thanks:)",
          "score": 2,
          "created_utc": "2026-01-18 23:49:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hun4t",
              "author": "Boliye",
              "text": "Thank you! Yeah at first I was sure there would be some kind of difference I wouldn't be able to find between the trained PyTorch model and my C implementation, I was pretty happy when the numbers matched up :)\n\nI honestly don't feel informed enough to recommend good textbooks. I personally enjoyed a lot these lectures on youtube about \"Deep Generative Models\" from Stanford [https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8](https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8) . It does cover the mathematics behind it to a degree I find satisfying. But yeah, if you prefer textbooks, I can't really advise any better than other students.",
              "score": 1,
              "created_utc": "2026-01-19 16:01:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0e7fqu",
          "author": "Fickle_Lettuce_2547",
          "text": "How long have you used C to be able to make something like this??",
          "score": 2,
          "created_utc": "2026-01-19 01:06:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hzdp8",
              "author": "Boliye",
              "text": "C is a surprisingly simple language really! It has very few keywords and features. What is tricky and it will take some studying if you have never seen it before is memory management. Memory management can become a mess if you aren't thoughtful about it.\n\nLuckily, for this project the memory management is quite simple. So the code is not that different from programing in any other language.\n\nAlso, you need to be careful about bugs and test things as you go. For many mistakes you could make, the code could keep running in invalid states and do who-knows-what. Ultimately crashing without giving any insight on where's the bug.",
              "score": 2,
              "created_utc": "2026-01-19 16:22:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0h9kcq",
          "author": "arsenic-ofc",
          "text": "cool, i love VAEs, helped me do similar stuff with dance videos.",
          "score": 2,
          "created_utc": "2026-01-19 14:19:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mcgex",
          "author": "MeticulousBioluminid",
          "text": "sick",
          "score": 1,
          "created_utc": "2026-01-20 05:52:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qnzjw",
          "author": "Lupoferrin",
          "text": "This is a fascinating project! Building a VAE from scratch in C is a deep dive into the fundamentals. It highlights how understanding core algorithms can lead to innovative applications, even in areas like game asset generation.",
          "score": 1,
          "created_utc": "2026-01-20 21:24:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0r01np",
          "author": "Scary-Opportunity848",
          "text": "Really cool stuff. For the latent space for iron you may be able to run svd over the matrix of all iron vectors. And instead of taking the average, take the top eigen values/vectors and whatever vector that creates. It should hold more iron since it is meant to capture the most common/consistent aspects across all of that batch. Really cool work btw",
          "score": 1,
          "created_utc": "2026-01-20 22:21:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qf6l1j",
      "title": "I‚Äôm working on an animated series to visualize the math behind Machine Learning (Manim)",
      "subreddit": "learnmachinelearning",
      "url": "https://v.redd.it/cnezsdqyzudg1",
      "author": "No_Skill_8393",
      "created_utc": "2026-01-17 07:08:36",
      "score": 254,
      "num_comments": 21,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qf6l1j/im_working_on_an_animated_series_to_visualize_the/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o02kh04",
          "author": "314159267",
          "text": "Very cool. Like the visual. Nicely done, looking forward for more.",
          "score": 10,
          "created_utc": "2026-01-17 07:48:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02mp88",
              "author": "No_Skill_8393",
              "text": "Thank you. The math animations and video editting is quite new to me and I'm learning and as I do. Hope you guys have patience with me as I improve.",
              "score": 4,
              "created_utc": "2026-01-17 08:08:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o02rool",
          "author": "Dramatic_Yam8355",
          "text": "I hope you don‚Äôt stop uploading videos‚Äîplease keep going.",
          "score": 6,
          "created_utc": "2026-01-17 08:55:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02y50b",
              "author": "No_Skill_8393",
              "text": "I'm already working on Episode 1 and It's receiving more effort and animation budget than Episode 0.\n\nStay in touch :D",
              "score": 3,
              "created_utc": "2026-01-17 09:56:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o02o1tm",
          "author": "tandir_boy",
          "text": "Cool idea. Also chech out Karpathy's [visualization](https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)",
          "score": 3,
          "created_utc": "2026-01-17 08:21:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02ochr",
              "author": "No_Skill_8393",
              "text": "Thank you. I'm also learning from Karpathy [https://karpathy.ai/zero-to-hero.html](https://karpathy.ai/zero-to-hero.html) \n\nGreat course. I learned alot from building micrograd. Excellent stuff to learn ML from scratch",
              "score": 3,
              "created_utc": "2026-01-17 08:24:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o031t4c",
          "author": "Image_Similar",
          "text": "manim is a boon to all of science",
          "score": 2,
          "created_utc": "2026-01-17 10:31:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03392n",
              "author": "No_Skill_8393",
              "text": "It is. God bless 3B1B and his splendid works. Really introduced me into Manim.",
              "score": 2,
              "created_utc": "2026-01-17 10:44:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o03p7z8",
          "author": "kindr_7000",
          "text": "Congrats, keep going.",
          "score": 2,
          "created_utc": "2026-01-17 13:36:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o033lla",
          "author": "Obvious-Shine-3573",
          "text": "hey, this is really interesting! I've tried some stuff with manim myself, could you share where you're learning from?",
          "score": 1,
          "created_utc": "2026-01-17 10:47:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0342a0",
              "author": "No_Skill_8393",
              "text": "Hi, I use https://docs.manim.community/en/stable/examples.html\n\nr/manim and alot of slamming my head against chatgpt lol",
              "score": 2,
              "created_utc": "2026-01-17 10:51:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o04hjqb",
          "author": "NightmareLogic420",
          "text": "Content seems great! The audio voiceover has something weird and off about it though.",
          "score": 1,
          "created_utc": "2026-01-17 16:04:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o04hqzk",
              "author": "No_Skill_8393",
              "text": "Im considering moving to elevenlabs for better audio :)\n\nSorry for using AI voice im really not confident with my voice.",
              "score": 2,
              "created_utc": "2026-01-17 16:05:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o04irxw",
                  "author": "NightmareLogic420",
                  "text": "Understandable choice, but just one man's opinion, human voice goes a long way to keep people locked in for longer, even if you aren't the best orator in the world, AI voice fatigues the ear faster. Especially if you're just worried about accent, accent is no big deal imo.",
                  "score": 2,
                  "created_utc": "2026-01-17 16:09:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o05k4q8",
          "author": "herooffjustice",
          "text": "Nice, keep going.  \nI'm doing something similar btw: [Link](https://www.reddit.com/r/learnmachinelearning/comments/1q3y6m5/ml_intuition_004_multilinear_regression/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",
          "score": 1,
          "created_utc": "2026-01-17 19:03:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05skzk",
              "author": "No_Skill_8393",
              "text": "Awesome. More Manim enthusiast!",
              "score": 2,
              "created_utc": "2026-01-17 19:44:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o05x8p7",
          "author": "Sea-Lettuce-9635",
          "text": "üî•üî•",
          "score": 1,
          "created_utc": "2026-01-17 20:07:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06xzwq",
          "author": "Naive-Extension7953",
          "text": "is this your voice?",
          "score": 1,
          "created_utc": "2026-01-17 23:13:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o08hbnr",
          "author": "2hands10fingers",
          "text": "instant subscribe",
          "score": 1,
          "created_utc": "2026-01-18 04:14:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qiguai",
      "title": "SVM from scratch in JS",
      "subreddit": "learnmachinelearning",
      "url": "https://v.redd.it/8s2nuomc6leg1",
      "author": "Ok-Statement-3244",
      "created_utc": "2026-01-20 23:10:56",
      "score": 179,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qiguai/svm_from_scratch_in_js/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0rcgp4",
          "author": "0uchmyballs",
          "text": "Looking forward to more ML in JS",
          "score": 6,
          "created_utc": "2026-01-20 23:25:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0slpwd",
          "author": "rshah4",
          "text": "Karpathy did a version of this as well: [https://cs.stanford.edu/\\~karpathy/svmjs/demo/](https://cs.stanford.edu/~karpathy/svmjs/demo/)",
          "score": 3,
          "created_utc": "2026-01-21 03:42:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rp1nw",
          "author": "sodapopenski",
          "text": "Now THIS is machine learning.",
          "score": 2,
          "created_utc": "2026-01-21 00:34:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qib764",
      "title": "[Cheat Sheet] I summarized the 10 most common ML Algorithms for my interview prep. Thought I'd share.",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qib764/cheat_sheet_i_summarized_the_10_most_common_ml/",
      "author": "IT_Certguru",
      "created_utc": "2026-01-20 19:41:10",
      "score": 134,
      "num_comments": 18,
      "upvote_ratio": 0.88,
      "text": "Hi everyone,\n\nI‚Äôve been reviewing the basics for upcoming interviews, and I realized I often get stuck trying to explain simple concepts without using jargon.\n\nI wrote down a summary for the top 10 algorithms to help me memorize them. I figured this might help others here who are just starting out or refreshing their memory.\n\nHere is the list:\n\n# 1. Linear Regression\n\n* **The Gist:** Drawing the straightest possible line through a scatter plot of data points to predict a value (like predicting house prices based on size).\n* **Key Concept:** Minimizing the \"error\" (distance) between the line and the actual data points.\n\n# 2. Logistic Regression\n\n* **The Gist:** Despite the name, it's for **classification**, not regression. It fits an \"S\" shaped curve (Sigmoid) to the data to separate it into two groups (e.g., \"Spam\" vs. \"Not Spam\").\n* **Key Concept:** It outputs a probability between 0 and 1.\n\n# 3. K-Nearest Neighbors (KNN)\n\n* **The Gist:** The \"peer pressure\" algorithm. If you want to know what a new data point is, you look at its 'K' nearest neighbors. If most of them are Blue, the new point is probably Blue.\n* **Key Concept:** It doesn't actually \"learn\" a model; it just memorizes the data (Lazy Learner).\n\n# 4. Support Vector Machine (SVM)\n\n* **The Gist:** Imagine two groups of data on the floor. SVM tries to put a wide street (hyperplane) between them. The goal is to make the street as wide as possible without touching any data points.\n* **Key Concept:** The \"Kernel Trick\" allows it to separate data that isn't easily separable by a straight line by projecting it into higher dimensions.\n\n# 5. Decision Trees\n\n* **The Gist:** A flowchart of questions. \"Is it raining?\" -> Yes -> \"Is it windy?\" -> No -> \"Play Tennis.\" It splits data into smaller and smaller chunks based on simple rules.\n* **Key Concept:** Easy to interpret, but prone to \"overfitting\" (memorizing the data too perfectly).\n\n# 6. Random Forest\n\n* **The Gist:** A democracy of Decision Trees. You build 100 different trees and let them vote on the answer. The majority wins.\n* **Key Concept:** Reduces the risk of errors that a single tree might make (Ensemble Learning).\n\n# 7. K-Means Clustering\n\n* **The Gist:** You have a messy pile of unlabelled data. You want to organize it into 'K' number of piles. The algorithm randomly picks centers for the piles and keeps moving them until the groups make sense.\n* **Key Concept:** Unsupervised learning (we don't know the answers beforehand).\n\n# 8. Naive Bayes\n\n* **The Gist:** A probabilistic classifier based on Bayes' Theorem. It assumes that all features are independent (which is \"naive\" because in real life, things are usually related).\n* **Key Concept:** Surprisingly good for text classification (like filtering emails).\n\n# 9. Principal Component Analysis (PCA)\n\n* **The Gist:** Data compression. You have a dataset with 50 columns (features), but you only want the 2 or 3 that matter most. PCA combines variables to reduce complexity while keeping the important information.\n* **Key Concept:** Dimensionality Reduction.\n\n# 10. Gradient Boosting (XGBoost/LightGBM)\n\n* **The Gist:** Similar to Random Forest, but instead of building trees at the same time, it builds them one by one. Each new tree tries to fix the mistakes of the previous tree.\n* **Key Concept:** Often the winner of Kaggle competitions for tabular data.\n\nIf you want to connect these concepts to real production workflows, one helpful resource is a hands-on course on Machine Learning on Google Cloud. It shows how algorithms like Linear/Logistic Regression, PCA, Random Forests, and Gradient Boosting: [Machine Learning on Google Cloud](https://www.netcomlearning.com/course/machine-learning-on-google-cloud)\n\nLet me know if I missed any major ones or if you have a better analogy for them!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qib764/cheat_sheet_i_summarized_the_10_most_common_ml/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o0qieow",
          "author": "Disastrous_Room_927",
          "text": ">Despite the name, it's for **classification**, not regression. It fits an \"S\" shaped curve (Sigmoid) to the data to separate it into two groups (e.g., \"Spam\" vs. \"Not Spam\").\n\nThis isn't correct: regression is descriptive of the model being fit, classification is one use case for it. It isn't fitting an s-shaped curve directly, the fit is actually linear with respect to the log-odds, not the probability itself. The s-curve arises when you convert the output of the model back to probabilities.\n\nUnderstanding that the output people normally work with \"lives\" in a different space than the model is important if you're interpreting coefficients, or if you want to use a linear model with other types of responses. You can use Poisson/Negative Binomial regression for count data, beta regression for continuous proportions, gamma regression for data that is positive continuous and skewed, etc. All of these (and logistic regression) are types of Generalized Linear Models that differ by the function used to go from the original scale of the data to the scale the model is being fit on. Interestingly enough, this is an avenue for modeling probabilities in alternative ways - you get different shaped s-curves if you use probit, Chauchit, Cloglog functions instead of logit, for example. Not super common but worth knowing about.\n\n>**The Gist:** Data compression. You have a dataset with 50 columns (features), but you only want the 2 or 3 that matter most. PCA combines variables to reduce complexity while keeping the important information.\n\nAlso worth noting that this is just one thing PCA is particularly useful for. What you're doing is making new variables that \"explain\" the variance of the data independently of one another (they're uncorrelated/orthogonal). The first one captures the most variability, the second one captures the most variance in the absence of the first, and so on and so forth. You end up with 50 new columns containing the same information as the original data, but past a certain point they're just capturing the leftovers.\n\nPCA doesn't necessarily reduce complexity (in terms of information) directly, it allows you to cut out as much of it as you can without sacrificing fidelity - past the first few components, the only thing being captured is smaller and smaller chunks of noise. Sort of like discarding pixels in a region of an image that appears purely black to the naked eye and probably only differs due to camera sensor noise - you could use interpolation to reconstruct most of them and nobody would be the wiser. Note: a fun idea to play with here is to actually use PCA for compression. You can start with a dataset that has a column for each color channel and a row for each pixel, and then see what happens if you start pooling information for nearby pixels. \n\nComplexity is also reduced in a practical sensed because correlated variables end up getting decomposed into variables that contain shared and unique information. If they're highly correlated, that unique information isn't contributing much and may end up getting discarded - you're effectively collapsing dimensions.",
          "score": 32,
          "created_utc": "2026-01-20 20:58:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0rjdcy",
              "author": "Disastrous_Room_927",
              "text": "Follow up comment: I got side tracked by this comment and made an image compression script with PCA. The image on the left is 55% of the size of the one on the right: [https://imgur.com/a/mrjir5o](https://imgur.com/a/mrjir5o)",
              "score": 4,
              "created_utc": "2026-01-21 00:03:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0s3oqp",
              "author": "FancyEveryDay",
              "text": "PCA is useful for all sorts of things, lately I've been using it to do Total Least Squares regression, but I've also used it for Factor Analysis where you apply varimax or other rotations to group correlated variables together, and PCR which is ordinary linear regression performed using the Principal Components as variables.",
              "score": 1,
              "created_utc": "2026-01-21 01:57:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0sh4t7",
                  "author": "Disastrous_Room_927",
                  "text": "It's also useful for connecting the dots between concepts. What do you get when you you add a non-linear transformation in to the outputs of PCA in PCR? The exact functional form of a neural network with one hidden layer. It doesn't work like a neural net when you train it, but if you set yourself to the task of gluing together statistical models to make them work like one you end up learning a lot. like why backprop isn't magic but is super useful.",
                  "score": 2,
                  "created_utc": "2026-01-21 03:14:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0qgwm1",
          "author": "Suspicious-Beyond547",
          "text": "You write just like my best friend Chad! What are the odds?!",
          "score": 41,
          "created_utc": "2026-01-20 20:51:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0re5kp",
              "author": "Nerdly_McNerd-a-Lot",
              "text": "Wait. Is this Chad??",
              "score": 6,
              "created_utc": "2026-01-20 23:35:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0uey6w",
                  "author": "sunil2000_babu",
                  "text": "Chat, who is Chad?",
                  "score": 1,
                  "created_utc": "2026-01-21 12:35:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0r9w24",
          "author": "EvilWrks",
          "text": "CNN and RNN would be good one to keep in mind.",
          "score": 3,
          "created_utc": "2026-01-20 23:12:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tujbu",
          "author": "NotMyRealName778",
          "text": "Ngl if I asked someone to explain linear regression and they answered with what you wrote as the gist I wouldn't hire them. Same goes for the other explanations.\n\nI think you need to read some textbooks. You understand something but this is way too surface level to be meaningful.",
          "score": 3,
          "created_utc": "2026-01-21 09:47:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tgyp5",
          "author": "swastik_K",
          "text": "Wait, you guys are still using SVM?",
          "score": 2,
          "created_utc": "2026-01-21 07:38:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0sht6r",
          "author": "MathProfGeneva",
          "text": "Ugh sorry, but logistic regression is ABSOLUTELY a regression. It's one part of the general linear models family with a logit link function. In ML it is generally turned into a classifier by applying a threshold, but it's used a lot without that.",
          "score": 1,
          "created_utc": "2026-01-21 03:18:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0svk5z",
          "author": "aibyzee",
          "text": "Thankyou for sharing this! A valueable info.",
          "score": 1,
          "created_utc": "2026-01-21 04:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0uxmk0",
          "author": "heggiepau",
          "text": "Linear regression does not mean a straight line through the data, it requires that the model is linear in the parameters. y = x1 + x2^2 is a curve but linear in the parameters. On the other hand y = x1 + 2^x2 is not linear in the parameters",
          "score": 1,
          "created_utc": "2026-01-21 14:23:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0v0hn2",
          "author": "TMHDD_TMBHK",
          "text": "Thumbs up my Chad, reliable as always. Prompto!",
          "score": 1,
          "created_utc": "2026-01-21 14:37:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0vbpng",
          "author": "KaleAnxious2863",
          "text": "Thanks for sharing",
          "score": 1,
          "created_utc": "2026-01-21 15:31:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0r0olb",
          "author": "Charming_Elk_9058",
          "text": "Great summary!",
          "score": 1,
          "created_utc": "2026-01-20 22:24:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0sdu5a",
          "author": "CorpusculantCortex",
          "text": "Particle swarm optimization",
          "score": 1,
          "created_utc": "2026-01-21 02:55:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qgvit5",
      "title": "(End to End) 20 Machine Learning Project in Apache Spark",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qgvit5/end_to_end_20_machine_learning_project_in_apache/",
      "author": "bigdataengineer4life",
      "created_utc": "2026-01-19 05:27:56",
      "score": 117,
      "num_comments": 7,
      "upvote_ratio": 0.98,
      "text": "Hi Guys,\n\nI hope you are well.\n\nFree tutorial on Machine Learning Projects (End to End) in **Apache Spark and Scala with Code and Explanation**\n\n1. [Life Expectancy Prediction using Machine Learning](https://projectsbasedlearning.com/apache-spark-machine-learning/life-expectancy-prediction-using-machine-learning/)\n2. [Predicting Possible Loan Default Using Machine Learning](https://projectsbasedlearning.com/apache-spark-machine-learning/predicting-possible-loan-default-using-machine-learning/)\n3. [Machine Learning Project - Loan Approval Prediction](https://projectsbasedlearning.com/apache-spark-machine-learning/machine-learning-project-loan-approval-prediction/)\n4. [Customer Segmentation using Machine Learning in Apache Spark](https://projectsbasedlearning.com/apache-spark-machine-learning/customer-segmentation-using-machine-learning-in-apache-spark/)\n5. [Machine Learning Project - Build Movies Recommendation Engine using Apache Spark](https://projectsbasedlearning.com/apache-spark-machine-learning/machine-learning-project-creating-movies-recommendation-engine-using-apache-spark/)\n6. [Machine Learning Project on Sales Prediction or Sale Forecast](https://projectsbasedlearning.com/apache-spark-machine-learning/machine-learning-project-on-sales-prediction-or-sale-forecast/)\n7. [Machine Learning Project on Mushroom Classification whether it's edible or poisonous](https://projectsbasedlearning.com/apache-spark-machine-learning/machine-learning-project-on-mushroom-classification-whether-its-edible-or-poisonous-part-1/)\n8. [Machine Learning Pipeline Application on Power Plant.](https://projectsbasedlearning.com/apache-spark-machine-learning/machine-learning-pipeline-application-on-power-plant/)\n9. [Machine Learning Project ‚Äì Predict Forest Cover](https://projectsbasedlearning.com/apache-spark-machine-learning/machine-learning-project-predict-forest-cover-part-1/)\n10. [Machine Learning Project Predict Will it Rain Tomorrow in Australia](https://projectsbasedlearning.com/apache-spark-machine-learning/machine-learning-project-predict-will-it-rain-tomorrow-in-australia/)\n11. [Predict Ads Click - Practice Data Analysis and Logistic Regression Prediction](https://projectsbasedlearning.com/apache-spark-machine-learning/predict-ads-click-practice-data-analysis-and-logistic-regression-prediction/)\n12. [Machine Learning Project -Drug Classification](https://projectsbasedlearning.com/apache-spark-machine-learning/drug-classification/)\n13. [Prediction task is to determine whether a person makes over 50K a year](https://projectsbasedlearning.com/apache-spark-machine-learning/prediction-task-is-to-determine-whether-a-person-makes-over-50k-a-year/)\n14. [Machine Learning Project - Classifying gender based on personal preferences](https://projectsbasedlearning.com/apache-spark-machine-learning/classifying-gender-based-on-personal-preferences/)\n15. [Machine Learning Project - Mobile Price Classification](https://projectsbasedlearning.com/apache-spark-machine-learning/mobile-price-classification/)\n16. [Machine Learning Project - Predicting the Cellular Localization Sites of Proteins in Yest](https://projectsbasedlearning.com/apache-spark-machine-learning/predicting-the-cellular-localization-sites-of-proteins-in-yest/)\n17. [Machine Learning Project - YouTube Spam Comment Prediction](https://projectsbasedlearning.com/apache-spark-machine-learning/youtube-spam-comment-prediction/)\n18. [Identify the Type of animal (7 Types) based on the available attributes](https://projectsbasedlearning.com/apache-spark-machine-learning/identify-the-type-of-animal-7-types-based-on-the-available-attributes/)\n19. [Machine Learning Project - Glass Identification](https://projectsbasedlearning.com/apache-spark-machine-learning/glass-identification/)\n20. [Predicting the age of abalone from physical measurements](https://projectsbasedlearning.com/apache-spark-machine-learning/predicting-the-age-of-abalone-from-physical-measurements-part-1/)\n\nI hope you'll enjoy these tutorials.",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qgvit5/end_to_end_20_machine_learning_project_in_apache/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o0gpeaf",
          "author": "RohanVipin",
          "text": "The code is in scala , I only know python",
          "score": 5,
          "created_utc": "2026-01-19 12:12:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hs8of",
              "author": "anal_pudding",
              "text": "Sounds like a personal problem.",
              "score": 2,
              "created_utc": "2026-01-19 15:51:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0kkzkf",
                  "author": "pm_me_your_smth",
                  "text": "Not so personal since scala is very rarely used anywhere and vast majority of ML learners are using python",
                  "score": 2,
                  "created_utc": "2026-01-19 23:44:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0l3x1k",
          "author": "Effective_Forever585",
          "text": "Thanks very much",
          "score": 1,
          "created_utc": "2026-01-20 01:26:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0fi22w",
          "author": "Pretend-Pangolin-846",
          "text": "Thanks, will take a look later!",
          "score": 1,
          "created_utc": "2026-01-19 05:49:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0fqed4",
          "author": "fnehfnehOP",
          "text": "Spark",
          "score": 1,
          "created_utc": "2026-01-19 06:57:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0h3ft5",
          "author": "padakpatek",
          "text": "saved",
          "score": 1,
          "created_utc": "2026-01-19 13:45:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdbghw",
      "title": "For people learning ML how are you thinking about long-term career direction right now?",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qdbghw/for_people_learning_ml_how_are_you_thinking_about/",
      "author": "RepairActual9047",
      "created_utc": "2026-01-15 05:51:30",
      "score": 106,
      "num_comments": 19,
      "upvote_ratio": 0.98,
      "text": "I‚Äôm currently learning machine learning and trying to be more intentional about where this path leads. With how fast models tooling and automation are evolving I‚Äôm finding it harder to answer questions like:\n\n* What kinds of ML-related roles are likely to grow vs get compressed?\n* Which skills actually compound over time instead of becoming quickly abstracted away?\n* How much should learners focus on theory vs applied vs domain depth?\n\nFor those already working in or around ML:  \nHow are you personally thinking about long-term career direction in this field?  \nWhat would you prioritize if you were starting again today?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qdbghw/for_people_learning_ml_how_are_you_thinking_about/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "nzp0u82",
          "author": "DataCamp",
          "text": "From what we're seeing and hearing from our learners, tooling will change fast, but some problems stay stubborn.\n\nWhat seems to keep growing: people who can take a model from ‚Äúworks in a notebook‚Äù to ‚Äúworks in production‚Äù (deployment, monitoring, versioning), and people who pair ML with real domain knowledge (finance/health/ops etc.).\n\nWhat compounds over time: data work (cleaning + feature thinking), evaluation (metrics, leakage, drift), and solid software habits (Git, tests, APIs, basic cloud/containers). Also: being able to explain tradeoffs to non-ML folks.\n\nTheory vs applied: learn enough theory to not cargo-cult, then spend most time shipping small end-to-end projects on real datasets. Add one ‚Äúproduction muscle‚Äù each time (e.g., simple API, logging, monitoring metric).\n\nIf you‚Äôre starting again: foundations first (stats + Python + data), then projects, then specialize once you‚Äôve built a few things you can show.",
          "score": 87,
          "created_utc": "2026-01-15 08:12:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzp4b9j",
          "author": "Gullible_Eggplant120",
          "text": "I am learning Machine Learning for the fun of it myself, so I wont be the best in telling where the careers go. \n\nHowever, I work in consulting and see tons of companies. When it comes to in-house data teams, I am still staggered by how most data people are removed from business common sense and vice versa. I think there is huge potential in being able to speak both langauges. It practically probably means learning Finance and spending some time on the front lines (Sales, Marketing, Ops). But yeah, take this advice with a degree of salt, as these are only my observations.",
          "score": 17,
          "created_utc": "2026-01-15 08:46:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpi6fy",
              "author": "Tender_Figs",
              "text": "The potential is dangerous though - I'm one of these people who can speak both languages and I get taken advantage of on a daily basis. Just be careful about thinking about becoming an \"in-between\", it sounds more fun and in demand, but there's no leverage.",
              "score": 7,
              "created_utc": "2026-01-15 10:58:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzpwbke",
                  "author": "BraindeadCelery",
                  "text": "I have Similar Experiences. I was a process consultant and learned enough SWE/ML to switch to the ML Engineering team. But because my slides were the best, it was always me who was set aside for non-technical / communications / meeting work which impeded my growth as an engineer cause i was also not getting shiny projects to have more time for comms work. \n\nI chaged companies (partly because of that) after a year or so.",
                  "score": 4,
                  "created_utc": "2026-01-15 12:46:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nztknze",
                  "author": "Mammoth_Visit_9044",
                  "text": "Could you elaborate on what you mean by being taken advantage of?",
                  "score": 2,
                  "created_utc": "2026-01-15 23:19:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzqubxd",
          "author": "Jaded_Individual_630",
          "text": "Mathematics will always outlive tool stacks.\n\n\nIt's a great time to be versed with the real fundamentals while tech bros chase the tool of the week.",
          "score": 7,
          "created_utc": "2026-01-15 15:45:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqwn82",
              "author": "smuhamm4",
              "text": "Which mathematics do you recommend besides stats",
              "score": 2,
              "created_utc": "2026-01-15 15:56:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzqy956",
                  "author": "Jaded_Individual_630",
                  "text": "Like learning many instruments or languages, all would prove useful, but some highlights:\n\n\nStatistical Learning Theory (which isn't \"stats\", colloquially), probability and measure theory, functional analysis, historical understanding of the development of ML, lin alg, matrix calculus,¬† operator theory, numerical analysis, actual computer science that isn't just tooling, CUDA....",
                  "score": 6,
                  "created_utc": "2026-01-15 16:03:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzt2i5b",
              "author": "apexvice88",
              "text": "We should also take the power back from tech bros, the way Zuckerberg took power away from the winklevoss twins, not sure if that story is 100% true but it needs to happen more often. I‚Äôm not for gate keeping, however there are certain people that it needs to be kept away from.",
              "score": 1,
              "created_utc": "2026-01-15 21:49:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzt8gw1",
                  "author": "smuhamm4",
                  "text": "What do you mean?",
                  "score": 1,
                  "created_utc": "2026-01-15 22:17:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzqjx2x",
          "author": "Maneisthebeat",
          "text": "Well it's quite obvious that data engineering will remain. It's possible there will be a contraction in budgets for creating systems, but the consulting mindset and requirements to set up a functional system that communicates with other systems will always be needed.\n\nAnd then it depends what field you are going in. Some can be simplified more easily and with less risk than others. If you are working with delicate personally identifiable information, strictness on processes and requirements go up and the consequences of failure to adhere to those standards along with it, meaning the budget to adhere to certain standards is more easily acquired.",
          "score": 3,
          "created_utc": "2026-01-15 14:56:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuu93n",
          "author": "argvalue",
          "text": "I really want to step foot into the healthcare/biotech domain and see what kind of application AI has in it. This has been my motivation to learn ML since the past few months. It's gonna take a lot of time for me, not only because I'm a very slow learner, but also the amount of material required to study is insane.\n\nI want to join one of the companies focusing on this domain (in a year's time mostly). I also have plans of becoming an entrepreneur, which I want to pursue maybe 4-5 years later, mostly into the domain I mentioned above",
          "score": 2,
          "created_utc": "2026-01-16 03:31:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzq4akt",
          "author": "zehen5",
          "text": "Hey, are you taking a university course for ML or something else?",
          "score": 1,
          "created_utc": "2026-01-15 13:35:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhs23q",
      "title": "Curated list of actually free AI courses (no hidden paywalls) - with time commitment for eac",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qhs23q/curated_list_of_actually_free_ai_courses_no/",
      "author": "Bubbly_Ad_2071",
      "created_utc": "2026-01-20 05:07:31",
      "score": 99,
      "num_comments": 10,
      "upvote_ratio": 0.98,
      "text": "I got tired of \"free\" courses that lock certificates or key content behind paywalls. So I went through the major platforms and put together a list of courses that are genuinely free to¬†complete:¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\n\n¬† 1. Elements of AI at Univ. Helsinki - 6 hrs  ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\n\n¬† 2. OpenAI Academy at OpenAI - 5 hrs  ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\n\n¬†3. Prompt Engineering at [DeepLearning.AI](http://DeepLearning.AI) \\- 5 hrs  ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\n\n¬†4. Salesforce AI at Trailhead - 5 hrs ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\n\n¬† 5. Google AI Essentials at Coursera - 10 hrs; Audit free, cert $49 ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\n\n¬† 6. Microsoft AI Fundamentals at MS Learn - 8 hrs; Content free, exam $165¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\n\n¬† Full breakdown with what each covers:  [ https://boredom-at-work.com/best-free-ai-courses/ ](https://boredom-at-work.com/best-free-ai-courses/)\n\n¬† What other free resources would you add? Always looking to expand the list.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qhs23q/curated_list_of_actually_free_ai_courses_no/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o0nf87h",
          "author": "diegoasecas",
          "text": "all of huggingface courses are free\n\nhttps://huggingface.co/learn",
          "score": 11,
          "created_utc": "2026-01-20 11:36:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0m9lni",
          "author": "Holiday_Lie_9435",
          "text": "This is a great list, thank you for putting it together! I would add the [fast.ai](http://fast.ai) courses (Practical Deep Learning for Coders) for something more hands-on and teaches practical skills. The structure is more top-down, and personally found it helpful for getting started on some beginner-friendly [AI/ML projects](https://www.interviewquery.com/p/ai-project-ideas).",
          "score": 9,
          "created_utc": "2026-01-20 05:31:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0p9bf7",
              "author": "famous_chalupa",
              "text": "Is the Practical Deep Learning for Coders still relevant material? It seems like it, but that course has been around for a while.\n\nI'm planning on starting it this week.",
              "score": 1,
              "created_utc": "2026-01-20 17:32:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mep1f",
          "author": "JasperTesla",
          "text": "Thanks! This is gold!",
          "score": 4,
          "created_utc": "2026-01-20 06:10:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mmfks",
          "author": "Jesus_is_King_agreed",
          "text": "Good",
          "score": 3,
          "created_utc": "2026-01-20 07:14:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mu93a",
          "author": "equqe",
          "text": "thank you a lot!",
          "score": 2,
          "created_utc": "2026-01-20 08:25:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mv9hl",
          "author": "Sharp_Level3382",
          "text": "Thanks!",
          "score": 2,
          "created_utc": "2026-01-20 08:34:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0py939",
          "author": "Thin-Chart4124",
          "text": "This is helpful, thank u!",
          "score": 2,
          "created_utc": "2026-01-20 19:25:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qzstr",
          "author": "Charming_Elk_9058",
          "text": "Thank you! I've been looking for something like this.",
          "score": 2,
          "created_utc": "2026-01-20 22:19:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rswo2",
          "author": "dv11JUN",
          "text": "I really appreciate it !",
          "score": 2,
          "created_utc": "2026-01-21 00:55:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhuy6z",
      "title": "The Space Warper (Matrices)",
      "subreddit": "learnmachinelearning",
      "url": "https://v.redd.it/i1hbr85llgeg1",
      "author": "No_Skill_8393",
      "created_utc": "2026-01-20 07:48:11",
      "score": 76,
      "num_comments": 10,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qhuy6z/the_space_warper_matrices/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0ng5qz",
          "author": "Habsu",
          "text": "Oh no, is AI.",
          "score": 3,
          "created_utc": "2026-01-20 11:43:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nfy19",
          "author": "Habsu",
          "text": "Who's this guy? I like this.",
          "score": 1,
          "created_utc": "2026-01-20 11:42:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0py1mm",
          "author": "notionocean",
          "text": "Would be nice if this explained how it actually works mathematically.",
          "score": 1,
          "created_utc": "2026-01-20 19:24:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tdn0s",
              "author": "Reclaimer2401",
              "text": "Honestly, you don't want that. Its best to explain what the numbers really are before you start manipulating the matrixs.¬†\n\n\nThere's a lot of stuff in linear algebra, and it was a pain for me until i actually understand wtf I was doing and why.¬†\n\n\nThis video sucks though¬†",
              "score": 1,
              "created_utc": "2026-01-21 07:08:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ulqtu",
                  "author": "notionocean",
                  "text": "I'm in Calc 3. It's disappointing how the video just handwaves the numbers like 'See these numbers? Look what it does!' with no indication of how it actually works mathematically.",
                  "score": 1,
                  "created_utc": "2026-01-21 13:18:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0q5y5w",
          "author": "Door_Number_Three",
          "text": "Ah yes, the hello world of math for ML. How many people crash and burn when learning linear transformations.",
          "score": 1,
          "created_utc": "2026-01-20 20:01:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tdrsr",
              "author": "Reclaimer2401",
              "text": "I did at first.¬†\n\n\nI just kept at it until it all clicked. It was slow and painful, but then I ended up with an A in the class and for fun used linear algebra to calculate the volume of a tesseract",
              "score": 1,
              "created_utc": "2026-01-21 07:09:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ncix0",
          "author": "bingbestsearchengine",
          "text": "I love his voice",
          "score": 0,
          "created_utc": "2026-01-20 11:14:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ne7g4",
              "author": "Archtarius",
              "text": "NotebookLM",
              "score": 1,
              "created_utc": "2026-01-20 11:28:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qe7si1",
      "title": "RNNs are the most challenging thing to understand in ML",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qe7si1/rnns_are_the_most_challenging_thing_to_understand/",
      "author": "radjeep",
      "created_utc": "2026-01-16 05:48:12",
      "score": 73,
      "num_comments": 35,
      "upvote_ratio": 0.88,
      "text": "I‚Äôve been thinking about this for a while, and I‚Äôm curious if others feel the same.\n\nI‚Äôve been reasonably comfortable building intuition around most ML concepts I‚Äôve touched so far. CNNs made sense once I understood basic image processing ideas. Autoencoders clicked as compression + reconstruction. Even time series models felt intuitive once I framed them as structured sequences with locality and dependency over time.\n\nBut RNNs? They‚Äôve been uniquely hard in a way nothing else has been.\n\nIt‚Äôs not that the math is incomprehensible, or that I don‚Äôt understand sequences. I *do*. I understand sliding windows, autoregressive models, sequence-to-sequence setups, and I‚Äôve even built LSTM-based projects before without fully ‚Äúgetting‚Äù what was going on internally.\n\nWhat trips me up is that RNNs don‚Äôt give me a stable mental model. The hidden state feels fundamentally opaque i.e. it's not like a feature map or a signal transformation, but a compressed, evolving internal memory whose semantics I can‚Äôt easily reason about. Every explanation feels syntactically different, but conceptually slippery in the same way.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qe7si1/rnns_are_the_most_challenging_thing_to_understand/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "nzvkpsj",
          "author": "ds_account_",
          "text": "How do you feel about SVM, VAE and latent diffusion.\n\nBut I agree RNN can be tough without first grasping time series analysis.",
          "score": 47,
          "created_utc": "2026-01-16 06:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvhgkn",
          "author": "newrockstyle",
          "text": "RNNs are tough because their hidden state is hard to visualise and reason about.",
          "score": 30,
          "created_utc": "2026-01-16 06:05:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw0tsq",
              "author": "narasadow",
              "text": "TBH I felt the same for Kalman Filters",
              "score": 11,
              "created_utc": "2026-01-16 08:52:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzw2yfr",
          "author": "UnusualClimberBear",
          "text": "I think the best explanation is this famous yet old blog post by C. Olah : [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)",
          "score": 15,
          "created_utc": "2026-01-16 09:12:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzynlep",
              "author": "CasulaScience",
              "text": "Agreed this is what made it click for me. OP, they're actually pretty simple and make a lot of sense if you come from a comp sci background (which I don't). It's very reminiscent of a turing machine, you have some hidden state (instead of \"tape\") that you write to and read from at every step. What you write/delete and what you read at each step is controlled by little neural nets. \n\nThat's really it.",
              "score": 4,
              "created_utc": "2026-01-16 18:05:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzvw5g8",
          "author": "Expensive_Fun4346",
          "text": "if you think that's opaque, wait until you look at reinforcement learning",
          "score": 11,
          "created_utc": "2026-01-16 08:09:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01o5vw",
              "author": "jsh_",
              "text": "which aspects of reinforcement learning do you find opaque? I feel like once you understand MDPs it's relatively straightforward",
              "score": 1,
              "created_utc": "2026-01-17 03:35:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o09t62o",
                  "author": "Organic_botulism",
                  "text": "MDPs are the absolute basics for understanding tabular RL but once you leave that setting and start relaxing assumptions things get hairy very quickly as convergence proofs (e.g. for Q-learning) rely on a tabular setting among other requirements.\n\nThe Bellman error not being learnable is quite opaque IMO, as is the intuition for why bootstrapping, off-policy learning and function approximation can lead to divergence and instability during training (e.g. one-step semi-gradient Q-learning will diverge in certain settings).\n\nThese are all opaque ideas. If you‚Äôre using any type of function approximation what is the requirement needed to guarantee stability? Why does linear function approximation with dynamic programming sometimes fail even if the least squares solution is found at each step?",
                  "score": 1,
                  "created_utc": "2026-01-18 10:55:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvn4mu",
          "author": "Silly_Guidance_8871",
          "text": "I agree, for reasons similar to why reasoning about loops and recursion are more difficult than non-branching code paths: There's a lot more implied state that's not easily managed",
          "score": 11,
          "created_utc": "2026-01-16 06:51:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvj3s2",
          "author": "TBSchemer",
          "text": "Then how do you feel about transformers?",
          "score": 18,
          "created_utc": "2026-01-16 06:18:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwuta8",
              "author": "quadrillio",
              "text": "I think self attention is actually easier to grasp. And 3blue1brown has a set of excellent videos explaining them. There aren‚Äôt very many accessible resources for things like LSTM and many of the diagrams found online are over simplified or badly explained.",
              "score": 2,
              "created_utc": "2026-01-16 12:56:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzz650x",
                  "author": "TBSchemer",
                  "text": "Transformers take the same mechanisms used in LSTMs and add more complexity to it. If you can understand LLMs, you can understand RNNs.",
                  "score": 2,
                  "created_utc": "2026-01-16 19:27:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvhkfl",
          "author": "d0r1h",
          "text": "On the same boat, I've been revising ML concepts and got stuck at RNN currently. I've opened 10s of blogs and two books, Just to grasp the RNN. Gonna give few more hrs before moving forward.",
          "score": 5,
          "created_utc": "2026-01-16 06:06:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwqjy1",
              "author": "CuriousAIVillager",
              "text": "What did you use as a checklist?",
              "score": 1,
              "created_utc": "2026-01-16 12:29:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o01aubi",
                  "author": "d0r1h",
                  "text": "checklist as in ?",
                  "score": 1,
                  "created_utc": "2026-01-17 02:11:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvyleu",
          "author": "DrXaos",
          "text": "RNNs are dynamical systems in the 'chaos theory' sense.  Their original difficulties in training were because they had in effect strong Lyapunov exponents in either forward or backward directions resulting in exponential decay or explosion in state or gradients. \n\nFunny that until 8 years ago, RNNs of various forms were the state of the art as the most complex and interesting machine learning architecture.",
          "score": 6,
          "created_utc": "2026-01-16 08:32:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzs1ck",
          "author": "s-jb-s",
          "text": "I come from a probability theory background, so my intuition for \"ML\" came from outside the way CS students tend to think about things, which I generally don't understand.\n\nIf you're struggling with the intuition, I'd advise taking a step back from RNNs and looking at the evolution of the problems they solve. I think a natural progression to build up your intuition regarding latent-state models is to start with Discrete-time Hidden Markov Models, which are very easy to intuit.\n\nThe problem is that HMMs are inefficient in high dimensions. Factorial HMMs improve this by distributing the state into multiple binary variables, but this makes inference much more expensive to calculate. Intuitively, the fix is to move to Linear Dynamical Systems, where the state vectors are now continuous rather than discrete (think Kalman Filters, if you're familiar). This solves the representation problem, but now you have a linearity issue because you can only model simple curves. How do we fix that? We take the Linear Dynamical System and wrap the transition in a non-linear activation function. That is effectively an RNN.\n\nThere's a lot of nuance missing here and I've been a bit handwavey (it's not intended to fix your intuition) but I think there's value in studying what came before RNN's and building your intuition up from there, particularly in relation to what actually is actually going on in latent space, and what that represents. I'm unsure how helpful this is if you're not particularly interested in the theory, and just want some intuition. But I think the intuition comes from the theory, and seeing how it progresses.",
          "score": 3,
          "created_utc": "2026-01-16 21:10:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwckfh",
          "author": "divided_capture_bro",
          "text": "Wait, you can't reason about evolving internal states? It's almost like it is a black box of some sorts...",
          "score": 2,
          "created_utc": "2026-01-16 10:40:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzyuqec",
              "author": "bythenumbers10",
              "text": "Meanwhile, statistical methods await the disillusioned deep learning practitioners with open arms, ready to soothe their crisis of blind faith unrewarded with transparency and explicable tuning methods.",
              "score": 2,
              "created_utc": "2026-01-16 18:36:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzz0hr6",
                  "author": "Fair_Treacle4112",
                  "text": "compare normal ad hoc arrest scary practice coordinated absorbed doll employ\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*",
                  "score": 0,
                  "created_utc": "2026-01-16 19:01:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzyg735",
          "author": "Longjumping_Echo486",
          "text": "Personally I feel lstm math is tougher due to multiple gates  and it's beautiful when u see mathematically how the gates are deleting and updating info using pointwise operations and create a refined long term memory",
          "score": 2,
          "created_utc": "2026-01-16 17:32:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzyiikb",
          "author": "dankwartrustow",
          "text": "It only really made sense to me when I learned about vanishing and exploding gradients",
          "score": 2,
          "created_utc": "2026-01-16 17:42:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzypcox",
          "author": "Mr_iCanDoItAll",
          "text": "Try learning HMMs if you haven't already. Might help intuit hidden states.",
          "score": 1,
          "created_utc": "2026-01-16 18:13:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o01qdmt",
          "author": "pool007",
          "text": "OP, I think you already understand the main concept. It's really not a curated memory structure or features but computation and flow of data that's supposed to become stuff to memorize and that is all it does, imo. Flow should be fast to compute but still have expression power while being stable for training.",
          "score": 1,
          "created_utc": "2026-01-17 03:50:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o064q4r",
          "author": "arsenale",
          "text": "Each RNN variant is a standalone architecture.\n\nVec2Seq\n\nSeq2Seq\n\n...\n\nEach one is a totally different model, they are called RNN just because yes, they share an equation which uses the same Wxh Whh etc, but those matrices are multiplied by different values making the architecture completely different. Just stick with one architecture until you get it?",
          "score": 1,
          "created_utc": "2026-01-17 20:45:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzz09km",
          "author": "HumbleJiraiya",
          "text": "I dont know man. I understood RNNs immediately. It was probably the most intuitive concept for me in DL üòÖ. \n\nAutoencoders were harder to understand than RNNs. \n\nWe‚Äôre all different I guess.",
          "score": 0,
          "created_utc": "2026-01-16 19:00:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qf2q85",
      "title": "decision tree from scratch in js. no libraries.",
      "subreddit": "learnmachinelearning",
      "url": "https://v.redd.it/alufthat0udg1",
      "author": "Ok-Statement-3244",
      "created_utc": "2026-01-17 03:51:32",
      "score": 71,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qf2q85/decision_tree_from_scratch_in_js_no_libraries/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o01ud0b",
          "author": "disquieter",
          "text": "Wow, like, wow!",
          "score": 1,
          "created_utc": "2026-01-17 04:17:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04gwgm",
          "author": "giadev",
          "text": "thats so cool",
          "score": 1,
          "created_utc": "2026-01-17 16:01:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfcotf",
      "title": "An introduction to Physics Informed Neural Networks (PINNs): Teach your neural network to ‚Äúrespect‚Äù Physics",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qfcotf/an_introduction_to_physics_informed_neural/",
      "author": "omunaman",
      "created_utc": "2026-01-17 13:03:11",
      "score": 69,
      "num_comments": 11,
      "upvote_ratio": 0.92,
      "text": "https://preview.redd.it/ll4z0ewvqwdg1.png?width=1100&format=png&auto=webp&s=e6a375679fb5575866953109c00e86d8eb31523a\n\nAs universal function approximators, neural networks can learn to fit any dataset produced by complex functions. With deep neural networks, overfitting is not a feature. It is a bug.\n\nMedium Link for better readability: [https://vizuara.medium.com/an-introduction-to-physics-informed-neural-networks-pinns-teach-your-neural-network-to-respect-af484ac650fc](https://vizuara.medium.com/an-introduction-to-physics-informed-neural-networks-pinns-teach-your-neural-network-to-respect-af484ac650fc)\n\nLet us consider a hypothetical set of experiments. You throw a ball up (or at an angle), and note down the height of the ball at different points of time.\n\nWhen you plot the height v/s time, you will see something like this.\n\nhttps://preview.redd.it/b9byjx62pwdg1.png?width=1100&format=png&auto=webp&s=22aebc098ad30d2b18505fcaa3d80cf61777f2b5\n\nIt is easy to train a neural network on this dataset so that you can predict the height of the ball even at time points where you did not note down the height in your experiments.\n\nFirst, let us discuss how this training is done.\n\n# Training a regular neural network\n\n\n\nhttps://preview.redd.it/732wrp23pwdg1.png?width=1100&format=png&auto=webp&s=5c65e4fc46e3a8fd8fcac281361ece4328932f2b\n\nYou can construct a neural network with few or multiple hidden layers. The input is time (t) and the output predicted by the neural network is height of the ball (h).\n\nThe neural network will be initialized with random weights. This means the predictions of h(t) made by the neural network will be very bad initially as shown in the image below.\n\nhttps://preview.redd.it/xdgeu9s4pwdg1.png?width=1100&format=png&auto=webp&s=2e97b932fe7bef937f45716295435c7d50c0212f\n\nWe need to penalize the neural network for making these bad predictions right? How do we do that? In the form of loss functions.\n\nLoss of a neural network is a measure of how bad its predictions are compared the real data. The close the predictions and data, the lower the loss.\n\nA singular goal of neural network training is to minimize the loss.\n\nSo how can we define the loss here? Consider the 3 options below.\n\nhttps://preview.redd.it/slcx6y27pwdg1.png?width=1100&format=png&auto=webp&s=fcccb9ec6c9aac8b976b71ae5a7f7f6dfd481c24\n\nIn all the 3 options, you are finding the average of some kind of loss.\n\n* **Option 1 is not good**¬†because positive and negative errors will cancel each other.\n* **Option 2 is okay**¬†because we are taking the absolute value of errors, but the problem is modulus function is not differentiable at x=0.\n* **Option 3 is the best**. It is a square function which means individual errors are converted to positive numbers and the function is differentiable. This is the famous Mean Squared Error (MSE). You are taking the mean value of the square of all individual errors.\n\nHere error means the difference between actual value and predicted value.\n\nMean Squared Error is minimum when the predictions are very close to the experimental data as shown in the figure below.\n\nhttps://preview.redd.it/vwm6mxq8pwdg1.png?width=1100&format=png&auto=webp&s=33983e165ecec1efca3a973e97b3d28aa2a89782\n\nBut there is a problem with this approach. What if your experimental data was not good? In the image below you can see that one of the data points is not following the trend shown by the rest of the dataset.\n\nhttps://preview.redd.it/mswknvl9pwdg1.png?width=1100&format=png&auto=webp&s=71546cc05f741175a11e486ae3fe6a77c44b82e7\n\nThere can be multiple reasons due to which such data points show up in the data.\n\n1. You did not perform the experiments well. You made a manual mistake while noting the height.\n2. The sensor or instrument using which you were making the height measurement was faulty.\n3. A sudden gush of wind caused a sudden jump in the height of the ball.\n\nThere could be many possibilities that results in outliers and noise in a dataset.\n\nKnowing that real life data may have noise and outliers, it will not be wise if we train a neural network to exactly mimic this dataset. It results in something called as overfitting.\n\nhttps://preview.redd.it/1e7r509apwdg1.png?width=1100&format=png&auto=webp&s=e3269c58b8ca9e873945ca9970aafac78bc53279\n\nhttps://preview.redd.it/l0fgrzrapwdg1.png?width=1100&format=png&auto=webp&s=28acb46d2af8e6398876ee107b7900e860061904\n\nIn the figure above, mean squared error will be low in both cases. However in one case neural network is fitting on outlier also, which is not good. So what should we do?\n\n# Bring physics into the picture\n\nIf you are throwing a ball and observing its physics, then you already have some knowledge about the trajectory of the ball, based on Newton‚Äôs laws of motion.\n\nSure, you may be making simplifications by assuming that the effect of wind or air drag or buoyancy are negligible. But that does not take away from the fact that you already have decent knowledge about this system even in the absence of a trained neural network.\n\nhttps://preview.redd.it/8cudgx0epwdg1.png?width=1100&format=png&auto=webp&s=9efaf22e50525030c0ceaa9995b0afe96a26c79d\n\nThe physics you assume may not be in perfect agreement with the experimental data as shown above, but it makes sense to think that the experiments will not deviate too much from physics.  \n\n\nhttps://preview.redd.it/fpy7q3oepwdg1.png?width=1100&format=png&auto=webp&s=dc5ff5cacaf8b8d2895139589897c6dd3d670be9\n\nSo if one of your experimental data points deviate too much from what physics says, there is probably something wrong with that data point. So how can you let you neural network take care of this?\n\n# How can you teach physics to neural networks?\n\nIf you want to teach physics to neural network, then you have to somehow incentivize neural network to make predictions closer to what is suggested by physics.\n\nIf the neural network makes a prediction where the height of the ball is far away from the purple dotted line, then loss should increase.\n\nIf the predictions are closer to the dotted line, then the loss should be minimum.\n\nWhat does this mean? Modify the loss function.\n\nHow can you modify the loss function such that the loss is high when predictions deviate from physics? And how does this enable the neural network make more physically sensible predictions?¬†**Enter PINN Physics Informed Neural Network.**  \n  \nPhysics Informed Neural Network (PINN)\n\nThe goal of PINNs is to solve (*or learn solutions to*) differential equations by embedding the known physics (or governing differential equations) directly into the neural network‚Äôs training objective (loss function).\n\nThe idea of PINNs were introduced in this seminal paper by Maziar Raissi et. al.:¬†[https://maziarraissi.github.io/PINNs/](https://maziarraissi.github.io/PINNs/)\n\nThe basic idea in PINN is to have a neural network is trained to minimize a loss function that includes:\n\n1. A¬†**data mismatch**¬†term (*if observational data are available*).\n2. A¬†**physics loss**¬†term enforcing the differential equation itself (and initial/boundary conditions).\n\n# Let us implement PINN on our example\n\nLet us look at what we know about our example. When a ball is thrown up, it trajectory h(t) varies according to the following ordinary differential equation (ODE).\n\nhttps://preview.redd.it/vacsz6dlpwdg1.png?width=1100&format=png&auto=webp&s=14111c810dba1e861fbcc71a1bf8d920e479448c\n\nHowever this ODE alone cannot fully describe h(t) uniquely. You also need an initial condition. Mathematically this is because to solve a first-order differential equation in time, you need 1 initial condition.\n\nLogically, to know height as a function of time, you need to know the starting height from which the ball was thrown. Look at the image below. In both cases, the balls are thrown at the exact same time with the exact same initial velocity component in the vertical direction. But the h(t) depends on the initial height. So you need to know h(t=0) for fully describing the height of the ball as a function of time.\n\nhttps://preview.redd.it/eobv9u1mpwdg1.png?width=1100&format=png&auto=webp&s=a28a6c8584f37683f703b4c72a5a8f436353dedc\n\nThis means it is not enough to make the neural network make accurate predictions on dh/dt, the neural network should also make accurate prediction on h(t=0) for fully matching the physics in this case.\n\n# Loss due to dh/dt (ODE loss)\n\nWe know the expected dh/dt because we know the initial velocity and acceleration due to gravity.\n\nHow do we get the dh/dt predicted by the neural network? After all it is predicting height h, not velocity v or dh/dt. The answer is¬†**Automatic differentiation (AD).**\n\nBecause most machine‚Äêlearning frameworks (e.g., TensorFlow, PyTorch, JAX) support automatic differentiation, you can compute dh/dt by differentiating the neural network.\n\nThus, we have a predicted dh/dt (from the neural network differentiation) for every experimental time points, and we have an actual dh/dt based on the physics.\n\nhttps://preview.redd.it/msf6gyunpwdg1.png?width=1100&format=png&auto=webp&s=1392d9e60f5ee011a480392af07e05bc5d094492\n\nNow we can define a loss due to the difference between predicted and physics-based dh/dt.\n\nhttps://preview.redd.it/68xl4xpopwdg1.png?width=1100&format=png&auto=webp&s=5b9a727be489bd8736e8ffc235f49fca5dc25b9a\n\nMinimizing this loss (which I prefer to call ODE loss) is a good thing to ensure that neural network learns the ODE. But that is not enough. We need to make the neural network follow the initial condition also. That brings us to the next loss term.Initial condition loss\n\n# Initial condition loss\n\nThis is easy. You know the initial condition. You make the neural network make a prediction of height for t=0. See how far off the prediction is from the reality. You can construct a squared error which can be called as the¬†*Initial Condition Loss.*\n\nhttps://preview.redd.it/4u4syj1qpwdg1.png?width=1100&format=png&auto=webp&s=591b7e0f46ebf32024533c9d727042a889c3007d\n\nSo is that it? You have ODE loss and Initial condition loss. Is it enough that the neural network tries to minimize these 2 losses? What about the experimental data? There are 3 things to consider.\n\n1. You cannot throw away the experimental data.\n2. You cannot neglect the physics described by the ODEs or PDEs.\n3. You cannot neglect the initial and/or boundary conditions.\n\nThus you have to also consider the data-based mean squared error loss along with ODE loss and Initial condition loss.\n\n# The modified loss term\n\nThe simple mean squared error based loss term can now be modified like below.\n\nhttps://preview.redd.it/n2xc18prpwdg1.png?width=1100&format=png&auto=webp&s=95fabc8b54b2b291292d6ab2c15f5810c13379ce\n\nIf there are boundary conditions in addition to initial conditions, you can add an additional term based on the difference between predicted boundary conditions and actual boundary conditions.\n\nhttps://preview.redd.it/ezh3in7spwdg1.png?width=1100&format=png&auto=webp&s=70367e6fbb1aa6e7924d93da8ff3b0ce8898419d\n\nHere the Data loss term ensures that the predictions are not too far from the experimental data points.\n\nThe¬†*ODE loss term*¬†\\+ the¬†*initial condition loss term*¬†ensures that the predictions are not too far from what described by the physics.\n\nIf you are pretty sure about the physics the you can set Œª1 to zero. In the ball throwing experiment, you will be sure about the physics described by our ODE if air drag, wind, buoyancy and any other factors are ignored. Only gravity is present. And in such cases, the PINN effectively becomes an ODE solver.\n\nHowever, for real life cases where only part of the physics is known or if you are not fully sure of the ODE, then you retain Œª1 and other Œª terms in the net loss term. That way you force the neural network to respect physics as well as the experimental data. This also suppress the effects of experimental noise and outliers.\n\n>",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qfcotf/an_introduction_to_physics_informed_neural/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o03l86k",
          "author": "n0obmaster699",
          "text": "So you just add a lagrange multiplier which follows the eom?",
          "score": 6,
          "created_utc": "2026-01-17 13:11:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0fark1",
              "author": "omunaman",
              "text": "Yep! It is essentially acting as a soft constraint added to the loss function, very similar to the penalty method in Lagrange multipliers.",
              "score": 1,
              "created_utc": "2026-01-19 04:55:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0h5w4b",
                  "author": "nickpsecurity",
                  "text": "I enjoyed reading it. Nice visuals to help with explanations, too.",
                  "score": 2,
                  "created_utc": "2026-01-19 13:59:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0hwma0",
                  "author": "n0obmaster699",
                  "text": "Seems fair",
                  "score": 1,
                  "created_utc": "2026-01-19 16:10:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o08qh47",
          "author": "inmadisonforabit",
          "text": "Look into regularization. Also, if you need your models to respect physics, it would be best to avoid using a NN to begin with and instead directly model it via the ODEs (in reality, likely PDEs) you're already using.",
          "score": 5,
          "created_utc": "2026-01-18 05:16:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0fb4ku",
              "author": "omunaman",
              "text": "If we fully know the ODE and the parameters, a standard numerical solver is definitely better and more accurate. I mainly used this simple case just to demonstrate the concept for beginners.\n\nThe real use of PINNs shines in inverse problems where we have the data but don't know the parameters (like inferring friction from trajectory) or when dealing with noisy data. Classical solvers often break down with noisy input, whereas the neural network can act as a natural regularizer to smooth it out while adhering to the physics.",
              "score": 2,
              "created_utc": "2026-01-19 04:58:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0k576i",
                  "author": "inmadisonforabit",
                  "text": "I do agree that classical solvers can break down when the input is noisy.\n\nWhat I'm curious about are the conditions in which this approach outperforms a standard neural network. I think it's an interesting approach, and similar to something I've implemented before.\n\nGenerally speaking, if you are measuring a physical system, one would hope the amount of anomalies beyond noise are substantially less than \"good\" or precise measurements. In that case, a neural network, given enough data, would basically average out those anomalies assuming one doesn't overfit, especially with regularization. To me, it looks like your proposal is basically regularization. \n\nSo in what situations would a PINN perform better that a typical neural net?",
                  "score": 1,
                  "created_utc": "2026-01-19 22:21:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0h5pj0",
              "author": "nickpsecurity",
              "text": "I believe a lot of people also don't know about those. They read about NN's all the time due to the AI bubble (err marketing investments). It's why I'm trying to promote in AI spaces both old school techniques and mixing them with AI.\n\nBtw, what's the best, open-source solvers for ODE's or PDE's?",
              "score": 0,
              "created_utc": "2026-01-19 13:58:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0k5opz",
                  "author": "inmadisonforabit",
                  "text": "That's a good question. I don't often encounter the need for solvers, but in my experience, it usually depends on the application. Solvers seems to be often built for specific applications.\n\nI generally just use MATLAB's solvepde for general problems. Otherwise, if you're in industry, you'll probably come across Ansys.",
                  "score": 2,
                  "created_utc": "2026-01-19 22:24:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0rf0pc",
          "author": "SadEntertainer9808",
          "text": "This is very interesting, but I'm getting a bit hung up on something: you're fitting a network to solutions to a known ODE. There's obviously some cool out-of-the box smoothing you get, but you're sort of losing the conventional advantage of a NN (approximation of an unknown function). I'd want to see outperformance of a conventional ODE solver in some dimension before I got excited about this.\n\nHowever, it is cool to see information underivable from the training data being burned in to the NN. It's provocative.",
          "score": 1,
          "created_utc": "2026-01-20 23:39:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhfh2a",
      "title": "Learning ML is clear but applying it to real problems feels overwhelming",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qhfh2a/learning_ml_is_clear_but_applying_it_to_real/",
      "author": "Waltace-berry59004",
      "created_utc": "2026-01-19 20:19:51",
      "score": 44,
      "num_comments": 12,
      "upvote_ratio": 0.94,
      "text": "Courses and tutorials make sense, but once I try to apply ML to a real problem, everything explodes: data quality, problem definition, deployment, and user needs.\n\nI‚Äôm not trying to publish papers, I want to build something useful. How do beginners move from I understand the algorithms to this actually solves a problem?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qhfh2a/learning_ml_is_clear_but_applying_it_to_real/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o0jyulc",
          "author": "RickSt3r",
          "text": "You don‚Äôt, you find a problem then design a solution. Sometimes it‚Äôs a ML sometimes it‚Äôs good old fashion software development.",
          "score": 26,
          "created_utc": "2026-01-19 21:51:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jh6q2",
          "author": "Password-55",
          "text": "Yes, that is why I think AI is too hyped right now. Making actual useful products is key and I think it‚Äòs quite hard, if not sometimes the wrong tool in many applications.\n\nI do not have the answer. Tell me if you find it.",
          "score": 11,
          "created_utc": "2026-01-19 20:25:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kfbec",
          "author": "Commercial_Note_210",
          "text": "> How do beginners move from I understand the algorithms to this actually solves a problem?\n\nDepends on your area of work, but largely it's flipped. You have a business problem and you work backwards towards a solution.\n\nSomething like... \"I want personalized recommendations on this page -> I have access to the users, items. I can build a label from clicks. I can build feature profiles for users, items -> I can frame this as a (user, item, context, label) recommendation task -> there's a bunch of approaches; next click prediction, CTR prediction, etc -> stop and do research and decide on exact problem framing -> do some research on modeling approaches -> learn about FTTransformer, AutoInt, XGBoost, SLIM, etc -> build a model -> iterate\"\n\nHowever, if \"there's a bunch of approaches\" lands on something like binary click prediction, Ill just use my textbook knowledge and just chuck AutoGluon or XGBoost at it, but as you iterate youll learn more modern approaches, if appropriate.",
          "score": 4,
          "created_utc": "2026-01-19 23:13:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kj2b0",
          "author": "Jaded_Individual_630",
          "text": "That's because there's a cottage industry that benefits from \"boot camping\" people with toolchains and tech stacks. This requires them to minimize the complexity of the task in all of their materials.¬†\n\n\nReal cutting edge ML is never going to come off the back of \"import py-machinelearningplease\" and a $5 udemy course.¬†\n\n\nNot saying that's what you did, but that is a big predatory industry.",
          "score": 5,
          "created_utc": "2026-01-19 23:33:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0l6dhl",
          "author": "Important_Sundae1632",
          "text": "Have you looked at Kaggle competitions? They‚Äôre full of concrete examples showing how ML is actually applied.\n\nWhat you‚Äôre describing is already most of real-world ML: defining the problem, cleaning data, deploying. You don‚Äôt need a complex model to start, and most effort usually goes into those steps anyway. Keep the scope small, start with the simplest approach that works, and walk through one complete example to see how everything connects. If you have a specific problem in mind, feel free to share it or DM me",
          "score": 2,
          "created_utc": "2026-01-20 01:40:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kz21d",
          "author": "Palmquistador",
          "text": "Amen to that.",
          "score": 1,
          "created_utc": "2026-01-20 00:59:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0lig1n",
          "author": "Ty4Readin",
          "text": "Machine learning is a lot more than just understanding the algorithms. I think understanding the algorithms is ironically one of the least practically useful skills that you need.\n\nIf you have decent stats, CS, and domain knowledge then everything becomes a lot more straightforward. \n\nIf you run into problems, it's often because you are lacking in one of these areas, and learning more about the algorithms behind some models won't help much unfortunately.\n\nFor example, problem formulation? Thats entirely stats & domain knowledge.\n\nDeployment? Almost entirely CS.\n\nUser workflow/needs? Mostly domain knowledge.\n\nThe best way to learn is to actually try to solve problems as side projects, and make the problems that you actually care about personally.",
          "score": 1,
          "created_utc": "2026-01-20 02:46:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0lydfb",
          "author": "Dark-Maverick",
          "text": "If you want to practice to build confidence and put projects in resume, then pick datasets from kaggle and practice it.\n\nThink of anything whatever you like football - there is a football dataset available, cricket - cricket dataset available, tv shows dataset ,movie dataset games dataset these are available just for starting building some projects.\n\nAfter that find innovative ideas explore different datasets on internet and try to find solutions.",
          "score": 1,
          "created_utc": "2026-01-20 04:16:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0m4gxm",
          "author": "Adept_Carpet",
          "text": "Data management is often where 90% of the effort is spent and that's assuming the data you need exists. It's generally futile if the organization wasn't managing data well to begin with.\n\n\nThen model deployment (and everything post-deployment like monitoring and debugging and updating and security) is really an unsolved problem. It reminds me of how web app deployment was 20+ years ago. Editing the production site with vi, having to recompile the webserver, old versions of files scattered everywhere, no backups, etc.",
          "score": 1,
          "created_utc": "2026-01-20 04:55:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0m8ese",
          "author": "patternpeeker",
          "text": "This feeling is normal, because tutorials hide the hardest parts on purpose. In real projects the algorithm is usually the easy piece, and everything around it is where time goes. What helps is scoping problems way down, pick something where a dumb baseline is acceptable and iterate from there. Focus on data first, then evaluation that matches what a user actually cares about, even if it is a rough metric. Deployment and feedback loops can be ugly and manual at the start, and that is fine. You get comfortable by shipping small, imperfect things and seeing where they break.",
          "score": 1,
          "created_utc": "2026-01-20 05:22:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0merqp",
          "author": "AccordingWeight6019",
          "text": "This feeling is very common, and it is usually the gap between toy problems and real systems that shows up. Most real ML work is not about the algorithm at all, but about turning a vague goal into a concrete, testable question and accepting imperfect data. One way to reduce the overwhelm is to start by stripping the problem down to the smallest version that would still be useful, even if it feels almost trivial. Build something that works end to end, even if it is naive, and then iterate. Over time, you realize the messiness is the work, not a sign that you are doing it wrong.",
          "score": 1,
          "created_utc": "2026-01-20 06:10:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0thvlz",
          "author": "stairwayfromheaven",
          "text": "I‚Äôve noticed some studios like thedreamers.us focus on helping people frame ML problems around real business or user needs, which can be helpful when you‚Äôre past theory but unsure how to apply it in practice.",
          "score": 1,
          "created_utc": "2026-01-21 07:46:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qf92tn",
      "title": "I implemented a GPT-style model from scratch using PyTorch to understand the math behind Attention & Fine-tuning (following Sebastian Raschka's book)",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qf92tn/i_implemented_a_gptstyle_model_from_scratch_using/",
      "author": "Bthreethree",
      "created_utc": "2026-01-17 09:39:19",
      "score": 39,
      "num_comments": 7,
      "upvote_ratio": 0.96,
      "text": "I've spent the last few weeks building a GPT-style LLM entirely from scratch in PyTorch to understand the architecture. This isn't just a wrapper; it's a full implementation covering the entire lifecycle from tokenization to instruction fine-tuning.\n\nI have followed Sebastian Raschka's 'Build a LLM from Scratch' book for the implementation, here is the breakdown of the repo:\n\n**1. Data & Tokenization (**`src/data.py`**)** Instead of using pre-built tokenizers, I implemented:\n\n* `SimpleTokenizerV2`: Handles regex-based splitting and special tokens (`<|endoftext|>`, `<|unk|>`).\n* `GPTDatasetV1`: A sliding-window dataset implementation for efficient autoregressive training.\n\n**2. The Attention Mechanism (**`src/attention.py`**)**\n\nI manually implemented `MultiHeadAttention` to understand the tensor math:\n\n* Handles the query/key/value projections and splitting heads.\n* Implements the **Causal Mask** (using `register_buffer`) to prevent the model from \"cheating\" by seeing future tokens.\n* Includes `SpatialDropout` and scaled dot-product attention.\n\n**3. The GPT Architecture (**`src/model.py`**)** A complete 124M parameter model assembly:\n\n* Combines `TransformerBlock`, `LayerNorm`, and `GELU` activations.\n* Features positional embeddings and residual connections exactly matching the GPT-2 spec.\n\n**4. Training & Generation (**`src/train.py`**)**\n\n* Custom training loop with loss visualization.\n* Implements `generate()` with **Top-K sampling** and **Temperature scaling** to control output creativity.\n\n**5. Fine-tuning:**\n\n* **Classification (**`src/finetune_classification.py`**):** Adapted the backbone to detect Spam/Ham messages (90%+ accuracy on the test set).\n* **Instruction Tuning (**`src/finetune_instructions.py`**):** Implemented an Alpaca-style training loop. The model can now handle instruction-response pairs rather than just completing text.\n\n**Repo:** [https://github.com/Nikshaan/llm-from-scratch](https://github.com/Nikshaan/llm-from-scratch)\n\nI‚Äôve tried to comment every shape transformation in the code. If you are learning this stuff too, I hope this reference helps!",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qf92tn/i_implemented_a_gptstyle_model_from_scratch_using/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o02wnfv",
          "author": "Bthreethree",
          "text": "This is the code snippet of the most interesting part - building Multi-head attention from scratch instead of using nn.MultiheadAttention.\n\n[https://github.com/Nikshaan/llm-from-scratch](https://github.com/Nikshaan/llm-from-scratch)\n\nclass MultiHeadAttention(nn.Module):  \ndef \\_\\_init\\_\\_(self, d\\_in, d\\_out, context\\_length, dropout, num\\_heads, qkv\\_bias=False): # context length is max sequence length for the mask  \nsuper().\\_\\_init\\_\\_()  \nassert d\\_out % num\\_heads == 0, \"d\\_out must be divisible by num\\_heads\"  \nself.d\\_out = d\\_out  \nself.num\\_heads = num\\_heads  \nself.head\\_dim = d\\_out // num\\_heads # dimension per head  \nself.W\\_query = nn.Linear(d\\_in, d\\_out, bias=qkv\\_bias)  \nself.W\\_key = nn.Linear(d\\_in, d\\_out, bias=qkv\\_bias)  \nself.W\\_value = nn.Linear(d\\_in, d\\_out, bias=qkv\\_bias)  \nself.out\\_proj = nn.Linear(d\\_out, d\\_out)  \nself.dropout = nn.Dropout(dropout)  \nself.register\\_buffer(\"mask\", torch.triu(torch.ones((context\\_length, context\\_length)) \\* float('-inf'), diagonal=1))  \n  \ndef forward(self, x):  \nb, num\\_tokens, d\\_in = x.shape  \nkeys = self.W\\_key(x)  \nqueries = self.W\\_query(x)  \nvalues = self.W\\_value(x)  \n  \nkeys = keys.view(b, num\\_tokens, self.num\\_heads, self.head\\_dim) # reshape for multi-head  \nqueries = queries.view(b, num\\_tokens, self.num\\_heads, self.head\\_dim)  \nvalues = values.view(b, num\\_tokens, self.num\\_heads, self.head\\_dim)  \n  \nkeys.transpose\\_(1, 2) # move head dimension to the front so that it is treated as batch dimension  \nqueries.transpose\\_(1, 2)  \nvalues.transpose\\_(1, 2)  \n  \nattn\\_scores = queries @ keys.transpose(2, 3) # flip last two dimensions for dot product  \nmask\\_bool = self.mask.bool()\\[:num\\_tokens, :num\\_tokens\\]  \nattn\\_scores.masked\\_fill\\_(mask\\_bool, -torch.inf)  \nattn\\_weights = torch.softmax(attn\\_scores / self.head\\_dim\\*\\*0.5, dim=-1)  \nattn\\_weights = self.dropout(attn\\_weights)  \ncontext\\_vec = (attn\\_weights @ values).transpose(1, 2).contiguous().view(b, num\\_tokens, self.d\\_out) # reshape back to original  \ncontext\\_vec = self.out\\_proj(context\\_vec) # final linear layer to mix heads  \nreturn context\\_vec",
          "score": 3,
          "created_utc": "2026-01-17 09:42:31",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o09fznv",
          "author": "chrisvdweth",
          "text": "Thanks for sharing. I have a few comments/questions:\n\n* Why does \\`attention.py\\` have so many classes? Some of them don't seem to be used meaningfully?\n* Not sure why you organized the code like that. For example, when importing \\`attention.py\\`, all the code gets executed. Why not do it cleaner cleaner with \\`if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":\\`?\n* Well, your \\`create\\_dataloader\\_v1\\` method seems to use a pretrained BPE tokenizer after all :). At least, I couldn't spot any code that trains a BPE tokenizer from scratch.",
          "score": 2,
          "created_utc": "2026-01-18 08:53:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0bjpd4",
              "author": "Bthreethree",
              "text": "Hey, the repo is an implementation of Sebastian Roschka's Build a LLM from scratch book, thus while learning and implementing from the book, I have made many classes which have improved further down in the file.\n\nYup I did forget to add \\`if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":\\` and will do that for sure! Thanks! :)",
              "score": 1,
              "created_utc": "2026-01-18 17:12:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o07h4dk",
          "author": "Bthreethree",
          "text": "I have added a colab notebook link in the readme of the repo on github to show the final results! The accuracy can be made better with experimentation of hyperparamaters & further fine-tuning.\n\n[https://github.com/Nikshaan/llm-from-scratch](https://github.com/Nikshaan/llm-from-scratch)",
          "score": 1,
          "created_utc": "2026-01-18 00:54:27",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o0b48se",
          "author": "Better_Pair_4608",
          "text": "How many parameters does your model have?",
          "score": 1,
          "created_utc": "2026-01-18 15:59:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0bjueb",
              "author": "Bthreethree",
              "text": "The model is trained over 124M parameters (inspired by GPT-2 architecture)",
              "score": 1,
              "created_utc": "2026-01-18 17:13:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0eta5p",
          "author": "Smergmerg432",
          "text": "This is awesome‚Äîthank you so much!",
          "score": 1,
          "created_utc": "2026-01-19 03:05:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qg28lm",
      "title": "Built a Multi-Source Knowledge Discovery API (arXiv, GitHub, YouTube, Kaggle) ‚Äî looking for feedback",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qg28lm/built_a_multisource_knowledge_discovery_api_arxiv/",
      "author": "Appropriate_West_879",
      "created_utc": "2026-01-18 07:29:50",
      "score": 38,
      "num_comments": 1,
      "upvote_ratio": 0.95,
      "text": "Hey everyone,\n\nI‚Äôve just finished building an open-source project called Knowledge Universe API and pushed it to GitHub.\n\nIt‚Äôs a FastAPI-based backend that discovers and ranks educational / technical resources from multiple sources in a single request.\n\nWhat it does (purely technical)\n\nParallel crawling from:\n\narXiv\n\nGitHub\n\nYouTube\n\nKaggle (API-based)\n\nUnified response schema across all sources\n\nQuality scoring pipeline (difficulty alignment, freshness, accessibility, social signals)\n\nRedis-based caching with TTL + background refresh\n\nAsync orchestration with timeout isolation per crawler\n\nDeduplication + diversity filtering\n\nDocker + local Redis support\n\nWhy I built it\n\nI wanted a single API that returns ranked, clean, structured learning resources instead of manually searching each platform.\n\nThis was mostly a backend / systems exercise:\n\nasync pipelines\n\ncrawling reliability\n\nscoring consistency\n\ncache correctness\n\nStack\n\nPython 3.11\n\nFastAPI\n\nhttpx / asyncio\n\nRedis\n\nDocker\n\nPydantic v2\n\nRepo\n\nüëâ GitHub: https://github.com/VLSiddarth/Knowledge-Universe.git\n\nWhat I‚Äôm looking for Open contribution to add new source collection from Internet to Create \"Knowledge Universe API\",\n\nCode review (especially async orchestration & scoring)\n\nArchitecture feedback\n\nAny obvious mistakes / improvements\n\nNot promoting anything ‚Äî just sharing what I built and learning from feedback.\n\nThanks üôè",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qg28lm/built_a_multisource_knowledge_discovery_api_arxiv/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o0flqt9",
          "author": "Appropriate_West_879",
          "text": "I could see More shares than Upvotes and Comments, Love to have contributions either as feature or financial help.\nThank you for your support üòäüôè",
          "score": 1,
          "created_utc": "2026-01-19 06:18:31",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfkal8",
      "title": "Which subfields of ML can I realistically achieve PhD level mastery of by self study at home with limited budget?",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qfkal8/which_subfields_of_ml_can_i_realistically_achieve/",
      "author": "Proof-Bed-6928",
      "created_utc": "2026-01-17 18:08:35",
      "score": 37,
      "num_comments": 38,
      "upvote_ratio": 0.72,
      "text": "Suppose you have somehow managed to generate 25k disposable income and only work 20hours a week so you have plenty of free time. You want to dedicate the remaining time to the mastery of one small but important ML niche just for the sake of it. To the level where you can theoretically waltz into a room full of FAANG level ML engineers and impress them with your contributions.\n\nIt will have to be a subfields where your competitive advantage plateaus with capital after some number (so not some compute arms race like LLM). \n\nWhich subfields in ML is this possible? What kind of benchmarks can you use to validate? How do you know you‚Äôve learned something without being in a university surrounded by academics?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qfkal8/which_subfields_of_ml_can_i_realistically_achieve/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o07bjun",
          "author": "notsofastaicoder",
          "text": "I think you are over estimating FAANG level ML engineers, and you don't need to a Phd to impress people either.\n\nSince most of these engineers do applied engineering, they are looking for solutions to current problems.\n\nMy experience has been in applied software engineering, worked in FAANG and AI startups. So advice from a non PhD guy:\n\nAfter you establish the base skills for ML, focus on an area that truly excites you, then make a meaningful contribution. Either by proving a hypothesis, or making an incremental improvement to existing solutions.\n\nBeing able to do that, in your own time, is really impressive.",
          "score": 62,
          "created_utc": "2026-01-18 00:24:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09evq5",
              "author": "belabacsijolvan",
              "text": "also [https://www.kaggle.com/competitions](https://www.kaggle.com/competitions)",
              "score": 5,
              "created_utc": "2026-01-18 08:43:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0dqr71",
              "author": "ChillmanITB",
              "text": "read an academic paper everyday, lots of books on theory, do all the free courses on calculus and linear algebra, probability and statistics (This is where you will develop an actual understanding), learn to code if you have not already and try and come up with new methods of improving current algorithms or create new ones all together.",
              "score": 3,
              "created_utc": "2026-01-18 23:38:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o06dtfy",
          "author": "williamkotoco_",
          "text": "anything related to efficient ml techniques such as pruning, quantization,  nas, efficient architectures and edge ai in general",
          "score": 19,
          "created_utc": "2026-01-17 21:32:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06slmz",
              "author": "Altruistic_Basis_69",
              "text": "My PhD was in DL optimization and I agree with this (though I am a little biased tbf). Compared to other niches, I‚Äôd argue that architecture optimization is one of the more intuitive areas to look into as opposed to some of the faster growing fields where you won‚Äôt even get a chance to catch-up.",
              "score": 9,
              "created_utc": "2026-01-17 22:45:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o06gpd8",
              "author": "Annual-Salamander-85",
              "text": "These are all pretty deep, why do you say that?",
              "score": 2,
              "created_utc": "2026-01-17 21:46:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o077s6k",
          "author": "Playful-Score-67",
          "text": "PhD level? It takes 5+ years of intense studying, independent research, mentoring from someone who is an expert on the field and contact with an academic community that can support and answer questions. It's not only about \"self study\".\nSo, realistically? Nah.",
          "score": 41,
          "created_utc": "2026-01-18 00:04:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o08wtuv",
              "author": "BreadBrowser",
              "text": "Well‚Ä¶ I don‚Äôt think I learned a single god damned thing from my advisor. Fuck that guy. Why I didn‚Äôt leave I‚Äôll never know. But yeah, you need a good five intense years.",
              "score": 4,
              "created_utc": "2026-01-18 06:04:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o09d6yl",
                  "author": "digiorno",
                  "text": "That‚Äôs a tragedy, if someone wants to be an advisor then they should take that role seriously and genuinely try to improve the next generation of researchers.",
                  "score": 3,
                  "created_utc": "2026-01-18 08:27:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o08nsh8",
              "author": "arihoenig",
              "text": "Nonsense, Faraday was self taught as was Galileo and Joule. All of their contributions are PhD level in the fields in which they contributed.",
              "score": -5,
              "created_utc": "2026-01-18 04:57:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o08tzoo",
                  "author": "RatKnees",
                  "text": "Ah yes, people born 200+ years ago. \n\nI don't disagree that anyone can make a contribution, but listing people from pre-electricity feels disingenuous",
                  "score": 22,
                  "created_utc": "2026-01-18 05:42:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o07b6gd",
          "author": "nickpsecurity",
          "text": "Any of the older techniques that are still widely used in industry. Then, smaller NN's, GA's, and LLM's. Also, techniques for things like time series and tabukar data which are highly important in industry but get little press or investment like GPT. That's the answer to your title.\n\nFar as FAANG, this [person](https://www.trybackprop.com/blog/top_ml_learning_resources) shares what they studied to do something like that.",
          "score": 9,
          "created_utc": "2026-01-18 00:22:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0awr63",
              "author": "iamevpo",
              "text": "What is a GA here?",
              "score": 1,
              "created_utc": "2026-01-18 15:22:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0bde7w",
                  "author": "NightmareLogic420",
                  "text": "Genetic Algorithms, I believe",
                  "score": 3,
                  "created_utc": "2026-01-18 16:42:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0btu0u",
                  "author": "nickpsecurity",
                  "text": "Genetic algorithms. They call the broader field evolutionary algorithms because they claim to use a highly-simplifies version of its principles. It's an optimization technique capable of working with non-differentiable data without getting stuck in local minima or maxima. It has heavier computation, though.\n\nIIRC, the top methods are differentiable evolution (in SciPy, too) and evolution strategies. If you use GA's, try tournament selection with tournament size of 7. Coevolution, or fitness tests evolving with the population, historically outperformed static, fitness measures. Such methods can be combined with NN's (neuroevolution) to find architectures, weights, hyperparameters, etc.",
                  "score": 2,
                  "created_utc": "2026-01-18 18:00:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o06rxw3",
          "author": "seriousgourmetshit",
          "text": "I don't think its possible to reach the level you are thinking of tbh",
          "score": 27,
          "created_utc": "2026-01-17 22:42:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o08n6md",
              "author": "arihoenig",
              "text": "George Boole was an autodidact. LLMs are just large assemblies of the logic he proposed.",
              "score": 3,
              "created_utc": "2026-01-18 04:53:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0b1060",
                  "author": "jonsca",
                  "text": "Found the guy who knows nothing about nothing¬†",
                  "score": 5,
                  "created_utc": "2026-01-18 15:43:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o08ks4e",
          "author": "Complex_Medium_7125",
          "text": "a PhD teaches you how to do novel research, is that what you want to do?  \nif not, taking some grad courses in ml from stanford/berkeley/cmu gets you the foundations of solid ml engineer level\n\ncheck out [https://stanford-cs336.github.io/spring2025/](https://stanford-cs336.github.io/spring2025/) and do the homeworks",
          "score": 5,
          "created_utc": "2026-01-18 04:36:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06x83c",
          "author": "UngratefulSheeple",
          "text": "That‚Äôs not gonna happen bro üòÇüòÇ\n\nSincerely, someone doing a phd in ai right now.",
          "score": 7,
          "created_utc": "2026-01-17 23:09:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o09cfu5",
          "author": "digiorno",
          "text": "For actual ML research and not just application, you need math skills. And sure those can be self taught but it could be incredibly difficult to get good at without the guidance of professors. \n\nSecondly for cutting edge stuff, a lot of that so done via collaborative efforts with other theorists in an academic setting.\n\nIf you get good enough at the math and have some novel ideas then a school or research would likely give you admittance to a PhD program so that you could work with them, learn from them and join their ranks. And they would pay you a bit more than the amount you mentioned.",
          "score": 2,
          "created_utc": "2026-01-18 08:20:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0e9h02",
          "author": "mpaes98",
          "text": "I‚Äôll be real with you, a PhD is basically self studying while also having to juggle 1-2 exploitative jobs where you report to different tiers of narcissistic individuals who have no proper management experience, and don‚Äôt even get me started on how bad teaching duties have gotten (overbloated classes, unprepared undergrads/grads).\n\nI had a decent experience, but it‚Äôs a rough time and now we‚Äôre in a terrible market.",
          "score": 2,
          "created_utc": "2026-01-19 01:18:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o07fihm",
          "author": "strangeanswers",
          "text": "I‚Äôd argue that this is possible in the operationalization of LLMs to address business problems such as automated customer support, sentiment scraping or any other forms of information retrieval and programmatic decision-making. \n\nyou need a firm grasp of how LLMs work, strong software engineering technicals, LLMOps/DevOps, prompt & context engineering skills and good product reasoning. each of these skills is difficult to develop but I‚Äôd argue that it‚Äôs feasible to become elite in this problem space through lots of self study and practice and very solid applied projects where you actually develop and deploy cloud infrastructure that can handle high throughput.",
          "score": 2,
          "created_utc": "2026-01-18 00:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05bvvp",
          "author": "tewmuchdrama",
          "text": "Reinforcement Learning",
          "score": -8,
          "created_utc": "2026-01-17 18:25:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o080svj",
              "author": "jsh_",
              "text": "?",
              "score": 1,
              "created_utc": "2026-01-18 02:39:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o05udrd",
              "author": "pb_syr",
              "text": "Why",
              "score": 0,
              "created_utc": "2026-01-17 19:52:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qgod93",
      "title": "What is it really like to work as an ML/AI engineer?",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qgod93/what_is_it_really_like_to_work_as_an_mlai_engineer/",
      "author": "Ana_Karen98",
      "created_utc": "2026-01-18 23:53:20",
      "score": 36,
      "num_comments": 9,
      "upvote_ratio": 0.95,
      "text": "I graduated from university a couple of months ago. Since 2024, I've been working at a startup as a software development intern, and almost a year ago I was promoted to Junior ML/AI.\n\nI have two questions. First, why haven't I been working for months? I'm still getting paid because it's a small startup, and the person in charge of me is always busy, so no matter how many projects I ask or how much they promise me, I haven't received any since august. Supposedly, we're supposed to have our first in-person meeting on Monday after almost two years working there.\n\nIn the few projects I've worked on, my boss saw potential in me for AI/ML, but since I started university, I've always planned to work in web development, so my actual knowledge of AI/ML is limited, and it wasn't even something I had considered working in.\n\nI recently got access to a Udemy account and even bought some O'Reilly books on Humble Bundle. Is that enough? Is there a practical roadmap?I don't expect to learn it all in just a few months or week, but I do want to start exploring this field. I want to know what to expect and what skills are most in demand for junior professionals these days.\n\nI also hope to be able to change jobs eventually because, although this is a comfortable job, I want to advance and learn in my career. Unfortunately, in my contry there aren't many opportunities for entry-level positions, only for more advanced engineers (I'm not from the USA).\n\nI really want to learn because I HATE doing things poorly or half-heartedly, and I also don't want to pass up the opportunity to learn in this area even though it wasn't what I was looking for.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qgod93/what_is_it_really_like_to_work_as_an_mlai_engineer/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o0dukhh",
          "author": "MRgabbar",
          "text": "move and clean data mostly, frameworks take care of mostly everything",
          "score": 24,
          "created_utc": "2026-01-18 23:58:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0entjk",
              "author": "chrisvdweth",
              "text": "This what I often tell my students...thanks for the confirmation :).",
              "score": 2,
              "created_utc": "2026-01-19 02:37:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ffw4f",
          "author": "AccordingWeight6019",
          "text": "what you‚Äôre describing is unfortunately pretty common at small startups, especially when there isn‚Äôt a clear technical roadmap or dedicated mentorship. being titled ‚ÄúML‚Äù often just means you‚Äôre nearby when someone thinks AI might be useful, not that there‚Äôs a steady stream of well-defined work. courses and books are a good start, but they only help if you pair them with concrete projects, even small ones you define yourself. for junior roles, the most valuable skills tend to be fundamentals, data handling, and the ability to ship something end to end, not cutting edge models. if you treat this period as time to build real examples of work rather than waiting for assignments, it will translate much better when you look elsewhere.",
          "score": 5,
          "created_utc": "2026-01-19 05:32:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0dxfpq",
          "author": "LeMalteseSailor",
          "text": "Data / ml / ai engineers have tons of overlap, so it depends on the job description. Some ml engineers are data engineers with some backend exp. Some ml engineers are backend engineers either some data experience.",
          "score": 6,
          "created_utc": "2026-01-19 00:13:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0fh1b8",
          "author": "patternpeeker",
          "text": "A lot of early ‚ÄúML/AI‚Äù roles feel like this, especially at small startups where there is no real roadmap or data maturity. In practice, most ML engineers spend far more time waiting on data, clarifying vague goals, or being blocked by infra and priorities than training models. That is often why you are idle, not because you are doing something wrong. Courses and books are useful, but they only stick once you are trying to solve a concrete problem end to end, even a small one, with messy data and unclear success criteria. The skills that actually matter early on are solid software fundamentals, data handling, and understanding why a model breaks in production, not knowing every algorithm. If your current role is not giving you projects, I would treat it as paid time to build small applied projects and learn the tooling, while being realistic that many ‚Äújunior ML‚Äù titles are closer to generic dev roles with an AI label.",
          "score": 3,
          "created_utc": "2026-01-19 05:41:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0fvyib",
          "author": "i_would_say_so",
          "text": "Sleep deprived",
          "score": 3,
          "created_utc": "2026-01-19 07:45:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g51fl",
          "author": "the_rat_from_endgame",
          "text": "there is NO clean roadmap say like in FrontEnd/Backend (like pick a framework and learn it)\n\nBe sure to know your supervised/unsupervised learning/hyperparameter tuning. then say data extraction is something you might learn on the job BUT won't hurt to know SQL (some window functions, grouping sets, aggregates, apart from the basics). Spark would be great too. I was a late bloomer when it came to spark and tbh, I know very little beyond the basics and I am aggressively relying on chatgpt for it (I do know Pandas a decent bit cause despite everything I feel comfy working the pipeline out in a notebook, reading a parquet/csv, the skeleton of the feature engineering, transfomrations, trainining, testing,persistence etc)\n\n\nAlso learn Gradient Boosting I ended up relying more on it than it I would have thought. I am working with some time series stuff (Bit new to it tbh) but trying out Gradient boosting there too (isolation forest) besides some other stuff (prophet for forecasting).\n\nAlso MLFlow, BEST to know of it, although I have been mostly using it as a boilerplate type thing.\n\n\nAnd above all, Try to write Good reusable CODE>",
          "score": 2,
          "created_utc": "2026-01-19 09:09:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0irug5",
          "author": "Keith_35",
          "text": "working in ML/AI can be a mix of excitement and frustration, as many spend a lot of time on data prep and dealing with unclear project goals rather than just model training.",
          "score": 1,
          "created_utc": "2026-01-19 18:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mqwec",
          "author": "Broad-Entrance5489",
          "text": "In my case I do a lot of infrastructure and deployment work. That means aws, terraform, CICD, kubernetes, etc. But I also do some normal data science: algorithms, models, data processing, etc.\n\nIt really depends by company, but I think you can think of an ml engineer as a mix between data scientist and cloud solutions architect? Let me know if you think otherwise",
          "score": 1,
          "created_utc": "2026-01-20 07:54:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qixrtx",
      "title": "If you had to learn AI/LLMs from scratch again, what would you focus on first?",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qixrtx/if_you_had_to_learn_aillms_from_scratch_again/",
      "author": "EngineerLoose5042",
      "created_utc": "2026-01-21 13:23:34",
      "score": 33,
      "num_comments": 11,
      "upvote_ratio": 0.97,
      "text": "I‚Äôm a web developer with about two years of experience. I recently quit my job and decided to spend the next 15 months seriously upskilling to land an AI/LLM role ‚Äî focused on building real products, not academic research.  \nIf you already have experience in this field, I‚Äôd really appreciate your advice on what I should start learning first. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qixrtx/if_you_had_to_learn_aillms_from_scratch_again/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o0uq03x",
          "author": "frivoflava29",
          "text": "Probabilities and statistics. I also wouldn't get into AI. You will find most people here are into machine learning (which has been studied for many decades) and not so much the current trend with transformers.",
          "score": 14,
          "created_utc": "2026-01-21 13:42:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0uy85r",
              "author": "3n91n33r",
              "text": "Any recommended resources on those?",
              "score": 1,
              "created_utc": "2026-01-21 14:26:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0v2rcv",
                  "author": "frivoflava29",
                  "text": "Geron's PyTorch book",
                  "score": 8,
                  "created_utc": "2026-01-21 14:49:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0v4wjh",
                  "author": "Annual-Beginning-352",
                  "text": "I'm a maths and stats student in Uni, our intro to machine learning started with simple linear regression before moving onto more complicated regression techniques like logarithmic. We also worked a bit with classification techniques like decision trees and random forests. Be warned regression is very limited when compared to the type of operations neural nets can do since, in regression you have to specify all the relationships between variables. But I think it can be a good jumping off point if you want to see what it means for a computer to come up with its own weights that relate the training data to the variable of interest.",
                  "score": 3,
                  "created_utc": "2026-01-21 14:59:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0vijcw",
          "author": "Vedranation",
          "text": "I graduated as Robotics engineer so I had some AI background, but it didn't really prepare me for how diverse it's in production compared to uni coursework. \n\nHonestly unpopular opinion but I'd have spent more time figuring out exactly why things do what they do and how. Like instead of jist writing Conv2D figuring out what a filter or kernel is, how they actually work, math behind SDG etc. I found myself in a lot of soft pits struggling with improving model performance because I didn't know stuff like that, like what BatchNorm actually does instead of automatically applying it after Conv2D because that's how I was taught. \n\nOh also Seq2Seq and RAG. Nobody told me I'd be doing so much RAG.",
          "score": 2,
          "created_utc": "2026-01-21 16:02:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0uqvob",
          "author": "QuiteMischief",
          "text": "Start with Transformers, then gradually move into Generative AI. Once you have that foundation, deep dive into RAG, how LLMs work, LLM fine-tuning, and agentic systems, and then explore the latest frameworks like A2A and MCP.\n\nThe key is to start from one end - once you begin, you‚Äôll naturally understand what you need to learn next. What matters most is starting now.",
          "score": 2,
          "created_utc": "2026-01-21 13:47:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0v9ecu",
              "author": "Imaginary_Tower_5518",
              "text": "Do you have any recommendations for books or content?",
              "score": 1,
              "created_utc": "2026-01-21 15:21:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0veqr6",
                  "author": "QuiteMischief",
                  "text": "Give this a read -AI Engineering by Chip Huyen",
                  "score": 1,
                  "created_utc": "2026-01-21 15:45:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0vbau9",
              "author": "unknowntrail20",
              "text": "Hey l need your help check your DMs",
              "score": 0,
              "created_utc": "2026-01-21 15:30:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0vo0dt",
          "author": "thinking_byte",
          "text": "I would start by building end to end things before going deep on theory. Get comfortable with data in, model out, and something users actually touch. A lot of people over index on model internals early and never learn where things break in practice. Focus on prompting, retrieval, evals, and failure modes first because that is where real products live right now. You can always go deeper on training and architecture later once you know why you need it. The fastest signal for roles is showing you can ship something imperfect and iterate.",
          "score": 1,
          "created_utc": "2026-01-21 16:27:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhzcwv",
      "title": "First ML interview",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qhzcwv/first_ml_interview/",
      "author": "livsh12345",
      "created_utc": "2026-01-20 12:06:06",
      "score": 24,
      "num_comments": 12,
      "upvote_ratio": 0.94,
      "text": "Hi,\n\nI‚Äôd really appreciate any advice as I feel like I‚Äôm going into this experience alone! \n\nI have an interview for a graduate role MLE position. The structure I‚Äôve been told is 1h discussion of my hackerrank submission (I had to essentially create an ML pipeline to identify fraudulent data) and then 1h ‚ÄúML generalist‚Äù interview.\n\nI‚Äôm really not sure what to expect. Also I‚Äôm a little nervous as I don‚Äôt come from a formal ML background (although this was the focus of an internship and my final year masters project so I‚Äôm familiar with what I‚Äôve worked with) but my worry is I may have missed some fundamental concepts due to the fact I learnt as I went when doing my projects (both very deep learning focussed). Currently working through Andrew Ngs courses on coursera and it doesn‚Äôt seem too alien so I guess that‚Äôs a good sign!?\n\nAny advice would be much appreciated.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qhzcwv/first_ml_interview/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o0nls6c",
          "author": "Boom_Boom_Kids",
          "text": "For the Hackerrank discussion, expect them to ask why you made certain choices, data cleaning, features, model choice, evaluation metrics, trade-offs, and what you‚Äôd improve with more time. Be honest about limits and show clear thinking.\n\nFor the ML generalist round, it‚Äôs usually fundamentals, bias vs variance, overfitting, train/validation/test splits, common metrics, basic models, and how you‚Äôd debug a bad model. They care more about reasoning than memorized formulas.\n\nNot having a formal ML degree is fine. Your internship and project matter more. If Andrew Ng‚Äôs course feels familiar, you‚Äôre in a good place. Focus on explaining concepts clearly and tying them back to your own work.",
          "score": 9,
          "created_utc": "2026-01-20 12:25:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nm3x5",
              "author": "livsh12345",
              "text": "That‚Äôs really helpful to know, thanks.",
              "score": 2,
              "created_utc": "2026-01-20 12:27:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0o8daf",
          "author": "AdInevitable161",
          "text": "Could you share the task they gave you, that would be really helpful for others that do interviews!",
          "score": 3,
          "created_utc": "2026-01-20 14:37:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ona8g",
          "author": "KitchenTaste7229",
          "text": "I suggest that for the HackerRank part, follow the structure of walking them through your code ->  explaining your design choices (why you chose that particular ML pipeline) -> discussing any tradeoffs you made. Remember that they're probably more interested in your thought process than the perfect solution.\n\nFor the \"ML generalist\" part, brush up on the basics like different ML algorithms (linear regression, logistic regression, SVM, decision trees, etc.), bias-variance tradeoff, regularization techniques, and evaluation metrics (precision, recall, F1-score, etc.). Honestly, knowing the fundamentals is key even if you've mostly worked with deep learning. Make sure to also brush up on real-world applications of ML and how you would approach a specific business problem using it. There's a ton of [ML interview questions](https://www.interviewquery.com/playlists/ml-engineering-50) resources online if you want to get used to how companies specifically evaluate you, like the one I've linked which covers everything from algorithms to evaluation. Good luck!",
          "score": 3,
          "created_utc": "2026-01-20 15:50:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0onx7i",
              "author": "livsh12345",
              "text": "Thanks for the advice! Much appreciated.",
              "score": 1,
              "created_utc": "2026-01-20 15:53:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0rebxj",
          "author": "IndependenceThen7898",
          "text": "Hey maybe not that relevant, but could you tell me in what you did your master ?",
          "score": 1,
          "created_utc": "2026-01-20 23:36:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0reqwn",
              "author": "livsh12345",
              "text": "Hi, I did Physics as an integrated masters course :)",
              "score": 1,
              "created_utc": "2026-01-20 23:38:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0riln8",
                  "author": "IndependenceThen7898",
                  "text": "interesting :), thanks and good luck with your interview",
                  "score": 1,
                  "created_utc": "2026-01-20 23:59:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0tm6gl",
          "author": "Ausartak93",
          "text": "The ML generalist part will probably cover basics like bias-variance tradeoff, overfitting, different model types and when to use them, evaluation metrics. Your pipeline discussion will be more important since they can see how you actually think through problems.",
          "score": 1,
          "created_utc": "2026-01-21 08:27:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tmc8c",
              "author": "livsh12345",
              "text": "I see, thanks for the advice! Looking back on my submission there‚Äôs quite a bit I could have done with more resource/time so I‚Äôm guessing that will be a good talking point?",
              "score": 1,
              "created_utc": "2026-01-21 08:28:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0tsel6",
          "author": "DataCamp",
          "text": "For the HackerRank review: they‚Äôll mostly want to hear how you think, not whether you picked the ‚Äúperfect‚Äù model. Maybe walk them through it like a story:\n\n* what you understood the problem to be (and what‚Äôs worse here: false positives or false negatives?)\n* how you checked/cleaned the data (missing values, weird outliers, duplicates, leakage)\n* why you chose the model you chose (and what you tried before it)\n* how you evaluated it (and why that metric made sense for fraud)\n* what you‚Äôd improve if you had more time (this is actually a great talking point)\n\nFraud data is usually imbalanced, so if you did anything around class weights / sampling / threshold tuning, bring that up. Even just saying ‚ÄúI used PR-AUC / focused on recall because‚Ä¶‚Äù is a good signal.\n\nFor the ‚ÄúML generalist‚Äù hour: it‚Äôs usually fundamentals + debugging mindset. Things like:\n\n* bias vs variance / overfitting\n* train/val/test splits + leakage\n* evaluation metrics (esp. precision/recall/F1, ROC vs PR)\n* how you‚Äôd diagnose a model that‚Äôs doing badly (data quality? label noise? leakage? feature issues?)\n\nAlso, since you said your experience is more deep-learning focused: try to make sure you can comfortably explain the basics of ‚Äúboring‚Äù models too (logistic regression, trees/GBMs, regularization). In a lot of real interview loops, they love candidates who don‚Äôt jump straight to neural nets for tabular problems.\n\nIf Andrew Ng feels familiar, that‚Äôs honestly a good sign. The big win is being able to explain your choices clearly and talk about trade-offs without panicking.",
          "score": 1,
          "created_utc": "2026-01-21 09:27:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vh6e4",
              "author": "livsh12345",
              "text": "Thanks, solid advice! Also, big fan of DataCamp üëç",
              "score": 1,
              "created_utc": "2026-01-21 15:56:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qdp54a",
      "title": "From Notebook to Production: A 3-Month Data Engineering Roadmap for ML Engineers on GCP",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qdp54a/from_notebook_to_production_a_3month_data/",
      "author": "IT_Certguru",
      "created_utc": "2026-01-15 17:03:59",
      "score": 21,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "I spent the last 6 months learning how to productionize ML models on Google Cloud. I realized many of us (myself included) get stuck in \"Jupyter Notebook Purgatory.\" Here is the complete roadmap I used to learn Data Engineering specifically for ML.\n\nPhase 1: The Foundation (Weeks 1-4)\n\n* Identity & Access (IAM): Why your permissions always fail and how to fix them.\n* Compute Engine vs. Cloud Run: When to use which for serving models.\n\nPhase 2: The Data Pipeline (Weeks 5-8)\n\n* BigQuery: It's not just for SQL. Using BQML (BigQuery ML) to train models without moving data.\n* Dataflow (Apache Beam): Real-time data processing.\n* Project Idea: Build a pipeline that ingests live crypto/stock data -> Pub/Sub -> Dataflow -> BigQuery.\n\nPhase 3: Orchestration & MLOps (Weeks 9-12)\n\n* Cloud Composer (Airflow): Scheduling your retraining jobs.\n* Vertex AI: The holy grail. Managing feature stores and model registry.\n\nIf anyone wants a more structured path for the data engineering side, this course helped me connect a lot of the dots from notebooks to production: [Data Engineering on Google Cloud](https://www.netcomlearning.com/course/data-engineering-on-google-cloud)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qdp54a/from_notebook_to_production_a_3month_data/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qgr6e3",
      "title": "I published a full free book on freeCodeCamp: \"The Math Behind Artificial Intelligence\"",
      "subreddit": "learnmachinelearning",
      "url": "/r/FreeCodeCamp/comments/1qgr5e4/i_published_a_full_free_book_on_freecodecamp_the/",
      "author": "Last-Risk-9615",
      "created_utc": "2026-01-19 01:59:11",
      "score": 21,
      "num_comments": 4,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qgr6e3/i_published_a_full_free_book_on_freecodecamp_the/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "o0enck4",
          "author": "chrisvdweth",
          "text": "Thanks for sharing!",
          "score": 1,
          "created_utc": "2026-01-19 02:35:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0epq8u",
              "author": "Last-Risk-9615",
              "text": "Sure!!",
              "score": 1,
              "created_utc": "2026-01-19 02:46:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ev2pk",
          "author": "affides",
          "text": "Very interesting book indeed and I like the way it is structured. I have read till chapter 3. Thank you very much for sharing",
          "score": 1,
          "created_utc": "2026-01-19 03:15:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0fg0pw",
              "author": "Last-Risk-9615",
              "text": "Sure!",
              "score": 1,
              "created_utc": "2026-01-19 05:33:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}