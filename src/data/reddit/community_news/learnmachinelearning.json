{
  "metadata": {
    "last_updated": "2026-01-22 16:59:57",
    "time_filter": "week",
    "subreddit": "learnmachinelearning",
    "total_items": 20,
    "total_comments": 143,
    "file_size_bytes": 183206
  },
  "items": [
    {
      "id": "1qg6q1w",
      "title": "I implemented a VAE in Pure C for Minecraft Items",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/gallery/1qg6q1w",
      "author": "Boliye",
      "created_utc": "2026-01-18 11:53:54",
      "score": 282,
      "num_comments": 34,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qg6q1w/i_implemented_a_vae_in_pure_c_for_minecraft_items/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0a2iu3",
          "author": "Cybyss",
          "text": "Damn. That is really cool! \n\nYou basically reinvented your own PyTorch from scratch in plain C and used that to create your own variational autoencoder? Ambitious. I also love the creativity of training on Minecraft images instead of the usual MNIST or CIFAR. \n\nWell done!",
          "score": 32,
          "created_utc": "2026-01-18 12:17:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a36ye",
              "author": "Boliye",
              "text": "Thank you!",
              "score": 4,
              "created_utc": "2026-01-18 12:22:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0a1xnh",
          "author": "Palmquistador",
          "text": "I donâ€™t think I understand. You created your own image generator specific to Minecraft images?",
          "score": 24,
          "created_utc": "2026-01-18 12:12:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a35l0",
              "author": "Boliye",
              "text": "Yeah, sort of! A VAE is a type of network that can be used for image generation. And I created and trained one of these with Minecraft images. But as a VAE is also an autoencoder, something you can also do is play with the embeddings and ask the network stuff like \"what's at the middle point between this and that?\" \"What would happen if you took this and subtracted that?\". If the network was successful in learning meaningful concepts, these averages won't be nonsense, and stuff like what I show in the images will happen.",
              "score": 35,
              "created_utc": "2026-01-18 12:22:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0acap7",
                  "author": "LumpyWelds",
                  "text": "Is this like word2vec?\n\n\"dog\" - \"puppy\" + \"kitten\" = \"cat\"",
                  "score": 20,
                  "created_utc": "2026-01-18 13:28:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0f0ys1",
              "author": "irekit_",
              "text": "A variational auto-encoder is something that encodes data like images into the latent space, it uses probabilities and outputs a gaussian distribution of where the data is likely to be in the latent space.",
              "score": 3,
              "created_utc": "2026-01-19 03:50:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0f29ye",
                  "author": "Palmquistador",
                  "text": "So like RAG? But the VAE is just one step in the image generation process, right? Thatâ€™s what ComfyUI implies with their connectors at least.",
                  "score": 1,
                  "created_utc": "2026-01-19 03:59:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0a6v1n",
          "author": "gocurl",
          "text": "Doing the \"- concept 1 + concept 2\" and having a relevant result is a very cool way to confirm your model understood key concept. Very well done!\nOut of curiositÃ©, in that \"- x + x\" step, what is the input you provides? Do you start from the middle layer?",
          "score": 11,
          "created_utc": "2026-01-18 12:50:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a95uo",
              "author": "Boliye",
              "text": "Yeah I use the encoder and decoder parts separately:  \n\nEncode iron\\_chestplate -> Latent for iron\\_chestplate  \n\nEncode all items that contain 'iron' in their name and average them out -> Latent for iron concept  \n\nEncode all items that contain 'diamond' in their name and average them out -> Latent for diamond concept  \n\nLiterally do the operation \"Latent for iron\\_chestplate\" - \"Latent for iron concept\" + \"Latent for diamond concept\"  \n\nFinally, pass this result to the decoder (second half of the network).",
              "score": 13,
              "created_utc": "2026-01-18 13:07:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0arh78",
                  "author": "gocurl",
                  "text": "It seems so clear now youâ€™ve said it, but I've never thought about it this way, thanks!",
                  "score": 3,
                  "created_utc": "2026-01-18 14:55:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0a81f9",
          "author": "JanBitesTheDust",
          "text": "Very cool idea! How did you find the latent vector for the concept of iron? Is it just averaging latent vectors for all iron related minecraft textures?",
          "score": 9,
          "created_utc": "2026-01-18 12:59:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a8q6v",
              "author": "Boliye",
              "text": "Yes! it is just the average",
              "score": 6,
              "created_utc": "2026-01-18 13:04:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0dtqkf",
                  "author": "possiblyquestionabl3",
                  "text": "This is a pretty cool trick!\n\nA couple of assumptions:\n\n1. your set of iron related items form some concept vector space like `v_{iron_sword} \\approx v_{iron} + v_{sword}`\n2. it's sufficiently large in dimension, so that different concepts tend to be naturally orthogonal (e.g. v\\_{sword} and v\\_{bar} are nearly orthogonal to each other)\n\nlet `z_{iron}` be the mean vector over all iron related vectors, then\n\n    z_{iron} = 1/N \\sum_{type} v_{iron type} = v_{iron} + 1/N \\sum_{type} v_{type}\n\nif N, the number of distinct iron X items, is large enough, and if we assume v_{type} are generally orthogonal to each other, then `\\sum_{type} v_{type}` can be seen as an isotropic ball of noise with a spherical radius of `||v_{type}|| * sqrt(N)`. As a result, you're essentially computing\n\n    z_{iron} = v_{iron} + noise ||v_{type}||/sqrt(N)\n\nnormalizing v, you're basically computing the actual concept vector for `v_{iron}`, plus noise of O(n^(-1/2)). E.g. if you can provide 100 `iron X` samples, your noise goes down to ~10%.\n\nIf structurally, iron and diamond items have the exact same types (e.g. swords, bars, etc), then the noise term will be structurally similar (same general direction), so `- z_{iron} + z_{diamond}` should be able to cancel the noise out almost perfectly. However, adding/subtracting concepts together without cancellation will amplify the noise, and categories with low # of types to average will have proportionally sqrt(N) times more noise.\n\nIt might be worthwhile to do a few power-iterations on the 2nd-moment operator `M = 1/N \\sum_{type} v_{iron type} v_{iron type}^T` using the mean `z_{iron}` direction to get to the real `v_{iron}` direction (probably converges in just 1-2 steps since the mean `z_{iron}` is already dominated in the `v_{iron}` direction). It's fairly cheap and would allow you to avoid the sqrt(N) noise term.",
                  "score": 3,
                  "created_utc": "2026-01-18 23:53:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0aus8u",
          "author": "ToSAhri",
          "text": "To what extent did this latent arithmetic operation rely on the convenience of the iron and diamond items being very similar (if not identical save for the color)?\n\nI guess Iâ€™m confused on how the latent arithmetic has inherent use of that method. If we only had a random sample of half of the iron items and half of the diamond items would it still work well? Cause then we could use it to generate diamond versions of iron pieces we donâ€™t have and vice versa.\n\nItâ€™s possible I just donâ€™t grasp the use case of VAEs in general and thatâ€™s where my confusion comes from.",
          "score": 5,
          "created_utc": "2026-01-18 15:12:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0b06wb",
              "author": "Boliye",
              "text": "You are right this is just a toy project, and the model is not that powerful to do anything really useful. If I ask it to generate new items, the quality is meh and I wouldn't be surprised if we did that experiment and asked it to generate something that did not exist in its training data, the VAE would struggle to generate something out of distribution like that. I am definetly making the task easier by trying to generate something that I know was present in the training data.\n\nTake the second example (the one where we turn a gold horse armor to a golden shovel). The fact that the generated shovel is yellow, shows that somewhere in the latent space, color of the object is one of the compressed features it learned to be useful for reconstructing.\n\nUltimately, the essence of the latents is that they are just 32 numbers trying to compress the information of a 3\\*16\\*16 = 768 pixel image. So the VAE has to find the most useful high-level features that characterize each image.",
              "score": 3,
              "created_utc": "2026-01-18 15:39:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ahb3j",
          "author": "Anas0101",
          "text": "so cool",
          "score": 2,
          "created_utc": "2026-01-18 13:59:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0al3sb",
              "author": "Boliye",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-18 14:20:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0bkd64",
          "author": "Poleski69",
          "text": "Thats really cool! \n\nIt's a variational autoencoder, so what happens if you sample the latent space with a normal distribution? I know var autoencoders aren't the best at generation but I'm really curious to see what your implementation thinks the 'average' minecraft item is!",
          "score": 2,
          "created_utc": "2026-01-18 17:15:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0cl97x",
              "author": "Boliye",
              "text": "Here are you go:  \n[https://imgur.com/a/TNYgQ32](https://imgur.com/a/TNYgQ32)\n\nFor convinience, for these extra generations I didn't use the C VAE, but the proof of concept in Python that I also created (it can be found in the folder poc\\_python in the git repo). It implements the exact same architecture and achieves the same loss.\n\nIn addition to \"the average item\", and some generations, I also added interpolations between a few items like a diamond chestplate turning into a diamond sword.",
              "score": 1,
              "created_utc": "2026-01-18 20:09:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0kqbj5",
                  "author": "Poleski69",
                  "text": "Super cool!\n\nAm I understanding correctly that the top image is the output of a latent tensor filled with zeros, and the ones under are from normally distributed \"noise\" latents (torch.randn ..)?\n\nInterpolations are really cool too, makes me wonder how hard it would be to implement latent ddpm in pure c and fully make use of the autoencoder.",
                  "score": 1,
                  "created_utc": "2026-01-20 00:13:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0dszkw",
          "author": "ami98",
          "text": "Super cool! Your code is very easy to understand, and it's neat that your python proof of concept matches your C results!\n\nBy any chance, do you know of a good textbook that discusses the mathematics behind these algorithms? Thanks:)",
          "score": 2,
          "created_utc": "2026-01-18 23:49:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hun4t",
              "author": "Boliye",
              "text": "Thank you! Yeah at first I was sure there would be some kind of difference I wouldn't be able to find between the trained PyTorch model and my C implementation, I was pretty happy when the numbers matched up :)\n\nI honestly don't feel informed enough to recommend good textbooks. I personally enjoyed a lot these lectures on youtube about \"Deep Generative Models\" from Stanford [https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8](https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8) . It does cover the mathematics behind it to a degree I find satisfying. But yeah, if you prefer textbooks, I can't really advise any better than other students.",
              "score": 1,
              "created_utc": "2026-01-19 16:01:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0e7fqu",
          "author": "Fickle_Lettuce_2547",
          "text": "How long have you used C to be able to make something like this??",
          "score": 2,
          "created_utc": "2026-01-19 01:06:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hzdp8",
              "author": "Boliye",
              "text": "C is a surprisingly simple language really! It has very few keywords and features. What is tricky and it will take some studying if you have never seen it before is memory management. Memory management can become a mess if you aren't thoughtful about it.\n\nLuckily, for this project the memory management is quite simple. So the code is not that different from programing in any other language.\n\nAlso, you need to be careful about bugs and test things as you go. For many mistakes you could make, the code could keep running in invalid states and do who-knows-what. Ultimately crashing without giving any insight on where's the bug.",
              "score": 2,
              "created_utc": "2026-01-19 16:22:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0h9kcq",
          "author": "arsenic-ofc",
          "text": "cool, i love VAEs, helped me do similar stuff with dance videos.",
          "score": 2,
          "created_utc": "2026-01-19 14:19:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0r01np",
          "author": "Scary-Opportunity848",
          "text": "Really cool stuff. For the latent space for iron you may be able to run svd over the matrix of all iron vectors. And instead of taking the average, take the top eigen values/vectors and whatever vector that creates. It should hold more iron since it is meant to capture the most common/consistent aspects across all of that batch. Really cool work btw",
          "score": 2,
          "created_utc": "2026-01-20 22:21:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mcgex",
          "author": "MeticulousBioluminid",
          "text": "sick",
          "score": 1,
          "created_utc": "2026-01-20 05:52:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qnzjw",
          "author": "Lupoferrin",
          "text": "This is a fascinating project! Building a VAE from scratch in C is a deep dive into the fundamentals. It highlights how understanding core algorithms can lead to innovative applications, even in areas like game asset generation.",
          "score": 1,
          "created_utc": "2026-01-20 21:24:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qf6l1j",
      "title": "Iâ€™m working on an animated series to visualize the math behind Machine Learning (Manim)",
      "subreddit": "learnmachinelearning",
      "url": "https://v.redd.it/cnezsdqyzudg1",
      "author": "No_Skill_8393",
      "created_utc": "2026-01-17 07:08:36",
      "score": 251,
      "num_comments": 21,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qf6l1j/im_working_on_an_animated_series_to_visualize_the/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o02kh04",
          "author": "314159267",
          "text": "Very cool. Like the visual. Nicely done, looking forward for more.",
          "score": 10,
          "created_utc": "2026-01-17 07:48:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02mp88",
              "author": "No_Skill_8393",
              "text": "Thank you. The math animations and video editting is quite new to me and I'm learning and as I do. Hope you guys have patience with me as I improve.",
              "score": 4,
              "created_utc": "2026-01-17 08:08:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o02rool",
          "author": "Dramatic_Yam8355",
          "text": "I hope you donâ€™t stop uploading videosâ€”please keep going.",
          "score": 4,
          "created_utc": "2026-01-17 08:55:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02y50b",
              "author": "No_Skill_8393",
              "text": "I'm already working on Episode 1 and It's receiving more effort and animation budget than Episode 0.\n\nStay in touch :D",
              "score": 3,
              "created_utc": "2026-01-17 09:56:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o02o1tm",
          "author": "tandir_boy",
          "text": "Cool idea. Also chech out Karpathy's [visualization](https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)",
          "score": 4,
          "created_utc": "2026-01-17 08:21:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02ochr",
              "author": "No_Skill_8393",
              "text": "Thank you. I'm also learning from Karpathy [https://karpathy.ai/zero-to-hero.html](https://karpathy.ai/zero-to-hero.html) \n\nGreat course. I learned alot from building micrograd. Excellent stuff to learn ML from scratch",
              "score": 3,
              "created_utc": "2026-01-17 08:24:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o031t4c",
          "author": "Image_Similar",
          "text": "manim is a boon to all of science",
          "score": 2,
          "created_utc": "2026-01-17 10:31:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03392n",
              "author": "No_Skill_8393",
              "text": "It is. God bless 3B1B and his splendid works. Really introduced me into Manim.",
              "score": 2,
              "created_utc": "2026-01-17 10:44:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o03p7z8",
          "author": "kindr_7000",
          "text": "Congrats, keep going.",
          "score": 2,
          "created_utc": "2026-01-17 13:36:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o033lla",
          "author": "Obvious-Shine-3573",
          "text": "hey, this is really interesting! I've tried some stuff with manim myself, could you share where you're learning from?",
          "score": 1,
          "created_utc": "2026-01-17 10:47:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0342a0",
              "author": "No_Skill_8393",
              "text": "Hi, I use https://docs.manim.community/en/stable/examples.html\n\nr/manim and alot of slamming my head against chatgpt lol",
              "score": 2,
              "created_utc": "2026-01-17 10:51:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o04hjqb",
          "author": "NightmareLogic420",
          "text": "Content seems great! The audio voiceover has something weird and off about it though.",
          "score": 1,
          "created_utc": "2026-01-17 16:04:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o04hqzk",
              "author": "No_Skill_8393",
              "text": "Im considering moving to elevenlabs for better audio :)\n\nSorry for using AI voice im really not confident with my voice.",
              "score": 2,
              "created_utc": "2026-01-17 16:05:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o04irxw",
                  "author": "NightmareLogic420",
                  "text": "Understandable choice, but just one man's opinion, human voice goes a long way to keep people locked in for longer, even if you aren't the best orator in the world, AI voice fatigues the ear faster. Especially if you're just worried about accent, accent is no big deal imo.",
                  "score": 4,
                  "created_utc": "2026-01-17 16:09:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o05k4q8",
          "author": "herooffjustice",
          "text": "Nice, keep going.  \nI'm doing something similar btw: [Link](https://www.reddit.com/r/learnmachinelearning/comments/1q3y6m5/ml_intuition_004_multilinear_regression/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",
          "score": 1,
          "created_utc": "2026-01-17 19:03:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05skzk",
              "author": "No_Skill_8393",
              "text": "Awesome. More Manim enthusiast!",
              "score": 2,
              "created_utc": "2026-01-17 19:44:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o05x8p7",
          "author": "Sea-Lettuce-9635",
          "text": "ðŸ”¥ðŸ”¥",
          "score": 1,
          "created_utc": "2026-01-17 20:07:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06xzwq",
          "author": "Naive-Extension7953",
          "text": "is this your voice?",
          "score": 1,
          "created_utc": "2026-01-17 23:13:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o08hbnr",
          "author": "2hands10fingers",
          "text": "instant subscribe",
          "score": 1,
          "created_utc": "2026-01-18 04:14:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qiguai",
      "title": "SVM from scratch in JS",
      "subreddit": "learnmachinelearning",
      "url": "https://v.redd.it/8s2nuomc6leg1",
      "author": "Ok-Statement-3244",
      "created_utc": "2026-01-20 23:10:56",
      "score": 215,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qiguai/svm_from_scratch_in_js/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0rcgp4",
          "author": "0uchmyballs",
          "text": "Looking forward to more ML in JS",
          "score": 6,
          "created_utc": "2026-01-20 23:25:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0slpwd",
          "author": "rshah4",
          "text": "Karpathy did a version of this as well: [https://cs.stanford.edu/\\~karpathy/svmjs/demo/](https://cs.stanford.edu/~karpathy/svmjs/demo/)",
          "score": 3,
          "created_utc": "2026-01-21 03:42:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rp1nw",
          "author": "sodapopenski",
          "text": "Now THIS is machine learning.",
          "score": 2,
          "created_utc": "2026-01-21 00:34:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0y24gy",
          "author": "pacukluka",
          "text": "\"Show Meth :3\" ðŸ§",
          "score": 1,
          "created_utc": "2026-01-21 22:59:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qib764",
      "title": "[Cheat Sheet] I summarized the 10 most common ML Algorithms for my interview prep. Thought I'd share.",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qib764/cheat_sheet_i_summarized_the_10_most_common_ml/",
      "author": "IT_Certguru",
      "created_utc": "2026-01-20 19:41:10",
      "score": 173,
      "num_comments": 21,
      "upvote_ratio": 0.9,
      "text": "Hi everyone,\n\nIâ€™ve been reviewing the basics for upcoming interviews, and I realized I often get stuck trying to explain simple concepts without using jargon.\n\nI wrote down a summary for the top 10 algorithms to help me memorize them. I figured this might help others here who are just starting out or refreshing their memory.\n\nHere is the list:\n\n# 1. Linear Regression\n\n* **The Gist:** Drawing the straightest possible line through a scatter plot of data points to predict a value (like predicting house prices based on size).\n* **Key Concept:** Minimizing the \"error\" (distance) between the line and the actual data points.\n\n# 2. Logistic Regression\n\n* **The Gist:** Despite the name, it's for **classification**, not regression. It fits an \"S\" shaped curve (Sigmoid) to the data to separate it into two groups (e.g., \"Spam\" vs. \"Not Spam\").\n* **Key Concept:** It outputs a probability between 0 and 1.\n\n# 3. K-Nearest Neighbors (KNN)\n\n* **The Gist:** The \"peer pressure\" algorithm. If you want to know what a new data point is, you look at its 'K' nearest neighbors. If most of them are Blue, the new point is probably Blue.\n* **Key Concept:** It doesn't actually \"learn\" a model; it just memorizes the data (Lazy Learner).\n\n# 4. Support Vector Machine (SVM)\n\n* **The Gist:** Imagine two groups of data on the floor. SVM tries to put a wide street (hyperplane) between them. The goal is to make the street as wide as possible without touching any data points.\n* **Key Concept:** The \"Kernel Trick\" allows it to separate data that isn't easily separable by a straight line by projecting it into higher dimensions.\n\n# 5. Decision Trees\n\n* **The Gist:** A flowchart of questions. \"Is it raining?\" -> Yes -> \"Is it windy?\" -> No -> \"Play Tennis.\" It splits data into smaller and smaller chunks based on simple rules.\n* **Key Concept:** Easy to interpret, but prone to \"overfitting\" (memorizing the data too perfectly).\n\n# 6. Random Forest\n\n* **The Gist:** A democracy of Decision Trees. You build 100 different trees and let them vote on the answer. The majority wins.\n* **Key Concept:** Reduces the risk of errors that a single tree might make (Ensemble Learning).\n\n# 7. K-Means Clustering\n\n* **The Gist:** You have a messy pile of unlabelled data. You want to organize it into 'K' number of piles. The algorithm randomly picks centers for the piles and keeps moving them until the groups make sense.\n* **Key Concept:** Unsupervised learning (we don't know the answers beforehand).\n\n# 8. Naive Bayes\n\n* **The Gist:** A probabilistic classifier based on Bayes' Theorem. It assumes that all features are independent (which is \"naive\" because in real life, things are usually related).\n* **Key Concept:** Surprisingly good for text classification (like filtering emails).\n\n# 9. Principal Component Analysis (PCA)\n\n* **The Gist:** Data compression. You have a dataset with 50 columns (features), but you only want the 2 or 3 that matter most. PCA combines variables to reduce complexity while keeping the important information.\n* **Key Concept:** Dimensionality Reduction.\n\n# 10. Gradient Boosting (XGBoost/LightGBM)\n\n* **The Gist:** Similar to Random Forest, but instead of building trees at the same time, it builds them one by one. Each new tree tries to fix the mistakes of the previous tree.\n* **Key Concept:** Often the winner of Kaggle competitions for tabular data.\n\nIf you want to connect these concepts to real production workflows, one helpful resource is a hands-on course on Machine Learning on Google Cloud. It shows how algorithms like Linear/Logistic Regression, PCA, Random Forests, and Gradient Boosting: [Machine Learning on Google Cloud](https://www.netcomlearning.com/course/machine-learning-on-google-cloud)\n\nLet me know if I missed any major ones or if you have a better analogy for them!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qib764/cheat_sheet_i_summarized_the_10_most_common_ml/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o0qieow",
          "author": "Disastrous_Room_927",
          "text": ">Despite the name, it's for **classification**, not regression. It fits an \"S\" shaped curve (Sigmoid) to the data to separate it into two groups (e.g., \"Spam\" vs. \"Not Spam\").\n\nThis isn't correct: regression is descriptive of the model being fit, classification is one use case for it. It isn't fitting an s-shaped curve directly, the fit is actually linear with respect to the log-odds, not the probability itself. The s-curve arises when you convert the output of the model back to probabilities.\n\nUnderstanding that the output people normally work with \"lives\" in a different space than the model is important if you're interpreting coefficients, or if you want to use a linear model with other types of responses. You can use Poisson/Negative Binomial regression for count data, beta regression for continuous proportions, gamma regression for data that is positive continuous and skewed, etc. All of these (and logistic regression) are types of Generalized Linear Models that differ by the function used to go from the original scale of the data to the scale the model is being fit on. Interestingly enough, this is an avenue for modeling probabilities in alternative ways - you get different shaped s-curves if you use probit, Chauchit, Cloglog functions instead of logit, for example. Not super common but worth knowing about.\n\n>**The Gist:** Data compression. You have a dataset with 50 columns (features), but you only want the 2 or 3 that matter most. PCA combines variables to reduce complexity while keeping the important information.\n\nAlso worth noting that this is just one thing PCA is particularly useful for. What you're doing is making new variables that \"explain\" the variance of the data independently of one another (they're uncorrelated/orthogonal). The first one captures the most variability, the second one captures the most variance in the absence of the first, and so on and so forth. You end up with 50 new columns containing the same information as the original data, but past a certain point they're just capturing the leftovers.\n\nPCA doesn't necessarily reduce complexity (in terms of information) directly, it allows you to cut out as much of it as you can without sacrificing fidelity - past the first few components, the only thing being captured is smaller and smaller chunks of noise. Sort of like discarding pixels in a region of an image that appears purely black to the naked eye and probably only differs due to camera sensor noise - you could use interpolation to reconstruct most of them and nobody would be the wiser. Note: a fun idea to play with here is to actually use PCA for compression. You can start with a dataset that has a column for each color channel and a row for each pixel, and then see what happens if you start pooling information for nearby pixels. \n\nComplexity is also reduced in a practical sensed because correlated variables end up getting decomposed into variables that contain shared and unique information. If they're highly correlated, that unique information isn't contributing much and may end up getting discarded - you're effectively collapsing dimensions.",
          "score": 36,
          "created_utc": "2026-01-20 20:58:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0rjdcy",
              "author": "Disastrous_Room_927",
              "text": "Follow up comment: I got side tracked by this comment and made an image compression script with PCA. The image on the left is 55% of the size of the one on the right: [https://imgur.com/a/mrjir5o](https://imgur.com/a/mrjir5o)",
              "score": 5,
              "created_utc": "2026-01-21 00:03:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0s3oqp",
              "author": "FancyEveryDay",
              "text": "PCA is useful for all sorts of things, lately I've been using it to do Total Least Squares regression, but I've also used it for Factor Analysis where you apply varimax or other rotations to group correlated variables together, and PCR which is ordinary linear regression performed using the Principal Components as variables.",
              "score": 1,
              "created_utc": "2026-01-21 01:57:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0sh4t7",
                  "author": "Disastrous_Room_927",
                  "text": "It's also useful for connecting the dots between concepts. What do you get when you you add a non-linear transformation in to the outputs of PCA in PCR? The exact functional form of a neural network with one hidden layer. It doesn't work like a neural net when you train it, but if you set yourself to the task of gluing together statistical models to make them work like one you end up learning a lot. like why backprop isn't magic but is super useful.",
                  "score": 2,
                  "created_utc": "2026-01-21 03:14:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0qgwm1",
          "author": "Suspicious-Beyond547",
          "text": "You write just like my best friend Chad! What are the odds?!",
          "score": 47,
          "created_utc": "2026-01-20 20:51:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0re5kp",
              "author": "Nerdly_McNerd-a-Lot",
              "text": "Wait. Is this Chad??",
              "score": 6,
              "created_utc": "2026-01-20 23:35:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0uey6w",
                  "author": "sunil2000_babu",
                  "text": "Chat, who is Chad?",
                  "score": 1,
                  "created_utc": "2026-01-21 12:35:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0r9w24",
          "author": "EvilWrks",
          "text": "CNN and RNN would be good one to keep in mind.",
          "score": 4,
          "created_utc": "2026-01-20 23:12:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tujbu",
          "author": "NotMyRealName778",
          "text": "Ngl if I asked someone to explain linear regression and they answered with what you wrote as the gist I wouldn't hire them. Same goes for the other explanations.\n\nI think you need to read some textbooks. You understand something but this is way too surface level to be meaningful.",
          "score": 3,
          "created_utc": "2026-01-21 09:47:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tgyp5",
          "author": "swastik_K",
          "text": "Wait, you guys are still using SVM?",
          "score": 2,
          "created_utc": "2026-01-21 07:38:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0sht6r",
          "author": "MathProfGeneva",
          "text": "Ugh sorry, but logistic regression is ABSOLUTELY a regression. It's one part of the general linear models family with a logit link function. In ML it is generally turned into a classifier by applying a threshold, but it's used a lot without that.",
          "score": 1,
          "created_utc": "2026-01-21 03:18:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0svk5z",
          "author": "aibyzee",
          "text": "Thankyou for sharing this! A valueable info.",
          "score": 1,
          "created_utc": "2026-01-21 04:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0uxmk0",
          "author": "heggiepau",
          "text": "Linear regression does not mean a straight line through the data, it requires that the model is linear in the parameters. y = x1 + x2^2 is a curve but linear in the parameters. On the other hand y = x1 + 2^x2 is not linear in the parameters",
          "score": 1,
          "created_utc": "2026-01-21 14:23:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0v0hn2",
          "author": "TMHDD_TMBHK",
          "text": "Thumbs up my Chad, reliable as always. Prompto!",
          "score": 1,
          "created_utc": "2026-01-21 14:37:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0vbpng",
          "author": "KaleAnxious2863",
          "text": "Thanks for sharing",
          "score": 1,
          "created_utc": "2026-01-21 15:31:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0wjlio",
          "author": "equqe",
          "text": "great summary",
          "score": 1,
          "created_utc": "2026-01-21 18:47:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0zeaga",
          "author": "BryanBeau27",
          "text": "Great, Thank you for sharing it. Appreciated.",
          "score": 1,
          "created_utc": "2026-01-22 03:26:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o101q7r",
          "author": "animalmad72",
          "text": "The analogies actually make sense instead of being overly technical. Saved for when I inevitably blank on explaining KNN in an interview.",
          "score": 1,
          "created_utc": "2026-01-22 06:05:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0r0olb",
          "author": "Charming_Elk_9058",
          "text": "Great summary!",
          "score": 1,
          "created_utc": "2026-01-20 22:24:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0sdu5a",
          "author": "CorpusculantCortex",
          "text": "Particle swarm optimization",
          "score": 1,
          "created_utc": "2026-01-21 02:55:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qgvit5",
      "title": "(End to End) 20 Machine Learning Project in Apache Spark",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qgvit5/end_to_end_20_machine_learning_project_in_apache/",
      "author": "bigdataengineer4life",
      "created_utc": "2026-01-19 05:27:56",
      "score": 114,
      "num_comments": 7,
      "upvote_ratio": 0.98,
      "text": "Hi Guys,\n\nI hope you are well.\n\nFree tutorial on Machine Learning Projects (End to End) in **Apache Spark and Scala with Code and Explanation**\n\n1. [Life Expectancy Prediction using Machine Learning](https://projectsbasedlearning.com/apache-spark-machine-learning/life-expectancy-prediction-using-machine-learning/)\n2. [Predicting Possible Loan Default Using Machine Learning](https://projectsbasedlearning.com/apache-spark-machine-learning/predicting-possible-loan-default-using-machine-learning/)\n3. [Machine Learning Project - Loan Approval Prediction](https://projectsbasedlearning.com/apache-spark-machine-learning/machine-learning-project-loan-approval-prediction/)\n4. [Customer Segmentation using Machine Learning in Apache Spark](https://projectsbasedlearning.com/apache-spark-machine-learning/customer-segmentation-using-machine-learning-in-apache-spark/)\n5. [Machine Learning Project - Build Movies Recommendation Engine using Apache Spark](https://projectsbasedlearning.com/apache-spark-machine-learning/machine-learning-project-creating-movies-recommendation-engine-using-apache-spark/)\n6. [Machine Learning Project on Sales Prediction or Sale Forecast](https://projectsbasedlearning.com/apache-spark-machine-learning/machine-learning-project-on-sales-prediction-or-sale-forecast/)\n7. [Machine Learning Project on Mushroom Classification whether it's edible or poisonous](https://projectsbasedlearning.com/apache-spark-machine-learning/machine-learning-project-on-mushroom-classification-whether-its-edible-or-poisonous-part-1/)\n8. [Machine Learning Pipeline Application on Power Plant.](https://projectsbasedlearning.com/apache-spark-machine-learning/machine-learning-pipeline-application-on-power-plant/)\n9. [Machine Learning Project â€“ Predict Forest Cover](https://projectsbasedlearning.com/apache-spark-machine-learning/machine-learning-project-predict-forest-cover-part-1/)\n10. [Machine Learning Project Predict Will it Rain Tomorrow in Australia](https://projectsbasedlearning.com/apache-spark-machine-learning/machine-learning-project-predict-will-it-rain-tomorrow-in-australia/)\n11. [Predict Ads Click - Practice Data Analysis and Logistic Regression Prediction](https://projectsbasedlearning.com/apache-spark-machine-learning/predict-ads-click-practice-data-analysis-and-logistic-regression-prediction/)\n12. [Machine Learning Project -Drug Classification](https://projectsbasedlearning.com/apache-spark-machine-learning/drug-classification/)\n13. [Prediction task is to determine whether a person makes over 50K a year](https://projectsbasedlearning.com/apache-spark-machine-learning/prediction-task-is-to-determine-whether-a-person-makes-over-50k-a-year/)\n14. [Machine Learning Project - Classifying gender based on personal preferences](https://projectsbasedlearning.com/apache-spark-machine-learning/classifying-gender-based-on-personal-preferences/)\n15. [Machine Learning Project - Mobile Price Classification](https://projectsbasedlearning.com/apache-spark-machine-learning/mobile-price-classification/)\n16. [Machine Learning Project - Predicting the Cellular Localization Sites of Proteins in Yest](https://projectsbasedlearning.com/apache-spark-machine-learning/predicting-the-cellular-localization-sites-of-proteins-in-yest/)\n17. [Machine Learning Project - YouTube Spam Comment Prediction](https://projectsbasedlearning.com/apache-spark-machine-learning/youtube-spam-comment-prediction/)\n18. [Identify the Type of animal (7 Types) based on the available attributes](https://projectsbasedlearning.com/apache-spark-machine-learning/identify-the-type-of-animal-7-types-based-on-the-available-attributes/)\n19. [Machine Learning Project - Glass Identification](https://projectsbasedlearning.com/apache-spark-machine-learning/glass-identification/)\n20. [Predicting the age of abalone from physical measurements](https://projectsbasedlearning.com/apache-spark-machine-learning/predicting-the-age-of-abalone-from-physical-measurements-part-1/)\n\nI hope you'll enjoy these tutorials.",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qgvit5/end_to_end_20_machine_learning_project_in_apache/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o0gpeaf",
          "author": "RohanVipin",
          "text": "The code is in scala , I only know python",
          "score": 6,
          "created_utc": "2026-01-19 12:12:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hs8of",
              "author": "anal_pudding",
              "text": "Sounds like a personal problem.",
              "score": 1,
              "created_utc": "2026-01-19 15:51:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0kkzkf",
                  "author": "pm_me_your_smth",
                  "text": "Not so personal since scala is very rarely used anywhere and vast majority of ML learners are using python",
                  "score": 2,
                  "created_utc": "2026-01-19 23:44:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0l3x1k",
          "author": "Effective_Forever585",
          "text": "Thanks very much",
          "score": 1,
          "created_utc": "2026-01-20 01:26:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0fi22w",
          "author": "Pretend-Pangolin-846",
          "text": "Thanks, will take a look later!",
          "score": 1,
          "created_utc": "2026-01-19 05:49:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0fqed4",
          "author": "fnehfnehOP",
          "text": "Spark",
          "score": 1,
          "created_utc": "2026-01-19 06:57:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0h3ft5",
          "author": "padakpatek",
          "text": "saved",
          "score": 1,
          "created_utc": "2026-01-19 13:45:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhs23q",
      "title": "Curated list of actually free AI courses (no hidden paywalls) - with time commitment for eac",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qhs23q/curated_list_of_actually_free_ai_courses_no/",
      "author": "Bubbly_Ad_2071",
      "created_utc": "2026-01-20 05:07:31",
      "score": 99,
      "num_comments": 10,
      "upvote_ratio": 0.98,
      "text": "I got tired of \"free\" courses that lock certificates or key content behind paywalls. So I went through the major platforms and put together a list of courses that are genuinely free toÂ complete:Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n\nÂ  1. Elements of AI at Univ. Helsinki - 6 hrs  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n\nÂ  2. OpenAI Academy at OpenAI - 5 hrs  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n\nÂ 3. Prompt Engineering at [DeepLearning.AI](http://DeepLearning.AI) \\- 5 hrs  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n\nÂ 4. Salesforce AI at Trailhead - 5 hrs Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n\nÂ  5. Google AI Essentials at Coursera - 10 hrs; Audit free, cert $49 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n\nÂ  6. Microsoft AI Fundamentals at MS Learn - 8 hrs; Content free, exam $165Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n\nÂ  Full breakdown with what each covers:  [ https://boredom-at-work.com/best-free-ai-courses/ ](https://boredom-at-work.com/best-free-ai-courses/)\n\nÂ  What other free resources would you add? Always looking to expand the list.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qhs23q/curated_list_of_actually_free_ai_courses_no/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o0nf87h",
          "author": "diegoasecas",
          "text": "all of huggingface courses are free\n\nhttps://huggingface.co/learn",
          "score": 10,
          "created_utc": "2026-01-20 11:36:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0m9lni",
          "author": "Holiday_Lie_9435",
          "text": "This is a great list, thank you for putting it together! I would add the [fast.ai](http://fast.ai) courses (Practical Deep Learning for Coders) for something more hands-on and teaches practical skills. The structure is more top-down, and personally found it helpful for getting started on some beginner-friendly [AI/ML projects](https://www.interviewquery.com/p/ai-project-ideas).",
          "score": 9,
          "created_utc": "2026-01-20 05:31:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0p9bf7",
              "author": "famous_chalupa",
              "text": "Is the Practical Deep Learning for Coders still relevant material? It seems like it, but that course has been around for a while.\n\nI'm planning on starting it this week.",
              "score": 1,
              "created_utc": "2026-01-20 17:32:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mep1f",
          "author": "JasperTesla",
          "text": "Thanks! This is gold!",
          "score": 3,
          "created_utc": "2026-01-20 06:10:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mmfks",
          "author": "Jesus_is_King_agreed",
          "text": "Good",
          "score": 3,
          "created_utc": "2026-01-20 07:14:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mu93a",
          "author": "equqe",
          "text": "thank you a lot!",
          "score": 2,
          "created_utc": "2026-01-20 08:25:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mv9hl",
          "author": "Sharp_Level3382",
          "text": "Thanks!",
          "score": 2,
          "created_utc": "2026-01-20 08:34:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0py939",
          "author": "Thin-Chart4124",
          "text": "This is helpful, thank u!",
          "score": 2,
          "created_utc": "2026-01-20 19:25:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qzstr",
          "author": "Charming_Elk_9058",
          "text": "Thank you! I've been looking for something like this.",
          "score": 2,
          "created_utc": "2026-01-20 22:19:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rswo2",
          "author": "dv11JUN",
          "text": "I really appreciate it !",
          "score": 2,
          "created_utc": "2026-01-21 00:55:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qixrtx",
      "title": "If you had to learn AI/LLMs from scratch again, what would you focus on first?",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qixrtx/if_you_had_to_learn_aillms_from_scratch_again/",
      "author": "EngineerLoose5042",
      "created_utc": "2026-01-21 13:23:34",
      "score": 90,
      "num_comments": 33,
      "upvote_ratio": 0.98,
      "text": "Iâ€™m a web developer with about two years of experience. I recently quit my job and decided to spend the next 15 months seriously upskilling to land an AI/LLM role â€” focused on building real products, not academic research.  \nIf you already have experience in this field, Iâ€™d really appreciate your advice on what I should start learning first. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qixrtx/if_you_had_to_learn_aillms_from_scratch_again/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o0uq03x",
          "author": "frivoflava29",
          "text": "Probabilities and statistics. I also wouldn't get into AI. You will find most people here are into machine learning (which has been studied for many decades) and not so much the current trend with transformers.",
          "score": 66,
          "created_utc": "2026-01-21 13:42:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0uy85r",
              "author": "3n91n33r",
              "text": "Any recommended resources on those?",
              "score": 6,
              "created_utc": "2026-01-21 14:26:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0v2rcv",
                  "author": "frivoflava29",
                  "text": "Geron's PyTorch book",
                  "score": 22,
                  "created_utc": "2026-01-21 14:49:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0v4wjh",
                  "author": "Annual-Beginning-352",
                  "text": "I'm a maths and stats student in Uni, our intro to machine learning started with simple linear regression before moving onto more complicated regression techniques like logarithmic. We also worked a bit with classification techniques like decision trees and random forests. Be warned regression is very limited when compared to the type of operations neural nets can do since, in regression you have to specify all the relationships between variables. But I think it can be a good jumping off point if you want to see what it means for a computer to come up with its own weights that relate the training data to the variable of interest.",
                  "score": 13,
                  "created_utc": "2026-01-21 14:59:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0vijcw",
          "author": "Vedranation",
          "text": "I graduated as Robotics engineer so I had some AI background, but it didn't really prepare me for how diverse it's in production compared to uni coursework. \n\nHonestly unpopular opinion but I'd have spent more time figuring out exactly why things do what they do and how. Like instead of jist writing Conv2D figuring out what a filter or kernel is, how they actually work, math behind SDG etc. I found myself in a lot of soft pits struggling with improving model performance because I didn't know stuff like that, like what BatchNorm actually does instead of automatically applying it after Conv2D because that's how I was taught. \n\nOh also Seq2Seq and RAG. Nobody told me I'd be doing so much RAG.",
          "score": 18,
          "created_utc": "2026-01-21 16:02:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0vo0dt",
          "author": "thinking_byte",
          "text": "I would start by building end to end things before going deep on theory. Get comfortable with data in, model out, and something users actually touch. A lot of people over index on model internals early and never learn where things break in practice. Focus on prompting, retrieval, evals, and failure modes first because that is where real products live right now. You can always go deeper on training and architecture later once you know why you need it. The fastest signal for roles is showing you can ship something imperfect and iterate.",
          "score": 9,
          "created_utc": "2026-01-21 16:27:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zurrp",
              "author": "EngineerLoose5042",
              "text": "This really resonates â€” it matches exactly the mindset Iâ€™m trying to follow (ship end-to-end, learn where things break, iterate). Iâ€™m starting my first 100 days focusing on just three basics: English, Python, and â€œjust enoughâ€ math for ML. After that, do you think I should jump straight into building products, or is there anything else youâ€™d prioritize before going all-in on shipping? Would love your thoughts.",
              "score": 1,
              "created_utc": "2026-01-22 05:12:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0xgwvt",
          "author": "drrednirgskizif",
          "text": "Thereâ€™s no need anymore. Itâ€™s so commoditized you either need to go get a PhD and work for one of the big model builders, or just learn more software engineering.",
          "score": 5,
          "created_utc": "2026-01-21 21:17:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zvazl",
              "author": "EngineerLoose5042",
              "text": "thank you !",
              "score": 1,
              "created_utc": "2026-01-22 05:16:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0yz8sm",
          "author": "Ok_Stranger8980",
          "text": "Probability, linear algebra..",
          "score": 5,
          "created_utc": "2026-01-22 02:00:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zuu2m",
              "author": "EngineerLoose5042",
              "text": "This is really helpful, thanks!",
              "score": 1,
              "created_utc": "2026-01-22 05:13:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0uqvob",
          "author": "QuiteMischief",
          "text": "Start with Transformers, then gradually move into Generative AI. Once you have that foundation, deep dive into RAG, how LLMs work, LLM fine-tuning, and agentic systems, and then explore the latest frameworks like A2A and MCP.\n\nThe key is to start from one end - once you begin, youâ€™ll naturally understand what you need to learn next. What matters most is starting now.",
          "score": 8,
          "created_utc": "2026-01-21 13:47:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0v9ecu",
              "author": "Imaginary_Tower_5518",
              "text": "Do you have any recommendations for books or content?",
              "score": 1,
              "created_utc": "2026-01-21 15:21:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0veqr6",
                  "author": "QuiteMischief",
                  "text": "Give this a read -AI Engineering by Chip Huyen",
                  "score": 3,
                  "created_utc": "2026-01-21 15:45:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0y4i4y",
                  "author": "rlNewbie",
                  "text": "This article from x has some good steps to follow https://x.com/i/status/2013608521900683765",
                  "score": 3,
                  "created_utc": "2026-01-21 23:11:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0vbau9",
              "author": "unknowntrail20",
              "text": "Hey l need your help check your DMs",
              "score": -1,
              "created_utc": "2026-01-21 15:30:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0y3er8",
          "author": "PeeVee_",
          "text": "Great question. If I had to start again, Iâ€™d focus much earlier on grounding understanding in a single source instead of hopping between explanations.\n\nOne thing I keep running into is that long-form content like lectures or podcasts is dense but hard to retrieve from later. Anyone know how would they would approach retaining and querying that kind of material?",
          "score": 3,
          "created_utc": "2026-01-21 23:05:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zwdni",
              "author": "EngineerLoose5042",
              "text": "This is a great point â€” I definitely struggle with hopping between sources too",
              "score": 1,
              "created_utc": "2026-01-22 05:24:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0w7pyt",
          "author": "Similar-Kangaroo-223",
          "text": "I am not from a technical background, but I also started to build and ship products since last year. Personally, I think using trying out different tools like Cursor, Claude Code, Kiro, and Antigravity while building a product is definitely very helpful.\n\nI think the hard part might be verification, which is that I know I am definitely learning after shipping the first product, but after shipping 3 different products, I can't really know if I am still learning new things or I am just repeating things over and over again.",
          "score": 2,
          "created_utc": "2026-01-21 17:55:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0yyx37",
          "author": "Disastrous_Room_927",
          "text": ">focused on building real products, not academic research.\n\nI studied statistics and ML in grad school, and one thing people got tripped up on was the difference between applied and theoretical. They had the impression that \"applied statistics\" was more about applying statistics to things, but the program was 90% derivations and proofs. The distinction is in what kind of theory is used and how: you need statistical theory to learn how the things people use work, which requires a fair bit of math. If you want to do academic research, this shifts to learning how the theory itself works.  \n\nAll of that is to say that if you want to get serious about building things with ML, approach it like engineering. Engineers need to take physics because their work depends on understanding it, but they don't need to understand it at the level of a physicist. For ML that might like being able to reason quantitatively about the tools you're using - you don't need to study the theory of optimization super deeply, but you should have an understanding of optimization as it pertains to what you're working with.",
          "score": 2,
          "created_utc": "2026-01-22 01:58:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zwh40",
              "author": "EngineerLoose5042",
              "text": "I really like this framing â€” treating ML as engineering rather than pure theory makes a lot of sense to me.",
              "score": 1,
              "created_utc": "2026-01-22 05:25:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o109viy",
          "author": "AccordingWeight6019",
          "text": "I would start by getting very clear on what problem you want models to solve in a system, not which models to learn. A lot of people jump straight into LLM tooling and miss fundamentals like data quality, evaluation, and failure modes, which end up dominating real products. If your goal is shipping, spend time understanding how models degrade, how you detect that, and how you iterate safely. LLMs make demos easy, but production work is mostly about constraints and trade-offs. the fastest progress usually comes from building small end-to-end things and being honest about where they break, and that intuition transfers better than chasing the latest architecture.",
          "score": 2,
          "created_utc": "2026-01-22 07:13:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12oj10",
              "author": "EngineerLoose5042",
              "text": "This honestly feels like an enlightenment moment for me â€” thank you so much for sharing this.",
              "score": 1,
              "created_utc": "2026-01-22 16:45:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o11jnk9",
          "author": "DataCamp",
          "text": "What we've seen work best for learners is building small projects early and learning things as you actually need them in the projects. Pick a simple end-to-end thing (data in â†’ model/LLM â†’ output you can test), get it working, and let the breakpoints tell you what to learn next. Tutorials are most useful as reference when youâ€™re stuck, not something to binge. Ship small, fix what breaks, repeat!",
          "score": 2,
          "created_utc": "2026-01-22 13:26:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12okpn",
              "author": "EngineerLoose5042",
              "text": "This genuinely feels like a lightbulb moment. Thanks a ton for this.",
              "score": 1,
              "created_utc": "2026-01-22 16:45:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjgqcw",
      "title": "Building an ML runtime from scratch, Day 1 - visualizing tensors in memory",
      "subreddit": "learnmachinelearning",
      "url": "https://i.redd.it/u2bceocvyseg1.jpeg",
      "author": "RefrigeratorFirm7646",
      "created_utc": "2026-01-22 01:23:35",
      "score": 82,
      "num_comments": 11,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qjgqcw/building_an_ml_runtime_from_scratch_day_1/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0znrvo",
          "author": "-CawmunGames",
          "text": "This is so good, there are literally no articles on this, at least none that i could find.\n\n\nYou did a really good job here.\n\n\nKeep it up! Looking forward to day 2.\n\n\nBtw, just out of curiosity, how old are you?",
          "score": 7,
          "created_utc": "2026-01-22 04:25:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zooc3",
              "author": "RefrigeratorFirm7646",
              "text": "Thanks! I really appreciate that. Im 17 and honestly i only made this post because i had the same issue as you, lack of resources on niche topics like these...",
              "score": 3,
              "created_utc": "2026-01-22 04:31:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o114qqb",
                  "author": "Moist-Matter5777",
                  "text": "Dude, that's super impressive for 17! It's great that you're tackling these complex topics early on. Keep pushing through, and donâ€™t hesitate to share your findings; itâ€™ll help a lot of others!",
                  "score": 1,
                  "created_utc": "2026-01-22 11:49:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o117enj",
                  "author": "frivoflava29",
                  "text": "Let me know if you'd like help with infographics, writing, etc! I'd love to work on a collaborative tutorial like this with you on my website sometime. Something like this would be perfect.",
                  "score": 1,
                  "created_utc": "2026-01-22 12:09:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o104qzy",
          "author": "JohnYellow333",
          "text": "what a coinsidance, I ahve decided same, now I am making an ai snake game, I finished game yet but I am planning to develop a brain too",
          "score": 0,
          "created_utc": "2026-01-22 06:29:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o106e09",
              "author": "RefrigeratorFirm7646",
              "text": "By 'developing a brain', do you mean you're working on a Reinforcement Learning agent or a specific Neural Network architecture? Working on projects you actually enjoy is the best way to learn complex topics like these",
              "score": 1,
              "created_utc": "2026-01-22 06:43:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o108lyw",
                  "author": "JohnYellow333",
                  "text": "Yes I am working on reinforcement learning, dqn, I found a lot sources, which confuses me, because hard to focus one of them, also I decreased my gemini usage, because I see that when I finished the tasks with ai, yes it works but I dont learn",
                  "score": 1,
                  "created_utc": "2026-01-22 07:02:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qhuy6z",
      "title": "The Space Warper (Matrices)",
      "subreddit": "learnmachinelearning",
      "url": "https://v.redd.it/i1hbr85llgeg1",
      "author": "No_Skill_8393",
      "created_utc": "2026-01-20 07:48:11",
      "score": 76,
      "num_comments": 10,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qhuy6z/the_space_warper_matrices/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0ng5qz",
          "author": "Habsu",
          "text": "Oh no, is AI.",
          "score": 3,
          "created_utc": "2026-01-20 11:43:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nfy19",
          "author": "Habsu",
          "text": "Who's this guy? I like this.",
          "score": 1,
          "created_utc": "2026-01-20 11:42:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0py1mm",
          "author": "notionocean",
          "text": "Would be nice if this explained how it actually works mathematically.",
          "score": 1,
          "created_utc": "2026-01-20 19:24:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tdn0s",
              "author": "Reclaimer2401",
              "text": "Honestly, you don't want that. Its best to explain what the numbers really are before you start manipulating the matrixs.Â \n\n\nThere's a lot of stuff in linear algebra, and it was a pain for me until i actually understand wtf I was doing and why.Â \n\n\nThis video sucks thoughÂ ",
              "score": 1,
              "created_utc": "2026-01-21 07:08:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ulqtu",
                  "author": "notionocean",
                  "text": "I'm in Calc 3. It's disappointing how the video just handwaves the numbers like 'See these numbers? Look what it does!' with no indication of how it actually works mathematically.",
                  "score": 1,
                  "created_utc": "2026-01-21 13:18:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0q5y5w",
          "author": "Door_Number_Three",
          "text": "Ah yes, the hello world of math for ML. How many people crash and burn when learning linear transformations.",
          "score": 1,
          "created_utc": "2026-01-20 20:01:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tdrsr",
              "author": "Reclaimer2401",
              "text": "I did at first.Â \n\n\nI just kept at it until it all clicked. It was slow and painful, but then I ended up with an A in the class and for fun used linear algebra to calculate the volume of a tesseract",
              "score": 1,
              "created_utc": "2026-01-21 07:09:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ncix0",
          "author": "bingbestsearchengine",
          "text": "I love his voice",
          "score": 0,
          "created_utc": "2026-01-20 11:14:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ne7g4",
              "author": "Archtarius",
              "text": "NotebookLM",
              "score": 1,
              "created_utc": "2026-01-20 11:28:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qe7si1",
      "title": "RNNs are the most challenging thing to understand in ML",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qe7si1/rnns_are_the_most_challenging_thing_to_understand/",
      "author": "radjeep",
      "created_utc": "2026-01-16 05:48:12",
      "score": 74,
      "num_comments": 35,
      "upvote_ratio": 0.88,
      "text": "Iâ€™ve been thinking about this for a while, and Iâ€™m curious if others feel the same.\n\nIâ€™ve been reasonably comfortable building intuition around most ML concepts Iâ€™ve touched so far. CNNs made sense once I understood basic image processing ideas. Autoencoders clicked as compression + reconstruction. Even time series models felt intuitive once I framed them as structured sequences with locality and dependency over time.\n\nBut RNNs? Theyâ€™ve been uniquely hard in a way nothing else has been.\n\nItâ€™s not that the math is incomprehensible, or that I donâ€™t understand sequences. I *do*. I understand sliding windows, autoregressive models, sequence-to-sequence setups, and Iâ€™ve even built LSTM-based projects before without fully â€œgettingâ€ what was going on internally.\n\nWhat trips me up is that RNNs donâ€™t give me a stable mental model. The hidden state feels fundamentally opaque i.e. it's not like a feature map or a signal transformation, but a compressed, evolving internal memory whose semantics I canâ€™t easily reason about. Every explanation feels syntactically different, but conceptually slippery in the same way.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qe7si1/rnns_are_the_most_challenging_thing_to_understand/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "nzvkpsj",
          "author": "ds_account_",
          "text": "How do you feel about SVM, VAE and latent diffusion.\n\nBut I agree RNN can be tough without first grasping time series analysis.",
          "score": 45,
          "created_utc": "2026-01-16 06:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvhgkn",
          "author": "newrockstyle",
          "text": "RNNs are tough because their hidden state is hard to visualise and reason about.",
          "score": 30,
          "created_utc": "2026-01-16 06:05:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw0tsq",
              "author": "narasadow",
              "text": "TBH I felt the same for Kalman Filters",
              "score": 10,
              "created_utc": "2026-01-16 08:52:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzw2yfr",
          "author": "UnusualClimberBear",
          "text": "I think the best explanation is this famous yet old blog post by C. Olah : [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)",
          "score": 15,
          "created_utc": "2026-01-16 09:12:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzynlep",
              "author": "CasulaScience",
              "text": "Agreed this is what made it click for me. OP, they're actually pretty simple and make a lot of sense if you come from a comp sci background (which I don't). It's very reminiscent of a turing machine, you have some hidden state (instead of \"tape\") that you write to and read from at every step. What you write/delete and what you read at each step is controlled by little neural nets. \n\nThat's really it.",
              "score": 4,
              "created_utc": "2026-01-16 18:05:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzvw5g8",
          "author": "Expensive_Fun4346",
          "text": "if you think that's opaque, wait until you look at reinforcement learning",
          "score": 11,
          "created_utc": "2026-01-16 08:09:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01o5vw",
              "author": "jsh_",
              "text": "which aspects of reinforcement learning do you find opaque? I feel like once you understand MDPs it's relatively straightforward",
              "score": 1,
              "created_utc": "2026-01-17 03:35:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o09t62o",
                  "author": "Organic_botulism",
                  "text": "MDPs are the absolute basics for understanding tabular RL but once you leave that setting and start relaxing assumptions things get hairy very quickly as convergence proofs (e.g. for Q-learning) rely on a tabular setting among other requirements.\n\nThe Bellman error not being learnable is quite opaque IMO, as is the intuition for why bootstrapping, off-policy learning and function approximation can lead to divergence and instability during training (e.g. one-step semi-gradient Q-learning will diverge in certain settings).\n\nThese are all opaque ideas. If youâ€™re using any type of function approximation what is the requirement needed to guarantee stability? Why does linear function approximation with dynamic programming sometimes fail even if the least squares solution is found at each step?",
                  "score": 1,
                  "created_utc": "2026-01-18 10:55:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvn4mu",
          "author": "Silly_Guidance_8871",
          "text": "I agree, for reasons similar to why reasoning about loops and recursion are more difficult than non-branching code paths: There's a lot more implied state that's not easily managed",
          "score": 9,
          "created_utc": "2026-01-16 06:51:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvj3s2",
          "author": "TBSchemer",
          "text": "Then how do you feel about transformers?",
          "score": 17,
          "created_utc": "2026-01-16 06:18:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwuta8",
              "author": "quadrillio",
              "text": "I think self attention is actually easier to grasp. And 3blue1brown has a set of excellent videos explaining them. There arenâ€™t very many accessible resources for things like LSTM and many of the diagrams found online are over simplified or badly explained.",
              "score": 2,
              "created_utc": "2026-01-16 12:56:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzz650x",
                  "author": "TBSchemer",
                  "text": "Transformers take the same mechanisms used in LSTMs and add more complexity to it. If you can understand LLMs, you can understand RNNs.",
                  "score": 2,
                  "created_utc": "2026-01-16 19:27:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvhkfl",
          "author": "d0r1h",
          "text": "On the same boat, I've been revising ML concepts and got stuck at RNN currently. I've opened 10s of blogs and two books, Just to grasp the RNN. Gonna give few more hrs before moving forward.",
          "score": 5,
          "created_utc": "2026-01-16 06:06:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwqjy1",
              "author": "CuriousAIVillager",
              "text": "What did you use as a checklist?",
              "score": 1,
              "created_utc": "2026-01-16 12:29:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o01aubi",
                  "author": "d0r1h",
                  "text": "checklist as in ?",
                  "score": 1,
                  "created_utc": "2026-01-17 02:11:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvyleu",
          "author": "DrXaos",
          "text": "RNNs are dynamical systems in the 'chaos theory' sense.  Their original difficulties in training were because they had in effect strong Lyapunov exponents in either forward or backward directions resulting in exponential decay or explosion in state or gradients. \n\nFunny that until 8 years ago, RNNs of various forms were the state of the art as the most complex and interesting machine learning architecture.",
          "score": 6,
          "created_utc": "2026-01-16 08:32:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzs1ck",
          "author": "s-jb-s",
          "text": "I come from a probability theory background, so my intuition for \"ML\" came from outside the way CS students tend to think about things, which I generally don't understand.\n\nIf you're struggling with the intuition, I'd advise taking a step back from RNNs and looking at the evolution of the problems they solve. I think a natural progression to build up your intuition regarding latent-state models is to start with Discrete-time Hidden Markov Models, which are very easy to intuit.\n\nThe problem is that HMMs are inefficient in high dimensions. Factorial HMMs improve this by distributing the state into multiple binary variables, but this makes inference much more expensive to calculate. Intuitively, the fix is to move to Linear Dynamical Systems, where the state vectors are now continuous rather than discrete (think Kalman Filters, if you're familiar). This solves the representation problem, but now you have a linearity issue because you can only model simple curves. How do we fix that? We take the Linear Dynamical System and wrap the transition in a non-linear activation function. That is effectively an RNN.\n\nThere's a lot of nuance missing here and I've been a bit handwavey (it's not intended to fix your intuition) but I think there's value in studying what came before RNN's and building your intuition up from there, particularly in relation to what actually is actually going on in latent space, and what that represents. I'm unsure how helpful this is if you're not particularly interested in the theory, and just want some intuition. But I think the intuition comes from the theory, and seeing how it progresses.",
          "score": 3,
          "created_utc": "2026-01-16 21:10:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwckfh",
          "author": "divided_capture_bro",
          "text": "Wait, you can't reason about evolving internal states? It's almost like it is a black box of some sorts...",
          "score": 2,
          "created_utc": "2026-01-16 10:40:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzyuqec",
              "author": "bythenumbers10",
              "text": "Meanwhile, statistical methods await the disillusioned deep learning practitioners with open arms, ready to soothe their crisis of blind faith unrewarded with transparency and explicable tuning methods.",
              "score": 2,
              "created_utc": "2026-01-16 18:36:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzz0hr6",
                  "author": "Fair_Treacle4112",
                  "text": "compare normal ad hoc arrest scary practice coordinated absorbed doll employ\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*",
                  "score": 0,
                  "created_utc": "2026-01-16 19:01:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzyg735",
          "author": "Longjumping_Echo486",
          "text": "Personally I feel lstm math is tougher due to multiple gates  and it's beautiful when u see mathematically how the gates are deleting and updating info using pointwise operations and create a refined long term memory",
          "score": 2,
          "created_utc": "2026-01-16 17:32:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzyiikb",
          "author": "dankwartrustow",
          "text": "It only really made sense to me when I learned about vanishing and exploding gradients",
          "score": 2,
          "created_utc": "2026-01-16 17:42:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzypcox",
          "author": "Mr_iCanDoItAll",
          "text": "Try learning HMMs if you haven't already. Might help intuit hidden states.",
          "score": 1,
          "created_utc": "2026-01-16 18:13:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o01qdmt",
          "author": "pool007",
          "text": "OP, I think you already understand the main concept. It's really not a curated memory structure or features but computation and flow of data that's supposed to become stuff to memorize and that is all it does, imo. Flow should be fast to compute but still have expression power while being stable for training.",
          "score": 1,
          "created_utc": "2026-01-17 03:50:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o064q4r",
          "author": "arsenale",
          "text": "Each RNN variant is a standalone architecture.\n\nVec2Seq\n\nSeq2Seq\n\n...\n\nEach one is a totally different model, they are called RNN just because yes, they share an equation which uses the same Wxh Whh etc, but those matrices are multiplied by different values making the architecture completely different. Just stick with one architecture until you get it?",
          "score": 1,
          "created_utc": "2026-01-17 20:45:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzz09km",
          "author": "HumbleJiraiya",
          "text": "I dont know man. I understood RNNs immediately. It was probably the most intuitive concept for me in DL ðŸ˜…. \n\nAutoencoders were harder to understand than RNNs. \n\nWeâ€™re all different I guess.",
          "score": 0,
          "created_utc": "2026-01-16 19:00:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qf2q85",
      "title": "decision tree from scratch in js. no libraries.",
      "subreddit": "learnmachinelearning",
      "url": "https://v.redd.it/alufthat0udg1",
      "author": "Ok-Statement-3244",
      "created_utc": "2026-01-17 03:51:32",
      "score": 72,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qf2q85/decision_tree_from_scratch_in_js_no_libraries/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o01ud0b",
          "author": "disquieter",
          "text": "Wow, like, wow!",
          "score": 1,
          "created_utc": "2026-01-17 04:17:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04gwgm",
          "author": "giadev",
          "text": "thats so cool",
          "score": 1,
          "created_utc": "2026-01-17 16:01:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfcotf",
      "title": "An introduction to Physics Informed Neural Networks (PINNs): Teach your neural network to â€œrespectâ€ Physics",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qfcotf/an_introduction_to_physics_informed_neural/",
      "author": "omunaman",
      "created_utc": "2026-01-17 13:03:11",
      "score": 68,
      "num_comments": 11,
      "upvote_ratio": 0.91,
      "text": "https://preview.redd.it/ll4z0ewvqwdg1.png?width=1100&format=png&auto=webp&s=e6a375679fb5575866953109c00e86d8eb31523a\n\nAs universal function approximators, neural networks can learn to fit any dataset produced by complex functions. With deep neural networks, overfitting is not a feature. It is a bug.\n\nMedium Link for better readability: [https://vizuara.medium.com/an-introduction-to-physics-informed-neural-networks-pinns-teach-your-neural-network-to-respect-af484ac650fc](https://vizuara.medium.com/an-introduction-to-physics-informed-neural-networks-pinns-teach-your-neural-network-to-respect-af484ac650fc)\n\nLet us consider a hypothetical set of experiments. You throw a ball up (or at an angle), and note down the height of the ball at different points of time.\n\nWhen you plot the height v/s time, you will see something like this.\n\nhttps://preview.redd.it/b9byjx62pwdg1.png?width=1100&format=png&auto=webp&s=22aebc098ad30d2b18505fcaa3d80cf61777f2b5\n\nIt is easy to train a neural network on this dataset so that you can predict the height of the ball even at time points where you did not note down the height in your experiments.\n\nFirst, let us discuss how this training is done.\n\n# Training a regular neural network\n\n\n\nhttps://preview.redd.it/732wrp23pwdg1.png?width=1100&format=png&auto=webp&s=5c65e4fc46e3a8fd8fcac281361ece4328932f2b\n\nYou can construct a neural network with few or multiple hidden layers. The input is time (t) and the output predicted by the neural network is height of the ball (h).\n\nThe neural network will be initialized with random weights. This means the predictions of h(t) made by the neural network will be very bad initially as shown in the image below.\n\nhttps://preview.redd.it/xdgeu9s4pwdg1.png?width=1100&format=png&auto=webp&s=2e97b932fe7bef937f45716295435c7d50c0212f\n\nWe need to penalize the neural network for making these bad predictions right? How do we do that? In the form of loss functions.\n\nLoss of a neural network is a measure of how bad its predictions are compared the real data. The close the predictions and data, the lower the loss.\n\nA singular goal of neural network training is to minimize the loss.\n\nSo how can we define the loss here? Consider the 3 options below.\n\nhttps://preview.redd.it/slcx6y27pwdg1.png?width=1100&format=png&auto=webp&s=fcccb9ec6c9aac8b976b71ae5a7f7f6dfd481c24\n\nIn all the 3 options, you are finding the average of some kind of loss.\n\n* **Option 1 is not good**Â because positive and negative errors will cancel each other.\n* **Option 2 is okay**Â because we are taking the absolute value of errors, but the problem is modulus function is not differentiable at x=0.\n* **Option 3 is the best**. It is a square function which means individual errors are converted to positive numbers and the function is differentiable. This is the famous Mean Squared Error (MSE). You are taking the mean value of the square of all individual errors.\n\nHere error means the difference between actual value and predicted value.\n\nMean Squared Error is minimum when the predictions are very close to the experimental data as shown in the figure below.\n\nhttps://preview.redd.it/vwm6mxq8pwdg1.png?width=1100&format=png&auto=webp&s=33983e165ecec1efca3a973e97b3d28aa2a89782\n\nBut there is a problem with this approach. What if your experimental data was not good? In the image below you can see that one of the data points is not following the trend shown by the rest of the dataset.\n\nhttps://preview.redd.it/mswknvl9pwdg1.png?width=1100&format=png&auto=webp&s=71546cc05f741175a11e486ae3fe6a77c44b82e7\n\nThere can be multiple reasons due to which such data points show up in the data.\n\n1. You did not perform the experiments well. You made a manual mistake while noting the height.\n2. The sensor or instrument using which you were making the height measurement was faulty.\n3. A sudden gush of wind caused a sudden jump in the height of the ball.\n\nThere could be many possibilities that results in outliers and noise in a dataset.\n\nKnowing that real life data may have noise and outliers, it will not be wise if we train a neural network to exactly mimic this dataset. It results in something called as overfitting.\n\nhttps://preview.redd.it/1e7r509apwdg1.png?width=1100&format=png&auto=webp&s=e3269c58b8ca9e873945ca9970aafac78bc53279\n\nhttps://preview.redd.it/l0fgrzrapwdg1.png?width=1100&format=png&auto=webp&s=28acb46d2af8e6398876ee107b7900e860061904\n\nIn the figure above, mean squared error will be low in both cases. However in one case neural network is fitting on outlier also, which is not good. So what should we do?\n\n# Bring physics into the picture\n\nIf you are throwing a ball and observing its physics, then you already have some knowledge about the trajectory of the ball, based on Newtonâ€™s laws of motion.\n\nSure, you may be making simplifications by assuming that the effect of wind or air drag or buoyancy are negligible. But that does not take away from the fact that you already have decent knowledge about this system even in the absence of a trained neural network.\n\nhttps://preview.redd.it/8cudgx0epwdg1.png?width=1100&format=png&auto=webp&s=9efaf22e50525030c0ceaa9995b0afe96a26c79d\n\nThe physics you assume may not be in perfect agreement with the experimental data as shown above, but it makes sense to think that the experiments will not deviate too much from physics.  \n\n\nhttps://preview.redd.it/fpy7q3oepwdg1.png?width=1100&format=png&auto=webp&s=dc5ff5cacaf8b8d2895139589897c6dd3d670be9\n\nSo if one of your experimental data points deviate too much from what physics says, there is probably something wrong with that data point. So how can you let you neural network take care of this?\n\n# How can you teach physics to neural networks?\n\nIf you want to teach physics to neural network, then you have to somehow incentivize neural network to make predictions closer to what is suggested by physics.\n\nIf the neural network makes a prediction where the height of the ball is far away from the purple dotted line, then loss should increase.\n\nIf the predictions are closer to the dotted line, then the loss should be minimum.\n\nWhat does this mean? Modify the loss function.\n\nHow can you modify the loss function such that the loss is high when predictions deviate from physics? And how does this enable the neural network make more physically sensible predictions?Â **Enter PINN Physics Informed Neural Network.**  \n  \nPhysics Informed Neural Network (PINN)\n\nThe goal of PINNs is to solve (*or learn solutions to*) differential equations by embedding the known physics (or governing differential equations) directly into the neural networkâ€™s training objective (loss function).\n\nThe idea of PINNs were introduced in this seminal paper by Maziar Raissi et. al.:Â [https://maziarraissi.github.io/PINNs/](https://maziarraissi.github.io/PINNs/)\n\nThe basic idea in PINN is to have a neural network is trained to minimize a loss function that includes:\n\n1. AÂ **data mismatch**Â term (*if observational data are available*).\n2. AÂ **physics loss**Â term enforcing the differential equation itself (and initial/boundary conditions).\n\n# Let us implement PINN on our example\n\nLet us look at what we know about our example. When a ball is thrown up, it trajectory h(t) varies according to the following ordinary differential equation (ODE).\n\nhttps://preview.redd.it/vacsz6dlpwdg1.png?width=1100&format=png&auto=webp&s=14111c810dba1e861fbcc71a1bf8d920e479448c\n\nHowever this ODE alone cannot fully describe h(t) uniquely. You also need an initial condition. Mathematically this is because to solve a first-order differential equation in time, you need 1 initial condition.\n\nLogically, to know height as a function of time, you need to know the starting height from which the ball was thrown. Look at the image below. In both cases, the balls are thrown at the exact same time with the exact same initial velocity component in the vertical direction. But the h(t) depends on the initial height. So you need to know h(t=0) for fully describing the height of the ball as a function of time.\n\nhttps://preview.redd.it/eobv9u1mpwdg1.png?width=1100&format=png&auto=webp&s=a28a6c8584f37683f703b4c72a5a8f436353dedc\n\nThis means it is not enough to make the neural network make accurate predictions on dh/dt, the neural network should also make accurate prediction on h(t=0) for fully matching the physics in this case.\n\n# Loss due to dh/dt (ODE loss)\n\nWe know the expected dh/dt because we know the initial velocity and acceleration due to gravity.\n\nHow do we get the dh/dt predicted by the neural network? After all it is predicting height h, not velocity v or dh/dt. The answer isÂ **Automatic differentiation (AD).**\n\nBecause most machineâ€learning frameworks (e.g., TensorFlow, PyTorch, JAX) support automatic differentiation, you can compute dh/dt by differentiating the neural network.\n\nThus, we have a predicted dh/dt (from the neural network differentiation) for every experimental time points, and we have an actual dh/dt based on the physics.\n\nhttps://preview.redd.it/msf6gyunpwdg1.png?width=1100&format=png&auto=webp&s=1392d9e60f5ee011a480392af07e05bc5d094492\n\nNow we can define a loss due to the difference between predicted and physics-based dh/dt.\n\nhttps://preview.redd.it/68xl4xpopwdg1.png?width=1100&format=png&auto=webp&s=5b9a727be489bd8736e8ffc235f49fca5dc25b9a\n\nMinimizing this loss (which I prefer to call ODE loss) is a good thing to ensure that neural network learns the ODE. But that is not enough. We need to make the neural network follow the initial condition also. That brings us to the next loss term.Initial condition loss\n\n# Initial condition loss\n\nThis is easy. You know the initial condition. You make the neural network make a prediction of height for t=0. See how far off the prediction is from the reality. You can construct a squared error which can be called as theÂ *Initial Condition Loss.*\n\nhttps://preview.redd.it/4u4syj1qpwdg1.png?width=1100&format=png&auto=webp&s=591b7e0f46ebf32024533c9d727042a889c3007d\n\nSo is that it? You have ODE loss and Initial condition loss. Is it enough that the neural network tries to minimize these 2 losses? What about the experimental data? There are 3 things to consider.\n\n1. You cannot throw away the experimental data.\n2. You cannot neglect the physics described by the ODEs or PDEs.\n3. You cannot neglect the initial and/or boundary conditions.\n\nThus you have to also consider the data-based mean squared error loss along with ODE loss and Initial condition loss.\n\n# The modified loss term\n\nThe simple mean squared error based loss term can now be modified like below.\n\nhttps://preview.redd.it/n2xc18prpwdg1.png?width=1100&format=png&auto=webp&s=95fabc8b54b2b291292d6ab2c15f5810c13379ce\n\nIf there are boundary conditions in addition to initial conditions, you can add an additional term based on the difference between predicted boundary conditions and actual boundary conditions.\n\nhttps://preview.redd.it/ezh3in7spwdg1.png?width=1100&format=png&auto=webp&s=70367e6fbb1aa6e7924d93da8ff3b0ce8898419d\n\nHere the Data loss term ensures that the predictions are not too far from the experimental data points.\n\nTheÂ *ODE loss term*Â \\+ theÂ *initial condition loss term*Â ensures that the predictions are not too far from what described by the physics.\n\nIf you are pretty sure about the physics the you can set Î»1 to zero. In the ball throwing experiment, you will be sure about the physics described by our ODE if air drag, wind, buoyancy and any other factors are ignored. Only gravity is present. And in such cases, the PINN effectively becomes an ODE solver.\n\nHowever, for real life cases where only part of the physics is known or if you are not fully sure of the ODE, then you retain Î»1 and other Î» terms in the net loss term. That way you force the neural network to respect physics as well as the experimental data. This also suppress the effects of experimental noise and outliers.\n\n>",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qfcotf/an_introduction_to_physics_informed_neural/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o03l86k",
          "author": "n0obmaster699",
          "text": "So you just add a lagrange multiplier which follows the eom?",
          "score": 6,
          "created_utc": "2026-01-17 13:11:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0fark1",
              "author": "omunaman",
              "text": "Yep! It is essentially acting as a soft constraint added to the loss function, very similar to the penalty method in Lagrange multipliers.",
              "score": 1,
              "created_utc": "2026-01-19 04:55:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0h5w4b",
                  "author": "nickpsecurity",
                  "text": "I enjoyed reading it. Nice visuals to help with explanations, too.",
                  "score": 2,
                  "created_utc": "2026-01-19 13:59:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0hwma0",
                  "author": "n0obmaster699",
                  "text": "Seems fair",
                  "score": 1,
                  "created_utc": "2026-01-19 16:10:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o08qh47",
          "author": "inmadisonforabit",
          "text": "Look into regularization. Also, if you need your models to respect physics, it would be best to avoid using a NN to begin with and instead directly model it via the ODEs (in reality, likely PDEs) you're already using.",
          "score": 5,
          "created_utc": "2026-01-18 05:16:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0fb4ku",
              "author": "omunaman",
              "text": "If we fully know the ODE and the parameters, a standard numerical solver is definitely better and more accurate. I mainly used this simple case just to demonstrate the concept for beginners.\n\nThe real use of PINNs shines in inverse problems where we have the data but don't know the parameters (like inferring friction from trajectory) or when dealing with noisy data. Classical solvers often break down with noisy input, whereas the neural network can act as a natural regularizer to smooth it out while adhering to the physics.",
              "score": 2,
              "created_utc": "2026-01-19 04:58:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0k576i",
                  "author": "inmadisonforabit",
                  "text": "I do agree that classical solvers can break down when the input is noisy.\n\nWhat I'm curious about are the conditions in which this approach outperforms a standard neural network. I think it's an interesting approach, and similar to something I've implemented before.\n\nGenerally speaking, if you are measuring a physical system, one would hope the amount of anomalies beyond noise are substantially less than \"good\" or precise measurements. In that case, a neural network, given enough data, would basically average out those anomalies assuming one doesn't overfit, especially with regularization. To me, it looks like your proposal is basically regularization. \n\nSo in what situations would a PINN perform better that a typical neural net?",
                  "score": 1,
                  "created_utc": "2026-01-19 22:21:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0h5pj0",
              "author": "nickpsecurity",
              "text": "I believe a lot of people also don't know about those. They read about NN's all the time due to the AI bubble (err marketing investments). It's why I'm trying to promote in AI spaces both old school techniques and mixing them with AI.\n\nBtw, what's the best, open-source solvers for ODE's or PDE's?",
              "score": 0,
              "created_utc": "2026-01-19 13:58:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0k5opz",
                  "author": "inmadisonforabit",
                  "text": "That's a good question. I don't often encounter the need for solvers, but in my experience, it usually depends on the application. Solvers seems to be often built for specific applications.\n\nI generally just use MATLAB's solvepde for general problems. Otherwise, if you're in industry, you'll probably come across Ansys.",
                  "score": 2,
                  "created_utc": "2026-01-19 22:24:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0rf0pc",
          "author": "SadEntertainer9808",
          "text": "This is very interesting, but I'm getting a bit hung up on something: you're fitting a network to solutions to a known ODE. There's obviously some cool out-of-the box smoothing you get, but you're sort of losing the conventional advantage of a NN (approximation of an unknown function). I'd want to see outperformance of a conventional ODE solver in some dimension before I got excited about this.\n\nHowever, it is cool to see information underivable from the training data being burned in to the NN. It's provocative.",
          "score": 1,
          "created_utc": "2026-01-20 23:39:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjozwj",
      "title": "how do you move past toy machine learning projects?",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qjozwj/how_do_you_move_past_toy_machine_learning_projects/",
      "author": "TeedyDelyon",
      "created_utc": "2026-01-22 08:18:59",
      "score": 50,
      "num_comments": 8,
      "upvote_ratio": 0.98,
      "text": "i am comfortable with sklearn, basic neural nets, and common workflows, but everything i build feels like a demo. how do you start working on projects that feel closer to real use cases? what changed for you when you hit that point?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qjozwj/how_do_you_move_past_toy_machine_learning_projects/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o10iz00",
          "author": "burntoutdev8291",
          "text": "Use real life unclean data, usually your local government should publish some stats.",
          "score": 27,
          "created_utc": "2026-01-22 08:35:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10jgh7",
          "author": "RefrigeratorFirm7646",
          "text": "In real world ML, you dont always have clean kaggle datasets... try writing a data scraper, then write an algorithm to clean up the \"edge case\" data.\n\nNext, try to optimize your models, then optimize them again, and again, until you are milking for milliseconds of performance. This will teach you a lot about the underlying logic of the frameworks you might be using.\n\nKeep pushing the boundaries of your knowledge with every new project that you start, you cannot just \"skip\" making toy projects, you have to actively \"outgrow\" them.\n\nI hope my advice helps...",
          "score": 13,
          "created_utc": "2026-01-22 08:40:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10phnk",
          "author": "Aidalon",
          "text": "1. Sometimes you donâ€™t have the capacity to evaluate performance. The domain knowledge is so complex that it requires experts you donâ€™t have to label data or define targets. So the capacity to assess becomes very hard. \n\n2. Explainability, if some fields you cannot use a model unless you can audit it. Why did it choose this and not that. \n\n3. Metric biases and data bias. This is a big one. \n\n4. Data centric ai. \n\n5. Active learning approach\n\n- Also quick question, when you want to solve a problem, what do you chose first, the metric or the models and why ?\n\nIf your answer is the model, you still have to learn the processus of solving problems.\n\n- What is train, test validation ? \n\n- At which step do you split dataset into train test validation?\n\n- do you know why some models will require normalization, which will not require it?\n\n- how do you handle feature reduction, correlations in the data?\n\n- do you know which models does classification which does regression? (Many does both) SVM for example can be used for regression, same for decision tree. \n\n- Between two classification model, do you understand how the boundary are made ? What the actual difference between logistic regression and SVM classification in the boundary decision ?\n\nThere are other questions. \n\nWhen presented with a problem, you have many options to choose from. What defines a good project is how you will answer many aspects of how you treat data, what metrics you chose, which models to try for. And then there is the actual use of those models in real cases. Will you experience model drift ? Etc. \n\nToy project a very simple in nature because they do not adresse many of those questions.",
          "score": 6,
          "created_utc": "2026-01-22 09:37:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12jtv6",
              "author": "luffygrows",
              "text": "Why is choosing the model first before the metric a bad thing? What if i wanna test specific models in specific scenarios? Imo it doesnt matter when u do what. I think lots of people dont define their metric correctly or assume perfect dataset and not long running processes, edge cases and all that. Or i misunderstood what u meant with that specific point",
              "score": 1,
              "created_utc": "2026-01-22 16:24:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o10rn55",
          "author": "mandevillelove",
          "text": "Start small with real world data problems or collaborations, making an impact changes it from toy to actual use.",
          "score": 2,
          "created_utc": "2026-01-22 09:57:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10vq0q",
          "author": "Holiday_Lie_9435",
          "text": "A big shift for me was focusing on industry-specific problems. Instead of just using standard datasets, I started looking at real-world data related to specific fields (like finance, healthcare, or manufacturing). This immediately makes the projects feel more relevant, and datasets are easy to find too! For example, this list of [AI/ML project ideas](https://www.interviewquery.com/p/ai-project-ideas) from Interview Query can help you get started. It categorizes projects not only by skills but also by domain, like doing a disease prediction model for healthcare or a trading bot for finance. Most projects also has the datasets linked, but you can also find them on government websites.",
          "score": 1,
          "created_utc": "2026-01-22 10:35:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10wt1j",
          "author": "divided_capture_bro",
          "text": "Try building a tool you can actually use.",
          "score": 1,
          "created_utc": "2026-01-22 10:44:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11l9sy",
          "author": "DataCamp",
          "text": "Typically this shift kind of jumps you when the problem starts to matter more than the model.\n\nToy projects usually stop at â€œtrain a model and report accuracy.â€ Real-feeling projects force you to deal with messy data, unclear targets, and uncomfortable questions like what happens when this is wrong or who pays for the mistakes. You end up spending more time on evaluation, edge cases, and iteration than on model selection.\n\nA good signal youâ€™re moving past demos is when youâ€™re thinking about things like thresholds, failure modes, drift, and how youâ€™d explain results to someone who isnâ€™t technical.",
          "score": 1,
          "created_utc": "2026-01-22 13:35:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjc5xm",
      "title": "Information theory fundamentals for Machine Learning",
      "subreddit": "learnmachinelearning",
      "url": "https://v.redd.it/96e7jgwr1seg1",
      "author": "Big-Stick4446",
      "created_utc": "2026-01-21 22:16:56",
      "score": 48,
      "num_comments": 3,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qjc5xm/information_theory_fundamentals_for_machine/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o113zd0",
          "author": "Grumlyly",
          "text": "Very cool! Can we play online with it? Do you a link please?",
          "score": 2,
          "created_utc": "2026-01-22 11:44:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11y15e",
              "author": "Big-Stick4446",
              "text": "[Tensortonic](https://tensortonic.com)",
              "score": 2,
              "created_utc": "2026-01-22 14:42:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zoeva",
          "author": "whiteorb",
          "text": "Pretty rad",
          "score": 1,
          "created_utc": "2026-01-22 04:29:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhfh2a",
      "title": "Learning ML is clear but applying it to real problems feels overwhelming",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qhfh2a/learning_ml_is_clear_but_applying_it_to_real/",
      "author": "Waltace-berry59004",
      "created_utc": "2026-01-19 20:19:51",
      "score": 45,
      "num_comments": 12,
      "upvote_ratio": 0.93,
      "text": "Courses and tutorials make sense, but once I try to apply ML to a real problem, everything explodes: data quality, problem definition, deployment, and user needs.\n\nIâ€™m not trying to publish papers, I want to build something useful. How do beginners move from I understand the algorithms to this actually solves a problem?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qhfh2a/learning_ml_is_clear_but_applying_it_to_real/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o0jyulc",
          "author": "RickSt3r",
          "text": "You donâ€™t, you find a problem then design a solution. Sometimes itâ€™s a ML sometimes itâ€™s good old fashion software development.",
          "score": 26,
          "created_utc": "2026-01-19 21:51:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jh6q2",
          "author": "Password-55",
          "text": "Yes, that is why I think AI is too hyped right now. Making actual useful products is key and I think itâ€˜s quite hard, if not sometimes the wrong tool in many applications.\n\nI do not have the answer. Tell me if you find it.",
          "score": 10,
          "created_utc": "2026-01-19 20:25:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kfbec",
          "author": "Commercial_Note_210",
          "text": "> How do beginners move from I understand the algorithms to this actually solves a problem?\n\nDepends on your area of work, but largely it's flipped. You have a business problem and you work backwards towards a solution.\n\nSomething like... \"I want personalized recommendations on this page -> I have access to the users, items. I can build a label from clicks. I can build feature profiles for users, items -> I can frame this as a (user, item, context, label) recommendation task -> there's a bunch of approaches; next click prediction, CTR prediction, etc -> stop and do research and decide on exact problem framing -> do some research on modeling approaches -> learn about FTTransformer, AutoInt, XGBoost, SLIM, etc -> build a model -> iterate\"\n\nHowever, if \"there's a bunch of approaches\" lands on something like binary click prediction, Ill just use my textbook knowledge and just chuck AutoGluon or XGBoost at it, but as you iterate youll learn more modern approaches, if appropriate.",
          "score": 5,
          "created_utc": "2026-01-19 23:13:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kj2b0",
          "author": "Jaded_Individual_630",
          "text": "That's because there's a cottage industry that benefits from \"boot camping\" people with toolchains and tech stacks. This requires them to minimize the complexity of the task in all of their materials.Â \n\n\nReal cutting edge ML is never going to come off the back of \"import py-machinelearningplease\" and a $5 udemy course.Â \n\n\nNot saying that's what you did, but that is a big predatory industry.",
          "score": 5,
          "created_utc": "2026-01-19 23:33:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0l6dhl",
          "author": "Important_Sundae1632",
          "text": "Have you looked at Kaggle competitions? Theyâ€™re full of concrete examples showing how ML is actually applied.\n\nWhat youâ€™re describing is already most of real-world ML: defining the problem, cleaning data, deploying. You donâ€™t need a complex model to start, and most effort usually goes into those steps anyway. Keep the scope small, start with the simplest approach that works, and walk through one complete example to see how everything connects. If you have a specific problem in mind, feel free to share it or DM me",
          "score": 2,
          "created_utc": "2026-01-20 01:40:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kz21d",
          "author": "Palmquistador",
          "text": "Amen to that.",
          "score": 1,
          "created_utc": "2026-01-20 00:59:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0lig1n",
          "author": "Ty4Readin",
          "text": "Machine learning is a lot more than just understanding the algorithms. I think understanding the algorithms is ironically one of the least practically useful skills that you need.\n\nIf you have decent stats, CS, and domain knowledge then everything becomes a lot more straightforward. \n\nIf you run into problems, it's often because you are lacking in one of these areas, and learning more about the algorithms behind some models won't help much unfortunately.\n\nFor example, problem formulation? Thats entirely stats & domain knowledge.\n\nDeployment? Almost entirely CS.\n\nUser workflow/needs? Mostly domain knowledge.\n\nThe best way to learn is to actually try to solve problems as side projects, and make the problems that you actually care about personally.",
          "score": 1,
          "created_utc": "2026-01-20 02:46:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0lydfb",
          "author": "Dark-Maverick",
          "text": "If you want to practice to build confidence and put projects in resume, then pick datasets from kaggle and practice it.\n\nThink of anything whatever you like football - there is a football dataset available, cricket - cricket dataset available, tv shows dataset ,movie dataset games dataset these are available just for starting building some projects.\n\nAfter that find innovative ideas explore different datasets on internet and try to find solutions.",
          "score": 1,
          "created_utc": "2026-01-20 04:16:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0m4gxm",
          "author": "Adept_Carpet",
          "text": "Data management is often where 90% of the effort is spent and that's assuming the data you need exists. It's generally futile if the organization wasn't managing data well to begin with.\n\n\nThen model deployment (and everything post-deployment like monitoring and debugging and updating and security) is really an unsolved problem. It reminds me of how web app deployment was 20+ years ago. Editing the production site with vi, having to recompile the webserver, old versions of files scattered everywhere, no backups, etc.",
          "score": 1,
          "created_utc": "2026-01-20 04:55:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0m8ese",
          "author": "patternpeeker",
          "text": "This feeling is normal, because tutorials hide the hardest parts on purpose. In real projects the algorithm is usually the easy piece, and everything around it is where time goes. What helps is scoping problems way down, pick something where a dumb baseline is acceptable and iterate from there. Focus on data first, then evaluation that matches what a user actually cares about, even if it is a rough metric. Deployment and feedback loops can be ugly and manual at the start, and that is fine. You get comfortable by shipping small, imperfect things and seeing where they break.",
          "score": 1,
          "created_utc": "2026-01-20 05:22:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0merqp",
          "author": "AccordingWeight6019",
          "text": "This feeling is very common, and it is usually the gap between toy problems and real systems that shows up. Most real ML work is not about the algorithm at all, but about turning a vague goal into a concrete, testable question and accepting imperfect data. One way to reduce the overwhelm is to start by stripping the problem down to the smallest version that would still be useful, even if it feels almost trivial. Build something that works end to end, even if it is naive, and then iterate. Over time, you realize the messiness is the work, not a sign that you are doing it wrong.",
          "score": 1,
          "created_utc": "2026-01-20 06:10:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0thvlz",
          "author": "stairwayfromheaven",
          "text": "Iâ€™ve noticed some studios like thedreamers.us focus on helping people frame ML problems around real business or user needs, which can be helpful when youâ€™re past theory but unsure how to apply it in practice.",
          "score": 1,
          "created_utc": "2026-01-21 07:46:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfkal8",
      "title": "Which subfields of ML can I realistically achieve PhD level mastery of by self study at home with limited budget?",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qfkal8/which_subfields_of_ml_can_i_realistically_achieve/",
      "author": "Proof-Bed-6928",
      "created_utc": "2026-01-17 18:08:35",
      "score": 43,
      "num_comments": 42,
      "upvote_ratio": 0.74,
      "text": "Suppose you have somehow managed to generate 25k disposable income and only work 20hours a week so you have plenty of free time. You want to dedicate the remaining time to the mastery of one small but important ML niche just for the sake of it. To the level where you can theoretically waltz into a room full of FAANG level ML engineers and impress them with your contributions.\n\nIt will have to be a subfields where your competitive advantage plateaus with capital after some number (so not some compute arms race like LLM). \n\nWhich subfields in ML is this possible? What kind of benchmarks can you use to validate? How do you know youâ€™ve learned something without being in a university surrounded by academics?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qfkal8/which_subfields_of_ml_can_i_realistically_achieve/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o07bjun",
          "author": "notsofastaicoder",
          "text": "I think you are over estimating FAANG level ML engineers, and you don't need to a Phd to impress people either.\n\nSince most of these engineers do applied engineering, they are looking for solutions to current problems.\n\nMy experience has been in applied software engineering, worked in FAANG and AI startups. So advice from a non PhD guy:\n\nAfter you establish the base skills for ML, focus on an area that truly excites you, then make a meaningful contribution. Either by proving a hypothesis, or making an incremental improvement to existing solutions.\n\nBeing able to do that, in your own time, is really impressive.",
          "score": 62,
          "created_utc": "2026-01-18 00:24:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09evq5",
              "author": "belabacsijolvan",
              "text": "also [https://www.kaggle.com/competitions](https://www.kaggle.com/competitions)",
              "score": 4,
              "created_utc": "2026-01-18 08:43:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0dqr71",
              "author": "ChillmanITB",
              "text": "read an academic paper everyday, lots of books on theory, do all the free courses on calculus and linear algebra, probability and statistics (This is where you will develop an actual understanding), learn to code if you have not already and try and come up with new methods of improving current algorithms or create new ones all together.",
              "score": 3,
              "created_utc": "2026-01-18 23:38:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0zqbgy",
              "author": "taichi22",
              "text": "Honestly it depends more on the room youâ€™re talking about cause some FAANG level engineers are basically impossible to impress â€” theyâ€™ve seen it all. A lot of FAANG engineers, in addition, are really more impressed by real stuff youâ€™ve built rather than pure research. Thereâ€™s some overlap there â€” if your research artifacts are commonly used as state of the art industrial benchmarks then your name will carry weight, but at that point youâ€™ll probably be leading a team of FAANG engineers. \n\nAnyways, to OOP your benchmarks and directions are sort of misaligned, to be honest. As the saying goes, â€œhow do you climb a mountain? Why, one step at a time, of course.â€",
              "score": 1,
              "created_utc": "2026-01-22 04:42:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o06dtfy",
          "author": "williamkotoco_",
          "text": "anything related to efficient ml techniques such as pruning, quantization,  nas, efficient architectures and edge ai in general",
          "score": 19,
          "created_utc": "2026-01-17 21:32:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06slmz",
              "author": "Altruistic_Basis_69",
              "text": "My PhD was in DL optimization and I agree with this (though I am a little biased tbf). Compared to other niches, Iâ€™d argue that architecture optimization is one of the more intuitive areas to look into as opposed to some of the faster growing fields where you wonâ€™t even get a chance to catch-up.",
              "score": 9,
              "created_utc": "2026-01-17 22:45:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o06gpd8",
              "author": "Annual-Salamander-85",
              "text": "These are all pretty deep, why do you say that?",
              "score": 3,
              "created_utc": "2026-01-17 21:46:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0zqhem",
              "author": "taichi22",
              "text": "I do have to agree here on this point that I was suitably impressed by the work done on RF-DETR.",
              "score": 1,
              "created_utc": "2026-01-22 04:43:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o077s6k",
          "author": "Playful-Score-67",
          "text": "PhD level? It takes 5+ years of intense studying, independent research, mentoring from someone who is an expert on the field and contact with an academic community that can support and answer questions. It's not only about \"self study\".\nSo, realistically? Nah.",
          "score": 42,
          "created_utc": "2026-01-18 00:04:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o08wtuv",
              "author": "BreadBrowser",
              "text": "Wellâ€¦ I donâ€™t think I learned a single god damned thing from my advisor. Fuck that guy. Why I didnâ€™t leave Iâ€™ll never know. But yeah, you need a good five intense years.",
              "score": 4,
              "created_utc": "2026-01-18 06:04:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o09d6yl",
                  "author": "digiorno",
                  "text": "Thatâ€™s a tragedy, if someone wants to be an advisor then they should take that role seriously and genuinely try to improve the next generation of researchers.",
                  "score": 4,
                  "created_utc": "2026-01-18 08:27:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o08nsh8",
              "author": "arihoenig",
              "text": "Nonsense, Faraday was self taught as was Galileo and Joule. All of their contributions are PhD level in the fields in which they contributed.",
              "score": -3,
              "created_utc": "2026-01-18 04:57:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o08tzoo",
                  "author": "RatKnees",
                  "text": "Ah yes, people born 200+ years ago. \n\nI don't disagree that anyone can make a contribution, but listing people from pre-electricity feels disingenuous",
                  "score": 21,
                  "created_utc": "2026-01-18 05:42:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0zqyiy",
                  "author": "taichi22",
                  "text": "This is comparing apples to oranges. Nobody in this thread is as brilliant as Joule or Faraday â€” people like that are so different than you or I that the paths that they walk canâ€™t even serve as guidance for us. It would be like asking Einstein how to become a professor; thereâ€™s nothing useful to really be gleaned from his answer because the ways he got there are inaccessible to us. \n\nThose kinds of people wonâ€™t be on reddit asking how to do it â€” they just do it, because they are the type of people who cannot imagine doing anything else. You and I and many others in this subreddit may qualify as â€œgiftedâ€ or â€œregular geniusesâ€, but the gulf between people like us and people like Faraday or Joule is as wide â€” probably wider â€” than the gulf between us and an average person.",
                  "score": 1,
                  "created_utc": "2026-01-22 04:46:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o07b6gd",
          "author": "nickpsecurity",
          "text": "Any of the older techniques that are still widely used in industry. Then, smaller NN's, GA's, and LLM's. Also, techniques for things like time series and tabukar data which are highly important in industry but get little press or investment like GPT. That's the answer to your title.\n\nFar as FAANG, this [person](https://www.trybackprop.com/blog/top_ml_learning_resources) shares what they studied to do something like that.",
          "score": 9,
          "created_utc": "2026-01-18 00:22:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0awr63",
              "author": "iamevpo",
              "text": "What is a GA here?",
              "score": 1,
              "created_utc": "2026-01-18 15:22:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0bde7w",
                  "author": "NightmareLogic420",
                  "text": "Genetic Algorithms, I believe",
                  "score": 3,
                  "created_utc": "2026-01-18 16:42:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0btu0u",
                  "author": "nickpsecurity",
                  "text": "Genetic algorithms. They call the broader field evolutionary algorithms because they claim to use a highly-simplifies version of its principles. It's an optimization technique capable of working with non-differentiable data without getting stuck in local minima or maxima. It has heavier computation, though.\n\nIIRC, the top methods are differentiable evolution (in SciPy, too) and evolution strategies. If you use GA's, try tournament selection with tournament size of 7. Coevolution, or fitness tests evolving with the population, historically outperformed static, fitness measures. Such methods can be combined with NN's (neuroevolution) to find architectures, weights, hyperparameters, etc.",
                  "score": 2,
                  "created_utc": "2026-01-18 18:00:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o06rxw3",
          "author": "seriousgourmetshit",
          "text": "I don't think its possible to reach the level you are thinking of tbh",
          "score": 27,
          "created_utc": "2026-01-17 22:42:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o08n6md",
              "author": "arihoenig",
              "text": "George Boole was an autodidact. LLMs are just large assemblies of the logic he proposed.",
              "score": 3,
              "created_utc": "2026-01-18 04:53:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0b1060",
                  "author": "jonsca",
                  "text": "Found the guy who knows nothing about nothingÂ ",
                  "score": 4,
                  "created_utc": "2026-01-18 15:43:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o08ks4e",
          "author": "Complex_Medium_7125",
          "text": "a PhD teaches you how to do novel research, is that what you want to do?  \nif not, taking some grad courses in ml from stanford/berkeley/cmu gets you the foundations of solid ml engineer level\n\ncheck out [https://stanford-cs336.github.io/spring2025/](https://stanford-cs336.github.io/spring2025/) and do the homeworks",
          "score": 5,
          "created_utc": "2026-01-18 04:36:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06x83c",
          "author": "UngratefulSheeple",
          "text": "Thatâ€™s not gonna happen bro ðŸ˜‚ðŸ˜‚\n\nSincerely, someone doing a phd in ai right now.",
          "score": 8,
          "created_utc": "2026-01-17 23:09:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o09cfu5",
          "author": "digiorno",
          "text": "For actual ML research and not just application, you need math skills. And sure those can be self taught but it could be incredibly difficult to get good at without the guidance of professors. \n\nSecondly for cutting edge stuff, a lot of that so done via collaborative efforts with other theorists in an academic setting.\n\nIf you get good enough at the math and have some novel ideas then a school or research would likely give you admittance to a PhD program so that you could work with them, learn from them and join their ranks. And they would pay you a bit more than the amount you mentioned.",
          "score": 2,
          "created_utc": "2026-01-18 08:20:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0e9h02",
          "author": "mpaes98",
          "text": "Iâ€™ll be real with you, a PhD is basically self studying while also having to juggle 1-2 exploitative jobs where you report to different tiers of narcissistic individuals who have no proper management experience, and donâ€™t even get me started on how bad teaching duties have gotten (overbloated classes, unprepared undergrads/grads).\n\nI had a decent experience, but itâ€™s a rough time and now weâ€™re in a terrible market.",
          "score": 2,
          "created_utc": "2026-01-19 01:18:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o07fihm",
          "author": "strangeanswers",
          "text": "Iâ€™d argue that this is possible in the operationalization of LLMs to address business problems such as automated customer support, sentiment scraping or any other forms of information retrieval and programmatic decision-making. \n\nyou need a firm grasp of how LLMs work, strong software engineering technicals, LLMOps/DevOps, prompt & context engineering skills and good product reasoning. each of these skills is difficult to develop but Iâ€™d argue that itâ€™s feasible to become elite in this problem space through lots of self study and practice and very solid applied projects where you actually develop and deploy cloud infrastructure that can handle high throughput.",
          "score": 2,
          "created_utc": "2026-01-18 00:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05bvvp",
          "author": "tewmuchdrama",
          "text": "Reinforcement Learning",
          "score": -8,
          "created_utc": "2026-01-17 18:25:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o080svj",
              "author": "jsh_",
              "text": "?",
              "score": 1,
              "created_utc": "2026-01-18 02:39:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o05udrd",
              "author": "pb_syr",
              "text": "Why",
              "score": 0,
              "created_utc": "2026-01-17 19:52:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qf92tn",
      "title": "I implemented a GPT-style model from scratch using PyTorch to understand the math behind Attention & Fine-tuning (following Sebastian Raschka's book)",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qf92tn/i_implemented_a_gptstyle_model_from_scratch_using/",
      "author": "Bthreethree",
      "created_utc": "2026-01-17 09:39:19",
      "score": 41,
      "num_comments": 7,
      "upvote_ratio": 0.94,
      "text": "I've spent the last few weeks building a GPT-style LLM entirely from scratch in PyTorch to understand the architecture. This isn't just a wrapper; it's a full implementation covering the entire lifecycle from tokenization to instruction fine-tuning.\n\nI have followed Sebastian Raschka's 'Build a LLM from Scratch' book for the implementation, here is the breakdown of the repo:\n\n**1. Data & Tokenization (**`src/data.py`**)** Instead of using pre-built tokenizers, I implemented:\n\n* `SimpleTokenizerV2`: Handles regex-based splitting and special tokens (`<|endoftext|>`, `<|unk|>`).\n* `GPTDatasetV1`: A sliding-window dataset implementation for efficient autoregressive training.\n\n**2. The Attention Mechanism (**`src/attention.py`**)**\n\nI manually implemented `MultiHeadAttention` to understand the tensor math:\n\n* Handles the query/key/value projections and splitting heads.\n* Implements the **Causal Mask** (using `register_buffer`) to prevent the model from \"cheating\" by seeing future tokens.\n* Includes `SpatialDropout` and scaled dot-product attention.\n\n**3. The GPT Architecture (**`src/model.py`**)** A complete 124M parameter model assembly:\n\n* Combines `TransformerBlock`, `LayerNorm`, and `GELU` activations.\n* Features positional embeddings and residual connections exactly matching the GPT-2 spec.\n\n**4. Training & Generation (**`src/train.py`**)**\n\n* Custom training loop with loss visualization.\n* Implements `generate()` with **Top-K sampling** and **Temperature scaling** to control output creativity.\n\n**5. Fine-tuning:**\n\n* **Classification (**`src/finetune_classification.py`**):** Adapted the backbone to detect Spam/Ham messages (90%+ accuracy on the test set).\n* **Instruction Tuning (**`src/finetune_instructions.py`**):** Implemented an Alpaca-style training loop. The model can now handle instruction-response pairs rather than just completing text.\n\n**Repo:** [https://github.com/Nikshaan/llm-from-scratch](https://github.com/Nikshaan/llm-from-scratch)\n\nIâ€™ve tried to comment every shape transformation in the code. If you are learning this stuff too, I hope this reference helps!",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qf92tn/i_implemented_a_gptstyle_model_from_scratch_using/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o02wnfv",
          "author": "Bthreethree",
          "text": "This is the code snippet of the most interesting part - building Multi-head attention from scratch instead of using nn.MultiheadAttention.\n\n[https://github.com/Nikshaan/llm-from-scratch](https://github.com/Nikshaan/llm-from-scratch)\n\nclass MultiHeadAttention(nn.Module):  \ndef \\_\\_init\\_\\_(self, d\\_in, d\\_out, context\\_length, dropout, num\\_heads, qkv\\_bias=False): # context length is max sequence length for the mask  \nsuper().\\_\\_init\\_\\_()  \nassert d\\_out % num\\_heads == 0, \"d\\_out must be divisible by num\\_heads\"  \nself.d\\_out = d\\_out  \nself.num\\_heads = num\\_heads  \nself.head\\_dim = d\\_out // num\\_heads # dimension per head  \nself.W\\_query = nn.Linear(d\\_in, d\\_out, bias=qkv\\_bias)  \nself.W\\_key = nn.Linear(d\\_in, d\\_out, bias=qkv\\_bias)  \nself.W\\_value = nn.Linear(d\\_in, d\\_out, bias=qkv\\_bias)  \nself.out\\_proj = nn.Linear(d\\_out, d\\_out)  \nself.dropout = nn.Dropout(dropout)  \nself.register\\_buffer(\"mask\", torch.triu(torch.ones((context\\_length, context\\_length)) \\* float('-inf'), diagonal=1))  \n  \ndef forward(self, x):  \nb, num\\_tokens, d\\_in = x.shape  \nkeys = self.W\\_key(x)  \nqueries = self.W\\_query(x)  \nvalues = self.W\\_value(x)  \n  \nkeys = keys.view(b, num\\_tokens, self.num\\_heads, self.head\\_dim) # reshape for multi-head  \nqueries = queries.view(b, num\\_tokens, self.num\\_heads, self.head\\_dim)  \nvalues = values.view(b, num\\_tokens, self.num\\_heads, self.head\\_dim)  \n  \nkeys.transpose\\_(1, 2) # move head dimension to the front so that it is treated as batch dimension  \nqueries.transpose\\_(1, 2)  \nvalues.transpose\\_(1, 2)  \n  \nattn\\_scores = queries @ keys.transpose(2, 3) # flip last two dimensions for dot product  \nmask\\_bool = self.mask.bool()\\[:num\\_tokens, :num\\_tokens\\]  \nattn\\_scores.masked\\_fill\\_(mask\\_bool, -torch.inf)  \nattn\\_weights = torch.softmax(attn\\_scores / self.head\\_dim\\*\\*0.5, dim=-1)  \nattn\\_weights = self.dropout(attn\\_weights)  \ncontext\\_vec = (attn\\_weights @ values).transpose(1, 2).contiguous().view(b, num\\_tokens, self.d\\_out) # reshape back to original  \ncontext\\_vec = self.out\\_proj(context\\_vec) # final linear layer to mix heads  \nreturn context\\_vec",
          "score": 4,
          "created_utc": "2026-01-17 09:42:31",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o09fznv",
          "author": "chrisvdweth",
          "text": "Thanks for sharing. I have a few comments/questions:\n\n* Why does \\`attention.py\\` have so many classes? Some of them don't seem to be used meaningfully?\n* Not sure why you organized the code like that. For example, when importing \\`attention.py\\`, all the code gets executed. Why not do it cleaner cleaner with \\`if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":\\`?\n* Well, your \\`create\\_dataloader\\_v1\\` method seems to use a pretrained BPE tokenizer after all :). At least, I couldn't spot any code that trains a BPE tokenizer from scratch.",
          "score": 2,
          "created_utc": "2026-01-18 08:53:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0bjpd4",
              "author": "Bthreethree",
              "text": "Hey, the repo is an implementation of Sebastian Roschka's Build a LLM from scratch book, thus while learning and implementing from the book, I have made many classes which have improved further down in the file.\n\nYup I did forget to add \\`if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":\\` and will do that for sure! Thanks! :)",
              "score": 1,
              "created_utc": "2026-01-18 17:12:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o07h4dk",
          "author": "Bthreethree",
          "text": "I have added a colab notebook link in the readme of the repo on github to show the final results! The accuracy can be made better with experimentation of hyperparamaters & further fine-tuning.\n\n[https://github.com/Nikshaan/llm-from-scratch](https://github.com/Nikshaan/llm-from-scratch)",
          "score": 1,
          "created_utc": "2026-01-18 00:54:27",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o0b48se",
          "author": "Better_Pair_4608",
          "text": "How many parameters does your model have?",
          "score": 1,
          "created_utc": "2026-01-18 15:59:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0bjueb",
              "author": "Bthreethree",
              "text": "The model is trained over 124M parameters (inspired by GPT-2 architecture)",
              "score": 1,
              "created_utc": "2026-01-18 17:13:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0eta5p",
          "author": "Smergmerg432",
          "text": "This is awesomeâ€”thank you so much!",
          "score": 1,
          "created_utc": "2026-01-19 03:05:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qgod93",
      "title": "What is it really like to work as an ML/AI engineer?",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qgod93/what_is_it_really_like_to_work_as_an_mlai_engineer/",
      "author": "Ana_Karen98",
      "created_utc": "2026-01-18 23:53:20",
      "score": 37,
      "num_comments": 9,
      "upvote_ratio": 0.95,
      "text": "I graduated from university a couple of months ago. Since 2024, I've been working at a startup as a software development intern, and almost a year ago I was promoted to Junior ML/AI.\n\nI have two questions. First, why haven't I been working for months? I'm still getting paid because it's a small startup, and the person in charge of me is always busy, so no matter how many projects I ask or how much they promise me, I haven't received any since august. Supposedly, we're supposed to have our first in-person meeting on Monday after almost two years working there.\n\nIn the few projects I've worked on, my boss saw potential in me for AI/ML, but since I started university, I've always planned to work in web development, so my actual knowledge of AI/ML is limited, and it wasn't even something I had considered working in.\n\nI recently got access to a Udemy account and even bought some O'Reilly books on Humble Bundle. Is that enough? Is there a practical roadmap?I don't expect to learn it all in just a few months or week, but I do want to start exploring this field. I want to know what to expect and what skills are most in demand for junior professionals these days.\n\nI also hope to be able to change jobs eventually because, although this is a comfortable job, I want to advance and learn in my career. Unfortunately, in my contry there aren't many opportunities for entry-level positions, only for more advanced engineers (I'm not from the USA).\n\nI really want to learn because I HATE doing things poorly or half-heartedly, and I also don't want to pass up the opportunity to learn in this area even though it wasn't what I was looking for.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qgod93/what_is_it_really_like_to_work_as_an_mlai_engineer/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o0dukhh",
          "author": "MRgabbar",
          "text": "move and clean data mostly, frameworks take care of mostly everything",
          "score": 25,
          "created_utc": "2026-01-18 23:58:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0entjk",
              "author": "chrisvdweth",
              "text": "This what I often tell my students...thanks for the confirmation :).",
              "score": 2,
              "created_utc": "2026-01-19 02:37:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ffw4f",
          "author": "AccordingWeight6019",
          "text": "what youâ€™re describing is unfortunately pretty common at small startups, especially when there isnâ€™t a clear technical roadmap or dedicated mentorship. being titled â€œMLâ€ often just means youâ€™re nearby when someone thinks AI might be useful, not that thereâ€™s a steady stream of well-defined work. courses and books are a good start, but they only help if you pair them with concrete projects, even small ones you define yourself. for junior roles, the most valuable skills tend to be fundamentals, data handling, and the ability to ship something end to end, not cutting edge models. if you treat this period as time to build real examples of work rather than waiting for assignments, it will translate much better when you look elsewhere.",
          "score": 6,
          "created_utc": "2026-01-19 05:32:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0dxfpq",
          "author": "LeMalteseSailor",
          "text": "Data / ml / ai engineers have tons of overlap, so it depends on the job description. Some ml engineers are data engineers with some backend exp. Some ml engineers are backend engineers either some data experience.",
          "score": 5,
          "created_utc": "2026-01-19 00:13:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0fh1b8",
          "author": "patternpeeker",
          "text": "A lot of early â€œML/AIâ€ roles feel like this, especially at small startups where there is no real roadmap or data maturity. In practice, most ML engineers spend far more time waiting on data, clarifying vague goals, or being blocked by infra and priorities than training models. That is often why you are idle, not because you are doing something wrong. Courses and books are useful, but they only stick once you are trying to solve a concrete problem end to end, even a small one, with messy data and unclear success criteria. The skills that actually matter early on are solid software fundamentals, data handling, and understanding why a model breaks in production, not knowing every algorithm. If your current role is not giving you projects, I would treat it as paid time to build small applied projects and learn the tooling, while being realistic that many â€œjunior MLâ€ titles are closer to generic dev roles with an AI label.",
          "score": 5,
          "created_utc": "2026-01-19 05:41:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0fvyib",
          "author": "i_would_say_so",
          "text": "Sleep deprived",
          "score": 3,
          "created_utc": "2026-01-19 07:45:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g51fl",
          "author": "the_rat_from_endgame",
          "text": "there is NO clean roadmap say like in FrontEnd/Backend (like pick a framework and learn it)\n\nBe sure to know your supervised/unsupervised learning/hyperparameter tuning. then say data extraction is something you might learn on the job BUT won't hurt to know SQL (some window functions, grouping sets, aggregates, apart from the basics). Spark would be great too. I was a late bloomer when it came to spark and tbh, I know very little beyond the basics and I am aggressively relying on chatgpt for it (I do know Pandas a decent bit cause despite everything I feel comfy working the pipeline out in a notebook, reading a parquet/csv, the skeleton of the feature engineering, transfomrations, trainining, testing,persistence etc)\n\n\nAlso learn Gradient Boosting I ended up relying more on it than it I would have thought. I am working with some time series stuff (Bit new to it tbh) but trying out Gradient boosting there too (isolation forest) besides some other stuff (prophet for forecasting).\n\nAlso MLFlow, BEST to know of it, although I have been mostly using it as a boilerplate type thing.\n\n\nAnd above all, Try to write Good reusable CODE>",
          "score": 2,
          "created_utc": "2026-01-19 09:09:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0irug5",
          "author": "Keith_35",
          "text": "working in ML/AI can be a mix of excitement and frustration, as many spend a lot of time on data prep and dealing with unclear project goals rather than just model training.",
          "score": 1,
          "created_utc": "2026-01-19 18:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mqwec",
          "author": "Broad-Entrance5489",
          "text": "In my case I do a lot of infrastructure and deployment work. That means aws, terraform, CICD, kubernetes, etc. But I also do some normal data science: algorithms, models, data processing, etc.\n\nIt really depends by company, but I think you can think of an ml engineer as a mix between data scientist and cloud solutions architect? Let me know if you think otherwise",
          "score": 1,
          "created_utc": "2026-01-20 07:54:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qg28lm",
      "title": "Built a Multi-Source Knowledge Discovery API (arXiv, GitHub, YouTube, Kaggle) â€” looking for feedback",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qg28lm/built_a_multisource_knowledge_discovery_api_arxiv/",
      "author": "Appropriate_West_879",
      "created_utc": "2026-01-18 07:29:50",
      "score": 37,
      "num_comments": 1,
      "upvote_ratio": 0.95,
      "text": "Hey everyone,\n\nIâ€™ve just finished building an open-source project called Knowledge Universe API and pushed it to GitHub.\n\nItâ€™s a FastAPI-based backend that discovers and ranks educational / technical resources from multiple sources in a single request.\n\nWhat it does (purely technical)\n\nParallel crawling from:\n\narXiv\n\nGitHub\n\nYouTube\n\nKaggle (API-based)\n\nUnified response schema across all sources\n\nQuality scoring pipeline (difficulty alignment, freshness, accessibility, social signals)\n\nRedis-based caching with TTL + background refresh\n\nAsync orchestration with timeout isolation per crawler\n\nDeduplication + diversity filtering\n\nDocker + local Redis support\n\nWhy I built it\n\nI wanted a single API that returns ranked, clean, structured learning resources instead of manually searching each platform.\n\nThis was mostly a backend / systems exercise:\n\nasync pipelines\n\ncrawling reliability\n\nscoring consistency\n\ncache correctness\n\nStack\n\nPython 3.11\n\nFastAPI\n\nhttpx / asyncio\n\nRedis\n\nDocker\n\nPydantic v2\n\nRepo\n\nðŸ‘‰ GitHub: https://github.com/VLSiddarth/Knowledge-Universe.git\n\nWhat Iâ€™m looking for Open contribution to add new source collection from Internet to Create \"Knowledge Universe API\",\n\nCode review (especially async orchestration & scoring)\n\nArchitecture feedback\n\nAny obvious mistakes / improvements\n\nNot promoting anything â€” just sharing what I built and learning from feedback.\n\nThanks ðŸ™",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qg28lm/built_a_multisource_knowledge_discovery_api_arxiv/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o0flqt9",
          "author": "Appropriate_West_879",
          "text": "I could see More shares than Upvotes and Comments, Love to have contributions either as feature or financial help.\nThank you for your support ðŸ˜ŠðŸ™",
          "score": 1,
          "created_utc": "2026-01-19 06:18:31",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj5185",
      "title": "Variational Autoencoders Explained From Scratch",
      "subreddit": "learnmachinelearning",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1qj5185/variational_autoencoders_explained_from_scratch/",
      "author": "omunaman",
      "created_utc": "2026-01-21 17:57:07",
      "score": 28,
      "num_comments": 1,
      "upvote_ratio": 0.92,
      "text": "Let us start with a simple example. Imagine that you have collected handwriting samples from all the students in your class (100). Let us say that they have written the word â€œHello.â€\n\nNow, students will write the word â€œhelloâ€ in many different ways. Some of them will write words which are more slanted towards the left. Some of them will write words which are slanted towards the right.\n\nSome words will be neat, some words will be messy. Here are some of the samples of the words â€œhelloâ€.\n\nhttps://preview.redd.it/i90ibqodpqeg1.png?width=1100&format=png&auto=webp&s=7aa01508bec1e042075668367a1d4fca9f0d3524\n\nNow, let us say that someone comes to you and asks,\n\nâ€œGenerate a machine which can produce samples of handwriting for the word â€˜helloâ€™ written by students of your class.â€\n\n**HOW WILL YOU SOLVE THIS PROBLEM?**\n\nMedium Link for better readability:Â [https://vizuara.medium.com/variational-autoencoders-explained-from-scratch-365fa5b75b0d)\n\n# Part 1\n\nThe first thing that will come to your mind is: What are the hidden factors that determine the handwriting style?\n\nEach studentâ€™s handwriting depends on many hidden characteristics:\n\n* How much pressure they apply?\n* Whether they write slanted\n* Whether their letters are wide or narrow\n* How fast they write?\n* How neat they are?\n\nThese are not directly seen in the final image, but they definitely cause the shape of the letters.\n\nIn other words, every handwriting has a secret recipe that determines the final shape of the handwriting.\n\nFor example, this person writes slightly tilted, thin strokes, medium speed, moderate neatness.\n\nSo, the general architecture of the machine looks as follows:\n\nhttps://preview.redd.it/uqgc9oghpqeg1.png?width=1100&format=png&auto=webp&s=3f778396417bd47a7683bbb4feb340f038eafb44\n\nPress enter or click to view image in full size\n\nThis secret recipe is something which is called as the latent variable. Latent variables are the hidden factors that determine the handwriting style.\n\nThese variables are denoted by the symbol â€œzâ€.\n\nThe latent variables (z) captures the essence of how the handwriting was formed.\n\nLet us try to understand the latent variables for the handwriting example.\n\nLet us assume that we have two latent variables:\n\n1. One which captures the slantness\n2. One which captures the neatness of the handwriting\n\nhttps://preview.redd.it/tu14neiipqeg1.png?width=1100&format=png&auto=webp&s=9d895eec9ce079ac406920f723f7a6fe9ccad5aa\n\nFrom the above graph, you can see that both axes carry some meaning.\n\n* Words which are on the right-hand side are more slanted towards the right\n* Words which are on the left-hand side are more slanted towards the left\n\nAlso, words which are on the top or down are very messy.\n\nSo, we can see that every single point on this plane corresponds to a specific style of handwriting.\n\nIn reality, the distribution for all 100 students in your class might look as follows.\n\nhttps://preview.redd.it/lfju2oljpqeg1.png?width=1100&format=png&auto=webp&s=ebb517fe7261df811317527a668ab8b0f52fdd49\n\nWe observe that each handwriting image is compressed into just two numbers: slant and neatness.\n\nSimilar handwritings end up as nearby points in this 2D latent space.\n\nNow, let us feed this to our machine which generates the handwriting.\n\nhttps://preview.redd.it/duk9bj5lpqeg1.png?width=1100&format=png&auto=webp&s=b6b29ee897e8bd876b47cab0f4ed4d59f5a31276\n\nThere is another word for this machine, which is called the â€œdecoderâ€\n\nSo far, we have just used the word â€œdecoderâ€ to generate samples from the latent variables, but what is this decoder exactly and how are the samples generated?\n\n**Let us say, instead of generating handwriting samples our task is to generate handwritten digits.**\n\nAgain, we start with the same thinking process. What are the hidden factors that determine the shape of the handwritten digits?\n\nAnd we create a latent space with the latent variables.\n\nJust as before, let us assume that there are two latent variables.\n\nhttps://preview.redd.it/pgvrsjfopqeg1.png?width=990&format=png&auto=webp&s=e00ae9db48af29d0563e76976594decfd37899ee\n\nNow letâ€™s assume that we have chosen a point in the latent space which corresponds to the number 5.\n\nhttps://preview.redd.it/g0em62kqpqeg1.png?width=1016&format=png&auto=webp&s=04e8e663e9afed4aed792428f8d11c6315e603a6\n\nThe main question is, how do we generate the actual sample for the digit 5 once we pass this to the decoder?\n\nhttps://preview.redd.it/k18g411spqeg1.png?width=1100&format=png&auto=webp&s=997c8681401708c100d9959bd1d645eb011f6e12\n\nFirst, let us begin by dividing the image of the digit 5 into a bunch of pixels like follows.\n\nhttps://preview.redd.it/ec37v2xspqeg1.png?width=1100&format=png&auto=webp&s=80c1e30b206f38accfbee5d8267b4c5dad939533\n\nEach pixel corresponds to a number. For example, white pixels correspond to 1 and black pixels correspond to 0.\n\nhttps://preview.redd.it/fcbhf81upqeg1.png?width=1100&format=png&auto=webp&s=c8957b407a7d13e51646abee20b7c4830d4d527f\n\nSo it looks like all we have to do is output a number, either 0 or 1, at the appropriate location so that we get the shape 5.\n\nHowever, there is one drawback of this approach: with this approach, we will get a fixed shape 5 every time. We will not get variations of it.\n\nBut we do want to get variations of number 5. Remember in all the image generation applications, in the same prompt, we can get different variations of the image? We want exactly that.\n\nSo instead of outputting a single number, what if you could output a probability density?\n\nhttps://preview.redd.it/18mvsurvpqeg1.png?width=1100&format=png&auto=webp&s=f1214ddcd3b371a0400ec712baec4d8d3cfde335\n\nSo, the actual value of the pixel intensity becomes the mean, and we add a small standard deviation to it.\n\nLet us look at a simple visualization to understand this better.\n\n[https://www.youtube.com/watch?v=IztgtOYgZgE](https://www.youtube.com/watch?v=IztgtOYgZgE)\n\n# Part 2:\n\nOkay, we have covered one part of the story which explains the decoder.\n\nNow letâ€™s cover the second part so that we get a complete picture.\n\nIf you paid close attention to the first part, you will understand that we have made a major assumption.\n\nRemember when we talked about the handwritten digit 5, we said that let us assume that this part of the latent space corresponds to the digit 5.\n\nhttps://preview.redd.it/vla67zsxpqeg1.png?width=1068&format=png&auto=webp&s=08e36f62b1fd6d928aede990b90edbab11761684\n\nBut how do we know this information beforehand?\n\nHow do we know which part of the latent space to access to generate the digit 5?\n\nOne option is to access all possible points in the latent space, generate an image for it using our decoder distribution, and see which images match closely to the digit 5.\n\nBut this does not make sense. This is completely intractable and not a practical solution.\n\nWouldnâ€™t it be better if we knew which part of the latent space to access for the type of image we want to generate?\n\nLet us see if we build another machine to do that.\n\nhttps://preview.redd.it/q9f6haczpqeg1.png?width=1100&format=png&auto=webp&s=4c1da3b91e9bf2bbf80442d03b7d80b5f8e572c9\n\nIf we do this, we can connect both these machines together.\n\nhttps://preview.redd.it/4jtasza0qqeg1.png?width=1100&format=png&auto=webp&s=0f1200708e63063df1297d9db0c3f3fa547343e8\n\nThis â€œmachineâ€ is also called as the encoder\n\nHave a look at the video below, which explains visually why the encoder is necessary. It also explains where the word â€œVariationalâ€ in â€œVariational Autoencodersâ€ comes from.\n\nhttps://preview.redd.it/u9mrcig1qqeg1.png?width=1100&format=png&auto=webp&s=54b362cfa2714602bf1dc0ae619fa5adb5018600\n\nThese two stories put together form the â€œVariational Autoencoderâ€\n\nBefore we understand how to train the variation auto-encoder, let us understand some mathematics:\n\n# Formal Representation for VAEs\n\nIn VAEs we distinguish between two types of variables:\n\nObserved variables (x), which correspond to the data we see, and latent variables (z) (which capture the hidden factors of variation).\n\nThe decoder distribution is denoted as follows:\n\nhttps://preview.redd.it/4qjfndijqqeg1.png?width=56&format=png&auto=webp&s=06e19c83a76f06e49994cf20c7f7eee986b0f1ea\n\nThe notation reads: Probability of x given z.\n\nThe encoder distribution is denoted as follows:\n\nhttps://preview.redd.it/fvm3o0tlqqeg1.png?width=52&format=png&auto=webp&s=dce09ec13a40e4db5d973977dd1de5a0afbea342\n\n[](https://medium.com/plans?source=upgrade_membership---post_li_non_moc_upsell--365fa5b75b0d---------------------------------------)\n\nThe notation reads: Probability of z given x.\n\nThe schematic representation for the variational autoencoder can be drawn as follows:\n\nhttps://preview.redd.it/zjskkb0nqqeg1.png?width=1100&format=png&auto=webp&s=35f3c2eebd0beefad9933ba1f692aea6cce41da4\n\n# Training of VAEs\n\nFrom the above diagram, we immediately see that there are two neural networks: the encoder and decoder, which we have to train.\n\nThe critical question is, what is the objective function that we want to optimize in this scenario?\n\nLet us think from first principles. We started off with the objective that we want our probability distribution to match the true probability distribution of the underlying data.\n\nThis means that we want to maximize the following:\n\nThis makes sense because, if the probability of drawing the real samples from our predicted distribution is high, we have done a good job in modeling the true distribution.\n\nhttps://preview.redd.it/m33qnqioqqeg1.png?width=42&format=png&auto=webp&s=15bb9920b6ed9afef44e83bb7fb10333d65ac282\n\nBut how do we calculate the above probability?\n\nOkay, let us start by using the following formula:\n\nWe have looked at the same analogy in the visual animation which we saw before.\n\nhttps://preview.redd.it/kpf4fjspqqeg1.png?width=187&format=png&auto=webp&s=81df2a681c502c549706eea5b1ffaacd46188278\n\nIt essentially means that we look at all possible variations in the hidden factors and sum over all the probabilities over all these hidden factors.\n\nHowever, this is mathematically intractable.\n\nHow can we possibly go over every single point in the latent space and find out the probability of the sample drawn from that point being real?\n\nThis does not even make use of the encoder.\n\nSo now we need a computable training objective.\n\n# Training via the Evidence Lower Bound\n\nHave a look at the video below:\n\nThe idea is to find a term which is always less than the true objective, so if we maximize this term, our true objective also will be maximized.\n\nThe evidence lower bound is made up of two terms given below.\n\n**Note from my side: Ahh, itâ€™s been too long and Iâ€™m not able to add more images. Itâ€™s saying â€œunable to add more than 20 imagesâ€. I think thatâ€™s the limit. It would be great if you could go through the blog itself:** [https://vizuara.medium.com/variational-autoencoders-explained-from-scratch-365fa5b75b0d](https://vizuara.medium.com/variational-autoencoders-explained-from-scratch-365fa5b75b0d)\n\n**Term 1: The Reconstruction Term**\n\nThis term essentially says that the reconstructed output should be similar to the original input. Itâ€™s quite intuitive.\n\n**Term 2: The Regularization Term**\n\nThis term encourages the encoder distribution to stay as close as possible to the assumed distribution of the latent variables, which is quite commonly a Gaussian distribution.\n\nThe reason why the latent space is assumed to be Gaussian in my opinion is that we assume that all real-world processes have variables which have a typical value and they have extremes where the probability is generally less.\n\n# Practical example\n\nLet us take a real-life example to understand how the ELBO is used to train a Variational AutoEncoder.\n\nOur task is to train a variation autoencoder to predict the true distribution that generates MNIST handwritten digits and generate samples from that distribution.\n\nPress enter or click to view image in full size\n\nFirst, let us start by understanding how we will set up our decoder. Remember our decoder setup looks as follows:\n\nPress enter or click to view image in full size\n\nThe decoder is a distribution which maps from the latent space to the input image space.\n\nFor every single pixel, the decoder should give as an output the mean and the variance of the probability distribution for that pixel.\n\nPress enter or click to view image in full size\n\nHence, the decoder neural network should do the following:\n\nPress enter or click to view image in full size\n\nWe use the following decoder network architecture:\n\nPress enter or click to view image in full size\n\nOkay, now we have the decoder architecture in place, but remember we need the second part of the story, which is the encoder as well.\n\nOur encoder process looks something as follows:\n\nPress enter or click to view image in full size\n\nThe encoder tells us which areas of the latent space the input maps to. However, the output is not given as a single point;\n\nIt is given as a distribution in the latent space.\n\nFor example, the image 3 might map onto the following region in the latent space.\n\nPress enter or click to view image in full size\n\nHence, the encoder neural network should do the following:\n\nPress enter or click to view image in full size\n\nWe use the following encoder architecture:\n\nPress enter or click to view image in full size\n\nThe overall encoder-decoder architecture looks as follows:\n\nPress enter or click to view image in full size\n\nNow, let us understand how the ELBO loss is defined.\n\nRemember the ELBO loss is made up of two terms:\n\n1. The Reconstruction term\n2. The Regularization term\n\nFirst, let us understand the reconstruction loss.\n\nThe goal of the reconstruction loss is to make the output image look exactly the same as the input image.\n\nThis compares every pixel of the input with the output. If the original pixel is black and the VAE predicts white, the penalty is huge. If the VAE predicts correctly, the penalty is low.\n\nHence, the reconstruction loss is simply written as the binary cross-entropy loss between the true image and the predicted image.\n\nNow, let us understand the KL-Divergence Loss:\n\nThe objective of the KL divergence loss is to make sure that the latent space distribution has a mean of 0 and a standard deviation of 1.\n\nTo ensure that the mean is zero, we add a penalty if the mean deviates from zero. The penalty looks as follows:\n\nSimilarly, if the standard deviation is huge, the model is penalized for being too messy. Also, if the standard deviation is tiny, then also the model is penalized for being too specific.\n\nThe Penalty looks as follows:\n\nPress enter or click to view image in full size\n\nPress enter or click to view image in full size\n\nHere is the Google Colab Notebook which you can use for training:Â [https://colab.research.google.com/drive/18A4ApqBHv3-1K0k8rSe2rVOQ5viNpqA8?usp=sharing](https://colab.research.google.com/drive/18A4ApqBHv3-1K0k8rSe2rVOQ5viNpqA8?usp=sharing)\n\n# Training the VAE on MNIST Dataset:\n\nLet us first visualize how the latent space distribution varies with the iterations. Because of the regularization term, both distributions tend to move towards the Gaussian distribution centered around the mean of 0 and the variance of 1.\n\nPress enter or click to view image in full size\n\nWhen categorized according to the digits, the latent space looks as follows:\n\nPress enter or click to view image in full size\n\nSee the quality of the Reconstructions:\n\nPress enter or click to view image in full size\n\nSampling from the latent space:\n\nPress enter or click to view image in full size\n\n# Drawbacks of Standard VAE\n\nDespite the theoretical appeal of the VAE framework, it suffers from a critical drawback: it often produces blurry outputs.\n\nThe VAE framework poses unique challenges in the training methodology:\n\nBecause the encoder and decoder must be optimized jointly, learning becomes unstable.\n\nNext, we will study diffusion models which effectively sidestep this central weakness.\n\nThanks!\n\nIf you like this content, please check out our research bootcamps on the following topics:\n\n**GenAI**:Â [https://flyvidesh.online/gen-ai-professional-bootcamp](https://flyvidesh.online/gen-ai-professional-bootcamp)\n\n**RL**:Â [https://rlresearcherbootcamp.vizuara.ai/](https://rlresearcherbootcamp.vizuara.ai/)\n\n**SciML**:Â [https://flyvidesh.online/ml-bootcamp](https://flyvidesh.online/ml-bootcamp)\n\n**ML-DL**:Â [https://flyvidesh.online/ml-dl-bootcamp](https://flyvidesh.online/ml-dl-bootcamp)\n\n**CV**:Â [https://cvresearchbootcamp.vizuara.ai/](https://cvresearchbootcamp.vizuara.ai/)\n\n[](https://medium.com/tag/variational-autoencoder?source=post_page-----365fa5b75b0d---------------------------------------)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/learnmachinelearning/comments/1qj5185/variational_autoencoders_explained_from_scratch/",
      "domain": "self.learnmachinelearning",
      "is_self": true,
      "comments": [
        {
          "id": "o0yf0lq",
          "author": "El_Grande_Papi",
          "text": "The Medium article you linked is incorrect, just FYI.",
          "score": 2,
          "created_utc": "2026-01-22 00:08:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}