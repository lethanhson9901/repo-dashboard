{
  "metadata": {
    "last_updated": "2026-03-02 09:15:17",
    "time_filter": "week",
    "subreddit": "opencodeCLI",
    "total_items": 20,
    "total_comments": 320,
    "file_size_bytes": 341387
  },
  "items": [
    {
      "id": "1re6jhd",
      "title": "OpenCode launches low cost OpenCode Go @ $10/month",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/hpxb0za5dllg1.jpeg",
      "author": "jpcaparas",
      "created_utc": "2026-02-25 07:17:06",
      "score": 346,
      "num_comments": 139,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1re6jhd/opencode_launches_low_cost_opencode_go_10month/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7aduu4",
          "author": "justDeveloperHere",
          "text": "Will be cool to be an \"Open\" and show some limits numbers.",
          "score": 57,
          "created_utc": "2026-02-25 07:23:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7aegbh",
              "author": "toadi",
              "text": "Agree. At my company we use github for the models. They have premium requests. You can upgrade get x3 more requests. you just don't know what that means. never told how many there are to start with so I have no clue.",
              "score": 4,
              "created_utc": "2026-02-25 07:28:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7afgdv",
                  "author": "oplianoxes",
                  "text": "It clearly says that.it starts with 300, X3 is 900",
                  "score": 18,
                  "created_utc": "2026-02-25 07:37:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7azc0p",
                  "author": "rothnic",
                  "text": "GitHub is one of the few that is super transparent about it. It just has a monthly number rather than a time based reset during the month. Iirc it is 1200 requests per month, no matter how many tokens or tool calls it takes to complete the request.",
                  "score": 9,
                  "created_utc": "2026-02-25 10:42:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7b68f4",
                  "author": "keroro7128",
                  "text": "Pro=300, Pro + =1500",
                  "score": 4,
                  "created_utc": "2026-02-25 11:41:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7aeckm",
          "author": "jpcaparas",
          "text": "https://preview.redd.it/064i2a50fllg1.png?width=2534&format=png&auto=webp&s=32281eb52d6d4d022ee198a455434ac881e431c7\n\nJust these models for now:\n\n‚Ä¢ Kimi K2.5\n\n‚Ä¢ GLM-5\n\n‚Ä¢ MiniMax M2.5",
          "score": 60,
          "created_utc": "2026-02-25 07:27:48",
          "is_submitter": true,
          "replies": [
            {
              "id": "o7c4jt4",
              "author": "xmnstr",
              "text": "Solid choices from the Opencode team, I have to say.",
              "score": 16,
              "created_utc": "2026-02-25 15:04:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ck2gq",
                  "author": "jpcaparas",
                  "text": "Dax is a huge fan of K2.5. He's raved about it multiple times. I actually think it's his daily driver.",
                  "score": 9,
                  "created_utc": "2026-02-25 16:16:50",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7aizm8",
              "author": "AdamSmaka",
              "text": "they were free so far",
              "score": 8,
              "created_utc": "2026-02-25 08:10:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bdtzp",
                  "author": "SidneyBae",
                  "text": "The only free one is minimax 2.5 now, and the free one often hit limit ",
                  "score": 5,
                  "created_utc": "2026-02-25 12:36:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7akjfa",
                  "author": "wokkieman",
                  "text": "Exactly, context matters. It's nice to see there are some free an cheaper options. Every budget and purpose something. For some things Opus is really not required.",
                  "score": 8,
                  "created_utc": "2026-02-25 08:24:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7afsdz",
              "author": "GasSea1599",
              "text": "please provide link",
              "score": 3,
              "created_utc": "2026-02-25 07:40:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ageae",
                  "author": "jpcaparas",
                  "text": "You'll need to go through the Zen page first at [https://opencode.ai/zen](https://opencode.ai/zen), work your way to the billing section, and then you'll see Go. It doesn't seem to have its own standalone URL.\n\nStep by step guide here with some deets about the models: [https://reading.sh/opencode-go-gives-you-three-frontier-models-for-10-a-month-9fa091be6fd1?sk=fdc57ad14073b8a3f3d919a5d4b6cbcf](https://reading.sh/opencode-go-gives-you-three-frontier-models-for-10-a-month-9fa091be6fd1?sk=fdc57ad14073b8a3f3d919a5d4b6cbcf)\n\nhttps://preview.redd.it/88r3bvrbillg1.png?width=1192&format=png&auto=webp&s=b6ae8c49f62810db0f3ac7d5117f289ff7c35a86",
                  "score": 13,
                  "created_utc": "2026-02-25 07:46:28",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7b01ou",
              "author": "gnaarw",
              "text": "\"just\" üò≥",
              "score": 2,
              "created_utc": "2026-02-25 10:48:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7beujn",
                  "author": "One_Pomegranate_367",
                  "text": "I've been personally paying for all three, and I will gladly welcome canceling all three of those subscriptions. \n\nMain reason is because each model is only good at certain things, and when I pay for these subscriptions, they're much cheaper than Claude. ",
                  "score": 2,
                  "created_utc": "2026-02-25 12:42:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7f8f7l",
              "author": "saggassa",
              "text": "minimax is already free(i hope it stays like that)  \ni tried glm last weekend and was weird to use\n\n  \nminimax is doing great for me, oneshoting almost everything",
              "score": 1,
              "created_utc": "2026-02-25 23:52:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7adis1",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 23,
          "created_utc": "2026-02-25 07:20:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ajc7f",
              "author": "Sheepza",
              "text": "Indeed",
              "score": 2,
              "created_utc": "2026-02-25 08:13:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7apc1h",
          "author": "Huge-Refrigerator95",
          "text": "10$ is pretty cheap and good, but be clear about the number of requests even if they are low, I don't mind, just be clear, don't be like other tools that we're scared to use because we'll reach the limit before pressing enter",
          "score": 10,
          "created_utc": "2026-02-25 09:09:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7apoe1",
              "author": "Bob5k",
              "text": "this is the problem with ollama cloud aswell. it says there's 'some' 5h and weekly cap but they don't say what roughly it is. So at least some part of the market would just hesitate to subscribe because they just don't know what to expect.  \nFrom the other side 10$ is pretty cheap and running newest openweight models, sounds.. interesting limits-wise.",
              "score": 3,
              "created_utc": "2026-02-25 09:13:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7apv45",
                  "author": "Huge-Refrigerator95",
                  "text": "Of course, running a business is not easy at all, you'll have to be sure of the demand on the servers and maybe they need to have a \"priority\" pass during heavy load, I guess fireworks is there sponsor so maybe they want to return the favor but adding it to zen\n\n  \nI mean tell me you get 10 requests per hour much better than \"good\" usage!\n\n  \nBest of luck opencode, your forever supporters",
                  "score": 2,
                  "created_utc": "2026-02-25 09:14:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7as9uq",
          "author": "TreeBearr",
          "text": "yea okay I thought synthetic was good, for $10/month this is awesome!!\n\nI've been running it for the past hour and a half or so and am at 60% of the 5 hour rolling limit. The pay as you go api pricing is solid.\n\nhttps://preview.redd.it/maru4etu1mlg1.png?width=897&format=png&auto=webp&s=34f92c74bae1031dfc406e486f6003318db177e9\n\nInference is very nice, especially m2.5",
          "score": 17,
          "created_utc": "2026-02-25 09:37:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7axdx2",
              "author": "gkon7",
              "text": "Not looking good actually. You'll hit the monthly limit in 15 hours of your coding.",
              "score": 23,
              "created_utc": "2026-02-25 10:24:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7h5zy5",
                  "author": "TreeBearr",
                  "text": "Yea you're right, I don't think it's a good plan for someone doing a ton of serious coding. Though I might recommend it to someone who is new to the tools and wants to get started with opencode asap.\n\nSynthetic was my fav for a hot minute but the jury is still out on their new plans and they've been kinda slow to add the new models.\n\nu/Far_Commercial3963 mentioned Chutes which looks interesting tho",
                  "score": 2,
                  "created_utc": "2026-02-26 07:19:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7bg3lv",
                  "author": "HebelBrudi",
                  "text": "Still an insane subsidy over paying for token!",
                  "score": 1,
                  "created_utc": "2026-02-25 12:50:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7auesk",
              "author": "Professional-Cup916",
              "text": "24% weekly for 1.5 hours? Really?\nLooks terrible.",
              "score": 17,
              "created_utc": "2026-02-25 09:57:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7aydfr",
                  "author": "SOBER-128",
                  "text": "And already at 11% of the monthly quota. Going to run out of quota in just a couple of days at this rate.",
                  "score": 9,
                  "created_utc": "2026-02-25 10:33:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7b6qcf",
              "author": "ForeverDuke2",
              "text": "11% mothly usage in 1.5 hr is bad",
              "score": 7,
              "created_utc": "2026-02-25 11:44:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7izrvn",
              "author": "GarauGarau",
              "text": "How can I recover this information? Is it a plugin?",
              "score": 1,
              "created_utc": "2026-02-26 15:20:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lizvf",
                  "author": "TreeBearr",
                  "text": "The usage meters? In a browser login to [opencode.ai](http://opencode.ai) and go to Zen. Then it's under billing.\n\n  \nFor anyone else who's confused about where to sign up for the Go plan that's also where I found it.",
                  "score": 2,
                  "created_utc": "2026-02-26 22:29:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7l2aoi",
              "author": "HebelBrudi",
              "text": "I do actually like the trend that tools provide inference providers. There are so many slop providers. I am glad the bar gets raised for open weight.",
              "score": 1,
              "created_utc": "2026-02-26 21:08:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7avx49",
          "author": "Magnus114",
          "text": "Anyone have a feeling how the usage compares with claude 20 USD plan?",
          "score": 6,
          "created_utc": "2026-02-25 10:11:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p5qka",
              "author": "Realistic-Key8396",
              "text": "Well. Claude-plan resets every week. i burned 20% of my monthly quota on OpenCode GO last night in 6 hours. \n\n",
              "score": 2,
              "created_utc": "2026-02-27 13:37:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7aehpm",
          "author": "Jeidoz",
          "text": "https://preview.redd.it/nvgxfp82fllg1.png?width=1061&format=png&auto=webp&s=0dd939724ddc474963537e917d54ca79a943fe00\n\nIt does not says any numbers about limits... I personally feel that NanoGPT with 8$ would be better (providing same and extra/more models)...",
          "score": 16,
          "created_utc": "2026-02-25 07:29:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7aez6z",
              "author": "nonpre10tious",
              "text": "Yeah I wish they had transparent limits - as a side note, tool calling has always been buggy for me on nanogpt, making it difficult to use claude code or opencode. Hopefully this doesn‚Äôt fave that same pitfall",
              "score": 14,
              "created_utc": "2026-02-25 07:33:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ain1g",
                  "author": "HornyEagles",
                  "text": "Not to mention the inference is very slow too and is known to time out occasionally. Other than that the community is welcoming and limits are generous",
                  "score": 5,
                  "created_utc": "2026-02-25 08:06:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7anco5",
              "author": "alovoids",
              "text": "nanogpt is cheap but I can't bear the speed. too slow. I'm impatient üòî",
              "score": 3,
              "created_utc": "2026-02-25 08:51:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7arqtz",
              "author": "evnix",
              "text": "impossible to code with nanoGPT, not sure what it is, its like 30% requests get repeatedly sent to GPT-2 which is enough to kill coding exxperience, probably to save costs. but I wont complain, its nice for roleplay, minimal image generation for the low price, ¬†If you are looking for a NanoGPT referral link with ongoing discount like I was, you can use mine:¬†[https://nano-gpt.com/r/wdD9Gnti](https://nano-gpt.com/r/wdD9Gnti)",
              "score": 6,
              "created_utc": "2026-02-25 09:32:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7cznyi",
              "author": "RandsFlute",
              "text": "I just paid for the $8 yesterday to try it out, it is not worth it for opencode, at all\nTried it with kimi 2.5 thinking and glm 5, requests just failed, it wasn't even slow they just failed after a couple requests.\nTried the same conversation with zen kimi 2.5 and it worked flawlessly.\nI liked the idea of nanogpt because they clearly don't care about nsfw and I want to turn opencode into a slutty code assistant. But their service sucks for it. May be good for sillytavern but opencode is unusable there.",
              "score": 2,
              "created_utc": "2026-02-25 17:28:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7dribc",
                  "author": "ExcellentDeparture71",
                  "text": "u/RandsFlute so what do you recommend?",
                  "score": 1,
                  "created_utc": "2026-02-25 19:34:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7pc4s1",
              "author": "Bac-Te",
              "text": "NanoGPT got a ton of problems with tool calls it's not even funny, and it's slow as hell. I think they heavily quantized the models, making them more stupid and less able to deal with tool calls. But if you're into images and roleplays then that's a solid offer.",
              "score": 1,
              "created_utc": "2026-02-27 14:12:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ah1z5",
          "author": "verkavo",
          "text": "A reminder for the community - new models/vendors/plans usually provide the best bang for the buck, because they reserve capacity for the launch event. Get it while it lasts.",
          "score": 11,
          "created_utc": "2026-02-25 07:52:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ar2p4",
              "author": "geckothegeek42",
              "text": "That's not a good sign, glm-5 is basically lobotomized on go right now. It can't do anything. Infinite spirals, broken tool calls, garbled text. Atleast I haven't technically lost any money yet",
              "score": 6,
              "created_utc": "2026-02-25 09:26:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7asg4z",
                  "author": "Resident-Ad-5419",
                  "text": "I got the same feeling. The GLM inside the opencode go is nerfed compared to the GLM on the Z.AI.",
                  "score": 8,
                  "created_utc": "2026-02-25 09:39:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7gde4f",
                  "author": "SelectionCalm70",
                  "text": "How's the overall limit in go plan?",
                  "score": 1,
                  "created_utc": "2026-02-26 03:45:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7an0um",
          "author": "onafoggynight",
          "text": "What happened to open code black?",
          "score": 5,
          "created_utc": "2026-02-25 08:48:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7aq1bk",
              "author": "jpcaparas",
              "text": "![gif](giphy|6T5uDarA77yOBWS5Ii|downsized)",
              "score": 10,
              "created_utc": "2026-02-25 09:16:32",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7apzhw",
              "author": "InternalFarmer2650",
              "text": "Stuck in Whitelist hell, i subscribed like a month ago and have yet to get access / money billed on my card. \n\nSo i kinda wonder why they offer this new sub if they can't even whitelist the people that \"applied\" for the other one",
              "score": 2,
              "created_utc": "2026-02-25 09:16:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7b3c2i",
                  "author": "Outrageous_Style_300",
                  "text": "yep same üòÇ is there even a way to get off that waitlist? I never heard anything",
                  "score": 1,
                  "created_utc": "2026-02-25 11:17:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ap0y5",
          "author": "Resident-Ad-5419",
          "text": "So I got the subscription on my personal email after reading this thread. It was not appearing with the account that has my custom domain. Performance feels similar between the free models and their outputs. But at least the rate limiting seems a bit less aggressive so far. The free versions would rate limit faster.",
          "score": 4,
          "created_utc": "2026-02-25 09:06:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7asr4c",
          "author": "Resident-Ad-5419",
          "text": "I have a feeling the limits are around $4.5 for 5 hour rolling, $10-15 for weekly and $30-40 for monthly. Cannot confirm yet though, need to spend more time to figure out.  \n\\---  \nThe glm 5 version inside this model seems to be heavily nerfed (I'm assuming same for all other models). The same query given to the [Z.AI](http://Z.AI) Coding plan finished a response instantly while the one in Opencode Go just went into a thinking frenzy for minutes and wasted bunch of token.",
          "score": 4,
          "created_utc": "2026-02-25 09:42:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cxkv2",
          "author": "alexx_kidd",
          "text": "Can we use the API with Go in other apps also?",
          "score": 5,
          "created_utc": "2026-02-25 17:18:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aw8o0",
          "author": "klippers",
          "text": "I love opencode but wouldn't nanoGPT or Synthetic.ai subscription",
          "score": 3,
          "created_utc": "2026-02-25 10:14:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7grfc4",
              "author": "HenryTheLion_12",
              "text": "Nanogpt - too slow for coding. Synthetic - they changed their pricing structure yesterday. They are a good provider though.",
              "score": 1,
              "created_utc": "2026-02-26 05:20:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7h4f65",
                  "author": "klippers",
                  "text": "Oh that's great to know, thank you",
                  "score": 2,
                  "created_utc": "2026-02-26 07:05:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7axv0j",
          "author": "SOBER-128",
          "text": "https://preview.redd.it/5hg0maa2bmlg1.png?width=1155&format=png&auto=webp&s=1f37717b82d055f6d9224879c03c29c01ebfb916\n\nTried it. The rolling usage quota seems fine, but weekly and monthly limits are very restrictive. I'll run out of weekly/monthly quota after a couple of days with basically any kind of programming work.  \n  \nQuota usage seems to depend on the token count and the model's API usage price, not just on the number of requests. Requests with large contexts or more generated tokens deplete the quota faster. The requests show up in the Zen usage history as usual with some per-request costs. My request history page shows that I've used $1.38 worth of requests with the Go subscription, and I'm already at 6% of my monthly quota. This means for $10 per month I get the equivalent of around $20 in pay-as-you-go credit. Not sure if it's worth it.",
          "score": 3,
          "created_utc": "2026-02-25 10:29:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bkmod",
          "author": "thermal-runaway",
          "text": "What is up with all these subscription models and not having a reasonable middle tier? They‚Äôre either dirt cheap, $10-20, or $100+. I‚Äôm not making money off of my work, I just find it fun, so I can‚Äôt justify $100, but I exhaust my cheap subscriptions 3-5 days into the week. I‚Äôd happily pay someone $40-50 for a single plan that comfortably covers a week of casual use",
          "score": 3,
          "created_utc": "2026-02-25 13:18:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bupk1",
              "author": "mikkel01",
              "text": "Check out Github Copilot Pro+ ($39 per month)",
              "score": 2,
              "created_utc": "2026-02-25 14:14:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7f3rq1",
          "author": "0xDezzy",
          "text": "The GLM-5 model is HEAVILY nerfed to be honest. It's messing up on outputs as well as doing things in a very stupid manner. ",
          "score": 3,
          "created_utc": "2026-02-25 23:26:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hoic6",
              "author": "Less_Ad_1505",
              "text": "Confirm! Had some issues with GLM-5, but Kimi and MiniMax work well",
              "score": 2,
              "created_utc": "2026-02-26 10:15:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7fs771",
          "author": "jempezen",
          "text": "GLM5 est inutilisable pour l'instant. J'ai pris l‚Äôabonnement pour l'utiliser et la il part compl√©tement en vrille. J'ai √©t√© habitu√© √† lui via la version free et la version compl√®te de zai et j'√©tais compl√©tement satisfait. La lui donner acc√®s √† un projet en cours serait du suicide... ",
          "score": 3,
          "created_utc": "2026-02-26 01:42:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ikkh1",
              "author": "jempezen",
              "text": "GLM5 a nouveau fonctionnel mais avec ce plan il n'a pas la vision, c'est un mod√®le brider",
              "score": 2,
              "created_utc": "2026-02-26 14:03:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7r6r20",
                  "author": "Minimum_Industry_978",
                  "text": "mistral any good?",
                  "score": 1,
                  "created_utc": "2026-02-27 19:34:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7an4u1",
          "author": "alovoids",
          "text": "I'm having decent speed with kimi and minimax. haven't tried glm. hopefully they're 'quick' enough",
          "score": 2,
          "created_utc": "2026-02-25 08:49:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aq2fx",
          "author": "NickeyGod",
          "text": "Well the question is right now what is generous and also there is other providers with more models that are equal in price. I mean its fine for what it has 10$ is not really of an ask if you really want to support them in their efforts go for it its fair",
          "score": 2,
          "created_utc": "2026-02-25 09:16:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aufj8",
          "author": "foolsgold1",
          "text": "\"generous\".. wtf does that mean?",
          "score": 2,
          "created_utc": "2026-02-25 09:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dort9",
          "author": "Anticode-Labs",
          "text": "$20 gets you Gpt plus with codex",
          "score": 2,
          "created_utc": "2026-02-25 19:21:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7eelq5",
          "author": "Just_Lingonberry_352",
          "text": "So are these models hosted in the US? Where does it host the actual models from?",
          "score": 2,
          "created_utc": "2026-02-25 21:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bkto2",
          "author": "trypnosis",
          "text": "To be honest this is moot for me as I won‚Äôt use AIs hosted outside the US and/or EU.",
          "score": 2,
          "created_utc": "2026-02-25 13:19:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7dcbr7",
              "author": "Depart_Into_Eternity",
              "text": "Same",
              "score": 1,
              "created_utc": "2026-02-25 18:24:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7g8dj2",
              "author": "not_particulary",
              "text": "You worry about foreign intelligence?",
              "score": 1,
              "created_utc": "2026-02-26 03:14:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7nsz70",
                  "author": "trypnosis",
                  "text": "Does it not worry you?",
                  "score": 1,
                  "created_utc": "2026-02-27 06:55:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7an4f5",
          "author": "AGiganticClock",
          "text": "Very cool, these are great models. Will wait a bit to hear about limits and speed/ratelimits",
          "score": 1,
          "created_utc": "2026-02-25 08:49:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c20w6",
          "author": "Lost-Ad-2259",
          "text": "the rod of morality at its finest",
          "score": 1,
          "created_utc": "2026-02-25 14:52:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7e2i74",
          "author": "Permit-Historical",
          "text": "Is it possible to use it outside of opencode?",
          "score": 1,
          "created_utc": "2026-02-25 20:25:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ggy2e",
          "author": "gameguy56",
          "text": "Need qwen3.5 then I'm gonna jump right in",
          "score": 1,
          "created_utc": "2026-02-26 04:07:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7iotsz",
          "author": "MorningFew1574",
          "text": "Instead of coding specifically, it can be used for something like Openclaw?",
          "score": 1,
          "created_utc": "2026-02-26 14:26:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7j79kj",
          "author": "SuperElephantX",
          "text": "So no more free MiniMax M2.5?",
          "score": 1,
          "created_utc": "2026-02-26 15:55:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jan45",
          "author": "clad87",
          "text": "What about the MCP web\\_search and image\\_analysis servers?\n\n",
          "score": 1,
          "created_utc": "2026-02-26 16:11:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jbdhv",
              "author": "jpcaparas",
              "text": "I just have a synthetic.new search and minimax mcp do that for me. separate subs. minimax vision is quite good",
              "score": 1,
              "created_utc": "2026-02-26 16:14:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7jbmnw",
                  "author": "clad87",
                  "text": "I use minimax vision too but it's very slow, like 40s for a result with a prompt",
                  "score": 1,
                  "created_utc": "2026-02-26 16:15:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7k2hxj",
          "author": "Available_Pass_7155",
          "text": "Has anyone tried it? With that subscription, do you notice everything runs faster?\n\n",
          "score": 1,
          "created_utc": "2026-02-26 18:19:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kix0z",
          "author": "mdrahiem",
          "text": "GLM 5.0 is unreachable here.. I get an error",
          "score": 1,
          "created_utc": "2026-02-26 19:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pykr5",
          "author": "DecisionOk4644",
          "text": "I used it with the GetShitDone plugin:\n\nhttps://preview.redd.it/brrium1j82mg1.png?width=2164&format=png&auto=webp&s=444b49fcf210c1d40d21fa1f84567c28095579a6\n\nso far this is the consumption I got, used for almost 4-5 hours in Yolo mode and Kimi K2.5 model. Decent  tok/s and didn't get any error so I'd say good reliability as well. ",
          "score": 1,
          "created_utc": "2026-02-27 16:04:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7v3hwc",
          "author": "lunied",
          "text": "i literally just subbed to alibaba coding plan, which is $3 (discounted from 10) per month, includes qwen 3.5 plus, m2.5, k2.5 and glm 5",
          "score": 1,
          "created_utc": "2026-02-28 11:25:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aifvw",
          "author": "revilo-1988",
          "text": "Mehr Details zu den Limits und so und das Abo ist gebucht",
          "score": 0,
          "created_utc": "2026-02-25 08:05:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7be4j9",
          "author": "Swimming_Ad_5205",
          "text": "–≠—ç—ç—Ö \n–ï—â—ë –±—ã –∏—Ö —Ä—Ñ –æ–ø–ª–∞—Ç–∏—Ç—å ) –±—ã–ª–æ –±—ã –≤–æ–æ–±—â–µ –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ",
          "score": -1,
          "created_utc": "2026-02-25 12:37:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7d7rxq",
              "author": "Competitive_Ad_2192",
              "text": "go away, there‚Äôs no vodka here!",
              "score": 0,
              "created_utc": "2026-02-25 18:04:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7aneis",
          "author": "Less_Ad_1505",
          "text": "https://preview.redd.it/6eg3j3xytllg1.png?width=720&format=png&auto=webp&s=deb828f47263566e7934bd2e5d1b562ed8ddb777\n\n",
          "score": 0,
          "created_utc": "2026-02-25 08:51:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7e4t1a",
          "author": "No-Friend7851",
          "text": "Considering they literally built censorship right into their software ‚Äî so even on my local model it was wasting tokens checking if I'm writing \"bad\" code ‚Äî yeah, no thanks. Hope they go bankrupt.",
          "score": -1,
          "created_utc": "2026-02-25 20:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7al0mu",
          "author": "ImMaury",
          "text": "Problem is, open source models suck.",
          "score": -9,
          "created_utc": "2026-02-25 08:29:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ang34",
              "author": "mintybadgerme",
              "text": "Confidently, and massively, incorrect. :)",
              "score": 9,
              "created_utc": "2026-02-25 08:52:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7an8o2",
              "author": "alovoids",
              "text": "i think the keys are to be more patient and thorough :))",
              "score": 1,
              "created_utc": "2026-02-25 08:50:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rfwlzk",
      "title": "I have 2,004 AI skills installed. Here's how I reduced my startup context from ~80K tokens to ~255 tokens (99.7% reduction)",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rfwlzk/i_have_2004_ai_skills_installed_heres_how_i/",
      "author": "blacksiders",
      "created_utc": "2026-02-27 04:15:48",
      "score": 126,
      "num_comments": 37,
      "upvote_ratio": 0.91,
      "text": "I've been collecting skill packs for OpenCode/Claude Code and hit 2,004 skills across 34 categories (ai-ml, security, devops, game-dev, etc.).\n\nThe problem: AI agents use a¬†[3-level progressive disclosure system](https://opencode.ai/docs/skills)¬†to load skills. Level 1 loads the¬†`name`¬†\\+¬†`description`¬†of¬†**every**¬†skill into the system prompt at startup. With 2,004 skills, that's¬†**\\~80,000 tokens consumed before I even type a prompt**¬†\\- roughly 40% of a 200K context window.\n\n# The fix: SkillPointer\n\nIt's not a plugin or library. It's an¬†**organizational pattern**¬†that works with native skills:\n\n1. Move all 2,004 raw skills to a hidden vault directory (outside the agent's scan path)\n2. Replace them with 35 lightweight \"category pointer\" skills\n3. Each pointer tells the AI: \"use¬†`list_dir`¬†and¬†`view_file`¬†to browse the vault and find the exact skill you need\"\n\nResult:\n\n||Before|After|\n|:-|:-|:-|\n|Startup tokens|\\~80,000|\\~255|\n|Skills accessible|2,004|2,004|\n|Reduction|\\-|99.7%|\n\nThe AI still accesses every skill - it just discovers them on-demand using file tools it already has, instead of loading all descriptions at startup.\n\n# How I verified this\n\n* Measured actual YAML frontmatter sizes from all 2,004 [SKILL.md](http://SKILL.md) files\n* Confirmed the¬†`<available_skills>`¬†loading behavior in¬†[OpenCode docs](https://opencode.ai/docs/skills)¬†and¬†[Claude Code docs](https://docs.anthropic.com/en/docs/claude-code/skills)\n* Real data from my own environment, not theoretical numbers\n\n# Repo\n\n[github.com/blacksiders/SkillPointer](https://github.com/blacksiders/SkillPointer)\n\nIncludes a zero-dependency Python setup script that auto-categorizes your skills and generates the pointers.\n\nHappy to answer questions about the approach. I know \"it's just skills organizing skills\" - that's literally the point. The value is in the pattern, not the tech. savings in scale.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rfwlzk/i_have_2004_ai_skills_installed_heres_how_i/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7nx2hq",
          "author": "Wrenky",
          "text": "Okay so I absolutely love this from a technical standpoint, I've seen several mini patterns like this and written a few- but mostly for chaining not condensing context!\n\nHowever, this feels pretty perfectly situated for rag or even just a database lookup right?  Then again, I don't think you can actually best text lookup at this scale, if you are willing to be militant on organization and policing descriptions\n\nWell done lmao",
          "score": 9,
          "created_utc": "2026-02-27 07:31:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o3qxy",
              "author": "blacksiders",
              "text": "Haha, thank you! You absolutely nailed the dilemma, but you actually read my mind, I'm not skipping vector search entirely, I‚Äôm just layering it.\n\nI use FAISS+Langchain and a custom Qdrant MCP at different layers under the hood (since Cursor/VS Code have built-in indexers by default anyway).\n\nThe problem with pure RAG/Vector DB lookups at the¬†*top*¬†level is semantic failure. If I ask a pure RAG setup to 'Build a stretchy IK arm', it pulls the IK node and all connected if you know those DBS can have up to 700+ vrctors, but misses the 'Naming Conventions' file because the mathematical overlap just isn't there.\n\nSo I use SkillPointer as a strict text router to force a custom dependency map first. This mixed, layer-by-layer approach saves massive tokens. Once inside the routed skill, the AI can trigger the Qdrant MCP for semantic search if it actually needs to deep-dive into the docs.\n\nAs you probably know, MCP is a context vampire and gives you 'auto-compact', where you loosing details and moving you closer to hallucinations. If you have hundreds of tools/skills active, MCP will literally eat 80% of your context window just defining the JSON schemas.\n\nBy putting SkillPointer's strict text routing in front, I shield the AI from that MCP bloat. The AI only loads the schema for the¬†*one*¬†tool it actually needs at that exact moment, rather than choking on the entire protocol dictionary.\n\nIn the end, it's all about ruthlessly optimizing context limits to prevent hallucinations in complex tasks. ",
              "score": 7,
              "created_utc": "2026-02-27 08:32:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7oiiek",
                  "author": "Xanian123",
                  "text": "People actually called you a bot, fucking hell. This is awesome man. Thanks for sharing",
                  "score": 3,
                  "created_utc": "2026-02-27 10:51:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7pmyev",
                  "author": "Wrenky",
                  "text": "Rube mcps sort of tries to solve that as well, but as you have dived this hard into skills you probably share my sentiment of mCP/tools only has a last resort haha. \n\nAnother aspect of this I didn't initially consider was how much better this is for sharing internally-  short of simlinks to outside locations and playing the game of \"what to include at repo/project level vs global or personal\" you can legitimately just have a usable skills repo where growth (if curated) is no longer a downside. Agh this alone means I'm trying it out today!  I think I understand how you grew into 2k skills now, because you actually can.",
                  "score": 1,
                  "created_utc": "2026-02-27 15:08:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7o0syb",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 6,
          "created_utc": "2026-02-27 08:05:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o52dr",
              "author": "blacksiders",
              "text": "Great question! Right now, the Heuristic Engine just assigns it to the first matching category in my priority dictionary. Code of idea is open and you can see examples and improve them for your specific needs.\n\nBut the real safeguard is the AI itself. Because the Pointers are incredibly token-cheap (just a few sentences), the agent will naturally trigger multiple pointers simultaneously if a task is ambiguous. But it also can skip most of them, depends on approach and rules llm follows.\n\nIf I ask for a 'secure React native login', the AI will hit the Security, Mobile, and Web-Dev pointers all at once, scan those three specific hidden vaults, and find the skill wherever the script dropped it. The gateways are so cheap it can afford to double-check, main thing if you have -60 token avarage description for all 2000 skills in context, it's just massive.  \n  \nTo be clear, I‚Äôm not trying to sell this as a universal ‚Äòbest‚Äô pattern or some magic skill standard. I‚Äôm just sharing one concrete way I‚Äôve been optimizing context usage in a messy, real pipeline so the agent doesn‚Äôt drown in skills metadata and start hallucinating. It‚Äôs a tradeoff tool for my use case, not a new religion.",
              "score": 3,
              "created_utc": "2026-02-27 08:45:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7o4qt7",
          "author": "PreparationAny8816",
          "text": "Cloudflare released code mode to target this issue: [Code Mode](https://blog.cloudflare.com/code-mode-mcp/)",
          "score": 5,
          "created_utc": "2026-02-27 08:42:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o65zm",
              "author": "blacksiders",
              "text": "Nice! Code Mode is super interesting, thanks for linking it. It‚Äôs tackling the same core problem (too much MCP/API surface in context).",
              "score": 1,
              "created_utc": "2026-02-27 08:55:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7nzb86",
          "author": "Rizarma",
          "text": "i need to invest my time with this tool",
          "score": 5,
          "created_utc": "2026-02-27 07:51:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7o0eq9",
          "author": "Revolutionary_Sir140",
          "text": "Wow",
          "score": 3,
          "created_utc": "2026-02-27 08:01:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7o2rav",
          "author": "rapkannibale",
          "text": "Thanks for sharing. Does this make sense if you only have 6 skills? At what volume of skills would you say this is really worth it?",
          "score": 3,
          "created_utc": "2026-02-27 08:23:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o5v9g",
              "author": "blacksiders",
              "text": "With 6 skills it‚Äôs not worth the complexity. This pattern is for when you have hundreds+ hyper‚Äëspecific skills (e.g., full Maya/Unity pipelines, maybe even skipping MCP and driving DCCs via Python/commandPort), and the skills metadata itself starts to bloat context and cause hallucinations. Each skill eats \\~60 tokens on average in a fresh session, so once you hit hundreds or thousands you can quickly do the math and see why I'm dig into it.",
              "score": 1,
              "created_utc": "2026-02-27 08:52:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7oayi4",
          "author": "Illustrious-Many-782",
          "text": "I do something different, and I've seen the Convex skill do the same: I create matter skills which have subskills as references.\n\nAlso, just install your skills per project.",
          "score": 2,
          "created_utc": "2026-02-27 09:41:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ocdvr",
              "author": "blacksiders",
              "text": "Cool! Matter skills + subskill refs (Convex style, just checked their page) nests nicely. Looks like everyone who see the problem in scale diggin into this issue. I think it will be resolved on higher level soon!",
              "score": 1,
              "created_utc": "2026-02-27 09:55:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7oo77h",
          "author": "Big_Bed_7240",
          "text": "Sorry but skills do not meaningfully change the code quality. It‚Äôs all over engineering.",
          "score": 2,
          "created_utc": "2026-02-27 11:39:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nfzuq",
          "author": "Michaeli_Starky",
          "text": "Why would you install so many useless stuff? These bot posts are tiring.",
          "score": 2,
          "created_utc": "2026-02-27 05:12:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7niaa3",
              "author": "blacksiders",
              "text": "Not a bot, just a Technical Director working in VFX/Game Dev pipelines.\n\nIn a real enterprise pipeline, 'skills' aren't just generic 'how to code Python' prompts. They are hyper-specific, multi-disciplinary rules. If I need an AI agent to build a single character rig in Maya via MCP, it requires a dozen different dependency documents (Naming Conventions, Stretchy IK math, Twist Extraction rules, Joint Orientations, Unity Export settings).\n\nWhen you map an entire studio's pipeline into agent skills, you hit hundreds or thousands of files instantly. This tool prevents the AI from choking on that metadata. If you only use AI for simple single-file web apps, you definitely don't need this yet. But at scale, flat structures break.",
              "score": 12,
              "created_utc": "2026-02-27 05:29:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7p4kzu",
                  "author": "Western_Objective209",
                  "text": "you definitely did not write the post, it's very clearly claude",
                  "score": 1,
                  "created_utc": "2026-02-27 13:30:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7n8jr4",
          "author": "HarjjotSinghh",
          "text": "this is unreasonably cool actually - my mind needs this.",
          "score": 2,
          "created_utc": "2026-02-27 04:21:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7nwfod",
              "author": "t4a8945",
              "text": "You made me think of Neo in the Matrix \"I know kung-fu\". Bro just loaded the skill in its context.¬†",
              "score": 3,
              "created_utc": "2026-02-27 07:26:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7n9u5d",
          "author": "xak47d",
          "text": "The models have been ignoring most skills anyway. They are mostly useless",
          "score": 2,
          "created_utc": "2026-02-27 04:30:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7nhuoj",
              "author": "blacksiders",
              "text": "The reason models ignore skills is¬†***exactly***¬†because of the architecture flaw I'm fixing here.\n\nBy hiding the raw skills behind a dynamic routing layer (SkillPointer), the AI only loads the exact dependency bundle it needs (e.g., Unity Naming + IK Rigging + Twist setup) in absolute isolation. When the context is clean, the model obeys the skill 100% of the time. The skills aren't useless, the default retrieval mechanism is.",
              "score": 3,
              "created_utc": "2026-02-27 05:26:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7nkhjh",
                  "author": "Codemonkeyzz",
                  "text": "Lol. You are a bot .  It seems you replied to wrong threads. Your reply to the other thread posted here, and your reply for this thread posted to the other thread.",
                  "score": -11,
                  "created_utc": "2026-02-27 05:46:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7o7x3d",
          "author": "mintybadgerme",
          "text": "Nice. But does this problem go away with one million context windows?",
          "score": 1,
          "created_utc": "2026-02-27 09:12:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o8dhw",
              "author": "blacksiders",
              "text": "Not really. ‚ÄòLost in the Middle‚Äô (arXiv:2307.03172) [https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172) shows models already struggle to use long contexts effectively, 1M tokens just makes retrieval, cost, and noise worse unless you stay selective.",
              "score": 2,
              "created_utc": "2026-02-27 09:16:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7oadgk",
                  "author": "mintybadgerme",
                  "text": "Excellent point. I forgot about that.",
                  "score": 1,
                  "created_utc": "2026-02-27 09:36:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7oqdqj",
          "author": "SourceCodeplz",
          "text": "What does this do to your cache?",
          "score": 1,
          "created_utc": "2026-02-27 11:56:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7orx33",
          "author": "Competitive-Yak-8255",
          "text": "Well done. Thanks üëç",
          "score": 1,
          "created_utc": "2026-02-27 12:07:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7q5fmg",
          "author": "TheMigthyOwl",
          "text": "lol.",
          "score": 1,
          "created_utc": "2026-02-27 16:37:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qm9cy",
          "author": "N2siyast",
          "text": "Do u have a repo of all the skills u use? This is pretty neat",
          "score": 1,
          "created_utc": "2026-02-27 17:56:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sqgks",
              "author": "blacksiders",
              "text": "No, but just search GitHub for¬†[`skill.md`](http://skill.md)  \nThere is a ton, but it's better to look what you will use.",
              "score": 1,
              "created_utc": "2026-02-28 00:29:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7rw9h2",
          "author": "Superb_Plane2497",
          "text": "TLDR:\n\nThe problem: AI agents use a¬†[3-level progressive disclosure system](https://opencode.ai/docs/skills)¬†to load skills.\n\nThe solution: 4 levels",
          "score": 1,
          "created_utc": "2026-02-27 21:42:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7s3ouo",
              "author": "blacksiders",
              "text": "Yeah, that‚Äôs a good TLDR. Today OpenCode effectively has 3 levels, but level 1 still dumps every skill‚Äôs name+description into the startup prompt. SkillPointer adds a 0th layer: a tiny, task-scoped index so only a handful of candidate skills ever make it into level‚Äë1 at all.",
              "score": 1,
              "created_utc": "2026-02-27 22:20:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7zitrn",
          "author": "dalton_alexandre",
          "text": "If it works, open a pr and make it the standard",
          "score": 1,
          "created_utc": "2026-03-01 02:28:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7omenx",
          "author": "Mishkun",
          "text": "I think the problem is having four digit number of skills. You just need to delete ones that are not used",
          "score": 1,
          "created_utc": "2026-02-27 11:25:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rgcvv2",
      "title": "Alibaba Coding Plan sounds too good to be true!?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rgcvv2/alibaba_coding_plan_sounds_too_good_to_be_true/",
      "author": "NerdistRay",
      "created_utc": "2026-02-27 17:22:13",
      "score": 117,
      "num_comments": 86,
      "upvote_ratio": 0.96,
      "text": "90,000 Requests for $15 first month and 18,000 Requests for $3 first month. This sounds too good to be true?\n\nAvailable Models: GLM 5, Minimax M2.5, Kimi K2.5 and Qwen 3.5 Plus.\n\nWhat's the catch? Bad unreliable service? Their definition of 'request' is misleading? I don't get it. If this is all true, then this is the most value for money plan, right?\n\nI'm searching everywhere and I see no one is talking about it at all.\n\nAlso, for my Indian brothers out there. Currently, they do not have a way to verify +91 phone numbers so they're not allowing registrations / account sign ups for India. I spoke with their contact, and they said something about their data center recently shutting down in India. Their system requires mandatory phone number verification before making any purchase so the agent was 'unofficially' recommending me to buy a virtual online phone number for another country and sign up that way.\n\nAnyway, I'd love to hear more about this from you guys. Maybe someone is already using it and can share their experience with it?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rgcvv2/alibaba_coding_plan_sounds_too_good_to_be_true/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7r5kxh",
          "author": "NerdistRay",
          "text": "Thank you for all of your comments and reviews. This discussion was exactly what I wanted. To invite people to talk about it so anyone else who searches about it will know what's up. And based on this, I knew my gut feeling was right.\n\nThey must be quantizing the models. Which is a big red flag for coding use so I'm not gonna be bothering with it. I maybe could use it for roleplay and general tasks but I already use NanoGPT subscription for that which also includes image gen, and Deepseek models. Add to the fact that I can't even sign up properly, this one's a big no for me.\n\nI'm considering just going ahead with Opencode Go plans for now.",
          "score": 13,
          "created_utc": "2026-02-27 19:28:30",
          "is_submitter": true,
          "replies": [
            {
              "id": "o7vds0n",
              "author": "HenryTheLion_12",
              "text": "Opencode Go looks good, but yesterday chutes introduced new pricing and max limits which has made it actually usable today. and the limits though nowhere as good as before (doesn't matter anyway if you can't use 100 out 2000 requests in a day due to overload) and today it is behaving rather consistent since the last few hours. I made an account on Alibaba but not really want to move unless there is clear benefit and performance review. What do you think about the opencode go vs new chutes (if you find the time to take a look) plans?",
              "score": 4,
              "created_utc": "2026-02-28 12:48:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o872c56",
                  "author": "Erebea01",
                  "text": "Can you explain the new chutes plan in more detail? I checked their site and the $3 says 300 request / day and also 5x times as payg, have they just not updated their pricing page yet? I'm assuming the new plan means the $3 plan is ~$15?",
                  "score": 1,
                  "created_utc": "2026-03-02 08:01:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7uwboc",
              "author": "Prime_Lobrik",
              "text": "Kimi K2.5 is even quantized by moonshot.ai themselves for the API providing\n\nIts INT4 quantization\n\nEvery provider quantize the model to be able to fit more requests through their GPUs \nNothing new here",
              "score": 4,
              "created_utc": "2026-02-28 10:19:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7rupl3",
              "author": "Status-Mixture-3252",
              "text": "You can't cancel the subscription and it quants the models too??? This plan sounds like a scam then. I was going to purchase it just for RP on sillytavern. But it says it can ban you if you use the api on anything other than coding apps. ",
              "score": 1,
              "created_utc": "2026-02-27 21:34:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qjyof",
          "author": "adasmephlab",
          "text": "I may have to give that a look. Didn't realize it came with additional model access",
          "score": 21,
          "created_utc": "2026-02-27 17:45:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qlj77",
              "author": "NerdistRay",
              "text": "Okay. If you purchase, let us know about your experience. I'm still figuring out how to get phone number online for account verification.",
              "score": 4,
              "created_utc": "2026-02-27 17:53:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7qz7up",
                  "author": "raydou",
                  "text": "I didn't get any phone number check the quality is good. But in GLM 5 there's some tool call errors. So you will loose some requests. Also I think that tool calls are billed as requests also.",
                  "score": 4,
                  "created_utc": "2026-02-27 18:57:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7re8q5",
                  "author": "adasmephlab",
                  "text": "I actually have some free credits on Alibaba Cloud that a Alibaba employee gave me in r/Qwen_AI . To my surprise the credits worked to purchase a coding plan (which i did last night). I just went to generate an api key this morning, but doesn't seem to work when i try to connect to alibaba from \"opencode auth login\". Opencode accepts the api key, however, when i try one of the Qwen models from alibaba i get an error. I'll give this another go when i get a chance",
                  "score": 4,
                  "created_utc": "2026-02-27 20:11:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7r2d74",
          "author": "West-Ad-2051",
          "text": "I cant recommend it (pro plan). Constant tool loops in opencode, and models prob have quant. Speed is about x2/x3 slower then openrouter or opencode zen. As context grows it gets even more slower.\n\nMaybe of they can fix those issue then it will be very nice offer, for now maybe it can be used in openclaw or something but not for agentic use with tools etc. \n\nFor 5$ maybe, but i think it‚Äôs almost the same as chutes or whatever that thing is called (that 3$ provider)\n\nFor now its unuseable for coding.",
          "score": 16,
          "created_utc": "2026-02-27 19:12:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vy4ch",
              "author": "Minimum_Ad9426",
              "text": "yes ,tool loop .but not very often . I just got the pro plan ,and I don‚Äôt think I will pay for the next month",
              "score": 1,
              "created_utc": "2026-02-28 14:53:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qs93v",
          "author": "klocus",
          "text": "Strange, just a few days ago their prices and plans looked different. The cheapest plan was $5 for the first month, then $10. Does anyone know how this translates to the new plans? Did they lower the price to $6 or raise it to $15?",
          "score": 5,
          "created_utc": "2026-02-27 18:24:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qw92z",
              "author": "Diligent_Net4349",
              "text": "lowered I think. just checked mine, I have 18k/month for $10.\n\nah, I checked their doc and it appears intentionally vague: $3 first month, $5 first renewal (50% discount). so it is still $10 after that",
              "score": 3,
              "created_utc": "2026-02-27 18:43:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7qwwo6",
                  "author": "MokoshHydro",
                  "text": "Can you share your experience with general service quality, please?",
                  "score": 1,
                  "created_utc": "2026-02-27 18:46:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qztpo",
          "author": "look",
          "text": "I created an account last night and tried it out. Might just be my bad luck so far, but the models seemed to be horribly lobotomized in some way.\n\nConstantly getting stuck in loops or going full crazy spew of gibberish even.\n\nWhen it would break down like that, I then tried stopping it and switching to the same model on a different provider, and it would continue fine.\n\nThe Kimi and MiniMax were the most fragile. GLM mostly worked (no full psychotic breaks at least), but it was definitely off compared to other providers, too.\n\nI‚Äôll keep trying it (have a month now anyway) but I‚Äôm pretty sure I wasted that $5.\n\nBut for just $5, you might as well try it yourself. And I bet it‚Äôs a lot better with their house model Qwen. I haven‚Äôt experimented with it yet.",
          "score": 5,
          "created_utc": "2026-02-27 19:00:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7u5dmd",
              "author": "aeroumbria",
              "text": "On closer inspection, I think the GLM definitely has some implementation issues. It works well for the first ~80k tokens with great speed, but almost consistently stalls on a malformed tool call around 100k context window for some reason.\n\nKimi seems to work fine, except a weird tendency to repeatedly call the todo list tool. Maybe the temperature and penalty profile is not correctly implemented?\n\nI also wonder how much not having a context length set for them contribute to unstable behaviour.",
              "score": 3,
              "created_utc": "2026-02-28 06:10:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7wdleg",
                  "author": "look",
                  "text": "I tried playing with a more detailed opencode config for them (`opencode models --verbose` will show the config for other models for reference) as well as the openai-compatible endpoint, but I never got them working well. \n\nAt this point, I‚Äôm fairly convinced Alibaba is just using stripped down, quantized versions of the models to make it cheaper to run.\n\nAnyway, I‚Äôve given up on subscriptions for now, and I‚Äôm just doing paygo with Deep Infra. It‚Äôs a dollar or two a day for my use, but at least it‚Äôs fast and high quality.",
                  "score": 1,
                  "created_utc": "2026-02-28 16:12:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7xbnl8",
                  "author": "look",
                  "text": "This also might be useful if you want to try tweaking the config options: https://github.com/anomalyco/models.dev/pull/1023/changes",
                  "score": 1,
                  "created_utc": "2026-02-28 19:02:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7r97js",
              "author": "aeroumbria",
              "text": "I think I might have figured it out. Their setup tutorial uses the anthropic SDK, but Kimi only seems to work with the openai endpoint, but after replicating the configuration with the openai-compatible SDK and endpoint, Kimi seems to work fine. \n\nThere seems to be some long chain tool calling instabilities with GLM 5 no matter which provider though...",
              "score": 2,
              "created_utc": "2026-02-27 19:46:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7smdti",
                  "author": "look",
                  "text": "Thanks! I‚Äôll try that out later to see if it helps.\n\nChutes just rug pulled on their service today, so a psychotic Alibaba model might be the best option now, regardless.",
                  "score": 1,
                  "created_utc": "2026-02-28 00:05:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7umk7t",
          "author": "ylxdzsw",
          "text": "The Chinese version explicily says your data will be used for model training (FAQ 3): https://help.aliyun.com/zh/model-studio/coding-plan . Its really great if you can live with that.",
          "score": 5,
          "created_utc": "2026-02-28 08:44:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7v1nus",
              "author": "AdditionImmediate510",
              "text": "Wow, that's sketchy af",
              "score": 0,
              "created_utc": "2026-02-28 11:09:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qqtnj",
          "author": "MofWizards",
          "text": "It's real! And the service is high quality.\n\nTheir KYC is annoying, and the subscription interface isn't easy.\n\nBut it's the best coding plan with OSS templates that exists.",
          "score": 7,
          "created_utc": "2026-02-27 18:18:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7r1wel",
              "author": "look",
              "text": "Might have just been bad luck for me, and I‚Äôll keep trying it, but it was complete shit for me last night. \n\nIt was like I was getting routed to the INT1 quantized rack or something.",
              "score": 5,
              "created_utc": "2026-02-27 19:10:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7rnx4p",
              "author": "echopraxia1",
              "text": "You can skip the ID upload part if you're just getting the coding subscription. I agree with the web dashboard being terrible though.",
              "score": 1,
              "created_utc": "2026-02-27 21:00:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7styiq",
          "author": "Revolutionary-Call26",
          "text": "Chinese dont do business like us, they are sharks willing to burn so much money for a share of market. And they can afford it. So i guess its legit. Enjoy while it last",
          "score": 5,
          "created_utc": "2026-02-28 00:49:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tir4f",
          "author": "AppointmentNew9761",
          "text": "Found out that the alibaba coding plan runs all your prompts through a pre filter first to block certain content",
          "score": 5,
          "created_utc": "2026-02-28 03:24:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qlz05",
          "author": "IPv6Address",
          "text": "The catch is... you. ",
          "score": 6,
          "created_utc": "2026-02-27 17:55:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qmgrd",
              "author": "NerdistRay",
              "text": "You mean my data? So they can train on it? Why is that any different from many other providers and does it really even matter?",
              "score": 11,
              "created_utc": "2026-02-27 17:57:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7qmyx4",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -10,
                  "created_utc": "2026-02-27 17:59:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7teqol",
              "author": "Euphoric_Oneness",
              "text": "No pedos better. Why did Antropic opposed mass surveillance of Americans? New China complaining about old China.",
              "score": 1,
              "created_utc": "2026-02-28 02:58:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qk7fb",
          "author": "kidousenshigundam",
          "text": "Where?",
          "score": 3,
          "created_utc": "2026-02-27 17:46:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qxa7x",
          "author": "MokoshHydro",
          "text": ">No API calls: Use Coding Plan only in coding tools, such as Claude Code or OpenClaw. Do not use it for automated scripts, custom application backends, or any non-interactive batch calls via API. Using your Coding Plan API key outside the allowed scope constitutes misuse or abuse. Your subscription may be paused, or your API key may be revoked.\n\nI never thought about OpenClaw as a \"coding tool\".\n\nUpdate: Also, the output for GLM-5 is limited to 16K tokens, which is very low compared to model 128K.",
          "score": 3,
          "created_utc": "2026-02-27 18:48:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rfif6",
              "author": "echopraxia1",
              "text": "What app are you using? In OpenCode I tried specifying the context and output size in the config, then it seemed to behave better. It could be placebo though. It still stops randomly in the middle of tasks sometimes.",
              "score": 1,
              "created_utc": "2026-02-27 20:18:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7t7kll",
          "author": "Sure_Desk3587",
          "text": "It‚Äôs very good value but the performance is sketchy.   I found GLM5 to be practically unusable as it would wander off for a coffee and never come back unless I was actively prodding it.  Qwen messed up so badly by not following instructions.  Changing things I never asked it to change and then breaking everything else when it tried to revert the change.   Yesterday it wasted about 25 million tokens in sudden tool call loops and another 25 million on a project branch I had to scrap completely it made such a mess.   GLM5 was able to (mostly) complete the project but I mostly used Zai‚Äôs endpoint.  Falling back to bailian when I got timeouts.   But then having to poke it to stop it falling asleep.    Not a good experience with Alibaba‚Äôs coding plan.   The one thing it‚Äôs sold for ‚Äúcoding‚Äù is the one thing I wouldn‚Äôt trust it to attempt ever again.   It‚Äôs just a waste of time and possibly tokens or requests if you‚Äôre paying for them.   On the plus side, qwen is fantastic for collating, sorting through project plans, and surprisingly: writing.  It makes great newsletters.",
          "score": 3,
          "created_utc": "2026-02-28 02:14:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ribxa",
          "author": "paflopes",
          "text": "I've been using it to test GLM 5 using the Anthropic endpoint, but I'm unimpressed by the performance/intelligence. It's horrible compared to Codex 5.3 medium, idk if this is even a fair comparison. Otherwise I didn't have any issues with speed or tool calls.\n\nI'm not sure if it's a model or a provider issue though as I haven't used GLM 5 anywhere else.",
          "score": 2,
          "created_utc": "2026-02-27 20:32:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7t3945",
              "author": "look",
              "text": "GLM 5 elsewhere is very good. Don‚Äôt judge it by this Alibaba version.",
              "score": 4,
              "created_utc": "2026-02-28 01:47:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o80yla5",
              "author": "harrypham2000",
              "text": "try Opencode Go inteference or elsewhere, GLM 5's not that bad ",
              "score": 2,
              "created_utc": "2026-03-01 09:16:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7rjh4h",
          "author": "yossa8",
          "text": "Use it for 3 days now.\nQwen3.5-plus is pretty solid and speed seems quite good. Understand concerns about data but tbh‚Ä¶ If you already bought something on Aliexpress or Alibaba, seems to be the same",
          "score": 2,
          "created_utc": "2026-02-27 20:38:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7rkopl",
          "author": "lemon07r",
          "text": "The catch is this is not a good plan until they have a good coding model. qwen 3.5, is not good at coding, we probably need to wait for a dedicated coding model. ",
          "score": 2,
          "created_utc": "2026-02-27 20:44:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rojzg",
              "author": "Fredrules2012",
              "text": "Kimi2.5 and glm-5 included \n\nIt's a killer plan, sign up is a bitch though and good luck finding all the right links, I spent last night trying to figure out how to pay them 5 dollars",
              "score": 2,
              "created_utc": "2026-02-27 21:03:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7rp7wx",
                  "author": "lemon07r",
                  "text": "oh I missed that. fantastic plan then. too bad there are so many good options right now I still dont see the point. I have more than enough opus and gpt for very cheap. ",
                  "score": 1,
                  "created_utc": "2026-02-27 21:07:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7s69y1",
          "author": "Practical_Arm_645",
          "text": "people on xiaohongshu are diacussing it, I see people constantly complaining the slow speed in request etc.",
          "score": 2,
          "created_utc": "2026-02-27 22:34:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7thnyi",
          "author": "TinyAres",
          "text": "I tried to sub for it, and their information collecting on the site is so invasive that I was yearning for anarcho capitalism, then after verifying everything about me and my card the discount went away. I could only pay the full price which i declined to do. Some of the buttons are not even hooked up.\n\nThe deal does look good, but their site is crap, and they have explicit no refund rules, with vague we ban you if you use it too much.",
          "score": 2,
          "created_utc": "2026-02-28 03:17:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7v20ij",
          "author": "Admirable-Carpet8675",
          "text": "My experience with it is kinda mixed so far, when it hits, i get really high token per second, when it misses, it slow itself to the point of unusable",
          "score": 2,
          "created_utc": "2026-02-28 11:12:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7r8l7g",
          "author": "Infamous_Pickle2975",
          "text": "Link or it doesn‚Äôt exist üòÉ",
          "score": 1,
          "created_utc": "2026-02-27 19:43:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7r9ur0",
          "author": "JellyfishLow4457",
          "text": "Assuming the models give you good output (highly subjective here) then it's a good deal. ",
          "score": 1,
          "created_utc": "2026-02-27 19:49:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sb5rs",
          "author": "greeneyedguru",
          "text": "Well the first thing that occurs to me is that I have no idea how requests per month compares to input/output token prices.  Is this cheap compared to Claude 20x?",
          "score": 1,
          "created_utc": "2026-02-27 23:00:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7shjo9",
          "author": "Chrisnba24",
          "text": "https://preview.redd.it/a1xc32roh4mg1.png?width=1327&format=png&auto=webp&s=137ede9af02897ecc407769a35154c571b4babb1\n\nthe first sentence i really wish its poor wording or a translation problem lol",
          "score": 1,
          "created_utc": "2026-02-27 23:37:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7x69d7",
              "author": "Diligent_Net4349",
              "text": "you can't get money back, but you can choose to not renew.",
              "score": 1,
              "created_utc": "2026-02-28 18:35:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7sl4ay",
          "author": "FormalAd7367",
          "text": "i‚Äôve been using it for a few weeks.  i use it for coding and also have qwen running a a family assistant",
          "score": 1,
          "created_utc": "2026-02-27 23:58:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tk4bw",
          "author": "RedParaglider",
          "text": "I'd love to see someone actually get that amount with the speed :D",
          "score": 1,
          "created_utc": "2026-02-28 03:33:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7um410",
          "author": "HarjjotSinghh",
          "text": "this alibaba deal feels like magic - can we just buy all of it?",
          "score": 1,
          "created_utc": "2026-02-28 08:40:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7v0885",
          "author": "Ang_Drew",
          "text": "i do have 2000 request every day.. that's actually a good deal..\n\nmaybe they did this to attract customer then increase the price gradually..",
          "score": 1,
          "created_utc": "2026-02-28 10:56:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vtbva",
          "author": "Specter_Origin",
          "text": "I just bought it to try and don‚Äôt recommend it for anything serious, model are just not there yet comparatively",
          "score": 1,
          "created_utc": "2026-02-28 14:26:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xw2q1",
          "author": "volvoxllc",
          "text": "I can't get past the purchase phase the create api key isn't working for me anyone have any ideas?\n\nhttps://preview.redd.it/hl200svxsamg1.png?width=390&format=png&auto=webp&s=ad90ddd67aa96b7d9c1c80d9bedc9eb113ec7abc\n\n",
          "score": 1,
          "created_utc": "2026-02-28 20:50:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o83swu7",
              "author": "Big_Asparagus_8961",
              "text": "You need to go [Alibaba Cloud Model Studio console](https://modelstudio.console.alibabacloud.com/ap-southeast-1/?tab=dashboard#/efm/model_experience_center/text) first then create key at the Subscription Plans.",
              "score": 1,
              "created_utc": "2026-03-01 19:36:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o83thny",
          "author": "Mayanktaker",
          "text": "Their website is horrible",
          "score": 1,
          "created_utc": "2026-03-01 19:39:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o8443e1",
              "author": "NerdistRay",
              "text": "Did you try speaking with their support? I was doing it in Zen browser (not sure if this problem exists in chrome or not), but I was literally unable to type properly. They have some sort of weird input field behavior where some key presses aren't being registered. I had to legit write the message in notepad and then copy paste it into the message field. And their entire documentation is apparently translated using AI and they claim that manual review is still pending and there may be inaccuracies.\n\nAnd I can't sign up using my indian phone number, because they have no way to verify +91 phone numbers apparently. Their support was telling me to go buy a virtual phone number to sign up to their service.\n\nWhat a joke.",
              "score": 2,
              "created_utc": "2026-03-01 20:33:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o85sq53",
                  "author": "Mayanktaker",
                  "text": "Indians can't sign up. It shows afghanistan automatically üòÅ and we cant change the country code for the telephone number. And I think we should stay away from this for a few months.",
                  "score": 2,
                  "created_utc": "2026-03-02 02:15:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7r430d",
          "author": "Just_Lingonberry_352",
          "text": "With these Chinese models there's an unspoken catch to why they are cheap \n\nbut I guess if you that doesn't bother you or your company doesn't care then fine I guess",
          "score": 1,
          "created_utc": "2026-02-27 19:21:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rav19",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -1,
              "created_utc": "2026-02-27 19:54:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7rsdxc",
                  "author": "Just_Lingonberry_352",
                  "text": "whats weird about that post wumao please explain",
                  "score": 1,
                  "created_utc": "2026-02-27 21:22:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7v7egv",
          "author": "BrokenEvil_",
          "text": "The Alibaba Coding Plan has a limitation in how requests are counted. Each session request is not treated as a single request. For example, if you send one prompt and the agent makes 10 tool calls during that session, it will count as 11 total requests (1 initial request + 10 tool calls), not just 1 request.\n\nAdditionally, the 5-hour limit works on a rolling basis. Each request you send is counted against your quota and will only be released after 5 hours from the time it was made.",
          "score": 1,
          "created_utc": "2026-02-28 11:59:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sqdnr",
          "author": "Euphoric-Doughnut538",
          "text": "Oh I‚Äôm on this. I‚Äôm so over Claude‚Äôs bullshit",
          "score": 0,
          "created_utc": "2026-02-28 00:28:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rhckrq",
      "title": "Are people lying about GLM-5 and MiniMax M2.5?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rhckrq/are_people_lying_about_glm5_and_minimax_m25/",
      "author": "TheDevilKnownAsTaz",
      "created_utc": "2026-02-28 20:07:56",
      "score": 91,
      "num_comments": 102,
      "upvote_ratio": 0.92,
      "text": "Since the releases of GLM-5, MiniMax M2.5, and Kimi K2.5, all I read is how amazing these LLMs are. So many people say how they can replace Sonnet 4.5 in most cases. To test this, I created my own personal benchmark: update a personal project that used to read from OpenCode‚Äôs JSON files to instead read from the SQLite db. Sonnet 4.5/4.6 and GPT 5.2/5.3 Codex finished these within 15 minutes and with no issues. GLM-5, MiniMax M2.5, and Kimi K2.5 failed spectacularly. For the same prompt, each model took 40+ minutes and didn‚Äôt even produce a working migration. MiniMax M2.5 had issues with tool calling and would just stop randomly. I have tested with OpenCode + Oh My OpenCode + GitHub Copilot (just to see if GPT/Sonnet would do). Am I missing something? How are others getting performance that is anything close to Sonnet/GPT from these cheaper models?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rhckrq/are_people_lying_about_glm5_and_minimax_m25/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7xpnpf",
          "author": "_Silly-Pumpkin_",
          "text": "Kimi is pretty decent in my opinion. GLM is ok but slower. I‚Äôve found MiniMax to be pretty terrible, only does the bare minimum.",
          "score": 47,
          "created_utc": "2026-02-28 20:15:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7xvibf",
              "author": "cutebluedragongirl",
              "text": "Kimi K2.5 is definitely better than GLM. No, seriously, they are not even on the same level.",
              "score": 11,
              "created_utc": "2026-02-28 20:47:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o80npxy",
                  "author": "xmnstr",
                  "text": "Agreed. It has a Claude-like tendency, like a mix of Sonnet and Haiku. Leaning Sonnet, of course. Works great with Codex as my daily driver.",
                  "score": 3,
                  "created_utc": "2026-03-01 07:33:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o8283hp",
                  "author": "Capital-Bag8693",
                  "text": "Opino lo mismo que vos Kimi es superior a estos otros 2 modelos.",
                  "score": 1,
                  "created_utc": "2026-03-01 15:03:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7xs0cf",
              "author": "ItsCalledDayTwa",
              "text": "I think Kimi is great, but I don't like minimax¬†",
              "score": 11,
              "created_utc": "2026-02-28 20:28:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7xyqgx",
              "author": "mobileka",
              "text": "I absolutely agree. Kimi is the only cheap model that is reliably useful to me. One more version and I'll be able to get rid of other more expensive models for coding.\n\n\nGLM 5 has potential, but it's annoyingly slow and not always reliable. I think it needs a couple more iterations to get there.\n\n\nMinimax is complete garbage.",
              "score": 8,
              "created_utc": "2026-02-28 21:04:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7xuiil",
              "author": "No_Yard9104",
              "text": "Kimi does have a problem with randomly halting mid-response.  But aside from that, it's one of the best models I've used.  GLM is slow and TOO thorough.  It likes to hear itself think.  I will never understand why people are on about miniMax though.  I'd be better off using GPT-nano or Phi than MiniMax.  ",
              "score": 3,
              "created_utc": "2026-02-28 20:41:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7xvu21",
                  "author": "queso184",
                  "text": "that problem is with the inference provider, not the model",
                  "score": 6,
                  "created_utc": "2026-02-28 20:48:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ya4k4",
                  "author": "Street_Smart_Phone",
                  "text": "I‚Äôve had better luck with hooking it up with Claude Code versus Opencode with the same provider.",
                  "score": 1,
                  "created_utc": "2026-02-28 22:05:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o80nspx",
                  "author": "xmnstr",
                  "text": "Not having that issue at all from Kimi.com. It's honestly solid and pretty amazing. And so fast! Sonnet feels slow in comparison.",
                  "score": 1,
                  "created_utc": "2026-03-01 07:34:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o8704t5",
                  "author": "Possible-Basis-6623",
                  "text": "but honestly i found minimax 2.5 is at least much better than the gpt free ones, e.g. GPT5 mini",
                  "score": 1,
                  "created_utc": "2026-03-02 07:40:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7zng01",
              "author": "toadi",
              "text": "I like it to do the bare minimum. Claude tends to overdo things. Changing stuff that wasn't even in scope...",
              "score": 1,
              "created_utc": "2026-03-01 02:57:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7xuz7b",
          "author": "aeroumbria",
          "text": "I wonder how much tooling bias comes into this. We seem to have developed the idea that prompts and agents should be universal for sufficiently intelligent models, but there is no reason for this to be guaranteed. I suspect prompts have model biases when I found that some models work surprisingly well in one framework but fail catastrophically in others (e.g. Kimi seems to be really competent with strict phase-gated frameworks like GSD and can recover from its own mistakes, but always seem to struggle with frameworks that rely on skill activation like OAC, while GLM 5 seems to be the opposite). The developers of agent frameworks will usually have one primary model they develop on to see whether something works reliably, so consciously or not, we end up engineering prompts against specific models. If everyone validates against one model, that one model will naturally perform better with tools validated on it. The problem with these newer models might be that they replace each other as favourite of the month too fast, that people don't have time to develop toolings that are more compatible with them.\n\nIt does seem that tools with shorter prompts, more defined workflow and less reliant on perfectly timed spontaneous decisions (think of instructing \"run test after coding\" vs having a \"test\" skill which says \"activate yourself after coding\") usually perform better across models.",
          "score": 15,
          "created_utc": "2026-02-28 20:44:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o80k7zw",
              "author": "nuclearbananana",
              "text": "> Kimi seems to be really competent with strict phase-gated frameworks like GSD and can recover from its own mistakes, but always seem to struggle with frameworks that rely on skill activation like OAC, while GLM 5 seems to be the opposite\n\nI've never heard of any of these. Feels like half the industry is chasing increasingly complex frameworks when benchmarks show a minimal harness+prompt often works just as well if not better.",
              "score": 3,
              "created_utc": "2026-03-01 07:01:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o80ximf",
                  "author": "aeroumbria",
                  "text": "I think the starting point of many complex frameworks are not bad, just that they often end up trying to implement a deterministic workflow with prompts and use tons of extra guidelines, situation check instructions, book keeping etc. to hold everything together. IMO \"prompt\" is not the correct programming language to enforce structured processes...",
                  "score": 1,
                  "created_utc": "2026-03-01 09:05:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7y8qfh",
              "author": "spookperson",
              "text": "I agree. I've messed around with Minimax 2.1 and 2.5 inside of Claude Code (instead of Anthropic) and been impressed with what you can do on a model you can run on a single machine at home (but I'm not trying to compare it to something huge like Sonnet or Opus)",
              "score": 1,
              "created_utc": "2026-02-28 21:58:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7zoyom",
              "author": "TheDevilKnownAsTaz",
              "text": "If the benchmarks are sonnet level performance, how is it too much to ask that I actually get the same performance for the same prompt with the same harness?",
              "score": 1,
              "created_utc": "2026-03-01 03:06:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7zrj8z",
                  "author": "aeroumbria",
                  "text": "There is an argument that if the developer of the framework used Claude to finetune the prompts, then the framework will be biased towards Claude. We do observe on that harness-included benchmark that models can swing wildly in performance levels across different harnesses.",
                  "score": 1,
                  "created_utc": "2026-03-01 03:23:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7xsekj",
          "author": "USMCamp0811",
          "text": "they're good for junior dev type workloads.. don't expect them to do do much planning and stuff but they can follow instructions.. ",
          "score": 11,
          "created_utc": "2026-02-28 20:30:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ziinv",
              "author": "techlos",
              "text": "i've swapped to using sequential thinking + agent coordinator using GLM-4.7-flash locally for most of my small workloads, it's genuinely outperforming GLM5 and minimax, and at least i don't have to worry about rate limits or claude eating my savings\n\n(i know using these with GLM5/minimax would outperform 4.7 with these tools, but agent swarms will easily hit the millions of tokens per task with this setup)",
              "score": 2,
              "created_utc": "2026-03-01 02:26:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7zj3n4",
                  "author": "USMCamp0811",
                  "text": "are you running that in Ollama or something? I've struggled to get ollama and opencode to play nice. I've got a 4070ti super... either the model gets into a loop of the same giberish over and over and just wont stop or simply tells me tools are supported. \n",
                  "score": 1,
                  "created_utc": "2026-03-01 02:30:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7xslwh",
          "author": "dasplanktal",
          "text": "So, I've been doing a lot of comparison between claude Opus 4.5, and GLM-5. Glm-5 tends to handle a large context window a lot better so if you give it a ton of information to do the job correctly, I find GLM5 produces better results than Opus 4.5. Keep in mind, though, I'm working with infrastructure and terraform, not necessarily programming code. I think this is really where GLM5 does a great job. It's on the infrastructure devops side of the house.",
          "score": 15,
          "created_utc": "2026-02-28 20:31:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xrkaj",
          "author": "Codemonkeyzz",
          "text": "Minimax m2.5 is definitely dog sh!t. Kimi k2.5 is decent not codex level but more like sonnet level. Glm 5 is hit and miss.  More like Haiku level\n\nCodex 5.3 > Opus 4.6 > Kimi k2.5  > glm 5  > minimax m2.5\n\n\nDon't trust benchmarks do your own tests",
          "score": 7,
          "created_utc": "2026-02-28 20:25:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zpkcf",
              "author": "TheDevilKnownAsTaz",
              "text": "I am not even coming close to sonnet level for Kimi K2.5. Even using the same harness. I would genuinely take Haiku 4.5 over Kimi.",
              "score": 1,
              "created_utc": "2026-03-01 03:10:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o80esr8",
                  "author": "Codemonkeyzz",
                  "text": "Which stack you are working on?\n I use it for typescript, bash/shell  and react stack.",
                  "score": 1,
                  "created_utc": "2026-03-01 06:13:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7xpx5c",
          "author": "TheOnlyArtz",
          "text": "I get better results talking with my cat than with M2.5",
          "score": 18,
          "created_utc": "2026-02-28 20:17:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7yt8r1",
              "author": "noiserr",
              "text": "Haven't used minimax 2.5 but 2 and 2.1 were great in my use. Did it get worse?",
              "score": 3,
              "created_utc": "2026-02-28 23:54:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o8344cg",
                  "author": "deadcoder0904",
                  "text": "I think so. It used to be great at 2.1 i think.",
                  "score": 2,
                  "created_utc": "2026-03-01 17:39:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7y4hpm",
              "author": "look",
              "text": "M2.5 is a typist. Don‚Äôt ask it to ‚Äúthink‚Äù; just give it a detailed plan (written by GLM or maybe Kimi).",
              "score": 2,
              "created_utc": "2026-02-28 21:35:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7y1ats",
          "author": "look",
          "text": "I‚Äôve only had issues with these models stopping or tool use failure when dealing with low quality providers.\n\nWith good providers, they are usable if you are reviewing work periodically. They won‚Äôt give you a finished product on their own like Opus or Codex, but it‚Äôll be 80-90% and then some guided clean up. Think more ‚Äúrough draft‚Äù.\n\nAlso, GLM and MiniMax (and Kimi) have different strengths and weaknesses, and you‚Äôll typically get better results (in my experience) if you use a mix, with the right tool for the current step, not just one all the way through.\n\nI typically do GLM for plan, MiniMax for initial build, then Kimi for review and cleanup.\n\nAt work, cost is irrelevant, and I run Claude Code with Opus 4.6 1M context in API fast mode. That is better, but OpenCode with the model setup above is comparable (for my workflow at least) and about 1/100th the cost.",
          "score": 5,
          "created_utc": "2026-02-28 21:18:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zot58",
              "author": "TheDevilKnownAsTaz",
              "text": "I use Zen.ai as the provider. Would you consider them to be bad?",
              "score": 1,
              "created_utc": "2026-03-01 03:05:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o80x255",
                  "author": "undisputedx",
                  "text": "it is likely the provider, use official for proper comparison",
                  "score": 1,
                  "created_utc": "2026-03-01 09:01:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ycwyq",
          "author": "SvenVargHimmel",
          "text": "Workflow -  Plan Mode ( insert any from the frontier labs ) - Build Mode ( Kimi k2.5 does really well). \n\nKimi K2.5 is like the eager junior. Very eager. It will happily attempt tasks that are beyond its capabilities. So it's good for Build. \n\nMinimax 2.5 is too inconsistent \n\nThere are pros and cons.  Kimi K2.5 is good for small tasks , even in plan mode. These are small tasks that Gemini, for example will just randomly ready your entire code base to fix a typo in your code base. \n\nThe models are pretty good for non coding tasks. I use the open weight models in subagents for prompt expansion for image models. ",
          "score": 4,
          "created_utc": "2026-02-28 22:21:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xv2dg",
          "author": "cutebluedragongirl",
          "text": "When it comes to Chinese models...¬†\n\n\nKimi K2.5 is decent in my personal experience.¬†\n\n\nI like DeepSeek a lot, especially considering how cheap it is.\n\n\nEverything else kind of sucks tbh.¬†\n\n\nChinese models are overhyped.\n\n",
          "score": 7,
          "created_utc": "2026-02-28 20:44:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xqnx0",
          "author": "SourceCodeplz",
          "text": "I've given up on these models, Opus-Codex for now until others catch up.",
          "score": 5,
          "created_utc": "2026-02-28 20:20:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xuxov",
          "author": "adelope",
          "text": "Exactly; its not just GLM or Minimax.  \n  \nEvery lab tries to benchmaxxing now, how well those benchmark correlates to the actual usage is different.   \n  \nI have my own personal eval benchmark, which i never shared over the internet, that i always check against each model or each harness.   \nAnother example is Gemini, on paper it claims it is better that Sonnet or even opus at SWE-bench verified, in practice it is worse than sonnet 3.7\n\nI also have my own arena, where i give the same prompt to all of these agents and see which one wins. In my experience win-rate fluctuates between gpt-5.3 and opus-4.6 but the other models never win. ",
          "score": 6,
          "created_utc": "2026-02-28 20:44:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xr9n8",
          "author": "smurff1975",
          "text": "M2.5 was terrible for me, I've gone back to Sonnet",
          "score": 3,
          "created_utc": "2026-02-28 20:24:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yei7j",
          "author": "Curtilia",
          "text": "I suppose it comes down to how much hand holding you're willing to do. The first thing I would do is ask it to make a plan. Then the plan will only need a few tweaks (or sometimes admittedly some big changes), then I get it to update the plan until I'm happy with it. Then I get it to implement the plan which it is very good at.\n\nYou're basically talking about one-shotting which I don't do, unless it's a small bug. The reason I want to review its plan is because I don't want to commit code that I haven't reviewed and I find it quicker to get involved in the process earlier.",
          "score": 3,
          "created_utc": "2026-02-28 22:29:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zamut",
              "author": "TheDevilKnownAsTaz",
              "text": "If these models are benchmarked at Sonnet 4.5 performance, it should be able to one shot like 4.5. This thread was created to state how I haven‚Äôt been able to come close to the benchmarked performance of these open models.",
              "score": 1,
              "created_utc": "2026-03-01 01:38:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o834rj5",
                  "author": "deadcoder0904",
                  "text": "No, that's not how u do it. That's just marketing.\n\nUse it for smaller work. You're trying too hard problems. Cannot compare 1T model with 400B parameter model. Also, harness / infra dependant. Most Chinese models (or maybe providers) don't have that since USA is a rich country so their VCs sponsor a lot of stuff to give away for free.",
                  "score": 1,
                  "created_utc": "2026-03-01 17:42:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7yl2bb",
          "author": "Bob5k",
          "text": "Minimax M2.5 is good when you provide it with a proper, detailed prompts and plans / prd to work on. It's not codex or opus level, but for majority of webdevelopment it's more than enough to move forward.\nI'm genuinely surprised with people who want opus to do all their stuff around - i am big fan of both opus and new codex but also i.e. can't imagine paying 200$ for any of those to get some landing page done or website for my client if i can do this using [minimax for 9$.](https://platform.minimax.io/subscribe/coding-plan?code=HO46LCwAJ5&source=link)",
          "score": 3,
          "created_utc": "2026-02-28 23:06:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7zb9pt",
          "author": "Illustrious-Many-782",
          "text": "There is no way GLM-5 is worth the $30/month that I'm paying for it. It is literally my last of four choices when I'm working. \n\n1. Claude Sonnet 4.6 $20\n2. GPT-5.3-codex $20\n3. Gemini-3.1 $20\n4. GLM-5 $30\n\nI don't even think I see more usage out of glm despite it costing 50% more and being at the bottom of my list.\n\nYes, I'm unhappy with my \"upgrade\" from a legacy coding lite plan with virtually no limits on glm-4.7 for $3/mo. Can you blame me? At least that was cheap and I got what I paid for. Fast, good, cheap -- pick two. I'm fine with that. \n\nWith GLM-5, I get \"pick zero.\"",
          "score": 3,
          "created_utc": "2026-03-01 01:42:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xvnda",
          "author": "jotabm",
          "text": "Kimi is great, GLM is fast, Minimax is a hallucination shitshow",
          "score": 2,
          "created_utc": "2026-02-28 20:47:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xxfop",
          "author": "biotech997",
          "text": "I agree with most of the comments here - Kimi is great, GLM is okay, MiniMax is kinda bad.",
          "score": 2,
          "created_utc": "2026-02-28 20:57:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xztf2",
          "author": "OnlineParacosm",
          "text": "Minimax rimmed me and kimi keeps me grounded and importantly doesn‚Äôt hallucinate phantom GitHub projects",
          "score": 2,
          "created_utc": "2026-02-28 21:10:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7y2fa4",
          "author": "Longjumping_Ad5434",
          "text": "That‚Äôs just it, YMWV, depends on the provider, the coding tool, what you work on, and on top of all that, it‚Äôs non-deterministic system to begin with.",
          "score": 2,
          "created_utc": "2026-02-28 21:24:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xr75i",
          "author": "poppear",
          "text": "I have a private benchmark based on several hard task with associated blind hand-written tests and Kimi 2.5 and Minimax both scored 2/10. GLM5 scored 8/10 and for reference Opus 4.5 scores 10/10 as well as Gemini 3 pro",
          "score": 4,
          "created_utc": "2026-02-28 20:23:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zawoq",
              "author": "touristtam",
              "text": "Where's codex in there?",
              "score": 1,
              "created_utc": "2026-03-01 01:40:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o83ar9d",
                  "author": "poppear",
                  "text": "I did not try It, i have no chatgpt subscription and it is too expensive via API",
                  "score": 1,
                  "created_utc": "2026-03-01 18:10:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7xt538",
          "author": "Michaeli_Starky",
          "text": "Amazing? They fail on simplest tasks on medium sized codebases.",
          "score": 2,
          "created_utc": "2026-02-28 20:34:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xpxws",
          "author": "Accomplished_Bear_95",
          "text": "Thanks for this post. These questions resonate with me too!  \nI researched this issue and got stuck because the benchmarks are synthetic, and I need benchmarks for use with popular agent programs.   \nI don't want to be the developer of an agent program for a specific language model. I want to complete my own tasks",
          "score": 1,
          "created_utc": "2026-02-28 20:17:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7xr0qz",
              "author": "rusl1",
              "text": "can you give me the recipe for cooking a good pizza?",
              "score": 0,
              "created_utc": "2026-02-28 20:22:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7y5fsi",
                  "author": "Accomplished_Bear_95",
                  "text": "I would like to see the rating/arena/benchmark",
                  "score": 1,
                  "created_utc": "2026-02-28 21:40:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7xt8fa",
          "author": "WangSora",
          "text": "That's a thing that's bothering me about switching from gpt plus to opencode go.\n\nI don't really know if it's worth it (from a performance perspective) and I couldn't find anyone talking about it, at least yet.",
          "score": 1,
          "created_utc": "2026-02-28 20:34:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xvwjk",
          "author": "Medium_Chemist_4032",
          "text": "How are you accessing them? OpenRouter?",
          "score": 1,
          "created_utc": "2026-02-28 20:49:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7z76pn",
              "author": "TheDevilKnownAsTaz",
              "text": "I use Zen.ai (OpenCodes own provider)",
              "score": 2,
              "created_utc": "2026-03-01 01:17:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7xx2aj",
          "author": "Most_Remote_4613",
          "text": "Glm 5 + claude code differs too much compared to opencode, you need to try in cc. it is sonnet 4.25 level IMO, not main driver but use to review opus/gpt plans and implement to save limits(if you are old zai subscriber otherwise forget about it for now) and also for new ideas/bugs. Quality: angular frontend quality: glm > kimi > m2, speed m2>kimi>glm, nestjs/devops backend quality: glm > m2.¬†",
          "score": 1,
          "created_utc": "2026-02-28 20:55:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xzzof",
          "author": "jarjoura",
          "text": "Fireworks.ai has fixed a lot of the inherent issues with K2.5 and it‚Äôs fast and quite stable for me. It‚Äôs probably the best way to access that model.",
          "score": 1,
          "created_utc": "2026-02-28 21:11:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7y1xfg",
          "author": "do_not_give_upvote",
          "text": "For GLM-5, are you using it with opencode (OC) or Claude Code (CC) as the harness? Harness matters too, not just the model. CC is better based on my testing. For some reason, OC is taking a lot longer, with lower quality result. I'm using GLM-5 with CC on daily basis, and it is imo, on par with Sonnet 4.5.",
          "score": 1,
          "created_utc": "2026-02-28 21:21:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7z7ogg",
              "author": "TheDevilKnownAsTaz",
              "text": "I tried with OpenCode and oh-my-OpenCode using Zen.ai as the provider.",
              "score": 1,
              "created_utc": "2026-03-01 01:20:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7y21b0",
          "author": "commandedbydemons",
          "text": "Kimi is definitely the one that‚Äôs worth it for some work.\n\nBut what Opus and Codex one/two shot, Kimi still needs 10 runs at it",
          "score": 1,
          "created_utc": "2026-02-28 21:22:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7y5a2k",
          "author": "annakhouri2150",
          "text": "IME, K2.5 is incredible, haven't had the halting midway problem others are mentioning here, and it's incredibly good at using feedback loops like tests and playwright to solve problems in its own; MiniMax is disappointing, but minimally competent, and I haven't had a chance to try GLM-5 yet.",
          "score": 1,
          "created_utc": "2026-02-28 21:39:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7y744t",
          "author": "UseMoreBandwith",
          "text": "never had that issue, they're all good.  \nBut I do give very clear instructions to the LLM.   \nMinimax does exactly what I tell it to do.",
          "score": 1,
          "created_utc": "2026-02-28 21:49:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7y7zmq",
          "author": "s5fs",
          "text": "I recently asked Opus 4.6 and GLM-4.7 and Cursor \"Auto\" to add a Reports feature to my existing iOS app. The results were pretty much expected:\n\n1. Opus did the best job, the ui looked good and it worked one-shot  \n2. GLM-4.7 ui looked pretty rough and it took like 2 hours of prompting to get it halfway working  \n3. Cursor made a ui pretty similar to Opus, also worked one-shot\n\nI went with the Opus design and that's the end of my story.",
          "score": 1,
          "created_utc": "2026-02-28 21:54:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yazlz",
          "author": "johannes_bertens",
          "text": "It's also a matter of personal opinion and what you need/use the model for.\n\nFor me MiniMax M2.5 with the NVFP4 quant is perfect. Fast (100 to 200 tps), stable, enough vram left for 2x 190k context with my 2x RTX 6000 cards.\n\nFor others it's \"too basic\" or maybe even \"too slow\". I've tried the Qwen models but they tend to \"overthink\". The other LLMs you mentioned don't run great local for me...",
          "score": 1,
          "created_utc": "2026-02-28 22:10:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o80n4eb",
              "author": "TheDevilKnownAsTaz",
              "text": "Tokens per second do not equal quality. Codex and Sonnet finished my task in 1m cache tokens. The others went over 10M and produced nothing of actual use.",
              "score": 1,
              "created_utc": "2026-03-01 07:27:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ybsql",
          "author": "K9_Surfer",
          "text": "Okay, I'm gonna tell you my story and use case.\nI own a small petrochemical plant, and AI is a game-changer in a few areas.\n\nI was testing it for a very specific use case at that point:\n\nAccount ledger, plus managerial info as input. And structured bases on Supabase, SQL schema, and a press release as formal output. A big report stating the financials, with a storytelling based on the managerial info.\n\nFirst, I tried a small but organized single prompt.\n\nMinimax 2.5\nIt didn't follow at all. It made several mistakes in terms of data crunching, and the report was atrocious.\n\nGLM 5\nIt impressed me with the organization and report output.\n\nLater on, I tried skills and a better prompt.\n\nMinimax 2.5\nIt couldn't follow.\n\nGLM 5\n\nIt improved, better overall. Good instruction following.\n\nSonnet 4.6\nHuman-like work... outstanding.\n\nGLM 5 can do the work, but the MCP servers weren't working all the time.\n\nSonnet was flawless. For me, time is the constraint... so I ended up with the Max plan and left GLM and Minimax.",
          "score": 1,
          "created_utc": "2026-02-28 22:14:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yck12",
          "author": "Lpaydat",
          "text": "I‚Äôm currently on the z.ai coding plan, which gives me almost unlimited access to GLM for a year.\n\nFrom my experience, GLM 4.x provided a poor overall experience. Because of that, I had to subscribe to Claude Max to handle most of my tasks, leaving GLM mostly unused.\n\nHowever, after GLM 5 was released, I decided to give it another try and used Opus only for code review. It turned out that the code quality was usually excellent, with only a few minor issues. I‚Äôm very impressed üëç\n\nDidn't touch other Chinese models yet.",
          "score": 1,
          "created_utc": "2026-02-28 22:19:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yhxkj",
          "author": "renan_william",
          "text": "I migrated from Claude Max 5x to GLM5 (4.5 at that time), and it works fine for me. What I noticed is that Sonnet and other \"top\" models can \"guess\" better what I need without complex specs. It's really like magic\n\nGLM is less guesser and you need to take time on the spec phase. I have a massive codebase, and GLM is shining day by day. For some very specific tasks, I use Codex 5.3, mainly to review GLM specs when they don't feel complete to me... Helps a lot on build phase",
          "score": 1,
          "created_utc": "2026-02-28 22:48:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yjs1h",
          "author": "SnooRobots2278",
          "text": "Last week i have been using them for coding tasks. So far the only one that feels in a similar way to sonnet or gpt 5.2 is GLM 5",
          "score": 1,
          "created_utc": "2026-02-28 22:59:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ysbpm",
          "author": "Training_Vehicle1916",
          "text": "Minimax 2.5 is usually the weaker one ‚Äî Kimi-2.5 is clearly better in most cases.  \nThat said, Kimi can have blind spots in certain specialized areas and occasionally makes mistakes, so relying on it alone can be risky. Yesterday when I had it review some audio synthesis code, it lacked the necessary knowledge and mistakenly flagged several normal features as critical bugs (like claiming reverb extends the total playback time, etc.).\n\nI don‚Äôt just use those two ‚Äî I also use GLM and others depending on the task, so combining multiple models is the way I go.",
          "score": 1,
          "created_utc": "2026-02-28 23:49:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ytmel",
          "author": "japherwocky",
          "text": "huh, I've actually had pretty good luck with minimax.  it's been really good at writing code for me in opencode.",
          "score": 1,
          "created_utc": "2026-02-28 23:57:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yxddz",
          "author": "StardockEngineer",
          "text": "MiniMax is great as long as you understand it doesn‚Äôt do well with vague.  The more detail the better.  It can even provide the detail if you just ask it to plan.  \n\nI‚Äôve never had a problem with tool calls with it. Not even once.",
          "score": 1,
          "created_utc": "2026-03-01 00:19:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7z4ea2",
          "author": "razrcallahan",
          "text": "Probabaly because you're taking a prompt/workflow/method that you developed optimizing for sonnet or codex and now you're judging these other models on that benchmark.",
          "score": 1,
          "created_utc": "2026-03-01 01:00:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zoljk",
              "author": "TheDevilKnownAsTaz",
              "text": "If these models are benchmarked to be sonnet level intelligence, how is it too much to ask that I actually get sonnet level performance?",
              "score": 1,
              "created_utc": "2026-03-01 03:04:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o802ehz",
                  "author": "razrcallahan",
                  "text": "Not talking about performance! I'm saying that your prompts/spec that you tailored for sonnet as in overtime your brain has learnt what used to work on sonnet well (optimized prompts/specs for sonnet) and now you're tryna slap those on these models. First find out what works better for these Chinese models, how to write prompts/spec for these and use that to judge.",
                  "score": 1,
                  "created_utc": "2026-03-01 04:38:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7z9m4m",
          "author": "keroro7128",
          "text": " According to data from OpenRouter, four of the five most used models last week were glm 5, kimi 2.5, maxmini m2.5, and deepseek, and these were all paid API users. Of course, this could be because users of models like Claude, Gemini, and Codex might choose to use the official subscription service.",
          "score": 1,
          "created_utc": "2026-03-01 01:32:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80bzbi",
          "author": "MrBansal",
          "text": "Minimax is terrible",
          "score": 1,
          "created_utc": "2026-03-01 05:50:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80cafs",
          "author": "Educational_Bed8895",
          "text": "Many propaganda bots nowadays",
          "score": 1,
          "created_utc": "2026-03-01 05:53:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o81chsg",
          "author": "Capevace",
          "text": "planning step sonnet 4.6, then let Kimi/GLM do the work",
          "score": 1,
          "created_utc": "2026-03-01 11:28:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o81k0g6",
          "author": "Key_Credit_525",
          "text": ">¬†GLM-5, MiniMax M2.5, Kimi K2\n\n\nsmelly comunists garbage like the rest disposable crap from garbage dump teemoo _(or peemoo no one cares)_",
          "score": 1,
          "created_utc": "2026-03-01 12:33:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o81nqgm",
          "author": "Desperate-Bath5208",
          "text": "GLM 5 is crazy good, like crazy good",
          "score": 1,
          "created_utc": "2026-03-01 13:00:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o829gub",
          "author": "AphexIce",
          "text": "It's ok for basic tasks and it's slow as ...",
          "score": 1,
          "created_utc": "2026-03-01 15:10:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o83h0zm",
          "author": "BingpotStudio",
          "text": "People who can‚Äôt afford Claude Max like to convince themselves that they don‚Äôt need it IMO. The competition isn‚Äôt close and probably won‚Äôt be so long as Claude keeps progressing.",
          "score": 1,
          "created_utc": "2026-03-01 18:38:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o847n7w",
          "author": "d4t1983",
          "text": "Glad I‚Äôm not the only one either, I‚Äôm about to play with qwen3.5 but basically had the same experience as you.",
          "score": 1,
          "created_utc": "2026-03-01 20:51:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o8580al",
              "author": "TheDevilKnownAsTaz",
              "text": "What provider are you using for qwen? I wanted to try it out just to confirm that it isn‚Äôt up to par",
              "score": 1,
              "created_utc": "2026-03-02 00:10:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o84kior",
          "author": "Cheeks2184",
          "text": "These three models are all excellent at executing tasks with specific instructions. They are terrible at large scale planning compared to Opus and Codex.",
          "score": 1,
          "created_utc": "2026-03-01 21:58:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o85pw9d",
          "author": "littlemissperf",
          "text": "M2.5 is forgetful and stupid.",
          "score": 1,
          "created_utc": "2026-03-02 01:58:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o85uo69",
          "author": "HTMLCSSJava",
          "text": "Use Kimi from Kimi for Code API. Miles better than any other open source. Honestly would say it‚Äôs around Sonnet 4-4.5‚Äôs level. I have Codex plan, use Kimi to execute. Pretty good results on my end.\n\nMinimax and GLM 5 are not great. The only runner up is Qwen 3.5 but it‚Äôs still not on Kimi‚Äôs level.",
          "score": 1,
          "created_utc": "2026-03-02 02:27:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o8606cq",
          "author": "phpadam",
          "text": "GLM-5 is quick and quality, until it's slow and stupid. Inference is so variable. In my experience, if it's slow, it's also going to be stupid.",
          "score": 1,
          "created_utc": "2026-03-02 03:01:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o86n6yi",
          "author": "alanw707",
          "text": "Worked pretty well for my experience, they are all slower for me when compare to sonnet and codex but they are definitely good enough to handle all tasks",
          "score": 1,
          "created_utc": "2026-03-02 05:46:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xph5j",
          "author": "HarjjotSinghh",
          "text": "wow that's way faster - glm must be eating the same diet.",
          "score": 0,
          "created_utc": "2026-02-28 20:14:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xpxhs",
          "author": "rescbr",
          "text": "At least for GLM-5, from Z.ai's coding plan.",
          "score": 0,
          "created_utc": "2026-02-28 20:17:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xvy79",
          "author": "telewebb",
          "text": "This is just my personal opinion. I think we might be seeing is a move away from everyone using \"the best\" and towards using the one we just like working with. I love minimax m2.5 and I dont think it's \"the best\" at anything. But just like there is certain personalities of coworkers I like working with, I think this is my favorite LLM model personality I like working with.",
          "score": 0,
          "created_utc": "2026-02-28 20:49:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7z7cai",
              "author": "TheDevilKnownAsTaz",
              "text": "I have honestly tried using it and MiniMax 2.5 has been useless for any tasks I give it.",
              "score": 2,
              "created_utc": "2026-03-01 01:18:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ze5tp",
                  "author": "telewebb",
                  "text": "Idk man, I wish there was an easier way to export and share workflows. Minimax just clicks with how I use OpenCode. I spend more time upfront doing design and spec writing just like I do at work and that model just completes it better then the others. I think what I really like is that it does less of the \"you're absolutely correct...\" and I value that a lot. I do have to gatekeep checking off tasks to a reviewer sub-agent. That's been a big win.",
                  "score": 1,
                  "created_utc": "2026-03-01 02:00:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rdocla",
      "title": "Found a way to touch grass and use Mac terminal from my iPhone so I can be vibecoding and live a balanced life",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/seyx8usijhlg1.jpeg",
      "author": "eureka_boy",
      "created_utc": "2026-02-24 18:30:13",
      "score": 89,
      "num_comments": 85,
      "upvote_ratio": 0.77,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rdocla/found_a_way_to_touch_grass_and_use_mac_terminal/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7798av",
          "author": "d2xdy2",
          "text": "I‚Äôll be a hater here. That‚Äôs not a balanced life.",
          "score": 23,
          "created_utc": "2026-02-24 20:26:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gzkev",
              "author": "veegaz",
              "text": "Wasn't OP being sarcastic",
              "score": 2,
              "created_utc": "2026-02-26 06:24:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o779ycv",
              "author": "eureka_boy",
              "text": "Hahaha this is the only balanced life i know of",
              "score": 4,
              "created_utc": "2026-02-24 20:29:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o77bidy",
                  "author": "d2xdy2",
                  "text": "Being extreme, I‚Äôd liken it a coke addict having a bump in their pocket for a quick pick me up vs binging the whole eight ball at home. \n\nYou‚Äôre still doing the thing. I‚Äôll never be onboard with this idea that being so attached to vibe coding is healthy or balanced",
                  "score": 6,
                  "created_utc": "2026-02-24 20:36:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76prac",
          "author": "Dudmaster",
          "text": "With opencode web you wouldn't need a terminal",
          "score": 13,
          "created_utc": "2026-02-24 18:56:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76q7lj",
              "author": "eureka_boy",
              "text": "well all my code is in my local mac, I still prefer a cli interface over a generic chat interface on the web. ",
              "score": -2,
              "created_utc": "2026-02-24 18:58:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o76sxz7",
                  "author": "Dudmaster",
                  "text": "\"web\" name might be misleading, it's a server that would be on your mac, the interface also has shell access",
                  "score": 3,
                  "created_utc": "2026-02-24 19:10:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76llww",
          "author": "drinksbeerdaily",
          "text": "IPhones doesn't have SSH clients? I see all these vibe coded ways to connect to claude, codex, opencode and I wonder what the point is. I use termux + mosh over VPN on my Android phone, and not really missing anything.",
          "score": 14,
          "created_utc": "2026-02-24 18:38:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76muhh",
              "author": "eureka_boy",
              "text": "yes if you have already done the VPN setup then sure go ahead with what you have. I hate setting up vpn, tailscale stuff because it is just too much. Also ssh is kind of slow if you don't setup right but I used webrtc here, so it should be significantly faster because now you are directly connecting instead of seperate vpn network in between. \n\nFYI: this is only for Mac and iPhone",
              "score": -16,
              "created_utc": "2026-02-24 18:43:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o76u4y7",
                  "author": "onil34",
                  "text": "brother what. How is installing tailscale more of a hassle than WRITING AN ENTIRE APP",
                  "score": 25,
                  "created_utc": "2026-02-24 19:16:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76z01v",
                  "author": "drinksbeerdaily",
                  "text": "How is ssh slow? We're talking kilobytes of text here. If you don't like tailscale, just use wireguard directly. I always use a vpn when remotely connecting home, because it's way more secure than exposing your service to the public internet will ever be.",
                  "score": 3,
                  "created_utc": "2026-02-24 19:38:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o773qve",
                  "author": "Delicious_Ease2595",
                  "text": "Tailscale is so simple not sure how you find it \"just too much\".",
                  "score": 3,
                  "created_utc": "2026-02-24 20:00:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77adhe",
          "author": "AltruisticRip5151",
          "text": "If you‚Äôre just trying to reach your own machines, Tailscale is the more robust default.\n\nMacky is clever, but it relies on a central signaling service to broker a direct WebRTC session and a custom Mac app, which adds extra third-party trust and attack surface compared to standard SSH over a private mesh.\n\nYou don‚Äôt need to toggle Tailscale on or off either. Traffic only goes over it if you point at app at a hostname. It runs 24/7 on my iPhone without issues even between restarts and updates.",
          "score": 5,
          "created_utc": "2026-02-24 20:31:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7n42wv",
              "author": "Vast-Information-779",
              "text": "love tailscale, it gives you access to all your server ports as well which really helpful for remote web dev. tailscale + termius + tmux is soo op for accessing servers from phone  . i use it for managing my home lab and honestly thats the primary way i access my homelab, no need to fuck around with tunnels and stuff",
              "score": 2,
              "created_utc": "2026-02-27 03:52:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o77dho7",
          "author": "getfitdotus",
          "text": "https://github.com/chriswritescode-dev/opencode-manager this is much more phone ui friendly. Plus all the integration GIT, tts , file editing",
          "score": 4,
          "created_utc": "2026-02-24 20:46:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7amebn",
              "author": "Potential-Leg-639",
              "text": "This!\nThat‚Äòs open source as it should be for OpenCode related stuff",
              "score": 2,
              "created_utc": "2026-02-25 08:42:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7bbirm",
              "author": "smvueno",
              "text": "Wooow thanks for sharing, this is really cool!! üòÅ Mobile first ftw!!",
              "score": 2,
              "created_utc": "2026-02-25 12:20:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o76zy60",
          "author": "rySeeR4",
          "text": "You vibe coded a shitty app and dont know anything about opencode web and how to expose it and just want the attention, take my down vote",
          "score": 5,
          "created_utc": "2026-02-24 19:42:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o772fz9",
              "author": "drinksbeerdaily",
              "text": "Charging $29 for a closed-source vibe-coded terminal relay when tailscale ssh exists for free is a tough sell.",
              "score": 7,
              "created_utc": "2026-02-24 19:54:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o77mh65",
                  "author": "onil34",
                  "text": "HE IS CHARGING 29$ FOR THIS?",
                  "score": 4,
                  "created_utc": "2026-02-24 21:27:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o79v9oc",
          "author": "lol_idk_234",
          "text": "Yo bring this to windows and make a IntelliJ plugin üôè",
          "score": 2,
          "created_utc": "2026-02-25 04:54:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7a4fa4",
          "author": "Ancient_Topic_6416",
          "text": "I have been doing this for free with termius+tailscale. Why make it complicated? Hmmm\n\n\nEdit: And pricey at 29/month.",
          "score": 2,
          "created_utc": "2026-02-25 06:03:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a8l8c",
              "author": "eureka_boy",
              "text": "$29/lifetime",
              "score": 1,
              "created_utc": "2026-02-25 06:37:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o771vgi",
          "author": "layer4down",
          "text": "A lot of shade in this thread bro never mind all that. Keep doin you and maybe post over in r/LocalLLama for like-minded folks who like local everything and reinventing wheels for the thrill of it.",
          "score": 3,
          "created_utc": "2026-02-24 19:51:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o772ac4",
              "author": "eureka_boy",
              "text": "thanks bro!",
              "score": 1,
              "created_utc": "2026-02-24 19:53:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o778gvp",
          "author": "MKU64",
          "text": "What is it with all the shade in here? Nice job! I made one myself and regardless of what everyone says it‚Äôs the journey to productivity that matters and what will actually make you continue coding anywhere.",
          "score": 2,
          "created_utc": "2026-02-24 20:22:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77y39k",
              "author": "ImagineBeingPoorLmao",
              "text": "It's the same app every day.",
              "score": 3,
              "created_utc": "2026-02-24 22:21:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o779t93",
              "author": "eureka_boy",
              "text": "yeah thanks bro! Yes I don‚Äôt know why people are taking offence to a side project lol",
              "score": 2,
              "created_utc": "2026-02-24 20:28:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76rkm6",
          "author": "jamesrossdev",
          "text": "I have been wanting to make something like this that allows me to connect to my local pc terminal from my phone, without having to walk 10 meters to my computer. \n\nReally cool seeing you do it and I guess I'll get working on it as well. Btw I also hate doing the VPN setup. It's nice in the long run but tedious to set up",
          "score": 1,
          "created_utc": "2026-02-24 19:04:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76ufik",
              "author": "onil34",
              "text": "Tailscale is literally \n1. install \n2. tailscale up\n3. Log in \n4. Done",
              "score": 6,
              "created_utc": "2026-02-24 19:17:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76veta",
                  "author": "jamesrossdev",
                  "text": "I should also add that I prefer making my own tools, but I guess I'll try out Tailscale too. Thanks!",
                  "score": -1,
                  "created_utc": "2026-02-24 19:21:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76vo8u",
                  "author": "eureka_boy",
                  "text": "sure bro, then you need a seperate app in your iphone for ssh. To connect, first you need to turn on the tailscale thing each time in your phone and then go to some ssh app in app store which costs $ if you want a good one. Also tailscale just raised a massive Series C funding of $160M (https://tailscale.com/blog/series-c), you think it is going to be free forever?",
                  "score": -1,
                  "created_utc": "2026-02-24 19:23:11",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o76ssvz",
              "author": "eureka_boy",
              "text": "yesss if you love networking stuff, this should be a fun thing to work on!",
              "score": 1,
              "created_utc": "2026-02-24 19:10:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76z0kr",
          "author": "ciprianveg",
          "text": "I like it! Can it be ported to android?",
          "score": 1,
          "created_utc": "2026-02-24 19:38:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77146y",
              "author": "drinksbeerdaily",
              "text": "Sir, there are FREE, battletested and open source alternatives to let you easily connect remotely to a terminal on Linux or MacOS.",
              "score": 4,
              "created_utc": "2026-02-24 19:48:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7723l4",
                  "author": "eureka_boy",
                  "text": "okie this is not simple ssh app, this is an app that allows you to establish a direct p2p webrtc connection between your mac and iphone. This just makes accessing your mac terminal much simpler and easier to setup. \n\nIf you want ssh, you need to first setup VPN so you can access it from anywhere. Which i hate to setup",
                  "score": 1,
                  "created_utc": "2026-02-24 19:52:46",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o76ziqs",
              "author": "eureka_boy",
              "text": "my google play dev account is being weird so might take a few weeks to release the app on android",
              "score": 2,
              "created_utc": "2026-02-24 19:40:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77no8u",
          "author": "andrewchen5678",
          "text": "I built something similar but entirely web based with mobile optimized interfaces that is completely open source that can run anything on the terminal [https://github.com/andrewtheguy/mywebterm](https://github.com/andrewtheguy/mywebterm)",
          "score": 1,
          "created_utc": "2026-02-24 21:32:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o796bbo",
          "author": "Otherwise_Bee_7330",
          "text": "the balls to charge for slop these days ü•µ",
          "score": 1,
          "created_utc": "2026-02-25 02:22:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7a0xt2",
          "author": "Alberion",
          "text": "For anyone else who is looking for a similar thing to this (with no other depencencies), I spun up a free OCI tier instance and ssh into the instance from phone with JuiceSSH.\n\nhttps://preview.redd.it/pa4d9bu2vklg1.png?width=1080&format=png&auto=webp&s=b410afd4e11b40b459bb8412aa8b702285c731ac",
          "score": 1,
          "created_utc": "2026-02-25 05:35:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ab9w7",
          "author": "Far_Complex8736",
          "text": "How are you going to verify the changes incase it is a front end or a backend change?",
          "score": 1,
          "created_utc": "2026-02-25 07:00:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aehk0",
          "author": "Procrastinator9Mil",
          "text": "Is this E2E encrypted?",
          "score": 1,
          "created_utc": "2026-02-25 07:29:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7afphq",
              "author": "eureka_boy",
              "text": "Yes: https://macky.dev/#architecture",
              "score": 1,
              "created_utc": "2026-02-25 07:40:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7bev6k",
          "author": "NerdistRay",
          "text": "Tailscale",
          "score": 1,
          "created_utc": "2026-02-25 12:42:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7en64d",
          "author": "IncomeAsOutcome",
          "text": "Tailscale + Terminus on Android.",
          "score": 1,
          "created_utc": "2026-02-25 22:01:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g27p8",
          "author": "throwaway12012024",
          "text": "why not just use opencode web in your computer + tailscale to access it in your iphone?",
          "score": 1,
          "created_utc": "2026-02-26 02:39:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gwj9p",
              "author": "soundslikeinfo",
              "text": "Here's how to set up OpenCode web and access it via Tailscale:\n\n1. Start OpenCode Web\n\nopencode web\n\nThis starts on [127.0.0.1](http://127.0.0.1) by default.\n\n2. Make it accessible over network\n\nTo use with Tailscale, bind to all interfaces and set a password:\n\nopencode web --host [0.0.0.0](http://0.0.0.0) \\--port 4096\n\nSet a password for security:\n\nexport OPENCODE\\_SERVER\\_PASSWORD=\"your-password\"\n\nopencode web --host [0.0.0.0](http://0.0.0.0)\n\n3. Connect via Tailscale\n\n1. Ensure Tailscale is running on both machines\n\n2. Find your Tailscale IP: tailscale ip -4\n\n3. Access from another machine at http://<tailscale-ip>:4096\n\n4. Optional: Enable HTTPS with Caddy or nginx reverse proxy\n\nFor encrypted traffic over Tailscale, you can set up a simple reverse proxy, or just use Tailscale's built-in encryption (it already encrypts all traffic).\n\nNote: Tailscale's network is already encrypted, so the OPENCODE\\_SERVER\\_PASSWORD provides app-level authentication. Make sure to set a strong password if exposing to network access.",
              "score": 2,
              "created_utc": "2026-02-26 05:59:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kfh7c",
          "author": "sullenisme",
          "text": "long ass nails",
          "score": 1,
          "created_utc": "2026-02-26 19:19:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7q8pov",
          "author": "atkr",
          "text": "more slop",
          "score": 1,
          "created_utc": "2026-02-27 16:52:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vizhy",
          "author": "Pixer---",
          "text": "When installing it on macOS I can‚Äôt login with Apple which I just created an account with on my iPhone",
          "score": 1,
          "created_utc": "2026-02-28 13:24:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vl4ta",
              "author": "eureka_boy",
              "text": "Are you the one who bought the lifetime pass from ios app? I emailed you maybe check your spam. If not then yes I didn't implement Apple Auth in the current Mac app version. I will be releasing the Mac app with my next major update in the next few days once App Store review approved my v2 iOS app. In the meantime, if you want you can signup with google account. I will lyk when it goes live!",
              "score": 1,
              "created_utc": "2026-02-28 13:37:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7vzfbd",
                  "author": "Pixer---",
                  "text": "I didn‚Äôt but this app is what I‚Äôm missing in opencode. Does communication run over your server ? How is the data send over private ?",
                  "score": 1,
                  "created_utc": "2026-02-28 15:00:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7yomym",
          "author": "HarjjotSinghh",
          "text": "this feels like life hack nirvana.",
          "score": 1,
          "created_utc": "2026-02-28 23:27:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76w4a7",
          "author": "momentary_blip",
          "text": "Lol just use exe.dev",
          "score": 0,
          "created_utc": "2026-02-24 19:25:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76wup4",
              "author": "eureka_boy",
              "text": "huh? are you a bot? ",
              "score": 1,
              "created_utc": "2026-02-24 19:28:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o79tdku",
                  "author": "momentary_blip",
                  "text": "why would i be a bot",
                  "score": 1,
                  "created_utc": "2026-02-25 04:41:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rfsbdm",
      "title": "Estimate of OpenCode Go Limits - I think its about 60M/mo, 30M/w, 12M/5hr",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rfsbdm/estimate_of_opencode_go_limits_i_think_its_about/",
      "author": "mcowger",
      "created_utc": "2026-02-27 00:58:53",
      "score": 62,
      "num_comments": 17,
      "upvote_ratio": 0.94,
      "text": "I paid the $10 just to see what the performance and limits look like.\n\nPerformance is average - no problems, but also not amazed.\n\nI recorded every single request I made for the first day in my [proxy](https://github.com/mcowger/plexus) \\- a total of 207 requests.\n\nBased on the token counts and the reported '% used' on the website:\n\n\\* Monthly: 60M tokens or 1150 requests  \n\\* Weekly: 30M tokens or 575 requests  \n\\* Rolling: 12M tokens or 225 requests\n\nThe numbers come out to within about 1% of those round numbers, so I think its pretty reasonable.  Its not clear if they count by requests or tokens.\n\nAssuming you consume all 60M tokens, with M2.5, thats about $18 worth of inference.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rfsbdm/estimate_of_opencode_go_limits_i_think_its_about/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7mhm9w",
          "author": "alovoids",
          "text": "these token estimates make sense. i also made some rough calculation in https://www.reddit.com/r/opencodeCLI/s/xY1i1xl2S6\n\nEDIT: to make it easier for anyone who sees this post, the current estimates ranged from $18-$22",
          "score": 8,
          "created_utc": "2026-02-27 01:39:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nvfr1",
          "author": "lemon07r",
          "text": "to put into perspective. $20 droid plan is 20m x 2.5 in GLM 5 tokens. so, 50m tokens a month. this makes opencode go better IF your minimax usage is counted the same as glm usage. if it's not.. you get 20m x 8.3 in minimax tokens for $20 on droid plan. That's 167m minimax tokens. So I guess droid plan is better value if you wanted to use minimax? If OP or someone else could test if GLM 5 counts as the same amount of usage as minimax that would be nice. ",
          "score": 7,
          "created_utc": "2026-02-27 07:17:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qcxpx",
              "author": "Wildnimal",
              "text": "Whats a $20 droid plan? ",
              "score": 1,
              "created_utc": "2026-02-27 17:12:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7rdgfy",
                  "author": "lemon07r",
                  "text": "$20 plan on factory froid",
                  "score": 1,
                  "created_utc": "2026-02-27 20:07:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7n129l",
          "author": "SelectionCalm70",
          "text": "That's actually a generous limit for a 10 dollar plan t h",
          "score": 5,
          "created_utc": "2026-02-27 03:33:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7negyq",
          "author": "MorningFew1574",
          "text": "Thanks for the analysis, it helps. I'm currently trying to decide whether to opt for Go plan or not. Just unsubscribed from GLM 5 pro quarterly plan due to price hike.",
          "score": 2,
          "created_utc": "2026-02-27 05:01:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mbfag",
          "author": "HarjjotSinghh",
          "text": "still counting on this one, genius!",
          "score": 2,
          "created_utc": "2026-02-27 01:03:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7paiba",
          "author": "HenryTheLion_12",
          "text": "That's quiet decent. I have a plan in mind to subscribe to opencode go and chutes 3 $ plan. mostly chutes works just fine but when it does not i can shift to opencode go. for 13 $ that looks like a good deal. Droid is also good, but the interface is not that good and the way it counts tokens it doesn't feel like it will last that long.",
          "score": 1,
          "created_utc": "2026-02-27 14:03:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pfty9",
          "author": "peyloride",
          "text": "I'm curious about prompt caching while counting these tokens. Does it reduce the usage if agent uses caching properly or does it no effect at all?",
          "score": 1,
          "created_utc": "2026-02-27 14:32:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7prqcr",
              "author": "mcowger",
              "text": "I had caching enabled (and working) for these tests.  \n\nIt didn‚Äôt seem to make a difference.",
              "score": 1,
              "created_utc": "2026-02-27 15:31:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qa984",
          "author": "jrhabana",
          "text": "How good is compared with nanogpt and kilogateway?",
          "score": 1,
          "created_utc": "2026-02-27 16:59:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7s0ogr",
              "author": "mcowger",
              "text": "I avoid nGPT so I can‚Äôt speak to that.  \n\nKilogateway is faster for sure, but also more expensive by a bit.",
              "score": 1,
              "created_utc": "2026-02-27 22:04:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7s722n",
                  "author": "jrhabana",
                  "text": "why you avoid nGpt? ",
                  "score": 1,
                  "created_utc": "2026-02-27 22:38:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7on1s2",
          "author": "odrakcir",
          "text": "which those numbers we can easily use nvidea rim",
          "score": 0,
          "created_utc": "2026-02-27 11:30:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7olm2t",
          "author": "evnix",
          "text": "will recommend NanoGPT which has higher limits, though its not great for coding as i found it slow during peak times, but should be ok for most other things,  \nalso, If you are looking for a NanoGPT referral link with that discount like I was, you can use mine:¬†[https://nano-gpt.com/r/wdD9Gnti](https://nano-gpt.com/r/wdD9Gnti)",
          "score": -1,
          "created_utc": "2026-02-27 11:18:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfdadw",
      "title": "Stay away from  synthetic.new",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rfdadw/stay_away_from_syntheticnew/",
      "author": "Codemonkeyzz",
      "created_utc": "2026-02-26 15:34:01",
      "score": 61,
      "num_comments": 67,
      "upvote_ratio": 0.85,
      "text": "I saw this provider a lot in reddit. Some guys keep promoting it and i got hooked.  20 USD a month, x3 Claude Usage ,  no weekly limits.  Too good to be true. However,  there are a problems with the provider: \n\n 1.  **Standard Plan 5 hour  limit is  x3 of Claude Pro Plan:**   Maybe this is correct in theory, but in practice not at all.  Maybe due to caching or another reason,   the plan hits the limit pretty quickly.  Also I believe Chinese models can be inefficient with the tool calling  hence,   Standard Plan 5 hour limit is same as  Codex/Claude  20 USD plan.  \n  \n2. **Impractial Usage:**  Since  for a regular coding task you will hit  5 hour limit pretty quickly on their standard model , having no weekly limit has no advantages for the developers at all.  The existing plan is actually made for the abusers , which is funny cause the provider keep complaining about some accounts abusing their system while  they are the one actually allowing it in the first place.  Cause the provider is  for  bots  not for regular developer.\n\n3. **Price Increase:**  They increased the price  from  20 USD to **30 USD** for standard plan last night . Their ratioanle is  \"They need a lot of compute\".  But the reason for the need for compute is  that, their bad planning.  There's no way an everyday  coder/user can abuse this system,  you need to be  24/7 online, which means this for bots and bots are abusing it but they want everyone to pay for it. \n\n**4. Delayed model release:**   Even  opencode was serving GLM5 , Minimax M2.5 and Kimi K2.5  for **free.** And as of today, they are still not serving  GLM5 and Minimax M2.5  only  K2.5.  They are using the same excuse ;   shorteage of compute/GPUs.\n\n  \nI already cancelled my subscription. Just shariing this so that , you don't fall for their false advertisement on reddit as i did. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rfdadw/stay_away_from_syntheticnew/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7j434t",
          "author": "dengar69",
          "text": "Thanks.  I was looking into this but for $30 I'll just pay $9 more and get Github Copilot+.",
          "score": 13,
          "created_utc": "2026-02-26 15:41:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7j4x61",
              "author": "Codemonkeyzz",
              "text": "That's what I did. Using codex +Copilot pro + nanogpt .  Good value of money. 38 USD. \n\nI think synthetic only makes sense with openclaw or something. Not for development",
              "score": 5,
              "created_utc": "2026-02-26 15:44:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7jeei0",
                  "author": "sucksesss",
                  "text": "can we use an AI model like Opus from Copilot Pro+ in OpenCode using an API key?",
                  "score": 1,
                  "created_utc": "2026-02-26 16:28:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o82lpjd",
                  "author": "blankeos",
                  "text": "I did not have a good experience w/ Nanogpt. Ttft was too slow and the same models seemed dumber than the source providers (i.e. GLM ) and I did compare",
                  "score": 1,
                  "created_utc": "2026-03-01 16:10:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ld95b",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-02-26 22:00:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lt1c7",
                  "author": "FlyingDogCatcher",
                  "text": "They use the same models you can get anywhere else",
                  "score": 1,
                  "created_utc": "2026-02-26 23:21:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7sd15c",
              "author": "BERLAUR",
              "text": "Yeah, synthetic made sense at 10 bucks or perhaps at 20 but definitely not at 30.¬†\n\n\nThey also don't have GLM 5 so effectively you're limited to Kimi 2.5, it's a capable model but others offer more for less.¬†",
              "score": 1,
              "created_utc": "2026-02-27 23:11:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ja996",
          "author": "FlyingDogCatcher",
          "text": "I have had their pro sub for a while. Even with the pricing changes still one of the best values you can get and has been a stable and fast provider.\n\nI like synthetic.new quite a lot. I understand their marketing but it doesn't make a ton of sense to directly compare them to claude. They are hosting free OSS models for you.\n\nMy prediction by the end of the year is that the buffet-style AI plans from the big guys are either going to get killed or massively nerfed as they try to make the the numbers make sense. Synth doesn't have the luxury of bottomless investment money so they are making that adjustment now",
          "score": 19,
          "created_utc": "2026-02-26 16:09:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sdstn",
              "author": "BERLAUR",
              "text": "Perhaps you'll be right.¬†\n\n\nA lot of the buffet style plans must be losing money (I'm using codex for 10+ hours a day at 20 bucks per month, there's no way that makes economic sense).\n\n\nBut in the current landscape synthetic just isn't competitive. I even considered getting everyone on the team a subscription but synthetic couldn't do company invoicing. I'm obviously not going to deal with 5-10x the hassle for an inferior product at a now higher price.¬†",
              "score": 1,
              "created_utc": "2026-02-27 23:15:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7jcbxd",
              "author": "Codemonkeyzz",
              "text": "I am just explaining myself from a developer point of view. I believe synthetic  current pricing make s a lot of sense for openclaw and bots, but it is definitely not developer friendly. ",
              "score": -4,
              "created_utc": "2026-02-26 16:18:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7jckfj",
                  "author": "FlyingDogCatcher",
                  "text": "If you say so. Been working great for me.",
                  "score": 5,
                  "created_utc": "2026-02-26 16:19:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7jd630",
                  "author": "ZeSprawl",
                  "text": "If anything the new pricing makes openclaw far less attractive, and has made it better for devs. I use it for daily software dev, and I voluntarily converted my plan yesterday from pro to two packs, and coded for 6+ hours without hitting a limit.",
                  "score": 3,
                  "created_utc": "2026-02-26 16:22:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7jc3qc",
          "author": "Infinite_Grab_7315",
          "text": "I agree too. I used to be subscribed. But they have changed their plan terms twice already in a single month. They really shot themselves in the foot by over promising in my opinion. \n\nThey also introduced a weird 500 tool call limit a day which is super easy to hit",
          "score": 6,
          "created_utc": "2026-02-26 16:17:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jck0v",
              "author": "Codemonkeyzz",
              "text": "Yeah i forgot to mention that 500 tool limit introduction  in my post. ",
              "score": 1,
              "created_utc": "2026-02-26 16:19:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7j7b3b",
          "author": "LittleChallenge8717",
          "text": "sadly it's true, initially it was great, Did't like new system at all",
          "score": 5,
          "created_utc": "2026-02-26 15:55:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pbp1m",
          "author": "harrypham2000",
          "text": "You actually forgot to mention that synthetic.new is having top tier tool-calling success rate between AI interference services. And their support is awesome respond immediately regarding any issues. Before releasing any models they have been going through different tests to make sure that the speed is fair enough for example I noticed glm 4.7 is slow from z.ai but from synthetic.new I got over 140 TPS. I agree that they are not catching up with the latest models but intend to giving the most optimized versions, like recently deploying Kimi 2.5 NVFP4 which slightly better than original Kimi 2.5, and still my goto model atm.",
          "score": 4,
          "created_utc": "2026-02-27 14:10:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pfvzh",
              "author": "Codemonkeyzz",
              "text": "Yeah, Kimi 2.5 is the only model i use from them as well. I think they need to figure our their business plan, i could understand model delays , but i cannot accept the unfair limits.",
              "score": 1,
              "created_utc": "2026-02-27 14:32:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ljl4e",
          "author": "GoranKrampe",
          "text": "I have been using Synthetic quite a lot and I find them super friendly and honest. So the title here irks me the wrong way \"stay away\" etc. Also, I do not think they have been doing much advertising (I may be wrong) but rather users have recommended them (I have done it a few times in comments). It is a very small company and they are bending over backwards to make the leading open weight models work well from the harnesses we like the most like Claude Code and Opencode etc. I started with the $20 which IMHO was much harder to run out of than the Anthropic Pro subscription. Then I upped to the $60 option which gave 10x and I never ever came close running out. But as Synthetic have clearly admitted, it was a non sustainable plan. I think the new plan scheme with \"packs\" is a good idea and I have every trust in that they are honestly trying to find a model that works well.  \n  \nAnyway, I am super happy with their service, they are extremely helpful and very transparent in what they do. I use mainly Kimi K2.5, Minimax M2.5 and a bit of Deepseek 3.2/GLM4.7. And I use it purely for coding. Just my 2c, felt this thread was... oddly negative given mine and many others experiences. :) ",
          "score": 10,
          "created_utc": "2026-02-26 22:31:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7mhv30",
              "author": "Terryble_",
              "text": "I was on their $20 plan and even I thought it was pretty generous before they moved to their current pricing model.\n\nI'm also baffled by this post because the Synthetic team has been very honest about the way they operate. They're also responsive in their Discord, so it's hard to think badly of them.\n\nThey're also helpful in their GitHub. I contributed a fix to their harness, [Octofriend](https://github.com/synthetic-lab/octofriend), and they were responsive all throughout the process of getting the PR merged.\n\nI still have a lot of trust in Synthetic at this point in time. From my perspective, they're still one of the good ones, so I have no reason to stop using them. ",
              "score": 3,
              "created_utc": "2026-02-27 01:40:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7llqhy",
              "author": "commandedbydemons",
              "text": "Full agree.",
              "score": 2,
              "created_utc": "2026-02-26 22:42:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7m5u16",
              "author": "Farm_Boss826",
              "text": "Fully agree, I am in the pro plan and never came even close to max it out for normal development task.",
              "score": 0,
              "created_utc": "2026-02-27 00:32:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7mythu",
              "author": "Codemonkeyzz",
              "text": "My post wasn't about they being dishonest or unfriendly. They are honest and friendly but they have a very bad business plan. It has been bad all along and they are not making any efforts to improve it.  As i mentioned,  their existing plans are not developer friendly,  they make their plans attractive for bot users, and yet complain the very same bot users are abusing their system.\n\nTheir recent  announcements, their recent  pricing changes  and everything proves my  point.",
              "score": -1,
              "created_utc": "2026-02-27 03:19:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7p50ju",
                  "author": "GoranKrampe",
                  "text": "Can you elaborate why you think the plan is \"attractive for bot users\"? I honestly am not sure I understand. You mean the absence of a weekly limit or?",
                  "score": 1,
                  "created_utc": "2026-02-27 13:33:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7javue",
          "author": "exploriann",
          "text": "It used to be good, but they introduced free tool calling daily limit then they hiked the price then removed the 10x pro plan, and reduced the concurrency to 1 per model on the 30$ plan, not worth it anymore.",
          "score": 5,
          "created_utc": "2026-02-26 16:12:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ovl1x",
          "author": "HarjjotSinghh",
          "text": "this is what happens when free feels too good.",
          "score": 2,
          "created_utc": "2026-02-27 12:33:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pg36d",
              "author": "Codemonkeyzz",
              "text": "I don't know what do you mean , but free is not  what i was looking for. I was looking for a fair deal for their subscribers. ",
              "score": 1,
              "created_utc": "2026-02-27 14:33:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vlp1u",
          "author": "ZeusBoltWraith",
          "text": "Stay away from chutes too. They just ultra nerfed developers",
          "score": 2,
          "created_utc": "2026-02-28 13:41:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jn5mw",
          "author": "Bob5k",
          "text": "have in mind opencode was serving those for free for a few days. now they charge you the standard price.  \namongst all other providers for now, synthetic is one of a very few that hosts opensource models in US (mainly) while setting up 0 data retention overall and maintaining a quite good overall quality of the model for everyone while also keeping no weekly cap at all. As 5h caps might be low or high, i don't care but what will stop you from progressing with your projects is a weekly cap suddenly hit after 3 days. and then you're done.   \ncan't be compared to gh copilot or anything monthly based quota as the quota amount provided by synthetic will be indefinitely higher than gh copilot pro.   \nalso the reasoning is not 'they need more gpu' but rather 'they need to make the plan fair for everyone' - if you'd read the announcement correctly then it'd be probably clear (or copy paste it into some ai to explain to you). It still makes a ton of sense for developers as you still get 135 prompts per 5h which is really hard to cap these out working on some serious projects and not just pushing idiotic prompts blindly around to force the ai to progress. And also it makes total sense for me to set the change so I as a dev can receive 80tps+ on kimi k2.5 because the infra doesn't struggle with openclaw bots just abusing the system to the very limit. And not gonna lie - their previous limit of 1.35k prompts per 5h sounded great on paper, but i always wondered how they can sustain it as I as dev even running parallel sessions was able to maye reach 400 prompts usage per 5h max - and yet some people abused the pro plan to very last token. ",
          "score": 2,
          "created_utc": "2026-02-26 17:08:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7k0lng",
              "author": "Infinite_Grab_7315",
              "text": "More than this i feel that current users and terms were not honoured. I was happy paying 20$ but a 50% increase I am not willing to\n\n135 calls with 0.1 toolcall made sense for devs. Since my total tool calls were less\n\nIf I start an explore task on opencode it takes up 40-50 tool call easily. So I can explore at max 1 times. It is same for things like review or any subagents or normal coding. After 500 it is counted as normal which is also easy to hit. Before I would have to make ~600 every 5hr which was generous \n\nAll in all I am not fan of their changes. But what worries me is that if they can change limits like this they will do it again in future and it keeps getting worse for us",
              "score": 2,
              "created_utc": "2026-02-26 18:10:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7k7ve9",
                  "author": "Bob5k",
                  "text": "Nah i don't think anything will be changed there anytime soon. Also they have monthly subscription and not annual so after all if you're unhappy - you can just unsubscribe and move on. For me legacy plan i have now seems to be just fine for real world usecases when I'm running Kimi K2.5. they don't host any annual plan which feels fair as if any changes are done then you can just move on. \n\nI'll just mention annual plans that messed up their userbase way way more that changed recently - Trae (totally messed up any usage within 100$ range), glm reducing quota usage by 33% and adding weekly caps, Kimi code being unusable for any serious dev under 79$ range because of weekly cap aswell. \nNot defending synthetic completely but they still have most balanced subscription model out there that feels just fair considering both sides of the business. Have in mind for past year everyone was running ai inference at a loss so it's not a big surprise companies are changing their plans and basically this is sort of an end of\n1. Using single provider all the time as terms will change \n2. Cheap vibecoding using sota or top opensource models at 10$ / monthly with almost unlimited usage.\n\nPeople will need to realize that running ai is expensive and nobody will throw own money at the business forever for peeps to just run their openclaw instances. And tbh for any sort of serious development price range within 40/60$ / month is still kinda occupied mainly by synthetic if you'd want a reliable provider for 5-8-10h of work daily.",
                  "score": 2,
                  "created_utc": "2026-02-26 18:43:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7jatcf",
          "author": "Juan_Ignacio",
          "text": "I agree. For me, the best value-for-money plans right now are Codex or Minimax. I really like Kimi as well, but its limits don‚Äôt make much sense to me.\n\nI‚Äôm also paying for Chutes, even though it‚Äôs not very popular because of the NanoGPT controversy and some other issues. Overall, the limits are actually pretty decent, but it has a serious speed problem with several of its providers. That said, if you‚Äôre using it in the background, switching between providers, or running agents like oh-my-opencode, it‚Äôs not that bad. However, if you need fast results, it‚Äôs definitely not a good option.\n\nIt‚Äôll be interesting to see what the limits look like for the new Open Zen Go subscription. I saw in another Reddit thread that, according to someone‚Äôs calculations, it comes out to around $20/month in Minimax/Kimi/GPT equivalent usage.",
          "score": 1,
          "created_utc": "2026-02-26 16:11:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jc08u",
              "author": "Codemonkeyzz",
              "text": "\"even though it‚Äôs not very popular because of the NanoGPT controversy and some other issues.\"\n\nHaven't heard about Chutes.  What's the controversy and what issues ?",
              "score": 1,
              "created_utc": "2026-02-26 16:17:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7jd7hn",
                  "author": "Juan_Ignacio",
                  "text": "Chutes is a provider that hosts its own models. Besides their direct subscription, you might also be using them indirectly through OpenRouter or other platforms.\n\nA few months ago, I think Chutes publicly accused NanoGPT of using stolen credit cards or engaging in unfair competition (or something along those lines), but as far as I know, there wasn‚Äôt much solid evidence provided.\n\nI only followed the situation loosely, though. I mostly heard about it because people in the NanoGPT subreddit and Discord *really* dislike Chutes lol.",
                  "score": 3,
                  "created_utc": "2026-02-26 16:22:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7oul08",
                  "author": "Fauxide",
                  "text": "I have chutes and wait 10mins+ for model responses. I'd not recommend them",
                  "score": 1,
                  "created_utc": "2026-02-27 12:26:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7jlpu3",
              "author": "Ang_Drew",
              "text": "well, some ppl posted about chutes controversy üòÖ\nthey said chutes quantized the model and being slow and hallucinating \n\ni honestly dont use it a lot that time (it was Nov 2025 btw), and from price point glm coding plan is more sexy to me back then..\n\nnow a new reddit popped said synt is not worth it..\n\ni think synthetic is pretty generous calculating tool calling 0.1 request. it makes other similar competitor (copilot, nano gpt, chutes, etc.. you name it) looks not worth it.. (at least that what i feel! about the pricing point)\n\nin practice it might not last long enough like the OP mentioned (chinese model inefficient in tool calling), but it's still good deal, and if they can last longer run than the big labs (openai, anthropic), it could be a good news for us actually.. we can still \"vibe code\" amidst chaos üòÇ\n\nim looking forward to synthetic tho.. im with codex now because the model is so freaking good..",
              "score": 1,
              "created_utc": "2026-02-26 17:01:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ocgxg",
              "author": "Bitsu92",
              "text": "codex 20$ subscription seems pretty good, at least until a new claude model releases, i have never hit usage limit ",
              "score": 1,
              "created_utc": "2026-02-27 09:56:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7sbtxi",
              "author": "Juan_Ignacio",
              "text": "Update:  \nI ended up canceling Chutes today because they made a pretty major change to their limits, and it just doesn‚Äôt work for me anymore.\n\nI just paid for NanoGPT, and so far it doesn‚Äôt seem bad at all. The limits might be a bit low, but considering it‚Äôs only **$8/month**, it seems fair.\n\nMaybe MiniMax has higher limits with its **$10 subscription**, though.",
              "score": 1,
              "created_utc": "2026-02-27 23:04:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jc2eb",
          "author": "RiskyBizz216",
          "text": "their subscription was a bad deal at $20 too.\n\nI stayed on the pay-as-you-go because its a little faster than openrouter.\n\nollama cloud is a good alternative if you want to stay in the $20 price range. they also have a free tier [https://ollama.com/pricing](https://ollama.com/pricing)\n\nor you could just sign up for NVidia NIM and get a free api key [https://build.nvidia.com/settings/api-keys](https://build.nvidia.com/settings/api-keys)\n\nor you can sign up to iFlow and get a free api key\n\n[https://platform.iflow.cn/en/models](https://platform.iflow.cn/en/models)",
          "score": 1,
          "created_utc": "2026-02-26 16:17:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jg172",
              "author": "flobblobblob",
              "text": "I've seen NVidia NIM mentioned a few times, sounds too good to be true so what is the catch? Why isn't everyone just doing that?",
              "score": 3,
              "created_utc": "2026-02-26 16:35:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7jkep0",
                  "author": "mdn0",
                  "text": "Everyone is doing that of course. So you have to wait, wait, wait...",
                  "score": 2,
                  "created_utc": "2026-02-26 16:55:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7jo7vz",
                  "author": "RiskyBizz216",
                  "text": "Very generous of NVidia to give it to us for free, but it can be unreliable. If you're using the high demand models like glm5/ kimi k2.5 then it will be slow. But other models like llama ones are pretty snappy.\n\nIt all depends on your use case.",
                  "score": 1,
                  "created_utc": "2026-02-26 17:13:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7lxx42",
          "author": "jerieljan",
          "text": "So their landing page still says:\n> Subscribe for $20/month, and use models in our app\n\nBut going to pricing with 1 subscription pack (default) indicates $1/day - $30/mo\n\nHeh, they clearly didn't update those pricing changes fully.",
          "score": 1,
          "created_utc": "2026-02-26 23:48:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mgfl1",
          "author": "klippers",
          "text": "Thank you for the heads up. I was actually going to sign up this weekend. \n\nI'm now looking at nano-gpt",
          "score": 1,
          "created_utc": "2026-02-27 01:32:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mnjgl",
          "author": "Maleficent_Radish807",
          "text": "I was also on their pay-per-use and on the wait list for the subscription.\n\nMy initial testing is that models like GLM 4.7 or Kimi 2.5 are running at half the token per second then directly from z.ai or moonshot.\n\nAnyone else experiencing that?",
          "score": 1,
          "created_utc": "2026-02-27 02:13:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7si475",
          "author": "sudoer777_",
          "text": ">  They increased the price from 20 USD to 30 USD for standard plan last night . Their ratioanle is \"They need a lot of compute\".\n\n>  They are using the same excuse ; shorteage of compute/GPUs.\n\nThis is an actual problem though. My hosting provider (Hetzner) just increased their price 60% due to the shortage of computing resources, and the OpenCode Zen providers have also been struggling to meet the demand, and the Fireworks CEO also tweeted about it IIRC.",
          "score": 1,
          "created_utc": "2026-02-27 23:40:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o85lxeu",
              "author": "ZeSprawl",
              "text": "Yes this will continue to be a problem, and demand is only going up, while PC supply chain slows down. Things will just get more expensive for a few years while supply catches up and then it will get cheap again. In the mean time it will require spending to keep up.",
              "score": 1,
              "created_utc": "2026-03-02 01:33:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vozgv",
          "author": "Competitive_Ad_2192",
          "text": "I had a similar experience with this service: I paid, the limits (5 hours) ran out faster than on Claude‚Äôs $20 plan, I wrote to support for a refund, they returned the money, and I forgot about this service like a bad dream",
          "score": 1,
          "created_utc": "2026-02-28 14:00:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7wiq45",
          "author": "t12e_",
          "text": "It's not bad when paired with github copilot sub. I use kimi k2.5 and configure subagents, explore agent, etc to work with gpt 5 mini. So simpler tasks use a smaller model, complex tasks use the bigger model. I've only hit twice and pretty much code all day at work with this setup",
          "score": 1,
          "created_utc": "2026-02-28 16:38:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o82lim4",
          "author": "blankeos",
          "text": "No way $30 now?",
          "score": 1,
          "created_utc": "2026-03-01 16:09:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o875arm",
          "author": "In-line0",
          "text": "Can't agree more, it got enshittified pretty quickly. ",
          "score": 1,
          "created_utc": "2026-03-02 08:30:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jdass",
          "author": "qmrelli",
          "text": "I can also recommend you to try [Dvina Code](https://dvina.ai/p/dvinacode/dvinacode), it has more usage of Opus 4.6, you can test it yourself in free plan, also if you wanna upgrade to pro I can give you 1 month free plus membership code, DM if you want that.",
          "score": 1,
          "created_utc": "2026-02-26 16:23:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7n6rg0",
          "author": "New-Fuel-2735",
          "text": "Immediately unsubbed when i discover alibaba coding plan. It rendered any other sub right now obsolete",
          "score": 1,
          "created_utc": "2026-02-27 04:09:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7nbozz",
              "author": "Codemonkeyzz",
              "text": "Wasn't aware of their plan. Just had a look , it looks like a great deal. Will definitely try it",
              "score": 0,
              "created_utc": "2026-02-27 04:42:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1refkcr",
      "title": "kimi k2.5 vs glm-5 vs minimax m2.5 pros and cons",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1refkcr/kimi_k25_vs_glm5_vs_minimax_m25_pros_and_cons/",
      "author": "tomdohnal",
      "created_utc": "2026-02-25 15:02:02",
      "score": 53,
      "num_comments": 29,
      "upvote_ratio": 0.98,
      "text": "in your own subjective experience, which of these models are best for what types of tasks?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1refkcr/kimi_k25_vs_glm5_vs_minimax_m25_pros_and_cons/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7c7h9a",
          "author": "rodabi",
          "text": "Out of the three I was really unimpressed with k2.5 and minimax. But GLM-5 feels like the real deal. It can actually do tool calls properly, and it's been pretty successful at larger changes. I'd rate it similar to Sonnet 4.6",
          "score": 35,
          "created_utc": "2026-02-25 15:18:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rsw05",
              "author": "jrop2",
              "text": "So odd, I have had such a different experience: Kimi K2.5 really impressed me, and GLM-5 seems close but not quite there for me. All this talk as of late, though, is making me think I should go back and spend more time with GLM-5.",
              "score": 2,
              "created_utc": "2026-02-27 21:25:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cstuh",
          "author": "SvenVargHimmel",
          "text": "GLM5 is pretty solid. \n\nKimi k2.5 is the eager junior but it gets it right but fails in more complex reasoning planning scenarios and on larger codebases.  \n\nMinimax m2.5 - i don't bother with. Kimi k.25  does build steps much better. \n\nNone of these models are good in planning mode for anything that is non trivial or unconventional. \n\n  \nMay workflow is:  Plan Mode ( Gemini Pro 3.1 ) , Build Mode (Kimi k.2.5) \n\nIf am relying on the open source model strictly then I \n\nin Plan Mode (Kimi | Minimax ) generate a spec file  exit and the start build mode with Kimi 2.5 referencing the spec file. \n\nI have not tried 1.) ralph loops yet with any of these models",
          "score": 11,
          "created_utc": "2026-02-25 16:56:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d1z4a",
          "author": "forgotten_airbender",
          "text": "Kimi 2.5 and glm 5x¬†\nBoth have their own strengths. Glm‚Äôs coding ability is better, kimi writes better + tool calling + orchestration.¬†\nDidnt find minimax 2.5 that good tbh. Okay as a fallback model but not as a main coding agent.¬†",
          "score": 10,
          "created_utc": "2026-02-25 17:38:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ggl4e",
              "author": "lundrog",
              "text": "This",
              "score": 1,
              "created_utc": "2026-02-26 04:05:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7caqes",
          "author": "seymores",
          "text": "I have not use GLM but paid for MiniMax and Kimi recently. MiniMax is the dumb one consistently.\nI did not expect much from Kimi but it gets code done quickly and correctly whilst MiniMax is a waste of time and money for coding.\nI am heavy Codex user so I evaluated on the basis of Codex 5.3 as the benchmark.",
          "score": 12,
          "created_utc": "2026-02-25 15:34:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cbpfg",
              "author": "Potential-Leg-639",
              "text": "All are selling quite quantized versions for sure",
              "score": 0,
              "created_utc": "2026-02-25 15:38:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7c8ceg",
          "author": "Bob5k",
          "text": "considering majority of accessible providers, especially when we talk about budget friendly:  \n\\- kimi via their coding plan is a big joke, but can be 'abused' for 0.99$ subscriptions. However they're on 3x usage quota right now till 'who knows when' and it's a joke as even lightweight work can cap out 5h quota in 1.5h and each 5h quota is 20% of weekly quota. You can rotate a few accounts tho easily  \n\\- glm via official api seems to be probably smartest out of these 3, best when it comes to actual frontend design and picking up logic tasks quite well. The main issue is that official api is generally slow / very slow for 95% of the time (and im EU based...) and other providers are having either quite low quota allowance or doesn't host it at all. Can't wait till synthetic provides it still (btw when they open the waitlist gates remember about [referral discount](https://synthetic.new/?referral=IDyp75aoQpW9YFt))  \n**have in mind that both kimi and glm via official api have weekly caps** which are quite.. low in both cases. GLM is better than kimi tho.  \n\\- minimax m2.5 via the minimax provider is my go-to for now, as it's reliable enough to develop 95% of usual work (i run amp free when i really need opus 4.6 for some super complex debugging) and also it has the -highspeed variant which is insanely fast as it provides constant 100tps. So for my fast paced workflow of ideate > plan > develop > review > fix > merge it's great, as minimax's speed vs kimi / glm both as a model and provider is a big win here. Also they still host [10% discount via reflinks aswell ](https://platform.minimax.io/subscribe/coding-plan?code=HO46LCwAJ5&source=link)\\- can recommend. Also **there is no weekly cap** and they say it's 100 / 300 / 1000 / 2000 prompts per 5h but the main pros of minimax system are:  \n\\- their 5h windows are fixed (00 - 05 am / 5 - 10 / 10 - 3pm / 3-8 / 8 - 11:59:59 pm) > which might sound as not like a big deal but you can basically start working at 8am, code for 2h and even if you'd get anywhere close to the cap - the limit resets at 10 so you can move forward with your work. It's IMO **way better system** than rolling 5h window as you know when to expect a reset so can potentially plan upfront.   \n\\- their system says as prompts, but what they don't say on the pricing page is that each prompt is at least 15-20 model calls - so even the 10$ (9 with discount) plan allows to do a **ton of coding**. I'm right now at 40$ highspeed plan, spinning 3 agents all the time and using their api for one of my SaaS and can't cap it out really through the day.   \nHave in mind that with 3 agents and highspeed minimax usually the human in the loop is worst piece of the whole system and workflow. :) ",
          "score": 8,
          "created_utc": "2026-02-25 15:22:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7emamq",
          "author": "Key_Credit_525",
          "text": "all of them just smart anough to implement detailed plan given them by Opus",
          "score": 5,
          "created_utc": "2026-02-25 21:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c79qc",
          "author": "Honest-Ad-6832",
          "text": "Well, glm just screwed me over by doing git checkout without stashing changes, and there were a lot of changes. So there's that I guess. Still, it¬†is smart. And slow.¬†\n\n\nYesterday both mm and kimi failed to debug something which codex oneshotted.¬†\n\n\nMinimax is fast but not to bright. Best for chores.¬†\n\n\nGlm feels smartest but the speed is annoying.¬†\n\n\nHaven't used kimi much.",
          "score": 3,
          "created_utc": "2026-02-25 15:17:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cdtn8",
              "author": "DasBlueEyedDevil",
              "text": "Claude did the same shit to me, tbh.  More than once actually.  I had to build in a hard stop because the bastard kept ignoring the md and trying to do it anyway.",
              "score": 4,
              "created_utc": "2026-02-25 15:48:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7caxmh",
              "author": "Bob5k",
              "text": "no opensource model is even remotely close to any sort of debugging vs codex5.3xhigh or opus4.6high. This is probably the reason that for serious development you'd like to combine those with something like codex / claude 20$ subscriptions (or just amp and use amp free if you have access to it / add a few $ for really really complex issues).   \nminimax in highspeed variant improved the overall delivery speed significantly tho, as both TTFT is super low and 100tps+ makes a serious difference, as usually in my usecases it's \\~2/2.5x as fast as kimi via their official api endpoints in both cases. ",
              "score": 2,
              "created_utc": "2026-02-25 15:34:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7cem85",
                  "author": "Honest-Ad-6832",
                  "text": "It was high too... League above free models for sure. I do like and use free models a lot. But I feel much more confident with codex",
                  "score": 2,
                  "created_utc": "2026-02-25 15:52:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7hdbw5",
                  "author": "xmnstr",
                  "text": "I use the $10 Github Copilot Pro sub. Get the big models, no risk of getting banned. Enough for all the planning/debugging/research/reviewing I need.",
                  "score": 1,
                  "created_utc": "2026-02-26 08:27:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7juokw",
              "author": "deadcoder0904",
              "text": "Codex for code. Kimi or Claude for writing.",
              "score": 1,
              "created_utc": "2026-02-26 17:43:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7chfjw",
          "author": "External_Ad1549",
          "text": "glm-5 supposed to be good but running slow if you are taking from coding plan however glm 4.7 does the job with good speed, minimax m2.5 starts well but with increase in context model little bit degrades",
          "score": 3,
          "created_utc": "2026-02-25 16:04:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cjpzo",
          "author": "ThingRexCom",
          "text": "GLM-5 is a clear winner for me. I use it for agentic coding, and it delivers solid results (especially when organized as a team of AI developers).\n\nI tried kimi 2.5k, but it produced a garbage stream of characters during \"thinking\" and never recovered.\n\nNote: I had Z.AI GLM Coding Max-Monthly¬†Plan, but the inference performance was very poor, and I switched to DeepInfra API (still using GLM-5).",
          "score": 3,
          "created_utc": "2026-02-25 16:15:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jpnl4",
          "author": "Codemonkeyzz",
          "text": "Kimi k2.5 > glm 5 > minimax m2.5",
          "score": 3,
          "created_utc": "2026-02-26 17:20:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7exw8k",
          "author": "tsimmist",
          "text": "Not much experience from glm5, but m2.5 vs k2.5 - I would pick k2.5 all the time (although the m2.5 coding plan has higher value than what kimi offers) \n\nWith omos - kimi is a lot more better in orchestrating to my experience, m2.5 in other hands - slower but seems code better. But sometime - m2.5 has its own personality that not 100% following my instruction (still doing the job in merit but just not as I planned to be) - it could be pros or cons depends on outcome‚Ä¶",
          "score": 2,
          "created_utc": "2026-02-25 22:54:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7frh8e",
              "author": "SingingRooster95",
              "text": "Does omos refer to Oh-My-OpenCode-slim?",
              "score": 1,
              "created_utc": "2026-02-26 01:38:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7g4fsv",
                  "author": "tsimmist",
                  "text": "Yes",
                  "score": 2,
                  "created_utc": "2026-02-26 02:52:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7hvker",
          "author": "Ivankax28",
          "text": "Kimi K2.5",
          "score": 2,
          "created_utc": "2026-02-26 11:18:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cft3z",
          "author": "dengar69",
          "text": "How is the token speed through Zen?  I tried Nano but it‚Äôs not reliable at all.",
          "score": 1,
          "created_utc": "2026-02-25 15:57:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7co4kq",
          "author": "JohnnyDread",
          "text": "Kimi and MiniMax would be useful if they were faster. I've tried to use GLM-5 and it does well for a while, if a bit slow, but then eventually starts going insane - rampant tool-use errors, spewing gibberish or just stopping mid-thought or task for no reason and I have to abandon the session. Promising, but not ready for prime time yet.",
          "score": 1,
          "created_utc": "2026-02-25 16:35:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7feix9",
          "author": "0xDezzy",
          "text": "If you're asking because of OpenCode Go, then GLM-5 is lobotomized . Haven't tried Kimi k2.5 or Minimax yet. ",
          "score": 1,
          "created_utc": "2026-02-26 00:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fzolo",
          "author": "aeroumbria",
          "text": "It seems to be quite sensitive to the workflow and control style you use...\n\nNot much experience with minimax yet. As for Kimi and GLM, Kimi is more reliable on \"fragile\" tasks where failed tool calls can derail the whole tasks (e.g. frameworks like GSD where each step must produce artifacts that later steps depend on). It sometimes decide to stop where it is not supposed to, but it is fairly easy to fix. GLM seems to be more intelligent and can solve more complex problems \"from scratch\" (basically using bare prompts), but it does not seem to be very reliable with tool calls, and will eventually start hallucinating tools or generating nonsensical texts if the task goes on for too long.\n\nI don't think I have enough evidence to tell which one works better generally, but for now I would prefer Kimi for orchestrated workflows and GLM for adhoc / interactive use. When trying popular prompting frameworks, GSD works better with Kimi (GLM derails in fully automated tasks), and OAC works better with GLM (Kimi impersonates user and fills user questions automatically).",
          "score": 1,
          "created_utc": "2026-02-26 02:25:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7twajm",
          "author": "HarjjotSinghh",
          "text": "this is my brain on a new bot!",
          "score": 1,
          "created_utc": "2026-02-28 04:58:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c4wj5",
          "author": "HarjjotSinghh",
          "text": "this is the closest to perfection yet!",
          "score": 0,
          "created_utc": "2026-02-25 15:06:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rce8jw",
      "title": "Built an open-source Telegram client for OpenCode CLI ‚Äî now dogfooding it daily from my phone",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rce8jw/built_an_opensource_telegram_client_for_opencode/",
      "author": "Less_Ad_1505",
      "created_utc": "2026-02-23 10:46:21",
      "score": 52,
      "num_comments": 11,
      "upvote_ratio": 0.98,
      "text": "OpenCode CLI has become my primary dev tool, and I want to give a huge shoutout to its authors for building such an incredible piece of software. The models seem to handle context and logic particularly well in it, especially when using the Plan agent first and then switching to Build.\n\nEven before Openclaw became popular, I kept thinking how useful it would be to access OpenCode from my phone. I noticed OpenCode has a server mode, which meant building a custom client was totally doable. Initially, I just wanted to write a simple Telegram bot for my own needs. But, as it usually goes, I got carried away, added more features, and eventually decided to open-source the project.\n\nI definitely won't call it \"fully functional\" yet - there are still rough edges. However, it currently has enough features to be used for actual development.\n\nHere is what works right now:\n\n* Switching between projects and sessions.\n* Selecting the agent, model, and variant (reasoning effort).\n* Tracking the agent's progress on a task.\n* Receiving code diffs directly in the chat as text files.\n\nIronically, I'm now at the point where I use the bot to write code for the bot itself. It‚Äôs a pretty great feeling to lie on the couch, watch a TV series, and casually send dev tasks to the agent via Telegram on my phone.\n\nI plan to keep actively developing the project since I use it daily. If anyone wants to try it out, the repo is here: [https://github.com/grinev/opencode-telegram-bot](https://github.com/grinev/opencode-telegram-bot)\n\nI would be really grateful for any feedback, thoughts, or suggestions!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rce8jw/built_an_opensource_telegram_client_for_opencode/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o6y66na",
          "author": "throwaway12012024",
          "text": "i am doing exactly the same! Great project! I am sad bc just discovered your work now so i am a bit in the sunken cost thing.",
          "score": 1,
          "created_utc": "2026-02-23 13:36:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o729fdr",
          "author": "bradjones6942069",
          "text": "Open code has become my daily driver as well. I would love to try this out",
          "score": 1,
          "created_utc": "2026-02-24 01:57:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74yj13",
              "author": "bradjones6942069",
              "text": "Been trying it out since last night. Works great.",
              "score": 1,
              "created_utc": "2026-02-24 14:04:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75oe9h",
          "author": "oVerde",
          "text": "Is the bot publicly available? So anyone would run agents on my machine?",
          "score": 1,
          "created_utc": "2026-02-24 16:09:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75pesn",
              "author": "Less_Ad_1505",
              "text": "No, the basic idea is that the bot only works with one allowed userId for security purposes.",
              "score": 1,
              "created_utc": "2026-02-24 16:14:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75pxyt",
                  "author": "oVerde",
                  "text": "thanks",
                  "score": 1,
                  "created_utc": "2026-02-24 16:16:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xk26d",
          "author": "HarjjotSinghh",
          "text": "this is how you turn phone dev dreams real!",
          "score": 0,
          "created_utc": "2026-02-23 10:50:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xlrh2",
              "author": "Less_Ad_1505",
              "text": "You can also run this bot on a remote server to have a personal assistant right in Telegram. It won't be as proactive as Openclaw, but you can still do a lot with it and see exactly what's happening in detail",
              "score": 1,
              "created_utc": "2026-02-23 11:06:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xp91n",
          "author": "revilo-1988",
          "text": "Mal ne Frage was ist den so n√ºtzlich um es vom Handy aus zu machen?",
          "score": -2,
          "created_utc": "2026-02-23 11:37:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfzwg1",
      "title": "I tested Opencode on 9 MCP tools, Firecrawl Skills + CLI and Oh My Opencode - Most of it is just extra steps you dont need.",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rfzwg1/i_tested_opencode_on_9_mcp_tools_firecrawl_skills/",
      "author": "lemon07r",
      "created_utc": "2026-02-27 07:12:17",
      "score": 51,
      "num_comments": 13,
      "upvote_ratio": 0.96,
      "text": "Thought I would share this here. Something I wanted to do for a long time, compare if MCP tools actually made any difference, and if Oh My Opencode was just snake oil. Most papers, and other testing I've seen mostly indicate these things are useless and actually have a negative impact. Thought I would test it myself. \n\nFull test results and data is available here if you want to skip to it: [https://sanityboard.lr7.dev/](https://sanityboard.lr7.dev/)\n\nMore about the eval here in previous posts if anyone is interested: [Post 1](https://www.reddit.com/r/LocalLLaMA/comments/1qp4ftj/i_made_a_coding_eval_and_ran_it_against_49/), [Post 2](https://www.reddit.com/r/LocalLLaMA/comments/1r2g7lq/new_minimax_m25_gpt53codex_glm_5_coding_eval/),  and an explanation of how the eval works [here](https://www.reddit.com/r/LocalLLaMA/comments/1r2g7lq/comment/o4wrsv6/). These are all results for the newer v1.8.x leaderboard, which I have not made a post about, but basically all the breaking changes I wanted to make, I've made them now, to improve overall fairness and fix a lot of other issues. Lot of stuff was fixed or improved. \n\n# Oh My Opencode - Opus with Extra Steps, but Worse\n\nLet's start with oh my opencode. I will save you some time, no OmO = 73.1% pass rate, with OmO Ultrawork = 69.2%. It also took 10 minutes longer, at 55 minutes to complete the eval, and made 96 total requests. Without OmO only 27 requests are made to Github Copilot. That's it. You can look for the next header and skip to the next section if that's all you wanted to know. \n\nHonestly, I had very low expectations for this one, so while it showed no improvement whatsoever and was somewhat worse, it was not worse by as much as I thought it would be. There are a *lot* of questionable decisions made in its design, in my opinion, but I won't get into that or this will turn into a very long post. I followed the readme, which literally told me to go ask my agent to set it up for me. I hated this. I prefer to do things manually so I can configure things exactly how I want, and know what is what. It took Junie CLI Opus 4.6 like 25 minutes to get things set up and working properly.. really? Below is how I configured my OmO, using my copilot and AG subscriptions via my cliproxy. \n\nhttps://preview.redd.it/mfznlwz38zlg1.png?width=748&format=png&auto=webp&s=fa7b4e207e529fa251835ac6cb35a856a298a284\n\nHonestly, I think if opus wasnt carrying this, OmO would have degraded scores much more significantly. Opus from all my testing I've done, has shown to be extremely resilient to harness differences. Weaker models are much more sensitive to the agent they are running in and how you have them set up. \n\n# MCP Servers -  Old news, just confirmed again\n\nI think most of have by now have probably already read one or two articles, or some testing and analysis out there of MCP servers concluding they usually have a negative impact. I confirmed nothing new and saw exactly this again. I used opencode + kimi k2.5 for all results because I saw Kimi had a higher MCP usage rate than other models like Opus (I did a bunch of runs to specifically figure this out), and was a good middle strength candidate in my opinion. Strong enough to call tools properly and use them right, but weak enough to have room to benefit from better tools (maybe?). I use an MCP (or SKILL) agnostic prompt to nudge the agent to use their external tools more without telling them how to use it or what to do with them. This was a little challenging, finding the right prompt, since I didn't want to steer how the agent solved tasks but also needed the agent to stop ignoring it's MCP tools. I ran evals against different prompts for 2 days straight to find the best one. Here are my test results against 9 different MCP servers, and throwing in one search cli tool + skills (Firecrawl). \n\nhttps://preview.redd.it/2y6rongkfzlg1.png?width=1108&format=png&auto=webp&s=19ecf7e13a9f8ef67d061d28b7f4d91be2ec16e0\n\nLeft column are the MCP servers used (with one entry being SKILL + cli rather than mcp). The gemini cli entry is incorrect, that was supposed to be \"Gemini MCP Tool\". The baseline is well.. just regular old kimi k2.5 running on vanilla opencode, no extra tools. \n\nThe ONLY MCP tool to actually make improvements is the only code indexing and semantic retrieval tool using embeddings here. Not only did it score higher than baseline, it also used less time than most of the other MCP tools. I do believe it used less tokens, which probably helped offset the number one weakness of mcp servers. I've been a big proponent of these kinds of tools, I feel they are super underrated. I don't recommend this one in particular, it was just what I saw was popular so I used it. My biggest grip with claude context is it wants you to use their cloud service instead of keeping things local (cmon, spinning up lancedbs would work just fine), and the lack of reranker support (which I think is super slept on). \n\nI was surprised that firecrawl cli + skills did worse than the MCP server. Maybe it comes with too much context/info in it's skills file that it ends up not really solving the MCP issue of polluting context with unnecessary tokens? I imagine it might only be pronounced here since we are solving small tasks rather than implementing whole projects. \n\n# Some rambly rambles about embeddings, indexing, etc that you can skip\n\nIf anyone is familiar with the subject, some of you might already know, that even using a very tiny embedding model + a very tiny reranker model will give you much better accuracy than even the largest and best embedding models alone. I'm not sure why I decided to test it myself since it's already pretty well established, but I did, since I wanted to see what it would be like working with lancedb instead of sqlite-vec (and benchmark some things along the way). [https://sanityboard.lr7.dev/evals/vecdb](https://sanityboard.lr7.dev/evals/vecdb) The interesting thing I found was, that it made an even bigger difference for coding, than it did in my tests on fictional writing. \n\nModern instruction tuned reranker models and embedding models are great, you provide them things like metadata, and you get amazing results. In the right system, this can be very good for code indexing, especially with the use of things like AST aware code chunking, tree-sitter, etc. We have all the tools to give these models the metadata to help it. Just thought this was really cool, and I have plans to make my own code indexing tool (again) since nobody else seems to make one with reranking support. My last attempt was to fork someone's vibe-slopped nightmare and fix it up.. and after that nightmare I've realized I would have had a better time making my own from scratch (I did have it working well at ONE point, but please dont go looking for it, ive broken it once more in the last few versions trying to fix more stuff and gave up on it). I did learn a lot though. A lot of the testing I have done was partially to see if it would even be a good idea, since it comes up in my circle of friends sometimes \"how do we know it wont just make things worse like most other mcp servers?\" I guess I will just have to do the best I can, and make both CLI + skills and MCP tool to see what works better.  \n\nOh yeah, I guess I also have a toy web api eval thing too I made. This is pretty low effort though. I just wanted to see what implementation was like for each API since I was building a research agent. [https://sanityboard.lr7.dev/evals/web-search](https://sanityboard.lr7.dev/evals/web-search) The most interesting part will be Semantic and Reranker scores at the bottom. There are a lot of random points of data here, so it's up to you guys to figure out what's actually substantial and what's noise here, since this wasnt really a serious eval project for me. Also firecrawl has an insanely aggressive rate limits for free users, that I could not work around even with generous retry attempts and timeout limits. \n\nIf you guys have any questions pls feel free to join my discord (linked in my eval site). I think we have some pretty cool discussions there sometimes. Not really trying to shill anything, I just enjoy talking about this stuff with others.  Stars would be cool too, on some of my github projects if you like any of them. Not sure how ppl be gettin these. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rfzwg1/i_tested_opencode_on_9_mcp_tools_firecrawl_skills/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7oan6t",
          "author": "Latter-Parsnip-5007",
          "text": "Nice, work. Thanks for sharing",
          "score": 7,
          "created_utc": "2026-02-27 09:38:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pxa7k",
          "author": "SvenVargHimmel",
          "text": "Can you talk a bit more about the embedding model and the rrabker. Did you integrate this with opencode and have that replace grep?\n\n\nAlso does this give you better performance than A big model + grep?¬†",
          "score": 4,
          "created_utc": "2026-02-27 15:58:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rd1yd",
              "author": "lemon07r",
              "text": "I haven't integrated it anywhere yet. Unless you count my abandoned mcp fork. I have plans to do a rewrite and make a new tool. Grep should still be available and used, ideally you just expose this tool to your LLM, give it a skills file or a description of what it does in your mcp server tool info, and then your LLM will decide how to use it best. The end result; your LLM will still want to grep for simple things but it will save a ton of time and tokens on the more ambiguous things but combining semantic searching and code indexing.",
              "score": 1,
              "created_utc": "2026-02-27 20:05:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ofv91",
          "author": "Timo_schroe",
          "text": "The mcp table with Opus would be interesting. Is it Tool using capability or a general result?",
          "score": 1,
          "created_utc": "2026-02-27 10:27:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rci62",
              "author": "lemon07r",
              "text": "That table is with Kimi k2.5. It's the result when the evals are run with those mcps tools enabled and a slight prompt injection to lightly encourage the LLM to review it's mcp tools and use them where useful.",
              "score": 1,
              "created_utc": "2026-02-27 20:02:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7p2wke",
          "author": "Khan_WuDeng",
          "text": "I also found that OMO is more often than not just a burden, so I switched to the relatively lighter oh my opencode slim. By the way, does anyone know any excellent ready-made MCP that integrates embedding models and reranking models? I am not a programmer, just a hobbyist for the most part. I use AI to handle matters related to my personal hobbies and repetitive, boring work in my main job. Anyway, thank you for sharing, this confirms the irrational subjective feeling I had.",
          "score": 1,
          "created_utc": "2026-02-27 13:20:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qzvkf",
          "author": "shroomgaze13",
          "text": "Can you check OmO Slim?\n\nhttps://github.com/alvinunreal/oh-my-opencode-slim",
          "score": 1,
          "created_utc": "2026-02-27 19:00:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rbcqs",
              "author": "lemon07r",
              "text": "I could but I really don't get the point of omo. It just ends up using your best model for 90% of your steps anyways only to score a little lower. It's literally just opus with more steps. Honestly these ai vibe slopped projects are getting annoying cause they clearly aren't testing anything and just going to Claude and saying \"make this but better\" then Claude comes back and goes \"here's your plan,this will be 50% better\" and everyone just goes okay I believe you. Im sure omo slim might be better but I'm almost sure that it's really not that different from just using vanilla opencode. I've tested over 120+ different agent/model combinations and there is a very apparent pattern that all the tools that try to do too much score much lower, and the tools that try to keep things as simple, efficient and lean as possible score the highest. If you use junie CLI you will literally see in its reasoning traces \"looking for the simplest approach\" multiple times over while it works (which I thought was interesting behavior).",
              "score": 4,
              "created_utc": "2026-02-27 19:57:08",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7vpbsu",
              "author": "maximhar",
              "text": "I like OMO for its hooks and background task execution. I think OMO slim is trying to optimise the wrong things. The base Opencode prompts + background workers and hooks that auto-prompt the model to continue when it decides to stop working before it‚Äôs done would be enough for me. I recently switched to Oh-My-Pi and it does pretty much that.",
              "score": 2,
              "created_utc": "2026-02-28 14:02:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7rlt8i",
          "author": "justjokiing",
          "text": "So what was the embedding vector db mcp that you used? or how did you set up the valuable mcp?",
          "score": 1,
          "created_utc": "2026-02-27 20:50:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rmsfs",
              "author": "lemon07r",
              "text": "I'm not sure I get the question but I used Qwen3 Embedding 8B + Claude Context MCP. ",
              "score": 1,
              "created_utc": "2026-02-27 20:54:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7rofe3",
                  "author": "justjokiing",
                  "text": "I am very interested in the embedding, I use 'code-index-mcp' but it does not seem to specify the model it uses if so at all. Maybe the code-index-mcp only does the AST parsing and not a vector-db? Was the embedding model expensive at all?",
                  "score": 1,
                  "created_utc": "2026-02-27 21:03:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7xxv65",
          "author": "HarjjotSinghh",
          "text": "this is actually fascinating!",
          "score": 1,
          "created_utc": "2026-02-28 20:59:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rea3gg",
      "title": "Potential limits of OpenCode Go plan",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rea3gg/potential_limits_of_opencode_go_plan/",
      "author": "alovoids",
      "created_utc": "2026-02-25 10:53:01",
      "score": 43,
      "num_comments": 23,
      "upvote_ratio": 0.96,
      "text": "Been looking at my OpenCode dashboard and here's the usage so far:\n\nTotal today: $0.44\n\nRolling (5-hour cycle): 11% (resets in \\~2 hours)\n\nWeekly: 4% (resets in 4d 13h, likely Monday)\n\nMonthly: 2% (resets in 27d 21h)\n\nIf today's usage is the only one so far, the limits seem to be:\n\nRolling (5h): $4.00\n\nWeekly: $11.00\n\nMonthly: $22.00\n\nAlso worth noting: among the three models, from cheapest to most expensive it's Minimax M2.5, Kimi K2.5, GLM 5. So choose your model wisely based on your needs and budget.\n\nThese are just indicative findings from my own dashboard. What's been your experience with the OpenCode Go plan so far? Do these numbers match what you're seeing?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rea3gg/potential_limits_of_opencode_go_plan/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7b8mys",
          "author": "Bob5k",
          "text": "i'd still pop that minimax for 9$ with [reflink](https://platform.minimax.io/subscribe/coding-plan?code=HO46LCwAJ5&source=link) has basically 1.5-2k requests per 5h window and no weekly cap. have that in mind. \n\nalso surprisingly codex on plus plan (the 20$ one) has quite high 5h caps recently.",
          "score": 8,
          "created_utc": "2026-02-25 11:59:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bnxme",
              "author": "Pleasant_Thing_2874",
              "text": "Their usage is 2xed atm I think until beginning of April.  So while we have some time with the plus plan being fairly usable don't get too used to it.",
              "score": 7,
              "created_utc": "2026-02-25 13:37:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7c3m06",
                  "author": "Bob5k",
                  "text": "yeah, but in meantime it can be used aswell. Not sure how many people would retain as customers once 2nd april hits lol ",
                  "score": 2,
                  "created_utc": "2026-02-25 14:59:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bhr54",
          "author": "trypnosis",
          "text": "Thanks for sharing but if they won‚Äôt share the limits then I will stick with synthetic.new for open source models.\n\nMight be bigger payment but at least I know what I‚Äôm getting.",
          "score": 7,
          "created_utc": "2026-02-25 13:01:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7blesx",
              "author": "alovoids",
              "text": "cheers! everyone is free to choose which provider they want :)",
              "score": 4,
              "created_utc": "2026-02-25 13:23:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bo1ur",
                  "author": "trypnosis",
                  "text": "Very true, and customer feedback is key to a successful business. \n\nSo I hope they tune in to Reddit and see that there are people who want to give OpenCode money like my self. \n\nIf it‚Äôs for AI, usage transparency and server locations are key. \n\nThis is super fresh so I‚Äôm sure lots of iterations are to come and I hope the feedback on here is considered.",
                  "score": 2,
                  "created_utc": "2026-02-25 13:38:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7bxvqf",
              "author": "Ok_Direction4392",
              "text": "Similar situation here, I was on Synthetic but switched to Ollama Cloud. Interested to see how OpenCode Go's offering develops.",
              "score": 2,
              "created_utc": "2026-02-25 14:30:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bz3hh",
                  "author": "trypnosis",
                  "text": "I think they are positioned to grab the biggest slice of the market. \n\nAnd the quality of these open source models are catching up with the big three. \n\nI think this has potential to do better for them than black.\n\nAssuming transparency and server locations are resolved. \n\nOn a side note why did you leave synthetic and what does 3 models mean on Ollama?(never seen there cloud offering before)",
                  "score": 3,
                  "created_utc": "2026-02-25 14:37:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7b2wuo",
          "author": "Glittering-Call8746",
          "text": "How's this compared to kimi coding plan ?",
          "score": 5,
          "created_utc": "2026-02-25 11:13:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7b4rqn",
              "author": "alovoids",
              "text": "the boosted usage (3x) of moderato plan seems to be lower than claude pro plan",
              "score": -1,
              "created_utc": "2026-02-25 11:29:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7b57w8",
                  "author": "Glittering-Call8746",
                  "text": "Claude pro opus 4.6 enough usage though? Lol",
                  "score": 0,
                  "created_utc": "2026-02-25 11:32:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7te30p",
          "author": "HarjjotSinghh",
          "text": "i can't wait to hit $4 today - imagine me screaming.",
          "score": 1,
          "created_utc": "2026-02-28 02:54:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfm8dq",
      "title": "OpenCode-Swarm v6.11 Release",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rfm8dq/opencodeswarm_v611_release/",
      "author": "Outrageous-Fan-2775",
      "created_utc": "2026-02-26 20:59:12",
      "score": 42,
      "num_comments": 36,
      "upvote_ratio": 0.94,
      "text": "I posted a few weeks ago about a very early build of my OpenCode plugin. I've iterated on it every day multiple times a day since then until we are here now with version 6.11. See below for a general guide on what it is and why it could help you. This comparison was built using Perplexity Computer over multiple iterations doing extensive market research on other plugins and capabilities.  \n  \nI've been working on opencode-swarm for a while now and figured I'd share what it actually does and why it exists.\n\nThe short version: most multi-agent coding tools throw a bunch of agents at your codebase in parallel and hope for the best. That works fine for demos. It falls apart on real projects where a bad merge or a missed security hole costs you a week of debugging.\n\nopencode-swarm does the opposite. One task at a time. Every task goes through a full QA gauntlet before the next one starts. Syntax validation (tree-sitter across 9 languages), static security analysis (63+ OWASP rules), placeholder/slop detection, secret scanning, lint, build check, then a reviewer on a *different model* than the coder, then a test engineer that writes both verification AND adversarial tests against your code. Only after all of that passes does the plan move forward.\n\nThe agents aren't generic workers either. There are 9 of them with actual permission boundaries. The Explorer can't write code. The SME can't execute anything. The Critic only reviews plans. The Architect owns the plan and delegates everything. Nobody touches what they shouldn't.\n\nSome stuff that took a lot of iteration to get right:\n\n* **Critic gate**: the plan gets reviewed by a separate agent before any code gets written. Prevents the most expensive failure mode, which is perfectly executing a bad plan\n* **Heterogeneous models**: coder and reviewer run on different LLMs on purpose. Different models have different blind spots, and this catches stuff single-model setups miss\n* **Retrospectives**: at the end of each phase, execution metrics (revisions, rejections, test failures) and lessons learned get captured and injected into the architect's prompt for the next phase. The swarm actually learns from its own mistakes within a project\n* **Everything persists**: plan.json, [context.md](http://context.md), evidence bundles, phase history. Kill your terminal, come back tomorrow, pick up exactly where you left off\n* **4,008 tests** on the plugin itself. Not the projects it builds. On the framework\n\nThe tradeoff is real. It's slower than parallel approaches. If you want 5 agents banging out code simultaneously, this isn't that. But if you've ever had an AI tool generate something that looked right, passed a vibe check, and then blew up in production... that's the problem this solves.\n\n**How it compares to other stuff out there**\n\nThere's a lot of multi-agent tooling floating around right now so here's how I see the landscape:\n\n**Swarm Tools (opencode-swarm-plugin)** is the closest competitor and honestly a solid project. Their focus is speed through parallelism: break a task into subtasks, spawn workers, file reservations to avoid conflicts. They also have a learning system that tracks what strategies worked. Where we differ is philosophy. Their workers are generic and share the same model. Mine are specialized with different models on purpose. They have optional bug scanning after the fact. I have 15+ QA gates that run on every single task before it moves on. If you want fast, go Swarm Tools. If you want verified, this is the one.\n\n**Get Shit Done (GSD)** is more of a meta-prompting and spec-driven framework than a true multi-agent system. It's great at what it does: interviews you, builds a detailed spec, then executes phase by phase. It recently added parallel wave execution and subagent orchestration. But it doesn't have a persistent QA pipeline, no security scanning, no heterogeneous models, and no evidence system. GSD is a planning tool that got good at execution. opencode-swarm is a verification system that happens to plan and execute.\n\n**Oh My OpenCode** gets a lot of attention because of the RPG theming and the YouTube coverage. Six agents with fun names, easy to set up, approachable. But when you look under the hood it's basically prompt engineering. No persistent state between sessions. No QA pipeline. No security analysis. No test suite on the plugin itself. It's a good entry point if you've never tried multi-agent coding, but it's not something I'd trust on a production codebase.\n\n**Claude Code Agent Teams** is native to Claude Code, which is a big advantage since there's no plugin to install. Peer-to-peer messaging between agents is cool architecturally. But it's still experimental with known limitations: no session resumption, no built-in QA, no evidence trail. Running multiple Opus-class agents in parallel also gets expensive fast with zero guarantees on output quality.\n\n**Codex multi-agent** gives you a nice macOS GUI and git worktree isolation so agents don't step on each other. But the workflow is basically \"agents do stuff in parallel branches, you manually review and merge.\" That's just branch management with extra steps. No automated QA, no verification, no persistence beyond conversation threads.\n\nThe common thread across all of these: none of them answer the question \"how do you know the AI's output is actually correct?\" They coordinate agents. They don't verify their work. That's the gap opencode-swarm fills.\n\nMIT licensed: [https://github.com/zaxbysauce/opencode-swarm](https://github.com/zaxbysauce/opencode-swarm)\n\nHappy to answer questions about the architecture or any of the design decisions.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rfm8dq/opencodeswarm_v611_release/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7lagwc",
          "author": "bitcoinbookmarks",
          "text": "Looks interesting and promising, but this made for commercial models. Can you add some easy swith or instruction how to fine tune it for one (max two) local models? At least I see time limits, maybe some agents redundant for local use with one model only... It will be cool if it will be possible to easy use with local model.",
          "score": 5,
          "created_utc": "2026-02-26 21:47:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7le6j2",
              "author": "Outrageous-Fan-2775",
              "text": "Its actually already built for and fully capable of local only. I can post the config when I get home. One of my requirements was that it run just as well fully locally.",
              "score": 3,
              "created_utc": "2026-02-26 22:05:01",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7lx48x",
              "author": "Outrageous-Fan-2775",
              "text": "I posted this above, but it aligns perfectly with your question.\n\nFor an example, I have one swarm called \"laptop\" that I run on my Ryzen AI Max 395+ 128gb system when I'm traveling. It only uses two models. GPT OSS 20b and some variant of Qwen coder. GPT serves as the architect, reviewer, etc while Qwen serves as the critic, coder, and test engineer. When at home I run 3 swarms, Mega, Paid, and Local. Mega has all the expensive top of the line models, Paid is one step down, and Local is all running here locally. I usually kick a project off with Mega and then let one of the other swarms take over. Or if its a smaller project the other two swarms can handle it themselves.\n\nTLDR I would recommend a minimum of two models NOT by the same company (i.e. dont do Qwen3.5 and also Qwen3 Coder. The plugin prompts are about as good as they can be, but no prompt will solve a blind spot caused by training data. Only a different data set can solve that.",
              "score": 2,
              "created_utc": "2026-02-26 23:44:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7o6gma",
                  "author": "bitcoinbookmarks",
                  "text": "I'm looking forward to try it, but a bug doesn't allow me to install your tool. I'm already using something like this with orchestrator agent and sequence subagents calls, but simplified. Works best.\n\nYou mentioned below that you feed cloud models with description how to make plan for your tool. Can you please also share this? Maybe include in repo and update periodically when tool change behavior?",
                  "score": 2,
                  "created_utc": "2026-02-27 08:58:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7n4vel",
          "author": "RainScum6677",
          "text": "Looking good. I'm working with huge code bases with some very convoluted and sometimes outdated flows(.NET 4.6-4.8, c#7), and need to deal with problematic parts of these code bases on a daily basis.\n\nQuestion: can you estimate how token efficient this system is? It looks like it might be costly to run.\n\nAlso, any way of introducing existing memory/context retention systems into the flow alongside/instead of the specified approach?\n\nVery interesting to try in workflow. Great work!",
          "score": 3,
          "created_utc": "2026-02-27 03:57:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7n6h3x",
              "author": "Outrageous-Fan-2775",
              "text": "I actually went back and forth with Perplexity about this a few days ago. The below was the result.  \n  \nShort version: it uses about 3-5x more tokens per task than base OpenCode or Claude Code. Every task goes through architect, coder, reviewer, and test engineer instead of one agent doing everything, so yeah, more tokens.\n\nBut that doesn't tell the full story.\n\nThe QA gates (syntax checking, SAST, secret scanning, build verification, placeholder detection) all run locally. No LLM calls. That stuff is free. Meanwhile Claude Code users are regularly posting about burning 10% of their weekly quota on a single plan-mode message because context just spirals.\n\nSerial execution helps too. Only one agent is loaded at a time. Claude Code's Agent Teams run at 7x overhead according to Anthropic's own docs because every teammate keeps its own full context window open.\n\nThe retrospective system also pays for itself over time. The swarm learns from past mistakes so you get fewer rework cycles, which is where most people actually waste tokens.\n\nWhere it genuinely costs more: simple stuff. A one-line typo fix still runs through the full pipeline. That's overkill and I know it.\n\nQuick comparison:\n\n* Base OpenCode/Claude Code: 1x (no review, no testing, no security scanning)\n* GSD: roughly 1x (single agent, good context isolation, but no verification)\n* Oh-My-OpenCode: 2-3x (subagents with lean context, less enforcement)\n* Claude Code Agent Teams: 7x (per Anthropic's docs)\n* opencode-swarm: 3-5x (code comes out reviewed, tested, and security scanned)\n\nThe way I think about it: what matters is cost per correct line of code, not cost per task. If you're spending tokens on rework because nothing got reviewed, you're paying anyway. The swarm just moves that cost upfront into verification instead of after the fact into debugging.",
              "score": 3,
              "created_utc": "2026-02-27 04:07:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7n84d3",
                  "author": "RainScum6677",
                  "text": "I appreciate this approach. \nUp until now, for most tasks complex tasks I've had to run,\nNone of the existing systems did better than using basic plan mode with a capable model(the longest part of the flow), revising cycles, then execution with close guidance and mostly manually reviewing(with some agent assistance thrown in).\nBut this is slow. It takes time, it's a bottleneck. And obviously it has some built in weaknesses that are difficult to handle.\n\nWill try your system. Thank you.",
                  "score": 2,
                  "created_utc": "2026-02-27 04:18:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7nu9in",
          "author": "disruptz",
          "text": "from the diag logger.\n\n---\n\nYou‚Äôre not looking at a remove failure ‚Äî this log is from:\n\nnpm install -g opencode-swarm\n\n‚Ä¶and it failed during postinstall:\n\nbun run copy-grammars\n\nModule not found \"scripts/copy-grammars.ts\"\n\nSo the package install is half-written, and npm also can‚Äôt clean it up because of EPERM (Windows file lock / permissions) in the global node_modules path.\n\nWhat‚Äôs going wrong (from your log)\n\nopencode-swarm@6.11.0 postinstall runs bun run copy-grammars (line 83‚Äì92).\n\nThat script points at scripts/copy-grammars.ts but it can‚Äôt be found (line 92).\n\nnpm then tries cleanup and hits EPERM: operation not permitted, rmdir ...\\zod\\...\n\n\n---",
          "score": 3,
          "created_utc": "2026-02-27 07:06:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pqijg",
              "author": "Outrageous-Fan-2775",
              "text": "Bug was found and should be resolved in v6.12 releasing in a couple hours.",
              "score": 2,
              "created_utc": "2026-02-27 15:26:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ldgiy",
          "author": "VVocach",
          "text": "Cool, ill test it tomorrow morning",
          "score": 2,
          "created_utc": "2026-02-26 22:01:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lifuc",
          "author": "Fit-Palpitation-7427",
          "text": "Full vibe coder building internal apps for the company, seems like I‚Äôm the perfect ginny pig with 15+ apps ATM",
          "score": 2,
          "created_utc": "2026-02-26 22:26:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lmtol",
              "author": "Outrageous-Fan-2775",
              "text": "Yeah you definitely want quality over speed for your situation. And this plugin values quality over everything else.",
              "score": 0,
              "created_utc": "2026-02-26 22:48:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lio1x",
          "author": "Soft_Syllabub_3772",
          "text": "Ill check it out!",
          "score": 2,
          "created_utc": "2026-02-26 22:27:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lusgr",
          "author": "BestUsernameLeft",
          "text": "Looks very promising. I just got opencode + oh-my-opencode running in a container and hooked up to Zen AI. But I spent more $$ than I want to yesterday evening. So, two questions. \n\n1. What's the effort to get this running in a container? \n2. Can I set up fallback models or otherwise configure to adjust between expensive models and free/local models?",
          "score": 2,
          "created_utc": "2026-02-26 23:31:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lwoyh",
              "author": "Outrageous-Fan-2775",
              "text": "1. Little to none. Same config process as oh my opencode. Add plugin to opencode.json and then create the opencode-swarm.json file for the config.  \n2. Yep. You can set up a single swarm, multiple swarms, a swarm that is all one model, and there is a defaults file that it will fall back to if necessary, although by default it just falls back to whatever model you set as the orchestrator. The most important part you want to be absolutely sure of is that the antagonistic agents are using different models. So Coder and Reviewer, Architect and Critic. You can prompt engineer all you want but if its the same model it will make the same mistakes. Two models trained on two different data sets will find problems that homogeneous setups miss.\n\nFor an example, I have one swarm called \"laptop\" that I run on my Ryzen AI Max 395+ 128gb system when I'm traveling. It only uses two models. GPT OSS 20b and some variant of Qwen coder. GPT serves as the architect, reviewer, etc while Qwen serves as the critic, coder, and test engineer. When at home I run 3 swarms, Mega, Paid, and Local. Mega has all the expensive top of the line models, Paid is one step down, and Local is all running here locally. I usually kick a project off with Mega and then let one of the other swarms take over. Or if its a smaller project the other two swarms can handle it themselves.\n\nThe last piece that is critical is generating an implementation plan BEFORE you get into OpenCode. I use Perplexity, Gemini, ChatGPT, Claude, QwenChat, etc via web chat to bounce ideas off each other until I can generate a single agreed upon implementation plan that is written specifically for the swarm workflow. I then pop it in the directory and tell the architect to implement. This saves a huge amount of API calls just nailing down the plan itself without doing any actual work.",
              "score": 1,
              "created_utc": "2026-02-26 23:41:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ly9ox",
                  "author": "BestUsernameLeft",
                  "text": "That's great advice (I'm doing similar on a more basic and inexpensive level) and great answers, I'll be kicking the tires soon!",
                  "score": 2,
                  "created_utc": "2026-02-26 23:50:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7o836w",
                  "author": "bitcoinbookmarks",
                  "text": "Nice, good to add this to docs :-)  \nAntagonistic models have some instructions to provide full solution to other model, right? Looks promising with new Qwen3.5 + GPT OSS",
                  "score": 2,
                  "created_utc": "2026-02-27 09:14:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7mun8s",
          "author": "Weird-Negotiation-27",
          "text": "Very good, I like this kind of project, it‚Äôs an improvement to our work. But honestly, I find its documentation extremely verbose without getting anywhere.\n\nI‚Äôm not a vibe coder, I‚Äôm a software engineer, and I had to read it three times and still only figured out how to use it by, well‚Ä¶ using it.\n\nThe project seems very good at first glance, but being good is not enough if people simply don‚Äôt know how to use it, what it‚Äôs for, how it actually works‚Ä¶ But even worse, what it is. Again, I, a technical professional, had to read it three times and still didn‚Äôt understand.\n\nA vibe coder or someone seriously entering the field will try to read it three times and won‚Äôt have the knowledge to explore it on their own, they‚Äôll just give up.\n\nAt the moment, my concerns are more about communication than technical aspects. I need to test it much more, I‚Äôll integrate it into the workflow of smaller projects at my company and see how it performs.\n\nI liked the suggestion of using models from different companies for tasks like QA, that perspective is usually ignored in this kind of workflow and you were spot on there, congratulations. There‚Äôs no point in asking the GLM to verify whether the code it wrote is good, it‚Äôs the same as asking me if the code I wrote is good, my answer will be ‚Äúobviously, I wrote it.‚Äù\n\nNow a question: I know they‚Äôre different proposals but the end goal is the same, how do you position yourself in relation to the GitHub Spec Kit? Yours feels much more ‚Äúvibe coder vibe‚Äù (sorry for the pun), Spec Kit involves a lot of manual action and direct user inference, yours seems more automated, fine, different proposals. But have you compared the final results both methods produce? It seems like something interesting to analyze.\n\nIn any case, I hope to see this project evolve further over time. Thank you for your dedication to this development.",
          "score": 2,
          "created_utc": "2026-02-27 02:54:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7mwnv1",
              "author": "Outrageous-Fan-2775",
              "text": "Valid points. I did use AI to write the readmes. I'm a back end engineer by trade, but being lazy and just letting AI write things for me is sometimes just the order of the day.\n\nI can definitely describe the project, reasons I went certain directions, or answer any questions you have if you want to post them here or just DM me.\n\nAs for Spec Kit, its basically a semi automated method to do what I already do when I build out specs. I bounce between Perplexity, ChatGPT, Claude, Gemini, QwenChat, Deepseek, all via their web chat and build a single implementation plan they all agree on. I give all of them a gitingest of my swarm plugin so they understand the workflow and can build the implementation plan specifically for it. AFAIK spec kit stops when it comes to creating actual code, which is where this plugin picks it up. It is about creating the highest quality code possible, but it can still be impacted by garbage in garbage out. If you give it a terrible plan that makes no sense it will try its best with the Critic to turn it into gold, but it may just be straw.",
              "score": 1,
              "created_utc": "2026-02-27 03:06:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7o0avt",
          "author": "ProvidenceXz",
          "text": "I don't know why but my AI slop alarm didn't go off on this post. I found a reason to try opencode.",
          "score": 2,
          "created_utc": "2026-02-27 08:00:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o8fi6",
              "author": "ProvidenceXz",
              "text": "But yeah. Npm install is broken.",
              "score": 2,
              "created_utc": "2026-02-27 09:17:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ps5ep",
                  "author": "Outrageous-Fan-2775",
                  "text": "Found the NPM bug. it was related to the git staging having improper bun files in it as well as listed in the package json. Fix will be live in a couple hours.",
                  "score": 1,
                  "created_utc": "2026-02-27 15:33:53",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7rmpvi",
          "author": "disruptz",
          "text": "I'm not sure if anyone else here has actually succesfully installed this plugin with opencode,\nanyway, I have used your 6.12.0, using trialing npm install/test/fail/remove, the bun install/test/fail\n\nI am using opencode desktop 1.2.15\nalso tried using opencode cli\n\nerror output below running /swarm diagnose, similar output if I use /swarm plan\n\n\n---\n\n{{arguments}}\ndiagnose\nThinking: The user sent \"{{arguments}}\\n\\ndiagnose\". That's weird. It seems placeholder. Likely the user expects a request with some arguments. Maybe the user inserted a placeholder? In the conversation, the system gave large instruction. But the user message just says \"{{arguments}}\\ndiagnose\". That suggests maybe there was a bug. They might want us to diagnose something. But no context. We need to interpret. Could be they want to run a command? Possibly the issue is that the repository is not a git repo (see environment). They might want to diagnose a bug.\n\n---\n\nI am intrigued in using this, as I found ohmycode etc too heavy",
          "score": 2,
          "created_utc": "2026-02-27 20:54:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rnr3j",
              "author": "Outrageous-Fan-2775",
              "text": "Your issue has also been reported on the GitHub repo and is in the pipe to be fixed now. See below. Fix will be shipped with v6.13.1. I am finishing v6.13 now so expect 6.13.1 later today.\n\n[https://github.com/zaxbysauce/opencode-swarm/issues/6#issuecomment-3974932059](https://github.com/zaxbysauce/opencode-swarm/issues/6#issuecomment-3974932059)",
              "score": 1,
              "created_utc": "2026-02-27 20:59:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ro007",
                  "author": "disruptz",
                  "text": "nice, great to see the active contribution and patching :)",
                  "score": 2,
                  "created_utc": "2026-02-27 21:00:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qth0x",
          "author": "atkr",
          "text": "this seems like total garbage",
          "score": 1,
          "created_utc": "2026-02-27 18:30:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qvzst",
              "author": "Outrageous-Fan-2775",
              "text": "Certainly open to all perspectives. Any reason why you say that? I've used it personally for about a month now to build real shippable and shipped projects.",
              "score": 1,
              "created_utc": "2026-02-27 18:42:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7l35z8",
          "author": "HarjjotSinghh",
          "text": "you nailed my dev soulmate now.",
          "score": 1,
          "created_utc": "2026-02-26 21:12:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l4ko4",
              "author": "Outrageous-Fan-2775",
              "text": "lol. It can still be annoying with AI slop, but the guardrails are so tight in this latest version that the occurrences are much fewer and further between. Plus the checks are balances are so rigorous its very hard for any of that to ever make it through to the final product.",
              "score": 2,
              "created_utc": "2026-02-26 21:18:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7mnbgi",
                  "author": "Ang_Drew",
                  "text": "hey FYI, he is bot.. dont take it seriously",
                  "score": 2,
                  "created_utc": "2026-02-27 02:12:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rg5uza",
      "title": "I wrote an open source package manager for skills, agents, and commands - OpenPackage",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/9vxwzyg9a1mg1.png",
      "author": "hyericlee",
      "created_utc": "2026-02-27 12:52:57",
      "score": 35,
      "num_comments": 11,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rg5uza/i_wrote_an_open_source_package_manager_for_skills/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7oywv0",
          "author": "ShagBuddy",
          "text": "An actually interesting and useful idea!  üëç. OpenCode would be a good addition for sure.",
          "score": 5,
          "created_utc": "2026-02-27 12:56:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ozdzd",
              "author": "hyericlee",
              "text": "Thanks, glad it‚Äôs of interest!",
              "score": 1,
              "created_utc": "2026-02-27 12:59:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7q1vdy",
          "author": "Nearby_Tumbleweed699",
          "text": "Te rifaste bueno con eso",
          "score": 1,
          "created_utc": "2026-02-27 16:20:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qqmjk",
          "author": "mintybadgerme",
          "text": "Nice.",
          "score": 1,
          "created_utc": "2026-02-27 18:17:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7r9850",
          "author": "SvenVargHimmel",
          "text": "I really like this project but I've had a tough time getting working. I  come back to this project about once every two weeks and it feel somewhat obtuse. \n\nCan we have a quick start for: \n\n1. Creating your own package ( can be simple ./agent/SUBAGENT.md ) and then installing in a project  abc \n\n2. How to install an existing github stored skill  in a.) current directory  b.) globally \n\nAlso it's not clear that you need a openpackage.yml , but apparently you do, even though \n\nopkg install /path/to/agent/artificact works ? \n\n\n\nSo this isn't a moan but just highlighting that the onboarding is very very confusing (for me perhaps, it's probably just me) but it could do with some love \n\n\n\n\n\n\n\n",
          "score": 1,
          "created_utc": "2026-02-27 19:46:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7szuy9",
              "author": "hyericlee",
              "text": "Thanks for the feedback, have actually just recently updated the GitHub README and CLI commands to outline the process better, do take a look!\n\nPackage creation is \\`opkg new <package-name>\\`, then add \\`./agents/SUBAGENT.md\\` to that package. Then install with \\`opkg install <package-name>\\`.  \n  \nTo install specific resources you can use \\`--agents\\`, \\`--skills\\`, \\`--commands\\` etc. options and for global installs use \\`-g\\` option.\n\nTo install from GitHub use \\`opkg install gh@<owner>/<repo>\\` format, or \\`opkg install <url>\\`.\n\nSo to install an existing GitHub skill, simply combine the GitHub install with \\`--skills <skill>\\` and use \\`-g\\` for global/user scoped installs.\n\nI'll update the Quick Start guide at [openpackage.dev/docs](http://openpackage.dev/docs) to outline these items specifically though, I do agree that doc is a bit behind.\n\nThis is really good feedback, thanks so much!",
              "score": 2,
              "created_utc": "2026-02-28 01:25:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7re36r",
          "author": "hdmcndog",
          "text": "opkg is already a thing, you might want to use a different name for the executable, to avoid confusion.\n\nhttps://openwrt.org/docs/guide-user/additional-software/opkg",
          "score": 1,
          "created_utc": "2026-02-27 20:10:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sgnka",
          "author": "sudoer777_",
          "text": "Personally I'm more interested in seeing integrations with existing tools like Nix rather than 50 more package managers",
          "score": 1,
          "created_utc": "2026-02-27 23:31:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ub6o7",
          "author": "QuinsZouls",
          "text": "Openslot",
          "score": 1,
          "created_utc": "2026-02-28 07:00:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7zc64i",
          "author": "HarjjotSinghh",
          "text": "this is unreasonably genius actually!",
          "score": 1,
          "created_utc": "2026-03-01 01:48:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7q2q0w",
          "author": "atkr",
          "text": "slop, garbage",
          "score": -3,
          "created_utc": "2026-02-27 16:24:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ril0ff",
      "title": "Official opencode go limits published",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1ril0ff/official_opencode_go_limits_published/",
      "author": "Resident-Ad-5419",
      "created_utc": "2026-03-02 06:05:04",
      "score": 33,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "This is an excerpt from the official docs:\n\nOpenCode Go includes the following limits:\n\n* **5 hour limit**¬†‚Äî $4 of usage\n* **Weekly limit**¬†‚Äî $10 of usage\n* **Monthly limit**¬†‚Äî $20 of usage\n\nIn terms of tokens, $20 of usage is roughly equivalent to:\n\n* 69 million GLM-5 tokens\n* 121 million Kimi K2.5 tokens\n* 328 million MiniMax M2.5 tokens\n\nBelow are the prices¬†**per 1M tokens**.\n\n|Model|Input|Output|Cached Read|\n|:-|:-|:-|:-|\n|GLM-5|$1.00|$3.20|$0.20|\n|Kimi K2.5|$0.60|$3.00|$0.10|\n|MiniMax M2.5|$0.30|$1.20|$0.03|\n\nOne important thing to note is the **chart** inside the zen page lists glm-5, kimi k2.5 and minimax m2.5 with a (lite) suffix. The suffix is not explained anywhere yet.\n\nhttps://preview.redd.it/btmotenxikmg1.png?width=2678&format=png&auto=webp&s=b007c8c8eb3c97509f487ec870d87eaeaf1fe1f3\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1ril0ff/official_opencode_go_limits_published/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o86rby0",
          "author": "_d1re",
          "text": "Thinking of subscribing to Go as secondary provider. Are they serving lobotimized versions of these models?",
          "score": 2,
          "created_utc": "2026-03-02 06:21:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o86tmdm",
              "author": "c0nfluks",
              "text": "I think so yes. If you browse through recent posts on this subreddit i think someone mentioned that in a post a few days ago.",
              "score": 1,
              "created_utc": "2026-03-02 06:40:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o86vr2j",
              "author": "Huge-Refrigerator95",
              "text": "Not sure if lobotomized but they‚Äôre good",
              "score": 1,
              "created_utc": "2026-03-02 06:59:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o871qeo",
                  "author": "0xDezzy",
                  "text": "GLM-5 was definitely lobotomized. Kept messing up and doing stupid shit more than when it was free.",
                  "score": 1,
                  "created_utc": "2026-03-02 07:55:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o8709wd",
          "author": "Specialist-Yard3699",
          "text": "Hmm, seems like there‚Äôs no point in purchasing this subscription if you code a lot. Minimax 2.5 + GLM-5 as separate subscriptions look more reliable.\nP.s. I spend about 160kk tokens per week only with glm-5 ü´†",
          "score": 2,
          "created_utc": "2026-03-02 07:42:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o86urms",
          "author": "Mistercheese",
          "text": "Do you know if there are concurrent request limits? Is it only one at a time sequential or can you parallelize subagents/prompts?",
          "score": 1,
          "created_utc": "2026-03-02 06:51:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o86wjj7",
          "author": "trypnosis",
          "text": "Now they just need to confirm a EU/US only hosted option. \n\nI will keep the dream alive.",
          "score": 1,
          "created_utc": "2026-03-02 07:06:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o86zf88",
          "author": "jellydn",
          "text": "I have just canceled it as this plan is not really for work. Better to get the ChatGPT Plus plan. \n\n\nBelow is my usage for the Go plan since yesterday:) I bought this plan yesterday and cancelled today. \n\nRolling Usage53%\nResets in 1 hour 15 minutes\nWeekly Usage27%\nResets in 6 days 16 hours\nMonthly Usage48%\nResets in 26 days 2 hours",
          "score": 1,
          "created_utc": "2026-03-02 07:33:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o872lxy",
          "author": "Confident_Tear_1920",
          "text": "So which sub is the cheapest for the minimax model? I am finding that model particularly useful for my coding tasks. Started running it with opencode and seems to work well but now it says I have hit the limit of free usage. What is the best option to keep using that model?\n\nI tried finding that Go subscription and I cannot find it. It's in opencode documentation but the link takes me to Zen, which requests me to enable billing by adding 20 dollars to my account for \"credits\". That seems a pay-as-you-go model, not a sub. I'm still interested in finding a sub and not a pay-as-you-go model!",
          "score": 1,
          "created_utc": "2026-03-02 08:04:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o873dv4",
              "author": "MofWizards",
              "text": "For Minimax, it's Minimax's own signature.",
              "score": 1,
              "created_utc": "2026-03-02 08:11:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o872pwm",
          "author": "wanllow",
          "text": "price of glm5 is more expensive than openrouter, why?",
          "score": 1,
          "created_utc": "2026-03-02 08:05:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rewp99",
      "title": "I got tired of rate limits, so I wired 80+ free models together",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rewp99/i_got_tired_of_rate_limits_so_i_wired_80_free/",
      "author": "rroj671",
      "created_utc": "2026-02-26 01:33:19",
      "score": 32,
      "num_comments": 25,
      "upvote_ratio": 0.84,
      "text": "Built a small routing layer that sits in front of OpenCode and automatically switches between 80+ free model endpoints.\n\nIt monitors latency and failures in real time and fails over when a provider gets slow or rate limited. It auto-selects the fastest healthy model at request time.\n\nhttps://preview.redd.it/p8ht0jf19qlg1.png?width=2858&format=png&auto=webp&s=ca5121d68b2b9eccc02c68a5dcc4c3b638c042fa\n\nhttps://preview.redd.it/dx8onhxksqlg1.png?width=2516&format=png&auto=webp&s=54e45da07ecd3c919094a0f670f64052e9de35ac\n\n  \n`npm install -g modelrelay`\n\n`modelrelay onboard`\n\nSource: [https://github.com/ellipticmarketing/modelrelay](https://github.com/ellipticmarketing/modelrelay)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rewp99/i_got_tired_of_rate_limits_so_i_wired_80_free/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7gn1de",
          "author": "Ang_Drew",
          "text": "how about the result consistency?",
          "score": 7,
          "created_utc": "2026-02-26 04:48:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7htlar",
              "author": "rroj671",
              "text": "You can pin a model and it will serve the same one for all requests until you unpin it or it goes down.",
              "score": 2,
              "created_utc": "2026-02-26 11:01:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7iyyxs",
                  "author": "Ang_Drew",
                  "text": "that's actually cool..",
                  "score": 2,
                  "created_utc": "2026-02-26 15:17:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7fsp7a",
          "author": "ELPascalito",
          "text": "So you're just hammering the free Nvidia API? Got it",
          "score": 11,
          "created_utc": "2026-02-26 01:45:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hu1te",
              "author": "rroj671",
              "text": "Hammering?",
              "score": 1,
              "created_utc": "2026-02-26 11:05:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7h6y6m",
          "author": "DJGreenHill",
          "text": "This is why we can‚Äôt have nice things",
          "score": 7,
          "created_utc": "2026-02-26 07:27:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hbqq9",
          "author": "Deep_Traffic_7873",
          "text": "great, this project will pop the AI bubble",
          "score": 5,
          "created_utc": "2026-02-26 08:12:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hrmio",
          "author": "FormalAd7367",
          "text": "If we want cheap/free, we could just switch between those Nvidia models and Deepseek.  My company uses Deepseek API to keep cost down (we were on Gemini API and Open Ai).",
          "score": 2,
          "created_utc": "2026-02-26 10:44:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mtg51",
          "author": "bradjones6942069",
          "text": "Thank you for this",
          "score": 2,
          "created_utc": "2026-02-27 02:47:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nnj3v",
          "author": "Glittering-Call8746",
          "text": "Anyone outside of USA able to get otp from build.nvidia.com ?",
          "score": 2,
          "created_utc": "2026-02-27 06:10:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7w4e84",
          "author": "HarjjotSinghh",
          "text": "this is insanely brilliant actually.",
          "score": 2,
          "created_utc": "2026-02-28 15:26:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7h2bov",
          "author": "MrMrsPotts",
          "text": "I need something like this but I am worried that switching like this will damage the quality of the code you get.",
          "score": 1,
          "created_utc": "2026-02-26 06:47:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7h34qy",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 4,
              "created_utc": "2026-02-26 06:54:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7omwnx",
                  "author": "Xanian123",
                  "text": "I shudder to think of the slop it'll generate lol",
                  "score": 1,
                  "created_utc": "2026-02-27 11:29:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7h8s3m",
          "author": "Timo_schroe",
          "text": "So like the openrouter free endpoint ?",
          "score": 1,
          "created_utc": "2026-02-26 07:44:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hgmoi",
              "author": "rroj671",
              "text": "Yes, openrouter is one of the providers. You can just add more.",
              "score": 2,
              "created_utc": "2026-02-26 08:59:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7oszjd",
          "author": "Competitive-Yak-8255",
          "text": "Thanks alot üôèüôè",
          "score": 1,
          "created_utc": "2026-02-27 12:15:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pirus",
          "author": "stevilg",
          "text": "How are you handling the fact that context size might vary quite a bit?",
          "score": 1,
          "created_utc": "2026-02-27 14:47:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gur26",
          "author": "crmfan",
          "text": "Doesnt it lose all the context when switching randomly",
          "score": 1,
          "created_utc": "2026-02-26 05:45:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7h2zj5",
              "author": "Delyzr",
              "text": "Your client sents the context each time. If you ar  e always using the same model, it might have a copy of your context from last call and might be able to reuse part of it. This is called a cache hit. If it doesn't, its input tokens.\n\nIf you look at something like openrouter you might be switching provider every request, so there goes your serverside cache.\n\nEven if always using the same provider/model directly, you might switch physical machines in their cloud from one moment to the next, and the new machine doesn't have your context cached.",
              "score": 6,
              "created_utc": "2026-02-26 06:53:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7h3le5",
              "author": "mdn0",
              "text": "No. They are stateless.",
              "score": 1,
              "created_utc": "2026-02-26 06:58:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jghyf",
          "author": "oxygen_addiction",
          "text": "People like you are why we can't have free shit.",
          "score": -1,
          "created_utc": "2026-02-26 16:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jgwx6",
              "author": "rroj671",
              "text": "How is this package hurting others? Your usage would be the same. The only thing automated is switching models that go offline.",
              "score": 1,
              "created_utc": "2026-02-26 16:39:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7jiqw7",
                  "author": "oxygen_addiction",
                  "text": "You are hammering all models and enabling non-techies to do the same. Fine for personal usage, but the more people use packages like this, the more free services degrade.\n\nSometimes it's better to keep things to yourself.",
                  "score": 2,
                  "created_utc": "2026-02-26 16:47:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7frjai",
          "author": "HarjjotSinghh",
          "text": "this is the future of ai magic, baby!",
          "score": -4,
          "created_utc": "2026-02-26 01:38:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1revdmm",
      "title": "OpenCode rocks",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1revdmm/opencode_rocks/",
      "author": "tiguidoio",
      "created_utc": "2026-02-26 00:36:53",
      "score": 25,
      "num_comments": 13,
      "upvote_ratio": 0.93,
      "text": "I tried it many months ago, and it was meh. Last week, I gave it another shot because we need cheaper solutions for Kosuke's code generation pipeline, so I deeply tested OpenCode with GLM-5 served through Fireworks Al. As of today, it is feature-rich, supports ALL providers, is highly customizable, and has a web interface too.\n\nVery nice.\n\nAll the companies that have been blocked by Anthropic's Terms of Service will need to find a more open and cheaper solution. The combination of OpenCode, GLM-5, and Fireworks Al is a solid option if you are frustrated by Anthropic's API token costs but don't want to compromise on quality for your users.\n\nWe are going to adopt this stack, and it is clear to me that options will only increase. Anthropic's centralization of intelligence is just a spike in the Al marathon.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1revdmm/opencode_rocks/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7g9k2u",
          "author": "Wrenky",
          "text": "I wish the web UI on mobile was better. I generally have to load the page, then send a command, then to see results I have to hard refresh.",
          "score": 5,
          "created_utc": "2026-02-26 03:21:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gm1nw",
              "author": "AsteroidCosmic",
              "text": "It works fine for me as a pwa. I mean there‚Äôs definitely things here and there, but makes vibe coding on iphone or ipad not a bad experience at all. Just with I had a way to work with multiple branches/worktrees at once. Haven‚Äôt found a way to make it work",
              "score": 3,
              "created_utc": "2026-02-26 04:42:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7fiawb",
          "author": "LittleYouth4954",
          "text": "I am using GLM 5 through siliconflow and directly through Z.ai in OpenCode and also getting great results",
          "score": 3,
          "created_utc": "2026-02-26 00:46:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fowia",
          "author": "RedParaglider",
          "text": "Yeah open code has pretty heavy tax on context, but that's the only thing that I can say is a negative it's a pretty amazing product. It handles development so much better than any other CLI/TUI in my opinion.",
          "score": 3,
          "created_utc": "2026-02-26 01:23:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hcs4e",
          "author": "Less_Ad_1505",
          "text": "Opencode is great, but I have issues exactly with GLM-5 (Opencode Zen, Opencode Go). It sometimes gets confused with calling tools, as well as the text of thinking blocks flowing into messages. It also happens that the model switches to Chinese. ",
          "score": 2,
          "created_utc": "2026-02-26 08:21:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ilxku",
          "author": "Peterako",
          "text": "I love integrating with Codex, and then I have Minimax do some of the builds after the plan is complete.",
          "score": 2,
          "created_utc": "2026-02-26 14:10:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yyrt8",
          "author": "JamerDoria",
          "text": "En verdad he estado varias tecnolog√≠as con IA para C√≥digo y en verdad OpenCode es Genial.",
          "score": 1,
          "created_utc": "2026-03-01 00:27:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fhas7",
          "author": "HarjjotSinghh",
          "text": "this is what i'd call a pipeline upgrade!",
          "score": 1,
          "created_utc": "2026-02-26 00:40:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7guxw2",
          "author": "soul105",
          "text": "This looks like an Ad",
          "score": -1,
          "created_utc": "2026-02-26 05:46:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hd21t",
              "author": "tiguidoio",
              "text": "Nope",
              "score": 2,
              "created_utc": "2026-02-26 08:24:37",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7kzml9",
              "author": "RegrettableBiscuit",
              "text": "Yeah, typical structure of a Reddit submarine ad. Fireworks AI, thanks for your service.¬†",
              "score": 1,
              "created_utc": "2026-02-26 20:55:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7oz59o",
          "author": "Apprehensive_Half_68",
          "text": "Ad. Stop it.",
          "score": 0,
          "created_utc": "2026-02-27 12:57:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7oznjr",
              "author": "tiguidoio",
              "text": "Not an ad, wtf are you saying",
              "score": 1,
              "created_utc": "2026-02-27 13:00:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdhsd0",
      "title": "The research is in: your AGENTS.md might be hurting you",
      "subreddit": "opencodeCLI",
      "url": "https://sulat.com/p/agents-md-hurting-you",
      "author": "jpcaparas",
      "created_utc": "2026-02-24 14:32:25",
      "score": 24,
      "num_comments": 19,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rdhsd0/the_research_is_in_your_agentsmd_might_be_hurting/",
      "domain": "sulat.com",
      "is_self": false,
      "comments": [
        {
          "id": "o75b4we",
          "author": "KnifeFed",
          "text": "TL;DR: Don't put a bunch of unnecessary shit in your AGENTS.md, dummy.",
          "score": 14,
          "created_utc": "2026-02-24 15:08:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o767ele",
              "author": "florinandrei",
              "text": "That repo prompt is very useful. Otherwise, the model has zero context on why that repo exists, and what it does. So it has to start from scratch, it hallucinates, etc.\n\nSo, write a prompt, duh. But keep it short and sweet. And tweak it as the repo changes.",
              "score": 2,
              "created_utc": "2026-02-24 17:35:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75kbna",
          "author": "Sea-Sir-2985",
          "text": "this matches what i've seen with claude code's CLAUDE.md too... i used to dump everything in there thinking more context equals better results but it actually made the model worse at following the important rules. now i keep the main file lean with just the critical stuff and use separate resource files for reference data that gets loaded on demand\n\nthe model's attention is finite so every unnecessary line is diluting the instructions that actually matter",
          "score": 8,
          "created_utc": "2026-02-24 15:50:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o779tgy",
          "author": "philosophical_lens",
          "text": "I think it depends on your goals. If you‚Äôre just looking for immediate results like ‚Äúfix this bug‚Äù or ‚Äúadd this feature‚Äù then I agree that Agents.md may be hurting you. \n\nBut if your goal is to build a long term sustainable codebase that adheres to your desired architectural principles and rules, then I think it‚Äôs still helpful. \n\nFor me Agents.md is partly an aspirational document describing what I want my code to look like. This pays off in the long run 20 new features later, I‚Äôll have a more maintainable code base.",
          "score": 3,
          "created_utc": "2026-02-24 20:28:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o764nsy",
          "author": "Lucky_Yam_1581",
          "text": "I always felt its limitations and wondered why AI labs asked us to keep updating it; it‚Äôs more proof that even AI labs are not expert at their own models or harnesses",
          "score": 1,
          "created_utc": "2026-02-24 17:22:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76ngsb",
              "author": "Xera1",
              "text": "Nobody is, yesterday's best practice is today's old hat.",
              "score": 1,
              "created_utc": "2026-02-24 18:46:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o76m16d",
          "author": "Honest-Ad-6832",
          "text": "Agents.md you say? I am building an entire .md ecosystem around each initial prompt. Time for another refactor I guess.¬†",
          "score": 1,
          "created_utc": "2026-02-24 18:40:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78j6vw",
          "author": "sjmaple",
          "text": "There's no point writing context and assuming it's right - you have to eval everything you add as context. Here's a counter argument to the paper's conclusions, which I believe are flawed.\n\nYour¬†[AGENTS.md](http://agents.md/)¬†file isn't the problem. Your lack of Evals is.¬†[https://tessl.io/blog/your-agentsmd-file-isnt-the-problem-your-lack-of-evals-is/](https://tessl.io/blog/your-agentsmd-file-isnt-the-problem-your-lack-of-evals-is/)",
          "score": 1,
          "created_utc": "2026-02-25 00:13:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7aikxt",
              "author": "touristtam",
              "text": "I have completely forgotten about tessl.io now that I have come accross skills.sh.",
              "score": 1,
              "created_utc": "2026-02-25 08:06:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7iofk4",
                  "author": "sjmaple",
                  "text": "You should take a look - the evals, optimizations etc are really valuable to know if your context is any good. Skills. sh is just a github download npx command.",
                  "score": 1,
                  "created_utc": "2026-02-26 14:23:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bvxns",
          "author": "trypnosis",
          "text": "This feels half true or incomplete. \n\n\nThis needs bigger data sets more review and broader data sets as this does not align fully with my experiences. \n\nWhere I agree that redundant info should not be in the md files. What is redundant in what scenario a file is not fully addressed. \n\n\nNor is what should be in the md file accurately addressed.\n\nI agree that certain things should and should not be in the md file. But what this article states should and should not needs a fair amount of work. \n\nPlus some of the numbers seem small. \n\nIt starts with number like 60k but then when the studies are described its numbers like 12 and 124.\n\nI think way more work is needed before I change my working behaviours.",
          "score": 1,
          "created_utc": "2026-02-25 14:20:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dhuu2",
          "author": "vistdev",
          "text": "This study gave me a moment of genuine doubt about something I‚Äôve been building‚Ä¶ I‚Äôm working on a tool called Vist, a note-taking and task management app with an MCP server that gives AI assistants persistent memory across sessions. The whole premise is that context makes AI more useful. So naturally, reading that context files ‚Äútend to reduce task success rates compared to providing no repository context‚Äù and increase inference cost by over 20% was‚Ä¶ not ideal timing. ü´£\n\nAm I building something counterproductive?\n\nAfter sitting with it for a bit, I don‚Äôt think so, and the paper actually explains why. The problem with AGENTS.md files isn‚Äôt context per se, it‚Äôs static, monolithic, often auto-generated descriptions that pile on ‚Äúunnecessary requirements‚Äù and push agents toward broad exploration when they should stay focused. The paper‚Äôs own conclusion is that human-written files should ‚Äúdescribe only minimal requirements.‚Äù That‚Äôs not an argument against context. That‚Äôs an argument against noise.\nWhat Vist tries to do is structurally different: recent and dynamic context (what you‚Äôve been working on, what decisions were made this week), things you deliberately chose to persist rather than LLM-generated summaries of your entire codebase, and short actionable snapshots rather than architectural essays.\nThe study‚Äôs behavioral finding actually supports this: agents did follow instructions in context files, they just got misled by noisy or overly prescriptive ones. The problem is signal quality, not context as a concept.\nIf anything, the paper is a useful reminder to keep Vist‚Äôs loaded context tight. The temptation when building a memory system is to surface more, always more. The right instinct is probably the opposite: surface less, surface it well, make sure it‚Äôs current. \nA lesson I‚Äôll almost certainly have to learn twice. üò¨",
          "score": 1,
          "created_utc": "2026-02-25 18:49:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7i07jk",
          "author": "Ok-Inspection-2142",
          "text": "It still needs to be structured in a way that‚Äôs easily consumed. I don‚Äôt truly believe they have a negative utility.",
          "score": 1,
          "created_utc": "2026-02-26 11:56:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75pv98",
          "author": "soul105",
          "text": "Just use /init and let the model write down one for you. Use a good reasonable model, thats it.",
          "score": -7,
          "created_utc": "2026-02-24 16:16:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75ykzf",
              "author": "KnifeFed",
              "text": "Is what someone who either hasn't read the study or who doesn't understand the implications of it would say.",
              "score": 10,
              "created_utc": "2026-02-24 16:54:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o767ndr",
                  "author": "florinandrei",
                  "text": "It's a decent start if you have literally zero experience with agentic AI.\n\nBut then you should maintain and evolve that file.",
                  "score": 0,
                  "created_utc": "2026-02-24 17:36:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o77845q",
              "author": "philosophical_lens",
              "text": "This is exactly the opposite of what this research suggests.",
              "score": 4,
              "created_utc": "2026-02-24 20:20:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o77oz3e",
              "author": "JMowery",
              "text": "That was actually the worst result from the research, genius.",
              "score": 3,
              "created_utc": "2026-02-24 21:38:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdliv0",
      "title": "Started using Skills on OpenCode",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rdliv0/started_using_skills_on_opencode/",
      "author": "No_Mousse_2765",
      "created_utc": "2026-02-24 16:51:05",
      "score": 24,
      "num_comments": 14,
      "upvote_ratio": 0.9,
      "text": "Set up Skills on Opencode similar to Claude Skills.\n\nDownloaded a repo from github to test few use cases.\n\nSuper happy.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rdliv0/started_using_skills_on_opencode/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o763k3p",
          "author": "soul105",
          "text": "https://skills.sh\n\n\nIt makes your life even easier.",
          "score": 16,
          "created_utc": "2026-02-24 17:17:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o764f13",
              "author": "No_Mousse_2765",
              "text": "This curates all the skills out there. Thank you for the recommendation.",
              "score": 1,
              "created_utc": "2026-02-24 17:21:19",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o76iuhz",
              "author": "Nearby_Tumbleweed699",
              "text": "Mis respetos hombre",
              "score": 1,
              "created_utc": "2026-02-24 18:25:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o78orf5",
              "author": "Competitive-Yak-8255",
              "text": "Thanks for the share",
              "score": 1,
              "created_utc": "2026-02-25 00:43:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75zm89",
          "author": "e979d9",
          "text": "Can you explain the concept and quote the repo ?",
          "score": 3,
          "created_utc": "2026-02-24 16:59:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76km1u",
          "author": "forgotten_airbender",
          "text": "Read this though\nhttps://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals\n",
          "score": 4,
          "created_utc": "2026-02-24 18:33:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76wrdd",
              "author": "No_Cheek5622",
              "text": "Read this though [https://arxiv.org/abs/2602.11988](https://arxiv.org/abs/2602.11988)",
              "score": 4,
              "created_utc": "2026-02-24 19:28:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79nu75",
                  "author": "forgotten_airbender",
                  "text": "Have read this. This boils down to agents.md not having relevant information / too much bloat. As long as your agent.md is lean and you maintain it constantly, it does work really well.¬†",
                  "score": 5,
                  "created_utc": "2026-02-25 04:04:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7etpgw",
          "author": "aeroumbria",
          "text": "I found skill activation to be pretty hit or miss no matter where I use them... Basically, if you completely leave it to the agent to decide when to use a skill, it is a coin toss. But if you explicitly instruct or conditionally instruct the agent to use a skill, it doesn't feel any different from asking the agent to follow any other md files. I get the appeal of \"pre-made situational prompts\" but it doesn't seem like the \"spontaneous activation\" aspect is reliable enough for me to want to depend on it...",
          "score": 1,
          "created_utc": "2026-02-25 22:33:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ix1dd",
          "author": "PieAdditional7324",
          "text": "What if my skills are not loading? I usually install them globally, but the opencode shows me I have 0 skills.\n\nI install them with npx skills to the \\~/.agents folder (the default)",
          "score": 1,
          "created_utc": "2026-02-26 15:07:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7k9lzi",
              "author": "No_Mousse_2765",
              "text": "You should move them to the Skills folder.",
              "score": 1,
              "created_utc": "2026-02-26 18:51:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7y3aww",
          "author": "HarjjotSinghh",
          "text": "this skill setup feels like goldmine found!",
          "score": 1,
          "created_utc": "2026-02-28 21:29:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78oam0",
          "author": "seymores",
          "text": "Also check out ‚Äúsuperpower‚Äù.",
          "score": 0,
          "created_utc": "2026-02-25 00:41:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rhk36v",
      "title": "Opencode Go GLM provider is nerfed / heavily quantized",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rhk36v/opencode_go_glm_provider_is_nerfed_heavily/",
      "author": "coding9",
      "created_utc": "2026-03-01 01:27:03",
      "score": 23,
      "num_comments": 11,
      "upvote_ratio": 0.9,
      "text": "I gave it a routine task, it was getting super confused and running a bunch of invalid commands.\n\n  \nSwitch to ollama cloud also glm5, run exact same first prompt, completely solved the problem I was working on intelligently.\n\n  \nThis is pretty bad and will leave people thinking glm 5 sucks when there is something bad going on with opencode go at least as of tonight while im testing it.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rhk36v/opencode_go_glm_provider_is_nerfed_heavily/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7znfnl",
          "author": "Superb_Plane2497",
          "text": "The one on [together.ai](http://together.ai) is 4 bit as well. The [z.ai](http://z.ai) coding plan is not quantized, I assume. It works very well, I use three models a lot, gpt-5.3 Codex (openai plan), gemini flash (google plan, I'm hoping for light usage I don't get whacked for using it with opencode) and GLM-5 (z.ai plan)",
          "score": 4,
          "created_utc": "2026-03-01 02:57:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o81mx2z",
          "author": "Ambitious-Call-7565",
          "text": "any paid service that doesn't display information about their models IN DETAIL is a fraud\n\n  \n",
          "score": 2,
          "created_utc": "2026-03-01 12:54:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7zbd0o",
          "author": "JohnnyDread",
          "text": "This has been my experience as well. GLM5 was great for me when I first started using it. I used it a couple of days, having it do a lot of the tasks that I would normally give to Sonnet and even Opus sometimes, and it did quite well. Then it fell off a cliff and started hallucinating badly, using tools wrong and generating long streams of gibberish. ",
          "score": 4,
          "created_utc": "2026-03-01 01:43:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o835c1a",
          "author": "sporez",
          "text": "What are the limits of ollama clouds $20 plan? Are they reasonable?",
          "score": 1,
          "created_utc": "2026-03-01 17:45:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o838myh",
              "author": "coding9",
              "text": "So far they seem pretty good for me",
              "score": 1,
              "created_utc": "2026-03-01 18:00:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o80meo8",
          "author": "Resident-Ad-5419",
          "text": "It's a lite version. You get what you pay for.\n\nEdit: Added screenshot. \n\nhttps://preview.redd.it/9eqshswk7hmg1.png?width=2770&format=png&auto=webp&s=b9a53e505912f8e578868e68fe8f47b91c55b608",
          "score": -1,
          "created_utc": "2026-03-01 07:21:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o838jii",
              "author": "coding9",
              "text": "It says glm-5 not glm-5-lite",
              "score": 0,
              "created_utc": "2026-03-01 18:00:10",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o83r6bk",
              "author": "coding9",
              "text": "This isn't in the opencode cli ui at all. They need to update it",
              "score": 0,
              "created_utc": "2026-03-01 19:27:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7z9mci",
          "author": "HarjjotSinghh",
          "text": "wowllama's got that golden touch - go team still in panic mode?",
          "score": -8,
          "created_utc": "2026-03-01 01:32:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rf8v3s",
      "title": "[PLUGIN] True-Mem: Automatic AI memory that actually works (inspired by PsychMem)",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rf8v3s/plugin_truemem_automatic_ai_memory_that_actually/",
      "author": "rizal72",
      "created_utc": "2026-02-26 12:28:43",
      "score": 22,
      "num_comments": 19,
      "upvote_ratio": 0.9,
      "text": "Hey everyone!\n\nI've been working on [**True-Mem**](https://github.com/rizal72/true-mem), a plugin that gives OpenCode persistent memory across sessions - **completely automatically**.  \nI made it for myself, taking inspiration from [PsychMem](https://github.com/muratg98/psychmem), but I tried to adapt it to my multi-agent workflow (I use [oh-my-opencode-slim](https://github.com/alvinunreal/oh-my-opencode-slim) of which I am an active contributor) and my likings, trying to minimize the flaws that I found in other similar plugins: it is much more restrictive and does not bloat your prompt with useless false positives. It's not a replacement for *AGENTS.md*: it is another layer of memory!  \nI'm actively maintaining it simply because **I use it**...\n\n# The Problem\n\nIf you've ever had to repeat your preferences to your AI assistant every new session - \"I prefer TypeScript\", \"Never use var\", \"Always run tests before commit\" - you know the pain. The AI forgets everything you've already told it.\n\nOther memory solutions require you to manually tag memories, use special commands, or explicitly tell the system what to remember. That's not how human memory works. Why should AI memory be any different?\n\n# The Solution\n\n**True-Mem** is **100% automatic**. Just have a normal conversation with OpenCode. The plugin extracts, classifies, stores, and retrieves memories without any intervention:\n\n* No commands to remember\n* No tags to add\n* No manual storage calls\n* No special syntax\n\nIt works like your brain: you talk, it remembers what matters, forgets what doesn't, and surfaces relevant context when you need it.\n\n# What Makes It Different\n\nIt's modeled after cognitive psychology research on human memory:\n\n* **Atkinson-Shiffrin Model** \\- Classic dual-store architecture (STM/LTM) with automatic consolidation based on memory strength\n* **Ebbinghaus Forgetting Curve** \\- Temporal decay for episodic memories using exponential decay function; semantic memories are permanent\n* **7-Feature Scoring Model** \\- Multi-factor strength calculation: Recency, Frequency, Importance, Utility, Novelty, Confidence, and Interference penalty\n* **Memory Reconsolidating** \\- Conflict resolution via similarity detection (Jaccard coefficient) with three-way handling: duplicate, complement, or conflict\n* **Four-Layer Defense System** \\- False positive prevention via Question Detection, Negative Pattern filtering (10 languages), Sentence-Level Scoring, and Confidence Thresholds\n* **ACT-R inspired Retrieval** \\- Context-aware memory injection based on current task, not blind retrieval\n\n# Signal vs Noise: The Real Difference\n\nMost memory plugins store **anything** that matches a keyword. \"Remember\" triggers storage. That's the problem.\n\nTrue-Mem understands **context and intent**:\n\n|You say...|Other plugins|True-Mem|Why|\n|:-|:-|:-|:-|\n|\"I remember when we fixed that bug\"|‚ùå Stores it|‚úÖ Skips it|You're recounting, not requesting storage|\n|\"Remind me how we did this\"|‚ùå Stores it|‚úÖ Skips it|You're asking AI to recall, not to store|\n|\"Do you remember this?\"|‚ùå Stores it|‚úÖ Skips it|It's a question, not a statement|\n|\"I prefer option 3\"|‚ùå Stores it|‚úÖ Skips it|List selection, not general preference|\n|\"Remember this: always run tests\"|‚úÖ Stores it|‚úÖ Stores it|Explicit imperative to store|\n\nAll filtering patterns work across **10 languages**: English, Italian, Spanish, French, German, Portuguese, Dutch, Polish, Turkish, and Russian.\n\nThe result: a clean memory database with **actual preferences and decisions**, not conversation noise.\n\n**Scope Behavior**:\n\nBy default, explicit intent memories are stored at **project scope** (only visible in the current project). To make them **global** (available in all projects), include a global scope keyword anywhere in your phrase:\n\n|Language|Global Scope Keywords|\n|:-|:-|\n|**English**|\"always\", \"everywhere\", \"for all projects\", \"in every project\", \"globally\"|\n|**Italian**|\"sempre\", \"ovunque\", \"per tutti i progetti\", \"in ogni progetto\", \"globalmente\"|\n|**Spanish**|\"siempre\", \"en todas partes\", \"para todos los proyectos\"|\n|**French**|\"toujours\", \"partout\", \"pour tous les projets\"|\n|**German**|\"immer\", \"√ºberall\", \"f√ºr alle projekte\"|\n|**Portuguese**|\"sempre\", \"em todos os projetos\"|\n\n# Why not just use Cloud Memory or an MCP?\n\nOther solutions like opencode-supermemory exist, but they take a different approach. True-Mem is **local-first** and **cognitive-first**. It doesn't just store text - it models how human memory actually works.\n\n# Key Features\n\n* 100% automatic - no commands, no tags, no manual calls\n* Smart noise filtering - understands context, not just keywords (10 languages)\n* Local-first - zero latency, full privacy, no subscription\n* Dual-scope memory (global + project-specific)\n* Non-blocking async extraction (no QUEUED states)\n* Multilingual support (15 languages)\n* Smart decay (only episodic memories fade)\n* Zero native dependencies (Bun + Node 22+)\n* Production-ready\n\n# Learn More\n\nGitHub: [https://github.com/rizal72/true-mem](https://github.com/rizal72/true-mem)\n\nFull documentation, installation instructions, and technical details available in the repo.\n\n*Inspired by* [*PsychMem*](https://github.com/muratg98/psychmem) *- big thanks for pioneering persistent psychology-grounded memory for OpenCode.*\n\nFeedback welcome!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rf8v3s/plugin_truemem_automatic_ai_memory_that_actually/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7i7mu1",
          "author": "landed-gentry-",
          "text": "... but does it improve output quality? Or does it introduce as many new problems due to irrelevant context?\n\nThese systems are not worth considering without some kind of data, IMO.",
          "score": 3,
          "created_utc": "2026-02-26 12:48:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7idgtl",
              "author": "rizal72",
              "text": "I built it for myself, exactly to address the issues you are saying. Maybe it is still an experiment, but it is the way to go for real memory management. It tries to filter whatever it can, gives precedency to what comes from the user prompt against text coming from the AI. It distinguishes between a real intent from a simple question, and many other things. Right now my db has just 12 memories, 4 are global scope, 8 are project related (the project is true-mem :D). So it does not bloat your prompt when injecting memories, and it does it stealthy.\n\n",
              "score": 1,
              "created_utc": "2026-02-26 13:24:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ismia",
                  "author": "Position_Emergency",
                  "text": "Find a benchmark you can test it with.  \nIt will help guide your development going forward and give us an idea if what you've made is actually useful.",
                  "score": 1,
                  "created_utc": "2026-02-26 14:45:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ozgpy",
          "author": "rizal72",
          "text": "u/Putrid-Pair-6194  \n**Recall:** When you send a message, the plugin searches your stored memories for matching keywords. It ranks them by similarity and injects only the top relevant ones into the prompt. Think of it as a smart search that runs automatically before every response.\n\n**Injection:** Memories are injected automatically into every prompt via a <true\\_memory\\_context> XML tag - no user action required. Only memories relevant to the current project and context are included. Core principle: minimal prompt bloat, zero token waste.\n\n**Relevance:** Two-stage filtering:\n\n1. Scope-based: Global memories available everywhere, project memories only in that project's worktree\n2. Similarity scoring: Jaccard compares query tokens vs memory content, returns top-k matches\n\n**Bonus:** Four-layer defense against false positives during extraction (question detection, negative patterns, multi-keyword validation, confidence threshold). Still refining to reduce noise (e.g., removing \"bugfix\" diaries that add little value).\n\n**EDIT:** Ah! In the last update I've also added a direct command (list-memories) that lists all the memories injected in the current prompt, grouped by GLOBAL and PROJECT scope. If you are unhappy of some memory you can always ask the AI assistant to delete it from the **true-mem** db and it will do it ;)  \nNext update will manage the \\[bugfix\\] category quite differently, maybe even deprecating it, I'm working on it right now...",
          "score": 2,
          "created_utc": "2026-02-27 12:59:42",
          "is_submitter": true,
          "replies": [
            {
              "id": "o7qiqqh",
              "author": "Putrid-Pair-6194",
              "text": "The transparency in list memories seems very helpful. Anything that can potentially pollute context behind the scenes warrants watching.",
              "score": 2,
              "created_utc": "2026-02-27 17:39:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7qjhd7",
              "author": "Putrid-Pair-6194",
              "text": "Based on your answer to ‚Äúrecall‚Äù above: Keyword search but not semantic search, true? Or is semantic implied when you say ‚Äúranks by similarity‚Äù? I guess it doesn‚Äôt matter a ton as long as it catches most applicable memories.\n\nI‚Äôm going to give it a try. Thanks for the repo.",
              "score": 2,
              "created_utc": "2026-02-27 17:43:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7slje1",
                  "author": "rizal72",
                  "text": "Good question. It's token-based similarity (Jaccard), not true semantic search with neural embeddings.\n\nHistory: Early versions of true-mem, used u/huggingface with a local model (all-MiniLM-L6-v2, \\~43MB) for true semantic embeddings. It worked but caused stability issues - OpenCode crashed on exit due to cleanup problems with the Transformers.js runtime.\n\nThe current Jaccard approach was a pragmatic swap: zero dependencies, zero native code, instant startup.\n\nHow it works:\n\n1. Tokenize query and memory summaries into word sets\n2. Calculate Jaccard = intersection / union\n3. Rank by similarity score, return top-k\n\nTrade-off: Catches exact word matches well, but won't find synonyms (e.g., \"error\" won't match \"exception\"). For a coding assistant context, this is usually sufficient - technical terms are fairly consistent, and the zero-dependency benefit outweighs the semantic gap.",
                  "score": 2,
                  "created_utc": "2026-02-28 00:00:24",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7lo4ok",
          "author": "Putrid-Pair-6194",
          "text": "OP, I‚Äôm interested. A few questions. How does recall work? How are the memories injected? How does the plugin determine relevance of memories to the current situation?",
          "score": 1,
          "created_utc": "2026-02-26 22:55:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p04kz",
              "author": "rizal72",
              "text": "I replied your questions in the main thread ;)",
              "score": 1,
              "created_utc": "2026-02-27 13:03:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7myeyc",
          "author": "cuba_guy",
          "text": "What did you use before? It sounds interesting but tbh I haven't had issues with bloated storage for a long time using multiple memory systems",
          "score": 1,
          "created_utc": "2026-02-27 03:17:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ozxiq",
              "author": "rizal72",
              "text": "check my reply to u/Putrid-Pair-6194 it should clarify my approach, exactly to avoid bloating storage, that s exactly the reason I wanted to develop this plugin for me: because the others I tried did what you say ;)",
              "score": 1,
              "created_utc": "2026-02-27 13:02:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qxfj6",
          "author": "reverse_macro",
          "text": "Feels like I should give it a try but too reluctant to do it w/o a benchmark.\n\nOP, what's the progress on that?",
          "score": 1,
          "created_utc": "2026-02-27 18:49:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7r4m9x",
              "author": "rizal72",
              "text": "Hi, the live benchmark is me using it in my everyday‚Äôs workflow. Right now I have 12 memories injected into true-mem project itself, and it‚Äôs very clean and not bloated at all. AI remembers relevant things and decisions and you always have the list-memories command for full transparency ;) I still use AGENTS both global and local for the workflow, the plugin is a companion to that. Give it a try,if you disable it from opencode.json it stops injecting so.. try it and check if it helps you ;)",
              "score": 1,
              "created_utc": "2026-02-27 19:23:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7uivkc",
                  "author": "reverse_macro",
                  "text": "https://preview.redd.it/gxx9auk717mg1.png?width=1498&format=png&auto=webp&s=5875b36a0caa05955639276abd4cc1bb84daedd5\n\nI personally don't prefer streaming the response in a language that I don't understand. If possible, prioritize fixing this. Thanks! ",
                  "score": 1,
                  "created_utc": "2026-02-28 08:10:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7oi526",
          "author": "xkn88",
          "text": "If you happen to use Claude Code , it already has it https://code.claude.com/docs/en/memory, read the ‚Äúautomatic memory‚Äù section",
          "score": 1,
          "created_utc": "2026-02-27 10:48:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p0d4o",
              "author": "rizal72",
              "text": "I use claude-code and I know about [memory.md](http://memory.md) , but it's very limited, still experimental, and does not use my psychological approach that makes the memory & forget thing work ;)",
              "score": 1,
              "created_utc": "2026-02-27 13:05:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}