{
  "metadata": {
    "last_updated": "2026-02-06 17:00:02",
    "time_filter": "week",
    "subreddit": "opencodeCLI",
    "total_items": 20,
    "total_comments": 205,
    "file_size_bytes": 202979
  },
  "items": [
    {
      "id": "1qrjgfn",
      "title": "Opencode v1.1.47 and auto updates",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/9l7wwnsu6kgg1.png",
      "author": "pi314ever",
      "created_utc": "2026-01-30 21:59:22",
      "score": 188,
      "num_comments": 25,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qrjgfn/opencode_v1147_and_auto_updates/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2qgk51",
          "author": "philosophical_lens",
          "text": "I think they should split into two releases - main and dev. Their current high velocity releases should stay on the dev branch, and they should also offer a main branch which lags behind by a week or so until itâ€™s confirmed stable.",
          "score": 26,
          "created_utc": "2026-01-31 04:12:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rgd4r",
              "author": "Michaeli_Starky",
              "text": "That's a no-brainer for anyone who had been doing high velocity software development. It puzzles me how it was not a thing for CC until like a month ago and not a thing for OC.",
              "score": 9,
              "created_utc": "2026-01-31 09:11:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2t7g0y",
                  "author": "Cast_Iron_Skillet",
                  "text": "I have enjoyed this on a few projects like cursor and comma ai sunnypilot. Nice to be able to see where things are headed, knowing risk of bugs, and to have peace of mind knowing you can revert to stable at any point.",
                  "score": 2,
                  "created_utc": "2026-01-31 16:23:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2pgo7m",
          "author": "MySkadi",
          "text": "I understand your feeling, i was a victim of 1.1.37 version bug where every tool call and subagent activities does cost me my copilot premium request, which reduce all of my 300 premium request at once, fortunately at least the objective is achieved, but at what cost..\n\nYou can turn off the autoupdate from global opencode.json config",
          "score": 21,
          "created_utc": "2026-01-31 00:36:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2widz4",
              "author": "throwaway12012024",
              "text": "where? my global opencode.json doenst have anything about autoupdate",
              "score": 1,
              "created_utc": "2026-02-01 02:35:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2xdef8",
              "author": "Remarkable_Week_2938",
              "text": "Is this issue fixed. I got the same and now my premium is refilled to 300 but dare not try to run copilot models again..",
              "score": 1,
              "created_utc": "2026-02-01 06:05:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2xe7qt",
                  "author": "MySkadi",
                  "text": "It is fixed now so you dont need to worry, i already tried it\n\nAs for the autoupdate see the config at https://opencode.ai/config.json",
                  "score": 1,
                  "created_utc": "2026-02-01 06:11:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ors93",
          "author": "Lyuseefur",
          "text": "![gif](giphy|iFnBpFxFGetn8BH0vZ)",
          "score": 8,
          "created_utc": "2026-01-30 22:22:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pqx38",
          "author": "Psidium",
          "text": "You shouldnâ€™t be running any ai coding tools barebones anyway. Create a sandbox and let it lose there. The models themselves can hallucinate dangerous commands, itâ€™s just inherent to the medium.",
          "score": 3,
          "created_utc": "2026-01-31 01:35:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2r5b7p",
              "author": "gbladeCL",
              "text": "Is there a recommended sandbox? I am looking at opencode-devcontainers",
              "score": 1,
              "created_utc": "2026-01-31 07:27:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ucuv9",
                  "author": "Psidium",
                  "text": "Iâ€™ve created one myself based on the Claude code devcontainer that anthropic provides on their docs",
                  "score": 2,
                  "created_utc": "2026-01-31 19:39:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2prrjd",
              "author": "pi314ever",
              "text": "While I agree with that and do sandboxing, the issue is that the vast majority of vulnerable users will probably not look that far into it. The people who don't know about the risks of auto updates are likely the same people who aren't aware of sandboxing as best practice.",
              "score": 0,
              "created_utc": "2026-01-31 01:40:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2oou60",
          "author": "Historical-Internal3",
          "text": "![gif](giphy|fT3OUK1DTJAI76ZF0i)",
          "score": 4,
          "created_utc": "2026-01-30 22:08:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2orr0x",
          "author": "Heavy-Focus-1964",
          "text": "most likely passed an empty string in to the release message generator because there were no commit hashes produced. harmless edge case. \n\nif this is enough to rattle your confidence maybe the breakneck speed and reckless abandon of AI programming is not for you",
          "score": 3,
          "created_utc": "2026-01-30 22:22:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p8ud5",
              "author": "carlanwray",
              "text": "Right? If it doesn't reseamble a seive, leaking everything everywhere it's too old school. ðŸ˜„",
              "score": 2,
              "created_utc": "2026-01-30 23:53:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2procs",
          "author": "mrpoopybruh",
          "text": "like just use it in a sandbox like ya supposed to!",
          "score": 1,
          "created_utc": "2026-01-31 01:39:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qtk0m",
              "author": "ProfessionNo3952",
              "text": "Could you tell please in which way?",
              "score": 1,
              "created_utc": "2026-01-31 05:46:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2r62p6",
                  "author": "RegrettableBiscuit",
                  "text": "Docker is a good option.Â ",
                  "score": -1,
                  "created_utc": "2026-01-31 07:34:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ufzgc",
          "author": "morglod",
          "text": "Imagine people in 2026 could not make simple chat with single peer without bugs",
          "score": 1,
          "created_utc": "2026-01-31 19:55:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p5qqu",
          "author": "alovoids",
          "text": "lol",
          "score": 1,
          "created_utc": "2026-01-30 23:36:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qb1fq",
          "author": "Ok_Road_8710",
          "text": "The default settings just let the agent rm rf your entire PC, so",
          "score": 0,
          "created_utc": "2026-01-31 03:36:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pcq5i",
          "author": "doodirock",
          "text": "Dude relax",
          "score": -3,
          "created_utc": "2026-01-31 00:15:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pm9hz",
          "author": "neamtuu",
          "text": "Clown. What are you afraid of? Check the files for yourself if you think of a security breach and come up with a conclusion. Stop assuming uncertain checkable realities.",
          "score": -11,
          "created_utc": "2026-01-31 01:08:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs9cpr",
      "title": "Which Model is the Most Intelligent From Here?",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/ue9m9fi74qgg1.png",
      "author": "Level-Dig-4807",
      "created_utc": "2026-01-31 18:01:54",
      "score": 104,
      "num_comments": 55,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qs9cpr/which_model_is_the_most_intelligent_from_here/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2ttpfa",
          "author": "SnooSketches1848",
          "text": "Kimiiiiiiiiiiiiiiii",
          "score": 32,
          "created_utc": "2026-01-31 18:09:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2u91ki",
              "author": "PsyGnome_FreeHuman",
              "text": "Kimi o big-pickle ?ðŸ«£",
              "score": 2,
              "created_utc": "2026-01-31 19:21:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2unb6l",
                  "author": "shikima",
                  "text": "big-pickle is GLM-4.6",
                  "score": 7,
                  "created_utc": "2026-01-31 20:31:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ebkea",
              "author": "Dry-Storm-5784",
              "text": "It's really fast",
              "score": 1,
              "created_utc": "2026-02-03 19:38:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tvg0d",
          "author": "annakhouri2150",
          "text": "Kimi K2.5 by far. It's the closest open model to Opus 4.5, and the only large, capable coding and agentic model that has vision:\n\nhttps://www.kimi.com/blog/kimi-k2-5.html",
          "score": 28,
          "created_utc": "2026-01-31 18:17:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u9i5r",
          "author": "noctrex",
          "text": "Kimi > GLM > MiniMax",
          "score": 24,
          "created_utc": "2026-01-31 19:24:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2uad26",
              "author": "PsyGnome_FreeHuman",
              "text": "And where is Big Pickle?",
              "score": 1,
              "created_utc": "2026-01-31 19:28:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ualrv",
                  "author": "noctrex",
                  "text": "That is essentially the previous GLM 4.6 model, so behind them",
                  "score": 9,
                  "created_utc": "2026-01-31 19:29:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ulyps",
          "author": "Orlandocollins",
          "text": "As an elixir developer I have had better success with MiniMax than GLM, though GLM isn't terrible by any means. I only run locally so I haven't had a chance to run Kimi as it is VERY large.",
          "score": 7,
          "created_utc": "2026-01-31 20:24:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tsirs",
          "author": "rusl1",
          "text": "I usually do GLM for planning and debugging, MiniMax sub agents for everything else. \n\nKimi looks good but I didn't test it extensively",
          "score": 7,
          "created_utc": "2026-01-31 18:04:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30ieuv",
              "author": "Impossible_Comment49",
              "text": "Youâ€™ll be surprised; itâ€™s much better.",
              "score": 2,
              "created_utc": "2026-02-01 18:29:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o30oxvj",
                  "author": "rusl1",
                  "text": "At coding or planning?",
                  "score": 1,
                  "created_utc": "2026-02-01 18:58:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ki4tz",
                  "author": "JuiceBoxJonny",
                  "text": "I have a thread ripper 12 core 4-5ghz cpu, with 128gb of ram, and like a 12gb graphics card Intel arc 1300 watt PSU. Without roasting my setup, what do u think I could run on this pos with airllm and some LoRa optimizations?\n\nMy end game is doubling/trippling the context window of kimi â€”-",
                  "score": 1,
                  "created_utc": "2026-02-04 17:57:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2tui35",
          "author": "RegrettableBiscuit",
          "text": "K2.5 is most likely the best, but I guess we're not sure if these are quantized models.Â ",
          "score": 13,
          "created_utc": "2026-01-31 18:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ua07z",
              "author": "noctrex",
              "text": "It is natively trained as INT4, so even if its 1T parameters, its 595 GB in size",
              "score": 3,
              "created_utc": "2026-01-31 19:26:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o30hsnp",
                  "author": "Impossible_Comment49",
                  "text": "Would you rather quant k2.5 to fit on 512gb ram or just use glm4.7 in fp8 or q6?",
                  "score": 1,
                  "created_utc": "2026-02-01 18:26:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2tv7m1",
          "author": "silurosound",
          "text": "I've been testing both GLM and Kimi these past few days thru paid API and my first impressions are that Kimi is snappier and smarter but burns tokens faster than GLM, which is solid too and didn't burn through tokens as quickly.",
          "score": 6,
          "created_utc": "2026-01-31 18:16:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wgtc3",
              "author": "DistinctWay9169",
              "text": "Kimi is HUNGRY. Might be better than GLM but not so much that would make sense paying much mor for Kimi",
              "score": 3,
              "created_utc": "2026-02-01 02:26:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2zpa5u",
              "author": "neamtuu",
              "text": "What? in no way does kimi k2.5 thinking max burn more tokens than glm 4.7 haha\n\nhttps://preview.redd.it/ne5nleakrwgg1.png?width=1829&format=png&auto=webp&s=157da48df2a016ad7d7d5f0eae51f1b87cdfb917\n\ni",
              "score": 1,
              "created_utc": "2026-02-01 16:17:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2zuobs",
              "author": "aimericg",
              "text": "I find GLM practically unusable on my side, mostly because its quite slow and easily hallucinates on my coding projects.",
              "score": 1,
              "created_utc": "2026-02-01 16:41:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2umnex",
          "author": "martinffx",
          "text": "I tried the kimi models again and they are still terrible at tool calling. At least the opencode zen one, constant errors calling tools. Straight up just throws some sort of reasoning error when in planning mode. So it may be better but Iâ€™ve not found it to be more usable at least with the opencode harness",
          "score": 3,
          "created_utc": "2026-01-31 20:28:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31oybk",
              "author": "NewEraFresh",
              "text": "Yup for example it struggles to even use playwright mcp correctly with tool calls. GLM handles it like a boss. Kimi does surprise me on the quality on certain tasks. Overall though itâ€™s looking like GLM is still way more usable as a backup plan for when you hit those limits on Claude Opus 4.5 or GPT 5.2 high.",
              "score": 1,
              "created_utc": "2026-02-01 21:51:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2uwzji",
          "author": "Repulsive_Educator61",
          "text": "Also, off topic but, opencode docs mention that all these models train on your data during the \"FREE\" period (only the free models)",
          "score": 5,
          "created_utc": "2026-01-31 21:19:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2v1d82",
              "author": "touristtam",
              "text": "Well good luck with the shitty code that being produced on my end. :D",
              "score": 8,
              "created_utc": "2026-01-31 21:40:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2twsbw",
          "author": "Flat_Cheetah_1567",
          "text": "From their site\nhttps://share.google/Fd6nPfo1PF4HNnLNo\nJust check the links and apply with your student account and also you have gemini options for free with student account",
          "score": 2,
          "created_utc": "2026-01-31 18:23:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2u29ef",
              "author": "atiqrahmanx",
              "text": "ChatGPT free for how many months?",
              "score": 1,
              "created_utc": "2026-01-31 18:49:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tysh7",
          "author": "SlopTopZ",
          "text": "Kimi 100%",
          "score": 2,
          "created_utc": "2026-01-31 18:33:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2urb5m",
          "author": "Michaeli_Starky",
          "text": "GPT 4.5 xhigh",
          "score": 2,
          "created_utc": "2026-01-31 20:51:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2uqetm",
          "author": "LittleChallenge8717",
          "text": "Kimi",
          "score": 1,
          "created_utc": "2026-01-31 20:46:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2whhvc",
          "author": "aeroumbria",
          "text": "Does anyone know if there is an official way to specify which variant of the model an agent / subagent will use? I only saw some unmerged pull requests when I search it up. Right now Kimi is a bit limited because it only runs the no reasoning variant in subagents, and it really does not like to plan or reason in \"white\" outputs.",
          "score": 1,
          "created_utc": "2026-02-01 02:30:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2wprvs",
          "author": "lucaasnp",
          "text": "Iâ€™ve been using Kimi and it is pretty good",
          "score": 1,
          "created_utc": "2026-02-01 03:20:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xbie7",
          "author": "Independent_Ad627",
          "text": "Kimi is great because it's in par with GLM but faster, and I use GLM pro plan, not the free one from opencode.\nNowadays I use the OPENCODE_EXPERIMENTAL_PLAN_MODE=1, and both models work consistently the same IMO. So I didn't see any much difference other than the token per second",
          "score": 1,
          "created_utc": "2026-02-01 05:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ycgz7",
          "author": "Careless-Plankton630",
          "text": "Kimi K2.5 is so good. Like it is insanely good",
          "score": 1,
          "created_utc": "2026-02-01 11:21:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3i39tw",
              "author": "Known_Philosophy3337",
              "text": "Compare to what?",
              "score": 1,
              "created_utc": "2026-02-04 09:24:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2yd73v",
          "author": "debba_",
          "text": "I am using Kimi and itâ€™s very good",
          "score": 1,
          "created_utc": "2026-02-01 11:27:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yekjo",
          "author": "Flashy_Reality8406",
          "text": "IMO, Kimi > Minimax M2 > GLM",
          "score": 1,
          "created_utc": "2026-02-01 11:39:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yg6fc",
          "author": "ManWhatCanIsay_K",
          "text": "actually i prefer minimax",
          "score": 1,
          "created_utc": "2026-02-01 11:53:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2z616i",
          "author": "stevilg",
          "text": "https://arena.ai/leaderboard/code",
          "score": 1,
          "created_utc": "2026-02-01 14:42:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2zv0kv",
          "author": "aimericg",
          "text": "Anyone tried Trinity Large a bit more extensively? Also what happened to Big Pickle?",
          "score": 1,
          "created_utc": "2026-02-01 16:43:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31sdt8",
          "author": "fergthh",
          "text": "Studying... is unlimited",
          "score": 1,
          "created_utc": "2026-02-01 22:08:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34s4un",
          "author": "sasha-zelts",
          "text": "I would say Kiki 2.5",
          "score": 1,
          "created_utc": "2026-02-02 10:18:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gifny",
          "author": "Connect_Win_7282",
          "text": "https://livebench.ai/#/?sort=Agentic+Coding+Average \nfind yourself an answer here",
          "score": 1,
          "created_utc": "2026-02-04 02:21:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3h9lpe",
          "author": "Independent_Seat4294",
          "text": "kimi",
          "score": 1,
          "created_utc": "2026-02-04 05:09:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hlmwj",
          "author": "valepiskiii",
          "text": "definetly kimi k2.5 from my experience",
          "score": 1,
          "created_utc": "2026-02-04 06:43:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qvh05",
          "author": "salary_pending",
          "text": "why are these models free?",
          "score": 1,
          "created_utc": "2026-02-05 17:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3uu8eo",
          "author": "Aromatic-Trust5494",
          "text": "kimi is really fast and performs sonnet levels sometimes",
          "score": 1,
          "created_utc": "2026-02-06 05:59:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tvtv4",
          "author": "Flat_Cheetah_1567",
          "text": "If you're student get the open ai free year with codex and done",
          "score": 1,
          "created_utc": "2026-01-31 18:19:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2tw3hr",
              "author": "Level-Dig-4807",
              "text": "really? from where?",
              "score": 4,
              "created_utc": "2026-01-31 18:20:40",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2u1no6",
              "author": "AkiDenim",
              "text": "Is this still a thing?",
              "score": 2,
              "created_utc": "2026-01-31 18:46:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2zvlnl",
              "author": "aimericg",
              "text": "ChatGPT Codex models don't hallucinate as much as some of these models but honestly don't find their output quite good. It always feels quite off in the UI and I am having issues with it when trying to fix more architecture level problems. It just doesnt seem to be able to handle that.",
              "score": 1,
              "created_utc": "2026-02-01 16:46:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qtqx2p",
      "title": "OpenCode Bar 2.1: Now with CLI + Per-Provider Subscription Tracking",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/sxdtdls302hg1.png",
      "author": "kargnas2",
      "created_utc": "2026-02-02 09:54:58",
      "score": 76,
      "num_comments": 16,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qtqx2p/opencode_bar_21_now_with_cli_perprovider/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o35kmbo",
          "author": "Possible-Text8643",
          "text": "mac only?",
          "score": 5,
          "created_utc": "2026-02-02 13:49:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38ikf2",
              "author": "trenescese",
              "text": "I think you'd need something like https://yasb.dev/ to run a widget like that on windows?",
              "score": 1,
              "created_utc": "2026-02-02 22:12:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37banq",
          "author": "Specialist-Yard3699",
          "text": "Looks nice. No Linux in plans?",
          "score": 2,
          "created_utc": "2026-02-02 18:49:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dbqhl",
              "author": "buggytheking",
              "text": "I've made something similar on Linux. Check it out here and lemme know what else to add.https://github.com/OmegAshEnr01n/GnomeCodexBar",
              "score": 3,
              "created_utc": "2026-02-03 16:54:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ddfr6",
                  "author": "Specialist-Yard3699",
                  "text": "Will test today. Any plans for opencode-zen/hype-chinese providers(zai/minimax/kimi)?",
                  "score": 1,
                  "created_utc": "2026-02-03 17:02:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34t6uv",
          "author": "jellydn",
          "text": "Nice. Thanks, I will give it a try soon :)",
          "score": 1,
          "created_utc": "2026-02-02 10:28:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34xfkk",
          "author": "Financial_Reward2512",
          "text": "Any similar product where we can use Claude Code with multiple Provider and show this bar as well.?",
          "score": 1,
          "created_utc": "2026-02-02 11:06:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35f0ob",
              "author": "United_Bandicoot1696",
              "text": "Quotio",
              "score": 1,
              "created_utc": "2026-02-02 13:17:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o36ptdn",
          "author": "_w_8",
          "text": "neat!! how are you using claude code sub in opencode still though?",
          "score": 1,
          "created_utc": "2026-02-02 17:11:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37hzpv",
          "author": "0sko59fds24",
          "text": "Anthropic still bans users using CC in opencode right",
          "score": 1,
          "created_utc": "2026-02-02 19:19:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bw028",
          "author": "VlaadislavKr",
          "text": "How to connect gemini quota based?",
          "score": 1,
          "created_utc": "2026-02-03 12:20:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3c0gnu",
          "author": "renan_william",
          "text": "Works on Intel Mac or just Silicon? ",
          "score": 1,
          "created_utc": "2026-02-03 12:51:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3grey4",
          "author": "Powerful_Associate85",
          "text": "Iâ€™m using Ollama on OpenCode, but thereâ€™s no support?",
          "score": 1,
          "created_utc": "2026-02-04 03:12:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3grint",
              "author": "kargnas2",
              "text": "What do you want to track of Ollama?",
              "score": 1,
              "created_utc": "2026-02-04 03:12:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3gs8zh",
                  "author": "Powerful_Associate85",
                  "text": "I mean Ollama cloud plan, for multiple models",
                  "score": 1,
                  "created_utc": "2026-02-04 03:17:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3hj1mu",
          "author": "touristtam",
          "text": "Side note: I like the review bot you are using. Any more info on that?",
          "score": 1,
          "created_utc": "2026-02-04 06:21:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu44yh",
      "title": "Notes after using Claude Code and OpenCode side by side",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qu44yh/notes_after_using_claude_code_and_opencode_side/",
      "author": "Arindam_200",
      "created_utc": "2026-02-02 19:00:16",
      "score": 69,
      "num_comments": 39,
      "upvote_ratio": 0.92,
      "text": "Iâ€™ve been using Claude Code pretty heavily for day-to-day work. Itâ€™s honestly one of the first coding agents Iâ€™ve trusted enough for real production tasks.\n\nThat said, once you start using it *a lot*, some tradeoffs show up.\n\nCost becomes noticeable. Model choice matters more than you expect. And because itâ€™s a managed tool, you donâ€™t really get to see or change how the agent works under the hood. You mostly adapt your workflow to it.\n\nOut of curiosity, I started testing OpenCode (Got Hyped up from X & reddit TBH). Didnâ€™t realize how big it had gotten until recently. The vibe is very different.\n\nClaude Code feels guarded and structured. It plans carefully, asks before doing risky stuff, and generally prioritizes safety and predictability.\n\nOpenCode feels more like raw infrastructure. You pick the model per task. It runs commands, edits files, and you validate by actually running the code. More control, less hand-holding.\n\nBoth got the job done when I tried real tasks (multi-file refactors, debugging from logs). Neither â€œfailed.â€ The difference was *how* they worked, not whether they could.\n\nIf you want something managed and predictable, Claude Code is great. If you care about flexibility, cost visibility, and owning the workflow, OpenCode is interesting.\n\nI wrote up a longer comparison [here](https://www.tensorlake.ai/blog/opencode-the-best-claude-code-alternative) if anyone wants the details.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qu44yh/notes_after_using_claude_code_and_opencode_side/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o38ob1f",
          "author": "Guinness",
          "text": "CC has horrible TUI issues, bad framerates, and oh god the bug where it constantly jumps to the top of the history scrollback.\n\nCC introduced me to all of these tools after years of development with nano/vi{m}, but it was so buggy I branched out to cline, and eventually to opencode.\n\nOpencode is clearly the winner right now. It feels like CC was written by an LLM. Like....it works? But its buggy as fuck.",
          "score": 13,
          "created_utc": "2026-02-02 22:41:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bdv87",
              "author": "ReporterCalm6238",
              "text": "I mean opencode was also written by LLMs",
              "score": 4,
              "created_utc": "2026-02-03 09:45:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fs2jr",
                  "author": "priestoferis",
                  "text": "The difference is that CC engineers clearly had no idea about TUI. There was an interview couple of months back where some lead said they didn't know if this could be made in a TUI. Like what is it you can't make in a TUI? I wouldn't pick a TUI for video rendering, but you _can_ if you want to.\n\nTbh, they almost couldn't considering how they try doing rendering :D",
                  "score": 2,
                  "created_utc": "2026-02-03 23:54:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3afs82",
              "author": "Accomplished-Toe7014",
              "text": "Well Anthropic was seemingly proud when they said that 100% of their code is written by AIs",
              "score": 3,
              "created_utc": "2026-02-03 04:47:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o38rmsv",
              "author": "Positive-Badger6588",
              "text": "the TUI has issues but ive kept tryng to go back to opencode just to find out the performance and harness is just much better optimized on cc. I have it hooked up to a nvim wrapper so all the diffs show up in nvim for me. the TUI outside of general explanation just becomes a bit irrelavant for me.",
              "score": 1,
              "created_utc": "2026-02-02 22:58:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37ijy1",
          "author": "hey_ulrich",
          "text": "The only feature from CC that I miss is that CC can run bash commands in the background and easily check its logs. That's it. For everything else, OpenCode is superior. Once you set your custom modes, and add plugins, it's awesome.",
          "score": 20,
          "created_utc": "2026-02-02 19:22:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37wb6o",
              "author": "nmiljkovic89",
              "text": "There is an opencode pty plugin for that",
              "score": 8,
              "created_utc": "2026-02-02 20:27:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37zkdh",
                  "author": "hey_ulrich",
                  "text": "Thanks, I'll check it out",
                  "score": 1,
                  "created_utc": "2026-02-02 20:42:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o393m7x",
                  "author": "MyriadAsura",
                  "text": "Care to share a link brother?",
                  "score": 1,
                  "created_utc": "2026-02-03 00:03:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3l9fzf",
              "author": "mdrahiem",
              "text": "Also being able to type multi line in CC ðŸ˜…",
              "score": 0,
              "created_utc": "2026-02-04 20:01:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o38qwbj",
          "author": "poop_harder_please",
          "text": "is anyone still using the oauth authentication method? Anthropic sort of backpedaled on banning accounts for using oauth with external providers, but I'm worried about taking the risk on my account until it's a sure thing",
          "score": 6,
          "created_utc": "2026-02-02 22:54:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3aq3sp",
              "author": "Fickle_Permi",
              "text": "Yeah I use it with no problem. Iâ€™m not a heavy user though. I maybe hit the daily limit once a week.",
              "score": 2,
              "created_utc": "2026-02-03 06:05:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o38skkd",
              "author": "Positive-Badger6588",
              "text": "wait, are they blocking access or banning people form if using it? blocking users sounds kind of retarded lol",
              "score": 1,
              "created_utc": "2026-02-02 23:03:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3capjw",
                  "author": "poop_harder_please",
                  "text": "Apparently full blocks. But they mightâ€™ve lightened up after the backlash and the counter positioning of OAIâ€™s plans being third-party-harness-friendly",
                  "score": 1,
                  "created_utc": "2026-02-03 13:51:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o38sogz",
              "author": "FunnyRocker",
              "text": "I'd really like to know this also",
              "score": 1,
              "created_utc": "2026-02-02 23:04:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dw774",
              "author": "james__jam",
              "text": "Was there any official announcement that indicated backpedaling? Or is it more of an observation that theyâ€™re not actively banning people now?",
              "score": 1,
              "created_utc": "2026-02-03 18:27:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3f38aa",
              "author": "Keep-Darwin-Going",
              "text": "When did they back pedal? I see people complaining about getting banned regularly",
              "score": 1,
              "created_utc": "2026-02-03 21:47:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fqyoy",
                  "author": "poop_harder_please",
                  "text": "[https://x.com/trq212/status/2009689816468992334?s=20](https://x.com/trq212/status/2009689816468992334?s=20)",
                  "score": 1,
                  "created_utc": "2026-02-03 23:48:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3kfoxz",
              "author": "IntrepidLawfulness42",
              "text": "Yep, I'm using it daily, no issues the last two weeks. Pro account, hitting the 5 hour session limits regularly.",
              "score": 1,
              "created_utc": "2026-02-04 17:46:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o38zizc",
          "author": "ianxiao",
          "text": "The only i miss when moving from CC is /rewind . Opencode has something less powerful /undo but itâ€™s tedious to use and buggy.",
          "score": 3,
          "created_utc": "2026-02-02 23:41:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3au8fx",
              "author": "aeroumbria",
              "text": "I hate it that a lot of their functions are locked to CLI and not available in the VSCode extention, but their CLI glitches out like crazy in IDE terminals, and I dislike infinite scrolling CLI in IDEs.",
              "score": 1,
              "created_utc": "2026-02-03 06:40:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37nviq",
          "author": "ellensen",
          "text": "I have connected my subscriptions to opencode, seems to give me the same control over cost as if using the subscription by the provider directly without opencode?",
          "score": 2,
          "created_utc": "2026-02-02 19:47:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3aovzm",
          "author": "cafesamp",
          "text": "I know thereâ€™s mixed opinions on how opinionated your workflow should be with these tools, but Iâ€™ve found Superpowers w/ OpenCode (you can use it with Claude Code too, but itâ€™s more redundant there) to be actually really awesome in making coding with OpenCode a more structured experience.\n\nThe main thing OpenCode can really fail on is picking back up on things if you need to run out of usage and/or need to switch models/providers.  OpenCodeâ€™s flow is already so rigid (with task management being handled in the conversation), and Superpowers expects the full lifecycle of its skills to complete a task, and doesnâ€™t wrap up things if you get interrupted.\n\nThat being said, I like using Superpowers in CC with Opus for brainstorming and planning, and then having it write to files that I can pick up and continue to work with in OpenCode with Codex for implementation/testing/review.\n\nPerfect?  No, but great mileage out of two $20/mo subscriptions.",
          "score": 2,
          "created_utc": "2026-02-03 05:55:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oms90",
              "author": "Top-Chain001",
              "text": "interesting, I too like super powers because it really puts you in the driver seat asking you confirmations on the data flow diagrams is this what you're talking about etc I would love to hear your comparison with codex or GPt 5.2 directly versus using something like superpowers and also your opinion on if you tried GSd",
              "score": 1,
              "created_utc": "2026-02-05 08:16:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3bjmsu",
          "author": "Tushar_BitYantriki",
          "text": "The only thing lagging in OpenCode is the lack of hooks in any language, like Claude code supports.\n\nWhat's the point of binding it to JS?\n\nI had posted a migration guide and a skill on this sub to move from Claude Code to OpenCode. But I am yet to find a clean way to migrate my Python+shell hooks from Claude code.\n\nFor me, hooks are a crucial part of my workflow, and I have collected a lot over the months, going from simple grep and regex matches in a shell script to AST-based DDD-enforcing via Python's tree-sitter.\n\nIt seems that OpenCode was made by the JS folks, for the JS folks.",
          "score": 2,
          "created_utc": "2026-02-03 10:39:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o392wv3",
          "author": "Western_Objective209",
          "text": "what can opencode actually do (besides use different models) that claude code can't, like what actually makes it more flexible? Claude code supports plugins, MCPs, and skills, in your extended write up:\n\n> Extensibility: [Claude Code] Managed core with limited extension of agent internals, \t[OpenCode] Open source, extensible with internal tools\n\nLike you don't even seem to understand how to extend claude code and \"open source\" is not an extension model",
          "score": 4,
          "created_utc": "2026-02-03 00:00:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3avs9f",
          "author": "Holiday_Degree_7721",
          "text": "opencode have critical issues with memory leaking, thats why I moved to cc",
          "score": 1,
          "created_utc": "2026-02-03 06:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bf865",
          "author": "raydou",
          "text": "for me the only thing i'm missing is the integration of CC rules files in OpenCode. I like the way it's surgical and don't load the context so much. \nI made a PR for this but it seems that OpenCode team only check the PRs of their buddies. No review nothing on mine since 2 weeks.. \nAt this rythm PR won't be mergeable and I would have to reedit all the changes ..",
          "score": 1,
          "created_utc": "2026-02-03 09:58:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3cfsb0",
          "author": "SpecKitty",
          "text": "I like running both - one implements, the other reviews. I manage with Spec Kitty.",
          "score": 1,
          "created_utc": "2026-02-03 14:19:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dc97r",
          "author": "buggytheking",
          "text": "Loved this",
          "score": 1,
          "created_utc": "2026-02-03 16:56:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hngsj",
          "author": "illusionst",
          "text": "There have been multiple times when Iâ€™ve given the same job to both and Claude failed.\n- Claude fails at maintaining context or sometimes not reading important stuff from claude.md\n- Asked claude to configure a MCP server, it had no idea what MCPâ€™s are, it searched the web, got confused and gave me instructions on how to do it.\n\n- Opencode remembered my instructions and did not hallucinate \n- Opencode searched the specific page and configured the MCP server in less than a minute. \n\nClaude Code comes with a lot of safety guard rails which make it less effective than OpenCode. \n\nIâ€™ve completely moved on to opencode.",
          "score": 1,
          "created_utc": "2026-02-04 06:59:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hse7c",
          "author": "dd768110",
          "text": "If I use a CC, will my account be banned?",
          "score": 1,
          "created_utc": "2026-02-04 07:42:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iwnkj",
          "author": "Comprehensive-Age155",
          "text": "Iâ€™m using OpenCode as backbone of my Saas product. Its architecture and extensibility is unbeatable.",
          "score": 1,
          "created_utc": "2026-02-04 13:17:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qi6iw",
          "author": "FriendAgile5706",
          "text": "Unless im doing something wrong I much prefer planmode on claude code vs opencode",
          "score": 1,
          "created_utc": "2026-02-05 16:00:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37tu8w",
          "author": "vixalien",
          "text": "I feel like OpenCode is much more ambitious than CC, not in a good way. For example, when you ask it why it did something in a certain way, instead of explaining why, it will just undo the change.\n\nOpenCode also seems to use dangerous commands more often, especially with git. It commits everything, and when you ask it to revert, it will happily git reset everything, including any uncommitted changes YOU (not OpenCode) had made.",
          "score": 1,
          "created_utc": "2026-02-02 20:15:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3eqwcg",
              "author": "gsxdsm",
              "text": "Opencode isnâ€™t making decisions. The models are",
              "score": 3,
              "created_utc": "2026-02-03 20:50:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o389fir",
              "author": "Arindam_200",
              "text": "Interesting\n\nI haven't personally faced this problem. But I'll give this a try",
              "score": 1,
              "created_utc": "2026-02-02 21:29:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qvo002",
      "title": "Kilo CLI 1.0 just launched - built on OpenCode as its open-source foundation",
      "subreddit": "opencodeCLI",
      "url": "https://blog.kilo.ai/p/kilo-cli",
      "author": "alokin_09",
      "created_utc": "2026-02-04 13:03:23",
      "score": 56,
      "num_comments": 21,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qvo002/kilo_cli_10_just_launched_built_on_opencode_as/",
      "domain": "blog.kilo.ai",
      "is_self": false,
      "comments": [
        {
          "id": "o3izcov",
          "author": "StephenAfamO",
          "text": "I'm curious, what's the difference between this and using opencode directly?",
          "score": 31,
          "created_utc": "2026-02-04 13:32:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3j271i",
              "author": "abeecrombie",
              "text": "Looks like it integrates back with the other kilo apps on your desktop \n\nAnd maybe they added some agents. I did find previously that kilo worked with many other open models. \n\nBut curious to hear the answer as well.",
              "score": 6,
              "created_utc": "2026-02-04 13:48:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3l9fcy",
                  "author": "touristtam",
                  "text": "> Looks like it integrates back with the other kilo apps on your desktop\n\nThat's a bit heavy, isn't it?",
                  "score": 2,
                  "created_utc": "2026-02-04 20:01:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jyk24",
              "author": "Coldshalamov",
              "text": "Orchestration, compaction. \n\nI really love the idea of kilo code but tbh when I run it with my GLM key it makes weird mistakes, makes the same code in triplicate sometimes, and Iâ€™d never let it touch an important codebase because itâ€™s always broken.\n\nIâ€™ll use it to bootstrap a project and get it off the ground because itâ€™ll run a long time, maybe itâ€™s better with a smarter model, but I really wish it had better handoff between modes. I think maybe better task handling or logging would help, it seems like itâ€™ll lose really important details across modes and GLM isnâ€™t smart enough not to do the same thing over and over without checking.",
              "score": 2,
              "created_utc": "2026-02-04 16:27:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3j0prc",
          "author": "Chrisnba24",
          "text": "Nice rebrand of opencode, it took a lot time to change the naming it seems",
          "score": 24,
          "created_utc": "2026-02-04 13:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3j1k2w",
              "author": "branik_10",
              "text": "lmao",
              "score": 7,
              "created_utc": "2026-02-04 13:44:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jnnvc",
          "author": "landed-gentry-",
          "text": "What value does the \"Kilo platform\" add?",
          "score": 9,
          "created_utc": "2026-02-04 15:36:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3p3081",
              "author": "TestTxt",
              "text": "It adds to their company valuation",
              "score": 2,
              "created_utc": "2026-02-05 10:51:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jb43d",
          "author": "Aggressive-Habit-698",
          "text": "I don't get it. Why rebrand OC instead of a oc plugin or create easily a CLI on your on like octo?\n\nKilo love copy ðŸ˜º ?",
          "score": 7,
          "created_utc": "2026-02-04 14:35:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jo4gv",
              "author": "WatchMySixWillYa",
              "text": "Exactly. It would be a lot more beneficial to contribute to their codebase and add support using some plugin. Fragmentation, on the other hand, can cause them a lot of headaches in the long run.",
              "score": 6,
              "created_utc": "2026-02-04 15:39:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ou6lw",
              "author": "bludgeonerV",
              "text": "Makes for better ads for them to spam across reddit.",
              "score": 2,
              "created_utc": "2026-02-05 09:28:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jb3gy",
          "author": "Charming_Support726",
          "text": "What is it good for? I already wondered when I saw Kilo aggressively do their ads when starting as copy-of-cline.",
          "score": 5,
          "created_utc": "2026-02-04 14:35:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j07rs",
          "author": "ReasonableReindeer24",
          "text": "No thinking for model on kilo ðŸ˜ž, kilo need add this on their cli",
          "score": 1,
          "created_utc": "2026-02-04 13:37:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ja726",
          "author": "jackai7",
          "text": "Does it have all the agents like orchestor/code/debug/ask as they were in the extension??",
          "score": 1,
          "created_utc": "2026-02-04 14:30:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jtu9y",
          "author": "trypnosis",
          "text": "Never heard of Kilo so I went to check them out. \n\nSeems like some kind of model provider. With plug in",
          "score": 1,
          "created_utc": "2026-02-04 16:05:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kyxny",
          "author": "jkz88",
          "text": "Have they fixed opencode freezing up after a few prompts? It looks so good but I could never get it to work once it started to make a few edits. Or maybe it's not happening for everyone?",
          "score": 1,
          "created_utc": "2026-02-04 19:12:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n0hg6",
              "author": "toadi",
              "text": "I update each time to the newest version. I also use it daily. Never happened to me.",
              "score": 1,
              "created_utc": "2026-02-05 01:27:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3mddxg",
          "author": "atkr",
          "text": "weak and feeble, like",
          "score": 1,
          "created_utc": "2026-02-04 23:18:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mdnb5",
          "author": "atkr",
          "text": "marketing is cringy AF, just going over how good opencode is without giving them enough credit",
          "score": 1,
          "created_utc": "2026-02-04 23:20:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mswqb",
          "author": "Demien19",
          "text": "Clone-slop, now every company will release own clone and confuse people even more",
          "score": 1,
          "created_utc": "2026-02-05 00:44:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nguua",
          "author": "bobthearsonist",
          "text": "sweet! cline+opencode=kilo. did you keep the mobile server?",
          "score": 0,
          "created_utc": "2026-02-05 03:00:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwiy50",
      "title": "My mobile setup",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/3wgo2hmdwnhg1.jpeg",
      "author": "tamtaradam",
      "created_utc": "2026-02-05 11:34:45",
      "score": 56,
      "num_comments": 15,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qwiy50/my_mobile_setup/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3q2uc4",
          "author": "redoubledit",
          "text": "If you want an alternative to Termius, look at [Termix](https://github.com/Termix-SSH/Termix). If you run a VPS already, itâ€™s easy to set Termix up so you get the Termius stuff but open source.",
          "score": 5,
          "created_utc": "2026-02-05 14:46:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qevy4",
              "author": "tamtaradam",
              "text": "hostinger vps already serves web console access, but I really like termius nativeness, especially when using external keyboard",
              "score": 1,
              "created_utc": "2026-02-05 15:45:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3pb1tv",
          "author": "PersonalityOne2559",
          "text": "Why not use a laptop atp?",
          "score": 4,
          "created_utc": "2026-02-05 11:59:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pbs8r",
              "author": "ahmetegesel",
              "text": "because",
              "score": 5,
              "created_utc": "2026-02-05 12:04:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3pf2fs",
                  "author": "Michaeli_Starky",
                  "text": "Because",
                  "score": 3,
                  "created_utc": "2026-02-05 12:28:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3pgyrc",
              "author": "tamtaradam",
              "text": "I'd say multiple reasons:\n\n\\- laptops are so 2025 ;)\n\n\\- I take ipad on my private trips anyway\n\n\\- I don't want to use company laptop for private stuff",
              "score": 4,
              "created_utc": "2026-02-05 12:41:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3pylo6",
                  "author": "bigh-aus",
                  "text": "I totally get that. If I was to travel with a laptop, I'd have my work laptop, personal laptop, iPad, work phone, personal phone. \n\n That said, my personal laptop is a Lenovo X1 Nano because it's so light.",
                  "score": 0,
                  "created_utc": "2026-02-05 14:24:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3p91vr",
          "author": "_d1re",
          "text": "Can you tell us how to do this?",
          "score": 1,
          "created_utc": "2026-02-05 11:43:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3p9gx2",
              "author": "tamtaradam",
              "text": "similar setup is covered in this video [https://www.youtube.com/watch?v=FEDiAHzS0zw](https://www.youtube.com/watch?v=FEDiAHzS0zw)",
              "score": 2,
              "created_utc": "2026-02-05 11:46:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3pnerx",
          "author": "Few-Mycologist-8192",
          "text": "nice set up , i have the same keybbord K380 logi; right?",
          "score": 1,
          "created_utc": "2026-02-05 13:21:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q19v3",
          "author": "sdexca",
          "text": "Noice, I am going to build a similar setup soon, iPhone 16 PM + netcup VPS 1000 G12 + termius + opencode + antigravity auth plugin / openai chagpt auth + opus/codex.",
          "score": 1,
          "created_utc": "2026-02-05 14:38:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3r7uv4",
          "author": "asmkgb",
          "text": "wow that's amazing to see, i just bought this exact keyboard too hahaha\n\ni love that we are now able to do great stuff on the go with just internet",
          "score": 1,
          "created_utc": "2026-02-05 17:59:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3wg08r",
          "author": "Aerion23",
          "text": "I have something similar, but i am missing the ability to quickly check the codebase like I can do with an code editor. Is there not already a opencode mobile app in the works?",
          "score": 1,
          "created_utc": "2026-02-06 13:48:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3wtgnr",
          "author": "Recent-Success-1520",
          "text": "Ditch the keyboard too with CodeNomad https://github.com/NeuralNomadsAI/CodeNomad",
          "score": 1,
          "created_utc": "2026-02-06 14:58:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pk1vt",
          "author": "atkr",
          "text": "go to bluetooth settings, connect the keyboard? (please not this model ðŸ˜‚) There is nothing to it",
          "score": 0,
          "created_utc": "2026-02-05 13:01:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qw108w",
      "title": "Thank you dax and opencode team",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qw108w/thank_you_dax_and_opencode_team/",
      "author": "jmhunter",
      "created_utc": "2026-02-04 21:07:30",
      "score": 49,
      "num_comments": 3,
      "upvote_ratio": 0.97,
      "text": "I just wanted to give a big thumbs up and thank you to dax and the team.. what a great product.. I do use claude primarily, but this has been my goto harness since since like march 2025.. i really appreciate your guys work and improvements.. The project rocks... no buts, just thanks!\n\nI will continue to support by buying at least $20 in credits a month from zen.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qw108w/thank_you_dax_and_opencode_team/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o3onsed",
          "author": "Complex_Initial_8309",
          "text": "Last time I checked, they mentioned, on their Zen page and dax's video about Zen, that they are not profiting from it.",
          "score": 3,
          "created_utc": "2026-02-05 08:26:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qq97y",
          "author": "Spirited-Milk-6661",
          "text": "Same, it's been my daily driver for months now. The cost transparency alone makes it a game-changer.",
          "score": 2,
          "created_utc": "2026-02-05 16:38:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lprai",
          "author": "Free-Stretch1980",
          "text": "Is 20 usd a month enough ? What models do you use ?",
          "score": 3,
          "created_utc": "2026-02-04 21:20:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsyzso",
      "title": "GPT 5.2 for difficult things and Kimi K2.5 for everything else seems to be the move, what the cheapest way to get there?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qsyzso/gpt_52_for_difficult_things_and_kimi_k25_for/",
      "author": "SweatyHands247",
      "created_utc": "2026-02-01 13:51:45",
      "score": 46,
      "num_comments": 32,
      "upvote_ratio": 0.98,
      "text": "Once the free period of Kimi K2.5 is finished, what's the cheapest, fast and private way to access it?\n\n  \nWe'll also want GPT access to tactically use it when necessary. What's the most cost effective way for this.\n\n  \nAnyone got OpenCode Black 20 access? Will that do the job? I imagine it'll get you pretty far for K2.5, but what about with some GPT sprinkled in. \n\nOr maybe Black 20 and a Chutes sub? \n\nAny other ideas?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qsyzso/gpt_52_for_difficult_things_and_kimi_k25_for/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2yy0fp",
          "author": "Simple_Split5074",
          "text": "Probably ChatGPT Plus (or maybe GH Copilot) and synthetic - nanogpt I found sadly cannot do Kimi K2.5 properly (otherwise probably the best deal out there if you want multiple models) and chutes I never had good experience with. To be fair, I have not yet signed up for synthetic.\n\nFWIW, the thinking goes that glm5 and DS4 might appear in the next two weeks (before CNY), picture might look different again then.",
          "score": 13,
          "created_utc": "2026-02-01 13:57:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zivvx",
              "author": "AlergDeNebun",
              "text": "Same, on nano-gpt, my experience so far is that it's quite slow and K2.5 tool calling is just broken 80% of the time, so unfortunately unusable for real work.\n\nThis being said, if they fix these issues and maybe improve the speed a bit (I'm OK with it being slower, but not THAT slow) it's a GREAT deal. But as it stands, if these issues are not resolved by the time my first month is up, I'm unsubscribing.",
              "score": 3,
              "created_utc": "2026-02-01 15:47:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zkoxz",
                  "author": "Simple_Split5074",
                  "text": "It also just flat out fails to respond at all quite often, according rto the discord they are trying to fix it",
                  "score": 2,
                  "created_utc": "2026-02-01 15:55:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2yxqw7",
          "author": "Recent-Success-1520",
          "text": "Depending upon your usage - Codex $20 + Nano-GPt  $8",
          "score": 4,
          "created_utc": "2026-02-01 13:56:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zjp2q",
              "author": "AlergDeNebun",
              "text": "It WOULD be a great deal, but here is a screenshot of me using Nano-GPT and trying to summarize some random files in a directory (it's my go to test for several reasons, just throw a bunch of text files together, some related some not, and ask the models to do things).\n\nYou can see how much it took, and at the end the tool calling failed.\n\nI get similar failures for Kimi K2.5 .\n\nSo my experience so far: very cheap, HUGE limits, but also slow and unreliable to the point of being unusable in the real world, for anything other than asking direct questions.\n\nhttps://preview.redd.it/g0yqofemmwgg1.png?width=1859&format=png&auto=webp&s=d10f14f2f650ca9212c628ee527a80cee62debe6",
              "score": 3,
              "created_utc": "2026-02-01 15:50:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34k7fx",
                  "author": "TastyIndividual6772",
                  "text": "I had this tool call failures on multiple plugins/clis on multiple Chinese models. I havenâ€™t had those inside kimi cli tho.",
                  "score": 1,
                  "created_utc": "2026-02-02 09:01:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2z1gvy",
              "author": "Ang_Drew",
              "text": "nano gpt seems to be good offer. what is the 60k message a month mean?\n\ndid tool call considered 1 messages?",
              "score": 1,
              "created_utc": "2026-02-01 14:17:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zkt8d",
                  "author": "Simple_Split5074",
                  "text": "My usage logs suggest they do count as a message",
                  "score": 2,
                  "created_utc": "2026-02-01 15:56:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30jpzz",
          "author": "drinksbeerdaily",
          "text": "Is kimi 2.5 really good enough to replace opus 4.5 for planning or debugging?",
          "score": 2,
          "created_utc": "2026-02-01 18:35:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32iu1t",
              "author": "Grand-Management657",
              "text": "No, I would not say so. In my experience, Opus 4.5 for planning, Kimi K2.5 for execution, and then review with GPT 5.2. Gets 3 LLMs to look at the same problem while being economical since most of the output happens on K2.5. \n\nK2.5 is not on the same level as Opus or GPT 5.2 but close to Sonnet 4.5 performance. I wrote more about it in my post here: https://www.reddit.com/r/opencodeCLI/s/MJoHjOdZGq",
              "score": 2,
              "created_utc": "2026-02-02 00:30:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36fq8i",
                  "author": "trypnosis",
                  "text": "Man after my own heart I never let model x review the work of model x. Model y should be used to review model x.\n\nI have not had much luck with kimi over the weekend  on synthetic but I hear they are now hosting it them selves so it is meant to be faster so will be trying that from tomorrow.",
                  "score": 1,
                  "created_utc": "2026-02-02 16:25:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3oyx2i",
              "author": "Key_Mousse_8034",
              "text": "As for the k2.5 model, it honestly feels like a downgraded Opus 4.5. It hallucinates, fails to complete tasks (while reporting success), and is generally lazy. I wouldn't rely on Kimi k2.5 for anything critical",
              "score": 1,
              "created_utc": "2026-02-05 10:14:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o32i9n5",
          "author": "Grand-Management657",
          "text": "Gpt 5.2 or Opus 4.5 as orchestrator, Kimi K2.5 for execution. Run K2.5 through synthetic and $20 codex sub for orchestration (run it in xhigh). It's pretty much the most economical combination while retaining performance. I wrote about the economics in my post here: https://www.reddit.com/r/opencodeCLI/s/MJoHjOdZGq",
          "score": 2,
          "created_utc": "2026-02-02 00:27:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31tmk5",
          "author": "oknowton",
          "text": "I think you're on the right track.  Chutes and Synthetic are two of the subscriptions I have active right now.  I'm having success with Kimi K2.5 on both, but I am not a professional programmer writing code 8 hours a day, and my coding work is pretty simple compared to what most people in here are doing.\n\nSynthetic is about 3x faster right Chutes, at least for Kimi K2, but Chutes has bigger quotas at a fraction of the price.\n\n> Or maybe Black 20 and a Chutes sub? \n\nI think OpenCode Black is too new to pin down.  They haven't published any information about how their quotas will work, and they probably won't have any of that nailed down until after the service goes live for everyone.\n\nOpenAI seems friendly towards the OpenCode community, and I'm always hearing people say how plentiful their quotas are compared to Anthropic.  Pairing a $20 Codex subscription with a $3 or $10 Chutes subscription seems like a good place to start.",
          "score": 2,
          "created_utc": "2026-02-01 22:14:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33th7b",
          "author": "TreeBearr",
          "text": "I've had the best luck with synthetic for the good open weight models. The inference can be a lil slow sometimes but the cost and privacy is worth. I use it with opencode and OpenWebUI.\n\nI think they're pretty generous usage limits for the $20 plan. 135 requests per 5 hours and the requests are fractional if they are tool calls or small agent runs. I'll insert a ss of the billing page.  \n\n\nIf u get referred you get some credit :3\n\n[https://synthetic.new/?referral=7JlVOLCkmEQv5oI](https://synthetic.new/?referral=7JlVOLCkmEQv5oI)\n\nhttps://preview.redd.it/v9co646ak0hg1.png?width=934&format=png&auto=webp&s=425697296c73a8656b541efbff6e16e6de94e4a2",
          "score": 2,
          "created_utc": "2026-02-02 05:08:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o340flg",
          "author": "bduyng",
          "text": "That combo makes sense â€” GPT-5.2 for heavy lifting and Kimi K2.5 for cheap grunt work seems solid ðŸ‘",
          "score": 1,
          "created_utc": "2026-02-02 06:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34p4o2",
          "author": "LordEli",
          "text": "i've had a lot of success with just kimi k2.5 alone but wondering how rate limits work",
          "score": 1,
          "created_utc": "2026-02-02 09:49:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35h7oy",
          "author": "pbalIII",
          "text": "One angle worth considering: task-based routing instead of just model tiers.\n\nK2.5 at $0.60/1M input and $3/1M output is already cheap. GPT 5.2 is ~10x more expensive per token. The math flips depending on what you're actually doing.\n\nFor agentic work with lots of tool calls and iterations, K2.5 handles the volume. For single-shot complex reasoning where you need one good answer, GPT 5.2 pays for itself in fewer retries.\n\nSo rather than Black 20 + another sub, you could route at the task level: classify incoming requests by complexity and let a cheap router model decide. OpenRouter and similar services support this natively. Cuts costs 30-40% without degrading quality on the tasks that matter.",
          "score": 1,
          "created_utc": "2026-02-02 13:30:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iewp2",
          "author": "Keep-Darwin-Going",
          "text": "How are you all setting this up? Opencode do not have a way to auto switch between model. Are you all planning first then switch to code the switch the model?",
          "score": 1,
          "created_utc": "2026-02-04 11:11:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2z77hs",
          "author": "Bob5k",
          "text": "[synthetic](https://synthetic.new/?referral=IDyp75aoQpW9YFt) as a provider for kimi / glm / minimax is a nobrainer, especially when you also consider reliable infrastructure and zero data retency as important feature.",
          "score": -1,
          "created_utc": "2026-02-01 14:49:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2z83ek",
              "author": "Funny-Advertising238",
              "text": "Literally anything is better than synthetic don't listen to this guyÂ ",
              "score": 3,
              "created_utc": "2026-02-01 14:53:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2z9nv9",
                  "author": "lundrog",
                  "text": "Not sure why you hate them but performance is good and you get good value for a monthly. If you can find me a subscription for the same or less with the usage I need that performs as well im all ears ðŸ˜‚ (not nano gpt ) they were called out on Reddit via a provider for credit fraud",
                  "score": 4,
                  "created_utc": "2026-02-01 15:01:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2zknxz",
                  "author": "red_rolling_rumble",
                  "text": "It would be more convincing if you would explain how itâ€™s bad!",
                  "score": 2,
                  "created_utc": "2026-02-01 15:55:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o35okb7",
                  "author": "blankeos",
                  "text": "Like what? :o",
                  "score": 1,
                  "created_utc": "2026-02-02 14:11:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zae4t",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-02-01 15:05:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zl7ro",
              "author": "Simple_Split5074",
              "text": "As opposed to spamming referral links...",
              "score": 1,
              "created_utc": "2026-02-01 15:57:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qssabi",
      "title": "The definitive guide to OpenCode: from first install to production workflows",
      "subreddit": "opencodeCLI",
      "url": "https://jpcaparas.medium.com/the-definitive-guide-to-opencode-from-first-install-to-production-workflows-aae1e95855fb?sk=69c1519ee7808c4eed582c44b016fc70",
      "author": "jpcaparas",
      "created_utc": "2026-02-01 07:50:10",
      "score": 44,
      "num_comments": 2,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qssabi/the_definitive_guide_to_opencode_from_first/",
      "domain": "jpcaparas.medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "o31etjx",
          "author": "touristtam",
          "text": "nice plugin in there. ;)",
          "score": 1,
          "created_utc": "2026-02-01 21:02:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3s3dmm",
          "author": "Donnybonny22",
          "text": "I bookmarked the guide because it is really good and I want to see updates",
          "score": 1,
          "created_utc": "2026-02-05 20:26:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs629t",
      "title": "Which coding plan?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qs629t/which_coding_plan/",
      "author": "Simple_Split5074",
      "created_utc": "2026-01-31 15:59:52",
      "score": 41,
      "num_comments": 42,
      "upvote_ratio": 0.98,
      "text": "OK so \n\n* GLM is unusably slow lately (even on pro plan; the graphs on the site showing 80tps are completely made up if you ask me)\n* nanogpt Kimi 2.5 mostly fails \n* Zen free Kimi 2.5 works until it does not (feels like it flip flops every hour). \n\nI do have a ChatGPT Plus sub which works but the quota is really low, so really only use it when I get stuck.\n\nThat makes me wonder where to go from here?\n\n* ChatGPT Pro: models are super nice, but the price,; the actual limits are super intransparent, too....\n* Synthetic: hard to say how much use you really get out of the 20$ plan? Plus how fast / stable are they (interestedin Kimi 2.5, potentially GLM5 and DS4 when they arrive)? Does caching work (that helps a lot with speed)?\n* Copilot: Again hard to understand the limits. I guess the free trial would shed light on it?\n\nAny other ideas? Thoughts?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qs629t/which_coding_plan/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2t3csi",
          "author": "soul105",
          "text": "GH Copilot is really easy to understand their limits: they are based on requests, and that's it.",
          "score": 19,
          "created_utc": "2026-01-31 16:03:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2t3zg0",
              "author": "Michaeli_Starky",
              "text": "Except for it's not THAT straightforward when it comes to counting the requests.",
              "score": 6,
              "created_utc": "2026-01-31 16:06:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2t4n1n",
                  "author": "Simple_Split5074",
                  "text": "This. Supposedly only user input counts but even that is hard to make sense of.",
                  "score": 1,
                  "created_utc": "2026-01-31 16:09:37",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2tjepz",
              "author": "NerasKip",
              "text": "Is opencode do a compact then continue. It count as 3 requests add 2 mores for each compact/continue",
              "score": 1,
              "created_utc": "2026-01-31 17:20:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tkypj",
          "author": "warpedgeoid",
          "text": "GitHub Copilot is a steal for $40/month. It has all of the most recent models and MS claims data are not retained for training purposes.",
          "score": 6,
          "created_utc": "2026-01-31 17:27:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xt2cb",
              "author": "typeof_goodidea",
              "text": "How fast do you tend to hit usage limits?",
              "score": 1,
              "created_utc": "2026-02-01 08:22:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2z58xp",
                  "author": "warpedgeoid",
                  "text": "I definitely hit them within a few days when using OC. Of course, Iâ€™m not one of these people running four OC sessions at a time. Still, once youâ€™ve hit the limit, they charge $0.04/request which means the total cost is going to be similar to Claude Max for extremely heavy usage.",
                  "score": 1,
                  "created_utc": "2026-02-01 14:38:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2t3voy",
          "author": "OnigiriFest",
          "text": "I  donâ€™t have experience with GLM and nanogpt.\n\nI bought synthetic just 2 days ago and been testing it for a bit, the 20 usd plan with Kimi 2.5 can handle one agent running no stop in the 5 hours window (I tested it with a Ralph loop)\n\nThe speed is hit or miss right now, sometimes itâ€™s good and some times itâ€™s slow, in theory they are working to fix it, they say itâ€™s a problem affecting only Kimi 2.5.",
          "score": 11,
          "created_utc": "2026-01-31 16:05:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t6pni",
          "author": "Torresr93",
          "text": "The GitHub copilot plan is easy to understand.You get 300 requests, and each model has a multiplier based on its cost. For example, one Opus request counts as tree. On top of that, for simple tasks you can use gtp5-mini for free.",
          "score": 5,
          "created_utc": "2026-01-31 16:19:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t8u7e",
          "author": "shaonline",
          "text": "ChatGPT Plus is opaque but rate limits have been decent. As all 20-ish bucks plans from frontier labs you better delegate the simple tasks (past planning/review) to a cheaper model if you don't want to smoke your weekly quota too fast.",
          "score": 5,
          "created_utc": "2026-01-31 16:29:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2tbjlg",
              "author": "Simple_Split5074",
              "text": "Which is why I am looking for the workhorse provider :-)",
              "score": 1,
              "created_utc": "2026-01-31 16:42:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2tcump",
                  "author": "shaonline",
                  "text": "I mean if you want to throw the top-tier expensive models at all problems you're left with paying a 200 bucks a month subscription, which is still heavily subsidized in its own rights (if stuff like viberank is to be believed as far as claude code is concerned lol)",
                  "score": 1,
                  "created_utc": "2026-01-31 16:48:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2t852t",
          "author": "LittleChallenge8717",
          "text": "[Synthetic.new](http://Synthetic.new) has generous 5h limits IMO, you also can get 10$ off for 20$ subscription, and 20$ off for 60$ subscription with referal codes -> has minimax, glm 4.7 and kimi k2.5 models (others too). you can use mine so we both benefit [https://synthetic.new/?referral=EoqzI9YNmWuGy3z](https://synthetic.new/?referral=EoqzI9YNmWuGy3z) or buy it directly from their website. Tool calling works great (counts as 0.1x or 0.2x it depends), also based on my experience -> GLM4.7 and minimax works great since they are directly hosted on synthetic gpu's, for other models like kimi k2.5 they use fireworks which has sometimes delay in generation. as i know from support they plan to host kimi in next weeks so i guess then synthetic  would be ideal offer, meanwhile GLM and minimax models working great in opencode with no additional delay/issues\n\nhttps://preview.redd.it/cbhqrqzpnpgg1.png?width=1220&format=png&auto=webp&s=b7023ae322082cb3c20c7a27654786249d5d1317",
          "score": 4,
          "created_utc": "2026-01-31 16:26:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2tarmz",
              "author": "Simple_Split5074",
              "text": "Which in some sense is great, fireworks is likely the best of the inference providers (if I wanted to pay by token I'd go there). In another sense, it does not inspire confidence in their infra...",
              "score": 4,
              "created_utc": "2026-01-31 16:38:52",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2t8duw",
              "author": "LittleChallenge8717",
              "text": "https://preview.redd.it/2mzav20iopgg1.png?width=2169&format=png&auto=webp&s=eb5df84cab3e389d73209338f330c3a77f040446\n\nthis is what i mean regarding provider",
              "score": 2,
              "created_utc": "2026-01-31 16:27:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2txvfc",
          "author": "gh0st777",
          "text": "I see a lot pushing synthetic referral hard lately.\n\nI assume you are on a tight budget or this is for a hobby and not a source of income. Why not try each one for a month or alot $5 for API usage to see what works for you?\n\nIf you use this for work or a source of income, might as well invest and get claude code max. $100 is the sweetspot to get things done with opus. But it no longer works for opencode, so consider that too.",
          "score": 3,
          "created_utc": "2026-01-31 18:28:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t8aiq",
          "author": "Bob5k",
          "text": "On synthetic end you can try it for 10$ first month with [reflink](https://synthetic.new/?referral=IDyp75aoQpW9YFt) if you don't mind. \nI'm using them on pro plan for quite a long time and generally I'm happy so far. Especially due to fact that any new frontier opensource model is instahosted there - rn using Kimi K2.5 as my baseline.\nUsually on self hosted models it's around 70-90tps (glm, minimax), for Kimi K2.5 right now a tad bit slower, ranging 60-80 tps for me.",
          "score": 4,
          "created_utc": "2026-01-31 16:27:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2t9ugk",
              "author": "ZeSprawl",
              "text": "They are currently forwarding Kimi k2.5 to fireworks because their infra is having trouble running it.",
              "score": 3,
              "created_utc": "2026-01-31 16:34:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ta583",
                  "author": "Bob5k",
                  "text": "Yeah i know, this is probably the reason of slightly lower tps aswell. In general works just fine, roughly 100m+ tokens already processed by Kimi on my projects ðŸ«¡",
                  "score": 3,
                  "created_utc": "2026-01-31 16:35:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2xxtdb",
              "author": "1234filip",
              "text": "Gotta say that I'm really happy with synthetic right now. Very reliable and the models do any tool calls perfectly!",
              "score": 2,
              "created_utc": "2026-02-01 09:07:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ydmph",
                  "author": "Bob5k",
                  "text": "Happy to hear. I can't be happier aswell - especially due to fact that stability is better than \"native\" providers and basically whatever comes out - i don't care, as they'll host it anyway so i don't need to pay somewhere else.\nWant a gig with deep seek? No problemo. Glm5 will be out? They're already ready for it. Kimi? Routed and working on self hosted. \nEssentially even for 60$ sub on synthetic it's stil cheaper than to have 3 diff subs across minimax Kimi and glm while also 1350 prompts on synthetic is insane amount while they charge 0.1 prompt per tool call. For coding even 2-3 projects at a time - infinite amount of LLM calls basically.",
                  "score": 1,
                  "created_utc": "2026-02-01 11:31:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2uhguu",
          "author": "cjazinski",
          "text": "I bought the pro glm 4.7 and it blows worst 200 ever",
          "score": 2,
          "created_utc": "2026-01-31 20:02:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2uiu3k",
              "author": "Simple_Split5074",
              "text": "Last year it was decent, then it gradually declined... Hoping for more capacity and GLM 5 now...",
              "score": 1,
              "created_utc": "2026-01-31 20:09:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2vc9dz",
          "author": "troyvit",
          "text": "I've been  enjoying OpenCode (and aider.chat) with my Mistral API key using mostly Mistral Large 3 but also Devstral. It works well  for my simple needs. It's cheap enough that I don't mind asking detailed questions about what it does and learning the answers, which keeps me from just vibe coding. I'm actually getting a little bit better at python (mostly in the realm of architecture)",
          "score": 2,
          "created_utc": "2026-01-31 22:35:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2y0s3x",
          "author": "Shep_Alderson",
          "text": "Iâ€™ve had a good time with Synthetic. I started with their $20 plan and only upgraded to the $60 plan when I hit the limit from running a Ralph loop for like an hour or two. Since I upgraded, Iâ€™ve not come anywhere near hitting limits and I quite like it. They also had Kimi K2.5 available the day it launched via their hosting partner, though I find myself preferring GLM 4.7 and Minimax M2.1 personally.",
          "score": 2,
          "created_utc": "2026-02-01 09:35:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t3f9q",
          "author": "tidoo420",
          "text": "Unpopular opinion, i use qwen coder 3 free with qwen cli, it is better than i expected please give it a go\nP.s. i have tried most of the above and not satisfied",
          "score": 2,
          "created_utc": "2026-01-31 16:03:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2t52c6",
              "author": "Simple_Split5074",
              "text": "I find qwen models (either 235 or 480) to be nigh useless for coding. Before I deal with that I'll use antigravity (gemini-cli somehow does not load anymore on my machine, go figure)...",
              "score": 1,
              "created_utc": "2026-01-31 16:11:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tts9o",
          "author": "Jakedismo",
          "text": "Kimi Code definetely has the edge over zai and minimax tested them all and kimi is the most broad specialist when vibing",
          "score": 2,
          "created_utc": "2026-01-31 18:09:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t65lj",
          "author": "BERLAUR",
          "text": "Why not combine them? GLM is cheap (2-3 bucks per month). Synthetic.new has a trail for 12 USD. ChatGPT usually offers a free month.Â \n\n\nIf you're a student you can get Copilot for cheap (free?).\n\n\nI have 5 subscriptions and I just switch between them when I run into a limit. Total cost is still less than a meal at a restaurant. Absolutely worth it.\n\n\nIf I have some tokens to spare I'll burn them on less important tasks.",
          "score": 1,
          "created_utc": "2026-01-31 16:16:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y0a4o",
              "author": "wizenith",
              "text": "would you like to share what are the 5 subscriptionsÂ you are using?   \nyou have mentioned GLM, Synthetic and ChatGPT, so i might assume you have subscribed them already ( or no? ). \n\nAnd what other subscriptionsÂ you had? just curious. ",
              "score": 2,
              "created_utc": "2026-02-01 09:30:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2z2a7v",
                  "author": "BERLAUR",
                  "text": "- Z.AI (cheap and great for the grunt work)\n- Copilot (great for Sonnet and Opus, plus free unlimited grok-code-fast/GPT Mini which is handy for e.g minor refactors)\n- Claude Code (but will probably cancel this one)\n- Synthetic (Kimi K2.5)\n- OpenAI Codex (really impressive for debugging and big fixes!)\n- Openrouter (for various things, mostly to test and try new models)\n\n\nI work as a CTO so this gives me the ability to play around with a whole bunch of stuff to see what might work best for my development teams and it's also fun! We're pushing quite hard on AI but I don't want to be one of those leaders who scream \"AI, AI, AI!\". I want to be at least somewhat experienced enough to actually push the teams towards delivering more value.\n\n\nNanoGPT and Cerbus (for the speed) is also worth checking out, I haven't tried those out yet.",
                  "score": 1,
                  "created_utc": "2026-02-01 14:22:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2t7mn1",
              "author": "Simple_Split5074",
              "text": "Oh I \\*do\\* combine them, mostly I am looking for another one...",
              "score": 1,
              "created_utc": "2026-01-31 16:23:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tajl3",
          "author": "esmurf",
          "text": "I tried a couple of different one it seems Github copilot is the best choice right now. I'm looking into to go all opencode though.Â ",
          "score": 1,
          "created_utc": "2026-01-31 16:37:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tamwz",
          "author": "tisDDM",
          "text": "I did not find the quota fÃ¼r GPTPlus low. Anyway there is no such thing as a cheap plan for SOTA models. \n\nIf you like it cheap - and working: Sign up for Mistral API. Their Devstral 2 models are good and currently still free.",
          "score": 1,
          "created_utc": "2026-01-31 16:38:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tummk",
          "author": "annakhouri2150",
          "text": "> Synthetic: hard to say how much use you really get out of the 20$ plan? Plus how fast / stable are they (interestedin Kimi 2.5, potentially GLM5 and DS4 when they arrive)? Does caching work (that helps a lot with speed)? \n\nIn my experience, having paid for the $20 Synthetic plan for a few months now, Synthetic is faster and more stable --- and the inference is higher-quality --- for their self hosted models (GLM 4.7, Kimi K2T, etc) than any other provider. Currently, they're proxying K2.5 to Fireworks.AI while they get their infrastructure and hardware ready to run it, so it's not nearly as reliable in tool calling as their general capabilities, but it's still faster and more stable than other services I've tried (to be fair, I haven't tried any of the Big Three --- Gemini, Claude, or GPT Codex). \n\nAlso, OpenCode is pretty API call efficient; when I was still using it, the 135 API calls provided by the $20/mo Synthetic plan felt like more than enough. If you have an agent that uses a lot more API requests, like the Zed Agent, you can start to run up against the limits more often if you've got, like, several agents running, or having them run in a really really tight loop where they output and do very little per API call, but for general usage even in more token-heavy agents, it takes some heavy nonstop usage to hit the limit within their 5 hour window. Their limits are more generous than what the Claude Code subscription gives you, for instance.",
          "score": 1,
          "created_utc": "2026-01-31 18:13:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yz2so",
          "author": "tibsmagee",
          "text": "I've been using cheapest minimax code plan the last month. Very reliable and includes web search and vision.Â \n\n\nSeems like a very capable model for day to day coding.",
          "score": 1,
          "created_utc": "2026-02-01 14:04:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3400nv",
          "author": "t12e_",
          "text": "Synthetic plus GH copilot\n\nYou'll have to update your opencode config so that you use gpt5 mini for subagents to save up on requests. Then kimi/glm for most tasks and any of the Claude/Codex models for complex tasks. I usually hit the 5 hour limit after about 4 hours (this is with 2 agents running)",
          "score": 1,
          "created_utc": "2026-02-02 05:59:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mu3kk",
          "author": "Tiny_Independent8238",
          "text": "whatever you do, just don't get synthetic",
          "score": 1,
          "created_utc": "2026-02-05 00:50:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t47zx",
          "author": "trypnosis",
          "text": "I feel your pain. Leaning to copilot trying that and synthetic will decide in a few weeks.",
          "score": 1,
          "created_utc": "2026-01-31 16:07:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ubyrp",
          "author": "SamatIssatov",
          "text": "A very good limit in ChatGPT Plus. Why lie here? So that someone will suggest â€œsyntheticâ€?",
          "score": 0,
          "created_utc": "2026-01-31 19:35:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qt428t",
      "title": "Using an AI Agent (opencode) To Teach Me Rust and Itâ€™s Kinda Blowing My Mind",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qt428t/using_an_ai_agent_opencode_to_teach_me_rust_and/",
      "author": "feursteiner",
      "created_utc": "2026-02-01 17:07:00",
      "score": 37,
      "num_comments": 46,
      "upvote_ratio": 0.92,
      "text": "Iâ€™ve been learning Rust with an AI agent through OpenCode, and itâ€™s honestly way cooler than I expected.\n\nComing from a TypeScript-heavy background, I thought Rust would break my brain, but the AI keeps mapping concepts to stuff I already know. Itâ€™s structured, but flexible enough that I can reshape the whole plan whenever I get stuck or suddenly decide to deep-dive ownership at 2am.\n\nIt uses a pyramid-style method where each layer builds on the last, and I can expand it as I go. The repo basically becomes a living skill tree. Also, I get to ask all the â€œdumbâ€ questions Iâ€™d never ask a human. No judgment. Just explanations until it finally clicks.\n\nLearning at my own pace, on my own time, has been way more comfortable, and honestly the speed is kind of wild. Rust went from intimidating to fun way faster than I expected.\n\nEdit:   \ntook down the link before, but happy to share it again, thanks for the support y'all!  \n[https://github.com/feuersteiner/learning-rust](https://github.com/feuersteiner/learning-rust)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qt428t/using_an_ai_agent_opencode_to_teach_me_rust_and/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o3012hq",
          "author": "Maasu",
          "text": "Covered async yet?",
          "score": 9,
          "created_utc": "2026-02-01 17:11:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30500e",
              "author": "coffee_brew69",
              "text": "the agent might delete itself on that part",
              "score": 13,
              "created_utc": "2026-02-01 17:29:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o307ol9",
                  "author": "feursteiner",
                  "text": "haha I see that x) I had to cycle through a couple of models to find one that is actually a good value for money. I am using github copilot as the provider (wide variety, and feels cheaper). Opus was best, but is most expensive, currently with codex-5.2",
                  "score": 2,
                  "created_utc": "2026-02-01 17:41:19",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30ee1c",
          "author": "debba_",
          "text": "I totally agree. Iâ€™m also using a *learn-by-doing* approach with Rust, with the help of OpenCode. On top of that, KIMI K2.5 Free with Zen is a really nice bonus.\n\nIn just one week, I managed to ship a first beta of a side project of mine: a lightweight database tool with a clean, pleasant UX.\n\nIf you want to take a look:  \n[https://github.com/debba/tabularis](https://github.com/debba/tabularis)",
          "score": 9,
          "created_utc": "2026-02-01 18:11:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30it3b",
              "author": "feursteiner",
              "text": "Tauri is the goat!",
              "score": 1,
              "created_utc": "2026-02-01 18:31:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o30veop",
          "author": "Ok_Layer2715",
          "text": "Hey, i would appreciate if you give me more details, as what you have written to opencode from first and what is the pyramid method",
          "score": 2,
          "created_utc": "2026-02-01 19:28:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31lmv6",
              "author": "feursteiner",
              "text": "absolutely! I can first refer you to the agents md (feel free to star the repo, please and thank you haha) and you can see everything. feel free to ask me any questions about it too!  \n[https://github.com/feuersteiner/learning-rust](https://github.com/feuersteiner/learning-rust)",
              "score": 1,
              "created_utc": "2026-02-01 21:35:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31okdl",
                  "author": "Ok_Layer2715",
                  "text": "Nice, i have checked both of them and they are awesome specially your repo hahah\nBut the thing that i cant understand till now is the pyramid method",
                  "score": 2,
                  "created_utc": "2026-02-01 21:49:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33dppo",
          "author": "mrpoopybruh",
          "text": "Its wild. I am currently binding them into cards on a big canvas. I will be in the matrix within days, if not hours. Not kidding -- I'm literally coding up a green on black style layer lol",
          "score": 2,
          "created_utc": "2026-02-02 03:26:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o364qtl",
              "author": "feursteiner",
              "text": "dude the matrix theme is my fav on opencode haha",
              "score": 1,
              "created_utc": "2026-02-02 15:34:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o36pp5y",
                  "author": "mrpoopybruh",
                  "text": "LOL me too. Man, I gotta say though, the CLI is slick, but the actual Rest API is very complex and its taking forever to get even something crappy up. Gotta give it to the opencode team on how slick the CLI is.",
                  "score": 2,
                  "created_utc": "2026-02-02 17:11:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3499i5",
          "author": "levu304",
          "text": "i think you should approach through napi-rs first",
          "score": 2,
          "created_utc": "2026-02-02 07:18:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o364urq",
              "author": "feursteiner",
              "text": "can you explain more please ? I am intrigued",
              "score": 1,
              "created_utc": "2026-02-02 15:34:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o34xf4w",
          "author": "chiroro_jr",
          "text": "I used to do this too when learning something new. I tell the AI to generate tests. Then I write code that tries to pass those tests. If it's too hard, I ask for a hint. When the tests pass I asked the AI to look at the code so that it tell me if I could have implemented a better solution or written more idiomatic code depending on the ecosystem. It's pretty cool.",
          "score": 2,
          "created_utc": "2026-02-02 11:06:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3651p9",
              "author": "feursteiner",
              "text": "exactly the same process! someone should make product around this... this is what school should look like in the future ...",
              "score": 2,
              "created_utc": "2026-02-02 15:35:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o308wxl",
          "author": "web_assassin",
          "text": "I'm advancing my Git skills with opencode and loving it. It doesn't give me snarky replies to my dumb questions.",
          "score": 1,
          "created_utc": "2026-02-01 17:46:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o309ufg",
              "author": "feursteiner",
              "text": "yeah, exactly. it's sad to see places like reddit turn like that, where people don't appreciate other's learning journeys and just pile on them... sad. Good luck to you too u/web_assassin !",
              "score": 2,
              "created_utc": "2026-02-01 17:51:06",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o30f6ey",
              "author": "vertigo235",
              "text": "Stackoverflow prevented so many eager people from learning, you only really learned from it if a previous person took some serious heat for asking a simple question. \n\nGone are those days!",
              "score": 2,
              "created_utc": "2026-02-01 18:14:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o30hn16",
                  "author": "feursteiner",
                  "text": "I re-posted this same exact post on another subreddit and gotten so much hate in 2 minutes I deleted the post...",
                  "score": 1,
                  "created_utc": "2026-02-01 18:25:56",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o30ik60",
                  "author": "web_assassin",
                  "text": "Hah yeah sacrificial lambs. The online haters are losing their jobs. So sad!",
                  "score": 1,
                  "created_utc": "2026-02-01 18:30:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30ejob",
          "author": "vertigo235",
          "text": "This is the way",
          "score": 1,
          "created_utc": "2026-02-01 18:12:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31lrbg",
              "author": "feursteiner",
              "text": "damn straight!! exciting times for learning!",
              "score": 1,
              "created_utc": "2026-02-01 21:36:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o30frws",
          "author": "antifeixistes",
          "text": "Could you share a bit more about the process, how did you set it up to learn rust from it? Thx",
          "score": 1,
          "created_utc": "2026-02-01 18:17:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30imc2",
              "author": "feursteiner",
              "text": "so it was a process, first I tried to setup just a readme with a curriculum (opus generated that I think, or gpt5.2), but then I went and setup the agents\\[.\\]md. I knew I wanted to have different level of answers depending on how much detail I want, so I setup the \"pyramid method\" which is how news articles are written.  \nthen I started slowly to scaffold what a lesson is and what an exercise is, then added an \"ex-00\" which just gives me basic syntax to learn, and other exercises to teach the concepts.  \nI found myself learning by analogy (bun vs cargo, memory management in C...) so I told the agents file about my background so that it explain conxepts in a relevant manner.  \nanyhow, it's a moving process, but I hitnk it's getting better as I advance in lessons, happy to give you more detail if you want (pyramid method again haha).",
              "score": 3,
              "created_utc": "2026-02-01 18:30:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3254p2",
                  "author": "antifeixistes",
                  "text": "Thanks! Also saw your other reply with the repo. Will check that out. Thanks a lot!",
                  "score": 2,
                  "created_utc": "2026-02-01 23:14:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30sqzq",
          "author": "larowin",
          "text": "Do you think you could explain a borrow checker without help yet?",
          "score": 1,
          "created_utc": "2026-02-01 19:16:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30svt1",
              "author": "feursteiner",
              "text": "oh yeah def haha",
              "score": 2,
              "created_utc": "2026-02-01 19:16:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o30teox",
                  "author": "feursteiner",
                  "text": "basically a variable's value can be borrowed, i.e. if a = 5, I can declare b that points to the value so to speak. I am allowed to do operations on b (multiple reads), if it's a mut (an actual variable), I can only have one mutation reference at a time. and finally, when a goes out of scope, it's freed from memory. and we can't have borrows outside the scope of a, htat's called hanging.. how did I do ? haha",
                  "score": 1,
                  "created_utc": "2026-02-01 19:19:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o31e3bm",
          "author": "Michaeli_Starky",
          "text": "Books. Use them for learning.",
          "score": 1,
          "created_utc": "2026-02-01 20:59:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ru1bk",
              "author": "25Violet",
              "text": "Books are only good up to a certain point. Their main flaw is that they are one-sided. You can only read what's in it, and if you have any questions about X and the book doesn't elaborate further, or their explanation still does not really make you understand, you are on your own. One of the good things about AI is that you can ask it to dumb it down as much as you need until you finally understand.",
              "score": 1,
              "created_utc": "2026-02-05 19:41:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o31ldm8",
              "author": "feursteiner",
              "text": "thanks granpa.",
              "score": 0,
              "created_utc": "2026-02-01 21:34:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31mjhw",
                  "author": "Michaeli_Starky",
                  "text": "You're welcome.",
                  "score": 1,
                  "created_utc": "2026-02-01 21:40:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o328e6c",
          "author": "haobes",
          "text": "how",
          "score": 1,
          "created_utc": "2026-02-01 23:32:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwszbc",
      "title": "Gpt 5.3 codex dropped",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/r15af3dwwphg1.jpeg",
      "author": "ReasonableReindeer24",
      "created_utc": "2026-02-05 18:18:24",
      "score": 37,
      "num_comments": 18,
      "upvote_ratio": 0.77,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qwszbc/gpt_53_codex_dropped/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3rf2xj",
          "author": "Xeon06",
          "text": "Nothing more idiotic than sharing a screenshot of the top of a blog post instead of the URL\n\nhttps://openai.com/index/introducing-gpt-5-3-codex/",
          "score": 60,
          "created_utc": "2026-02-05 18:33:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3roc4r",
          "author": "Timo_schroe",
          "text": "Its available for me in Opencode after reauth",
          "score": 8,
          "created_utc": "2026-02-05 19:15:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3s8p51",
              "author": "Aggravating_Win2960",
              "text": "Hi, can you share the exact the command? I tried /connect inside opencode and als 'opencode auth login' in terminal/ghostty but I only get the GPT-5.2 Codex model.  \nps: have latest 1.1.51 version of opencode",
              "score": 2,
              "created_utc": "2026-02-05 20:51:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3saeb4",
                  "author": "Timo_schroe",
                  "text": "Opencode auth logout after that Login, Opus 4.6 available too now ! (I use pro at openai and max at anthrophic). Ah and I use oh-my-opencode. Maybe they bring an other with plugin (?)",
                  "score": 2,
                  "created_utc": "2026-02-05 20:59:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ru5eh",
          "author": "drinksbeerdaily",
          "text": "Really impressed with it so far. I might finally drop Claude 5x and get by with two ChatGPT plus subs.",
          "score": 5,
          "created_utc": "2026-02-05 19:42:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3sqi7d",
              "author": "Impossible_Secret80",
              "text": "Opus 4.6 with a 1-million token context window is out also today :)",
              "score": 1,
              "created_utc": "2026-02-05 22:18:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3sssxf",
                  "author": "Chris266",
                  "text": "1 mil only in API...",
                  "score": 2,
                  "created_utc": "2026-02-05 22:29:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3s2840",
          "author": "web_assassin",
          "text": "I'm trying to save it as my opencode default model but it's not having any effect. I can set it with /models though.   \"model\": \"openai/gpt-5.3-codex\", is what i'm using in the opencode.json",
          "score": 3,
          "created_utc": "2026-02-05 20:20:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rm1q6",
          "author": "MegamillionsJackpot",
          "text": "I can see it in Codex CLI, but not in Opencode. Anyone see it ?",
          "score": 1,
          "created_utc": "2026-02-05 19:04:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3s0zke",
              "author": "MegamillionsJackpot",
              "text": "Sorted. Used codex to set it up as agent in opencode",
              "score": 2,
              "created_utc": "2026-02-05 20:14:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3vnaqk",
                  "author": "oulu2006",
                  "text": "I might have to do that as well",
                  "score": 1,
                  "created_utc": "2026-02-06 10:24:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3tgmyt",
              "author": "MattU2000",
              "text": "do opencode auth logout and then opencode auth login.",
              "score": 1,
              "created_utc": "2026-02-06 00:40:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3vn9tt",
                  "author": "oulu2006",
                  "text": "that's interesting, from iterm still didn't work for me and shot gpt5.2 only \n\n    opencode auth logout + \n    opencode auth login",
                  "score": 2,
                  "created_utc": "2026-02-06 10:24:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3usldg",
          "author": "Fit-Mulberry-8611",
          "text": "Is it also 25% faster in opencode",
          "score": 1,
          "created_utc": "2026-02-06 05:46:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3wxwm5",
          "author": "JuiceBoxJonny",
          "text": "https://preview.redd.it/gsqmpae36whg1.jpeg?width=1170&format=pjpg&auto=webp&s=2a728f5ec5192e98d238e3bdefd90d2e14127821\n\nYall seriously use ts ðŸ¤”",
          "score": 1,
          "created_utc": "2026-02-06 15:20:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtweb2",
      "title": "OpenCode Swarm Plugin",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qtweb2/opencode_swarm_plugin/",
      "author": "Outrageous-Fan-2775",
      "created_utc": "2026-02-02 14:23:53",
      "score": 34,
      "num_comments": 11,
      "upvote_ratio": 0.96,
      "text": "I created a swarm plugin for OpenCode that I've been rigorously testing on my own and I think its in a good enough state to get additional feedback. Github link is below but all you have to do is add the plugin to your OpenCode config and NPM will download the latest package for you automatically.\n\n[https://github.com/zaxbysauce/opencode-swarm](https://github.com/zaxbysauce/opencode-swarm)  \n[https://www.npmjs.com/package/opencode-swarm](https://www.npmjs.com/package/opencode-swarm)\n\nGeneral idea is that of perspective management. When you code with the traditional Plan/Build method in OpenCode, you are forcing a slightly different perspective on the LLM but in the end it is still a perspective borne of the same exact training set. My intent was to collate genuinely different data sets by calling different models for each agent.\n\nA single architect guides the entire process. This is your most capable LLM be it local or remote. Its job is to plan the project, collate all intake, and ensure the project proceeds as planned. The architect knows to break the task down into domains and then solicit Subject Matter Expert input from up to 3 domains it has detected. So if you are working on a python app, it would ask for input from a Python SME. This input is then collated, plan adjusted, and implementation instructions are sent to the coding agent one task at a time. The architect knows that it is the most capable LLM and writes all instructions for the lowest common denominator. All code changes are sent to an independent auditor and security agent for review. Lastly, the Test Engineer writes robust testing frameworks and scripts and runs them against the code base.\n\nIf there are any issues with any of these phases they will be sent back to the architect who will interpret and adjust fire. The max number of iterations the architect is allowed to roll through is configurable, I usually leave it at 5.\n\nClaude put together a pretty good readme on the github so take a look at that for more in depth information. Welcoming all feedback. Thanks!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qtweb2/opencode_swarm_plugin/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o3gca9y",
          "author": "stephen_S27",
          "text": "It looks like oh-my-opencode to me, we also have multi agents with different roles",
          "score": 3,
          "created_utc": "2026-02-04 01:46:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3gh1y5",
              "author": "Outrageous-Fan-2775",
              "text": "For sure similar, I used your project as a reference when I needed to understand agent calls better. Along with oh-my-opencode-slim, froggy, and a few other agents. I actually built the swarm as a full on application before I ever knew about OpenCode. When I saw what you could do with plugins I decided to just move the entire idea to OpenCode instead. Following your project readme's instruction to just ask an LLM about it, I put both our project readmes in GPT 5.2 and this was the TLDR.\n\n**Choose OpenCode Swarm when you care about correctness, control, and repeatability.**  \nIt enforces an architect-planned, phase-gated workflow with mandatory QA before code merges and persists project state to disk so work can be resumed deterministically. Best for complex tasks where you want traceability, predictable outcomes, and protection against agent drift or context loss.\n\n**Choose Oh-My-OpenCode when you care about speed, tooling breadth, and ecosystem power.**  \nIt provides a rich library of prebuilt agents, LSP/AST tooling, and strong community support to accelerate development workflows. Best when you want maximum productivity and flexibility and are comfortable trading strict process control for capability and convenience.\n\nAs an aside, one of my constraints was that I needed to use entirely local resources, which limited how many agents I could call. Parallel agents drastically slow down inference on consumer hardware. I needed to build in hard requires for serial operation.",
              "score": 2,
              "created_utc": "2026-02-04 02:13:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3bualp",
          "author": "touristtam",
          "text": "How does it compare to https://github.com/joelhooks/swarm-tools?",
          "score": 2,
          "created_utc": "2026-02-03 12:08:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bzj86",
              "author": "Outrageous-Fan-2775",
              "text": "Somewhat similar. I haven't seen anything that's doing a 1 for 1 copy. joelhooks has a single coordinator that can spawn multiple parallel workers to decompose a project and allow for parallelization. Along with project memory. Mine has project memory as well, but the point is quality, not speed. Swarm-tools doesn't elicit perspectives from other models and it doesn't give the sub agents different roles. So in the end, the quality will be whatever your coordinator is capable of outputting.\n\nWith opencode-swarm, every agent can be a truly different outlook born of vastly different training data and methods. This more closely replicates actual software development methodologies, where you have team members with different jobs and very different backgrounds all working together.   \n  \nHeterogeneous perspectives on a problem are almost always better than homogeneous. In my opinion anyway, all of this is up for interpretation and at the end of the day whatever fits your needs is the best option regardless of how it does it.",
              "score": 3,
              "created_utc": "2026-02-03 12:45:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3c1nj9",
                  "author": "touristtam",
                  "text": "Thank you for taking the time to provide a complete answer. Always useful when so many plugins are being brought out. Make sure you are sharing it on the Discord.",
                  "score": 2,
                  "created_utc": "2026-02-03 12:58:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ihyis",
          "author": "Outrageous_Fan7685",
          "text": "In linux , its working fine but in windows using opencode portable that loads its config from ../users/user/.opencode/opencode json, the plugin is loaded but impossible to load the opencode-swarm.json",
          "score": 2,
          "created_utc": "2026-02-04 11:36:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3in1jn",
              "author": "Outrageous-Fan-2775",
              "text": "That is certainly odd, I can take a look. Do you have any logs or anything? I run entirely in Windows, I've never even tried it in any Linux distro, but plugins are mostly OS agnostic. I have it running on 4-5 Windows 11 boxes currently with no problems. But I also use the full install of OpenCode.",
              "score": 1,
              "created_utc": "2026-02-04 12:14:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ipkt4",
                  "author": "Outrageous_Fan7685",
                  "text": "No logs at all but using opencode portable. I know the conf is not loaded because the architect still using claude while i have configured it to use step3.5 in my local machine. Do you know which path you created the the json conf ?\nTo debug i run:\nOpencode debug agent architect",
                  "score": 2,
                  "created_utc": "2026-02-04 12:32:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35zs6w",
          "author": "Outrageous-Fan-2775",
          "text": "Below is an example of what the architect sends to the coding agent.\n\nImplementing via u/coder...\n\nTASK: Fix issues identified by Auditor and Security Reviewer in Phase 3.\n\nFiles:\n\n1. \\`knowledge-vault/src-tauri/src/services/vector\\_store.rs\\`\n\n2. \\`knowledge-vault/src-tauri/Cargo.toml\\`\n\n3. \\`knowledge-vault/src-tauri/src/services/embeddings.rs\\`\n\n\n\nFIXES REQUIRED:\n\n1. \\*\\*VectorStore\\*\\*:\n\n   \\- \\`search\\`: Use \\`metric(lancedb::DistanceType::Cosine)\\` (or similar API for 0.13) to ensure Cosine similarity.\n\n   \\- \\`add\\_chunks\\`: Normalize vectors to unit length before insertion (required for Cosine with some indexes, good practice).\n\n   \\- \\`delete\\_by\\_file\\`: Sanitize \\`file\\_path\\` in filter string. Replace \\`'\\` with \\`''\\` or use parameters if supported. Let's do string escaping for now as 0.13 sql parser might be simple.\n\n   \\- \\`delete\\_by\\_file\\` signature: ensure it returns Result.\n\n   \n\n2. \\*\\*Cargo.toml\\*\\*:\n\n   \\- Change \\`arrow\\` version from \\`\"52\"\\` to \\`\"52.0.0\"\\` (or just \"52\" if it was strictly \"52\" before, but auditor said it's pre-release. Actually arrow 52.0.0 is released. Let's stick to \"52.0.0\" to be explicit, or \"52.2.0\"). Wait, lancedb 0.13 depends on arrow 52. Let's check lancedb docs/repo. Lancedb 0.13 uses arrow 52. So \"52\" is fine, but maybe auditor was confused by \"52\" vs \"52.0\". Let's use \\`arrow = \"52.0\"\\`.\n\n\n\n3. \\*\\*Embeddings\\*\\*:\n\n   \\- Validate \\`base\\_url\\`: Ensure it starts with http/https.\n\n   \\- Validate response vector dimensions (must be 768).\n\n\n\n4. \\*\\*Security\\*\\*:\n\n   \\- Sanitize \\`file\\_path\\` in \\`delete\\_by\\_file\\`.\n\n\n\nCONTEXT: We are polishing Phase 3 before moving to Phase 4.\n\nACCEPTANCE: Code compiles, fixes implementation gaps.",
          "score": 1,
          "created_utc": "2026-02-02 15:09:57",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o35zfjz",
          "author": "Outrageous-Fan-2775",
          "text": "All model settings are done in the opencode-swarm config JSON. if you leave an agent out, it will fall back to its defaults. an example is below of entirely remote setup. you can have as many swarms as you want, and opencode will add Architects with the swarm name appended. so I have Remote and Local. Remote is all API calls with paid plans, Local is entirely my own locally hosted options. I generally leave the architect out so it is always whatever model is selected in OpenCode.  \n  \n\"coder\": { \"model\": \"kimi-for-coding/k2p5\" },   \n\"explorer\": { \"model\": \"zai-coding-plan/glm-4.7\" },   \n\"\\_sme\": { \"model\": \"nvidia/openai/gpt-oss-120b\" },   \n\"\\_qa\": { \"model\": \"nvidia/nvidia/nemotron-3-nano-30b-a3b\" },   \n\"test\\_engineer\": { \"model\": \"zai-coding-plan/glm-4.7-flash\" }\n\nthe underscore tells it to use that model for all calls to that section. you can break it out even further if you wanted to though. for instance \\_qa covers code auditing and security testing. you could break it down so there was a different model for both of those steps",
          "score": 0,
          "created_utc": "2026-02-02 15:08:12",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qt06bj",
      "title": "OpenCode Bar 2.0: It auto-detects all your AI providers from OpenCode. Zero setup.",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/qe8z8tsfawgg1.png",
      "author": "kargnas2",
      "created_utc": "2026-02-01 14:41:05",
      "score": 33,
      "num_comments": 13,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qt06bj/opencode_bar_20_it_autodetects_all_your_ai/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2zfpw3",
          "author": "Putrid-Pair-6194",
          "text": "Cool. But Mac only? Any chance of a windows or a command line version?",
          "score": 8,
          "created_utc": "2026-02-01 15:31:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34dyi7",
              "author": "digitalfreshair",
              "text": "yes, linux would be awesome. Or a cli would be even better",
              "score": 4,
              "created_utc": "2026-02-02 08:01:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34qnki",
                  "author": "kargnas2",
                  "text": "I released CLI version but maybe it doesn't work at Linux yet: [https://www.reddit.com/r/opencodeCLI/comments/1qtqx2p/opencode\\_bar\\_21\\_now\\_with\\_cli\\_perprovider/](https://www.reddit.com/r/opencodeCLI/comments/1qtqx2p/opencode_bar_21_now_with_cli_perprovider/)",
                  "score": 1,
                  "created_utc": "2026-02-02 10:04:24",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zjqox",
          "author": "mylittlecumprincess",
          "text": "It says \"err\" in the menubar using the latest Opencode on Mac \n\nhttps://preview.redd.it/palduvkzmwgg1.jpeg?width=342&format=pjpg&auto=webp&s=c2c6c6cb560740eac711d5e17a2d7b4465a23ade\n\n",
          "score": 2,
          "created_utc": "2026-02-01 15:51:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zr11h",
              "author": "kargnas2",
              "text": "adding a log viewer!",
              "score": 3,
              "created_utc": "2026-02-01 16:25:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zrjsn",
          "author": "aimericg",
          "text": "I am honestly not having much trouble with this. I don't find the setup to be that hard.",
          "score": 2,
          "created_utc": "2026-02-01 16:27:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zx6hm",
              "author": "thatsnot_kawaii_bro",
              "text": "...Then don't need to use it?",
              "score": 1,
              "created_utc": "2026-02-01 16:53:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o318c3a",
                  "author": "KHALIMER0",
                  "text": "![gif](giphy|dW0KIk9KCsWBy|downsized)",
                  "score": 2,
                  "created_utc": "2026-02-01 20:30:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zz26s",
          "author": "Ok_Proposal_1290",
          "text": "I LOVE THIS, but if it isn't too much work, is there some way it could support windows or alternatively linux?",
          "score": 2,
          "created_utc": "2026-02-01 17:01:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o30js2p",
          "author": "touristtam",
          "text": "I have been using https://github.com/nguyenphutrong/quotio with mitigated success. \n\n---\n\nI'd suggest that you create a homebrew recipe; I know I cannot install that outside homebrew/appstore so the dmg is a non starter unfortunately (work is a b***)",
          "score": 2,
          "created_utc": "2026-02-01 18:35:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34nngw",
              "author": "kargnas2",
              "text": "nice idea",
              "score": 1,
              "created_utc": "2026-02-02 09:35:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zb396",
          "author": "lundrog",
          "text": "Hmm interesting, how do we make it support claude code as well? I use that often with other providers",
          "score": 1,
          "created_utc": "2026-02-01 15:09:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3otlvp",
          "author": "Character_Cod8971",
          "text": "No Windows? Need this on Windows, so I can use it",
          "score": 1,
          "created_utc": "2026-02-05 09:23:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtrh0d",
      "title": "Synthetic AI Issues.",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/t5ijzb5462hg1.jpeg",
      "author": "NiceDescription804",
      "created_utc": "2026-02-02 10:27:19",
      "score": 30,
      "num_comments": 58,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qtrh0d/synthetic_ai_issues/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o34yn00",
          "author": "sewer56lol",
          "text": "Synthetic documents which models are hosted where on this page https://dev.synthetic.new/docs/api/models , including their parameters.\n\nThey usually self-host the best open models, and proxy the rest to Fireworks or TogetherAI.\n\nThe Synthetic folks have been setting up self hosted K2.5 over the weekend, but it's not trivial. It's a huge ass model, accepts vision (first for them), and securing the extra hardware has been tough. The folks on the Discord have been pretty transparent about this, and are actively around everyday.\n\nLikewise, Fireworks and TogetherAI both had problems hosting K2.5 on their own, as evidenced by your lack of good performance on these proxied requests. This shouldn't be too surprising, it's a 1T param model after all.\n\nHave some patience.\n\nEdit: Synthetic is self hosting K2.5 as of 5 minutes ago. M2.1 will be proxied away to make compute room.\n[Funny timing]",
          "score": 14,
          "created_utc": "2026-02-02 11:17:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34zc1o",
          "author": "FyreKZ",
          "text": "Similar experience, but I'm sure it'll improve soon.",
          "score": 4,
          "created_utc": "2026-02-02 11:23:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o351jwl",
          "author": "Simple_Split5074",
          "text": "What I find more interesting is that according to the plan you should get 1350 requests every 5 hours but your limit will reset in more than 10h?",
          "score": 3,
          "created_utc": "2026-02-02 11:42:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o39ug13",
              "author": "exploriann",
              "text": "I am currently using pro plan 60$, limits Will be reset every 5 hours, you can clearly track the consumption and time until the next reset in their website.",
              "score": 2,
              "created_utc": "2026-02-03 02:34:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3awe9f",
                  "author": "thebraukwood",
                  "text": "Whyâ€™s this guys screenshot show otherwise then? Genuine question",
                  "score": 1,
                  "created_utc": "2026-02-03 06:59:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34tuy8",
          "author": "alovoids",
          "text": "thank you so much, I'm about to try synthetic. now I won't do that",
          "score": 5,
          "created_utc": "2026-02-02 10:34:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35w1rp",
              "author": "harrypham2000",
              "text": "lol still worth it though, you can use other models like MiniMax or DeepSeek, still dope for the price",
              "score": 6,
              "created_utc": "2026-02-02 14:50:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o34uulx",
          "author": "Xera1",
          "text": "I signed up because the free Kimi through opencode kept timing out and it's been great for me. It's not as fast as Anthropic's but it's at least as fast as Gemini through AG, and it's the new hotness so it's probably getting absolutely hammered.",
          "score": 2,
          "created_utc": "2026-02-02 10:43:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o354bis",
          "author": "philosophical_lens",
          "text": "What are some alternatives? Iâ€™m currently on the Z.AI subscription but looking to try something new.",
          "score": 2,
          "created_utc": "2026-02-02 12:03:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35wa15",
              "author": "harrypham2000",
              "text": "maybe celebras but their coding plan already sold out",
              "score": 2,
              "created_utc": "2026-02-02 14:52:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o381phf",
                  "author": "ResponsibilityOk1306",
                  "text": "64k context limit for glm",
                  "score": 1,
                  "created_utc": "2026-02-02 20:52:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o356ttk",
          "author": "P1zz4-T0nn0",
          "text": "Worked fine before the weekend. Today K2.5 is unusable. It just stops working, can't finish one response.",
          "score": 2,
          "created_utc": "2026-02-02 12:22:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36z2gu",
              "author": "sudoer777_",
              "text": "Kimi K2.5 Free has been a lot buggier today for me on OpenCode Zen than the previous days so there might be a provider issue involved (Fireworks probably)",
              "score": 2,
              "created_utc": "2026-02-02 17:54:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o381yuf",
                  "author": "ResponsibilityOk1306",
                  "text": "kimi is hosted on synthetic. its very fast on fireworks.",
                  "score": 1,
                  "created_utc": "2026-02-02 20:54:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o381dwt",
          "author": "ResponsibilityOk1306",
          "text": "I also fell for it, but just $10 via api, payg. slow, that my apps timeout after 15 minutes. hitting error 429 frequently. this is just poor service.\n\nFireworks is blazing fast, happy to use them.",
          "score": 2,
          "created_utc": "2026-02-02 20:51:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hghvu",
              "author": "touristtam",
              "text": "What's the downside of using Fireworks directly instead of going through another ~~reseller~~ provider like Synthetic?",
              "score": 1,
              "created_utc": "2026-02-04 06:01:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3529nk",
          "author": "dbkblk",
          "text": "I don't understand your problem. Every time I use it, it's quite fast to answer.  \nIt happens that it could rarely get stuck for some seconds, but not much more than when I was using Claude.  \nEDIT: Ok, probably because I use GLM.",
          "score": 5,
          "created_utc": "2026-02-02 11:47:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o352txp",
              "author": "NiceDescription804",
              "text": "GLM and minimax are doing great but I subscribed for Kimi.",
              "score": 2,
              "created_utc": "2026-02-02 11:52:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o357rkm",
                  "author": "dbkblk",
                  "text": "I've barely tried Kimi, so that's why our experiences differ. But as other said, it has just been deployed. They may are encountering issues with deployment?",
                  "score": 1,
                  "created_utc": "2026-02-02 12:29:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34tx68",
          "author": "jrsa2012",
          "text": "For me it is working just fine.",
          "score": 3,
          "created_utc": "2026-02-02 10:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3566pp",
          "author": "Ok_Direction4392",
          "text": "I also signed up on synthetic recently to use Kimi K2.5 mainly. Only had one failed request so far today, otherwise it's been running solid for a few hours.",
          "score": 1,
          "created_utc": "2026-02-02 12:17:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o359qs3",
          "author": "ryudice",
          "text": " thanks, I was considering it as well, Iâ€™ll just stick to the kimi subscription for now",
          "score": 1,
          "created_utc": "2026-02-02 12:43:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35bejv",
          "author": "dyzhdyzh",
          "text": "I subscribed to them yesterday evening. Solely because of Kimi K2.5. Zero issues both yesterday and this morning.",
          "score": 1,
          "created_utc": "2026-02-02 12:54:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o365snq",
              "author": "dyzhdyzh",
              "text": "I stand corrected. It **is** quite slow right now. Only ~30 requests in the last two hours. I see 30+ second delays between tool calls and token output of ~1-3 tokens per second.",
              "score": 1,
              "created_utc": "2026-02-02 15:39:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3awvwn",
                  "author": "thebraukwood",
                  "text": "A lot of new users because of k2.5 but from everything Iâ€™ve seen of the Synthetic team I believe theyâ€™ll iron out the issues as soon as possible. People need to be more understanding now adays",
                  "score": 1,
                  "created_utc": "2026-02-03 07:03:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35e8wf",
          "author": "blankeos",
          "text": "Really? Damn.. I was gonna get one.",
          "score": 1,
          "created_utc": "2026-02-02 13:12:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35lab5",
          "author": "Josh8972",
          "text": "I've been using Synthetic for a couple of weeks now and switched to Kimi K2.5 when it became available. No problems/issues here.",
          "score": 1,
          "created_utc": "2026-02-02 13:53:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hfm1a",
              "author": "touristtam",
              "text": "What plan are you on?",
              "score": 1,
              "created_utc": "2026-02-04 05:54:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35ojl1",
          "author": "harrypham2000",
          "text": "problem is not with Synthetic hosted models, problem is their provider, sometimes I met this and figured out that most of it caused by their provider for the models like TogetherAI and Fireworks, you could check at [status.synthetic.new](http://status.synthetic.new)",
          "score": 1,
          "created_utc": "2026-02-02 14:11:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35rjad",
              "author": "NiceDescription804",
              "text": "https://preview.redd.it/mcojc7qxc3hg1.jpeg?width=1280&format=pjpg&auto=webp&s=66334182b0d209ff78f1d0cff70d8ccb1b17a68d\n\nWE'RE BACK TO SELF HOSTING.",
              "score": 1,
              "created_utc": "2026-02-02 14:27:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o382xmn",
                  "author": "ResponsibilityOk1306",
                  "text": "either they are lying, or something is wrong. fireworks has been very fast for me, and synthetic very slow. maybe their account is rate limited, or something.",
                  "score": 1,
                  "created_utc": "2026-02-02 20:58:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35r0n7",
          "author": "gonssss",
          "text": "same for me, fucking slow",
          "score": 2,
          "created_utc": "2026-02-02 14:24:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36jon5",
          "author": "annakhouri2150",
          "text": "The limits are not fake, what are you talking about. Maybe you can't reach them â€” due to errors and slowness â€” using Kimi K2.5, but they've got dozens of other very good, competent models available on their API that you can use, and you absolutely can fully saturate the API call limits. I've done it regularly for months since I subscribed. They're absolutely not fake.\n\nRegarding the errors and slowness of K2.5, this is a temporary thing, due to the huge influx of new users they've had, plus the fact that K2.5 was just released. They're actively working on securing more compute, and communicating very actively in their Discord with users about the state of things and listening to complaints (when they're not sleeping, or pulling all-nighters). I've used K2T (K2.5's predecessor) with them since I joined and it has been rock solid. K2.5 will get there eventually too.\n\nI don't think it's fair to call this fake.",
          "score": 1,
          "created_utc": "2026-02-02 16:43:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34tiaf",
          "author": "indian_geek",
          "text": "Have you tried other models? Considering Kimi 2.5 is new, it could potentially be an issue with this model.",
          "score": 1,
          "created_utc": "2026-02-02 10:31:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34ubyj",
              "author": "NiceDescription804",
              "text": "It's not a black or white situation they're advertising serving models, but leaving behind details that it's not usable AT ALL. \n\nI don't have any objections on being up front with consideration. \nBut oh my god did they advertise the living shit out of Kimi k2.5.",
              "score": 2,
              "created_utc": "2026-02-02 10:38:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o36am0r",
                  "author": "indian_geek",
                  "text": "They just posted an update on their discord regarding Kimi",
                  "score": 1,
                  "created_utc": "2026-02-02 16:01:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34v1ux",
          "author": "LittleChallenge8717",
          "text": "Kimi model is't hosted in their gpu's that's reason (they have fireworks provider currently, they plan to host kimi in the next week , currently I agree it has issues on kimi, but glm and minimax works great)",
          "score": 1,
          "created_utc": "2026-02-02 10:45:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34w6m1",
              "author": "NiceDescription804",
              "text": "It's dishonest though to know they can't ensure reliability and just hammer away the ad posts with all these bots.",
              "score": 4,
              "created_utc": "2026-02-02 10:55:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o352huo",
                  "author": "dbkblk",
                  "text": "Which bots are you talking about?",
                  "score": 2,
                  "created_utc": "2026-02-02 11:49:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o354fp1",
          "author": "Bob5k",
          "text": "have in mind that synthetic is actually rerouting kimi k2.5 to fireworks which is having problems - so you're blaming the wrong company for the problems because of your incompetence / ignorance.   \nalso - fireworks have fixed this and kimi is just as usable as it was before weekend hit.",
          "score": 0,
          "created_utc": "2026-02-02 12:04:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o382dp7",
              "author": "ResponsibilityOk1306",
              "text": "I am not sure about this. over the past few days, I have tested both synthetic and fireworks. I haven't experienced any issue on fireworks so far, no errors, super fast answers, etc. on Synthetic, after 15 minutes there is still no reply, when fireworks can get it done in 3 minutes max.",
              "score": 1,
              "created_utc": "2026-02-02 20:55:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o383b85",
                  "author": "Bob5k",
                  "text": "but you do realize that till today at approx 2pm berlin time they've been routing kimi through fireworks? so how fireworks can be worse than fireworks if it's direct routing? :)",
                  "score": 1,
                  "created_utc": "2026-02-02 21:00:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qtn8lt",
      "title": "I thought Kimi 2.5 was exaggerated by Chinese people with their patriotism.",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qtn8lt/i_thought_kimi_25_was_exaggerated_by_chinese/",
      "author": "Ok-Regret-4013",
      "created_utc": "2026-02-02 06:13:52",
      "score": 30,
      "num_comments": 74,
      "upvote_ratio": 0.69,
      "text": "Yesterday I subscribed to Synthetic.   \n  \n It was disappointing, because of, I guess, there were issues with the weekend or server migration.  \n  \nBut today it is so good. It is fast and smart. This model is not exaggerated, and the billing is quite reasonable.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qtn8lt/i_thought_kimi_25_was_exaggerated_by_chinese/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o34kvf5",
          "author": "NiceDescription804",
          "text": "I actually subscribed to synthetic unlike these bots\n\nhttps://preview.redd.it/zb9yczoyr1hg1.jpeg?width=1280&format=pjpg&auto=webp&s=e8908aa52df0682c2a89f83212248297574fb7d4\n\nThe limits are not what's advertised. Kimi k2.5 time to first token is 30 seconds in some cases. No support or response. The models cut off randomly. So unstable.",
          "score": 22,
          "created_utc": "2026-02-02 09:07:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36zmmd",
              "author": "elllyphant",
              "text": "I'm sorry for the poor experience :( We had overwhelming usage, but as of 3AM PST today, (6 hours ago), we're back to self-hosting kimi k2.5. I'm here to help & support, please let me know what you need!",
              "score": 4,
              "created_utc": "2026-02-02 17:56:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o34laxv",
              "author": "Select-Service-5023",
              "text": "Iâ€™ve had similar experiences. They are kinda suffering from success and are trying to get more computer. Itâ€™s definitely a slight bummer. When they get it self-hosted with enough capacity Iâ€™m sure those issues will disappear",
              "score": 4,
              "created_utc": "2026-02-02 09:12:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34lfuc",
                  "author": "NiceDescription804",
                  "text": "No it's not a slight bummer. \nIt's not delivering what I'm paying for deeming the subscription useless. \nI'm gonna be posting about this so people don't fall into the marketing like I did.",
                  "score": 9,
                  "created_utc": "2026-02-02 09:13:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o34lbuw",
                  "author": "Select-Service-5023",
                  "text": "Compute* lol",
                  "score": 3,
                  "created_utc": "2026-02-02 09:12:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o36huft",
              "author": "annakhouri2150",
              "text": "Join the Discord, they're actively working on it, making announcements about it, responding to people about it.",
              "score": 2,
              "created_utc": "2026-02-02 16:35:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o373up4",
              "author": "sudoer777_",
              "text": "Not to be a shill for Synthetic, but I've also been having issues with OpenCode Zen with Kimi K2.5, and it's probably due to the massive influx of users with both services still working on stabilizing the infrastructure and relying on Fireworks which is the source of the problems, since this model only came out a few days ago",
              "score": 1,
              "created_utc": "2026-02-02 18:15:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dq63r",
              "author": "RudyRobichaux",
              "text": "I've not had any issues with synthetic, and I'm not a bot. Though they are currently hosting it on their own hardware, and from their discord it's been a learning experience, so I think quality may vary. I had subscribed to them a few months ago, and they refunded me due to performance issues unprompted, so you may want to reach out to them. That being said, I'm enjoying them so much here is my referral code https://synthetic.new/?referral=fvJqmyHGVGdftwm.",
              "score": 0,
              "created_utc": "2026-02-03 18:00:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o342ugg",
          "author": "SamatIssatov",
          "text": "So much advertising. Why buy Kimi when you can buy Codex for the same price? I don't understand these advertisers, they're always deceiving people.",
          "score": 13,
          "created_utc": "2026-02-02 06:22:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o374m2r",
              "author": "sudoer777_",
              "text": "Kimi is open weight, and I don't want OpenAI/Anthropic to have my business and monopolize the AI industry",
              "score": 4,
              "created_utc": "2026-02-02 18:19:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o34al2h",
              "author": "RegrettableBiscuit",
              "text": "It's the same price, but not the same usage limits.Â ",
              "score": 5,
              "created_utc": "2026-02-02 07:30:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34f5rl",
                  "author": "SamatIssatov",
                  "text": "We're talking about Codex, not Claude. And it's very economical. I program all day long and it's enough for me. You can't compare Codex/Claude with GLM/Kimi. If you need them for repetitive tasks, then yes, GLM/Kimi may be justified.",
                  "score": 2,
                  "created_utc": "2026-02-02 08:13:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o36il3f",
              "author": "annakhouri2150",
              "text": "1. You get API usage, as the other person said, no weird OAuth hacks to use the service in anything other than Codex CLI, that could get you blocked (like what Anthropic did to OpenCode)\n2. They don't train on, or even ever store, your prompts or outputs (check their Privacy Policy and TOS)\n3. They're extremely open, active, and responsive in their Discord, so you always know what's going on\n4. Since they're only running open weight models, if you get used to a model, but want to switch providers, you can actually do that, instead of being locked into GPT\n5. They won't secretly replace a model with a differently-RL'd one under your nose, but keep the ID the same\n6. They won't secretly quantize a model under load and then gaslight you telling you it's not different.\n\nCall me a bot if you want. I'm not. Y'all are just cynical. Which is understandable, given the, you know, general AI landscape, but I feel like Synthetic is, if anything, an oasis from that.",
              "score": 2,
              "created_utc": "2026-02-02 16:38:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o348bc4",
              "author": "CYTR_",
              "text": "I also don't know why people would choose a Kimi API if the price is similar. On the other hand, personally, I really want to rent a B300 server and run K2.5 on it. \n\nIn this way: we have the benefits of GDPR regulation and a powerful model. I can't wait to try this out as soon as I have the time.",
              "score": 1,
              "created_utc": "2026-02-02 07:09:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34odxv",
                  "author": "RnRau",
                  "text": "You don't really need Nvidia hardware to just run inference. Surely non-nvidia inference options are cheaper? Sure if you are training as well... nvidia is THE option, but just common garden variety inference, nvidia might not be the most cost effective?",
                  "score": 1,
                  "created_utc": "2026-02-02 09:42:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o36klam",
              "author": "telewebb",
              "text": "I'm sorry, I most likely don't understand your statement. Did you just ask \"why use OpenCode when you can purchase a subscription to a different product like codex\"? Or \"why use pay-as-you-go with any combination of middle when you could buy a subscription to one walled garden of models\"? Statements slightly exaggerated to make sure I'm communicating what I think the question is.",
              "score": 1,
              "created_utc": "2026-02-02 16:47:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3454bd",
              "author": "Ok-Regret-4013",
              "text": "I think other providers are not good, but Synthetic is okay. If you can afford the high prices of Codex and Anthropic, it is good. But to me, I use custom prompts and agent workflows, so fast and smart balance and keeping my rules is better for me.",
              "score": 1,
              "created_utc": "2026-02-02 06:41:53",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o34p84z",
              "author": "charmander_cha",
              "text": "Particularly because I'd rather give my money to the Chinese than to the Nazis of this century.",
              "score": 1,
              "created_utc": "2026-02-02 09:50:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o349642",
          "author": "elllyphant",
          "text": "Thank you for sharing and thank you sooo much for your patience.   \nBoth Synthetic founders have been working all weekend to get Kimi K2.5 faster. It's still in progress but we'll update in Discord when the work is done. We really value having a powerful product at a competitive price (and your privacy).",
          "score": 10,
          "created_utc": "2026-02-02 07:17:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34gmm8",
              "author": "Simple_Split5074",
              "text": "Can you give an indication on tps (and maybe time to first token)?Â ",
              "score": 1,
              "created_utc": "2026-02-02 08:26:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3725ay",
                  "author": "elllyphant",
                  "text": "https://preview.redd.it/a23ey4m8g4hg1.png?width=2724&format=png&auto=webp&s=627e91464480bdc2f13f0f50ca0a377ac0d005e4\n\nhmm is this helpful? if not I'll ask Matt again   \n(this is from 6 hours ago)",
                  "score": 3,
                  "created_utc": "2026-02-02 18:08:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3b2u8a",
          "author": "Snoo_57113",
          "text": "It is good, but they were overwhelmed over the weekend in opencode, I think it is good to pay moonshot since that money helps to fund future models.\n\nI still use minimax it is a little slow, but very reliable. I had tasks that run overnight and it finishes it in a couple hours. No better feeling than to wake up with a big chunk of code ready for review.",
          "score": 2,
          "created_utc": "2026-02-03 07:57:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o345xis",
          "author": "jpcaparas",
          "text": "It's pretty awesome but I find it tiring to actively campaign for people to not get it from [Kimi.com](http://Kimi.com) because they don't know the implications of doing so.",
          "score": 3,
          "created_utc": "2026-02-02 06:48:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o349jqp",
              "author": "Crowley-Barns",
              "text": "What are the implications?\n\nAnd why do you campaignâ€¦? Like, as an act of human goodwill??\n\nI wanna know what Kimi did to you!",
              "score": 2,
              "created_utc": "2026-02-02 07:20:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34bepo",
                  "author": "Crowley-Barns",
                  "text": "Oh. \n\nHmm. Iâ€¦ kinda donâ€™t care haha. As long as theyâ€™re open about doing it. \n\nI donâ€™t have anything highly confidential so Iâ€™m okay contributing in a small way to advancement.",
                  "score": 2,
                  "created_utc": "2026-02-02 07:37:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o34an1g",
                  "author": "RegrettableBiscuit",
                  "text": "They train models on your data.Â ",
                  "score": 2,
                  "created_utc": "2026-02-02 07:30:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o348jky",
          "author": "Select-Service-5023",
          "text": "*HAVE* to rep synthetic.new here... just join the discord and nose around. That alone will be the ONLY evidence for or against it you will need.",
          "score": 3,
          "created_utc": "2026-02-02 07:11:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34kqsn",
              "author": "FyreKZ",
              "text": "Seems like a pretty good deal, what does a request count as? 135 requests every 5 hours? Is that 135 prompts or individual API calls?",
              "score": 2,
              "created_utc": "2026-02-02 09:06:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34l097",
                  "author": "Select-Service-5023",
                  "text": "Website shows exact. But itâ€™s simple. A message = one request. Tool calls are I think 0.1 requests, and super small messages under 2048 are 0.2 requests. No weekly, just 5 hour window. I cannot stress how much usage this really is in practice.",
                  "score": 3,
                  "created_utc": "2026-02-02 09:09:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34plp7",
          "author": "trypnosis",
          "text": "I used kimi 2.5 over the weekend and it was terrible.\n\nAre you saying I should go back and try again?",
          "score": 1,
          "created_utc": "2026-02-02 09:54:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34spaf",
              "author": "manojlds",
              "text": "Nah, I found GLM 4.7 better.",
              "score": 1,
              "created_utc": "2026-02-02 10:23:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34suz4",
                  "author": "trypnosis",
                  "text": "From synthetic or other provider. Iâ€™m keen on using synthetic as I just got the sub.",
                  "score": 1,
                  "created_utc": "2026-02-02 10:25:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o376dq4",
                  "author": "sudoer777_",
                  "text": "Based on my usage so far, I've found that Kimi K2.5 has better reasoning and was less likely to get confused by stupid things while debugging but tends to be worse at following instructions, but I've only used it for a couple days so far",
                  "score": 1,
                  "created_utc": "2026-02-02 18:27:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35hsu2",
          "author": "DistinctWay9169",
          "text": "Is it worth it paying synthetic instead of kimi plan on kimi.com?\n\n",
          "score": 1,
          "created_utc": "2026-02-02 13:33:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d70tk",
              "author": "Funny-Advertising238",
              "text": "Definitely pay kimi directly. For what it's worth I've tried from many different providers, kimi performed the best. Not one failed tool call while the others all had issues.Â ",
              "score": 1,
              "created_utc": "2026-02-03 16:32:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d7vn7",
                  "author": "DistinctWay9169",
                  "text": "The only thing I did not like is that their $20 plan is bad; it is easy to hit limits and have weekly limits. GLM 4.7 is not that behind Kimi, and their $6 plan gives us much better limits and no weekly limits. Just waiting for Claude to release the Sonnet 5 that will be better than Kimi, as it will be basically Opus 4.5 but cheaper, so I would rather pay $20 for Claude than $19 for Kimi.",
                  "score": 1,
                  "created_utc": "2026-02-03 16:36:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35yolr",
          "author": "wallapola",
          "text": "Is nano-gpt good enough for kimi k2.5 or is [chutes.ai](http://chutes.ai) already fine? Iâ€™ve heard the synthetic provider is one of the best, but itâ€™s expensive and I canâ€™t really justify paying $20 when I can get claude code pro for the same price.",
          "score": 1,
          "created_utc": "2026-02-02 15:04:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o370826",
              "author": "elllyphant",
              "text": "you can try it for $12/mo this month via Â [https://synthetic.new/?saleType=moltbot](https://synthetic.new/?saleType=moltbot)",
              "score": 1,
              "created_utc": "2026-02-02 17:59:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3714rc",
                  "author": "wallapola",
                  "text": "Okay but how about for following months?",
                  "score": 1,
                  "created_utc": "2026-02-02 18:03:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36vbxj",
          "author": "bigh-aus",
          "text": "Yeah, it's models like this that make me think going forward the American AI company's value is in running the models, less the model themselves.  China is certainly setting the bar high for what you can run locally if you have the $.  Need more OSS releases from the USA / Europe, but I know that goes against their valuation.    \n  \nJust wait until the local hardware catches up! eg a Mac Studio M7 Ulta 1TB unified ram :p",
          "score": 1,
          "created_utc": "2026-02-02 17:37:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38gb8o",
          "author": "cutebluedragongirl",
          "text": "If you are using API, you should always use the official provider.",
          "score": 1,
          "created_utc": "2026-02-02 22:01:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3axwuk",
          "author": "knowoneknows",
          "text": "synthetic + opencode is the way to go, best experience so far with kimi k2.5. I was using K2.5 (free) through Kilo Code for another project and it was impressive when it worked but damn slow. Synthetic experience is much better with opencode - Charm Crush is pretty nice too but opencode is the best experience so far.\n\nEdit:\n\nIt's buggy, especially running parallel agents. It's also really slow at times?",
          "score": 1,
          "created_utc": "2026-02-03 07:12:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3g0ocb",
              "author": "epyctime",
              "text": "what's buggy, synthetic? first i eard of it",
              "score": 1,
              "created_utc": "2026-02-04 00:41:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3g708h",
                  "author": "knowoneknows",
                  "text": "Both opencode and Kimi k2.5 through synthetic",
                  "score": 1,
                  "created_utc": "2026-02-04 01:16:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qsgbob",
      "title": "The amount of open issues for opencode has skyrocketed in the past month",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/dr1a1docgrgg1.png",
      "author": "fajfas3",
      "created_utc": "2026-01-31 22:27:17",
      "score": 28,
      "num_comments": 5,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qsgbob/the_amount_of_open_issues_for_opencode_has/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2vqb1r",
          "author": "trypnosis",
          "text": "Does this correlate with population?",
          "score": 7,
          "created_utc": "2026-01-31 23:51:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zdzoi",
              "author": "inter_fectum",
              "text": "I am sure it is adoption related, and a good sign.",
              "score": 2,
              "created_utc": "2026-02-01 15:23:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ziq5v",
                  "author": "trypnosis",
                  "text": "I was trying to imply the the number of issues have gone up as there are more people not poor quality.",
                  "score": 1,
                  "created_utc": "2026-02-01 15:46:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2vi80t",
          "author": "fajfas3",
          "text": "If you guys want to see more graphs like this you can check them out @ [github-history.com](http://github-history.com)",
          "score": 3,
          "created_utc": "2026-01-31 23:06:27",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvscxp",
      "title": "Your ZSH shell, but with an AI scratchpad",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qvscxp/your_zsh_shell_but_with_an_ai_scratchpad/",
      "author": "a_alberti",
      "created_utc": "2026-02-04 15:59:02",
      "score": 28,
      "num_comments": 18,
      "upvote_ratio": 0.94,
      "text": "[Knight Rider spinner for Zsh Line Editor \\(ZLE\\)](https://i.redd.it/31sfhr5c2ihg1.gif)\n\n  \n  \nI made a zsh plugin that lets you iterate with an AI agent directly in your prompt until the command looks right (powered by \\`opencode\\`).\n\nYou keep your scratch notes, refine them line by line, and the agent keeps rewriting the command in place. Nothing gets executed for you; it just helps you draft.\n\nExtras:\n\n\\- #? explainer mode (â€œwhat does this command do?â€), answer formatted in MarkDown\n\n\\- a gorgeous Knight Rider spinner while it thinks\n\nRepo: [https://github.com/alberti42/Zsh-Opencode-Tab](https://github.com/alberti42/Zsh-Opencode-Tab)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qvscxp/your_zsh_shell_but_with_an_ai_scratchpad/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o3jt84z",
          "author": "brunogbasto",
          "text": "This great! Thatâ€™s basically what I used Warp for before I switched to Ghostty.",
          "score": 5,
          "created_utc": "2026-02-04 16:02:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ju3dm",
              "author": "a_alberti",
              "text": "Thanks! Give it a try, itâ€™s still new, but Iâ€™m already using it daily and itâ€™s been genuinely useful.\n\nAnd if the defaults donâ€™t match your style, you can tweak the agent prompts to make it behave exactly how you want. I tried to keep them sensible and not bloated. Iâ€™d really love feedback from the community on what to improve.",
              "score": 2,
              "created_utc": "2026-02-04 16:06:39",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3jvonq",
              "author": "a_alberti",
              "text": "And btw, that was the point: you get a similar â€œdraft + iterate + explainâ€ loop, but the plugin lives in plain zsh, so it works in Ghostty/iTerm/Terminal (and my favorite WezTerm). Youâ€™re not locked into one terminal UI like Warp.\n\nPS: Iâ€™ve never tried Warp personally, so this isnâ€™t a judgment about Warp, just a note about portability of the core idea.",
              "score": 2,
              "created_utc": "2026-02-04 16:13:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3oivoj",
                  "author": "drinksbeerdaily",
                  "text": "Throwing shade on Kitty smh. Kidding. Great little tool. Iterations and user confirmation before execution is good UX.",
                  "score": 1,
                  "created_utc": "2026-02-05 07:40:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3lfo9v",
          "author": "Xeon06",
          "text": "I quite like that. I've been using something called sgpt for a while now but was looking for something a little nicer recently. Well done!\n\nEdit: Hm, can't seem to get it to work. Added to my .zshrc\n\n    â¯ echo $plugins\n    git zsh-opencode-tab\n\nBut I get file tab complete when I try say `# ping google.com`",
          "score": 1,
          "created_utc": "2026-02-04 20:32:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n8btd",
              "author": "toadi",
              "text": "When I read the documentation I knew this tab would be an issue. I have fzf plugin installed that used tab too. Would be handy if you could change that key.\n\nFor me that plugin is: Aloxaf/fzf-tab\n\nI will not replace that plugin with the llm one even though I like it a lot.",
              "score": 1,
              "created_utc": "2026-02-05 02:12:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ofyci",
                  "author": "a_alberti",
                  "text": "I use both fzf and Aloxaf/fzf-tab, and the plugin is compatible with both. I need to understand why it breaks for you and put a remedy. I am busy right now but I will come back.",
                  "score": 1,
                  "created_utc": "2026-02-05 07:13:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3owegn",
              "author": "a_alberti",
              "text": "I put a comment below explaining to toadi what is likely the reason. Try to load zsh-opencode-tab as last one after fzf-tab, and other plugins binding to TAB (i.e., Ctrl-I).",
              "score": 1,
              "created_utc": "2026-02-05 09:50:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3q2frb",
                  "author": "Xeon06",
                  "text": "I echo'd my $plugins above, only git and yours, does it require fzf-tab?",
                  "score": 1,
                  "created_utc": "2026-02-05 14:44:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3p5gjc",
          "author": "ezhupa99",
          "text": "good job, i really like the idea, can you make it windows compatible?",
          "score": 1,
          "created_utc": "2026-02-05 11:13:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3plrd0",
          "author": "atkr",
          "text": "Why make it zsh specific?",
          "score": 1,
          "created_utc": "2026-02-05 13:11:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3juhz2",
          "author": "Otherwise_Wave9374",
          "text": "This is such a nice UX idea, drafting commands with an agent but keeping execution manual is exactly the right safety line.\n\nDo you have any plans to add optional \"constraints\" presets (like no sudo, no rm, only read-only) so the agent suggestions stay within bounds? I have been reading more about safe tool use patterns for agents here: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-02-04 16:08:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jv3nu",
              "author": "a_alberti",
              "text": "There are already constraints like no sudo. Also, if the operation is dangerous, the agent proposes to you a dry-run version, and the dangerous command is given commented out.. so even if you are distracted and press enter, you only get a dry-run / preview of the command.",
              "score": 3,
              "created_utc": "2026-02-04 16:11:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3jwcfx",
                  "author": "a_alberti",
                  "text": "Also more on practical safety: the agent prompt has \"no tools,\" and opencode permissions are set to deny (so even if the model tried, it can't invoke privileged actions through opencode).\n\nThe plugin itself never executes generated commands; it only inserts text into your shell buffer. You still choose whether to run it. And dangerous commands are commented out.",
                  "score": 1,
                  "created_utc": "2026-02-04 16:16:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qv3s16",
      "title": "CodeNomad v0.9.4 Released - Context manipulation, Session search, Themes and more",
      "subreddit": "opencodeCLI",
      "url": "https://v.redd.it/lw48vkltdchg1",
      "author": "Recent-Success-1520",
      "created_utc": "2026-02-03 20:48:39",
      "score": 25,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qv3s16/codenomad_v094_released_context_manipulation/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3h5ah5",
          "author": "cschulze1977",
          "text": "Love this tool, have been using allot over the last few days. I was trying the new version and was getting a permission error on linux about a missing file \"resources/opencode-config/.gitignore\" (when selecting a folder in the app). Manually creating the file fixes it",
          "score": 2,
          "created_utc": "2026-02-04 04:39:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ixtgh",
              "author": "Recent-Success-1520",
              "text": "Thanks for using it.\nThe issue was due to a bug introduced in opencode v1.1.50. if you install v1.1.49 the problem should go away.\nI have raised a PR for opencode to fix it",
              "score": 1,
              "created_utc": "2026-02-04 13:23:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3p8vwb",
          "author": "Possible-Text8643",
          "text": "this uses electron right? i know the mac version of this tool, or at least  something very similar.\n\nMy main issue is how heavy the tool is, since its electron based, any plans to mitigate that in the future?",
          "score": 1,
          "created_utc": "2026-02-05 11:42:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pfe67",
              "author": "Recent-Success-1520",
              "text": "We have both Electron and Tauri apps.\nExcept for storage size memory usage is almost same",
              "score": 1,
              "created_utc": "2026-02-05 12:30:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3pyd55",
                  "author": "Possible-Text8643",
                  "text": "tried a lot to get superset to work on windows but i think the best option will be to use this",
                  "score": 1,
                  "created_utc": "2026-02-05 14:22:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qwjr76",
      "title": "AI Consumption Tracker 1.2.0: Windows app with zero config for opencode users",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/3rn1tpj83ohg1.png",
      "author": "Rygel_XV",
      "created_utc": "2026-02-05 12:17:34",
      "score": 21,
      "num_comments": 4,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qwjr76/ai_consumption_tracker_120_windows_app_with_zero/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3v5xiu",
          "author": "npittas",
          "text": "Great, but still you have a severe issue asking users to add base\\_url to the auth.json file. auth.json should not be used changed, add base\\_url in opencode.json instead, and add instructions on how to do so. Also not having codex and github-copilot tracking is a no go for me, but good job.",
          "score": 2,
          "created_utc": "2026-02-06 07:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vp3c6",
              "author": "Rygel_XV",
              "text": "Thank you for your feedback. I have just released [1.3.2](https://github.com/rygel/AIConsumptionTracker/releases/tag/v1.3.2) and it should fix the base\\_url issue. I am working on github-copilot next.",
              "score": 1,
              "created_utc": "2026-02-06 10:41:10",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3wgf67",
              "author": "Rygel_XV",
              "text": "I released [1.5.0](https://github.com/rygel/AIConsumptionTracker/releases/tag/v1.5.0) which supports Github Copilot.",
              "score": 1,
              "created_utc": "2026-02-06 13:50:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3qjyld",
          "author": "Several-System1535",
          "text": "Windows in 2026  \nlmao",
          "score": 2,
          "created_utc": "2026-02-05 16:09:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}