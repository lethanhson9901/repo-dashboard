{
  "metadata": {
    "last_updated": "2026-02-03 17:18:12",
    "time_filter": "week",
    "subreddit": "opencodeCLI",
    "total_items": 20,
    "total_comments": 251,
    "file_size_bytes": 288866
  },
  "items": [
    {
      "id": "1qrjgfn",
      "title": "Opencode v1.1.47 and auto updates",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/9l7wwnsu6kgg1.png",
      "author": "pi314ever",
      "created_utc": "2026-01-30 21:59:22",
      "score": 181,
      "num_comments": 25,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qrjgfn/opencode_v1147_and_auto_updates/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2qgk51",
          "author": "philosophical_lens",
          "text": "I think they should split into two releases - main and dev. Their current high velocity releases should stay on the dev branch, and they should also offer a main branch which lags behind by a week or so until it‚Äôs confirmed stable.",
          "score": 25,
          "created_utc": "2026-01-31 04:12:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rgd4r",
              "author": "Michaeli_Starky",
              "text": "That's a no-brainer for anyone who had been doing high velocity software development. It puzzles me how it was not a thing for CC until like a month ago and not a thing for OC.",
              "score": 9,
              "created_utc": "2026-01-31 09:11:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2t7g0y",
                  "author": "Cast_Iron_Skillet",
                  "text": "I have enjoyed this on a few projects like cursor and comma ai sunnypilot. Nice to be able to see where things are headed, knowing risk of bugs, and to have peace of mind knowing you can revert to stable at any point.",
                  "score": 2,
                  "created_utc": "2026-01-31 16:23:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2pgo7m",
          "author": "MySkadi",
          "text": "I understand your feeling, i was a victim of 1.1.37 version bug where every tool call and subagent activities does cost me my copilot premium request, which reduce all of my 300 premium request at once, fortunately at least the objective is achieved, but at what cost..\n\nYou can turn off the autoupdate from global opencode.json config",
          "score": 21,
          "created_utc": "2026-01-31 00:36:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2widz4",
              "author": "throwaway12012024",
              "text": "where? my global opencode.json doenst have anything about autoupdate",
              "score": 1,
              "created_utc": "2026-02-01 02:35:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2xdef8",
              "author": "Remarkable_Week_2938",
              "text": "Is this issue fixed. I got the same and now my premium is refilled to 300 but dare not try to run copilot models again..",
              "score": 1,
              "created_utc": "2026-02-01 06:05:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2xe7qt",
                  "author": "MySkadi",
                  "text": "It is fixed now so you dont need to worry, i already tried it\n\nAs for the autoupdate see the config at https://opencode.ai/config.json",
                  "score": 1,
                  "created_utc": "2026-02-01 06:11:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ors93",
          "author": "Lyuseefur",
          "text": "![gif](giphy|iFnBpFxFGetn8BH0vZ)",
          "score": 9,
          "created_utc": "2026-01-30 22:22:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pqx38",
          "author": "Psidium",
          "text": "You shouldn‚Äôt be running any ai coding tools barebones anyway. Create a sandbox and let it lose there. The models themselves can hallucinate dangerous commands, it‚Äôs just inherent to the medium.",
          "score": 4,
          "created_utc": "2026-01-31 01:35:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2r5b7p",
              "author": "gbladeCL",
              "text": "Is there a recommended sandbox? I am looking at opencode-devcontainers",
              "score": 1,
              "created_utc": "2026-01-31 07:27:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ucuv9",
                  "author": "Psidium",
                  "text": "I‚Äôve created one myself based on the Claude code devcontainer that anthropic provides on their docs",
                  "score": 2,
                  "created_utc": "2026-01-31 19:39:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2prrjd",
              "author": "pi314ever",
              "text": "While I agree with that and do sandboxing, the issue is that the vast majority of vulnerable users will probably not look that far into it. The people who don't know about the risks of auto updates are likely the same people who aren't aware of sandboxing as best practice.",
              "score": 0,
              "created_utc": "2026-01-31 01:40:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2oou60",
          "author": "Historical-Internal3",
          "text": "![gif](giphy|fT3OUK1DTJAI76ZF0i)",
          "score": 4,
          "created_utc": "2026-01-30 22:08:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2orr0x",
          "author": "Heavy-Focus-1964",
          "text": "most likely passed an empty string in to the release message generator because there were no commit hashes produced. harmless edge case. \n\nif this is enough to rattle your confidence maybe the breakneck speed and reckless abandon of AI programming is not for you",
          "score": 2,
          "created_utc": "2026-01-30 22:22:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p8ud5",
              "author": "carlanwray",
              "text": "Right? If it doesn't reseamble a seive, leaking everything everywhere it's too old school. üòÑ",
              "score": 2,
              "created_utc": "2026-01-30 23:53:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2procs",
          "author": "mrpoopybruh",
          "text": "like just use it in a sandbox like ya supposed to!",
          "score": 1,
          "created_utc": "2026-01-31 01:39:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qtk0m",
              "author": "ProfessionNo3952",
              "text": "Could you tell please in which way?",
              "score": 1,
              "created_utc": "2026-01-31 05:46:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2r62p6",
                  "author": "RegrettableBiscuit",
                  "text": "Docker is a good option.¬†",
                  "score": -1,
                  "created_utc": "2026-01-31 07:34:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ufzgc",
          "author": "morglod",
          "text": "Imagine people in 2026 could not make simple chat with single peer without bugs",
          "score": 1,
          "created_utc": "2026-01-31 19:55:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p5qqu",
          "author": "alovoids",
          "text": "lol",
          "score": 1,
          "created_utc": "2026-01-30 23:36:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qb1fq",
          "author": "Ok_Road_8710",
          "text": "The default settings just let the agent rm rf your entire PC, so",
          "score": 0,
          "created_utc": "2026-01-31 03:36:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pcq5i",
          "author": "doodirock",
          "text": "Dude relax",
          "score": -3,
          "created_utc": "2026-01-31 00:15:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pm9hz",
          "author": "neamtuu",
          "text": "Clown. What are you afraid of? Check the files for yourself if you think of a security breach and come up with a conclusion. Stop assuming uncertain checkable realities.",
          "score": -11,
          "created_utc": "2026-01-31 01:08:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr6u36",
      "title": "I tried Kimi K2.5 with OpenCode it's really good",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qr6u36/i_tried_kimi_k25_with_opencode_its_really_good/",
      "author": "orucreiss",
      "created_utc": "2026-01-30 14:24:39",
      "score": 113,
      "num_comments": 60,
      "upvote_ratio": 0.96,
      "text": "Been testing Kimi For Coding (K2.5) with OpenCode and I am impressed. The model handles code really well and the context window is massive (262K tokens).\n\nIt actually solved a problem I could not get Opus 4.5 to solve which surprised me.\n\nHere is my working config: [https://gist.github.com/OmerFarukOruc/26262e9c883b3c2310c507fdf12142f4](https://gist.github.com/OmerFarukOruc/26262e9c883b3c2310c507fdf12142f4)\n\n# Important fix\n\nIf you get thinking is enabled but reasoning\\_content is missing - the key is adding the interleaved option with \"field\": \"reasoning\\_content\". That's what makes it work.\n\nHappy to help if anyone has questions!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qr6u36/i_tried_kimi_k25_with_opencode_its_really_good/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2mllgx",
          "author": "RegrettableBiscuit",
          "text": "The more I use it, the more impressed I am. GLM 4.7 seemed good initially, but as I kept using it, I noticed issues with more complex tasks. But if you put K2.5 and Sonnet 4.5 in front of me and asked me to tell which is which based on how well they work, I probably would need a bit of time to figure it out, if I could at all.¬†",
          "score": 23,
          "created_utc": "2026-01-30 16:25:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mrtkx",
              "author": "Grand-Management657",
              "text": "Right now I am not doing super complex tasks, mostly telling K2.5 to convert an old wordpress site to modern static site. It handles it very well. I spun up 18 subagents at once to do each page individually and it executed without any errors. Did a much better job at the UI than GLM 4.7 IMO. But GLM's UI design was never its strong suite. With better prompting and skills, I probably wouldn't notice much of a difference between K2.5 and GLM 4.7. But at the same time K2.5's raw intelligence due to its training and parameter size just makes it so much smarter IMO.",
              "score": 6,
              "created_utc": "2026-01-30 16:52:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rofo9",
                  "author": "erranticus",
                  "text": "Hi, how to do for use sub agents in paralel?",
                  "score": 1,
                  "created_utc": "2026-01-31 10:28:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o36hccb",
                  "author": "xapep",
                  "text": "I do wonder; Which coding tools are you using LLMs or CLI only?",
                  "score": 1,
                  "created_utc": "2026-02-02 16:32:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2u5ll9",
              "author": "jmhunter",
              "text": "I feel like GLM's issue is they seemed to have buried their servers with the coder plan",
              "score": 2,
              "created_utc": "2026-01-31 19:05:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2mqbzb",
          "author": "epicfilemcnulty",
          "text": "Lots of folks praising this model, and I guess it does deliver for their use cases (particularly, I'd assume that it should be good for TS/JS and Python coding), but I've tried it several times with my codebase, which is C + Lua mix and pretty complex, and while it usually comes up with a pretty decent plan, but the execution is bad -- it looses focus, it changes function signatures but forgets to update the invocation calls, and so on. Opus nails the same task with the same prompt. But it is really fast, that's true.",
          "score": 7,
          "created_utc": "2026-01-30 16:46:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2msd91",
              "author": "Grand-Management657",
              "text": "Exactly you hit the nail on the head. I found it very good in TS/JS environements but I hear reviews from those who use it for other languages or libraries and it falls short. Have you tried to use Opus as your planner and K2.5 as your executor? I am curious if that would yield better results for you.",
              "score": 5,
              "created_utc": "2026-01-30 16:55:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2n2qqa",
                  "author": "epicfilemcnulty",
                  "text": "Have not tried this approach yet, will give it a shot. I'd very much love to improve its performance on my codebase, because it's much cheaper than Opus, it's fast and it's open weights.",
                  "score": 2,
                  "created_utc": "2026-01-30 17:41:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2u3ds4",
                  "author": "zarrasvand",
                  "text": "Got any experience on how it handles Rust and Go?\n\nAnd html/css?",
                  "score": 1,
                  "created_utc": "2026-01-31 18:54:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2p5xuj",
              "author": "Federal-Initiative18",
              "text": "I have been using it with C# mainly with no issues and the code looks much better than Sonnet 4.5",
              "score": 6,
              "created_utc": "2026-01-30 23:37:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2oj4jg",
              "author": "thatsnot_kawaii_bro",
              "text": "It's the usual cycle:\n\n*Hype up model X as the second coming of christ. Say it's the real deal compared to previous models*\n\nWeeks/months later:\n\n*Hype up new model as the second coming of christ, say that X was overhyped but this is the real deal*",
              "score": 5,
              "created_utc": "2026-01-30 21:40:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ofsxf",
              "author": "frasiersbrotherniles",
              "text": "I know benchmarking is kind of broken but it would be very interesting to see a rating of each model's competency at different languages. Do you know if anyone tries to evaluate that?",
              "score": 2,
              "created_utc": "2026-01-30 21:25:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2olyfo",
                  "author": "epicfilemcnulty",
                  "text": "No, unfortunately, I don't know if anyone is working on that. I'd be very interested to see it, though, but I think it's not a trivial task to do, if we are talking about a thorough benchmark -- last time I looked at some of python benchmarks I was not impressed at all, usually it's just a set of one-shot tasks. On one hand, it does make sense -- if you ask a model to create a function that does X, you can actually verify if the implementation is correct. But it's much harder to create a benchmark that would include complex tasks like code refactoring involving multiple files -- particularly when it comes to assessing the results... But I was not actually following this benchmarking area lately, maybe there is something like this already... My approach is empirical -- I just try different models with my real codebase and see how they perform. But of course that is not a \"real\" benchmarking.",
                  "score": 2,
                  "created_utc": "2026-01-30 21:54:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2n6299",
          "author": "jmhunter",
          "text": "I think it's really great that OpenCode was able to get it for free for a period for us.   \n  \nSo far it works fairly well, but it seems to kind of fizzle after one task, it reminds me of Sonnet 3.5. You will definitely have to keep an eye on your task management. It does not seem to have its own. We probably need a good agent harness/opening prompt/system prompt for this?   \n  \nI have not tried it with something like Beads and see if it can keep an eye on that. But it does actively engage with Serena it seems to be fairly good at recognizing tools and utilizing them.   \n  \nI made a video about some changes I made on a personal use project and it did an OK job but now that I've messed with it some more and done some IT tasks with it I recognize that it kind of fizzles after one task and comes back to the user. I'd be curious to hear from people who use hooks like Ralph Wiggum.\n\n[https://youtu.be/vWylCQtQ1Bs?si=2VqriQL\\_yMlNKJ1c](https://youtu.be/vWylCQtQ1Bs?si=2VqriQL_yMlNKJ1c)",
          "score": 4,
          "created_utc": "2026-01-30 17:56:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ng6so",
          "author": "Visual_Weather_7937",
          "text": "Hello! I can't understand: why do I need such a config if I can simply choose from the list of Kimi 2.5 models in OC?",
          "score": 4,
          "created_utc": "2026-01-30 18:40:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2njg7t",
              "author": "orucreiss",
              "text": "its because i am using [https://github.com/code-yeongyu/oh-my-opencode](https://github.com/code-yeongyu/oh-my-opencode) and i want to customize an agent (Atlas) to use the model. ",
              "score": 0,
              "created_utc": "2026-01-30 18:54:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2mjvg4",
          "author": "xmnstr",
          "text": "I have the same experience, very impressed! Got the $20 subscription for $3.49 and cancelled my Cursor subscription immediately. This is so much better, and the limits are insane. I can't get over how fast it is!",
          "score": 8,
          "created_utc": "2026-01-30 16:17:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2nlvdg",
              "author": "MarvNC",
              "text": "If you have a lot of time on your hands you can get it to $0.99. Pretty fun honestly.",
              "score": 2,
              "created_utc": "2026-01-30 19:05:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pjomh",
                  "author": "xmnstr",
                  "text": "Well I guess I need to start new conversations. Mine hit a point where I needed to share to my socials to get it lower. Not worth it.",
                  "score": 1,
                  "created_utc": "2026-01-31 00:53:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2u4k3d",
                  "author": "Pleasant_Thing_2874",
                  "text": "I just had codex talk with it.  Managed to get it down to 1.99 before demanding I share it first",
                  "score": 1,
                  "created_utc": "2026-01-31 19:00:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2w4gzq",
                  "author": "LEO-PomPui-Katoey",
                  "text": "Kilo Code has it available for free now",
                  "score": 1,
                  "created_utc": "2026-02-01 01:11:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2mnc33",
              "author": "bigh-aus",
              "text": "can you tell me more about the $3.49 sub?",
              "score": 4,
              "created_utc": "2026-01-30 16:33:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mwe1x",
                  "author": "shaonline",
                  "text": "You need to haggle with the web chatbot on kimi's website to knock the price down, it's the \"Moderato\" sub.",
                  "score": 8,
                  "created_utc": "2026-01-30 17:13:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2n6u7q",
                  "author": "slothkenny",
                  "text": "I couldn‚Äôt get it to go below 4 bucksüò≠",
                  "score": 2,
                  "created_utc": "2026-01-30 17:59:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ml2wr",
          "author": "cartazio",
          "text": "patch the default prompt to be more chill and the reasoning will work better¬†",
          "score": 3,
          "created_utc": "2026-01-30 16:23:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nzfpg",
          "author": "throwaway12012024",
          "text": "tried w/opencode. This model is so slow, almost codex-level slow. Still hard to beat opus codex for planning and flash for coding.",
          "score": 3,
          "created_utc": "2026-01-30 20:07:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r35v2",
          "author": "Queasy_Asparagus69",
          "text": "not really; I got the $20 plan and it can't figure out how to do a simple website oath; been going for an hour trying to make the login work....",
          "score": 3,
          "created_utc": "2026-01-31 07:08:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mw1o3",
          "author": "Aardvark_Says_What",
          "text": "not for me. it just fucked up my svelte / css stack and couldn't unfuck it. \n\nthank Linus for git.",
          "score": 4,
          "created_utc": "2026-01-30 17:11:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ncfer",
          "author": "Aggravating_Bad4163",
          "text": "It really looks good. I tried it with opencode and it just worked fine.",
          "score": 2,
          "created_utc": "2026-01-30 18:24:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2nerhd",
              "author": "orucreiss",
              "text": "yeah feels smotth with opencode",
              "score": 1,
              "created_utc": "2026-01-30 18:34:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nsiou",
          "author": "uttkarsh26",
          "text": "Json parse errors are not good, but nonetheless pretty solid so far\n\nDoes misunderstand sometime if not being explicit",
          "score": 2,
          "created_utc": "2026-01-30 19:35:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2o1dtx",
          "author": "Putrid-Pair-6194",
          "text": "Tried it for the first time today using a monthly subscription, which I got for $3.49. Could have been lower but I got tired of haggling.\n\nI don‚Äôt have enough usage yet for feedback on quality. But speed was very fast compared to other models I use in opencode. Leaves GLM 4.7 in the dust.",
          "score": 2,
          "created_utc": "2026-01-30 20:16:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2o998n",
              "author": "funzbag",
              "text": "How did you get that low price?",
              "score": 2,
              "created_utc": "2026-01-30 20:54:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ooshn",
                  "author": "Putrid-Pair-6194",
                  "text": "They encourage negotiation with their online bot. Start telling the bot innovative ways you will promote their service to other people. After about 7 back and forth chats, I got down to $3.49 for the first month.",
                  "score": 3,
                  "created_utc": "2026-01-30 22:08:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2p1c5b",
          "author": "OffBoyo",
          "text": "Opus has been terrible as of late so not very suprising. Test alongside 5.2 Xhigh",
          "score": 2,
          "created_utc": "2026-01-30 23:12:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pl3mo",
          "author": "Finn55",
          "text": "How big is the context window? For local and hosted (does it have a difference?). I‚Äôm using MiniMax 2.1 Q6 GGUF Unsloth, and I‚Äôm ok with it but the 200k context is difficult to work with for longer sessions and larger repos",
          "score": 2,
          "created_utc": "2026-01-31 01:01:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30tnpk",
              "author": "Wurrsin",
              "text": "I think its around 250k context window, slightly below it like 246k or something in opencode",
              "score": 1,
              "created_utc": "2026-02-01 19:20:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2v0i1c",
          "author": "aliabbassp",
          "text": "Hey, it worked here. But tell me.. is it included in the plan?",
          "score": 2,
          "created_utc": "2026-01-31 21:36:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2v0qfk",
              "author": "orucreiss",
              "text": "what plan do u have currently?",
              "score": 1,
              "created_utc": "2026-01-31 21:37:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2vxviu",
          "author": "Much-Strawberry4483",
          "text": "Cant wait to try.",
          "score": 2,
          "created_utc": "2026-02-01 00:34:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bcdvx",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-02-03 09:30:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bcylj",
              "author": "orucreiss",
              "text": "ü´∂üòò",
              "score": 1,
              "created_utc": "2026-02-03 09:36:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2r2l1a",
          "author": "npittas",
          "text": "For me kimi for coding works fine without the interleave option, but I cannot make the normal kimi API key to work for the non coding models, the normal [Moonshot.ai](http://Moonshot.ai) API. That is the one that shows the \"reasoning\\_content is missing\" error. I had not needed to make any changes to the opencode.json at all to make kimi for coding work. But the [moonshot.ai](http://moonshot.ai) API, well, nothing...  \nIf anyone has any idea, that would be awsome.  \nMy experience with kimi 2.5 is far superior that expected, and I am actively using it along side opus. And it is fast enough, that I can relly on it and even let it run as main for clawdbot!",
          "score": 1,
          "created_utc": "2026-01-31 07:02:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tw2me",
          "author": "Pleasant_Thing_2874",
          "text": "My biggest issue with Kimi is the usage limits in their coding plan.   They burn up very quickly.",
          "score": 1,
          "created_utc": "2026-01-31 18:20:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lw80a",
          "author": "pokemonplayer2001",
          "text": "The sadness I feel for people scrambling to post their experience with things is accumulating.\n\nCongrats u/orucreiss, here's your participant ribbon.",
          "score": -30,
          "created_utc": "2026-01-30 14:27:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m1mzq",
              "author": "Dyhart",
              "text": "Some people don't have others to talk to about this kind of stuff so this is their way to connect with people. No need to talk others down",
              "score": 10,
              "created_utc": "2026-01-30 14:54:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2m0ybj",
              "author": "disgruntledempanada",
              "text": "Please talk to a therapist or ask Kimi to act like one.",
              "score": 6,
              "created_utc": "2026-01-30 14:50:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2m1cze",
                  "author": "pokemonplayer2001",
                  "text": "No.",
                  "score": -9,
                  "created_utc": "2026-01-30 14:52:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qqmdcs",
      "title": "Kimi is FREE for a limited time in OpenCode CLI!",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qqmdcs/kimi_is_free_for_a_limited_time_in_opencode_cli/",
      "author": "jpcaparas",
      "created_utc": "2026-01-29 21:55:41",
      "score": 110,
      "num_comments": 44,
      "upvote_ratio": 0.98,
      "text": "https://preview.redd.it/2t904wj91dgg1.png?width=581&format=png&auto=webp&s=ec530b8a251fccf7440a64d426c59d2e846c50d0\n\nYou heard that right boys and gals!\n\nEdit: Kimi K2.5 *specifically*.\n\nEdit 2: Check out the benchmarks and capabilities [here](https://generativeai.pub/moonshots-kimi-k2-5-can-spawn-100-ai-agents-to-do-your-work-5c7f0bd90a88?sk=f68e605853d40ad859c9d5507b9e0749).\n\nEdit 3: [Dax](https://x.com/thdxr/status/2016993820940943623?s=20) stands by Kimi K2.5, says it's at par with Opus 4.5.\n\nEdit 4: Here's my longform, non-paywalled review after trying it out for the last 24 hours (with a solid recommendation from OpenCode's co-creator, Dax):\n\n(Obviously, try it out for free first before you make the switch to a paid provider, either with Zen, Chutes, NanoGPT, or Synthetic)\n\n‚û°Ô∏è¬†[Stop using Claude‚Äôs API for Moltbot (and OpenCode)](https://jpcaparas.medium.com/stop-using-claudes-api-for-moltbot-and-opencode-52f8febd1137)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qqmdcs/kimi_is_free_for_a_limited_time_in_opencode_cli/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2hzt1j",
          "author": "jpcaparas",
          "text": "https://preview.redd.it/a4d728zubdgg1.png?width=628&format=png&auto=webp&s=3d5a1573eefed1803f4df89a7a9daf21d63e17a0\n\nThis is all the validation you need right now.",
          "score": 18,
          "created_utc": "2026-01-29 22:55:05",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2j52wk",
              "author": "Sensitive_Song4219",
              "text": "For debugging I'm finding Kimi 2.5 a definite step down from Opus / Codex-High. Feels more like Sonnet/Codex-Medium/GLM4.7 in that regard, at least in the projects I've tried it in so far. (I don't think I can do without at least a small subscription to a frontier-model - at least not quite yet!)\n\nBut  *Kimi K2.5 Free* via OpenCode-Zen is quite fast, I assume Moonshot's doing the hosting for them. Impressive. (Edit: poster below correctly mentions the X post says *fireworks* is hosting - they're doing a good job!)",
              "score": 5,
              "created_utc": "2026-01-30 02:39:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2kqio6",
                  "author": "lofuller",
                  "text": "Fireworks is providing the inference (Dax confirmed)",
                  "score": 3,
                  "created_utc": "2026-01-30 09:42:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2jl483",
                  "author": "jmhunter",
                  "text": "ya, agree...",
                  "score": 1,
                  "created_utc": "2026-01-30 04:12:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2jfpna",
              "author": "philosophical_lens",
              "text": "I‚Äôm looking forward to trying K2.5, but that post is marketing, not validation.",
              "score": 6,
              "created_utc": "2026-01-30 03:40:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o337fbd",
                  "author": "Myfinalform87",
                  "text": "Agreed.",
                  "score": 1,
                  "created_utc": "2026-02-02 02:49:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o335dal",
              "author": "Myfinalform87",
              "text": "While very cool so far in my testing I personally don‚Äôt think they should drop their pricing. I want the OpenCode project to succeed and that means they got to make money. I‚Äôd hate for this project to get abandoned due to just people having lives but no steady income to focus on open their open source projects. They already have competitive pricing and so many models and services intergrated.\n\nOne thing I‚Äôm hoping for is better support for using local models. So far I haven‚Äôt been able to intergrate LM Studio support. The setup process is straight forward but I can‚Äôt seem to get them to link together in terms of OpenCode using LM studio for local generation",
              "score": 1,
              "created_utc": "2026-02-02 02:37:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ht1mo",
          "author": "Metalwell",
          "text": "I just had signed up for free tier in moonshot, imma cancel it now",
          "score": 4,
          "created_utc": "2026-01-29 22:20:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iao1l",
          "author": "baldreus",
          "text": "It‚Äôs insanely cheap on chutes too.  300 requests a day on the lowest tier for 3$ a month.  You can‚Äôt beat free but the advantage with chutes is privacy if you care (which may be a concern if pitching a provider at work)",
          "score": 6,
          "created_utc": "2026-01-29 23:52:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2idghp",
              "author": "minato_shikamaru",
              "text": "Is chutes a good provider, some people are telling me not to use third party providers because they don't run well.¬†",
              "score": 3,
              "created_utc": "2026-01-30 00:08:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2in5cv",
                  "author": "baldreus",
                  "text": "I've only been using it for a couple of days since this model came out, so take this with a grain of salt. As long as the model is hot loaded (in other words it‚Äôs actively being used by others), it seems to be pretty responsive. If the model is cold and hasn't been used for a while, it does take some time for it to spin up instances. Yes there could be occasions when you have to wait a bit but in my experience 98% of the time it's been fast. Let's wait and see what happens when everyone piles on once the hype reaches critical mass. \n\nThat said it's definitely way better than free tier openrouter models that are insanely rate limited and slow - so, so far it's been a great experience.",
                  "score": 5,
                  "created_utc": "2026-01-30 01:00:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2i8an9",
          "author": "ECrispy",
          "text": "how do I find the free model? /connect doesnt list it",
          "score": 2,
          "created_utc": "2026-01-29 23:40:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2k4tr2",
              "author": "deadcoder0904",
              "text": "https://github.com/anomalyco/opencode/issues/11261\n\ntl;dr do `opencode models --refresh` before launching `opencode`",
              "score": 10,
              "created_utc": "2026-01-30 06:31:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2k591k",
                  "author": "ECrispy",
                  "text": "thanks",
                  "score": 1,
                  "created_utc": "2026-01-30 06:35:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ik91x",
              "author": "____HakunaMatata____",
              "text": "/models",
              "score": 1,
              "created_utc": "2026-01-30 00:44:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2idiek",
          "author": "hexa01010",
          "text": "Sweet! I agree this model is awesome first time we have real SOTA with open models finally!\n\nI think Kimi also increased their quotas a lot I went from like 10% yesterday to 1% today on the weekly",
          "score": 2,
          "created_utc": "2026-01-30 00:08:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2j3w8d",
              "author": "Hoak-em",
              "text": "Yeah they switched to token-based (instead of API-request based) and have a limited-time 3x for the higher-tier plans (I'm not sure if the lower-tier one has it too). Go ham with it now I figure -- also it's fast :3",
              "score": 1,
              "created_utc": "2026-01-30 02:33:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2hsr7s",
          "author": "krimpenrik",
          "text": "Nice",
          "score": 1,
          "created_utc": "2026-01-29 22:19:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hukiu",
          "author": "SilverMethor",
          "text": "Nice!",
          "score": 1,
          "created_utc": "2026-01-29 22:28:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hwjb4",
          "author": "MeasurementPlenty514",
          "text": "I was a mega-slut for mimo 2 when free when I needed to just munch and gobble tokens.",
          "score": 1,
          "created_utc": "2026-01-29 22:38:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2idfma",
          "author": "ianxiao",
          "text": "So no thinking ?",
          "score": 1,
          "created_utc": "2026-01-30 00:08:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2jhwkf",
          "author": "LtCommanderDatum",
          "text": "Error: 401 Unauthorized",
          "score": 1,
          "created_utc": "2026-01-30 03:52:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ji0te",
              "author": "jpcaparas",
              "text": "It got reddit hugged lol",
              "score": 3,
              "created_utc": "2026-01-30 03:53:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2jm7n6",
          "author": "kr_roach",
          "text": "Provider of Kimi 2.5 free model is moonshot ai??",
          "score": 1,
          "created_utc": "2026-01-30 04:19:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l79cz",
              "author": "mizzuri",
              "text": "Fireworks AI",
              "score": 1,
              "created_utc": "2026-01-30 12:02:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tg54u",
          "author": "ethereal_intellect",
          "text": "At the op and others, i see you mention glm 4.7 apparently that's cheap and huge limits for bots on cerebras 50$ plan, but can it deal with openclaw or is too old and dumb",
          "score": 1,
          "created_utc": "2026-01-31 17:04:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2z636q",
          "author": "Comprehensive-Age155",
          "text": "I got this today, when connecting to their api: \n\n From the logs, the Kimi API returned a 403 status code with this response body:                                                                                                                        \n\n  {                                                                                                                                         \n\n\"error\": {                                                                                                                              \n\n\"message\": \"Kimi For Coding is currently only available for Coding Agents such as Kimi CLI, Claude Code, Roo Code, Kilo Code, etc.\",  \n\n\"type\": \"access\\_terminated\\_error\"                                                                                                     \n\n}                                                                                                                                       \n\n  }                                                                                                                                                                                                                                                                                     \n\n  The full error in the OpenCode logs showed:                                                                                               \n\n  ERROR service=llm providerID=kimi-for-coding modelID=kimi-k2.5                                                                            \n\n  error={\"statusCode\":403,\"responseBody\":\"{\\\\\"error\\\\\":{\\\\\"message\\\\\":\\\\\"Kimi For Coding is currently only available for Coding Agents such as   \n\n  Kimi CLI, Claude Code, Roo Code, Kilo Code, etc.\\\\\",\\\\\"type\\\\\":\\\\\"access\\_terminated\\_error\\\\\"}}\"}                                                                                                                                                                                           \n\n  So Kimi is checking something (likely User-Agent header or some other identifier) to verify the request comes from a recognized coding    \n\n  agent. OpenCode isn't on their allowlist, so they reject the request with access\\_terminated\\_error.",
          "score": 1,
          "created_utc": "2026-02-01 14:43:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o334gmv",
          "author": "Myfinalform87",
          "text": "I‚Äôve been using it so far for a fork of my project and yes it‚Äôs pretty comparable. I do agree tho in that CC is smarter so keep that in mind when using Kimi K. You may have to be more specific in your wording.\n\nSomething I have done is have a glossary of terms for my programs to avoid confusions when editing or working on a specific element.\n\nCC is better at interpreting plain instructions but both are comparable for code execution so far",
          "score": 1,
          "created_utc": "2026-02-02 02:32:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o336yj8",
              "author": "jpcaparas",
              "text": "\"Kimi K\" oh god I have a new nickname for it now.",
              "score": 1,
              "created_utc": "2026-02-02 02:46:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o337hz0",
                  "author": "Myfinalform87",
                  "text": "lol you‚Äôre welcome",
                  "score": 1,
                  "created_utc": "2026-02-02 02:49:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37ht9z",
          "author": "Nobilitytoken",
          "text": "Do you still need to pay for API tokens to be able to use it effectively?",
          "score": 1,
          "created_utc": "2026-02-02 19:19:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2i6mpr",
          "author": "kpgalligan",
          "text": "Just bought zen credits. Have to see if I can use some extra gas here...",
          "score": 1,
          "created_utc": "2026-01-29 23:31:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2i73n0",
              "author": "jpcaparas",
              "text": "The token pricing itself will get you a lot of range for that fill.",
              "score": 1,
              "created_utc": "2026-01-29 23:33:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ij5a0",
                  "author": "kpgalligan",
                  "text": "I went to look at the dashboard after poking around for a while, and the chart struggled to show <$1 of usage :)\n\nWill try to use free, but not sure it'll matter all that much. I'm not exactly hammering it.",
                  "score": 2,
                  "created_utc": "2026-01-30 00:38:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2rfjat",
          "author": "Willing_Somewhere356",
          "text": "Slow slow slow",
          "score": 0,
          "created_utc": "2026-01-31 09:03:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2n0qfa",
          "author": "neamtuu",
          "text": "https://preview.redd.it/8q48vc28vigg1.png?width=2151&format=png&auto=webp&s=eca507bf5d5424e206f2cbd499ed7cd8be1234f8\n\n‚Ä¢ OpenCode 1.1.45 - oh yeah. it works so well. Fuck off.",
          "score": -1,
          "created_utc": "2026-01-30 17:32:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ic1yk",
          "author": "atkr",
          "text": "why advertise it?",
          "score": -2,
          "created_utc": "2026-01-30 00:00:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs9cpr",
      "title": "Which Model is the Most Intelligent From Here?",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/ue9m9fi74qgg1.png",
      "author": "Level-Dig-4807",
      "created_utc": "2026-01-31 18:01:54",
      "score": 92,
      "num_comments": 47,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qs9cpr/which_model_is_the_most_intelligent_from_here/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2ttpfa",
          "author": "SnooSketches1848",
          "text": "Kimiiiiiiiiiiiiiiii",
          "score": 33,
          "created_utc": "2026-01-31 18:09:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2u91ki",
              "author": "PsyGnome_FreeHuman",
              "text": "Kimi o big-pickle ?ü´£",
              "score": 2,
              "created_utc": "2026-01-31 19:21:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2unb6l",
                  "author": "shikima",
                  "text": "big-pickle is GLM-4.6",
                  "score": 6,
                  "created_utc": "2026-01-31 20:31:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2tvg0d",
          "author": "annakhouri2150",
          "text": "Kimi K2.5 by far. It's the closest open model to Opus 4.5, and the only large, capable coding and agentic model that has vision:\n\nhttps://www.kimi.com/blog/kimi-k2-5.html",
          "score": 30,
          "created_utc": "2026-01-31 18:17:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u9i5r",
          "author": "noctrex",
          "text": "Kimi > GLM > MiniMax",
          "score": 22,
          "created_utc": "2026-01-31 19:24:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2uad26",
              "author": "PsyGnome_FreeHuman",
              "text": "And where is Big Pickle?",
              "score": 1,
              "created_utc": "2026-01-31 19:28:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ualrv",
                  "author": "noctrex",
                  "text": "That is essentially the previous GLM 4.6 model, so behind them",
                  "score": 8,
                  "created_utc": "2026-01-31 19:29:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ulyps",
          "author": "Orlandocollins",
          "text": "As an elixir developer I have had better success with MiniMax than GLM, though GLM isn't terrible by any means. I only run locally so I haven't had a chance to run Kimi as it is VERY large.",
          "score": 7,
          "created_utc": "2026-01-31 20:24:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tsirs",
          "author": "rusl1",
          "text": "I usually do GLM for planning and debugging, MiniMax sub agents for everything else. \n\nKimi looks good but I didn't test it extensively",
          "score": 7,
          "created_utc": "2026-01-31 18:04:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30ieuv",
              "author": "Impossible_Comment49",
              "text": "You‚Äôll be surprised; it‚Äôs much better.",
              "score": 2,
              "created_utc": "2026-02-01 18:29:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o30oxvj",
                  "author": "rusl1",
                  "text": "At coding or planning?",
                  "score": 1,
                  "created_utc": "2026-02-01 18:58:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2tui35",
          "author": "RegrettableBiscuit",
          "text": "K2.5 is most likely the best, but I guess we're not sure if these are quantized models.¬†",
          "score": 12,
          "created_utc": "2026-01-31 18:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ua07z",
              "author": "noctrex",
              "text": "It is natively trained as INT4, so even if its 1T parameters, its 595 GB in size",
              "score": 4,
              "created_utc": "2026-01-31 19:26:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o30hsnp",
                  "author": "Impossible_Comment49",
                  "text": "Would you rather quant k2.5 to fit on 512gb ram or just use glm4.7 in fp8 or q6?",
                  "score": 1,
                  "created_utc": "2026-02-01 18:26:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2tv7m1",
          "author": "silurosound",
          "text": "I've been testing both GLM and Kimi these past few days thru paid API and my first impressions are that Kimi is snappier and smarter but burns tokens faster than GLM, which is solid too and didn't burn through tokens as quickly.",
          "score": 4,
          "created_utc": "2026-01-31 18:16:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wgtc3",
              "author": "DistinctWay9169",
              "text": "Kimi is HUNGRY. Might be better than GLM but not so much that would make sense paying much mor for Kimi",
              "score": 3,
              "created_utc": "2026-02-01 02:26:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2zpa5u",
              "author": "neamtuu",
              "text": "What? in no way does kimi k2.5 thinking max burn more tokens than glm 4.7 haha\n\nhttps://preview.redd.it/ne5nleakrwgg1.png?width=1829&format=png&auto=webp&s=157da48df2a016ad7d7d5f0eae51f1b87cdfb917\n\ni",
              "score": 1,
              "created_utc": "2026-02-01 16:17:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2zuobs",
              "author": "aimericg",
              "text": "I find GLM practically unusable on my side, mostly because its quite slow and easily hallucinates on my coding projects.",
              "score": 1,
              "created_utc": "2026-02-01 16:41:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2umnex",
          "author": "martinffx",
          "text": "I tried the kimi models again and they are still terrible at tool calling. At least the opencode zen one, constant errors calling tools. Straight up just throws some sort of reasoning error when in planning mode. So it may be better but I‚Äôve not found it to be more usable at least with the opencode harness",
          "score": 4,
          "created_utc": "2026-01-31 20:28:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31oybk",
              "author": "NewEraFresh",
              "text": "Yup for example it struggles to even use playwright mcp correctly with tool calls. GLM handles it like a boss. Kimi does surprise me on the quality on certain tasks. Overall though it‚Äôs looking like GLM is still way more usable as a backup plan for when you hit those limits on Claude Opus 4.5 or GPT 5.2 high.",
              "score": 1,
              "created_utc": "2026-02-01 21:51:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2uwzji",
          "author": "Repulsive_Educator61",
          "text": "Also, off topic but, opencode docs mention that all these models train on your data during the \"FREE\" period (only the free models)",
          "score": 5,
          "created_utc": "2026-01-31 21:19:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2v1d82",
              "author": "touristtam",
              "text": "Well good luck with the shitty code that being produced on my end. :D",
              "score": 9,
              "created_utc": "2026-01-31 21:40:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2twsbw",
          "author": "Flat_Cheetah_1567",
          "text": "From their site\nhttps://share.google/Fd6nPfo1PF4HNnLNo\nJust check the links and apply with your student account and also you have gemini options for free with student account",
          "score": 2,
          "created_utc": "2026-01-31 18:23:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2u29ef",
              "author": "atiqrahmanx",
              "text": "ChatGPT free for how many months?",
              "score": 1,
              "created_utc": "2026-01-31 18:49:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tysh7",
          "author": "SlopTopZ",
          "text": "Kimi 100%",
          "score": 2,
          "created_utc": "2026-01-31 18:33:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2urb5m",
          "author": "Michaeli_Starky",
          "text": "GPT 4.5 xhigh",
          "score": 2,
          "created_utc": "2026-01-31 20:51:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2uqetm",
          "author": "LittleChallenge8717",
          "text": "Kimi",
          "score": 1,
          "created_utc": "2026-01-31 20:46:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2whhvc",
          "author": "aeroumbria",
          "text": "Does anyone know if there is an official way to specify which variant of the model an agent / subagent will use? I only saw some unmerged pull requests when I search it up. Right now Kimi is a bit limited because it only runs the no reasoning variant in subagents, and it really does not like to plan or reason in \"white\" outputs.",
          "score": 1,
          "created_utc": "2026-02-01 02:30:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2wprvs",
          "author": "lucaasnp",
          "text": "I‚Äôve been using Kimi and it is pretty good",
          "score": 1,
          "created_utc": "2026-02-01 03:20:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xbie7",
          "author": "Independent_Ad627",
          "text": "Kimi is great because it's in par with GLM but faster, and I use GLM pro plan, not the free one from opencode.\nNowadays I use the OPENCODE_EXPERIMENTAL_PLAN_MODE=1, and both models work consistently the same IMO. So I didn't see any much difference other than the token per second",
          "score": 1,
          "created_utc": "2026-02-01 05:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ycgz7",
          "author": "Careless-Plankton630",
          "text": "Kimi K2.5 is so good. Like it is insanely good",
          "score": 1,
          "created_utc": "2026-02-01 11:21:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yd73v",
          "author": "debba_",
          "text": "I am using Kimi and it‚Äôs very good",
          "score": 1,
          "created_utc": "2026-02-01 11:27:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yekjo",
          "author": "Flashy_Reality8406",
          "text": "IMO, Kimi > Minimax M2 > GLM",
          "score": 1,
          "created_utc": "2026-02-01 11:39:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yg6fc",
          "author": "ManWhatCanIsay_K",
          "text": "actually i prefer minimax",
          "score": 1,
          "created_utc": "2026-02-01 11:53:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2z616i",
          "author": "stevilg",
          "text": "https://arena.ai/leaderboard/code",
          "score": 1,
          "created_utc": "2026-02-01 14:42:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2zv0kv",
          "author": "aimericg",
          "text": "Anyone tried Trinity Large a bit more extensively? Also what happened to Big Pickle?",
          "score": 1,
          "created_utc": "2026-02-01 16:43:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31sdt8",
          "author": "fergthh",
          "text": "Studying... is unlimited",
          "score": 1,
          "created_utc": "2026-02-01 22:08:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34s4un",
          "author": "sasha-zelts",
          "text": "I would say Kiki 2.5",
          "score": 1,
          "created_utc": "2026-02-02 10:18:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tvtv4",
          "author": "Flat_Cheetah_1567",
          "text": "If you're student get the open ai free year with codex and done",
          "score": 1,
          "created_utc": "2026-01-31 18:19:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2tw3hr",
              "author": "Level-Dig-4807",
              "text": "really? from where?",
              "score": 3,
              "created_utc": "2026-01-31 18:20:40",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2u1no6",
              "author": "AkiDenim",
              "text": "Is this still a thing?",
              "score": 2,
              "created_utc": "2026-01-31 18:46:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2zvlnl",
              "author": "aimericg",
              "text": "ChatGPT Codex models don't hallucinate as much as some of these models but honestly don't find their output quite good. It always feels quite off in the UI and I am having issues with it when trying to fix more architecture level problems. It just doesnt seem to be able to handle that.",
              "score": 1,
              "created_utc": "2026-02-01 16:46:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qqrbry",
      "title": "Tested free Kimi K2.5 in opencode: good stuff",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qqrbry/tested_free_kimi_k25_in_opencode_good_stuff/",
      "author": "ReporterCalm6238",
      "created_utc": "2026-01-30 01:17:03",
      "score": 84,
      "num_comments": 13,
      "upvote_ratio": 0.97,
      "text": "It's fast, it's smart BUT sometimes it makes mistakes with tool calling. I would put it above glm 4.7 and minimax M2.1. \n\n  \nWe are getting close boys. Open source Opus is not too far. There are some extremely smart people in China working around the clock to crush Anthropic, that's for sure.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qqrbry/tested_free_kimi_k25_in_opencode_good_stuff/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2ixorf",
          "author": "AlternativeAir7087",
          "text": "Open source will definitely prevail!",
          "score": 10,
          "created_utc": "2026-01-30 01:58:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2iy36w",
              "author": "ReporterCalm6238",
              "text": "I surely hope so. It's in the interest of the whole humanity that open source AI resists and thrive",
              "score": 7,
              "created_utc": "2026-01-30 02:01:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lrkng",
          "author": "0xfe",
          "text": "It's really excellent -- it finished some tasks that GLM just spun forever on. I currently use codex as my daily driver and claude for smaller things, but Kimi really feels like the first open model that truly competes.",
          "score": 4,
          "created_utc": "2026-01-30 14:03:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2l0p37",
          "author": "Villain_99",
          "text": "Using it via Kimi api, really good\nEven as a general purpose Chatbot it‚Äôs really good answers like sonnet/opus",
          "score": 3,
          "created_utc": "2026-01-30 11:11:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kh6y9",
          "author": "krimpenrik",
          "text": "How/what provider are you using it with?",
          "score": 2,
          "created_utc": "2026-01-30 08:16:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sas99",
          "author": "hey_ulrich",
          "text": "I saw in the discord server that there was a bug in opencode that affected kimi tool calling, but they fixed it¬†",
          "score": 2,
          "created_utc": "2026-01-31 13:28:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2k00l8",
          "author": "kr_roach",
          "text": "I tested it. But I'm wondering if Opencode Zen Kimi is using Moonshot AI directly or serving it through their server.",
          "score": 3,
          "created_utc": "2026-01-30 05:54:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2k1lvg",
              "author": "Useful-Mistake-4221",
              "text": "It's through Fireworks",
              "score": 7,
              "created_utc": "2026-01-30 06:06:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2k0u7e",
              "author": "Recent-Success-1520",
              "text": "Most probably Moonshot directly",
              "score": -2,
              "created_utc": "2026-01-30 06:00:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2kx6b6",
                  "author": "this-is-hilarours",
                  "text": "Not true. Served through fireworks  in us. Opencode confirmed this via x",
                  "score": 5,
                  "created_utc": "2026-01-30 10:41:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2nb0ok",
          "author": "jmhunter",
          "text": "i like how agressively it calls tools.. like right away its like I'm not spending all of my context right here",
          "score": 1,
          "created_utc": "2026-01-30 18:18:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xel7c",
          "author": "Even_Sea_8005",
          "text": "The Chinese ppl at Anthropic : feeling the rage of our ancestors ain‚Äôt no fun",
          "score": 1,
          "created_utc": "2026-02-01 06:14:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xhxcy",
          "author": "arzzka777",
          "text": "Seems a lot better than GLM 4.7. It is able to one shot things GLM 4.7 took forever to fix.",
          "score": 1,
          "created_utc": "2026-02-01 06:42:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq8sgu",
      "title": "Be careful when using Claude Code with OpenCode",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/zqcyddtrjagg1.png",
      "author": "HzRyan",
      "created_utc": "2026-01-29 13:38:42",
      "score": 64,
      "num_comments": 38,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qq8sgu/be_careful_when_using_claude_code_with_opencode/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2eqcy2",
          "author": "HeavyDluxe",
          "text": "This is old news.  And Claude's TOS specifically speak to this use as prohibited.  (I know, I know. Who reads that stuff?)\n\nIf you want to use Opencode as your CLI environment with Claude, YOU CAN DO THAT.  You just need to buy credits for the API and create a key.  Plug it into Opencode and you're off to the races.",
          "score": 15,
          "created_utc": "2026-01-29 13:42:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2exu2z",
              "author": "OofOofOof_1867",
              "text": "My decided workaround has been to get a GitHub CoPilot Pro+ license, and then pay as I go afterwards. They pay by \"premium request\" - so it does change the way I work because I make sure every request is FAT to get most bang for my buck. But so far it's close to the same price as Claude Max5.",
              "score": 5,
              "created_utc": "2026-01-29 14:21:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2fupbn",
                  "author": "jrop2",
                  "text": "\\>¬†But so far it's close to the same price as Claude Max5.\n\nEesh. I was wondering about switching to GH CoPilot from the Claude plans. However, I was shooting for the $40/month plan, and your experience makes it sound like it still evens out to lots of money either way....",
                  "score": 2,
                  "created_utc": "2026-01-29 16:53:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2etls8",
              "author": "Sensitive_Song4219",
              "text": "API is very expensive tho. I know it's old news, but it'd be better news if Anthropic would follow OpenAI's lead and allow subscription use in OpenCode: OC still needs some polish but after two weeks or so of using it, it's really kinda incredible. (Heck, I just used *Microsoft's official OAUTH to sign into my Copilot subscription via OpenCode*!). \n\n  \nIf OpenAI and freakin' *Microsoft* (of all companies!) can see the benefit of giving their paid subscribers access (because let's face it: OpenCode isn't more token hungry than Claude Code), then why can't Anthropic follow suit?\n\nFor the first time in a year, I've gone a full week without looking at Claude Code - I'm OpenCode exclusive now, chopping-and-changing providers, models, and reasoning levels mid-conversation. It's liberating. And maybe that's the problem.",
              "score": 4,
              "created_utc": "2026-01-29 13:59:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ew0le",
                  "author": "acmethunder",
                  "text": "> If OpenAI and freakin' Microsoft (of all companies!) can see the benefit of giving their paid subscribers access (because let's face it: OpenCode isn't more token hungry than Claude Code), then why can't Anthropic follow suit?\n\nAnthropic can, they are choosing not to. It is a form of vendor lock in hoping to keep those monthly/yearly revenue streams.",
                  "score": 7,
                  "created_utc": "2026-01-29 14:12:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ko7bk",
                  "author": "Keep-Darwin-Going",
                  "text": "It is called paying for mind share. People remember Claude code more than anthropic, if you look at a lot of comparison article and video people say Claude code not opus or sonnet or anthropic. Which is why you can use cc for everything else but not use max x20 for opencode.  All this loses they have to attribute to something to answer to shareholders. OpenAI allow it because their plan is already more expensive although they are more efficient? Opus already capture majority of the enterprise and non enterprise coding users. It is like if you talk about chat AI it is always chatgpt, coding it is always opus at least for now.",
                  "score": 1,
                  "created_utc": "2026-01-30 09:20:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2er00f",
              "author": "HzRyan",
              "text": "That's on me, haha. I'm still not going to read any T&Cs though üåù",
              "score": 1,
              "created_utc": "2026-01-29 13:45:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2erzr5",
                  "author": "HeavyDluxe",
                  "text": "Can't blame you. :)",
                  "score": 1,
                  "created_utc": "2026-01-29 13:51:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ew7xg",
          "author": "aeroumbria",
          "text": "They are pretty much alone in the fight against third party UIs at this point, though. Almost all their competition are either actively supporting or acquiescing.",
          "score": 8,
          "created_utc": "2026-01-29 14:13:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gdffw",
              "author": "flexrc",
              "text": "Google antigravity just started blocking opencode usage as well.",
              "score": 6,
              "created_utc": "2026-01-29 18:17:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ez4id",
              "author": "Sensitive_Song4219",
              "text": "Yup. This can't be what developers want though: it's like saying \"You can use Visual Studio Professional but VSCode is prohibited or you get booted.\" No experienced dev would accept that.",
              "score": 3,
              "created_utc": "2026-01-29 14:28:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ezzqq",
                  "author": "Ok_Road_8710",
                  "text": "Of course not, they can try (Anthropic), their heads are too far up their own asses to make any progress I guess",
                  "score": 1,
                  "created_utc": "2026-01-29 14:32:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ezsc4",
          "author": "kiwibonga",
          "text": "Hey so... As soon as you open opencode you get banned and refunded, no questions asked?",
          "score": 1,
          "created_utc": "2026-01-29 14:31:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f0f0j",
              "author": "HzRyan",
              "text": "no I used it for 3 day straight, burn through quite a lot of tokens through Claude subs maybe that's why I got banned",
              "score": 1,
              "created_utc": "2026-01-29 14:34:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2f3db3",
                  "author": "Scriverry",
                  "text": "If you used more than one sub that's the reason you got banned, not that you used it with opencode. They are way more strict about that.",
                  "score": 2,
                  "created_utc": "2026-01-29 14:49:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lnrji",
          "author": "Peace_Seeker_1319",
          "text": "damn they refunded but banned you. probably flagged the usage pattern - too many requests too fast or something that looked automated. honestly the bigger issue is what you're shipping when you code that aggressively. if you're cranking out PRs without really understanding what's being generated, you're building tech debt bombs.  \nwe had to add automated review [codeant.ai](http://codeant.ai) in CI specifically because people were merging AI code too fast. caught race conditions, memory leaks, stuff that looked fine but broke under load. slowed us down but saved us from prod fires. maybe the ban is a feature not a bug lol.",
          "score": 1,
          "created_utc": "2026-01-30 13:44:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mf7sy",
              "author": "HzRyan",
              "text": "I have 6 years of experience DEV-ing and have worked on quite a few large projects before, so I'm not just yolo pushing any generated code and hoping it works. Every worktree branch gets manually+claude reviewed and tested before it gets locally merged. Of course I don't read every single line of code, but I do know how each section of my system works and I make sure it's specified in the prompt so CC knows where to extend or implement the feature.\n\nIt's honestly amazing how much CC has boosted my productivity. I'm holding a full time job right now while working on 3 other separate projects, and I think there is still room for improvement in my current workflow. Trying to get out of the 9-5 rat race, Locked in!! \n\nNice plug btwüòâ",
              "score": 1,
              "created_utc": "2026-01-30 15:56:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2o23t9",
          "author": "Brucesquared2",
          "text": "Your best bet, if you have a 16vram card is LM Studio and Ollama, give it a Claude wrapper. Its killer",
          "score": 1,
          "created_utc": "2026-01-30 20:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yriyl",
          "author": "TheThinker012",
          "text": "Do they say how long the suspension is for?",
          "score": 1,
          "created_utc": "2026-02-01 13:18:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq4vxu",
      "title": "Kimi K2.5, a Sonnet 4.5 alternative for a fraction of the cost",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qq4vxu/kimi_k25_a_sonnet_45_alternative_for_a_fraction/",
      "author": "Grand-Management657",
      "created_utc": "2026-01-29 10:19:28",
      "score": 63,
      "num_comments": 99,
      "upvote_ratio": 0.76,
      "text": "\n\nYes you read the title correctly. Kimi K2.5 is THAT good. \n\nI would place it around Sonnet 4.5 level quality. It‚Äôs great for agentic coding and uses structured to-do lists similar to other frontier models, so it‚Äôs able to work autonomously like Sonnet or Opus.\n\nIt's thinking is very methodical and highly logical, so its not the best at creative writing but the tradeoff is that it is very good for agentic use.\n\nThe move from K2 -> K2.5 brought multimodality, which means that you can drive it to self-verify changes. Prior to this, I used antigravity almost exclusively because of its ability to drive the browser agent to verify its changes. This is now a core agentic feature of K2.5. It can build the app, open it in a browser, take a screenshot to see if it rendered correctly, and then loop back to fix the UI based on what it \"saw\". Hookup playwright or vercel's browser-agent and you're good to go.\n\nNow like I said before, I would still classify Opus 4.5 as superior outside of JS or TS environments. If you are able to afford it you should continue using Opus, especially for complex applications.¬†\n\nBut for many workloads the best economical and capable pairing would be Opus as an orchestrator/planner + Kimi K2.5 as workers/subagents. This way you save a ton of money while getting 99% of the performance (depending on your workflow).  \n\n\n\\+ You don't have to be locked into a single provider for it to work.\n\n\\+ Screw closed source models.\n\n\\+ Spawn hundreds of parallel agents like you've always wanted WITHOUT despawning your bank account.\n\n\n\n*Btw this is coming from someone who very much disliked GLM 4.7 and thought it was benchmaxxed to the moon*\n\n\n\n# Get Started\n\n  \nThere are plenty of providers for open source models and only one for claude (duh)\n\n[Nano-GPT](https://nano-gpt.com/invite/mNibVUUH)\n\nA provider aggregator. Essentially routing all of your requests to a provider in their network. This is by far the most cost effective way to drive opencode, claude code, vscode (insiders), or any other harness. For the cost of a one extremely large cup of coffee, $8/month, you get 60,000 requests/month. That is $0.00013 per request regardless of input or output size. To put that into perspective, Sonnet 4.5 would cost you $0.45 for a request of 100k in/1k out (small-medium codebase) and not taking caching into account. Sonnet is 3,461x more expensive.\n\nAlso you can use Opus 4.5 through nano-gpt at API rates like I do to drive the orchestrator and then my subscription covers K2.5 subagents.\n\nCheap AF, solid community, founders are very active and helpful  \n  \nMy referral for 5% off web: [https://nano-gpt.com/invite/mNibVUUH](https://nano-gpt.com/invite/mNibVUUH)\n\n  \n[**Synethetic.new**](https://synthetic.new/?referral=KBL40ujZu2S9O0G)\n\nThis is what I would recommend for anyone needing maximum security and lightning fast inference. It costs a premium of $20/month ($10 with my referral), but compared to claude pro plan's usage limit, its a bargain. 135 requests/5hrs with tool calls only counting as 0.1 requests. This is the best plan for professionals and you can hook it up with practically any tool like claude code and opencode. Within a 10 hour period, you can use up to 270 requests which comes out to $0.002. Sonnet 4.5 is 225x more expensive.\n\nCheap, fast speed, $60/month plan gets you 1,350 requests/5hr, data not trained on \n\nMy referral for $10 or $20 off: [https://synthetic.new/?referral=KBL40ujZu2S9O0G](https://synthetic.new/?referral=KBL40ujZu2S9O0G)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qq4vxu/kimi_k25_a_sonnet_45_alternative_for_a_fraction/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2ewxfg",
          "author": "rokicool",
          "text": "Yesterday I tried their 'native' subscription (via kimi.com) - Moderato ($20 per month).\n\nI spent 5 hours allowance within 30 min. This tier of subscription seems useless.\n\nThe next tier is $40... I will be working for 1 hour and 4 hours cooldown. Useless as well.\n\nSo, the only tier that gives access (for one thread of work!) is $200. And... Why spending the same amount for something that barely imitates the original (Anthropic) when the original costs the same?\n\nI don't understand why people call it 'cheap'. It is on par with Anthropic's subscriptions.\n\nhttps://preview.redd.it/vri1l76sqagg1.png?width=2582&format=png&auto=webp&s=09f5ad25b14b14f9d6b96c11b1ccc81957ea66a7\n\n  \nUPD: There were some changes to the Console interface and I looks different and shows different metrics. And IF they are relevant, I have a lot of allowance with my $20 subscription. \n\nSorry for jumping to conclusions.",
          "score": 14,
          "created_utc": "2026-01-29 14:17:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2exv9x",
              "author": "Grand-Management657",
              "text": "Its more expensive through the moonshot subscription compared to the ones I linked in the post. From what I remember, \"Moderato\" allows 2048 requests per week. Nano-gpt allows 15,000 requests per week. Also nano is $8 instead of $20. If you get two nano subs for $16, you will get almost \\~15x the usage of \"moderato\" for less.\n\nMy referral to nano if you want to give it a try: [https://nano-gpt.com/invite/mNibVUUH](https://nano-gpt.com/invite/mNibVUUH)",
              "score": 5,
              "created_utc": "2026-01-29 14:21:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2fx86f",
                  "author": "rokicool",
                  "text": "Thank your for your research.\n\nUnfortunately, I remember complains about sluggishness of nano-gpt and wanted to test 'original' provider. And despite the really impressive outcome of the Kimi2.5 model I find the Kimi Subscriptions useless.\n\nUPD: Since there are some changes to the Console interface and it looks much more logical and promising now... I should admit that my previous assumption 'everything is useless' might be wrong. Time will show!",
                  "score": 2,
                  "created_utc": "2026-01-29 17:04:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2hoe1q",
                  "author": "Western_Objective209",
                  "text": "is nano-gpt legit? seems like it automatically creates an anonymous account, even takes XMR for payments",
                  "score": 2,
                  "created_utc": "2026-01-29 21:58:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2fw5h2",
              "author": "rokicool",
              "text": "It is getting ridiculous. I managed to spend week allowance of $20 subscription within 1-1.5 hour(s) of OpenCode development.\n\nhttps://preview.redd.it/h1fip0d8kbgg1.png?width=2290&format=png&auto=webp&s=9936622b91898f936ee0670068a9c67a40a9e6c1\n\nAre you sure you would call something like $20 an hour as 'cheap'?\n\nUPD: \n\nIt seems to me that they were changing the interface while I was bitching. Now, after several hours it look  1% and 11%. \n\nSo, I might got it wrong. And it might be cheap.",
              "score": 3,
              "created_utc": "2026-01-29 16:59:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mjejg",
                  "author": "Grand-Management657",
                  "text": "That's where you're messing up, use synthetic as your provider and you will get more limits. Kimi was limited to 2048 requests/week last I checked. Synthetic is 135/5hrs or 1350/5hr on the pro plan.\n\n[https://synthetic.new/?referral=KBL40ujZu2S9O0G](https://synthetic.new/?referral=KBL40ujZu2S9O0G)",
                  "score": 1,
                  "created_utc": "2026-01-30 16:15:31",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2j79oq",
              "author": "GTHell",
              "text": "Same experience. Why spend $20 just to use something that replicates the OG. It barely any improvement over GLM 4.7 and GLM $40 get you 3 months and the speed is very good.",
              "score": 2,
              "created_utc": "2026-01-30 02:51:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2gjutd",
              "author": "chiroro_jr",
              "text": "What do y'all be doing really?",
              "score": 1,
              "created_utc": "2026-01-29 18:46:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2he0z2",
              "author": "chvmnaveen",
              "text": "I agree with you same behavior for me to on $20 plan. I consumed all the weekly limit in just one night üòí",
              "score": 1,
              "created_utc": "2026-01-29 21:09:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mjlqc",
                  "author": "Grand-Management657",
                  "text": "That's where you're messing up, use synthetic as your provider and you will get more limits. Kimi was limited to 2048 requests/week last I checked. Synthetic is 135/5hrs or 1350/5hr on the pro plan.\n\n[https://synthetic.new/?referral=KBL40ujZu2S9O0G](https://synthetic.new/?referral=KBL40ujZu2S9O0G)",
                  "score": -1,
                  "created_utc": "2026-01-30 16:16:24",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2hel4l",
              "author": "I_HEART_NALGONAS",
              "text": "That's still better than Sonnet 4.5 where a couple of times I blew through Anthropic's ridiculous 5-hour quota in two (2) prompts on the Pro plan.",
              "score": 1,
              "created_utc": "2026-01-29 21:12:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2kiwl3",
              "author": "_Belgarath",
              "text": "It's cheap regarding the API cost. It's about 10x cheaper than Claude when using a per token billing system, not using the subscription.",
              "score": 1,
              "created_utc": "2026-01-30 08:32:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2xn9co",
              "author": "Alternative_Bag_9927",
              "text": "They were giving that 20$ subscription for 0.99$ for a month. I bought it like 2weeks ago",
              "score": 1,
              "created_utc": "2026-02-01 07:29:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dz8i6",
          "author": "Hozukr",
          "text": "Marketing hype is really strong with this one. Running away as fast as possible.",
          "score": 26,
          "created_utc": "2026-01-29 10:34:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dzeg0",
              "author": "Repulsive_Educator61",
              "text": "I tried it and it's somewhere between sonnet 4.5 and opus 4.5\n\ndefinitely not benchmaxxed",
              "score": 22,
              "created_utc": "2026-01-29 10:35:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2f3p92",
                  "author": "kr_roach",
                  "text": "Is it fast? Im using GLM 4.7 but its so slow",
                  "score": 2,
                  "created_utc": "2026-01-29 14:51:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2dzqfu",
                  "author": "Grand-Management657",
                  "text": "That is similar to my findings but I'm not sure if its really better or on par with Sonnet 4.5  \nI think it may just come down to preference or its performance in the harness you use. I was very surprised it wasn't benchmaxxed unlike GLM 4.7. I tried to love that model but nope.",
                  "score": 1,
                  "created_utc": "2026-01-29 10:38:42",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ef7mp",
              "author": "annakhouri2150",
              "text": "Nah, K2.5 is actually this good imo",
              "score": 3,
              "created_utc": "2026-01-29 12:36:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2fb5se",
              "author": "randvoo12",
              "text": "No hype tbh, quality of work produced is comparable to Opus not even Sonnet.",
              "score": 3,
              "created_utc": "2026-01-29 15:26:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2g8bdh",
              "author": "RegrettableBiscuit",
              "text": "I've started running it in opencode today, and it's a strong model. I would put it above GLM4.7 and at least in the same general ballpark as Sonnet 4.5.\n\n\nThere's lots of hype, and I find it increasingly difficult to tell real grassroots support from manufactured hype (like why TF is synthetic mentioned in every post, is that real or just BS), but it is a genuinely good model.¬†",
              "score": 3,
              "created_utc": "2026-01-29 17:54:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mlw54",
                  "author": "Grand-Management657",
                  "text": "I'm not familiar with the hype around synthetic. Its just one of the only providers that I found that is subscription based, has privacy, gets around \\~60tok/s on K2.5, and pretty cheap compared to Sonnet 4.5. There's not really any other options and if you know any, feel free to link it for everyone.",
                  "score": 0,
                  "created_utc": "2026-01-30 16:26:39",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2l5xdy",
              "author": "zarrasvand",
              "text": "Yeah, and it is distilling Claude under the hood anyway so not a lot of new things to see here...",
              "score": 1,
              "created_utc": "2026-01-30 11:52:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2dze6l",
              "author": "Grand-Management657",
              "text": "Which part is hype? Please elaborate.",
              "score": 1,
              "created_utc": "2026-01-29 10:35:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2dztzr",
                  "author": "Repulsive_Educator61",
                  "text": "I would say only [synthetic.new](http://synthetic.new) part is hype, not kimi k2.5\n\nseeing lots of posts here about [synthetic.new](http://synthetic.new), could be their marketing team, i can't confirm",
                  "score": 5,
                  "created_utc": "2026-01-29 10:39:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2fpd52",
          "author": "Muted_Standard175",
          "text": "Have anyone tried to use opus 4.5 or gpt 5.2 as plan and k2.5 as build? How good it was?",
          "score": 4,
          "created_utc": "2026-01-29 16:29:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2js2xn",
              "author": "degenbrain",
              "text": "In my case, I did it the other way around. K2.5 tends to provide simple solutions and plans. There are no additional features. It's straightforward. Then, I ask Opus to execute it perfectly",
              "score": 1,
              "created_utc": "2026-01-30 04:57:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2efqyi",
          "author": "HotFats",
          "text": "I think k2.5 is definitely better than sonnet might be performing as close to opus. Its not only cheaper, but its way faster. Alsovi use synthetic.new, its pretty good. I think K2.5 with thinking is the closet we've gotten to giving anthropic models a run for their money. Currently its handling browser automation and building scripts and n8n workflows just as well if not better than opus  4.5. Not canceling my claude max subscription yet, but its promising.",
          "score": 3,
          "created_utc": "2026-01-29 12:40:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2egq4o",
              "author": "Grand-Management657",
              "text": "I would wait for two more weeks to cancel that sub. I think deepseek v4 might be even better and potentially releasing before the chinese new lunar year. And that gives you enough time to really put K2.5 to the test.",
              "score": 3,
              "created_utc": "2026-01-29 12:46:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ee6jl",
          "author": "MegamillionsJackpot",
          "text": "https://preview.redd.it/wlhxth6c8agg1.jpeg?width=1080&format=pjpg&auto=webp&s=eb03955b77875aaee196565e06e862affd807354\n\nExpensive if you are not on a plan?",
          "score": 2,
          "created_utc": "2026-01-29 12:29:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eer53",
              "author": "Grand-Management657",
              "text": "Seems like you are looking at agent swarm which I do not know too much of. I do know that it spins up hundreds of K2.5's, so its going to cost significantly more. Using the model without swarm is $0.50 in/$3.00 out with API rates. With nano or synthetic as providers, your cost is significantly lower than API rates.",
              "score": 2,
              "created_utc": "2026-01-29 12:33:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2efty9",
                  "author": "MegamillionsJackpot",
                  "text": "Yeah, I know. It's just a funny bug in the pricing. And that bug was there before I wrote the agent swarm thing.\n\n\nDo you know if synthetic models work okay for multi step deep research?",
                  "score": 1,
                  "created_utc": "2026-01-29 12:40:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2eubz0",
          "author": "Salty-Standard-104",
          "text": "PR slop. why kimi would hire such terrible person for writing crap like this?",
          "score": 2,
          "created_utc": "2026-01-29 14:03:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fw2gb",
              "author": "seaal",
              "text": "kimi? this chud and all the others are just spamming their referral links are just trying to get their credits for nanogpt and synthetic.new.",
              "score": 2,
              "created_utc": "2026-01-29 16:59:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2k90zr",
          "author": "Lower_Temperature709",
          "text": "I have been working with minimax + glm + codex + code. All bare minimum plan. Coding non stop from last week. It‚Äôs crazy efficient and dirt cheap. \n\nUsing oh my open code as the agent harness with alots of agent and sub agent configured.",
          "score": 2,
          "created_utc": "2026-01-30 07:05:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dzu61",
          "author": "N2siyast",
          "text": "No way Im using this vibe coded slop site",
          "score": 2,
          "created_utc": "2026-01-29 10:39:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e0pub",
              "author": "Grand-Management657",
              "text": "Haha I agree. I was just browsing earlier and saw the home page and it is ugly",
              "score": 0,
              "created_utc": "2026-01-29 10:47:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2e0bup",
          "author": "BitterAd6419",
          "text": "Kimi is better than GLM but not as good as anthropic models.",
          "score": 2,
          "created_utc": "2026-01-29 10:43:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e5cpp",
              "author": "awfulalexey",
              "text": "GLM has approximately 350 billion parameters, Kimi has 1 trillion parameters. It's interesting why Kimi is stronger than GLM.",
              "score": 4,
              "created_utc": "2026-01-29 11:25:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ec0si",
                  "author": "Grand-Management657",
                  "text": "Not sure where I read it but K2.5 is built on K2 but with an additional training of 15 trillion mixed visual and text tokens. Not sure about GLM 4.7 but I would suspect its nowhere close to that.",
                  "score": 2,
                  "created_utc": "2026-01-29 12:15:05",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2e1086",
              "author": "Grand-Management657",
              "text": "This is a reasonable take. Is your use case mostly web? I haven't gotten a chance to test it on anything other than web development.",
              "score": 1,
              "created_utc": "2026-01-29 10:49:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2e9a5r",
                  "author": "BitterAd6419",
                  "text": "Yea only tested on few web related task nothing too complicated",
                  "score": 1,
                  "created_utc": "2026-01-29 11:55:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2grb7t",
          "author": "alexeiz",
          "text": "You're here just to push your referrals.  That's it.",
          "score": 3,
          "created_utc": "2026-01-29 19:20:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e07y8",
          "author": "joakim_ogren",
          "text": "Does Synthetic.new support Kimi K2.5? (It seems supported by vLLM)",
          "score": 1,
          "created_utc": "2026-01-29 10:42:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e0u9c",
              "author": "Grand-Management657",
              "text": "They do support it but since its a new model, they haven't updated the page I'm guessing. Go to [https://synthetic.new/pricing](https://synthetic.new/pricing) and you will see it in the list.",
              "score": 5,
              "created_utc": "2026-01-29 10:48:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2e8z2y",
          "author": "seeKAYx",
          "text": "$10 discount / month with that referral or only first month?",
          "score": 1,
          "created_utc": "2026-01-29 11:53:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ea8bm",
              "author": "Grand-Management657",
              "text": "I'm pretty sure its only the first month. I wish it was recurring!\n\nhttps://preview.redd.it/l9hg8i5e3agg1.png?width=460&format=png&auto=webp&s=037b29a3023c9f2205bf51c2f67e020038f70145",
              "score": 1,
              "created_utc": "2026-01-29 12:02:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2f7hyt",
          "author": "Galendel",
          "text": "I am using deepseek v3.2 with and without thinking, I really like it for the cost, did anyones else use deepseek ?",
          "score": 1,
          "created_utc": "2026-01-29 15:09:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f8efy",
              "author": "Grand-Management657",
              "text": "I really like deepseek v3.2 for creative writing. I think it would be great for its intelligence and writing style even in agentic coding. But it just wasn't tailored towards software development like claude models, Kimi K2.5, or GLM 4.7\n\nFor the cost though, its hard to beat. Almost costs nothing to run. I have very high hopes for deepseek v4 and I think that will be on par with Opus 4.5, or at least I hope. Fingers crossed!",
              "score": 2,
              "created_utc": "2026-01-29 15:13:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2h8kmn",
                  "author": "Galendel",
                  "text": "I am spending like 3-4$ a day on it, the code he does is fine to me, it's just too slow and way more  with thinking, on aider benchmark [https://aider.chat/docs/leaderboards/](https://aider.chat/docs/leaderboards/) Kimi K2 is really low compare to deepseek. I tried GLM 4.7 free on zen ai and it was really bad for agentic coding, maybe they are overloaded. The ratio quality / cost doesn't seem to be a subject, but to me if a good LLM is 10x cheaper it can do 9x more coding with same budget. It's been a while I didn't use subscription so I can't compare yet.",
                  "score": 2,
                  "created_utc": "2026-01-29 20:43:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gtr37",
          "author": "SunflowerOS",
          "text": "Can I use my suscription on opencode like Anthropic or I need to pay the api?",
          "score": 1,
          "created_utc": "2026-01-29 19:31:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gybi9",
              "author": "Grand-Management657",
              "text": "Yes you can use any subscription with opencode but I don't recommend using claude subscription on opencode. They will ban you.\n\nThe two I recommend is\n\nNano-gpt: https://nano-gpt.com/invite/mNibVUUH\n\nor\n\nSynthetic: https://synthetic.new/?referral=KBL40ujZu2S9O0G",
              "score": 0,
              "created_utc": "2026-01-29 19:53:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2gzf7c",
                  "author": "SunflowerOS",
                  "text": "I know it, but I suscribe to kimi on december thinking that i could use it on opencode",
                  "score": 1,
                  "created_utc": "2026-01-29 19:58:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gxlll",
          "author": "VaizardX",
          "text": "How did you setup the orchestrator and agents?",
          "score": 1,
          "created_utc": "2026-01-29 19:50:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gzjxn",
              "author": "Grand-Management657",
              "text": "In OpenCode, you can set a specific model for a subagent by configuring the model property in the subagent's definition within the opencode.json or opencode.jsonc configuration file.\n\nYou can find more information here: https://opencode.ai/docs/agents",
              "score": 1,
              "created_utc": "2026-01-29 19:59:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2h8u8u",
              "author": "Galendel",
              "text": "have a look at bmad, they do have an orchestrator",
              "score": 1,
              "created_utc": "2026-01-29 20:44:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2s9nwn",
                  "author": "Pleasant_Thing_2874",
                  "text": "BMAD is god tier imo for proper project development",
                  "score": 1,
                  "created_utc": "2026-01-31 13:21:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2l5v22",
          "author": "zarrasvand",
          "text": "https://preview.redd.it/c5h94oni6hgg1.png?width=1266&format=png&auto=webp&s=bd25da84a71a56eaa05ea6dfeb42dbd2196d3407\n\n\"Ok bro\"",
          "score": 1,
          "created_utc": "2026-01-30 11:52:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mpsi4",
          "author": "Grand-Management657",
          "text": "For those of you wondering about speeds\n\nI am currently getting \\~18tok/s with nano-gpt and \\~60tok/s with synthetic.\n\nI recommend synthetic for any enterprise workloads or anything you will make money from. Its super fast, privacy centered and much cheaper than Sonnet 4.5. It also gives you the stability that is required for enterprise workloads. Combine it with your favorite frontier model (Opus 4.5/GPT 5.2) for best performance.\n\nNano-gpt is much slower but much more economical. Recommending this for side projects and hobbyists. I find this to be a great option if you need to spin up many subagents at once. Currently there are some multi-turn tool call issues which the devs are working on actively to rectify. Combine with your favorite frontier model to get best results (Opus 4.5/GPT 5.2)\n\nNano: [https://nano-gpt.com/invite/mNibVUUH](https://nano-gpt.com/invite/mNibVUUH)\n\nSynthetic: [https://synthetic.new/?referral=KBL40ujZu2S9O0G](https://synthetic.new/?referral=KBL40ujZu2S9O0G)",
          "score": 1,
          "created_utc": "2026-01-30 16:43:53",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2ok1rf",
          "author": "Purple_Wear_5397",
          "text": "Thanks for the feedback, one question: which provider do you use to consume K2.5 with high parallelism ?",
          "score": 1,
          "created_utc": "2026-01-30 21:45:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2oxmrp",
              "author": "Grand-Management657",
              "text": "I've been using synthetic because when deploying parallel agents, speed matters. How much that matters is really up to you. The wait for more complex reasoning and longer chain of thoughts becomes exponentially higher if you use a more economical provider. But for any sort of production or enterprise work, you want to be running the fastest you can. Synthetic runs at ~100tok/s and nano at ~18tok/s.\n\nTry Synthetic at a discount: https://synthetic.new/?referral=KBL40ujZu2S9O0G",
              "score": 1,
              "created_utc": "2026-01-30 22:52:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ow4o5",
          "author": "pbalIII",
          "text": "Ran K2.5 for a week on a mixed TypeScript/Python codebase. Few observations from the trenches:\n\n- Frontend gen is where it shines. Visual debugging loop (screenshot, fix, verify) genuinely works and saved hours on CSS issues.\n- Backend refactors hit the SWE-bench gap. That 76.8% vs Claude's 80.9% shows up when you're touching multiple files with shared state.\n- Speed matters for iteration. 34 tok/s vs Sonnet's 91 means longer feedback loops. If you're doing tight edit-test cycles, that adds up.\n\nThe Opus-orchestrator + K2.5-workers pattern you mentioned is probably the right call. Route expensive reasoning to Claude, parallelize the grunt work with K2.5. CLI tooling is still rough though... no cost tracking and context fills fast.",
          "score": 1,
          "created_utc": "2026-01-30 22:44:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2oz7og",
              "author": "Grand-Management657",
              "text": "Great insights. Was that 80.9% SWE bench for opus 4.5 or sonnet 4.5? If that's opus, then I'm surprised K2.5 got that close to it. Also, I 100% agree on the speed part, especially when using agents in parallel. You don't want your parallelization to get sluggish due to slow inference, especially if your complexity requires more interleaved thinking. \n\nI did find synthetic as a provider to have the speed required to keep up with Opus 4.5 as an orchestrator at ~100tok/s. They are using firework ai's infrastructure, which is known for its stability and speed. I honestly have no clue how they managed those kind of speeds, but I'll take it!",
              "score": 1,
              "created_utc": "2026-01-30 23:01:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o35ytnf",
                  "author": "pbalIII",
                  "text": "That 80.9% was Opus 4.5. K2.5 hitting 76.8% is impressive for open-source, especially at those inference speeds.",
                  "score": 1,
                  "created_utc": "2026-02-02 15:05:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zeycz",
          "author": "sharp-dev",
          "text": "I don‚Äôt get how nanogpt can be so cheap",
          "score": 1,
          "created_utc": "2026-02-01 15:28:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30jiok",
              "author": "Grand-Management657",
              "text": "Nano-gpt is an aggregator, meaning they will route your requests to one of its in-network providers. Most people will never come close to their entire quota for the month and nano-gpt knows this. Its baked into their pricing. Also its not the most stable or consistent experience, sometimes you will get tool call failures and slower inference. But on a budget, its unbeatable. If you need privacy, consistency, and speed, synthetic is the best option as of right now.",
              "score": 1,
              "created_utc": "2026-02-01 18:34:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3byc9m",
          "author": "benyamynbrkyc",
          "text": "Honestly not sure how people are reaching the limits that fast, I've been using it extensively every day since release on the Moderato plan ($20), and haven't even come close to hitting the limits. I know that's pretty subjective but compared to something like Claude Code Pro ($20) it's not even fair to compare the amount of usage you get with Kimi",
          "score": 1,
          "created_utc": "2026-02-03 12:37:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3cmg0s",
              "author": "Grand-Management657",
              "text": "Compared to claude code pro, anything is better haha. I do think kimi increased their limits recently, but still synthetic is a better deal, faster (usually) and private.",
              "score": 1,
              "created_utc": "2026-02-03 14:54:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dzhon",
          "author": "pokemonplayer2001",
          "text": "Always be shilling.\n\nHaha, you mad.",
          "score": 1,
          "created_utc": "2026-01-29 10:36:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2gykks",
          "author": "mustafamohsen",
          "text": "Kimi, you still post slop?",
          "score": 0,
          "created_utc": "2026-01-29 19:54:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu44yh",
      "title": "Notes after using Claude Code and OpenCode side by side",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qu44yh/notes_after_using_claude_code_and_opencode_side/",
      "author": "Arindam_200",
      "created_utc": "2026-02-02 19:00:16",
      "score": 56,
      "num_comments": 27,
      "upvote_ratio": 0.91,
      "text": "I‚Äôve been using Claude Code pretty heavily for day-to-day work. It‚Äôs honestly one of the first coding agents I‚Äôve trusted enough for real production tasks.\n\nThat said, once you start using it *a lot*, some tradeoffs show up.\n\nCost becomes noticeable. Model choice matters more than you expect. And because it‚Äôs a managed tool, you don‚Äôt really get to see or change how the agent works under the hood. You mostly adapt your workflow to it.\n\nOut of curiosity, I started testing OpenCode (Got Hyped up from X & reddit TBH). Didn‚Äôt realize how big it had gotten until recently. The vibe is very different.\n\nClaude Code feels guarded and structured. It plans carefully, asks before doing risky stuff, and generally prioritizes safety and predictability.\n\nOpenCode feels more like raw infrastructure. You pick the model per task. It runs commands, edits files, and you validate by actually running the code. More control, less hand-holding.\n\nBoth got the job done when I tried real tasks (multi-file refactors, debugging from logs). Neither ‚Äúfailed.‚Äù The difference was *how* they worked, not whether they could.\n\nIf you want something managed and predictable, Claude Code is great. If you care about flexibility, cost visibility, and owning the workflow, OpenCode is interesting.\n\nI wrote up a longer comparison [here](https://www.tensorlake.ai/blog/opencode-the-best-claude-code-alternative) if anyone wants the details.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qu44yh/notes_after_using_claude_code_and_opencode_side/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o38ob1f",
          "author": "Guinness",
          "text": "CC has horrible TUI issues, bad framerates, and oh god the bug where it constantly jumps to the top of the history scrollback.\n\nCC introduced me to all of these tools after years of development with nano/vi{m}, but it was so buggy I branched out to cline, and eventually to opencode.\n\nOpencode is clearly the winner right now. It feels like CC was written by an LLM. Like....it works? But its buggy as fuck.",
          "score": 8,
          "created_utc": "2026-02-02 22:41:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38rmsv",
              "author": "Positive-Badger6588",
              "text": "the TUI has issues but ive kept tryng to go back to opencode just to find out the performance and harness is just much better optimized on cc. I have it hooked up to a nvim wrapper so all the diffs show up in nvim for me. the TUI outside of general explanation just becomes a bit irrelavant for me.",
              "score": 1,
              "created_utc": "2026-02-02 22:58:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3bdv87",
              "author": "ReporterCalm6238",
              "text": "I mean opencode was also written by LLMs",
              "score": 1,
              "created_utc": "2026-02-03 09:45:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3afs82",
              "author": "Accomplished-Toe7014",
              "text": "Well Anthropic was seemingly proud when they said that 100% of their code is written by AIs",
              "score": 0,
              "created_utc": "2026-02-03 04:47:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37ijy1",
          "author": "hey_ulrich",
          "text": "The only feature from CC that I miss is that CC can run bash commands in the background and easily check its logs. That's it. For everything else, OpenCode is superior. Once you set your custom modes, and add plugins, it's awesome.",
          "score": 13,
          "created_utc": "2026-02-02 19:22:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37wb6o",
              "author": "nmiljkovic89",
              "text": "There is an opencode pty plugin for that",
              "score": 7,
              "created_utc": "2026-02-02 20:27:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37zkdh",
                  "author": "hey_ulrich",
                  "text": "Thanks, I'll check it out",
                  "score": 1,
                  "created_utc": "2026-02-02 20:42:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o393m7x",
                  "author": "MyriadAsura",
                  "text": "Care to share a link brother?",
                  "score": 1,
                  "created_utc": "2026-02-03 00:03:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o38qwbj",
          "author": "poop_harder_please",
          "text": "is anyone still using the oauth authentication method? Anthropic sort of backpedaled on banning accounts for using oauth with external providers, but I'm worried about taking the risk on my account until it's a sure thing",
          "score": 3,
          "created_utc": "2026-02-02 22:54:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3aq3sp",
              "author": "Fickle_Permi",
              "text": "Yeah I use it with no problem. I‚Äôm not a heavy user though. I maybe hit the daily limit once a week.",
              "score": 2,
              "created_utc": "2026-02-03 06:05:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o38skkd",
              "author": "Positive-Badger6588",
              "text": "wait, are they blocking access or banning people form if using it? blocking users sounds kind of retarded lol",
              "score": 1,
              "created_utc": "2026-02-02 23:03:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3capjw",
                  "author": "poop_harder_please",
                  "text": "Apparently full blocks. But they might‚Äôve lightened up after the backlash and the counter positioning of OAI‚Äôs plans being third-party-harness-friendly",
                  "score": 1,
                  "created_utc": "2026-02-03 13:51:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o38sogz",
              "author": "FunnyRocker",
              "text": "I'd really like to know this also",
              "score": 1,
              "created_utc": "2026-02-02 23:04:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37nviq",
          "author": "ellensen",
          "text": "I have connected my subscriptions to opencode, seems to give me the same control over cost as if using the subscription by the provider directly without opencode?",
          "score": 2,
          "created_utc": "2026-02-02 19:47:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37tu8w",
          "author": "vixalien",
          "text": "I feel like OpenCode is much more ambitious than CC, not in a good way. For example, when you ask it why it did something in a certain way, instead of explaining why, it will just undo the change.\n\nOpenCode also seems to use dangerous commands more often, especially with git. It commits everything, and when you ask it to revert, it will happily git reset everything, including any uncommitted changes YOU (not OpenCode) had made.",
          "score": 2,
          "created_utc": "2026-02-02 20:15:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o389fir",
              "author": "Arindam_200",
              "text": "Interesting\n\nI haven't personally faced this problem. But I'll give this a try",
              "score": 1,
              "created_utc": "2026-02-02 21:29:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o38zizc",
          "author": "ianxiao",
          "text": "The only i miss when moving from CC is /rewind . Opencode has something less powerful /undo but it‚Äôs tedious to use and buggy.",
          "score": 2,
          "created_utc": "2026-02-02 23:41:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3au8fx",
              "author": "aeroumbria",
              "text": "I hate it that a lot of their functions are locked to CLI and not available in the VSCode extention, but their CLI glitches out like crazy in IDE terminals, and I dislike infinite scrolling CLI in IDEs.",
              "score": 1,
              "created_utc": "2026-02-03 06:40:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o392wv3",
          "author": "Western_Objective209",
          "text": "what can opencode actually do (besides use different models) that claude code can't, like what actually makes it more flexible? Claude code supports plugins, MCPs, and skills, in your extended write up:\n\n> Extensibility: [Claude Code] Managed core with limited extension of agent internals, \t[OpenCode] Open source, extensible with internal tools\n\nLike you don't even seem to understand how to extend claude code and \"open source\" is not an extension model",
          "score": 2,
          "created_utc": "2026-02-03 00:00:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3aovzm",
          "author": "cafesamp",
          "text": "I know there‚Äôs mixed opinions on how opinionated your workflow should be with these tools, but I‚Äôve found Superpowers w/ OpenCode (you can use it with Claude Code too, but it‚Äôs more redundant there) to be actually really awesome in making coding with OpenCode a more structured experience.\n\nThe main thing OpenCode can really fail on is picking back up on things if you need to run out of usage and/or need to switch models/providers.  OpenCode‚Äôs flow is already so rigid (with task management being handled in the conversation), and Superpowers expects the full lifecycle of its skills to complete a task, and doesn‚Äôt wrap up things if you get interrupted.\n\nThat being said, I like using Superpowers in CC with Opus for brainstorming and planning, and then having it write to files that I can pick up and continue to work with in OpenCode with Codex for implementation/testing/review.\n\nPerfect?  No, but great mileage out of two $20/mo subscriptions.",
          "score": 1,
          "created_utc": "2026-02-03 05:55:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3avs9f",
          "author": "Holiday_Degree_7721",
          "text": "opencode have critical issues with memory leaking, thats why I moved to cc",
          "score": 1,
          "created_utc": "2026-02-03 06:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bf865",
          "author": "raydou",
          "text": "for me the only thing i'm missing is the integration of CC rules files in OpenCode. I like the way it's surgical and don't load the context so much. \nI made a PR for this but it seems that OpenCode team only check the PRs of their buddies. No review nothing on mine since 2 weeks.. \nAt this rythm PR won't be mergeable and I would have to reedit all the changes ..",
          "score": 1,
          "created_utc": "2026-02-03 09:58:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bjmsu",
          "author": "Tushar_BitYantriki",
          "text": "The only thing lagging in OpenCode is the lack of hooks in any language, like Claude code supports.\n\nWhat's the point of binding it to JS?\n\nI had posted a migration guide and a skill on this sub to move from Claude Code to OpenCode. But I am yet to find a clean way to migrate my Python+shell hooks from Claude code.\n\nFor me, hooks are a crucial part of my workflow, and I have collected a lot over the months, going from simple grep and regex matches in a shell script to AST-based DDD-enforcing via Python's tree-sitter.\n\nIt seems that OpenCode was made by the JS folks, for the JS folks.",
          "score": 1,
          "created_utc": "2026-02-03 10:39:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3cfsb0",
          "author": "SpecKitty",
          "text": "I like running both - one implements, the other reviews. I manage with Spec Kitty.",
          "score": 1,
          "created_utc": "2026-02-03 14:19:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dc97r",
          "author": "buggytheking",
          "text": "Loved this",
          "score": 1,
          "created_utc": "2026-02-03 16:56:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtqx2p",
      "title": "OpenCode Bar 2.1: Now with CLI + Per-Provider Subscription Tracking",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/sxdtdls302hg1.png",
      "author": "kargnas2",
      "created_utc": "2026-02-02 09:54:58",
      "score": 51,
      "num_comments": 12,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qtqx2p/opencode_bar_21_now_with_cli_perprovider/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o35kmbo",
          "author": "Possible-Text8643",
          "text": "mac only?",
          "score": 3,
          "created_utc": "2026-02-02 13:49:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38ikf2",
              "author": "trenescese",
              "text": "I think you'd need something like https://yasb.dev/ to run a widget like that on windows?",
              "score": 1,
              "created_utc": "2026-02-02 22:12:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o34t6uv",
          "author": "jellydn",
          "text": "Nice. Thanks, I will give it a try soon :)",
          "score": 1,
          "created_utc": "2026-02-02 10:28:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34xfkk",
          "author": "Financial_Reward2512",
          "text": "Any similar product where we can use Claude Code with multiple Provider and show this bar as well.?",
          "score": 1,
          "created_utc": "2026-02-02 11:06:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35f0ob",
              "author": "United_Bandicoot1696",
              "text": "Quotio",
              "score": 1,
              "created_utc": "2026-02-02 13:17:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o36ptdn",
          "author": "_w_8",
          "text": "neat!! how are you using claude code sub in opencode still though?",
          "score": 1,
          "created_utc": "2026-02-02 17:11:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37banq",
          "author": "Specialist-Yard3699",
          "text": "Looks nice. No Linux in plans?",
          "score": 1,
          "created_utc": "2026-02-02 18:49:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dbqhl",
              "author": "buggytheking",
              "text": "I've made something similar on Linux. Check it out here and lemme know what else to add.https://github.com/OmegAshEnr01n/GnomeCodexBar",
              "score": 2,
              "created_utc": "2026-02-03 16:54:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ddfr6",
                  "author": "Specialist-Yard3699",
                  "text": "Will test today. Any plans for opencode-zen/hype-chinese providers(zai/minimax/kimi)?",
                  "score": 1,
                  "created_utc": "2026-02-03 17:02:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37hzpv",
          "author": "0sko59fds24",
          "text": "Anthropic still bans users using CC in opencode right",
          "score": 1,
          "created_utc": "2026-02-02 19:19:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bw028",
          "author": "VlaadislavKr",
          "text": "How to connect gemini quota based?",
          "score": 1,
          "created_utc": "2026-02-03 12:20:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3c0gnu",
          "author": "renan_william",
          "text": "Works on Intel Mac or just Silicon? ",
          "score": 1,
          "created_utc": "2026-02-03 12:51:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsyzso",
      "title": "GPT 5.2 for difficult things and Kimi K2.5 for everything else seems to be the move, what the cheapest way to get there?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qsyzso/gpt_52_for_difficult_things_and_kimi_k25_for/",
      "author": "SweatyHands247",
      "created_utc": "2026-02-01 13:51:45",
      "score": 44,
      "num_comments": 30,
      "upvote_ratio": 0.97,
      "text": "Once the free period of Kimi K2.5 is finished, what's the cheapest, fast and private way to access it?\n\n  \nWe'll also want GPT access to tactically use it when necessary. What's the most cost effective way for this.\n\n  \nAnyone got OpenCode Black 20 access? Will that do the job? I imagine it'll get you pretty far for K2.5, but what about with some GPT sprinkled in. \n\nOr maybe Black 20 and a Chutes sub? \n\nAny other ideas?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qsyzso/gpt_52_for_difficult_things_and_kimi_k25_for/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2yy0fp",
          "author": "Simple_Split5074",
          "text": "Probably ChatGPT Plus (or maybe GH Copilot) and synthetic - nanogpt I found sadly cannot do Kimi K2.5 properly (otherwise probably the best deal out there if you want multiple models) and chutes I never had good experience with. To be fair, I have not yet signed up for synthetic.\n\nFWIW, the thinking goes that glm5 and DS4 might appear in the next two weeks (before CNY), picture might look different again then.",
          "score": 12,
          "created_utc": "2026-02-01 13:57:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zivvx",
              "author": "AlergDeNebun",
              "text": "Same, on nano-gpt, my experience so far is that it's quite slow and K2.5 tool calling is just broken 80% of the time, so unfortunately unusable for real work.\n\nThis being said, if they fix these issues and maybe improve the speed a bit (I'm OK with it being slower, but not THAT slow) it's a GREAT deal. But as it stands, if these issues are not resolved by the time my first month is up, I'm unsubscribing.",
              "score": 3,
              "created_utc": "2026-02-01 15:47:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zkoxz",
                  "author": "Simple_Split5074",
                  "text": "It also just flat out fails to respond at all quite often, according rto the discord they are trying to fix it",
                  "score": 2,
                  "created_utc": "2026-02-01 15:55:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2yxqw7",
          "author": "Recent-Success-1520",
          "text": "Depending upon your usage - Codex $20 + Nano-GPt  $8",
          "score": 4,
          "created_utc": "2026-02-01 13:56:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zjp2q",
              "author": "AlergDeNebun",
              "text": "It WOULD be a great deal, but here is a screenshot of me using Nano-GPT and trying to summarize some random files in a directory (it's my go to test for several reasons, just throw a bunch of text files together, some related some not, and ask the models to do things).\n\nYou can see how much it took, and at the end the tool calling failed.\n\nI get similar failures for Kimi K2.5 .\n\nSo my experience so far: very cheap, HUGE limits, but also slow and unreliable to the point of being unusable in the real world, for anything other than asking direct questions.\n\nhttps://preview.redd.it/g0yqofemmwgg1.png?width=1859&format=png&auto=webp&s=d10f14f2f650ca9212c628ee527a80cee62debe6",
              "score": 3,
              "created_utc": "2026-02-01 15:50:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34k7fx",
                  "author": "TastyIndividual6772",
                  "text": "I had this tool call failures on multiple plugins/clis on multiple Chinese models. I haven‚Äôt had those inside kimi cli tho.",
                  "score": 1,
                  "created_utc": "2026-02-02 09:01:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2z1gvy",
              "author": "Ang_Drew",
              "text": "nano gpt seems to be good offer. what is the 60k message a month mean?\n\ndid tool call considered 1 messages?",
              "score": 1,
              "created_utc": "2026-02-01 14:17:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zkt8d",
                  "author": "Simple_Split5074",
                  "text": "My usage logs suggest they do count as a message",
                  "score": 1,
                  "created_utc": "2026-02-01 15:56:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30jpzz",
          "author": "drinksbeerdaily",
          "text": "Is kimi 2.5 really good enough to replace opus 4.5 for planning or debugging?",
          "score": 2,
          "created_utc": "2026-02-01 18:35:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32iu1t",
              "author": "Grand-Management657",
              "text": "No, I would not say so. In my experience, Opus 4.5 for planning, Kimi K2.5 for execution, and then review with GPT 5.2. Gets 3 LLMs to look at the same problem while being economical since most of the output happens on K2.5. \n\nK2.5 is not on the same level as Opus or GPT 5.2 but close to Sonnet 4.5 performance. I wrote more about it in my post here: https://www.reddit.com/r/opencodeCLI/s/MJoHjOdZGq",
              "score": 2,
              "created_utc": "2026-02-02 00:30:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36fq8i",
                  "author": "trypnosis",
                  "text": "Man after my own heart I never let model x review the work of model x. Model y should be used to review model x.\n\nI have not had much luck with kimi over the weekend  on synthetic but I hear they are now hosting it them selves so it is meant to be faster so will be trying that from tomorrow.",
                  "score": 1,
                  "created_utc": "2026-02-02 16:25:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o32i9n5",
          "author": "Grand-Management657",
          "text": "Gpt 5.2 or Opus 4.5 as orchestrator, Kimi K2.5 for execution. Run K2.5 through synthetic and $20 codex sub for orchestration (run it in xhigh). It's pretty much the most economical combination while retaining performance. I wrote about the economics in my post here: https://www.reddit.com/r/opencodeCLI/s/MJoHjOdZGq",
          "score": 2,
          "created_utc": "2026-02-02 00:27:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31tmk5",
          "author": "oknowton",
          "text": "I think you're on the right track.  Chutes and Synthetic are two of the subscriptions I have active right now.  I'm having success with Kimi K2.5 on both, but I am not a professional programmer writing code 8 hours a day, and my coding work is pretty simple compared to what most people in here are doing.\n\nSynthetic is about 3x faster right Chutes, at least for Kimi K2, but Chutes has bigger quotas at a fraction of the price.\n\n> Or maybe Black 20 and a Chutes sub? \n\nI think OpenCode Black is too new to pin down.  They haven't published any information about how their quotas will work, and they probably won't have any of that nailed down until after the service goes live for everyone.\n\nOpenAI seems friendly towards the OpenCode community, and I'm always hearing people say how plentiful their quotas are compared to Anthropic.  Pairing a $20 Codex subscription with a $3 or $10 Chutes subscription seems like a good place to start.",
          "score": 3,
          "created_utc": "2026-02-01 22:14:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33th7b",
          "author": "TreeBearr",
          "text": "I've had the best luck with synthetic for the good open weight models. The inference can be a lil slow sometimes but the cost and privacy is worth. I use it with opencode and OpenWebUI.\n\nI think they're pretty generous usage limits for the $20 plan. 135 requests per 5 hours and the requests are fractional if they are tool calls or small agent runs. I'll insert a ss of the billing page.  \n\n\nIf u get referred you get some credit :3\n\n[https://synthetic.new/?referral=7JlVOLCkmEQv5oI](https://synthetic.new/?referral=7JlVOLCkmEQv5oI)\n\nhttps://preview.redd.it/v9co646ak0hg1.png?width=934&format=png&auto=webp&s=425697296c73a8656b541efbff6e16e6de94e4a2",
          "score": 2,
          "created_utc": "2026-02-02 05:08:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o340flg",
          "author": "bduyng",
          "text": "That combo makes sense ‚Äî GPT-5.2 for heavy lifting and Kimi K2.5 for cheap grunt work seems solid üëç",
          "score": 1,
          "created_utc": "2026-02-02 06:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34p4o2",
          "author": "LordEli",
          "text": "i've had a lot of success with just kimi k2.5 alone but wondering how rate limits work",
          "score": 1,
          "created_utc": "2026-02-02 09:49:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35h7oy",
          "author": "pbalIII",
          "text": "One angle worth considering: task-based routing instead of just model tiers.\n\nK2.5 at $0.60/1M input and $3/1M output is already cheap. GPT 5.2 is ~10x more expensive per token. The math flips depending on what you're actually doing.\n\nFor agentic work with lots of tool calls and iterations, K2.5 handles the volume. For single-shot complex reasoning where you need one good answer, GPT 5.2 pays for itself in fewer retries.\n\nSo rather than Black 20 + another sub, you could route at the task level: classify incoming requests by complexity and let a cheap router model decide. OpenRouter and similar services support this natively. Cuts costs 30-40% without degrading quality on the tasks that matter.",
          "score": 1,
          "created_utc": "2026-02-02 13:30:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2z77hs",
          "author": "Bob5k",
          "text": "[synthetic](https://synthetic.new/?referral=IDyp75aoQpW9YFt) as a provider for kimi / glm / minimax is a nobrainer, especially when you also consider reliable infrastructure and zero data retency as important feature.",
          "score": 0,
          "created_utc": "2026-02-01 14:49:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2z83ek",
              "author": "Funny-Advertising238",
              "text": "Literally anything is better than synthetic don't listen to this guy¬†",
              "score": 3,
              "created_utc": "2026-02-01 14:53:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2z9nv9",
                  "author": "lundrog",
                  "text": "Not sure why you hate them but performance is good and you get good value for a monthly. If you can find me a subscription for the same or less with the usage I need that performs as well im all ears üòÇ (not nano gpt ) they were called out on Reddit via a provider for credit fraud",
                  "score": 5,
                  "created_utc": "2026-02-01 15:01:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2zknxz",
                  "author": "red_rolling_rumble",
                  "text": "It would be more convincing if you would explain how it‚Äôs bad!",
                  "score": 2,
                  "created_utc": "2026-02-01 15:55:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o35okb7",
                  "author": "blankeos",
                  "text": "Like what? :o",
                  "score": 1,
                  "created_utc": "2026-02-02 14:11:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zae4t",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-02-01 15:05:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zl7ro",
              "author": "Simple_Split5074",
              "text": "As opposed to spamming referral links...",
              "score": 1,
              "created_utc": "2026-02-01 15:57:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qs629t",
      "title": "Which coding plan?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qs629t/which_coding_plan/",
      "author": "Simple_Split5074",
      "created_utc": "2026-01-31 15:59:52",
      "score": 42,
      "num_comments": 41,
      "upvote_ratio": 0.98,
      "text": "OK so \n\n* GLM is unusably slow lately (even on pro plan; the graphs on the site showing 80tps are completely made up if you ask me)\n* nanogpt Kimi 2.5 mostly fails \n* Zen free Kimi 2.5 works until it does not (feels like it flip flops every hour). \n\nI do have a ChatGPT Plus sub which works but the quota is really low, so really only use it when I get stuck.\n\nThat makes me wonder where to go from here?\n\n* ChatGPT Pro: models are super nice, but the price,; the actual limits are super intransparent, too....\n* Synthetic: hard to say how much use you really get out of the 20$ plan? Plus how fast / stable are they (interestedin Kimi 2.5, potentially GLM5 and DS4 when they arrive)? Does caching work (that helps a lot with speed)?\n* Copilot: Again hard to understand the limits. I guess the free trial would shed light on it?\n\nAny other ideas? Thoughts?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qs629t/which_coding_plan/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2t3csi",
          "author": "soul105",
          "text": "GH Copilot is really easy to understand their limits: they are based on requests, and that's it.",
          "score": 19,
          "created_utc": "2026-01-31 16:03:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2t3zg0",
              "author": "Michaeli_Starky",
              "text": "Except for it's not THAT straightforward when it comes to counting the requests.",
              "score": 6,
              "created_utc": "2026-01-31 16:06:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2t4n1n",
                  "author": "Simple_Split5074",
                  "text": "This. Supposedly only user input counts but even that is hard to make sense of.",
                  "score": 1,
                  "created_utc": "2026-01-31 16:09:37",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2tjepz",
              "author": "NerasKip",
              "text": "Is opencode do a compact then continue. It count as 3 requests add 2 mores for each compact/continue",
              "score": 1,
              "created_utc": "2026-01-31 17:20:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tkypj",
          "author": "warpedgeoid",
          "text": "GitHub Copilot is a steal for $40/month. It has all of the most recent models and MS claims data are not retained for training purposes.",
          "score": 6,
          "created_utc": "2026-01-31 17:27:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xt2cb",
              "author": "typeof_goodidea",
              "text": "How fast do you tend to hit usage limits?",
              "score": 1,
              "created_utc": "2026-02-01 08:22:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2z58xp",
                  "author": "warpedgeoid",
                  "text": "I definitely hit them within a few days when using OC. Of course, I‚Äôm not one of these people running four OC sessions at a time. Still, once you‚Äôve hit the limit, they charge $0.04/request which means the total cost is going to be similar to Claude Max for extremely heavy usage.",
                  "score": 1,
                  "created_utc": "2026-02-01 14:38:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2t3voy",
          "author": "OnigiriFest",
          "text": "I  don‚Äôt have experience with GLM and nanogpt.\n\nI bought synthetic just 2 days ago and been testing it for a bit, the 20 usd plan with Kimi 2.5 can handle one agent running no stop in the 5 hours window (I tested it with a Ralph loop)\n\nThe speed is hit or miss right now, sometimes it‚Äôs good and some times it‚Äôs slow, in theory they are working to fix it, they say it‚Äôs a problem affecting only Kimi 2.5.",
          "score": 11,
          "created_utc": "2026-01-31 16:05:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t6pni",
          "author": "Torresr93",
          "text": "The GitHub copilot plan is easy to understand.You get 300 requests, and each model has a multiplier based on its cost. For example, one Opus request counts as tree. On top of that, for simple tasks you can use gtp5-mini for free.",
          "score": 4,
          "created_utc": "2026-01-31 16:19:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t8u7e",
          "author": "shaonline",
          "text": "ChatGPT Plus is opaque but rate limits have been decent. As all 20-ish bucks plans from frontier labs you better delegate the simple tasks (past planning/review) to a cheaper model if you don't want to smoke your weekly quota too fast.",
          "score": 6,
          "created_utc": "2026-01-31 16:29:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2tbjlg",
              "author": "Simple_Split5074",
              "text": "Which is why I am looking for the workhorse provider :-)",
              "score": 1,
              "created_utc": "2026-01-31 16:42:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2tcump",
                  "author": "shaonline",
                  "text": "I mean if you want to throw the top-tier expensive models at all problems you're left with paying a 200 bucks a month subscription, which is still heavily subsidized in its own rights (if stuff like viberank is to be believed as far as claude code is concerned lol)",
                  "score": 1,
                  "created_utc": "2026-01-31 16:48:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2t852t",
          "author": "LittleChallenge8717",
          "text": "[Synthetic.new](http://Synthetic.new) has generous 5h limits IMO, you also can get 10$ off for 20$ subscription, and 20$ off for 60$ subscription with referal codes -> has minimax, glm 4.7 and kimi k2.5 models (others too). you can use mine so we both benefit [https://synthetic.new/?referral=EoqzI9YNmWuGy3z](https://synthetic.new/?referral=EoqzI9YNmWuGy3z) or buy it directly from their website. Tool calling works great (counts as 0.1x or 0.2x it depends), also based on my experience -> GLM4.7 and minimax works great since they are directly hosted on synthetic gpu's, for other models like kimi k2.5 they use fireworks which has sometimes delay in generation. as i know from support they plan to host kimi in next weeks so i guess then synthetic  would be ideal offer, meanwhile GLM and minimax models working great in opencode with no additional delay/issues\n\nhttps://preview.redd.it/cbhqrqzpnpgg1.png?width=1220&format=png&auto=webp&s=b7023ae322082cb3c20c7a27654786249d5d1317",
          "score": 4,
          "created_utc": "2026-01-31 16:26:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2tarmz",
              "author": "Simple_Split5074",
              "text": "Which in some sense is great, fireworks is likely the best of the inference providers (if I wanted to pay by token I'd go there). In another sense, it does not inspire confidence in their infra...",
              "score": 5,
              "created_utc": "2026-01-31 16:38:52",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2t8duw",
              "author": "LittleChallenge8717",
              "text": "https://preview.redd.it/2mzav20iopgg1.png?width=2169&format=png&auto=webp&s=eb5df84cab3e389d73209338f330c3a77f040446\n\nthis is what i mean regarding provider",
              "score": 2,
              "created_utc": "2026-01-31 16:27:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2txvfc",
          "author": "gh0st777",
          "text": "I see a lot pushing synthetic referral hard lately.\n\nI assume you are on a tight budget or this is for a hobby and not a source of income. Why not try each one for a month or alot $5 for API usage to see what works for you?\n\nIf you use this for work or a source of income, might as well invest and get claude code max. $100 is the sweetspot to get things done with opus. But it no longer works for opencode, so consider that too.",
          "score": 4,
          "created_utc": "2026-01-31 18:28:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t8aiq",
          "author": "Bob5k",
          "text": "On synthetic end you can try it for 10$ first month with [reflink](https://synthetic.new/?referral=IDyp75aoQpW9YFt) if you don't mind. \nI'm using them on pro plan for quite a long time and generally I'm happy so far. Especially due to fact that any new frontier opensource model is instahosted there - rn using Kimi K2.5 as my baseline.\nUsually on self hosted models it's around 70-90tps (glm, minimax), for Kimi K2.5 right now a tad bit slower, ranging 60-80 tps for me.",
          "score": 4,
          "created_utc": "2026-01-31 16:27:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2t9ugk",
              "author": "ZeSprawl",
              "text": "They are currently forwarding Kimi k2.5 to fireworks because their infra is having trouble running it.",
              "score": 4,
              "created_utc": "2026-01-31 16:34:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ta583",
                  "author": "Bob5k",
                  "text": "Yeah i know, this is probably the reason of slightly lower tps aswell. In general works just fine, roughly 100m+ tokens already processed by Kimi on my projects ü´°",
                  "score": 3,
                  "created_utc": "2026-01-31 16:35:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2xxtdb",
              "author": "1234filip",
              "text": "Gotta say that I'm really happy with synthetic right now. Very reliable and the models do any tool calls perfectly!",
              "score": 2,
              "created_utc": "2026-02-01 09:07:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ydmph",
                  "author": "Bob5k",
                  "text": "Happy to hear. I can't be happier aswell - especially due to fact that stability is better than \"native\" providers and basically whatever comes out - i don't care, as they'll host it anyway so i don't need to pay somewhere else.\nWant a gig with deep seek? No problemo. Glm5 will be out? They're already ready for it. Kimi? Routed and working on self hosted. \nEssentially even for 60$ sub on synthetic it's stil cheaper than to have 3 diff subs across minimax Kimi and glm while also 1350 prompts on synthetic is insane amount while they charge 0.1 prompt per tool call. For coding even 2-3 projects at a time - infinite amount of LLM calls basically.",
                  "score": 1,
                  "created_utc": "2026-02-01 11:31:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2uhguu",
          "author": "cjazinski",
          "text": "I bought the pro glm 4.7 and it blows worst 200 ever",
          "score": 2,
          "created_utc": "2026-01-31 20:02:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2uiu3k",
              "author": "Simple_Split5074",
              "text": "Last year it was decent, then it gradually declined... Hoping for more capacity and GLM 5 now...",
              "score": 1,
              "created_utc": "2026-01-31 20:09:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2vc9dz",
          "author": "troyvit",
          "text": "I've been  enjoying OpenCode (and aider.chat) with my Mistral API key using mostly Mistral Large 3 but also Devstral. It works well  for my simple needs. It's cheap enough that I don't mind asking detailed questions about what it does and learning the answers, which keeps me from just vibe coding. I'm actually getting a little bit better at python (mostly in the realm of architecture)",
          "score": 2,
          "created_utc": "2026-01-31 22:35:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2y0s3x",
          "author": "Shep_Alderson",
          "text": "I‚Äôve had a good time with Synthetic. I started with their $20 plan and only upgraded to the $60 plan when I hit the limit from running a Ralph loop for like an hour or two. Since I upgraded, I‚Äôve not come anywhere near hitting limits and I quite like it. They also had Kimi K2.5 available the day it launched via their hosting partner, though I find myself preferring GLM 4.7 and Minimax M2.1 personally.",
          "score": 2,
          "created_utc": "2026-02-01 09:35:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t3f9q",
          "author": "tidoo420",
          "text": "Unpopular opinion, i use qwen coder 3 free with qwen cli, it is better than i expected please give it a go\nP.s. i have tried most of the above and not satisfied",
          "score": 2,
          "created_utc": "2026-01-31 16:03:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2t52c6",
              "author": "Simple_Split5074",
              "text": "I find qwen models (either 235 or 480) to be nigh useless for coding. Before I deal with that I'll use antigravity (gemini-cli somehow does not load anymore on my machine, go figure)...",
              "score": 1,
              "created_utc": "2026-01-31 16:11:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tts9o",
          "author": "Jakedismo",
          "text": "Kimi Code definetely has the edge over zai and minimax tested them all and kimi is the most broad specialist when vibing",
          "score": 2,
          "created_utc": "2026-01-31 18:09:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t65lj",
          "author": "BERLAUR",
          "text": "Why not combine them? GLM is cheap (2-3 bucks per month). Synthetic.new has a trail for 12 USD. ChatGPT usually offers a free month.¬†\n\n\nIf you're a student you can get Copilot for cheap (free?).\n\n\nI have 5 subscriptions and I just switch between them when I run into a limit. Total cost is still less than a meal at a restaurant. Absolutely worth it.\n\n\nIf I have some tokens to spare I'll burn them on less important tasks.",
          "score": 1,
          "created_utc": "2026-01-31 16:16:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y0a4o",
              "author": "wizenith",
              "text": "would you like to share what are the 5 subscriptions¬†you are using?   \nyou have mentioned GLM, Synthetic and ChatGPT, so i might assume you have subscribed them already ( or no? ). \n\nAnd what other subscriptions¬†you had? just curious. ",
              "score": 2,
              "created_utc": "2026-02-01 09:30:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2z2a7v",
                  "author": "BERLAUR",
                  "text": "- Z.AI (cheap and great for the grunt work)\n- Copilot (great for Sonnet and Opus, plus free unlimited grok-code-fast/GPT Mini which is handy for e.g minor refactors)\n- Claude Code (but will probably cancel this one)\n- Synthetic (Kimi K2.5)\n- OpenAI Codex (really impressive for debugging and big fixes!)\n- Openrouter (for various things, mostly to test and try new models)\n\n\nI work as a CTO so this gives me the ability to play around with a whole bunch of stuff to see what might work best for my development teams and it's also fun! We're pushing quite hard on AI but I don't want to be one of those leaders who scream \"AI, AI, AI!\". I want to be at least somewhat experienced enough to actually push the teams towards delivering more value.\n\n\nNanoGPT and Cerbus (for the speed) is also worth checking out, I haven't tried those out yet.",
                  "score": 1,
                  "created_utc": "2026-02-01 14:22:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2t7mn1",
              "author": "Simple_Split5074",
              "text": "Oh I \\*do\\* combine them, mostly I am looking for another one...",
              "score": 1,
              "created_utc": "2026-01-31 16:23:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tajl3",
          "author": "esmurf",
          "text": "I tried a couple of different one it seems Github copilot is the best choice right now. I'm looking into to go all opencode though.¬†",
          "score": 1,
          "created_utc": "2026-01-31 16:37:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tamwz",
          "author": "tisDDM",
          "text": "I did not find the quota f√ºr GPTPlus low. Anyway there is no such thing as a cheap plan for SOTA models. \n\nIf you like it cheap - and working: Sign up for Mistral API. Their Devstral 2 models are good and currently still free.",
          "score": 1,
          "created_utc": "2026-01-31 16:38:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tummk",
          "author": "annakhouri2150",
          "text": "> Synthetic: hard to say how much use you really get out of the 20$ plan? Plus how fast / stable are they (interestedin Kimi 2.5, potentially GLM5 and DS4 when they arrive)? Does caching work (that helps a lot with speed)? \n\nIn my experience, having paid for the $20 Synthetic plan for a few months now, Synthetic is faster and more stable --- and the inference is higher-quality --- for their self hosted models (GLM 4.7, Kimi K2T, etc) than any other provider. Currently, they're proxying K2.5 to Fireworks.AI while they get their infrastructure and hardware ready to run it, so it's not nearly as reliable in tool calling as their general capabilities, but it's still faster and more stable than other services I've tried (to be fair, I haven't tried any of the Big Three --- Gemini, Claude, or GPT Codex). \n\nAlso, OpenCode is pretty API call efficient; when I was still using it, the 135 API calls provided by the $20/mo Synthetic plan felt like more than enough. If you have an agent that uses a lot more API requests, like the Zed Agent, you can start to run up against the limits more often if you've got, like, several agents running, or having them run in a really really tight loop where they output and do very little per API call, but for general usage even in more token-heavy agents, it takes some heavy nonstop usage to hit the limit within their 5 hour window. Their limits are more generous than what the Claude Code subscription gives you, for instance.",
          "score": 1,
          "created_utc": "2026-01-31 18:13:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yz2so",
          "author": "tibsmagee",
          "text": "I've been using cheapest minimax code plan the last month. Very reliable and includes web search and vision.¬†\n\n\nSeems like a very capable model for day to day coding.",
          "score": 1,
          "created_utc": "2026-02-01 14:04:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3400nv",
          "author": "t12e_",
          "text": "Synthetic plus GH copilot\n\nYou'll have to update your opencode config so that you use gpt5 mini for subagents to save up on requests. Then kimi/glm for most tasks and any of the Claude/Codex models for complex tasks. I usually hit the 5 hour limit after about 4 hours (this is with 2 agents running)",
          "score": 1,
          "created_utc": "2026-02-02 05:59:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t47zx",
          "author": "trypnosis",
          "text": "I feel your pain. Leaning to copilot trying that and synthetic will decide in a few weeks.",
          "score": 1,
          "created_utc": "2026-01-31 16:07:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ubyrp",
          "author": "SamatIssatov",
          "text": "A very good limit in ChatGPT Plus. Why lie here? So that someone will suggest ‚Äúsynthetic‚Äù?",
          "score": 0,
          "created_utc": "2026-01-31 19:35:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qssabi",
      "title": "The definitive guide to OpenCode: from first install to production workflows",
      "subreddit": "opencodeCLI",
      "url": "https://jpcaparas.medium.com/the-definitive-guide-to-opencode-from-first-install-to-production-workflows-aae1e95855fb?sk=69c1519ee7808c4eed582c44b016fc70",
      "author": "jpcaparas",
      "created_utc": "2026-02-01 07:50:10",
      "score": 41,
      "num_comments": 1,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qssabi/the_definitive_guide_to_opencode_from_first/",
      "domain": "jpcaparas.medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "o31etjx",
          "author": "touristtam",
          "text": "nice plugin in there. ;)",
          "score": 1,
          "created_utc": "2026-02-01 21:02:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq5m8o",
      "title": "Kimi K2.5 in opencode",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qq5m8o/kimi_k25_in_opencode/",
      "author": "t4a8945",
      "created_utc": "2026-01-29 11:02:09",
      "score": 35,
      "num_comments": 20,
      "upvote_ratio": 0.86,
      "text": "Hello,\n\nI'm a big fan of Opus 4.5 especially in opencode. Fits my workflow very well and enjoy the conversational aspect of it a lot.\n\nI'm always trying new models as they come, because the space is moving so fast and also because Anthropic doesn't seem to want me as a customer. I tried GLM 4.7, MiniMax-2, Devstral 2, Mistral Large 3, and I never was satisfied by the results. Too many errors that couldn't compete with what Opus 4.5 was delivering. I also tried GPT5.2 (medium or high) but I hate it so much (good work but the interactions are hell).\n\nSo I set Kimi K2.5 up to work with a SPEC.md file that I used in a previous project  (typescript node + react, status notification app) and here is how it went:\n\n* Some tool calls error with truncated input which halted the task (solved by just saying \"continue and be careful about your tool calls\")\n* It offered to implement tests, which none of the other models did\n* It had a functional implementation quite quickly without too many back and forth\n* It lacked some logic in the UI (missing buttons) but pointing it out led to a working fix\n* Conversation with it is on par with what I get from Opus, albeit it feels like a little bit less competent coworker ; but if feels GOOD.\n* The end result is very good!\n\nI highly recommend you try it out for yourself. It is better than I expected. (edit to clarify: not as good as Opus, but better than anything else I tried - \"better\" is  very personal as I tried to laid out above, it's more about the process than the end result)\n\nWhat is your experience with it? Did I develop some patience with these models or is it quite competent?\n\nedit: I'm using the official Kimi Code sub, as I've read integration in vendors can lead to less success in tool calls especially. Since this is open weight, not all providers are equal. See [https://github.com/MoonshotAI/K2-Vendor-Verifier](https://github.com/MoonshotAI/K2-Vendor-Verifier) for instance (they updated it for K2.5 and it should equalize vendors more, but keep that in mind)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qq5m8o/kimi_k25_in_opencode/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2e546t",
          "author": "DistinctWay9169",
          "text": "I found Kimi 2.5 to be the most overrated model. I asked it to fix a problem I already knew how to fix, and it told me the problem was not what I was talking about. Then I told it, \"Then fix it with your solution\" and guess what. After a bunch of tokens spent on loop thinking, it did not solve the problem. This model is not better than opus at all. I found this model is great for a bunch of things, but for coding, it is meh.",
          "score": 10,
          "created_utc": "2026-01-29 11:23:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e96gd",
              "author": "patlux",
              "text": "Same for me. I compared it with the responses from Opus 4.5 and Opus make much more suggestions and asks better questions back than Kim 2.5.",
              "score": 3,
              "created_utc": "2026-01-29 11:54:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2e9jeo",
              "author": "mintybadgerme",
              "text": "Yep, I agree. It's vastly overrated. It's okay, but definitely nowhere near Opus.",
              "score": 2,
              "created_utc": "2026-01-29 11:57:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2epxzx",
              "author": "aeroumbria",
              "text": "I am starting to assign specific models to specific tasks rather than trusting in a generalist model. I feel that Deepseek might be the best debugging / validation model. It is slow, and does not follow detailed workflow instructions very well (spent too much time debating what output document style to use), but it is very thorough, has maximum self doubt and almost zero self-confidence, and will actually debate with its former self, which is perfect for error catching. It is also markedly different in reasoning trace compared to most other models (probably due to different training data, heavier RL use and not relying much on distilling competitions), so in theory it should also be less prone to shared blind spots of other models.",
              "score": 2,
              "created_utc": "2026-01-29 13:40:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2h4oja",
              "author": "RegrettableBiscuit",
              "text": "I like it. I don't think anyone expects it to be as good as Opus, but unlike other open models that feel like a year behind Anthropic or OpenAI's current models, this feels more like six months behind.\n\n\nI could be fine with only using K2.5, which I can't say for models like GLM4.7.",
              "score": 1,
              "created_utc": "2026-01-29 20:24:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lm7b6",
              "author": "hey_ulrich",
              "text": "Interesting to hear this. I'm having a great experience with Kimi 2.5, and I have Claude Max and use Opus everyday. I mostly develop webapps with python backend and postgres. What kind of products and languages are you working with?",
              "score": 1,
              "created_utc": "2026-01-30 13:35:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2e5mfd",
              "author": "t4a8945",
              "text": "Interesting, what is your context (type of project, language, etc)?\n\nTo try it I just gave it my \"benchmark\" (start a new project from scratch, see how it works and interacts), but I'll keep throwing more cases at it to see out it fares.",
              "score": 1,
              "created_utc": "2026-01-29 11:27:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2e5sms",
                  "author": "DistinctWay9169",
                  "text": "Electron + Typescript + React.",
                  "score": 1,
                  "created_utc": "2026-01-29 11:29:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2e7sm4",
          "author": "Funny-Advertising238",
          "text": "I've been having incredible success with it! Better than any other open source model by far.¬†\n\n\nIf you've ever used Opus 4.5 and watched the way it thinks you know that kimi was definitely trained on Opus/Sonnet. The way it thinks and goes through tasks, and the way it responds, they definitely did the same scheme that deepseek did on openai.¬†\n\n\nNot saying it's Opus level but I personally love the way it goes through tasks and the interactions with it. Gpt 5.2 interactions makes my brain hurt sometimes.¬†\n\n\n\nI was having trouble with something that GPT 5.2 took an hour and still couldn't solve it, and kimi solved it in a few minutes.¬†\n\n\nNot sure how good it would be in the wild, one shotting etc. as my agents.md, skills and subagents setup is quite thorough.¬†\n\n\nBut for my use case it's absolutely killing it! Using the Kimi Coder plan too.¬†",
          "score": 6,
          "created_utc": "2026-01-29 11:44:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eaz4f",
              "author": "t4a8945",
              "text": "Haha we're the same! It feels very \"Opus\" in the interaction, your intuition about it being trained on it feels right.\n\nI'm just happy to have found again this \"coworker\" feeling when working with it. (Not like GPT5.2)",
              "score": 3,
              "created_utc": "2026-01-29 12:07:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2kzsz2",
          "author": "LongBit",
          "text": "I tried it today for the first time on an issue Opus 4.5 could not fix.  Kimi 2.5 solved it without help.",
          "score": 3,
          "created_utc": "2026-01-30 11:04:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hwj82",
          "author": "kpgalligan",
          "text": "I've been dabbling. I'm on the CC 20x plan. Always assume the \"___ is as good as Opus\" is BS, but eventually it won't be. Maybe not as good, but at least usable. In the past I've found other models to be a mess with actual work.\n\nOn Kimi, I have to agree. So far. I'm only using it for analysis tasks, but it has handled tools well, which has not be true of other models I've tried (to be fair, 6+ months ago). I haven't swapped into any major tasks, mostly because I have plenty of Claude headroom, but will over time. Kind of on an urgent project at the moment so not a lot of \"play\" time.\n\nI do want to integrate it into our tool. We're building a focused coding agent. API costs are high, so if Kimi could handle analysis that is chewing up tokens, it would probably be a great option. Sometime in the next week or two, likely.",
          "score": 2,
          "created_utc": "2026-01-29 22:38:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ocpe7",
          "author": "reduhh",
          "text": "legit I like it better than opus maybe because of speed but rn it‚Äôs my favorite model",
          "score": 2,
          "created_utc": "2026-01-30 21:10:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sv7et",
          "author": "ArthurOnCode",
          "text": "Kimi is specifically trained for Kimi's workflows that are very different from Opencode. See \"Agent Swarm\" for the motivation for this.",
          "score": 1,
          "created_utc": "2026-01-31 15:23:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eckex",
          "author": "Michaeli_Starky",
          "text": "Alleged Opus killer? Yeah, expected",
          "score": -3,
          "created_utc": "2026-01-29 12:18:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ed0s4",
              "author": "t4a8945",
              "text": "I haven't stated that in the slightest. Read again",
              "score": 4,
              "created_utc": "2026-01-29 12:22:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2enb3o",
                  "author": "Michaeli_Starky",
                  "text": "I didn't say you said it. Read again",
                  "score": -2,
                  "created_utc": "2026-01-29 13:25:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qp1b6e",
      "title": "Black 100 sub is too limiting as compared to Claude Max",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/18ye8k58o0gg1.png",
      "author": "Top_Shake_2649",
      "created_utc": "2026-01-28 04:24:54",
      "score": 35,
      "num_comments": 14,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qp1b6e/black_100_sub_is_too_limiting_as_compared_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o25uehb",
          "author": "jpcaparas",
          "text": "Yeah it's because of this:\n\nhttps://pub.towardsai.net/why-your-expensive-claude-subscription-is-actually-a-steal-02f10893940c?sk=65a39127cbd10532ba642181ba41fb8a\n\nYou‚Äôre paying $20 for $259 worth of api usage on a Claude Pro Plan and you're getting  $1.3k worth of api tokens on a $100 max plan and opencode black afaik, doesn't do the same level of subsidy that Anthropic does.\n\nSo if I were on OpenCode Black, I'd just stick to the low-cost Chinese models.\n\nEdit:\n\nI'll report back when I try out synthetic dot new",
          "score": 12,
          "created_utc": "2026-01-28 05:11:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o265y3y",
              "author": "aeroumbria",
              "text": "Is it really a \"steal\" if there exist providers offering similarly capable models at a flat API rate that is cheaper than the \"subsidised\" package? Seems more like self-inflicted cost bloat to me...",
              "score": 5,
              "created_utc": "2026-01-28 06:38:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o29aga9",
              "author": "Tushar_BitYantriki",
              "text": "Well, you are comparing their price with THEIR price. That itself is a fallacy.\n\nIt's like comparing the cost of an iPhone with another iPhone. Playing right into the anchor.\n\nWhile in reality their price needs to be compared to other alternatives that are comparable. At this point, they no longer have the \"SOTA\" benefit anymore.\n\nGLM 4.7 is really killing it, and while I started planning with Opus and implementing with GLM, I have gradually moved to doing most of my planning with GLM-4.7\n\nHaven't tried Kimi, but it seems to beat them both on many benchmarks.\n\nSure, they are in their \"Give it all for cheap\" phase, but the price difference between Anthropic and everyone else is not proportionate.\n\nSo people really need to ask themselves if they really need Claude for either the sub or the APIs.\n\nAt this point, if you are writing some code that any AI company might be interested in stealing, then by all means stick to Claude or OpenAI (in case they are absolutely trustworthy). I hope no model is actually seeing any customer data to begin with.\n\nAnd if someone is really doing something that is so complicated that they feel only Claude can do it, then maybe, they either their employer or their revenue should be able to pay for it.\n\nFor everyone else, there are cheaper, and equally (in some cases, more) capable models.\n\nI have personally been weaning out my usage from Anthropic to GLM within Claude code. And at the same time, also moving my workflows to Opencode, for the day when Anthropic decides to also ban accounts for using other models in Claude code (I bet they are collecting those stats even when you are using a different model)\n\nAt this stage, I wouldn't care much, even if my account is banned. Since I moved to GLM, I haven't hit my Claude limits for 2-3 weeks. And already planning to move to $20 sub at the end of this month (already moved from 200 to 100 1.5 months ago)\n\nOr maybe, they will be happy that people who they were losing money on, are finally leaving.",
              "score": 3,
              "created_utc": "2026-01-28 18:10:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o269lal",
              "author": "armindvd2018",
              "text": "In that case why black ? I can use NanoGPT just $8 !!!",
              "score": 2,
              "created_utc": "2026-01-28 07:07:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o26cr5x",
              "author": "Top_Shake_2649",
              "text": "yeah, I've read this article before, and I knew about it before jumping ship. That's why I am not really complaining as I have said. Still, slightly disappointed tho. Also, another thing, I might have accidentally used too much since MoltBot (ClawdBot) hype... üò¨",
              "score": 2,
              "created_utc": "2026-01-28 07:34:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o29ajqs",
                  "author": "kkordikk",
                  "text": "Nigha you buy 100 sub and plug into Clawdbot lmao",
                  "score": 0,
                  "created_utc": "2026-01-28 18:10:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o262gmc",
          "author": "lundrog",
          "text": "Here is my set up. While its not perfect. I do think it works very well. Why? Because the claude code pro account hits a limit within minutes. Or slow performance elsewhere with overseas providers. Aka I can't afford a claude code max plan.\n\nI primarily use glm 4.7 for workflow with deep seek v3.2 for troubleshooting. But now k2.5 is out! \n\nI use claude code, with it the agent, Work flow works unattended for at least a few minutes. Opencode is good also, doesn't run as long unattended. \n\nFor agents https://github.com/VoltAgent/awesome-claude-code-subagents\n\nFor skills https://github.com/VoltAgent/awesome-claude-skills\n\nI am running it with this api gateway ( check your toc ) https://github.com/looplj/axonhub\n\nFor a main provider I use synthetic.new , great performance and privacy is much better than most. Text models but have optional image on demand available. You can back that up with Claude code or a ZAI account or anti gravity, etc. I believe official Claude code and anti gravity support are coming soon.\n\nI have a referral link  \"Invite your friends to Synthetic and both of you will receive $10.00 for standard signups. $20.00 for pro signups. in subscription credit when they subscribe!\" \n\nhttps://synthetic.new/?referral=UAWqkKQQLFkzMkY\n\nI am on my second month and on the $60 plan which gives you 1350 requests every 5 hours without a weekly limit. Should give about 5x a cluade code max plan.\n\nAnywho long story longer... It gives you a lower cost option with a higher quota to use with other plans via the api gateway. \n\n\nMaybe its helpful, ü§î\n\nGood luck on everything",
          "score": 4,
          "created_utc": "2026-01-28 06:10:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26cga8",
              "author": "Top_Shake_2649",
              "text": "[synthetic.new](http://synthetic.new) sounds promising, but I haven't heard much about it before. And the website doesn't give me confident. I rather consider go subscribe to [z.ai](http://z.ai) or moonshot directly?",
              "score": 1,
              "created_utc": "2026-01-28 07:31:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2754vj",
                  "author": "lundrog",
                  "text": "Small team, but its all hosted in usa based providers. Right now they have very generous limits; vs going direct from a cost perspective.",
                  "score": 3,
                  "created_utc": "2026-01-28 11:44:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2dcj44",
          "author": "telewebb",
          "text": "I'm sorry, I'm new to OpenCode black. Why are you not just throwing 10 bucks on OpenRouter and doing pay as you go instead of subscription? I don't understand what you get from a subscription.",
          "score": 1,
          "created_utc": "2026-01-29 07:07:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dji76",
              "author": "Top_Shake_2649",
              "text": "Yes great question. That‚Äôs exactly what I‚Äôm trying to find out with a black subscription. There is supposed to be some subsidy, so far it doesn‚Äôt seem like it.",
              "score": 1,
              "created_utc": "2026-01-29 08:08:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2flg4c",
          "author": "Ordinary-You8102",
          "text": "why not just use antigravity which have a lot models + github copilot which also have a lot of models for way cheaper? (both oauth)",
          "score": 1,
          "created_utc": "2026-01-29 16:11:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp0w55",
      "title": "Anyone using Kimi K2.5 with OpenCode?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qp0w55/anyone_using_kimi_k25_with_opencode/",
      "author": "harrsh_in",
      "created_utc": "2026-01-28 04:05:28",
      "score": 33,
      "num_comments": 49,
      "upvote_ratio": 0.95,
      "text": "Yesterday I did top up recharge for Kimi API and connected it with OpenCode via API. While I can see Kimi K2 models in the models selection, I can‚Äôt find K2.5 models. \n\nCan someone please help me with it?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qp0w55/anyone_using_kimi_k25_with_opencode/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o25nzgw",
          "author": "aeroumbria",
          "text": "`opencode models --refresh` and try again?",
          "score": 13,
          "created_utc": "2026-01-28 04:30:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2t94xy",
              "author": "bhavik-c",
              "text": "Work for me, Thanks u/aeroumbria",
              "score": 1,
              "created_utc": "2026-01-31 16:31:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2checj",
              "author": "jpcaparas",
              "text": "I'm only getting K2 thinking\n\nhttps://preview.redd.it/o4e4n2rfj7gg1.png?width=1002&format=png&auto=webp&s=2f62a5a4bceaf02f2992a5304ee1abf378244c4e",
              "score": 0,
              "created_utc": "2026-01-29 03:26:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2cie9d",
                  "author": "jpcaparas",
                  "text": "NVM, when I connected Synthetic provider I was able to see it \n\nhttps://preview.redd.it/9fwq380ik7gg1.png?width=1032&format=png&auto=webp&s=cc81eb47bc7ba7c31932b46c07ef63e93a3e1046",
                  "score": 1,
                  "created_utc": "2026-01-29 03:32:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27piz4",
          "author": "chiroro_jr",
          "text": "Yes. Used the moonshot 20$ sub. Create an API key. Updated open code. Then run auth login. Selected kimi code. Pasted my key. Done. No issues at all.",
          "score": 6,
          "created_utc": "2026-01-28 13:51:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cljdh",
              "author": "Nooddlleee",
              "text": "What is the code quality? Is it hallucinating on complex and long tasks?",
              "score": 2,
              "created_utc": "2026-01-29 03:51:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2d9a2d",
                  "author": "chiroro_jr",
                  "text": "Code quality depends on the code quality already in your codebase and your prompting skills for the most part. You can't expect top quality code in an app that has messy code and largely been vide coded and at the same time the prompts are bad. Most of these models generate decent code already. Kimi K2.5 is on the same level as an Opus or Codex, especially with a good codebase and a good prompt.\n\nEven Opus can produce shit code if it's working on shit code with a shit prompt.",
                  "score": 1,
                  "created_utc": "2026-01-29 06:39:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27sdme",
              "author": "Villain_99",
              "text": "Is the subscription better than Claude code ?\nPrice wise is same, wondering the consumption limits",
              "score": 1,
              "created_utc": "2026-01-28 14:06:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27x1t6",
                  "author": "chiroro_jr",
                  "text": "For me Kimi K2.5 is the first model that feels that close to Opus 4.5. Because it's dirt cheap, that bridges the gap. I have been using it to do tickets for the past 3 hours. It only failed to do exactly what I wanted probably once or twice. I corrected it and it immediately got back on track. So if it's not one shotting my requirements, the next prompt with do it. All for what? A fifth the price. For me this is the best value. Right now I am on their 20$ plan. 200 messages per 5 hour window. 2048 messages per week. I got a shit tonne of work done with my first 200 messages. I think Claude Code 20$ doesn't even have Opus. Only Sonnet and Hauku. Kimi 2.5 is definitely better than Sonnet.",
                  "score": 13,
                  "created_utc": "2026-01-28 14:30:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2cmfiz",
              "author": "shantz-khoji",
              "text": "For 20$ how many credits for it provides?",
              "score": 1,
              "created_utc": "2026-01-29 03:57:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2d9e42",
                  "author": "chiroro_jr",
                  "text": "2048 per week. 200 per 5 hour window. It's been enough for me  so far.",
                  "score": 1,
                  "created_utc": "2026-01-29 06:40:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2b2vc4",
              "author": "SunflowerOS",
              "text": "You can create a api ley with subscription? I subscribe on december but i didn't understand how connect the kimi with opencode I just cancellled it after the month",
              "score": 0,
              "created_utc": "2026-01-28 22:55:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hpkiv",
                  "author": "eduknives",
                  "text": "You can create here [https://www.kimi.com/code/console](https://www.kimi.com/code/console)",
                  "score": 1,
                  "created_utc": "2026-01-29 22:03:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2cj4di",
          "author": "rokicool",
          "text": "I think I found a solution that worked for me. \n\nJust as everyone else I bought a $20 subscription from https://www.kimi.com/. Then I generated API Key at [Kimi Code Console](https://www.kimi.com/code/console?from=kfc_overview_topbar). \n\nAnd then I used `/connect` command in OpenCode and chose \"`Kimi For Coding`\" as a Provider (Not `Moonshot AI` !). Put the API Key and everything started working. \n\nHappy coding!",
          "score": 5,
          "created_utc": "2026-01-29 03:37:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xa2cs",
              "author": "Alberion",
              "text": "Thank you for mentioning this! I was trying to use \\`Moonshot AI\\`, and using \\`Kimi For Coding\\` fixed it for me.",
              "score": 2,
              "created_utc": "2026-02-01 05:39:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26fmjm",
          "author": "shaonline",
          "text": "Don't forget to upgrade opencode (opencode upgrade) as it had to be added to models.dev.\n\nOverall it's pretty decent and the biggest improvement over, in my case, using GPT 5.2, is the speed, on the fastest providers (I'm using \"fireworks\" on OpenRouter, at the same API price) it reaches 100 tok/sec which is really good for execution I think. I might switch to it entirely as my \"executor/builder\".",
          "score": 3,
          "created_utc": "2026-01-28 07:59:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26pt2h",
              "author": "harrsh_in",
              "text": "How did you fix the error\n\n`invalid temperature: only 1 is allowed for this model`",
              "score": 1,
              "created_utc": "2026-01-28 09:33:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2fa1dn",
                  "author": "Ponchito147",
                  "text": "Add this to the opencode.json in the provider section:\n\n    \"moonshotai\": {\n          \"models\": {\n            \"kimi-k2.5\": {\n              \"temperature\": false,\n              \"interleaved\": {\n                \"field\": \"reasoning_content\"\n              }\n            }\n          }\n        }",
                  "score": 2,
                  "created_utc": "2026-01-29 15:21:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26py0n",
                  "author": "shaonline",
                  "text": "I think you cannot set the temperature explicitely, it only has two hardcoded values (1 for thinking mode and 0.6 for non-thinking/instruct mode).",
                  "score": 1,
                  "created_utc": "2026-01-28 09:34:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27bjp1",
                  "author": "Phukovsky",
                  "text": "I'm getting this error too.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:30:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28at2u",
          "author": "Juan_Ignacio",
          "text": "I tried the subscription directly from [https://www.kimi.com/](https://www.kimi.com/) and had no issues.\n\nTry running:\n\n    opencode upgrade\n    opencode models --refresh\n\nAfter that, you should be able to log in and use it in opencode without problems.  \nTo use it with oh-my-opencode, I had to define the model in `opencode.json` like this:\n\n    \"provider\": {\n      \"kimi-for-coding\": {\n        \"models\": {\n          \"kimi-k2.5\": {\n            \"id\": \"kimi-k2.5\"\n          }\n        }\n      }\n    }\n\nAlso worth mentioning: the 1-month subscription basically costs $1 if you use someone's referral link and send them jokes in the chat that opens until the price drops to $0.99. \n\nJust in case, here‚Äôs my referral link:  \n[https://www.kimi.com/kimiplus/sale?activity\\_enter\\_method=h5\\_share&invitation\\_code=AN9SGQ](https://www.kimi.com/kimiplus/sale?activity_enter_method=h5_share&invitation_code=AN9SGQ)\n\nThe jokes don‚Äôt have to be from Kimi specifically. You can generate them with any other AI and just copy paste them.",
          "score": 6,
          "created_utc": "2026-01-28 15:35:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bzr4h",
              "author": "rick_1125",
              "text": "you don't have to add provider in \\`opencode.json\\`, once you logged in provider will be generated in model list, just copy model name to \\`oh-my-opencode.json\\`, its \\`kimi-for-coding/k2p5\\` for kimi-k2.5",
              "score": 2,
              "created_utc": "2026-01-29 01:49:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2c5nwp",
                  "author": "Juan_Ignacio",
                  "text": "Thank! . I had written k2.5 and k2-5 instead of k2p5.",
                  "score": 1,
                  "created_utc": "2026-01-29 02:21:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2dp4xv",
              "author": "Appropriate_Yak_1468",
              "text": "https://preview.redd.it/d14c5y0479gg1.png?width=546&format=png&auto=webp&s=356b5cd6aef608a0ec848481e7d8877ab2acdc7c",
              "score": 2,
              "created_utc": "2026-01-29 09:01:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2g7rxb",
                  "author": "trenescese",
                  "text": "lmao it was legitimately funny to do that",
                  "score": 1,
                  "created_utc": "2026-01-29 17:52:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27fk5p",
          "author": "Simple_Split5074",
          "text": "I tried on nano-gpt, it's slow as molasses (like one rerquest per minute!) and occasionally tool calls fail or it simply gets stuck (no observable progress for 5+ min).\n\nMy suspicion: the inference providers do not have it completely figured out yet.\n\nMoonshot via openrouter was decent last night but now it crawls around at 15tps. Fireworks still claims to do 100+ tps but I have no idea if caching works with opencode and without it would get ruinous quickly.",
          "score": 2,
          "created_utc": "2026-01-28 12:56:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2uej4b",
              "author": "Complex_Initial_8309",
              "text": "Hey, have you figured out why the NanoGPT one doesn't work? Any potential fixes? \n\nI'm SUFFERING because of this exact issue.",
              "score": 1,
              "created_utc": "2026-01-31 19:48:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2uhx96",
                  "author": "Simple_Split5074",
                  "text": "Sadly not - might log a bug report on Monday",
                  "score": 1,
                  "created_utc": "2026-01-31 20:04:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o29ue7c",
          "author": "Ok-Connection7755",
          "text": "I upgraded opencode, did auth and then put the API key, works perfectly. So far I love the speed and responsiveness of the model. Will test more send post here.",
          "score": 2,
          "created_utc": "2026-01-28 19:36:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26nzqy",
          "author": "StrangeJedi",
          "text": "I didn‚Äôt see the 2.5 model either have they added it?",
          "score": 1,
          "created_utc": "2026-01-28 09:16:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27bsld",
          "author": "Phukovsky",
          "text": "I see two options available: Moonshot AI and Moonshot AI (China)\n\nI first tried entering API key for the (China) and I get invalid auth error. Then I added key for Moonshot AI, chose Kimi 2.5 and am now getting: \\`invalid temperature: only 1 is allowed for this model\\`",
          "score": 1,
          "created_utc": "2026-01-28 12:31:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33yl5l",
              "author": "itchykittehs",
              "text": "There's a Provider called Kimi Code or something, it's not Moonshot",
              "score": 1,
              "created_utc": "2026-02-02 05:47:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27is6k",
          "author": "Phukovsky",
          "text": "I got an API key from [platform.moonshot.ai](http://platform.moonshot.ai) after adding some funds to my account there. I added the API key to Opencode but doesn't seem to work. Thinking I need an actual Kimi Code subscription and an API key from there instead?",
          "score": 1,
          "created_utc": "2026-01-28 13:15:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27q1g3",
          "author": "zach978",
          "text": "I see it on opencode zen, haven‚Äôt tried it though.",
          "score": 1,
          "created_utc": "2026-01-28 13:54:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a0zv2",
          "author": "Aggravating_Bad4163",
          "text": "I checked this on opencode with openrouter and it worked fine without any issues.",
          "score": 1,
          "created_utc": "2026-01-28 20:06:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fnh52",
          "author": "dxcore_35",
          "text": "This is direct answer from Kimi-K-2 official chat AI. Does it make sense?\n\nI need to be very direct with you: **You cannot use your Kimi Moderato plan with Claude Code. They are completely incompatible.**\n\nHere's why this won't work:\n\n# The Hard Technical Reality\n\n**Kimi Moderato Plan** = Subscription for Kimi's **chat interface only** (app and web)\n\n* ‚ùå Does **NOT** include API credits\n* ‚ùå Does **NOT** work with Claude Code\n* ‚ùå Does **NOT** work with any third-party tools\n\n**Claude Code** = Anthropic's CLI tool\n\n* Only connects to **Anthropic's API** (claude-3-5-sonnet, claude-3-opus)\n* Hardcoded to Anthropic's servers (`api.anthropic.com`)\n* Requires an **Anthropic API key**, not Kimi credential",
          "score": 1,
          "created_utc": "2026-01-29 16:20:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fnsvt",
              "author": "Ok_Box7357",
              "text": "Yeah, just noticed that weekly limits decreased: 2048 -> 100  \nThat sucks ...",
              "score": 1,
              "created_utc": "2026-01-29 16:22:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hp5pn",
                  "author": "Hyp3rSoniX",
                  "text": "no they switched it to % view, that's why it showed 100 for a moment. if you visit it again it should show some % number.",
                  "score": 1,
                  "created_utc": "2026-01-29 22:02:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2udhcq",
          "author": "jakob1379",
          "text": "So does the three available kimi plans work with opencode?",
          "score": 1,
          "created_utc": "2026-01-31 19:42:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bcvqa",
          "author": "elllyphant",
          "text": "Yes you can use Synthetic's $20 standard sub for $12 (they're 40% off right now) to use kimi k2.5\n\nhttps://preview.redd.it/xttlpui8g6gg1.png?width=788&format=png&auto=webp&s=522494ed86621bccbde7188444f8e7b9892173ac",
          "score": 0,
          "created_utc": "2026-01-28 23:47:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dwwc2",
              "author": "kr_roach",
              "text": "What is difference between Synthetic‚Äôs and official moonshot ai",
              "score": 2,
              "created_utc": "2026-01-29 10:13:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bcwrp",
              "author": "elllyphant",
              "text": "[https://synthetic.new/?saleType=moltbot](https://synthetic.new/?saleType=moltbot)",
              "score": 0,
              "created_utc": "2026-01-28 23:47:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qt428t",
      "title": "Using an AI Agent (opencode) To Teach Me Rust and It‚Äôs Kinda Blowing My Mind",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qt428t/using_an_ai_agent_opencode_to_teach_me_rust_and/",
      "author": "feursteiner",
      "created_utc": "2026-02-01 17:07:00",
      "score": 33,
      "num_comments": 45,
      "upvote_ratio": 0.92,
      "text": "I‚Äôve been learning Rust with an AI agent through OpenCode, and it‚Äôs honestly way cooler than I expected.\n\nComing from a TypeScript-heavy background, I thought Rust would break my brain, but the AI keeps mapping concepts to stuff I already know. It‚Äôs structured, but flexible enough that I can reshape the whole plan whenever I get stuck or suddenly decide to deep-dive ownership at 2am.\n\nIt uses a pyramid-style method where each layer builds on the last, and I can expand it as I go. The repo basically becomes a living skill tree. Also, I get to ask all the ‚Äúdumb‚Äù questions I‚Äôd never ask a human. No judgment. Just explanations until it finally clicks.\n\nLearning at my own pace, on my own time, has been way more comfortable, and honestly the speed is kind of wild. Rust went from intimidating to fun way faster than I expected.\n\nEdit:   \ntook down the link before, but happy to share it again, thanks for the support y'all!  \n[https://github.com/feuersteiner/learning-rust](https://github.com/feuersteiner/learning-rust)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qt428t/using_an_ai_agent_opencode_to_teach_me_rust_and/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o3012hq",
          "author": "Maasu",
          "text": "Covered async yet?",
          "score": 7,
          "created_utc": "2026-02-01 17:11:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30500e",
              "author": "coffee_brew69",
              "text": "the agent might delete itself on that part",
              "score": 11,
              "created_utc": "2026-02-01 17:29:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o307ol9",
                  "author": "feursteiner",
                  "text": "haha I see that x) I had to cycle through a couple of models to find one that is actually a good value for money. I am using github copilot as the provider (wide variety, and feels cheaper). Opus was best, but is most expensive, currently with codex-5.2",
                  "score": 2,
                  "created_utc": "2026-02-01 17:41:19",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30ee1c",
          "author": "debba_",
          "text": "I totally agree. I‚Äôm also using a *learn-by-doing* approach with Rust, with the help of OpenCode. On top of that, KIMI K2.5 Free with Zen is a really nice bonus.\n\nIn just one week, I managed to ship a first beta of a side project of mine: a lightweight database tool with a clean, pleasant UX.\n\nIf you want to take a look:  \n[https://github.com/debba/tabularis](https://github.com/debba/tabularis)",
          "score": 4,
          "created_utc": "2026-02-01 18:11:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30it3b",
              "author": "feursteiner",
              "text": "Tauri is the goat!",
              "score": 1,
              "created_utc": "2026-02-01 18:31:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o30veop",
          "author": "Ok_Layer2715",
          "text": "Hey, i would appreciate if you give me more details, as what you have written to opencode from first and what is the pyramid method",
          "score": 2,
          "created_utc": "2026-02-01 19:28:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31lmv6",
              "author": "feursteiner",
              "text": "absolutely! I can first refer you to the agents md (feel free to star the repo, please and thank you haha) and you can see everything. feel free to ask me any questions about it too!  \n[https://github.com/feuersteiner/learning-rust](https://github.com/feuersteiner/learning-rust)",
              "score": 1,
              "created_utc": "2026-02-01 21:35:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31okdl",
                  "author": "Ok_Layer2715",
                  "text": "Nice, i have checked both of them and they are awesome specially your repo hahah\nBut the thing that i cant understand till now is the pyramid method",
                  "score": 2,
                  "created_utc": "2026-02-01 21:49:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33dppo",
          "author": "mrpoopybruh",
          "text": "Its wild. I am currently binding them into cards on a big canvas. I will be in the matrix within days, if not hours. Not kidding -- I'm literally coding up a green on black style layer lol",
          "score": 2,
          "created_utc": "2026-02-02 03:26:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o364qtl",
              "author": "feursteiner",
              "text": "dude the matrix theme is my fav on opencode haha",
              "score": 1,
              "created_utc": "2026-02-02 15:34:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o36pp5y",
                  "author": "mrpoopybruh",
                  "text": "LOL me too. Man, I gotta say though, the CLI is slick, but the actual Rest API is very complex and its taking forever to get even something crappy up. Gotta give it to the opencode team on how slick the CLI is.",
                  "score": 2,
                  "created_utc": "2026-02-02 17:11:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3499i5",
          "author": "levu304",
          "text": "i think you should approach through napi-rs first",
          "score": 2,
          "created_utc": "2026-02-02 07:18:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o364urq",
              "author": "feursteiner",
              "text": "can you explain more please ? I am intrigued",
              "score": 1,
              "created_utc": "2026-02-02 15:34:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o34xf4w",
          "author": "chiroro_jr",
          "text": "I used to do this too when learning something new. I tell the AI to generate tests. Then I write code that tries to pass those tests. If it's too hard, I ask for a hint. When the tests pass I asked the AI to look at the code so that it tell me if I could have implemented a better solution or written more idiomatic code depending on the ecosystem. It's pretty cool.",
          "score": 2,
          "created_utc": "2026-02-02 11:06:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3651p9",
              "author": "feursteiner",
              "text": "exactly the same process! someone should make product around this... this is what school should look like in the future ...",
              "score": 2,
              "created_utc": "2026-02-02 15:35:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o308wxl",
          "author": "web_assassin",
          "text": "I'm advancing my Git skills with opencode and loving it. It doesn't give me snarky replies to my dumb questions.",
          "score": 1,
          "created_utc": "2026-02-01 17:46:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o309ufg",
              "author": "feursteiner",
              "text": "yeah, exactly. it's sad to see places like reddit turn like that, where people don't appreciate other's learning journeys and just pile on them... sad. Good luck to you too u/web_assassin !",
              "score": 2,
              "created_utc": "2026-02-01 17:51:06",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o30f6ey",
              "author": "vertigo235",
              "text": "Stackoverflow prevented so many eager people from learning, you only really learned from it if a previous person took some serious heat for asking a simple question. \n\nGone are those days!",
              "score": 2,
              "created_utc": "2026-02-01 18:14:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o30hn16",
                  "author": "feursteiner",
                  "text": "I re-posted this same exact post on another subreddit and gotten so much hate in 2 minutes I deleted the post...",
                  "score": 1,
                  "created_utc": "2026-02-01 18:25:56",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o30ik60",
                  "author": "web_assassin",
                  "text": "Hah yeah sacrificial lambs. The online haters are losing their jobs. So sad!",
                  "score": 1,
                  "created_utc": "2026-02-01 18:30:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30ejob",
          "author": "vertigo235",
          "text": "This is the way",
          "score": 1,
          "created_utc": "2026-02-01 18:12:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31lrbg",
              "author": "feursteiner",
              "text": "damn straight!! exciting times for learning!",
              "score": 1,
              "created_utc": "2026-02-01 21:36:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o30frws",
          "author": "antifeixistes",
          "text": "Could you share a bit more about the process, how did you set it up to learn rust from it? Thx",
          "score": 1,
          "created_utc": "2026-02-01 18:17:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30imc2",
              "author": "feursteiner",
              "text": "so it was a process, first I tried to setup just a readme with a curriculum (opus generated that I think, or gpt5.2), but then I went and setup the agents\\[.\\]md. I knew I wanted to have different level of answers depending on how much detail I want, so I setup the \"pyramid method\" which is how news articles are written.  \nthen I started slowly to scaffold what a lesson is and what an exercise is, then added an \"ex-00\" which just gives me basic syntax to learn, and other exercises to teach the concepts.  \nI found myself learning by analogy (bun vs cargo, memory management in C...) so I told the agents file about my background so that it explain conxepts in a relevant manner.  \nanyhow, it's a moving process, but I hitnk it's getting better as I advance in lessons, happy to give you more detail if you want (pyramid method again haha).",
              "score": 3,
              "created_utc": "2026-02-01 18:30:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3254p2",
                  "author": "antifeixistes",
                  "text": "Thanks! Also saw your other reply with the repo. Will check that out. Thanks a lot!",
                  "score": 2,
                  "created_utc": "2026-02-01 23:14:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30sqzq",
          "author": "larowin",
          "text": "Do you think you could explain a borrow checker without help yet?",
          "score": 1,
          "created_utc": "2026-02-01 19:16:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30svt1",
              "author": "feursteiner",
              "text": "oh yeah def haha",
              "score": 2,
              "created_utc": "2026-02-01 19:16:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o30teox",
                  "author": "feursteiner",
                  "text": "basically a variable's value can be borrowed, i.e. if a = 5, I can declare b that points to the value so to speak. I am allowed to do operations on b (multiple reads), if it's a mut (an actual variable), I can only have one mutation reference at a time. and finally, when a goes out of scope, it's freed from memory. and we can't have borrows outside the scope of a, htat's called hanging.. how did I do ? haha",
                  "score": 1,
                  "created_utc": "2026-02-01 19:19:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o31e3bm",
          "author": "Michaeli_Starky",
          "text": "Books. Use them for learning.",
          "score": 1,
          "created_utc": "2026-02-01 20:59:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31ldm8",
              "author": "feursteiner",
              "text": "thanks granpa.",
              "score": 0,
              "created_utc": "2026-02-01 21:34:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31mjhw",
                  "author": "Michaeli_Starky",
                  "text": "You're welcome.",
                  "score": 1,
                  "created_utc": "2026-02-01 21:40:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o328e6c",
          "author": "haobes",
          "text": "how",
          "score": 1,
          "created_utc": "2026-02-01 23:32:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qt06bj",
      "title": "OpenCode Bar 2.0: It auto-detects all your AI providers from OpenCode. Zero setup.",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/qe8z8tsfawgg1.png",
      "author": "kargnas2",
      "created_utc": "2026-02-01 14:41:05",
      "score": 33,
      "num_comments": 12,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qt06bj/opencode_bar_20_it_autodetects_all_your_ai/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2zfpw3",
          "author": "Putrid-Pair-6194",
          "text": "Cool. But Mac only? Any chance of a windows or a command line version?",
          "score": 8,
          "created_utc": "2026-02-01 15:31:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34dyi7",
              "author": "digitalfreshair",
              "text": "yes, linux would be awesome. Or a cli would be even better",
              "score": 2,
              "created_utc": "2026-02-02 08:01:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34qnki",
                  "author": "kargnas2",
                  "text": "I released CLI version but maybe it doesn't work at Linux yet: [https://www.reddit.com/r/opencodeCLI/comments/1qtqx2p/opencode\\_bar\\_21\\_now\\_with\\_cli\\_perprovider/](https://www.reddit.com/r/opencodeCLI/comments/1qtqx2p/opencode_bar_21_now_with_cli_perprovider/)",
                  "score": 1,
                  "created_utc": "2026-02-02 10:04:24",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zjqox",
          "author": "mylittlecumprincess",
          "text": "It says \"err\" in the menubar using the latest Opencode on Mac \n\nhttps://preview.redd.it/palduvkzmwgg1.jpeg?width=342&format=pjpg&auto=webp&s=c2c6c6cb560740eac711d5e17a2d7b4465a23ade\n\n",
          "score": 2,
          "created_utc": "2026-02-01 15:51:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zr11h",
              "author": "kargnas2",
              "text": "adding a log viewer!",
              "score": 3,
              "created_utc": "2026-02-01 16:25:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zrjsn",
          "author": "aimericg",
          "text": "I am honestly not having much trouble with this. I don't find the setup to be that hard.",
          "score": 2,
          "created_utc": "2026-02-01 16:27:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zx6hm",
              "author": "thatsnot_kawaii_bro",
              "text": "...Then don't need to use it?",
              "score": 1,
              "created_utc": "2026-02-01 16:53:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o318c3a",
                  "author": "KHALIMER0",
                  "text": "![gif](giphy|dW0KIk9KCsWBy|downsized)",
                  "score": 2,
                  "created_utc": "2026-02-01 20:30:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zz26s",
          "author": "Ok_Proposal_1290",
          "text": "I LOVE THIS, but if it isn't too much work, is there some way it could support windows or alternatively linux?",
          "score": 2,
          "created_utc": "2026-02-01 17:01:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o30js2p",
          "author": "touristtam",
          "text": "I have been using https://github.com/nguyenphutrong/quotio with mitigated success. \n\n---\n\nI'd suggest that you create a homebrew recipe; I know I cannot install that outside homebrew/appstore so the dmg is a non starter unfortunately (work is a b***)",
          "score": 2,
          "created_utc": "2026-02-01 18:35:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34nngw",
              "author": "kargnas2",
              "text": "nice idea",
              "score": 1,
              "created_utc": "2026-02-02 09:35:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zb396",
          "author": "lundrog",
          "text": "Hmm interesting, how do we make it support claude code as well? I use that often with other providers",
          "score": 1,
          "created_utc": "2026-02-01 15:09:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsgbob",
      "title": "The amount of open issues for opencode has skyrocketed in the past month",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/dr1a1docgrgg1.png",
      "author": "fajfas3",
      "created_utc": "2026-01-31 22:27:17",
      "score": 28,
      "num_comments": 5,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qsgbob/the_amount_of_open_issues_for_opencode_has/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2vqb1r",
          "author": "trypnosis",
          "text": "Does this correlate with population?",
          "score": 7,
          "created_utc": "2026-01-31 23:51:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zdzoi",
              "author": "inter_fectum",
              "text": "I am sure it is adoption related, and a good sign.",
              "score": 1,
              "created_utc": "2026-02-01 15:23:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ziq5v",
                  "author": "trypnosis",
                  "text": "I was trying to imply the the number of issues have gone up as there are more people not poor quality.",
                  "score": 1,
                  "created_utc": "2026-02-01 15:46:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2vi80t",
          "author": "fajfas3",
          "text": "If you guys want to see more graphs like this you can check them out @ [github-history.com](http://github-history.com)",
          "score": 3,
          "created_utc": "2026-01-31 23:06:27",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtrh0d",
      "title": "Synthetic AI Issues.",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/t5ijzb5462hg1.jpeg",
      "author": "NiceDescription804",
      "created_utc": "2026-02-02 10:27:19",
      "score": 25,
      "num_comments": 52,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qtrh0d/synthetic_ai_issues/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o34yn00",
          "author": "sewer56lol",
          "text": "Synthetic documents which models are hosted where on this page https://dev.synthetic.new/docs/api/models , including their parameters.\n\nThey usually self-host the best open models, and proxy the rest to Fireworks or TogetherAI.\n\nThe Synthetic folks have been setting up self hosted K2.5 over the weekend, but it's not trivial. It's a huge ass model, accepts vision (first for them), and securing the extra hardware has been tough. The folks on the Discord have been pretty transparent about this, and are actively around everyday.\n\nLikewise, Fireworks and TogetherAI both had problems hosting K2.5 on their own, as evidenced by your lack of good performance on these proxied requests. This shouldn't be too surprising, it's a 1T param model after all.\n\nHave some patience.\n\nEdit: Synthetic is self hosting K2.5 as of 5 minutes ago. M2.1 will be proxied away to make compute room.\n[Funny timing]",
          "score": 15,
          "created_utc": "2026-02-02 11:17:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34zc1o",
          "author": "FyreKZ",
          "text": "Similar experience, but I'm sure it'll improve soon.",
          "score": 4,
          "created_utc": "2026-02-02 11:23:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o351jwl",
          "author": "Simple_Split5074",
          "text": "What I find more interesting is that according to the plan you should get 1350 requests every 5 hours but your limit will reset in more than 10h?",
          "score": 4,
          "created_utc": "2026-02-02 11:42:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o39ug13",
              "author": "exploriann",
              "text": "I am currently using pro plan 60$, limits Will be reset every 5 hours, you can clearly track the consumption and time until the next reset in their website.",
              "score": 2,
              "created_utc": "2026-02-03 02:34:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3awe9f",
                  "author": "thebraukwood",
                  "text": "Why‚Äôs this guys screenshot show otherwise then? Genuine question",
                  "score": 1,
                  "created_utc": "2026-02-03 06:59:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34tuy8",
          "author": "alovoids",
          "text": "thank you so much, I'm about to try synthetic. now I won't do that",
          "score": 5,
          "created_utc": "2026-02-02 10:34:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35w1rp",
              "author": "harrypham2000",
              "text": "lol still worth it though, you can use other models like MiniMax or DeepSeek, still dope for the price",
              "score": 5,
              "created_utc": "2026-02-02 14:50:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o34uulx",
          "author": "Xera1",
          "text": "I signed up because the free Kimi through opencode kept timing out and it's been great for me. It's not as fast as Anthropic's but it's at least as fast as Gemini through AG, and it's the new hotness so it's probably getting absolutely hammered.",
          "score": 2,
          "created_utc": "2026-02-02 10:43:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o354bis",
          "author": "philosophical_lens",
          "text": "What are some alternatives? I‚Äôm currently on the Z.AI subscription but looking to try something new.",
          "score": 2,
          "created_utc": "2026-02-02 12:03:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35wa15",
              "author": "harrypham2000",
              "text": "maybe celebras but their coding plan already sold out",
              "score": 2,
              "created_utc": "2026-02-02 14:52:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o381phf",
                  "author": "ResponsibilityOk1306",
                  "text": "64k context limit for glm",
                  "score": 1,
                  "created_utc": "2026-02-02 20:52:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o356ttk",
          "author": "P1zz4-T0nn0",
          "text": "Worked fine before the weekend. Today K2.5 is unusable. It just stops working, can't finish one response.",
          "score": 2,
          "created_utc": "2026-02-02 12:22:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36z2gu",
              "author": "sudoer777_",
              "text": "Kimi K2.5 Free has been a lot buggier today for me on OpenCode Zen than the previous days so there might be a provider issue involved (Fireworks probably)",
              "score": 1,
              "created_utc": "2026-02-02 17:54:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o381yuf",
                  "author": "ResponsibilityOk1306",
                  "text": "kimi is hosted on synthetic. its very fast on fireworks.",
                  "score": 1,
                  "created_utc": "2026-02-02 20:54:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3529nk",
          "author": "dbkblk",
          "text": "I don't understand your problem. Every time I use it, it's quite fast to answer.  \nIt happens that it could rarely get stuck for some seconds, but not much more than when I was using Claude.  \nEDIT: Ok, probably because I use GLM.",
          "score": 3,
          "created_utc": "2026-02-02 11:47:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o352txp",
              "author": "NiceDescription804",
              "text": "GLM and minimax are doing great but I subscribed for Kimi.",
              "score": 2,
              "created_utc": "2026-02-02 11:52:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o357rkm",
                  "author": "dbkblk",
                  "text": "I've barely tried Kimi, so that's why our experiences differ. But as other said, it has just been deployed. They may are encountering issues with deployment?",
                  "score": 1,
                  "created_utc": "2026-02-02 12:29:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34tx68",
          "author": "jrsa2012",
          "text": "For me it is working just fine.",
          "score": 2,
          "created_utc": "2026-02-02 10:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3566pp",
          "author": "Ok_Direction4392",
          "text": "I also signed up on synthetic recently to use Kimi K2.5 mainly. Only had one failed request so far today, otherwise it's been running solid for a few hours.",
          "score": 1,
          "created_utc": "2026-02-02 12:17:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o359qs3",
          "author": "ryudice",
          "text": " thanks, I was considering it as well, I‚Äôll just stick to the kimi subscription for now",
          "score": 1,
          "created_utc": "2026-02-02 12:43:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35bejv",
          "author": "dyzhdyzh",
          "text": "I subscribed to them yesterday evening. Solely because of Kimi K2.5. Zero issues both yesterday and this morning.",
          "score": 1,
          "created_utc": "2026-02-02 12:54:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o365snq",
              "author": "dyzhdyzh",
              "text": "I stand corrected. It **is** quite slow right now. Only ~30 requests in the last two hours. I see 30+ second delays between tool calls and token output of ~1-3 tokens per second.",
              "score": 1,
              "created_utc": "2026-02-02 15:39:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3awvwn",
                  "author": "thebraukwood",
                  "text": "A lot of new users because of k2.5 but from everything I‚Äôve seen of the Synthetic team I believe they‚Äôll iron out the issues as soon as possible. People need to be more understanding now adays",
                  "score": 1,
                  "created_utc": "2026-02-03 07:03:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35e8wf",
          "author": "blankeos",
          "text": "Really? Damn.. I was gonna get one.",
          "score": 1,
          "created_utc": "2026-02-02 13:12:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35lab5",
          "author": "Josh8972",
          "text": "I've been using Synthetic for a couple of weeks now and switched to Kimi K2.5 when it became available. No problems/issues here.",
          "score": 1,
          "created_utc": "2026-02-02 13:53:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35ojl1",
          "author": "harrypham2000",
          "text": "problem is not with Synthetic hosted models, problem is their provider, sometimes I met this and figured out that most of it caused by their provider for the models like TogetherAI and Fireworks, you could check at [status.synthetic.new](http://status.synthetic.new)",
          "score": 1,
          "created_utc": "2026-02-02 14:11:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35rjad",
              "author": "NiceDescription804",
              "text": "https://preview.redd.it/mcojc7qxc3hg1.jpeg?width=1280&format=pjpg&auto=webp&s=66334182b0d209ff78f1d0cff70d8ccb1b17a68d\n\nWE'RE BACK TO SELF HOSTING.",
              "score": 1,
              "created_utc": "2026-02-02 14:27:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o382xmn",
                  "author": "ResponsibilityOk1306",
                  "text": "either they are lying, or something is wrong. fireworks has been very fast for me, and synthetic very slow. maybe their account is rate limited, or something.",
                  "score": 1,
                  "created_utc": "2026-02-02 20:58:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35r0n7",
          "author": "gonssss",
          "text": "same for me, fucking slow",
          "score": 2,
          "created_utc": "2026-02-02 14:24:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36jon5",
          "author": "annakhouri2150",
          "text": "The limits are not fake, what are you talking about. Maybe you can't reach them ‚Äî due to errors and slowness ‚Äî using Kimi K2.5, but they've got dozens of other very good, competent models available on their API that you can use, and you absolutely can fully saturate the API call limits. I've done it regularly for months since I subscribed. They're absolutely not fake.\n\nRegarding the errors and slowness of K2.5, this is a temporary thing, due to the huge influx of new users they've had, plus the fact that K2.5 was just released. They're actively working on securing more compute, and communicating very actively in their Discord with users about the state of things and listening to complaints (when they're not sleeping, or pulling all-nighters). I've used K2T (K2.5's predecessor) with them since I joined and it has been rock solid. K2.5 will get there eventually too.\n\nI don't think it's fair to call this fake.",
          "score": 1,
          "created_utc": "2026-02-02 16:43:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o381dwt",
          "author": "ResponsibilityOk1306",
          "text": "I also fell for it, but just $10 via api, payg. slow, that my apps timeout after 15 minutes. hitting error 429 frequently. this is just poor service.\n\nFireworks is blazing fast, happy to use them.",
          "score": 1,
          "created_utc": "2026-02-02 20:51:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34tiaf",
          "author": "indian_geek",
          "text": "Have you tried other models? Considering Kimi 2.5 is new, it could potentially be an issue with this model.",
          "score": 1,
          "created_utc": "2026-02-02 10:31:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34ubyj",
              "author": "NiceDescription804",
              "text": "It's not a black or white situation they're advertising serving models, but leaving behind details that it's not usable AT ALL. \n\nI don't have any objections on being up front with consideration. \nBut oh my god did they advertise the living shit out of Kimi k2.5.",
              "score": 1,
              "created_utc": "2026-02-02 10:38:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o36am0r",
                  "author": "indian_geek",
                  "text": "They just posted an update on their discord regarding Kimi",
                  "score": 1,
                  "created_utc": "2026-02-02 16:01:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34v1ux",
          "author": "LittleChallenge8717",
          "text": "Kimi model is't hosted in their gpu's that's reason (they have fireworks provider currently, they plan to host kimi in the next week , currently I agree it has issues on kimi, but glm and minimax works great)",
          "score": 1,
          "created_utc": "2026-02-02 10:45:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34w6m1",
              "author": "NiceDescription804",
              "text": "It's dishonest though to know they can't ensure reliability and just hammer away the ad posts with all these bots.",
              "score": 4,
              "created_utc": "2026-02-02 10:55:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o352huo",
                  "author": "dbkblk",
                  "text": "Which bots are you talking about?",
                  "score": 2,
                  "created_utc": "2026-02-02 11:49:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o354fp1",
          "author": "Bob5k",
          "text": "have in mind that synthetic is actually rerouting kimi k2.5 to fireworks which is having problems - so you're blaming the wrong company for the problems because of your incompetence / ignorance.   \nalso - fireworks have fixed this and kimi is just as usable as it was before weekend hit.",
          "score": 0,
          "created_utc": "2026-02-02 12:04:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o382dp7",
              "author": "ResponsibilityOk1306",
              "text": "I am not sure about this. over the past few days, I have tested both synthetic and fireworks. I haven't experienced any issue on fireworks so far, no errors, super fast answers, etc. on Synthetic, after 15 minutes there is still no reply, when fireworks can get it done in 3 minutes max.",
              "score": 1,
              "created_utc": "2026-02-02 20:55:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o383b85",
                  "author": "Bob5k",
                  "text": "but you do realize that till today at approx 2pm berlin time they've been routing kimi through fireworks? so how fireworks can be worse than fireworks if it's direct routing? :)",
                  "score": 1,
                  "created_utc": "2026-02-02 21:00:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qq1mqp",
      "title": "Try out Kimi K2.5 right via the Synthetic provider NOW",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qq1mqp/try_out_kimi_k25_right_via_the_synthetic_provider/",
      "author": "jpcaparas",
      "created_utc": "2026-01-29 07:03:29",
      "score": 24,
      "num_comments": 87,
      "upvote_ratio": 0.76,
      "text": "If you are using Opus 4.5 now, do yourself a favour and get Kimi K2.5 from synthetic (dot) new and try it out asap. There's a promotion going on with Moltbot where you get 40% off your first month.\n\nK2.5 absolutely SLAYS with tool calling and reasoning. It's nuts. It's a night and day difference with the other Chinese models. GLM 4.7 and Minimax 2.1 don't even hold a candle against it.\n\nI have 20 subagents doing tool calls in parallel and K2.1 IT. DOES. NOT. MISS.\n\n~~I won't even post a referral link~~.   \n  \nHere's my longform, non-paywalled review after trying it out for the last 24 hours (with a solid recommendation from OpenCode's co-creator, Dax):\n\n‚û°Ô∏è [Stop using Claude‚Äôs API for Moltbot (and OpenCode)](https://jpcaparas.medium.com/stop-using-claudes-api-for-moltbot-and-opencode-52f8febd1137)\n\nTry it out and see for yourself.\n\nhttps://preview.redd.it/yoydh7uxl8gg1.png?width=1038&format=png&auto=webp&s=619ecc34f56f66a87f0392ac7bdbfe7993ebe236",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qq1mqp/try_out_kimi_k25_right_via_the_synthetic_provider/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2ddmda",
          "author": "Metalwell",
          "text": "how do you access it inside opencode? What is the synthetic provider? I could not find em on google",
          "score": 5,
          "created_utc": "2026-01-29 07:16:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xcx0e",
              "author": "elllyphant",
              "text": "I made a video tutorial to use Synthetic in OpenCode here! [https://youtu.be/1KAxl7IsrHE?si=5QFlf1G\\_kVJuk3X5](https://youtu.be/1KAxl7IsrHE?si=5QFlf1G_kVJuk3X5)  \nHere's our documentation as well [https://dev.synthetic.new/docs/guides/opencode](https://dev.synthetic.new/docs/guides/opencode)   \nIf you have further questions, please join us on [Discord](https://discord.com/invite/syntheticlab) and we can help you there!",
              "score": 1,
              "created_utc": "2026-02-01 06:01:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2de496",
              "author": "jpcaparas",
              "text": "you sign up for a provider like kimi.com or sythetic first then get an api key. and the link that provider with ctrl + p, or command + p (idk I'm about to take a shower now) within opencode and then select the k2.5 model. by default that model won't show up if the provider isn't linked to begin with.\n\nthen yeah start prompting.",
              "score": 1,
              "created_utc": "2026-01-29 07:20:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2dq73z",
                  "author": "Metalwell",
                  "text": "I understand thanks. [https://synthetic.new/landing/home](https://synthetic.new/landing/home)",
                  "score": 3,
                  "created_utc": "2026-01-29 09:11:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2de7ds",
          "author": "jpcaparas",
          "text": "synthetic.new is their url. you can also just sign up for a kimi for coding subscription and get an api key there.",
          "score": 5,
          "created_utc": "2026-01-29 07:21:32",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2df36j",
              "author": "joakim_ogren",
              "text": "But using directly Kimi K2.5 from Moonshot isn‚Äôt that dangerous compared to synthetic.new?\nMoonshot will train on your data?\nSynthetic will not, right?",
              "score": 3,
              "created_utc": "2026-01-29 07:29:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2dfgh6",
                  "author": "jpcaparas",
                  "text": "Yes that's why I went with synthetic",
                  "score": 6,
                  "created_utc": "2026-01-29 07:32:34",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o2ijlyz",
                  "author": "elllyphant",
                  "text": "Hi I'm Elly from the Synthetic team and want to confirm we are privacy-first and do not train on your data! You can view our privacy policy here: [https://synthetic.new/policies/privacy](https://synthetic.new/policies/privacy)",
                  "score": 1,
                  "created_utc": "2026-01-30 00:41:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2divwq",
              "author": "f2ame5",
              "text": "Have you ever used their coding plan?\n\nI've seen they have a plan but I never seen anyone comment about it. (Limits, performance etc)",
              "score": 2,
              "created_utc": "2026-01-29 08:02:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ett4p",
                  "author": "Lucky_Yam_1581",
                  "text": "They have a week of trial",
                  "score": 2,
                  "created_utc": "2026-01-29 14:00:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2dju6y",
                  "author": "jpcaparas",
                  "text": "I used K2 Turbo on their coding plan and stopped after the $1.5 promo period. I was impressed a bit with the UI but the inference speed was right smack dab in the middle between [Z.ai](http://Z.ai) (slowest) and Minimax (fastest). \n\nI actually wrote an article about it with some examples of how it deals with UI elements:\n\n[https://jpcaparas.medium.com/kimi-k2-turbo-for-claude-code-a-practical-setup-with-a-small-wrapper-0af24da5445f](https://jpcaparas.medium.com/kimi-k2-turbo-for-claude-code-a-practical-setup-with-a-small-wrapper-0af24da5445f)",
                  "score": 1,
                  "created_utc": "2026-01-29 08:11:36",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o2dt89g",
                  "author": "martinsky3k",
                  "text": "there was no real point in using it on k2 imo because it was pretty trash.\n\nI am not gonna get on the same hype train as OP. But I used it in Kimi CLI yesterday, and yeah it's pretty good. Nothing super complicated yet, but it's fast and good reasoning, did resolve some minor issues no problem.\n\nFor me, it felt so far already like a big jump from 2.0. But really have to use it more to know how good it is etc. But Kimi for Coding works fine.",
                  "score": 1,
                  "created_utc": "2026-01-29 09:40:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2dpx8h",
          "author": "NiceDescription804",
          "text": "1350 requests per 5 hours is a bit suspicious are the models quantized?",
          "score": 4,
          "created_utc": "2026-01-29 09:08:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ffb4q",
              "author": "minaskar",
              "text": "No, they are not quantized.",
              "score": 6,
              "created_utc": "2026-01-29 15:44:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2e6p67",
              "author": "Spaetzlefan",
              "text": "(I know nothing^2) but when you use the model through synthetic it gets served through hugging face. That made me suspect it was quantized but maybe im wrong.",
              "score": 2,
              "created_utc": "2026-01-29 11:36:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ijxqh",
                  "author": "elllyphant",
                  "text": "Hi I'm Elly from Synthetic and wanted to confirm that we do NOT quantize any models ourselves.\n\nhttps://preview.redd.it/7lnwizs2vdgg1.png?width=1384&format=png&auto=webp&s=fe12b2bd98127c1e339481cc39d82fed540b4852",
                  "score": 3,
                  "created_utc": "2026-01-30 00:42:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ebiag",
                  "author": "NiceDescription804",
                  "text": "Yeah I stopped trusting all these super cheap providers. \n\nAlthough I really like Kimi 2.5 I'm on the 7 day trial it's doing pretty well with the frontend.",
                  "score": 2,
                  "created_utc": "2026-01-29 12:11:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o36bk9b",
                  "author": "annakhouri2150",
                  "text": "the models aren't run \"through huggingface\", they just use the huggingface identifier, because their underlying infrastructure uses that because it's also used for their on-demand model inference service that runs any hugging face model you want on their cloud compute infra.",
                  "score": 1,
                  "created_utc": "2026-02-02 16:06:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2i9ozm",
              "author": "Select-Service-5023",
              "text": "You can ask in their discord. Yes and no. They at quantized to whatever the lab officially releases. IE: Kimi 2.5 is a quant4 since that‚Äôs what Kimi states is their intended quant.\n\nAka: no they don‚Äôt further quantize just to save compute.",
              "score": 2,
              "created_utc": "2026-01-29 23:47:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o36bebw",
              "author": "annakhouri2150",
              "text": "I'm regularly on their discord and use them a lot, the models are not quantized at all.",
              "score": 1,
              "created_utc": "2026-02-02 16:05:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dxak8",
          "author": "mintybadgerme",
          "text": "What an absolute load of tosh. Kimi 2.5 fails regularly when you start getting into more complex stuff. I've tried it with two projects over the past two days which have melted down because of the failure of the model.",
          "score": 3,
          "created_utc": "2026-01-29 10:16:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e38q3",
              "author": "aeroumbria",
              "text": "I think opencode just fixed a glitch causing write tools to fail with large edits. Might be related to that. The model felt much more usable afterwards.",
              "score": 2,
              "created_utc": "2026-01-29 11:08:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2e6wi1",
                  "author": "mintybadgerme",
                  "text": "Oh interesting, thanks. I'll try it out.",
                  "score": 1,
                  "created_utc": "2026-01-29 11:38:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2gscu4",
                  "author": "jpcaparas",
                  "text": "Yes that glitch happens quite a lot with Codex for me",
                  "score": 1,
                  "created_utc": "2026-01-29 19:25:18",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2e2k2a",
          "author": "fistikidis",
          "text": "I just signed up for the standard plan (20USD) yesterday. I see why the limits are high. I get a lot of 503 not healthy upstream errors, especially with Deepseek and Kimi right now, but they get resolved in under a minute or two. The value you get is great, won't lie but I will have to use it more to actually see if that's an issue.",
          "score": 3,
          "created_utc": "2026-01-29 11:02:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dlfie",
          "author": "SamatIssatov",
          "text": "How annoying these salespeople and marketers are. For $20, you're better off buying a Codex subscription. It's much better than your Kimi, Zai, Mai.",
          "score": 8,
          "created_utc": "2026-01-29 08:26:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dlyx4",
              "author": "jpcaparas",
              "text": "I have a ChatGPT Plus subscription. It works fine in Codex itself (I stan for it and I've written numerous guides about it), but on OpenCode it's really bad at tool-calling, plus it's slow (but that's already common knowledge).  \n  \nI need something that's reliable on this particular harness.",
              "score": 5,
              "created_utc": "2026-01-29 08:31:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2dnkbc",
                  "author": "RainScum6677",
                  "text": "Codex is very good with openCode. Not the fastest, for sure, but very good. I have the 200$ codex plan, and with openCode, that's all I need as a professional (full stack, HUGE codebase, legacy .NET). I run it for about 6 hrs a day.",
                  "score": 6,
                  "created_utc": "2026-01-29 08:46:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2dmia9",
                  "author": "SamatIssatov",
                  "text": "In my case, on the contrary, Codex works very well in opencode. opencode has better instructions. I fell in love with codex after switching to Opencode. The output in the terminal is very readable, and in the desktop version, you can manage multiple sessions. There was also a lot of advertising for GLM from Zai, and in the end, I realized that it was more economical to just buy codex. You will also be deceiving everyone, and after a while, everyone will leave anyway. What is the point of deceiving everyone?",
                  "score": 2,
                  "created_utc": "2026-01-29 08:36:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2dmrfn",
              "author": "No_Success3928",
              "text": "Yes much better.. for a few hours a week before youre at the limit ü§£ü§£ü§£",
              "score": 1,
              "created_utc": "2026-01-29 08:38:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dey26",
          "author": "Select-Service-5023",
          "text": "wanna bump synthetic.new.  got a sub a month ago and their founders are really active and involved in their discord. They had kimi 2.5 hosted within 24 hours.\n\nPeople post \"complaints\" about model usage and I see responses and fixes within minutes to hours. Can NOT understate how impossible this kind of accountability is to get nowadays.",
          "score": 6,
          "created_utc": "2026-01-29 07:28:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dfd1i",
              "author": "jpcaparas",
              "text": "I actually emailed them just now wanting to go from the standard to pro plan while keeping my first month discount\n\ni couldnt find a worthy contender to opus 4.5 for tool-calling until today. I can finally stop being billed thousands of dollars on something I can pay $60/month for (yes I'm on bedrock)",
              "score": 2,
              "created_utc": "2026-01-29 07:31:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2dwzry",
                  "author": "LittleChallenge8717",
                  "text": "How you got 1st month discount?",
                  "score": 1,
                  "created_utc": "2026-01-29 10:14:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2dqa8t",
              "author": "harrypham2000",
              "text": "agreed, [synthetic.new](http://synthetic.new) is dope",
              "score": 2,
              "created_utc": "2026-01-29 09:12:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2eyxp3",
                  "author": "touristtam",
                  "text": "What sub are you on though as a point of reference?",
                  "score": 2,
                  "created_utc": "2026-01-29 14:27:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2dufoq",
          "author": "belgatron",
          "text": "For me, Kimi K2.5 is very slow in Synthetic",
          "score": 2,
          "created_utc": "2026-01-29 09:51:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2iktiz",
              "author": "elllyphant",
              "text": "ah is it still? Our team has been working on it to make it faster!",
              "score": 1,
              "created_utc": "2026-01-30 00:47:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2nui33",
                  "author": "francescored94",
                  "text": "Careful guys, during today I have noticed at some point it got less accurate at following instructions.\n\ncontext:\nI have subscription. I use Kimi K2.5 Synthetic with my own agentic tool (a pseudo Opencode/Crush, trying to take what I like best from this two different project and build my own).",
                  "score": 2,
                  "created_utc": "2026-01-30 19:44:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o32uxfo",
                  "author": "AmalgamDragon",
                  "text": "Yes, it is still very slow.  I've only managed 14.9 request in my first 3.5 hours as new subscriber on the $60 plan.  I was using Kimi K2.5 via Fireworks yesterday.  While that isn't exactly speedy compared to Sonnet 4.5 or Cursor's Composer 1, it wasn't this glacial.  Hopefully you'll get it self-hosted soon and get responses times down.",
                  "score": 1,
                  "created_utc": "2026-02-02 01:38:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ecg7a",
          "author": "hexa01010",
          "text": "I agree the model is insane I spent last night playing with it. first it's super fast so much so that I had the time to go from being sceptical about it and checking all code outputs to letting it refactor my code with 6 agents in parallel in a matter of hours!! \n\nI was testing got 5.2 with the same use cases the last few days and it was crawling to do anything and I never reached the confidence level I have with Kimi now. It will also loop on solutions do one thing and then undo it... Kimi so far stays on track!\n\nAnd opus 4.5 seems like it's just dumb now and can't use Claude code in opencode so no fun\n\nUsed it via Zen and moonshot so far will check you synthetic today",
          "score": 2,
          "created_utc": "2026-01-29 12:18:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2edt5q",
              "author": "Metalwell",
              "text": "I am a huge fanboy of 5.2. It is amazing but I hate its UI capabilities. I cannot get it to design proper UI and UX. Do you think Kimi 2.5 is good in this matter? I am deciding from which I should get 2.5.",
              "score": 1,
              "created_utc": "2026-01-29 12:27:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ees8m",
                  "author": "hexa01010",
                  "text": "I have not tested that so much yet but the thing is you'll be able to write 10 iterations or more of your design in the same time that gpt will give you one hehe also they do advertise a lot of visual capabilities like giving it a video and it replicates a website in detail so yeah it's certainly one to try with 5$ on zen honestly I think you would have enough to test and make yourself an idea ( or free trial on moon if you don't mind their terms for testing)",
                  "score": 1,
                  "created_utc": "2026-01-29 12:33:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2il8yd",
              "author": "elllyphant",
              "text": "If you try out synthetic today, here's 40% off :) [https://synthetic.new/?saleType=moltbot](https://synthetic.new/?saleType=moltbot) ends 2/1!",
              "score": 1,
              "created_utc": "2026-01-30 00:49:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2df5cj",
          "author": "jpcaparas",
          "text": "Also an obligatory callout to Z.ai for their laughable inference even on their Max plan (cant complain that much though, I got mine for free). I could finish a pilates class and my task wouldn't be finished yet. \n\nhalf the time, the right skills wouldn't even load on Z.ai even if declare them on the command/subagent. \n\nonly thing that I like about z.ai are their MCP servers.",
          "score": 2,
          "created_utc": "2026-01-29 07:29:47",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2dm4pd",
              "author": "Queasy_Asparagus69",
              "text": "Mcp servers? To do what? I didn‚Äôt know they had MCPs",
              "score": 1,
              "created_utc": "2026-01-29 08:32:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2dmuue",
                  "author": "No_Success3928",
                  "text": "Not on the lite plan but the other two have them",
                  "score": 1,
                  "created_utc": "2026-01-29 08:39:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2dn3nz",
                  "author": "jpcaparas",
                  "text": "Yes they do, 4 of them:\n\n(no paywalls on any of the below):\n\n[https://jpcaparas.medium.com/claude-code-with-z-ai-vision-mcp-master-the-full-toolbelt-4447c2f953a0](https://jpcaparas.medium.com/claude-code-with-z-ai-vision-mcp-master-the-full-toolbelt-4447c2f953a0)\n\n[https://jpcaparas.medium.com/search-vs-reader-vs-zread-a-claude-code-guide-to-z-ai-mcp-servers-134cece1ad96](https://jpcaparas.medium.com/search-vs-reader-vs-zread-a-claude-code-guide-to-z-ai-mcp-servers-134cece1ad96)\n\nWhen I was on their Pro plan, most of my quota was actually against MCP usage.",
                  "score": 1,
                  "created_utc": "2026-01-29 08:42:07",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2i3eir",
              "author": "ZeSprawl",
              "text": "Yeah I can't use [z.ai](http://z.ai) provider for in the loop inference, but limits are so high on my pro plan that I just use it when I want to step away for hours and not worry about hitting limits. So it's my implementation model for overnight tasks, or when I want to hang with friends or go on a hike and come back to something built.",
              "score": 1,
              "created_utc": "2026-01-29 23:13:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dst1q",
          "author": "Simple_Split5074",
          "text": "How much use do you really get out of the 135 requests with the tool call discount? In other words, can you realistically code for a few hours straight (one or two agents at a time)? What tps do you roughly get?¬†\n\n\nI am trying k2.5 on nanogpt but still experiencing a fair number of failed tool calls...¬†",
          "score": 1,
          "created_utc": "2026-01-29 09:36:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dtf9x",
              "author": "martinsky3k",
              "text": "sounds like a harness thing then. didn't have any failed tool calls at all for hours yesterday.",
              "score": 1,
              "created_utc": "2026-01-29 09:41:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2du1uc",
                  "author": "Simple_Split5074",
                  "text": "With nanogpt or synthetic?\n\n\n\nMy suspicion is one of the nanogpt backends is misconfigured as sometimes it's fast and smooth then all breaks again.¬†\nI was using latest opencode (and briefly CC, same story)¬†",
                  "score": 1,
                  "created_utc": "2026-01-29 09:47:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2dxc12",
          "author": "klocus",
          "text": "Isn't Chutes or NanoGPT cheaper? $10 for 2.000 requests per day at Chutes. $8 for 60.000 request per month at NanoGPT.",
          "score": 1,
          "created_utc": "2026-01-29 10:17:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e2em2",
              "author": "abeecrombie",
              "text": "Have you tried both?  Hows the speed",
              "score": 2,
              "created_utc": "2026-01-29 11:01:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2k0zpc",
                  "author": "febryanvald0",
                  "text": "Chutes should be better as they can scale up, as it needs while NanoGPT heavily depends on Fireworks if im not mistaken",
                  "score": 1,
                  "created_utc": "2026-01-30 06:01:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ex15m",
          "author": "marrabld",
          "text": "Yes but it's late and I'm trying to sleep so don't shout.  NOW at me please",
          "score": 1,
          "created_utc": "2026-01-29 14:17:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fd4zi",
              "author": "jpcaparas",
              "text": "sorry",
              "score": 1,
              "created_utc": "2026-01-29 15:35:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2i2jhp",
          "author": "ZeSprawl",
          "text": "So far I think it's great at planning and reviewing, but GLM 4.7 still seems smarter at actually generating code and consistent tool calling, but you need to configure GLM 4.7 properly as described here: \n\n[https://www.linkedin.com/posts/jan-m-feddersen-2283bbb8\\_airesearch-cerebras-glm4-activity-7419140674207412224-Fuk2](https://www.linkedin.com/posts/jan-m-feddersen-2283bbb8_airesearch-cerebras-glm4-activity-7419140674207412224-Fuk2)",
          "score": 1,
          "created_utc": "2026-01-29 23:09:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2phvr5",
          "author": "EasyDev_",
          "text": "Is there a proper opencode.json model setting when using synthetic.new?\n\nhttps://preview.redd.it/372trbs20lgg1.png?width=333&format=png&auto=webp&s=ebed3c5cfef39596addb889bafe39b31b3dc3944",
          "score": 1,
          "created_utc": "2026-01-31 00:43:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2piy66",
              "author": "jpcaparas",
              "text": "they're a provider. if you add your api key when you add them as a provider, the models just pop up.",
              "score": 1,
              "created_utc": "2026-01-31 00:49:09",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2xdh1x",
              "author": "elllyphant",
              "text": "Sometimes you need to ensure you're on the latest version. If so, I made a video tutorial to use Synthetic in OpenCode here! [https://youtu.be/1KAxl7IsrHE?si=5QFlf1G\\_kVJuk3X5](https://youtu.be/1KAxl7IsrHE?si=5QFlf1G_kVJuk3X5)\n\nHere's our doc: [https://dev.synthetic.new/docs/guides/opencode](https://dev.synthetic.new/docs/guides/opencode)¬†\n\nIf you have further questions, please join us on [Discord](https://discord.com/invite/syntheticlab) and we can help you there!",
              "score": 1,
              "created_utc": "2026-02-01 06:05:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2etjq9",
          "author": "Bob5k",
          "text": "[have in mind that you can grab up to 50% off on standard plan via just using reflink](https://synthetic.new/?referral=IDyp75aoQpW9YFt)",
          "score": 1,
          "created_utc": "2026-01-29 13:59:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}