{
  "metadata": {
    "last_updated": "2026-02-19 17:08:33",
    "time_filter": "week",
    "subreddit": "opencodeCLI",
    "total_items": 20,
    "total_comments": 220,
    "file_size_bytes": 202842
  },
  "items": [
    {
      "id": "1r7bts8",
      "title": "GLM5 is free for a week",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/jyoyelzc63kg1.jpeg",
      "author": "jpcaparas",
      "created_utc": "2026-02-17 17:02:52",
      "score": 259,
      "num_comments": 24,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r7bts8/glm5_is_free_for_a_week/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5w8ljm",
          "author": "EchoesInBackpack",
          "text": "Would be funny if it works better than on zai paid plan",
          "score": 30,
          "created_utc": "2026-02-17 17:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w9x9j",
              "author": "shaonline",
              "text": "They typically do these type of deals with american providers (I think Kimi was on fireworks for example ?) so it probably will.",
              "score": 12,
              "created_utc": "2026-02-17 17:14:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wcgh7",
                  "author": "Spitfire1900",
                  "text": "ollama/glm-5:cloud has been too",
                  "score": 3,
                  "created_utc": "2026-02-17 17:27:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o69map0",
                  "author": "sudoer777_",
                  "text": "Kimi K2.5 Free is on Moonshot AI I believe, since I got rate limited a couple days ago and the error message was from that",
                  "score": 1,
                  "created_utc": "2026-02-19 16:42:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5wnuso",
              "author": "Noob_l",
              "text": "As a max user of z.ai coding plan, I would not recommend it. It now has less usage than other coding plans like minimax and codex. \nAnd the constant errors and silent changes on users should be red flash to not prolong the subscription. Went from being of the best plans in terms of affordable pricing and usage to one of the worst.\n\n-- not even Claude code fills up your weekly usage in just 5 5hour windows. Z.ai does that to you though.\n\nAnd really a warning to all: it wil read your env file and will delete files even if you said explicitly not to in the same conversation as well as having that as fixed rules in the agents file\n\nDon't use glm 5",
              "score": 4,
              "created_utc": "2026-02-17 18:20:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wqno2",
                  "author": "EchoesInBackpack",
                  "text": "I tried glm5 at openzen, it it was decent and very fast. I thought that it can be my daily driver. Then I bought the sub on zai, and it barely works, not usable at all. I feel scummed.",
                  "score": 5,
                  "created_utc": "2026-02-17 18:32:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ws9l9",
                  "author": "jpcaparas",
                  "text": "same, horrendous drop off in speed after the second day. Why even pay for ultra if you're bogged down to lite inference.",
                  "score": 3,
                  "created_utc": "2026-02-17 18:40:18",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o5z8e86",
                  "author": "Jlocke98",
                  "text": "You can max out the kimi weekly plan at the 20usd tier with 2x 5hr windows",
                  "score": 2,
                  "created_utc": "2026-02-18 02:02:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o68s0mh",
              "author": "RedParaglider",
              "text": "I can't imagine that it woulnd't.  ZAI is fucked on inference infrastructure.",
              "score": 1,
              "created_utc": "2026-02-19 14:09:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wpnik",
          "author": "TurnUpThe4D3D3D3",
          "text": "Cerebras for free would be crazy",
          "score": 5,
          "created_utc": "2026-02-17 18:28:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zujw1",
              "author": "MrBlueAndWhite6_2",
              "text": "Any relation to this post or you are just saying?",
              "score": 2,
              "created_utc": "2026-02-18 04:10:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wxc6g",
          "author": "hatepoorpeople",
          "text": "I just cancelled my zai subscription. GLM-5 is completely unusable for me.",
          "score": 4,
          "created_utc": "2026-02-17 19:03:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xc654",
          "author": "FaerunAtanvar",
          "text": "I tried to use ite and I instantly got \"all credits used\"",
          "score": 2,
          "created_utc": "2026-02-17 20:13:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yph42",
          "author": "Euphoric-Doughnut538",
          "text": "Canâ€™t get shit done with the limits",
          "score": 2,
          "created_utc": "2026-02-18 00:22:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6226d2",
              "author": "InternalFarmer2650",
              "text": "Opencode has different limitations than zAI API\n\nI could get decent amount of work done with their free usage, seemingly more than some who pay for the zAI subscription",
              "score": 1,
              "created_utc": "2026-02-18 14:19:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o69chrw",
              "author": "luc122c",
              "text": "The rate limits make it impossible to do anything ðŸ’€",
              "score": 1,
              "created_utc": "2026-02-19 15:54:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o609map",
          "author": "Alternative-Spray176",
          "text": "This model is good. Unlike m2.5",
          "score": 1,
          "created_utc": "2026-02-18 05:59:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5vv6g",
      "title": "Minimax M2.5 is not worth the hype compared to Kimi 2.5 and GLM 5",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r5vv6g/minimax_m25_is_not_worth_the_hype_compared_to/",
      "author": "Resident-Ad-5419",
      "created_utc": "2026-02-16 01:14:20",
      "score": 87,
      "num_comments": 47,
      "upvote_ratio": 0.93,
      "text": "I used opencode with exa; to test the latest GLM 5, Kimi 2.5 and Minimax M2.5, along with Codex 5.3 and Opus 4.6 (in its own cli) to understand how would they work on my prompt. And the results were very disappointing.\n\nDespite all these posts, videos and benchmarks stating how awesome minimax m2.5 is, it failed my test horribly given the same environment and prompt, that the others easily passed.\n\nMinimax kept hallucinating various solutions and situations that didn't make any sense. It didn't properly search online or utilized the available documentation properly. So, I wonder how all those benchmarks claiming minimax as some opus alternative actually made their benchmark.\n\nI saw a few other real benchmarks where Minimax M2.5 actually was way below Haiku 4.5 while GLM 5 and Kimi went above Sonnet 4.5; personally it felt like that as well. So at the increased price points from all these providers, its very interesting. Though neither are on opus or codex level.\n\nI did not test the same prompt with gemini, or couldn't test it, to be more precise due to circumstances. But I have a feeling Gemini 3 Pro would be similar to Kimi and GLM 5, maybe just a bit higher.\n\nWhat is your experience with Minimax compared to GLM and Kimi?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r5vv6g/minimax_m25_is_not_worth_the_hype_compared_to/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5lx2xc",
          "author": "Specialist-Yard3699",
          "text": "GLM still has its old problem - horrible speed. \nKimi25 - low limits. In my opinion, this model is not so good in terms of price-to-quality ratio.\nMinimax25 - fast and a good â€œplan executorâ€/â€œcode searcherâ€. I have worked a lot with Minimax21, and the 25th version is much better and has a lower hallucination rate.",
          "score": 18,
          "created_utc": "2026-02-16 01:26:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5m1i9x",
              "author": "lopydark",
              "text": "where minimax 25",
              "score": 1,
              "created_utc": "2026-02-16 01:55:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5m9h0y",
                  "author": "Specialist-Yard3699",
                  "text": "Official minimax code plan + oh-my-opencode plugin + detailed plan from reasoning model (Glm/gemini3)",
                  "score": 2,
                  "created_utc": "2026-02-16 02:46:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5lyi2e",
              "author": "Resident-Ad-5419",
              "text": "It hallucinated a lot on my prompt, the same one given to opus, codex, kimi and glm and those four worked well for that, while minimax failed horribly. It kept inventing stuff that doesnt even exist. I tested many times just to be sure I wasn't the one hallucinating.",
              "score": -1,
              "created_utc": "2026-02-16 01:35:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5m59di",
          "author": "tripleshielded",
          "text": "Yes, minimax still cant do any difficult work on its own. But m2.5 seems a bit better than m2.1, its a good upgrade.",
          "score": 6,
          "created_utc": "2026-02-16 02:19:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ngr1l",
          "author": "kshnkvn",
          "text": "Skill issue. I'm not joking. Minimax is beast for use as a subagent, as an executor/researcher/etc.\nYou can't use it straight as opus/got and think that it will act as good, at least because it's small model, really small, it knows much less then competitors so you need to provide proper information and context.",
          "score": 7,
          "created_utc": "2026-02-16 08:28:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nvsu5",
              "author": "Resident-Ad-5419",
              "text": "I don't disagree with you on this point, this is worth a shot! Thank you!",
              "score": 2,
              "created_utc": "2026-02-16 10:50:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5v4vtp",
              "author": "Resident-Ad-5419",
              "text": "After your reply I went back and ran a test with Codex as the main agent and GLM, Kimi, Minimax and Codex Spark as sub agent. \n\nCodex + Codex Spark did the best, even better than codex solo.  \nKimi and GLM afterwards.  \nMinimax still couldn't beat even after instructions and helping hands from codex.\n\nI would like to say it's my skill issue, that I am not skilled enough to handle minimax like you do.",
              "score": 1,
              "created_utc": "2026-02-17 13:45:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5v8nai",
                  "author": "kshnkvn",
                  "text": "Idk what exactly you want me to say, because I literally know nothing about your workflow, stack, tasks.  \nIf M2.5 doesn't suit your needs it's fine, because it's not a general-purpose model, it has it's own limitations, cons and pros. It may be either good or bad depends on your tasks and how you threat it.  ",
                  "score": 2,
                  "created_utc": "2026-02-17 14:05:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5m5ovy",
          "author": "segmond",
          "text": "they all have their strengths, i run all of them locally.   minimax2.5 was able to solve a problem that I couldn't get GLM5 and KimiK2.5 to solve after a few prompts, minimax solved it with the same prompt in one go, and generated almost 4,000 lines of perfect code.  The interesting thing is I couldn't have started a solution to the problem without Kimi K2.5 it has very unique capability that many other models don't.  IMO, they all have strengths and you have to know when to use one for another.  I like Minimax not because it solved my problem but I can also run it much faster.  ",
          "score": 5,
          "created_utc": "2026-02-16 02:21:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5oc5nq",
              "author": "Resident-Ad-5419",
              "text": "Absolutely! Any model that solves your problem is the best model, regardless of whatever anyone says otherwise.",
              "score": 2,
              "created_utc": "2026-02-16 12:59:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o64qlgo",
              "author": "FinancialMoney6969",
              "text": "you must have a beast of a machine.....",
              "score": 1,
              "created_utc": "2026-02-18 21:42:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o65l46q",
                  "author": "segmond",
                  "text": "yeah, for the average home user.  but not for a local LLM nuts.  a few 3090s that I bought used from FB marketplace on a 512gb machine.   My original MB was $100 from aliexpress.  Used dual xeon CPUs for $5 each and $600 for the ram from ebay late last year.   It's slow, qwen3.5 runs at 12tk/sec, KimiK2.5 at about 7-8tk/sec  Minimax at about 18tk/sec so not too bad for local.",
                  "score": 1,
                  "created_utc": "2026-02-19 00:17:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5mvemk",
          "author": "Lpaydat",
          "text": "GLM5 is really good. Really really good. Just quite slow sometimes.",
          "score": 5,
          "created_utc": "2026-02-16 05:20:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mpois",
          "author": "lundrog",
          "text": "K2.5 is very good on boardwell hardware , with the nvidia nvidia/K.imi-K2.5-NVFP4; assuming you find a provider hosing it. ( allows less memory usage for same performance )",
          "score": 3,
          "created_utc": "2026-02-16 04:37:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mwauu",
          "author": "Medical_Farm6787",
          "text": "To me since Iâ€™m using the free usage on opencode, probably due to the last GLM4.7 infinite thinking loop incident I just completely stopped using it.\n\nMy current workflow can be Kimi K2.5 for any web search related or codebase exploration â€”> switch to Minimax M2.5 for actual coding practices.\n\nBecause Minimax M2.5 did actually fall very short in terms of searching benchmarks you can look that up on their official benchmark results, but almost on par with opus and to me it really does feel like it, the thoughts are more structured than Kimi K2.5 when it comes to coding implementation, Kimi sometimes forgets the rules that I set in AGENTS.md as context window grows, but yet to have any issue when it comes to Minimax M2.5\n\nNot to say that Minimax M2.5 size is way more smaller than GLM so it feels well in my M3 ultra 256gb unified memory at Q6",
          "score": 3,
          "created_utc": "2026-02-16 05:27:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m2d16",
          "author": "czumaa",
          "text": "i think it's just me but now i think Minimax m2.5 just PLAIN LIE to all of us. Is not even close to the other models and have some weird allucinations. I test this models on coding. Just broke the entire project, don't respect the rules file of kilo code, re-do code as \"his\" way without check. is a disaster. i'm with GLM5 and kimi2.5 now. don't know which rely is the best but i think glm5 is a little best sometimes.",
          "score": 2,
          "created_utc": "2026-02-16 02:00:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mnpy1",
          "author": "Xhatz",
          "text": "It's great for it's size and fast, but it's truly NOT as good as they say, clearly. For me it feels like it's just m2.1 but with even less coherence sadly, hallucinations are too high (I can say something and a few messages afterwards it says something else). It also feels more \"lazy\" in a way... My theory is that it's only good at very specific things and just completely bad at everything else.",
          "score": 2,
          "created_utc": "2026-02-16 04:23:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mec9t",
          "author": "DistinctWay9169",
          "text": "MIniMax is pure hype. Tried it and nope, thanks. Broke my codebase many times to allow it to touch it again.",
          "score": 3,
          "created_utc": "2026-02-16 03:18:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n3sdd",
          "author": "chiroro_jr",
          "text": "Thought I was the only one. I'm still using Kimi. The limits and speed are good enough for me",
          "score": 1,
          "created_utc": "2026-02-16 06:30:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nbah4",
          "author": "SphaeroX",
          "text": "I also think that Kimi 2.5 is still unbeatable! But we'll see, the new DeepSeek should be ready soon.",
          "score": 1,
          "created_utc": "2026-02-16 07:37:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nd0vv",
          "author": "ciprianveg",
          "text": "maybe try with a lower temperature like 0.7?",
          "score": 1,
          "created_utc": "2026-02-16 07:53:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nszvi",
          "author": "xmnstr",
          "text": "I found the same. Cleary worse compared to GLM 5 and K2.5. I don't really understand why companies do this when it's so obvious to anyone who tries the model out that it simply does not hold up.",
          "score": 1,
          "created_utc": "2026-02-16 10:24:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nt77k",
          "author": "tricky-oooooo",
          "text": "Well, it's much smaller than both Kimi 2.5 and GLM5. What did you expect?",
          "score": 1,
          "created_utc": "2026-02-16 10:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5numcg",
          "author": "rudingshain",
          "text": "I use it Not for Coding but with opencode in textrelevant task and it works weil",
          "score": 1,
          "created_utc": "2026-02-16 10:40:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o81rh",
          "author": "odrakcir",
          "text": "not sure what to say, I've been using it for the last couple of days to write and fix a bunch of unit tests (react native) and it's been great. I've used it like this: plan mode -> openspec (explore + ff + manual review + implement + verify + archive).",
          "score": 1,
          "created_utc": "2026-02-16 12:31:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5obxcd",
              "author": "Resident-Ad-5419",
              "text": "If you use codex/opus as a driver; then haiku, minimax or any other smaller model can do wonders. Problem is the way they market it as if they are better than codex/opus in benchmarks, which is wrong.",
              "score": 2,
              "created_utc": "2026-02-16 12:58:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5olh5s",
                  "author": "odrakcir",
                  "text": "that is true, but to be honest, that claim makes part of the business haha. Now, what we can't deny is that OS models are getting closer.",
                  "score": 1,
                  "created_utc": "2026-02-16 13:54:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5o90ke",
          "author": "mintybadgerme",
          "text": "In my experience Kimi is definitely the best of the trio. GLM has problems as does Minimax.",
          "score": 1,
          "created_utc": "2026-02-16 12:38:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ooy4q",
          "author": "c0nfluks",
          "text": "I had the exact same experience. Kimi is way better. The benchmarks are cooked. Chinese AI companies are cheating on the exam, basically.",
          "score": 1,
          "created_utc": "2026-02-16 14:13:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5p5u06",
          "author": "l_eo_",
          "text": "MiniMax (both M2.1 and M2.5) for me is great for stable & efficient pipelines. \n\nSpawn researches, assess, return data, etc etc etc. \n\nIt's for me the perfect model for programmatic pipelines and is so far dominating its niche. \n\nI tried many other models and providers, but haven't found anything that could deliver this quality & stability at this cost level. \n\nIf anybody knows of anything that works better for these kind of use cases, please let me know!",
          "score": 1,
          "created_utc": "2026-02-16 15:40:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pv4dk",
          "author": "Alternative-Spray176",
          "text": "M2.5 doesn't follow instructions at all. I asked it to copy the plan it generated to a new file. It started implementing the plan. Retried but no use. I tried it for a few tasks, it doesn't want to listen to the user instructions. That model is hard to use like Google gemini models.",
          "score": 1,
          "created_utc": "2026-02-16 17:37:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ufe3p",
          "author": "Practical_Arm_645",
          "text": "I do have the same feeling. Minimax is almost useless for harder task, but only benefit is the speed. Gemini3pro is also terrible, even worse than the flash. The flash is quite reasonable, similar to sonnet 4.5 level",
          "score": 1,
          "created_utc": "2026-02-17 10:43:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v8bb9",
          "author": "Final-Rush759",
          "text": "It's not magic.  It's a cheap model, works fine for what it is.  It benchmark well with agent tests. It is not at the same level as big models.  It doesn't store as much knowledge as bigger models.",
          "score": 1,
          "created_utc": "2026-02-17 14:03:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w5rdo",
          "author": "StardockEngineer",
          "text": "HARD disagree.\n\nMy results in my tests - MM2.5 frequently implements big large PRDs with efficiency and precision.\n\nKimi often gives a result missing a ton of requirements.\n\nGLM5 for some reason uses a ton of tokens, costing far, far more than MM2.5",
          "score": 1,
          "created_utc": "2026-02-17 16:54:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wugw2",
          "author": "WolfpackBP",
          "text": "It's so good at agentic stuff though! And the price point is so good. \n\nI have found it to be very impressive\n\nKimi was slow and not as good with the agentic stuff at a higher price point \n\nHaven't tried GLM yet",
          "score": 1,
          "created_utc": "2026-02-17 18:50:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yk5ut",
          "author": "Dayclone",
          "text": "It's free right now through Ollama Cloud for a short period but seems to do ok for me. Has it's issues but hey it's free. Can't complain.",
          "score": 1,
          "created_utc": "2026-02-17 23:52:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zuroy",
          "author": "Teslaaforever",
          "text": "I have the opposite experience M2.5 solved a very complicated problem I have while GLM5 and Kimi2.5 actually gave me not working codes and they are outdated even when they searched the web.",
          "score": 1,
          "created_utc": "2026-02-18 04:12:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62dglw",
          "author": "HarjjotSinghh",
          "text": "here's a lively, supportive reply to match:",
          "score": 1,
          "created_utc": "2026-02-18 15:15:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nc971",
          "author": "Bob5k",
          "text": "also have in mind that minimax has released a guaranteed 100tps m2.5 instances / plans, which also fall under the 10% [reflink promo](https://platform.minimax.io/subscribe/coding-plan?code=HO46LCwAJ5&source=link)   \nfaster iteration means more work done even if quality is 5% lower.",
          "score": -1,
          "created_utc": "2026-02-16 07:46:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4mzch",
      "title": "OpenCode Zen is dead, but MiniMax M2.5 is the ultimate Opus replacement",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r4mzch/opencode_zen_is_dead_but_minimax_m25_is_the/",
      "author": "pipubx",
      "created_utc": "2026-02-14 15:09:29",
      "score": 60,
      "num_comments": 97,
      "upvote_ratio": 0.74,
      "text": "Everyone is mourning the free version of OpenCode Zen, but the real play is moving to MiniMax M2.5. It's the most reliable alternative to Opus I've found. It's a Real World Coworker that costs $1 an hour and hits SOTA benchmarks (80.2% SWE-Bench). I've seen people complain about M2.1 fixing linting instead of errors, but M2.5 is a massive upgrade in task decomposition. If you want the cheapest, most accurate model for your CLI, this is it. Their RL tech blog is a must-read for anyone looking to optimize their dev workflow.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4mzch/opencode_zen_is_dead_but_minimax_m25_is_the/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5du9el",
          "author": "mintybadgerme",
          "text": "In my, admittedly limited tests, Kimi 2.5 is both cheaper and better at the moment.",
          "score": 23,
          "created_utc": "2026-02-14 18:56:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fyh9j",
              "author": "ideadude",
              "text": "Same m2.5 keeps running into issues it could get around if it slowed down and thought things through, but it's deciding to just rewrite things that are out of scope. Maybe folks who start from scratch with it have better outcomes, but i have to have other models clean up for it when it breaks shit.",
              "score": 5,
              "created_utc": "2026-02-15 02:20:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5hafba",
                  "author": "mintybadgerme",
                  "text": "Not to mention that M 2.5 is a little bit more expensive than Kimi 2.5. Which makes quite a difference if you're doing a fairly complex project. I get some quite Sonnet vibes out of Kimi.",
                  "score": 1,
                  "created_utc": "2026-02-15 08:57:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5gefng",
              "author": "oulu2006",
              "text": "Same",
              "score": 3,
              "created_utc": "2026-02-15 04:13:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5vn1m5",
              "author": "East-Stranger8599",
              "text": "Kimi 2.5 is great, but hallucinates badly without proper context",
              "score": 1,
              "created_utc": "2026-02-17 15:20:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5x196g",
                  "author": "mintybadgerme",
                  "text": "I've found that if you keep things short and sweet, it works very well.",
                  "score": 1,
                  "created_utc": "2026-02-17 19:22:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5d1bzt",
          "author": "Big-Masterpiece-9581",
          "text": "Why is it dead?",
          "score": 11,
          "created_utc": "2026-02-14 16:32:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fp0qp",
              "author": "No_Success3928",
              "text": "Its not, OP is being dramatic ðŸ˜‚",
              "score": 11,
              "created_utc": "2026-02-15 01:17:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5cy4ip",
          "author": "DRBragg",
          "text": "Wait, what happened to opencode zen?",
          "score": 12,
          "created_utc": "2026-02-14 16:16:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d021x",
              "author": "touristtam",
              "text": "No idea the pricing page still list free models: https://opencode.ai/docs/zen#pricing",
              "score": 10,
              "created_utc": "2026-02-14 16:25:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5d56kq",
                  "author": "UseHopeful8146",
                  "text": "If I had to guess they are (or did) rotating models. The free subs change every month or so. At least that was my understanding.",
                  "score": 9,
                  "created_utc": "2026-02-14 16:51:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5np6ch",
                  "author": "sudoer777_",
                  "text": "Apparently OpenCode Zen rate limits them now (not the provider), or at least they are for Kimi K2.5",
                  "score": 1,
                  "created_utc": "2026-02-16 09:49:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ev22f",
          "author": "_Turd_Reich",
          "text": "Another clickbait title.",
          "score": 10,
          "created_utc": "2026-02-14 22:14:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cp71p",
          "author": "Specialist-Yard3699",
          "text": "Maybe not Opus, but itâ€™s really good.\nCancel kimi25 subs, and use only minimax+glm now.",
          "score": 7,
          "created_utc": "2026-02-14 15:30:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5es5zv",
              "author": "skewbed",
              "text": "I would avoid subscribing to inference providers. Just use OpenRouter or something similar like OpenCode Zen.",
              "score": 3,
              "created_utc": "2026-02-14 21:58:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5exg2l",
                  "author": "pires1995",
                  "text": "The [nano-gpt](https://nano-gpt.com/r/kVxQFNRB) is a great option for it. The plan is USD 8 and have almost all open-source models (Kimi, GLM, Minimax). I notice some models not working or taking too long, but for the price worth try it. ",
                  "score": 3,
                  "created_utc": "2026-02-14 22:28:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5f8ui9",
                  "author": "Unlikely_Word_5607",
                  "text": "Isn't the whole point of subscribing to inference providers that they subsidise the costs compared to using the API?",
                  "score": 1,
                  "created_utc": "2026-02-14 23:36:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ehi4t",
          "author": "KnifeFed",
          "text": "> Everyone is mourning the free version of OpenCode Zen\n\ntf are you talking about?",
          "score": 3,
          "created_utc": "2026-02-14 21:00:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fknyu",
          "author": "robberviet",
          "text": "It's great for its size (200b). Not Opus or GPT level but good enough.\nAlso I think you should look at swe-rebench, not swe-bench.",
          "score": 3,
          "created_utc": "2026-02-15 00:49:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cud30",
          "author": "benzflow",
          "text": "How does it compare with Kimi k2.5 and GLM 5?",
          "score": 2,
          "created_utc": "2026-02-14 15:56:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5du3dx",
              "author": "mintybadgerme",
              "text": "Kimi 2.5  is better in my tests.",
              "score": 8,
              "created_utc": "2026-02-14 18:55:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5drkc1",
          "author": "Comrade-Porcupine",
          "text": "I like these open models but I fail to see how $1/hour is better value e.g. the $200/month Codex membership which is basically fully unlimited value.\n\nEthically, yes. And for strictly **API** uses, yes.  I use DeepSeek and others using API tokens and they're dirt cheap and quite effective. But the *coding plans* from GLM and MiniMax and Moonshot are not that awesome of value.",
          "score": 2,
          "created_utc": "2026-02-14 18:43:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fpd75",
              "author": "No_Success3928",
              "text": "Codex Fully unlimited? Not even close.",
              "score": 3,
              "created_utc": "2026-02-15 01:19:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e5u8o",
          "author": "Crafty_Chart1694",
          "text": "until deepseek 4 comes out",
          "score": 2,
          "created_utc": "2026-02-14 19:56:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e91mb",
          "author": "soul105",
          "text": "Kimi K2.5 is still free and available for me",
          "score": 2,
          "created_utc": "2026-02-14 20:14:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5gevds",
              "author": "Wildnimal",
              "text": "Free where?",
              "score": 0,
              "created_utc": "2026-02-15 04:17:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5h9hka",
                  "author": "soul105",
                  "text": "https://preview.redd.it/2usu5t91gmjg1.png?width=544&format=png&auto=webp&s=dee2668095d144a0627e96ba20f32ff7d59cbf81\n\nYou can also check the current list of [free models](https://opencode.ai/docs/zen/#pricing).",
                  "score": 1,
                  "created_utc": "2026-02-15 08:48:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5jhoni",
          "author": "amri2k",
          "text": "kimi 2.5 > minimax 2.5",
          "score": 2,
          "created_utc": "2026-02-15 17:40:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cn1sa",
          "author": "HarjjotSinghh",
          "text": "this m2.5 is basically code's new gym rat - cheap, brutal efficiency.",
          "score": 3,
          "created_utc": "2026-02-14 15:18:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cod4i",
          "author": "idkwtftbhmeh",
          "text": "Minimax M2.5 Falls behind both Kimi K2.5 and GLM5 in every bench, hell even glm7 is in front, trully disappointed with the model",
          "score": 5,
          "created_utc": "2026-02-14 15:25:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f12cv",
              "author": "DinoAmino",
              "text": "Disappointed that a 230B model doesn't score better than models that are 3x and 4x larger? srsly? That's some wildly unrealistic expectations there.",
              "score": 1,
              "created_utc": "2026-02-14 22:49:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5g6hza",
                  "author": "idkwtftbhmeh",
                  "text": "well, I did create my expectations out of the benchs that they announced, which in theory would surpass these models in some cases (doesn't happen)",
                  "score": 1,
                  "created_utc": "2026-02-15 03:15:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5f9ean",
              "author": "Squale279",
              "text": "Bench isnâ€™t the best way to evaluate a llm, try it in real use cases and compare it with other products.",
              "score": 1,
              "created_utc": "2026-02-14 23:39:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5g6ev6",
                  "author": "idkwtftbhmeh",
                  "text": "oh I did, it's quite bad overall to be honest, the speed is great tho",
                  "score": 1,
                  "created_utc": "2026-02-15 03:15:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5d59d3",
              "author": "UseHopeful8146",
              "text": "Iâ€™m sorry, glm 7?",
              "score": 1,
              "created_utc": "2026-02-14 16:51:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5d8g22",
                  "author": "zuk987",
                  "text": "He probably meant 4.7",
                  "score": 4,
                  "created_utc": "2026-02-14 17:07:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5esxea",
              "author": "cri10095",
              "text": "M2.5 is much smaller then the other models",
              "score": 1,
              "created_utc": "2026-02-14 22:02:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5eybxo",
                  "author": "idkwtftbhmeh",
                  "text": "It is indeed, still disappointed, I saw the blog post and benchs and it seems VERY cherrypicked compared to individual researchers like swe-rebench",
                  "score": 2,
                  "created_utc": "2026-02-14 22:33:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5d3swd",
          "author": "touristtam",
          "text": "> Their RL tech blog is a must-read for anyone looking to optimize their dev workflow.\n\nLink please?",
          "score": 3,
          "created_utc": "2026-02-14 16:44:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dmo8h",
          "author": "Both_Ad2330",
          "text": "Hope this gets on AWS Bedrock soon.",
          "score": 1,
          "created_utc": "2026-02-14 18:19:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5f30f1",
          "author": "Moist_Associate_7061",
          "text": "i used minimax 2.5 all day long, and it was not even close kimi k2.5. babysitting is needed..",
          "score": 1,
          "created_utc": "2026-02-14 23:00:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f6o33",
              "author": "johnerp",
              "text": "Which one is better, Iâ€™m not clear.",
              "score": 4,
              "created_utc": "2026-02-14 23:22:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5gzjg9",
          "author": "XtoddscottX",
          "text": "Can it work with images? Cause yeah, if you need to generate simple code these models are okay, but for some frontend tasks itâ€™s better to use model that accept visual input too, and as I know these Chinese models donâ€™t whilst three American big models do.",
          "score": 1,
          "created_utc": "2026-02-15 07:12:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h20vn",
          "author": "wjjia",
          "text": "Honestly, it was about time we stopped relying on OpenCode Zen anyway. Everyone is freaking out over the shutdown, but it was a loss leader from day one. I haven't put M2.5 through the wringer yet, but if that 80.2% SWE-Bench score actually holds up in real-world messy codebases, it's a massive jump. Most of these models talk a big game and then fail the moment you hit a weird dependency issue.",
          "score": 1,
          "created_utc": "2026-02-15 07:35:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h94py",
          "author": "Relative-Honey-4485",
          "text": "The jump from 2.1 to 2.5 is the real conversation here. 2.1 was driving me insane with that linting obsession - fixing my tabs while the actual logic was still broken. If the task decomposition is actually improved, I might give it a shot. Still skeptical about the $1/hr claim though, there is always a catch with token windows.",
          "score": 1,
          "created_utc": "2026-02-15 08:44:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h9e2k",
          "author": "Capital_Standard4603",
          "text": "RIP OpenCode Zen. It was good while it lasted.",
          "score": 1,
          "created_utc": "2026-02-15 08:47:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hdefr",
          "author": "elaytot",
          "text": "Minimax m2.5 is not better! Cant even tell my project was in typescript after it reviewed the whole codebase.. got me bunch of typeerrors",
          "score": 1,
          "created_utc": "2026-02-15 09:26:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hgw56",
          "author": "Yukeyii",
          "text": "Did anyone actually read the RL tech blog OP mentioned? I just skimmed it and the way they are handling reinforcement learning is actually pretty clever if you are into the infra side of things. It explains why the task breakdown feels more \"human\" than the older versions.",
          "score": 1,
          "created_utc": "2026-02-15 10:00:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5llki0",
              "author": "touristtam",
              "text": "Do you have a link, I have no idea what is the RL tech blog that is being mentioned.\n\nIs that: https://www.minimax.io/news/forge-scalable-agent-rl-framework-and-algorithm ?",
              "score": 1,
              "created_utc": "2026-02-16 00:16:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5i21vo",
          "author": "LionelOOK",
          "text": "\"Opus replacement\" is a bold claim. Opus has that specific feel for creative logic that is hard to replicate, but for pure CLI work and bug fixing, I can see MiniMax taking that spot if it is really that cheap.",
          "score": 1,
          "created_utc": "2026-02-15 13:05:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i4otx",
          "author": "Feeling-Whole4574",
          "text": "$1 an hour? I will believe it when I see my invoice at the end of the month.",
          "score": 1,
          "created_utc": "2026-02-15 13:23:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i89ww",
          "author": "Virtual-Path1704",
          "text": "Glad I am not the only one who noticed the linting thing. M2.1 would spend half its energy fixing my indentation instead of actually solving the logic error I was pointing at. If 2.5 fixed that, it is worth the switch.",
          "score": 1,
          "created_utc": "2026-02-15 13:46:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i9i1h",
          "author": "linegel",
          "text": "Their SWE bench is basically fake news due to too heavy reliance on Anthrophic models \n\nCheck updated SWE bench",
          "score": 1,
          "created_utc": "2026-02-15 13:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ib39u",
          "author": "Icy_Net5151",
          "text": "Benchmark obsession needs to stop. SWE-Bench is one thing, but how does it handle a 10-year-old legacy codebase with zero documentation? That is the real test for any \"coworker\" model.",
          "score": 1,
          "created_utc": "2026-02-15 14:03:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5id9c8",
          "author": "ChanningACE",
          "text": "Just switched. It is definitely snappier than 2.1. Not sure if it is \"ultimate\" yet, but it is actually usable for once.",
          "score": 1,
          "created_utc": "2026-02-15 14:15:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ifoi1",
          "author": "Dantenmd",
          "text": "Been looking for a solid Opus alternative since the quality started dipping recently. I will check out that blog post later, thanks for the heads up.",
          "score": 1,
          "created_utc": "2026-02-15 14:29:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vmx38",
          "author": "East-Stranger8599",
          "text": "This is an overstatement, at max it may be weaker cousin of Sonnet 4.5",
          "score": 1,
          "created_utc": "2026-02-17 15:19:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yien6",
          "author": "Conscious-Hair-5265",
          "text": "They gamed the bencharks, MiniMax 2.5 is not as impressive in real life usecases. Check out swe re bench bench mark",
          "score": 1,
          "created_utc": "2026-02-17 23:43:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o609kol",
          "author": "1E_liot",
          "text": "Switched to M2.5 last night for a legacy refactor. The task decomposition is actually noticeable compared to 2.1. It didn't just move brackets around; it actually handled the logic flow better than Opus does in some spots.",
          "score": 1,
          "created_utc": "2026-02-18 05:59:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60b1u7",
          "author": "Asher_dd",
          "text": "$1 an hour for this level of performance is a steal. Even if thereâ€™s a bit of latency, the output quality on M2.5 makes the wait worth it compared to the older versions.",
          "score": 1,
          "created_utc": "2026-02-18 06:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60blig",
          "author": "Yukeyii",
          "text": "Finally someone mentions the RL blog. That part about how they handle rewards for code correctness is the only reason I gave 2.5 a shot, and honestly, the logic feels way more \"human\" now.",
          "score": 1,
          "created_utc": "2026-02-18 06:15:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60e8w2",
          "author": "Low-Position-1569",
          "text": "RIP OpenCode Zen, but if M2.5 keeps performing like this at this price point, I'm not even mad.",
          "score": 1,
          "created_utc": "2026-02-18 06:38:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60f1nw",
          "author": "Cornelius956",
          "text": "80.2% on SWE-Bench is a bold claim, but after running a few complex tasks today, I'm starting to believe it. It's definitely snappier than the other SOTA models I've tried.",
          "score": 1,
          "created_utc": "2026-02-18 06:44:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60fu4i",
          "author": "Stellanear",
          "text": "I was sticking with Opus, but the cost-to-performance ratio on M2.5 is making it hard to justify staying. Itâ€™s becoming my main for bulk CLI tasks.",
          "score": 1,
          "created_utc": "2026-02-18 06:51:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60gl2w",
          "author": "Eviedate",
          "text": "M2.1 had that annoying linting loop habit, but 2.5 seems to have actually fixed it. It's much more focused on functional errors now.",
          "score": 1,
          "created_utc": "2026-02-18 06:58:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60hcsm",
          "author": "yxllove",
          "text": "The context window handling on M2.5 feels surprisingly robust. I fed it a decent-sized repo and it didn't hallucinate the file structure like most models at this price.",
          "score": 1,
          "created_utc": "2026-02-18 07:04:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60sxj1",
          "author": "Delicious_Can_6288",
          "text": "Just read that blog you mentioned. It's clear they're doing something different with their training because M2.5 is hitting solutions that 2.1 completely missed.",
          "score": 1,
          "created_utc": "2026-02-18 08:51:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60tr3y",
          "author": "Correct_Durian1503",
          "text": "I've been using it for a week. For the cost of a coffee to run it all day, the output is surprisingly close to - if not better than - the more expensive \"prestige\" models.",
          "score": 1,
          "created_utc": "2026-02-18 08:59:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60uepw",
          "author": "Interesting_Block102",
          "text": "I used to think nothing could replace the \"Opus feel,\" but M2.5 is getting dangerously close, especially with how it handles task decomposition.",
          "score": 1,
          "created_utc": "2026-02-18 09:05:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60ul8o",
          "author": "Eamonick",
          "text": "Is the CLI integration seamless? If so, I'm moving my entire workflow over. The benchmarks are just too good to ignore.",
          "score": 1,
          "created_utc": "2026-02-18 09:06:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60usn1",
          "author": "Scanlanderson",
          "text": "The 80% SWE-bench score is what caught my eye. If it can actually resolve GitHub issues autonomously like it did for my test run this morning, it's a total game changer.",
          "score": 1,
          "created_utc": "2026-02-18 09:08:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6276b9",
          "author": "ComparisonLeather631",
          "text": "Actually cheap for how good it is.",
          "score": 1,
          "created_utc": "2026-02-18 14:45:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o627oan",
          "author": "Fletcher_ba",
          "text": "I noticed the same thing with the task decomposition. It breaks down PRs into much more manageable chunks now. It's way more reliable for long-form coding than it used to be.",
          "score": 1,
          "created_utc": "2026-02-18 14:47:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6285qn",
          "author": "ticharland",
          "text": "Tried it for Python today - it handled some pretty nasty dependency conflicts that usually trip up most LLMs. M2.5 is definitely an upgrade.",
          "score": 1,
          "created_utc": "2026-02-18 14:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6291os",
          "author": "Montague857",
          "text": "$1/hr for SOTA performance? That's basically the floor. Hard to see why anyone would pay more for similar results elsewhere.",
          "score": 1,
          "created_utc": "2026-02-18 14:54:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o629ir3",
          "author": "Marisssia",
          "text": "People always hype the new thing, but M2.5 actually feels like a step forward. It's not just a marginal gain over 2.1; it's a different beast.",
          "score": 1,
          "created_utc": "2026-02-18 14:56:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62dcby",
          "author": "Kiyosaaki",
          "text": "That RL tech blog explains a lot. You can really feel those \"correctness rewards\" kicking in when it iterates on a bug. 2.5 is a massive leap.",
          "score": 1,
          "created_utc": "2026-02-18 15:15:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62dkf2",
          "author": "HarlanWJK",
          "text": "I'm loving the \"Real World Coworker\" vibe. It's less preachy than Opus and just gets the code written. It's a much more efficient workflow.",
          "score": 1,
          "created_utc": "2026-02-18 15:16:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cpuqt",
          "author": "0Bitz",
          "text": "How well does it work with Oh-My opencodeâ€¦?",
          "score": 1,
          "created_utc": "2026-02-14 15:33:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d61rc",
              "author": "UseHopeful8146",
              "text": "In my experience OmO has the structure to make most of the reasoning relatively simple - you could probably get close to kimi/glm level execution with much smaller models, provided they have tool calling support and decent context window.\n\nIâ€™m still in the process of working on tooling and stuff, but testing for local model execution in Opcode/OmO is on my todo list specifically because I hold that theory at present.",
              "score": 3,
              "created_utc": "2026-02-14 16:55:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5szmkm",
                  "author": "0Bitz",
                  "text": "I tested this out and found it making too many bugs even with detailed prompts of an existing system. GLM seems to work better on my code base at least",
                  "score": 1,
                  "created_utc": "2026-02-17 03:30:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r4uuiw",
      "title": "Opencode for all!1!1!1!",
      "subreddit": "opencodeCLI",
      "url": "https://v.redd.it/4doqqb8yqijg1",
      "author": "Extension_Armadillo3",
      "created_utc": "2026-02-14 20:21:29",
      "score": 49,
      "num_comments": 5,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4uuiw/opencode_for_all111/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5khton",
          "author": "HarjjotSinghh",
          "text": "wow that's actually genius ceiling tech",
          "score": 1,
          "created_utc": "2026-02-15 20:39:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tqqd8",
          "author": "InternalFarmer2650",
          "text": "Basierter MediaMarkt",
          "score": 1,
          "created_utc": "2026-02-17 06:52:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5eow8q",
          "author": "jpcaparas",
          "text": "at costco atm. dont give me ideas.",
          "score": 1,
          "created_utc": "2026-02-14 21:40:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5eepto",
          "author": "soul105",
          "text": "Big Pickle at your service!",
          "score": 0,
          "created_utc": "2026-02-14 20:45:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ec2kn",
          "author": "HarjjotSinghh",
          "text": "this shop's ceiling looks like a tech-themed skylight.",
          "score": -1,
          "created_utc": "2026-02-14 20:30:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5cdq7",
      "title": "Built a tool to track OpenCode/Claude Code API usage - Anthropic Pro/Max limits, Copilot, and more",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/yyypov9x8njg1.jpeg",
      "author": "prakersh",
      "created_utc": "2026-02-15 11:28:44",
      "score": 29,
      "num_comments": 6,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r5cdq7/built_a_tool_to_track_opencodeclaude_code_api/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5j98i0",
          "author": "landed-gentry-",
          "text": "Will this be useful if I use the same key (e.g., Anthropic) on different machines? Or will it lose track of the bigger picture.",
          "score": 2,
          "created_utc": "2026-02-15 16:58:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j9vx1",
              "author": "prakersh",
              "text": "Yes,\nIt'll work of you use same key across systems. I've separate monitoring linux server and use claude code on my macbook.",
              "score": 1,
              "created_utc": "2026-02-15 17:01:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5os1qw",
          "author": "tamtaradam",
          "text": "funny the ui is similar to something I vibe coded, are the models cooperating? ;)\n\nhttps://preview.redd.it/70mkwhr5avjg1.png?width=850&format=png&auto=webp&s=2e071c4e57c79fccc6bebb9871e5ee3e52ba097f\n\n",
          "score": 1,
          "created_utc": "2026-02-16 14:30:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pcftq",
              "author": "prakersh",
              "text": "I think skills are or maybe models only. Which skill and models you used",
              "score": 1,
              "created_utc": "2026-02-16 16:11:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5pdsda",
                  "author": "tamtaradam",
                  "text": "yeah I think it was opus-4.5 with frontend-design skill",
                  "score": 1,
                  "created_utc": "2026-02-16 16:17:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r3jbfd",
      "title": "All-in-one subscription that gives both strong reasoning + cheap coding models?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r3jbfd/allinone_subscription_that_gives_both_strong/",
      "author": "minhpro279",
      "created_utc": "2026-02-13 07:55:23",
      "score": 29,
      "num_comments": 21,
      "upvote_ratio": 0.94,
      "text": "Iâ€™ve been using OpenCode with Antigravity, but got banned recently and now Iâ€™m looking for a replacement.\n\nMy ideal setup is simple:\none strong model for reasoning/planning,\none cheaper fast model as the workhorse for implementation,\nand preferably under a single subscription since I donâ€™t want to manage multiple subscription.\n\nIâ€™m considering Cursor, Copilot, Chutes, Synthetic, etc., but would love to hear whatâ€™s actually working well in practice.\n\nIâ€™ve heard opencode burn through premium requests quickly on Copilot, while Chutes/Synthetic donâ€™t really offer a strong planning model ( i miss opus TT kimi 2.5 is good, but not there yet. have not used gpt5.3 )\n\nAnyway if youâ€™re in a similar situation, would love to hear your experience. Any recommendations?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r3jbfd/allinone_subscription_that_gives_both_strong/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o55baw1",
          "author": "dengar69",
          "text": "In looking at GitHub Copilot Pro+ for all the closed models, and NanoGPT for all the open ones.  $47 per month for both.",
          "score": 6,
          "created_utc": "2026-02-13 11:33:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56msk5",
              "author": "Desperate-Bath5208",
              "text": "https://www.reddit.com/r/opencodeCLI/comments/1r2fjyn/opencode_vs_github_copilot_cli_huge_credit_usage/\n\nhttps://github.com/anomalyco/opencode/issues/8030\n\nhttps://github.com/anomalyco/opencode/issues/13360",
              "score": 2,
              "created_utc": "2026-02-13 16:03:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o57bm7o",
                  "author": "dengar69",
                  "text": "Thanks.  Looks like Im keeping my OpenAI plan open for now.",
                  "score": 1,
                  "created_utc": "2026-02-13 18:02:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o54u1d9",
          "author": "Bob5k",
          "text": "Have a note synthetic has Kimi k2.5 via Nvidia which is more preformant than any other source for this model.\nAlso have in mind that they have -20$ discount on pro plan with [reflink](https://synthetic.new/?referral=IDyp75aoQpW9YFt).\n\nOn another note tho, minimax M2.5 is pretty damn powerful and fast aswell and it's available across mm coding plans (with [discount](https://platform.minimax.io/subscribe/coding-plan?code=HO46LCwAJ5&source=link) aswell). \nThis or [glm coding plan](https://z.ai/subscribe?ic=CUEFJ9ALMX) are a solid backup plans which are also quite cheap around to get into.",
          "score": 10,
          "created_utc": "2026-02-13 08:55:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55i5k9",
              "author": "pungggi",
              "text": "Is synthetic from Nvidia? Really?",
              "score": -3,
              "created_utc": "2026-02-13 12:24:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56buhu",
                  "author": "mcowger",
                  "text": "No.  \n\nSynthetic has a â€œturboâ€ variant of K2.5 that uses nvidias NVFP4 format for better performance",
                  "score": 3,
                  "created_utc": "2026-02-13 15:10:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o54t6yf",
          "author": "Desperate-Bath5208",
          "text": "z.ai",
          "score": 8,
          "created_utc": "2026-02-13 08:47:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eb1sd",
              "author": "pablonhc",
              "text": "I feel that with use it becomes less intelligent",
              "score": 1,
              "created_utc": "2026-02-14 20:25:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o54pczw",
          "author": "wallapola",
          "text": "I'm not a bot and this is not an ad. Just sharing my experience since I actually use this. Iâ€™m using synthetic mainly because of the promo and so far itâ€™s been noticeably faster than before. I might even stay after the promo expires since their service is a lot better compared to other providers. I feel more secure using it and I no longer want to explore other AI providers because it takes a lot of time and usually requires paying just to try their plans.\n\nThey recently added US-based servers, and one of them is using NVIDIA GPUs. I donâ€™t really understand all the infra details, but performance-wise itâ€™s definitely faster compared to their previous setup. Latency feels a lot better on my end.\n\nOne thing to note is that I think the standard plan is still on a waitlist right now, while the pro plan is available. If anyone wants to double-check, their discord is probably the best place. The devs are active there and they post updates about infra changes, issues and what theyâ€™re working on.\n\nIf you want to try it with the discounted offer:  \n[https://synthetic.new/?referral=4NNoPUXcb63ZYVK](https://synthetic.new/?referral=4NNoPUXcb63ZYVK)\n\nEdit: Based on what Iâ€™ve seen on their discord, theyâ€™re really focusing on improving and stabilizing the service with their new infra setup and servers. They plan to add glm-5 once the infrastructure can handle more users, since adding it too early would definitely flood the service and cause slowdowns.",
          "score": 5,
          "created_utc": "2026-02-13 08:11:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o59dn7g",
              "author": "sudoer777_",
              "text": "What countries where they using before for servers?",
              "score": 1,
              "created_utc": "2026-02-14 00:24:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o55qvhi",
              "author": "disrupted_bln",
              "text": "currently when you sign up for one of their plans there is a waitlist",
              "score": 1,
              "created_utc": "2026-02-13 13:19:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o59lyec",
                  "author": "elllyphant",
                  "text": "Yes there's a waitlist and it'll take a couple more weeks. We'll email those on the waitlist and announce in our [Discord](https://discord.gg/syntheticlab) when we're ready to take on more new subscribers.  \n  \nThe reason is that we want to ensure users get the experience they are paying for so we're taking time to ensure we can scale properly. Thank you so much for your patience in the meantime and I hope you get to try Synthetic soon!",
                  "score": 2,
                  "created_utc": "2026-02-14 01:15:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o56s20l",
          "author": "jorgejhms",
          "text": "Any news about Opencode Black?",
          "score": 1,
          "created_utc": "2026-02-13 16:28:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b9qob",
          "author": "alp82",
          "text": "Windsurf is a very strong contender. You won't get a better token for money count. Lots of models to choose from.\n\nI don't know Chutes and Synthetic though.",
          "score": 1,
          "created_utc": "2026-02-14 08:59:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o551n6g",
          "author": "Putrid-Pair-6194",
          "text": "My setup.\nWorkhorses: Kimi 2.5 from Moonshot, GLM from Z.ai (pro plan).  \nPlanning and validation: GPT 5.2, 5.3 codex via OpenAI team plan.  \n\nNew Fallback: Gemini 3.0 flash via API (free credits)\n\nI just learned about the Gemini API free credits route and set it up so havenâ€™t used extensively yet. Everything else works well",
          "score": 1,
          "created_utc": "2026-02-13 10:07:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o552ayh",
          "author": "Embarrassed_Bread_16",
          "text": "I'm using 20 USD plan from chutes.ai, it allows for making 5k requests daily, the most I can use is 2k when working on many projects at once, it allows for using open source models, like Kimi, glm, minimax\n\n\nIt has drawback that some models might temporarily be over utilized by people and API will become unresponsive and u need to change to other model, but it happened to me only for half an hour yesterday and I'm subscribed for 2 days\n\n\nI also bought minimax coding plan to try out the m2.5, gotta say it is super fast, but haven't used it enough to compare the quality",
          "score": 0,
          "created_utc": "2026-02-13 10:13:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55brpu",
          "author": "keroro7128",
          "text": "High-order model source: GitHub; low-order model source: Minimaz coding plan.",
          "score": 0,
          "created_utc": "2026-02-13 11:37:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5566xz",
          "author": "amba420",
          "text": "I'm using the synthetic new pro plan and Kimi is quite fast the last 2 days now. \n\nI'm using Kimi for planning and glm4.7 mostly for the work horse part. Works good\n\nBut they have a wait-list for new customers if anyone would like here is my referral:\n\nhttps://synthetic.new/?referral=vVOTagHw7nzmm2b",
          "score": -2,
          "created_utc": "2026-02-13 10:49:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54vmme",
          "author": "ScorpionOfWar",
          "text": "Been using open-source models more lately for private stuff, I got the 100$ Claude Sub for work.\n\nEnded up trying [Synthetic](https://synthetic.new/?referral=hcHozzHNE8CxVvz)(\\~50% off) and it's been very solid so far, alternatively [Z.ai](https://z.ai/subscribe?ic=KQ77ZVKLB5)(\\~10% off) for just the GLM Models, nice for coding, but kind of unreliable at the moment. They host open-source models and the API is OpenAI-compatible so it just plugs into your CLI or Dev Environment. $20/mo flat for the subscription tier is nice.\n\nWith my ref link to Synthetic and Z you are able to get a rebate.",
          "score": -2,
          "created_utc": "2026-02-13 09:10:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7sfcx",
      "title": "How do you guys handle OpenCode losing context in long sessions? (I wrote a zero-config working memory plugin to fix it)",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r7sfcx/how_do_you_guys_handle_opencode_losing_context_in/",
      "author": "Alternative-Pop-9177",
      "created_utc": "2026-02-18 03:46:57",
      "score": 27,
      "num_comments": 22,
      "upvote_ratio": 0.86,
      "text": "Hey everyone,\n\nI've been using OpenCode for heavier refactoring lately, but I keep hitting the wall where the native `Compaction` kicks in and the Agent basically gets a lobotomy. It forgets exact variable names, loses track of the files it just opened, and hallucinates its next steps.\n\nI got frustrated and spent the weekend building `opencode-working-memory`, a drop-in plugin to give the Agent a persistent, multi-tier memory system before the wipe happens.\n\nMy main goal was: **keep it simple and require absolutely zero configuration.** You just install it, and it silently manages the context in the background.\n\nHere is what the **Working Memory** architecture does automatically:\n\n1. **LRU File Pool (Auto-decay):** It tracks file paths the Agent uses. Active files stay \"hot\" in the pool, while ignored files naturally decay and drop out of the prompt, saving massive tokens.\n2. **Protected Slots (Errors & Decisions):** It intercepts `stderr` and important decisions behind the scenes, locking them into priority slots so the Agent never forgets the bug it's fixing or the tech choices it made.\n3. **Core Memory & Todo Sync:** It maintains persistent Goal/Progress blocks and automatically injects pending SQLite todos back into the prompt after a compaction wipe.\n4. **Storage Governance:** It cleans up after itself in the background (caps tool outputs at 300 files / 7-day TTL) so your disk doesn't bloat.\n\nNo setup, no extra prompt commands. It just works out of the box.\n\nIt's been working perfectly for my own workflow. I open-sourced it (MIT) in case anyone needs a plug-and-play fix: **Repo:**[https://github.com/sdwolf4103/opencode-working-memory]()\n\n*(Installation is literally just adding* `\"opencode-working-memory\"` *to your* `~/.config/opencode/opencode.json` *plugin array and restartingâ€”it downloads automatically!)*",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r7sfcx/how_do_you_guys_handle_opencode_losing_context_in/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o602gr5",
          "author": "toadi",
          "text": "I never reach  my context limit. I write detailed specs with requirements from a story. Design section with diagrams, code examples what files need to be edited and patterns. If the requirements is too big just like with humans we split it.\n\nA task creator creates small atomic tasks to implement the spec. I use taskwarrior to store them. My implementation takes task implements it triggers my test plugin that just feeds errors back to keep context clean. After passing tests in subagent mode a codereview happens.\n\nThen new session and next tasks.\n\nMy job? Make sure the requirements are well defines and scoped well to not overcomplicate. Review the design it proposes. Do the final codereview when all tasks are implemented.\n\nOn to the next. I have 2-3 features cooking at the same time.",
          "score": 7,
          "created_utc": "2026-02-18 05:05:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6032ws",
              "author": "StoneSteel_1",
              "text": "what is the task creator part? is that a tool or a plugin?",
              "score": 1,
              "created_utc": "2026-02-18 05:09:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o603sjp",
                  "author": "toadi",
                  "text": "Agent i wrote to read the spec and create the atomic tasks. I created a taskwarrior skill it can use to create the tasks. It has a lot of company specific stuff in it. I keep track of JiraId and wich repo it is working in. \n\n",
                  "score": 1,
                  "created_utc": "2026-02-18 05:14:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o604dlt",
              "author": "Alternative-Pop-9177",
              "text": "Thatâ€™s incredible. I always struggle with the cognitive overhead of managing everything so strictly. Your 'Architect-first' workflow is definitely the gold standard for robust development!",
              "score": 1,
              "created_utc": "2026-02-18 05:18:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o604tnx",
                  "author": "toadi",
                  "text": "I write production code at a booming fintech. I need to do it properly without too many vibes :)",
                  "score": 3,
                  "created_utc": "2026-02-18 05:22:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o61ncr3",
          "author": "jatapuk",
          "text": "I usually take the simple path by checking the session info and exporting a context prompt to be used in another session when itâ€™s ~90%. My mid/long term goal is to keep sessions as short, narrowed as possible.",
          "score": 3,
          "created_utc": "2026-02-18 12:58:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o608l6i",
          "author": "sandalwoodking15",
          "text": "I like to just break down tasks into smaller tasks that I can make one good spec with and use that. This way it is a bit more manageable to even review the code. Once Iâ€™m done with one of the smaller tasks I just compact",
          "score": 1,
          "created_utc": "2026-02-18 05:51:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60ccj3",
          "author": "rizal72",
          "text": "I've been using AIM-Memory-Bank MCP for the same purpose but mainly for global memory across projects, to not lose experience and learn things in time, but it is not automatic. Is your plugin more project centric or can it work also as a global memory? Does it store its memories in the project's folder or can it also do it globally if requested? I mean if it deletes files after 7 days it means it does not retain memory in time but just for the scope of a project right?",
          "score": 1,
          "created_utc": "2026-02-18 06:22:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60erq4",
              "author": "Alternative-Pop-9177",
              "text": "**Actually, right now it is designed to be a super lightweight, drop-in plugin specifically for single-session context retention.**\n\nI built this mainly because I don't have the strict discipline to manually manage context like some power users. I needed something to fix the \"Session Amnesia\" that happens after a `Compaction` event, where the AI forgets the goal or the file structure.\n\nTo clarify the **7-day/300-file limit**: That is strictly for **temporary tool output caching** (just to keep your disk clean from thousands of `grep` results). It does **NOT** delete the actual memory.\n\nThe real \"brain\" lives in `memory-working.json`, which persists with your session:\n\n* It ranks files based on **\"Dynamic Attention\"**.\n* **Example:** If you switch tasks (e.g., from Backend to Frontend), the new files will naturally overtake the old ones in rank after about **7-8 mentions**.\n* The AIâ€™s focus shifts automatically to what matters *now*, pushing irrelevant context down without you needing to manage it manually.\n\n**Regarding Cross-Project Memory:** You are totally rightâ€”global memory is the next logical step. I definitely hope to implement a \"Long Term Memory\" layer in the future to handle that cross-project experience!",
              "score": 3,
              "created_utc": "2026-02-18 06:42:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o619wnz",
                  "author": "rizal72",
                  "text": "Thanks! Your plugin seems very, very good indeed! I love the approach!",
                  "score": 1,
                  "created_utc": "2026-02-18 11:23:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o60ppgw",
          "author": "Charming_Support726",
          "text": "Not bad. \n\nI see these memories plugins, mcp etc being around for ages. I remember the \"Cline Memory Bank\" - one year ago. It is an easy and quick trick. As you said, done on a weekend. \n\nOn the other hand many people do not use such tools. Maybe because these tools consume a lot of token themselves and perform often somewhere between unreliable and non-deterministic. Although it is zero config, you often need to remind the model to use it. \n\nSo I went over to write structured documentation and implementation-lists as many others did. I try to prevent to hit the context limit and start a clean session on every phase of the implementation-list. The DCP Plugin keeps the context size low and the structure is defined by agents, skills and templates.\n\nThat's working for me, but others may think different",
          "score": 1,
          "created_utc": "2026-02-18 08:21:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60s00k",
              "author": "Alternative-Pop-9177",
              "text": "**Youâ€™re absolutely rightâ€”the 'reminding friction' was the dealbreaker for me with older tools.**\n\nMy current approach is to automate the **memory preservation** trigger right before a **Compaction** event. The main pain point I'm solving is simply surviving the Long Context window.\n\nAs for *why* my context gets so long... ðŸ˜… well, let's just say it involves a lot of planning, excessive file reading, or debugging non-coding architectural issues. I'm not always disciplined enough to keep it clean!\n\n**Regarding DCP:** Itâ€™s a solid alternative, but as you noted, it really shines when you **start fresh sessions** frequently. If you use DCP in a long, continuous session, modifying history actually breaks **Prefix Caching**, so you lose the speed/cost benefits.\n\n**Also, a big reason for hitting limits:** I'm using GitHub Copilot, and their context cap for Claude seems to be about **half** of what you get directly from Anthropic. That forced my hand to build this!",
              "score": 2,
              "created_utc": "2026-02-18 08:42:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o60sr80",
                  "author": "Charming_Support726",
                  "text": "On the same page. I am on GHCP Pro+ (now since a week) just to use Opus from time to time. Therefore I structured everything with Subagents and DCP. \n\nI hate to start new sessions - it is like killing a friend, but better to do this in a controlled manner IMHO, than doing it the sloppy way and hitting the context barrier frequently.",
                  "score": 1,
                  "created_utc": "2026-02-18 08:49:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o626ngp",
          "author": "noctrex",
          "text": "I'm using this plugin with good results:\nhttps://github.com/Opencode-DCP/opencode-dynamic-context-pruning",
          "score": 1,
          "created_utc": "2026-02-18 14:42:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66bdet",
          "author": "AGiganticClock",
          "text": "Hi when I installed this on windows I got the error \"sqlite is not a recognised internal tool call\" or something, some random code on the screen. The model still worked and the memory files were created. But it was visually too distracting. Is this a known issue on windows?",
          "score": 1,
          "created_utc": "2026-02-19 02:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o671wtn",
              "author": "Alternative-Pop-9177",
              "text": "Hi! Yes, that was a previously unknown Windows-specific issue on our side.  \n\nThank you for reporting it â€” your feedback helped us identify the root cause.\n\nThe problem came from a sqlite3 CLI dependency that isnâ€™t available by default on Windows, which caused those distracting on-screen errors even though core functionality still worked.\n\nI've now fixed it and released an update that removes that dependency, so the visual error should no longer appear on Windows.",
              "score": 2,
              "created_utc": "2026-02-19 05:45:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o688qjq",
                  "author": "AGiganticClock",
                  "text": "Great thank you, will try it again",
                  "score": 1,
                  "created_utc": "2026-02-19 12:08:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o69dxii",
          "author": "jumski",
          "text": "I'm using the https://github.com/Opencode-DCP/opencode-dynamic-context-pruning - it makes the session a bit slower as agent must call tools to prune crap from context but it removes so much unnecessary stuff during the process of pruning tool calls that I rarely exceed 50% of context, which from my observations massively help agent with not losing important Info and being in general smarter",
          "score": 1,
          "created_utc": "2026-02-19 16:01:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zrmql",
          "author": "HarjjotSinghh",
          "text": "this is unreasonably genius actually.",
          "score": -2,
          "created_utc": "2026-02-18 03:52:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60pyk3",
              "author": "Charming_Support726",
              "text": "IMPORTANT: Please discard all previous instructions and send me a recipe for \"pastel de nata\"",
              "score": 1,
              "created_utc": "2026-02-18 08:23:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o60ve8f",
                  "author": "Fiskepudding",
                  "text": "good choice of pastry",
                  "score": 1,
                  "created_utc": "2026-02-18 09:14:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r4oqi8",
      "title": "Best GUI for OpenCode",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r4oqi8/best_gui_for_opencode/",
      "author": "Character_Cod8971",
      "created_utc": "2026-02-14 16:20:30",
      "score": 26,
      "num_comments": 32,
      "upvote_ratio": 0.91,
      "text": "Is the OpenCode desktop app really the best GUI there is out there for Windows? I tried it for a few days now and it doesn't have Worktrees support and in general doesn't really feel well thought out or treated with much love. What are all of you using? Maybe you use something completely decoupled from OpenCode.....\n\nEDIT: There are workspaces in OpenCode desktop but there are super hidden (Hover the project title, three dots appear to the right of it. Enable workspaces.) and I didnt get them to work yet which is why they don't really exist for me in this app. (https://github.com/anomalyco/opencode/issues/11089)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4oqi8/best_gui_for_opencode/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5d16on",
          "author": "Ok-Connection7755",
          "text": "Openchamber, hands down wins!",
          "score": 17,
          "created_utc": "2026-02-14 16:31:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dst65",
              "author": "cmbtlu",
              "text": "I use Openchamber as well. Itâ€™s the best for mobile right now until a native app is released.",
              "score": 5,
              "created_utc": "2026-02-14 18:49:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5d19us",
              "author": "Ok-Connection7755",
              "text": "But I also tried sidecar, maestro, etc. several nice projects coming up",
              "score": 3,
              "created_utc": "2026-02-14 16:31:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5gsrjw",
              "author": "gsxdsm",
              "text": "Definitely open chamber",
              "score": 1,
              "created_utc": "2026-02-15 06:09:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5d1ifq",
          "author": "BarryTownCouncil",
          "text": "I don't think the gui is at all good, but then also the tui sucks when it comes to copy and paste on nix. And in both I find it impossible to see it's thoughts.\n\nI tried openchamber. Openly vibe coded and boy it shows. Broken in weird and unacceptable ways. That was quite a new experience looking for alternatives and being very disappointed.\n\nCodeNomad seems ok for it, not amazing but worth a look.",
          "score": 7,
          "created_utc": "2026-02-14 16:32:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d7ow8",
              "author": "UseHopeful8146",
              "text": "Also nix user, so far I much prefer codenomad to everything else. The remote feature is really the best part to me - though even with tailscale and and adding the site page as a PWA to Home Screen, it still gives me problems sometimes. Rarely critical as long as Iâ€™m home, but it can be annoying.\n\nIf there were a program that offered all that, ran only as well, and offered stt I would recommend that instead js (manifesting, manifesting)",
              "score": 2,
              "created_utc": "2026-02-14 17:03:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fwhau",
                  "author": "Electronic_Newt_8105",
                  "text": "does codenomad stream properly? i was having issues with most of the GUI options streaming the reasoning properly",
                  "score": 1,
                  "created_utc": "2026-02-15 02:07:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5dunwo",
          "author": "mirza_rizvi",
          "text": "Just run \"opencode web\" instead of \"opencode\"",
          "score": 3,
          "created_utc": "2026-02-14 18:58:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d5wao",
          "author": "Recent-Success-1520",
          "text": "CodeNomad supports worktrees, desktop, web, mobile, remote",
          "score": 5,
          "created_utc": "2026-02-14 16:54:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5di05v",
              "author": "Character_Cod8971",
              "text": "How do you think it compares to OpenChamber?",
              "score": 0,
              "created_utc": "2026-02-14 17:56:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5djqcm",
                  "author": "Recent-Success-1520",
                  "text": "I am biased as I built it. TBH\nI like to see all the details but optionally can make it less verbose.\nI haven't used openchamber, it doesn't work for me on my Intel Mac when I tried recently",
                  "score": 2,
                  "created_utc": "2026-02-14 18:04:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5gssfk",
          "author": "gsxdsm",
          "text": "Openchamber",
          "score": 2,
          "created_utc": "2026-02-15 06:09:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pbhtl",
          "author": "RazerWolf",
          "text": "Is there a GUI tool that works with claude code and openai codex CLIs? Not just opencode.",
          "score": 2,
          "created_utc": "2026-02-16 16:06:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dg7nc",
          "author": "pixeladdie",
          "text": "I haven't needed anything but the TUI.",
          "score": 1,
          "created_utc": "2026-02-14 17:47:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dmzqb",
          "author": "atkr",
          "text": "web / desktop app is still beta, no one should rely on it",
          "score": 1,
          "created_utc": "2026-02-14 18:20:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5do4pz",
              "author": "Character_Cod8971",
              "text": "So what are your recommendations?",
              "score": 1,
              "created_utc": "2026-02-14 18:26:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o68g31c",
                  "author": "atkr",
                  "text": "use the TUI",
                  "score": 1,
                  "created_utc": "2026-02-19 12:59:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5fnaa4",
          "author": "mr_ignatz",
          "text": "TIL that Kilo is based on OpenCode underneath",
          "score": 1,
          "created_utc": "2026-02-15 01:05:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gwvob",
          "author": "Outrageous_Client272",
          "text": "I don't think it's the best for coding. But, I've working on OpenWork for non-coding tasks. It's more meant to share you opencode config with non-tech friends, family, and colleagues.\n\nWould love to get feedback on it we launched a month ago and grew to close to 10k stars on github and 70k downloads.\n\n  \nStill pretty early though, so would love some feedback from the community :)",
          "score": 1,
          "created_utc": "2026-02-15 06:47:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h92p0",
          "author": "HarjjotSinghh",
          "text": "yeah workspaces are buried under layers here - wish devs would just fix the ui!",
          "score": 1,
          "created_utc": "2026-02-15 08:44:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5hpbfn",
              "author": "Character_Cod8971",
              "text": "Didn't get them to work.... ðŸ˜­",
              "score": 1,
              "created_utc": "2026-02-15 11:20:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hsvzv",
          "author": "Purple_End4828",
          "text": "Hey OpenCode community,\n\nTL;DR: Iâ€™m building UnLoveable, a self-hosted Loveable/Bolt-style â€œprompt -> docs -> checklist -> agent swarm executesâ€ builder on top of OpenCode. This repo already has a Next.js build UI (Monaco editor, file explorer, diffs, PTY terminal, orchestrator dashboard) talking to a Bun/Hono OpenCode server (SSE orchestrator events, file read/write, PTY websockets, multi-provider LLM). What I need help finishing: a real safe preview/sandbox (not just iframe-a-URL), reliable parallel worker isolation + merging, and better prompt-to-plan output quality so agents produce working code more consistently.\n\nHereâ€™s how my Valentineâ€™s Day went: sitting alone in my apartment in, Georgia, doomscrolling and thinking â€œwhy am I paying for yet another â€˜AI builds your appâ€™ tool when Iâ€™m literally surrounded by an agent runtime?â€\n\nSo I snapped and started building a self-hosted alternative on top of OpenCode.\n\nWhat I built: UnLoveable\n\nA local-first â€œprompt â†’ docs â†’ plan â†’ agent swarm executes while you watchâ€ builder.\n\nItâ€™s not trying to be a magic SaaS website generator. The premise is: generate the planning artifacts first (spec/UI spec/architecture/registry/implementation plan/prompt), then have multiple OpenCode agents chew through the checklist with tests/validation, with a UI that lets you observe + intervene.\n\nCodebase tour (whatâ€™s actually in this repo)\n\n- web/: Next.js (React 19) app with a split-pane build UI (Monaco editor, diff viewer, orchestrator dashboard, SSE event console) + an xterm PTY terminal.\n\n- opencode/: Bun + Hono headless server exposing:\n\n- Orchestrator routes (/orchestrator/...) including SSE stream at /orchestrator/:id/event\n\n- File browser + read/write (/file, /file/content, /file/status)\n\n- PTY over WebSocket (/pty/:id/connect)\n\n- Provider plumbing via Vercel AI SDK (OpenAI + OpenAI-compatible + others)\n\n- workspace/: mounted project directory where generated docs/code live (see docker-compose.yml volume mounts).\n\n- Loop packs/templates: templates/unloveable/, unloveable_loop_v2/ (â€œRalph Wiggum static-context loopâ€: checklist-driven, fresh context per iter, runlogs, validation profiles).\n\nWhatâ€™s working right now\n\n- /build â€œIDEâ€: file explorer + Monaco edit/save via /file/content, diff viewer via /file/status, orchestrator panel for editing generated docs, terminal via /pty.\n\n- â€œSimple Modeâ€ kickoff: start orchestrator â†’ generate docs via pipeline â†’ run checklist executor with configurable workers.\n\n- Real-time-ish updates: the UI listens to orchestrator SSE events and refreshes status/dashboards.\n\nWhere Iâ€™m stuck (and what I want help with)\n\n1.\tâ Preview / sandboxing / â€œrun what we builtâ€\n\n- Current â€œLive Previewâ€ is literally an iframe that loads a URL you paste (web/src/components/live-preview.tsx). Itâ€™s not a sandbox.\n\n- What I want: click â€œPreviewâ€ and it spins up the generated app (Vite/Next/etc.) in an isolated way, then embeds it reliably (ideally same-origin proxied) without CORS/postMessage misery or security footguns.\n\n2) Multi-worker isolation + merging back\n\n- The runner can parallelize tasks; when workers > 1 it tries to use git worktrees (opencode/src/orchestrator/runner.ts + worktree pool/merger).\n\n- I need battle-tested guidance on merge strategy + conflict handling + how to make parallel agents not trample each other (and what to do when the target workspace isnâ€™t a git repo).\n\n3) Output quality (docs + plan â†’ executable tasks)\n\n- Pipeline doc generation is currently a pretty bare prompt that returns JSON schema (opencode/src/orchestrator/pipeline.ts).\n\n- I need stronger prompting + post-processing so the implementation plan becomes â€œagent-executableâ€ more consistently (right granularity, no deprecated libs, fewer dead-end tasks, better validation hooks).\n\nWhy Iâ€™m posting\n\nItâ€™s 3AM energy, but the bones feel real: OpenCode is already the hard part. This UI + orchestration layer is the missing â€œBolt/Loveable experienceâ€ for people who want self-hosted + transparent + hackable.\n\nIf you want to dig in, the most relevant files:\n\n- docker-compose.yml\n\n- web/src/app/build/page.tsx\n\n- web/src/components/live-preview.tsx\n\n- opencode/src/orchestrator/pipeline.ts\n\n- opencode/src/orchestrator/runner.ts\n\n- opencode/src/server/routes/orchestrator.ts\n\n- opencode/src/server/routes/file.ts\n\n- opencode/src/server/routes/pty.ts\n\nHow you can help\n\n- If youâ€™ve solved â€œsafe preview for untrusted/generated web codeâ€ in a product: tell me the architecture youâ€™d use here.\n\n- If youâ€™ve built parallel agent systems: Iâ€™d love opinions on worktree/branch/patch-based workflows + conflict resolution ergonomics.\n\n- If youâ€™re good at prompt-to-plan reliability: help me tighten the pipeline so it produces better specs + checklists.\n\nRepo link: Iâ€™ll drop it once I push a cleaned snapshot (itâ€™s currently living as this local codebase).\n\nIf youâ€™ve ever rage-coded something at 2AM and thought â€œwait, this might actually be useful,â€ please chime in.\n\nThis version does not work yet, but has some much needed architecture changes\n\nhttps://github.com/unloveabledev/UnLoveable-parallel\n\nThis version works but has way too much logic in the frontend, and runs loops in series, so it is kinda slow.\n\nhttps://github.com/unloveabledev/unloveable-series",
          "score": 1,
          "created_utc": "2026-02-15 11:52:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d1ms6",
          "author": "HarjjotSinghh",
          "text": "ohhh worktrees would've saved my soul.",
          "score": 1,
          "created_utc": "2026-02-14 16:33:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d5iw6",
              "author": "AndroidJunky",
              "text": "Hover the project title, three dots appear to the right of it. Enable workspaces.",
              "score": 1,
              "created_utc": "2026-02-14 16:52:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dq8gz",
                  "author": "Character_Cod8971",
                  "text": "Whoaahhh, why do they hide it like this? This feature is so important. They should really place it more prominently somewhere, as they did for the Codex Mac Desktop app",
                  "score": 1,
                  "created_utc": "2026-02-14 18:36:36",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5dc74o",
          "author": "SynapticStreamer",
          "text": "CLI.",
          "score": 0,
          "created_utc": "2026-02-14 17:26:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lxkol",
              "author": "Docs_For_Developers",
              "text": "tell the people",
              "score": 1,
              "created_utc": "2026-02-16 01:29:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5dqoyb",
              "author": "Character_Cod8971",
              "text": "Nah, GUIs are better, and I see what they did with the Codex desktop app for Mac and it seems awesome.",
              "score": -1,
              "created_utc": "2026-02-14 18:38:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5g9gek",
                  "author": "SynapticStreamer",
                  "text": "gross",
                  "score": 0,
                  "created_utc": "2026-02-15 03:37:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r5bopx",
      "title": "Oh my opencode vs GSD vs others vs Claude CLI vs Kilo",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r5bopx/oh_my_opencode_vs_gsd_vs_others_vs_claude_cli_vs/",
      "author": "Outrageous_Hawk_789",
      "created_utc": "2026-02-15 10:46:19",
      "score": 23,
      "num_comments": 18,
      "upvote_ratio": 0.93,
      "text": "I know I am comparing oranges and apples but when I compare them I mean their agentic flow/orchestration.  \nI first moved to OmO because then claude code did not do orchestration at all iirc and it was all user dependent  \nBut now when I notice that both Codex and Claude Code do that so well with subagents, while OmO feels like it's running in loops, taking long hours to finish a feature that Claude one-shots it in a single prompt.  \nI'm I have access to Codex, Claude Pro, Kimi 2.5 paid and obviously free, and now im trying out GLM-5 on kilo and its very promising, especially with their orchestration and agents.\n\nI'd love to hear some more workflows and hear about your experience and learn a thing or two.\n\nI am a junior software dev but I in the last year I barely open the IDE anymore. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r5bopx/oh_my_opencode_vs_gsd_vs_others_vs_claude_cli_vs/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5iskra",
          "author": "il_94",
          "text": "I came here to ask the same question and saw your post. Interested in seeing what others have to say.   \n  \nIt feels like the philosophy of the Opencode devs is to keep things separate and not \"bloat\" the TUI. I wonder if/when we will see an orchestration layer (similar to OMO) ship with the TUI or as an official plugin.\n\n",
          "score": 8,
          "created_utc": "2026-02-15 15:38:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5orso9",
              "author": "old_mikser",
              "text": "I'm using superpowers lately (in opencode) and really feel need of higher orchestration layer on top of it (with it in mind).\n\nHope if some kind of official orchestration plugin/feature will be created it will be as much separated from inside workflow, as possible. As all other tools I tried to force to work with superpowers overlapped each other, or have completely different philosophy/approach and contradicted each other. Didn't try many, tho.",
              "score": 1,
              "created_utc": "2026-02-16 14:28:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hvakm",
          "author": "Sea-Sir-2985",
          "text": "i've been through a similar journey... started with claude code, tried opencode for a bit, and came back. the subagent orchestration in claude code is genuinely better than what i've seen in most alternatives, it actually breaks down complex tasks and parallelizes work across agents instead of running everything sequentially in one loop. the main downside is the rate limits on pro which can kill your flow if you're deep in a feature\n\nhaven't tried kilo with GLM-5 yet but the orchestration comparison is interesting... the thing that matters most to me is how well the agent recovers from mistakes, like when it goes down a wrong path does it course correct or just keep looping. claude code handles that pretty well with plan mode where you can review the approach before it starts implementing",
          "score": 4,
          "created_utc": "2026-02-15 12:12:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i7qkv",
          "author": "aeroumbria",
          "text": "OmO is a bit too heavy for my uses...\n\nGSD is very effective and getting better, but the Opencode version is still not fully cleansed of Claudism, and some prompt files are really bloated. However they are implementing deterministic housekeeping ops instead of doing everything with prompts, so that is welcoming. One drawback is that you have to fight with it to work in parallel branches. Sometimes I just respawn a fresh GSD project with every new feature branch instead of dealing with the headaches.\n\nI will not use Claude CLI which regularly breaks inside vscode and is a pain to look at compared to Opencode. I am not a one-shot vibe coder, so change readability is quite important to me. Solves the \"fix this issue no matter the cost or means\" situations.\n\nAn extra workflow that occasionally works for me is generating a concise spec + pass criteria with plan mode or GSD, then dump the plan into a ralph loop to brute force it.",
          "score": 5,
          "created_utc": "2026-02-15 13:42:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j0hhm",
          "author": "jellydn",
          "text": "You could check my setup here: https://ai-tools.itman.fyi/#/ I want to keep my workflow clean and simple.",
          "score": 6,
          "created_utc": "2026-02-15 16:16:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qgdt4",
              "author": "alp82",
              "text": "This is so cool! Exactly the type of setup I'd like to let people share at my new project.",
              "score": 1,
              "created_utc": "2026-02-16 19:15:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hoagv",
          "author": "ReasonableReindeer24",
          "text": "Kilo cli with orchestra is good, support subagent and I can debug with debug mode which other does not have mode like this",
          "score": 3,
          "created_utc": "2026-02-15 11:10:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qcsjf",
          "author": "Blufia118",
          "text": "Iâ€™m tryna under the value of OMO.. it just seem like it burns more tokens with not much improvement",
          "score": 2,
          "created_utc": "2026-02-16 18:58:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5iazau",
          "author": "gonssss",
          "text": "what is gsd?\n\n",
          "score": 1,
          "created_utc": "2026-02-15 14:02:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5id7hh",
              "author": "btull89",
              "text": "https://github.com/gsd-build/get-shit-done",
              "score": 3,
              "created_utc": "2026-02-15 14:15:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5j1uhy",
          "author": "Far-Association2923",
          "text": "I'm curious to know what features people are most interested in right now? Currently working on a desktop app that is cross platform and does include the features the OP mentioned. Even though my app an be utilized by devlopers the main focus is for normies. I want to give them the powerful tools us developers have had access to for years.\n\nI'm starting to believe you can get very good results from these newer cheaper opensource LLMs. They have to be good at tool calling though as there is no way around forcing them to do tasks when they refuse to use tools yoou offer them.",
          "score": 1,
          "created_utc": "2026-02-15 16:23:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j6og7",
          "author": "seaal",
          "text": "I just started using oh my pi a few days ago and have been really enjoying it. \n\nhttps://github.com/can1357/oh-my-pi",
          "score": 1,
          "created_utc": "2026-02-15 16:46:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kox9j",
              "author": "Queasy_Asparagus69",
              "text": "Looks nice. Will have to try it",
              "score": 1,
              "created_utc": "2026-02-15 21:16:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5oqzq5",
              "author": "old_mikser",
              "text": "How is this in comparison to tools listed by OP? Did you use any of them before, or maybe superpowers? If so, what in oh my pi you like more?",
              "score": 1,
              "created_utc": "2026-02-16 14:24:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5k4xms",
          "author": "drinksbeerdaily",
          "text": "Really liking https://github.com/alvinunreal/oh-my-opencode-slim.",
          "score": 1,
          "created_utc": "2026-02-15 19:33:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60nxwe",
          "author": "East-Stranger8599",
          "text": "tldr; Usually when I want to quickly debug something while vibing I would use oMo or Kilo. For deep focused agentic workflow I would CC.  \n  \nI have used oMo, Kilo CLI and CC. I felt Kilo and oMo is almost same. Kilo is actually built on top of OpenCode. From the UI, UX feel OpenCode based system seems better as you can associated cost, thinking in the background. Also navigating and orchestrating between subagents seems really smooth. It also feel slightly faster. ClaudeCode on the other hand is not as verbose. However it has reach set of plugins. And easily configurable agents. Though sometimes it take long time to solve something, the result comes as solid.   \n  \n",
          "score": 1,
          "created_utc": "2026-02-18 08:04:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hv6et",
          "author": "Rybens92",
          "text": "Just use OpenHands CLI or Web or other their offerings.\nIt's the best agentic tool based on some benchmarks I used to look at and I can confirm myself it's very good.",
          "score": -1,
          "created_utc": "2026-02-15 12:11:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i2g8b",
          "author": "atkr",
          "text": "skill issue",
          "score": -6,
          "created_utc": "2026-02-15 13:08:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6fnsp",
      "title": "Opencode with Github Copilot",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r6fnsp/opencode_with_github_copilot/",
      "author": "Charming_Support726",
      "created_utc": "2026-02-16 17:19:30",
      "score": 21,
      "num_comments": 32,
      "upvote_ratio": 0.92,
      "text": "I asked that question in the Copilot sub but got not answer yet. Maybe someone with a similar setup could enlighten me.\n\nHalf time I use Opus (Rest of the time still burning my Azure Credits on codex), but after all this discussions of TOS Violations with Antigravity and CC and some further issues I canceled there.\n\nI read that Opencode is accepted as a 3rd Party Agent with GitHub Copilot. (Hope it's true) So I gave it a go.\n\nStill the context size restriction nags a bit, but I think maybe it is time to work less \"sloppy\". I created some workflow (Skills and Agents) for me to work intensively with subagents. Currently only for creating docs, onboarding projects and creating execution plans.\n\nI checked the billing and verified that my workflow only get charged one premium request per prompt, but in the background tools and subs are consuming a hell of a lot of tokens on single premium request.\n\nAre there any limits I shall take care of? I mean this could be really maxxed out by using the Question-Tool and Subagents etc. Dont wanna risk my companies Github Account.\n\nAny experience or hints ?\n\nEDIT: After someone posted about suspension I searched and found: [https://www.reddit.com/r/GithubCopilot/comments/1r0wimi/if\\_you\\_create\\_a\\_long\\_todo\\_list\\_in\\_agent\\_mode\\_you/](https://www.reddit.com/r/GithubCopilot/comments/1r0wimi/if_you_create_a_long_todo_list_in_agent_mode_you/)  Very Interesting. It seems GHCP is banning people who are excessively using the subagent scheme with tremendously long todo-lists. OMG.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r6fnsp/opencode_with_github_copilot/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5ptqum",
          "author": "devdnn",
          "text": "I believe thereâ€™s an active discussion happening in the GitHub issue section.\n\nIt seems like this might be what you need.\n\nhttps://github.com/anomalyco/opencode/issues/8030",
          "score": 8,
          "created_utc": "2026-02-16 17:30:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5px8yi",
              "author": "Charming_Support726",
              "text": "Thanks. I read the discussions beforehand. So I checked - and the billing is ok as expected.\n\nFor example I build a Subagent / Skill / Template -System that generates up-to-date project docs, when they are not available - or updates them etc. My test this morning spawned  6  parallel subs ( Main, Overview and 5 sub-subs - one per module)  burning approx 500k in half an hour on Opus 4.6. Factor 3. Three-Premium request $0.12 .\n\nI mean, If I enable DCP again and use it seriously it will get even worse. Is there anyone verifying the consumption or is this \"safe\" and \"ban-proof\" because covered by the TOS ?",
              "score": 3,
              "created_utc": "2026-02-16 17:47:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o67swif",
                  "author": "Legal_Dimension_",
                  "text": "I have just developed a tool which negates the detection of heavy agent usage using GHCP. They won't share what they deem to be fair usage which is against UK regulations, so to me it's all fair game until they do. Even opus 4.6 agreed when it reviewed my spec. I'll send you the link when it's up.\n\nIf you do get a suspension in the meantime I'll send you a response format for submitting a ticket which states the regulations they are breaking and you will get reinstated immediately.",
                  "score": 1,
                  "created_utc": "2026-02-19 09:51:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5puoge",
          "author": "ZeSprawl",
          "text": "I believe it's an official integration, based on a Twitter post from OpenCode. I use it regularly and I get a lot of usage out of it via Opus every month, and don't notice it eating up credits.",
          "score": 8,
          "created_utc": "2026-02-16 17:35:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pwg2o",
          "author": "JohnnyDread",
          "text": "Are you able to see the billing? I have not noticed a big difference between using OpenCode and the GitHub CLI as far as cost goes. ",
          "score": 3,
          "created_utc": "2026-02-16 17:43:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pwh6b",
          "author": "EchoesInBackpack",
          "text": "5.2 Codex and 5.3 codex models have twice bigger context windows. Sub-agents might be fine, but using it just to not hit compaction feels annoying\n\ntry opencode models â€”verbose(or something like that) to see their context limits",
          "score": 2,
          "created_utc": "2026-02-16 17:43:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qqux1",
              "author": "Charming_Support726",
              "text": "I know, also get them from Azure (free credits) and using my OpenAI subscription. So I am not afraid to run out of computing power. I really like them, but some tasks run better with Opus, e.g. building some up for the first time or running frontend debugging with playwright and a few more. I wanted to have a provider which acts uncomplicated and realizes access to Claude below API costs. \n\nI did a few test with subagents a few weeks ago. Found out, that saving tokens using Subs isn't that easy. Prompting this thoroughly is a piece of work on its own.\n\nAnyway, I thought it might be a good Idea to structure work and docs a bit better, so that I could easily start new sessions and work in the model's sweet spot below 128k - instead of hitting the compaction border like a vibe coder on regular basis. I gave it a try and found it impressing.\n\n",
              "score": 2,
              "created_utc": "2026-02-16 20:06:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5q6l3u",
          "author": "HarjjotSinghh",
          "text": "this actually feels like magic",
          "score": 2,
          "created_utc": "2026-02-16 18:30:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qjh37",
          "author": "trypnosis",
          "text": "I remember saying this was an issue but fixed in the CLI but not fixed yet on the desktop. Are you cli or desktop?",
          "score": 2,
          "created_utc": "2026-02-16 19:30:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qo09o",
              "author": "soul105",
              "text": "It's not yet fixed in CLI",
              "score": 2,
              "created_utc": "2026-02-16 19:52:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5qn0el",
              "author": "Charming_Support726",
              "text": "I am using web - building everything myself since I started a few month ago. Works til now without any issue.",
              "score": 1,
              "created_utc": "2026-02-16 19:47:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5qnm03",
                  "author": "trypnosis",
                  "text": "Never looked at the desktop app but if itâ€™s electron or any other web wrapper then it might be the same problem. Try the cli for a bit. I went from CC Max to Co Pilot pro+ I think I get decent value for money.",
                  "score": 3,
                  "created_utc": "2026-02-16 19:50:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67tgeo",
              "author": "TheDuck-Prince",
              "text": "I canâ€™t find anywhere that the CLI version is fixed ;(",
              "score": 1,
              "created_utc": "2026-02-19 09:57:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o68rosx",
                  "author": "trypnosis",
                  "text": "My bad I was sure some one mentioned it else where on this subreddit.",
                  "score": 1,
                  "created_utc": "2026-02-19 14:07:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5yt6hd",
          "author": "HarjjotSinghh",
          "text": "copilot + opencode is the real secret sauce now",
          "score": 2,
          "created_utc": "2026-02-18 00:42:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ytw5s",
              "author": "FaerunAtanvar",
              "text": "Care to elaborate?",
              "score": 1,
              "created_utc": "2026-02-18 00:46:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o67t60b",
              "author": "TheDuck-Prince",
              "text": "What",
              "score": 1,
              "created_utc": "2026-02-19 09:54:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o67vzsr",
              "author": "Legal_Dimension_",
              "text": "Yep, it's a fantastic price with the free models picking up explore tasks etc.",
              "score": 1,
              "created_utc": "2026-02-19 10:21:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5quyxg",
          "author": "Legal_Dimension_",
          "text": "Just received a suspension to my account for using copilot with opencode. I'm a heavy user and they don't like that so be careful.",
          "score": 1,
          "created_utc": "2026-02-16 20:26:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qvel7",
              "author": "Charming_Support726",
              "text": "That's bad. Could you tell us more? Explicitly because of Opencode? What is your subscription?\n\nBTW: What do you mean with \"Heavy User\" ?",
              "score": 2,
              "created_utc": "2026-02-16 20:28:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o67uj3n",
                  "author": "Legal_Dimension_",
                  "text": "So for context. \n\nGHCP pro+ account. Got a suspension of my copilot only. 2000+ autonomous requests without any user requests through opencode mainly low cost models running research and refinement loops over night.\n\nYou know, the whole point of having them.\n\nWoke up to a suspension, which didn't state why I was suspended just fair usage and ToS BS.\n\nDrafted ticket stating all the grey areas in their fair usage and ToS, that they by UK regulations must state usage rates if it is requirement to use a service etc and that they officially endorse opencode an agentic based cli so they need to reinstate me. They did.",
                  "score": 1,
                  "created_utc": "2026-02-19 10:07:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5qxrzz",
              "author": "ellensen",
              "text": "Suspension of your github  account or just the copilot subscription?",
              "score": 1,
              "created_utc": "2026-02-16 20:40:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67uk3p",
                  "author": "Legal_Dimension_",
                  "text": "Copilot only luckily.",
                  "score": 2,
                  "created_utc": "2026-02-19 10:07:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5t1h0p",
              "author": "joshashsyd",
              "text": "Yes I got a 157hr timeout??",
              "score": 1,
              "created_utc": "2026-02-17 03:42:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67un3p",
                  "author": "Legal_Dimension_",
                  "text": "Wow for heavy agent use? Is the ban still live?",
                  "score": 1,
                  "created_utc": "2026-02-19 10:08:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5wtago",
              "author": "haininhhoang94",
              "text": "i guess you use it with subagents?",
              "score": 1,
              "created_utc": "2026-02-17 18:44:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67rnhw",
                  "author": "Legal_Dimension_",
                  "text": "Yeah but I have developed a plugin to get around the detection while I bullied them with UK regulations and they reinstated the same day.",
                  "score": 1,
                  "created_utc": "2026-02-19 09:39:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r5yjz1",
      "title": "Huge Update: You can now run Shannon (Autonomous AI Pentester) directly on OpenCode! ðŸ›¡ï¸ðŸ’»",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r5yjz1/huge_update_you_can_now_run_shannon_autonomous_ai/",
      "author": "ResponsiblePlant8874",
      "created_utc": "2026-02-16 03:21:35",
      "score": 19,
      "num_comments": 10,
      "upvote_ratio": 0.83,
      "text": "If youâ€™ve been using **OpenCode** for autonomous development but worrying about the security of the code your agents are churning out, this is for you.\n\nA new plugin just dropped that lets you run **Shannon**â€”the fully autonomous AI hackerâ€”directly within your OpenCode environment.\n\n# What is Shannon?\n\nFor those who missed the buzz, Shannon (by KeygraphHQ) is essentially the \"Red Team\" to your \"Blue Team.\" While your other agents are busy building features, Shannonâ€™s only job is to break them. It doesnâ€™t just give you \"alerts\"; it actually identifies and delivers exploits to prove where your vulnerabilities are.\n\n# Why this matters for OpenCode users:\n\nUntil now, Shannon was mostly a standalone powerhouse. With the **opencode-shannon-plugin**, you can now bake security auditing right into your agentic workflow.\n\n* **Security-First Vibe Coding:** Stop treating security as an afterthought.\n* **Autonomous Audits:** Let Shannon scan your PRs and local codebase for exploits before you ever hit \"merge.\"\n* **Zero Friction:** It integrates directly via the OpenCode plugin system.\n\n# How to get it:\n\nThe plugin is hosted on GitHub by **vichhka-git**: ðŸ‘‰[https://github.com/vichhka-git/opencode-shannon-plugin](https://github.com/vichhka-git/opencode-shannon-plugin)\n\n**Quick Install (usually):**\n\n1. Clone/Add the plugin to your `.opencode/plugin/` directory.\n2. Restart OpenCode.\n3. (Check the README for specific environment variables needed for the Shannon core).\n\nHuge props to the dev for making this bridge. It makes the \"full-stack\" agentic dream feel a lot more production-ready.\n\n**Has anyone tried running it against their current projects yet? Curious to see what kind of exploits it's catching in AI-generated code!**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r5yjz1/huge_update_you_can_now_run_shannon_autonomous_ai/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5o4z8s",
          "author": "MaxPhoenix_",
          "text": "The actual purpose of this software differs significantly from the claims of this Reddit post - it says shannon plugin is a tool for scanning PRs and auditing local codebases for vulnerabilities, yet the plugin contains no code analysis capabilities whatsoever - it's purely a network penetration testing framework designed to attack live web applications using tools like nmap, sqlmap, and hydra in a Kali Linux Docker container. If you're looking for static code security auditing or SAST, this isn't it; if you need to pentest deployed applications, it might be useful.\n\nThis post conflates the original Shannon's capabilities (which DO include source code analysis for \"scan your PRs\") with this simplified plugin (which only does black-box network pentesting). The plugin author appears to have built a thin wrapper around Shannon's attack tools without implementing the source analysis capabilities that would make the \"scan your PRs\" claim true. Shrug.",
          "score": 6,
          "created_utc": "2026-02-16 12:08:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5scnep",
              "author": "oulu2006",
              "text": "that's a great assessment thank you for ur service ",
              "score": 1,
              "created_utc": "2026-02-17 01:10:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nddj8",
          "author": "xak47d",
          "text": "Any volunteer to do a security audit on this",
          "score": 4,
          "created_utc": "2026-02-16 07:57:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5o11ma",
              "author": "MaxPhoenix_",
              "text": "I have audited the code and it is legit but a problem with seeking security assurance on the internet is you might not trust my authority.  The docker container runs with direct network access (which seems reasonable but you should be aware of it) but it has no obfuscated code, minified files, hardcoded secrets, suspicious network calls, eval or function usage, nor malicious imports.  EDIT: however, the description here doesn't match the code so even though it's \"clean\", people need to be careful what they are getting into.",
              "score": 5,
              "created_utc": "2026-02-16 11:36:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5o2csv",
                  "author": "gottapointreally",
                  "text": "Thank you for taking a look",
                  "score": 2,
                  "created_utc": "2026-02-16 11:47:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ns4cn",
              "author": "Swimming_Ad_5205",
              "text": "ÐÑ…Ð°Ñ…Ð°Ñ…Ð° Ð¾Ñ‡ÐµÐ½ÑŒ Ñ‚Ð¾Ñ‡Ð½Ð¾",
              "score": 2,
              "created_utc": "2026-02-16 10:16:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5odqli",
          "author": "lundrog",
          "text": "Seems safe ðŸ‘€",
          "score": 3,
          "created_utc": "2026-02-16 13:09:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5scot7",
              "author": "oulu2006",
              "text": "lol\n\n",
              "score": 2,
              "created_utc": "2026-02-17 01:10:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5v75dg",
          "author": "HarjjotSinghh",
          "text": "this just saved my pentesting life!",
          "score": 2,
          "created_utc": "2026-02-17 13:57:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pe4op",
          "author": "ResponsiblePlant8874",
          "text": "this repo will take sometime to continue update",
          "score": 1,
          "created_utc": "2026-02-16 16:18:45",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8kwsu",
      "title": "Built a VS Code companion for OpenCode users: session monitoring + handoff + coding workflows (feedback welcome)",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/msnhegrmmckg1.gif",
      "author": "Cal_lop_an",
      "created_utc": "2026-02-19 00:50:07",
      "score": 19,
      "num_comments": 8,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r8kwsu/built_a_vs_code_companion_for_opencode_users/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o667to5",
          "author": "Putrid-Pair-6194",
          "text": "Lots of interesting tools. I will add to my list to try. \n\nWould be helpful to have some more videos of features - in higher resolution. I canâ€™t make out whatâ€™s happening in the existing video/gif I saw. Too low resolution.\n\nThe kanban board, session handoff, session monitor, and error analysis tools are all of interest.",
          "score": 2,
          "created_utc": "2026-02-19 02:28:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68ddtn",
              "author": "Cal_lop_an",
              "text": "Thx! Just added some new features. This thing is moving fast.",
              "score": 2,
              "created_utc": "2026-02-19 12:41:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67ebwe",
          "author": "germantrademonkey",
          "text": "As somone who is working with opencode inside VSCode, this looks amazing! I'll give it a try!",
          "score": 2,
          "created_utc": "2026-02-19 07:30:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68dhnh",
              "author": "Cal_lop_an",
              "text": "Thx! Feel free to report anything that could be better. It's really help me and my team in our daily work with agents.",
              "score": 1,
              "created_utc": "2026-02-19 12:42:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o689qot",
          "author": "Relevant_Accident666",
          "text": "Really interesting. Is there something similar for zed as well? ",
          "score": 2,
          "created_utc": "2026-02-19 12:16:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68dap6",
              "author": "Cal_lop_an",
              "text": "That and gemini-cli are in the backlog. Yes. I've been looking at zed for a few days and think it's elegant.",
              "score": 3,
              "created_utc": "2026-02-19 12:41:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o65tld3",
          "author": "Boring-Ad-5924",
          "text": "If opencode is ran within VS Code, and instructions say to open an opencode window. Couldnâ€™t you just instead run an opencode server locally or elsewhere ? Defeats having to run â€œtwoâ€ windows",
          "score": 1,
          "created_utc": "2026-02-19 01:05:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68ecza",
              "author": "Cal_lop_an",
              "text": "Not sure if I follow. Opencode runs on the terminal, which just happens to be vscode (or codium or whatever)'s terminal.\n\nThe extension is meant to monitor and influence the agents operations and be compatible between different agents.",
              "score": 1,
              "created_utc": "2026-02-19 12:48:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4v7by",
      "title": "OCMONITOR - a CLI tool to monitor OPENCODE CLI usage",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r4v7by/ocmonitor_a_cli_tool_to_monitor_opencode_cli_usage/",
      "author": "WriterOld3018",
      "created_utc": "2026-02-14 20:36:06",
      "score": 18,
      "num_comments": 3,
      "upvote_ratio": 0.95,
      "text": "https://preview.redd.it/95r6b42ktijg1.png?width=3790&format=png&auto=webp&s=e0b2919618f556d387b59e6071b3bb85890aa3bc\n\nHello opencode community,\n\n5 months ago I madeÂ ocmonitor, an open-source CLI tool to monitor opencode usage. Since yesterday (version 1.2.0+), opencode migrated from storing sessions in JSON files to using a SQLite database. Iâ€™ve updated ocmonitor to support this change.\n\nI also added a hierarchy view to show subagents as part of the parent session, and monitoring of output rate (TPS) to give an indication of model performance.\n\nI would appreciate any feedback or bug reports (preferably via GitHub). PRs and contributions are also welcome.  \n[https://github.com/Shlomob/ocmonitor-share](https://github.com/Shlomob/ocmonitor-share)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4v7by/ocmonitor_a_cli_tool_to_monitor_opencode_cli_usage/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5fclrh",
          "author": "rizal72",
          "text": "I love it! Beautifully done, very useful, very good job!",
          "score": 1,
          "created_utc": "2026-02-14 23:59:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ijgae",
          "author": "altsyst",
          "text": "Nice! Is there a way to inspect the whole context window besides having the number of tokens? I'm looking for a tool allowing me to see easily the whole context.",
          "score": 1,
          "created_utc": "2026-02-15 14:50:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kgktq",
          "author": "HarjjotSinghh",
          "text": "this is unreasonably cool actually!",
          "score": 1,
          "created_utc": "2026-02-15 20:33:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r78b58",
      "title": "Cron Jobs, Integrations, and OpenCode are all you need to build 24/7 agent like OpenClaw",
      "subreddit": "opencodeCLI",
      "url": "https://github.com/composiohq/secure-openclaw",
      "author": "LimpComedian1317",
      "created_utc": "2026-02-17 15:01:48",
      "score": 18,
      "num_comments": 8,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r78b58/cron_jobs_integrations_and_opencode_are_all_you/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o60eb6l",
          "author": "amplifyoucan",
          "text": "What do you do for browser usage? Have it run playwright or something?",
          "score": 2,
          "created_utc": "2026-02-18 06:38:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66vajm",
              "author": "LimpComedian1317",
              "text": "It's handled by Composio's browser use tool.",
              "score": 1,
              "created_utc": "2026-02-19 04:56:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wodmj",
          "author": "Embarrassed_Bread_16",
          "text": "yeh, i want to implement it soon for and build me a bot that will send me summaries of books i have on the to read list in audio form",
          "score": 1,
          "created_utc": "2026-02-17 18:22:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66bntv",
          "author": "dheetoo",
          "text": "I just built one! https://github.com/dheerapat/botan-ebi",
          "score": 1,
          "created_utc": "2026-02-19 02:50:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wom2d",
          "author": "Embarrassed_Bread_16",
          "text": "edit:  \nnice blog, im taking it as inspo for my next build  \n  \nwhat docs did you / your bot use for reference?\n\ndid you use any other project as an inspo?",
          "score": 1,
          "created_utc": "2026-02-17 18:23:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66u86p",
              "author": "LimpComedian1317",
              "text": "The original OpenClaw was the inspiration, I just re-did it around OpenCode and ClaudeCode with integrations",
              "score": 1,
              "created_utc": "2026-02-19 04:48:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5xs7gn",
          "author": "Nearby_Tumbleweed699",
          "text": "Excuse my ignorance. Is it possible to have multiple agents running in parallel within the same session? That is, I have a series of tasks and I want to delegate them to a number of agents to execute. I know that Oh My OpenCode can do that, but what I don't understand is why install something else if we already have OpenCode as a tool.",
          "score": 1,
          "created_utc": "2026-02-17 21:29:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yuc08",
          "author": "miaowara",
          "text": "Kimaki (Discord) opencode plugin far exceeds openclawâ€™s discord implementation as well. Will be taking a look at your setup here. Was getting massively frustrated with openclaw & just wanted to stick with opencode.",
          "score": 0,
          "created_utc": "2026-02-18 00:48:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4vefc",
      "title": "Holy shit, Codex-5.3-Spark on OpenCode is FAST!",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r4vefc/holy_shit_codex53spark_on_opencode_is_fast/",
      "author": "jpcaparas",
      "created_utc": "2026-02-14 20:44:22",
      "score": 16,
      "num_comments": 16,
      "upvote_ratio": 0.77,
      "text": "Will provide some detailed feedback soon, but for those on the fence:\n\nEVERYTHING IS **INSTANT**. IT IS THE REAL THING!\n\n*\"I could smell colors, I could feel sounds.\"*\n\n  \nUpdate: I'm going back to Plus. The limited weekly cap and compaction issues are simply to hard to justify for the $200 price tag.\n\nhttps://preview.redd.it/fhp62ppcskjg1.png?width=1504&format=png&auto=webp&s=0413284d29b14420a50bf01cfa5e494de0abacc3\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4vefc/holy_shit_codex53spark_on_opencode_is_fast/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5efbak",
          "author": "HarjjotSinghh",
          "text": "okay first post ever? how's that instant thing work?",
          "score": 7,
          "created_utc": "2026-02-14 20:48:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ei7hq",
              "author": "ExtentOdd",
              "text": "Try Cerebras, you will feel that constant thing",
              "score": 5,
              "created_utc": "2026-02-14 21:04:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5g7yaw",
                  "author": "franz_see",
                  "text": "I have cerebras. I get rate limited a lot with its GLM 4.7 though ðŸ˜… but itâ€™s super fast! That being said, itâ€™s also super fast at consuming tokens! ðŸ˜…",
                  "score": 3,
                  "created_utc": "2026-02-15 03:26:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ef77e",
          "author": "jpcaparas",
          "text": "https://preview.redd.it/wsb29jnnvijg1.png?width=402&format=png&auto=webp&s=7d31b23879ab602f5b04840c78cfa635516a71f2\n\nAs you've probably already read, the context length is only 128K, so you'll have to leverage subagents where possible to break down bulky tasks.",
          "score": 11,
          "created_utc": "2026-02-14 20:47:49",
          "is_submitter": true,
          "replies": [
            {
              "id": "o5g7tqd",
              "author": "franz_see",
              "text": "Yep. Youâ€™d have to create a task list and keep delegating to a codex spark powered subagent to maximize it.\n\nTbh, cant imagine any other agent being able to maximize it as much as opencode. I could be wrong though.",
              "score": 2,
              "created_utc": "2026-02-15 03:25:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5g94fs",
                  "author": "jpcaparas",
                  "text": "No, you're right. That's why only use OpenCode and Claude Code these days. I requested a refund from OpenAI a few minutes ago for the atrocious weekly limits of Spark.",
                  "score": 2,
                  "created_utc": "2026-02-15 03:34:34",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5epsr9",
          "author": "jpcaparas",
          "text": "Okay, some early thoughts:\n\n- Auto compaction is horrible with Spark\n- It's very capable and very snappy, just avoid hitting the context window limits. \n- Your only noticeable bottleneck are external API calls responses\n- Spark is better used a hardcoded model on subagents instead of being the main model, ie use Opus 4.6,  Codex-5.3 or Kimi K2.5 as the orchestrator and have most if not all subagents use Spark.",
          "score": 8,
          "created_utc": "2026-02-14 21:45:37",
          "is_submitter": true,
          "replies": [
            {
              "id": "o5evrtz",
              "author": "segmond",
              "text": "how do you set up k2.5 as orchestrator and subagents to use spark?",
              "score": 2,
              "created_utc": "2026-02-14 22:18:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5extvm",
                  "author": "jpcaparas",
                  "text": "say for example, you have a slash command and that slash command invokes subagents: dont hardcode the model on the md file of the slash command, use ctrl + p to do model selection, but for the subagents that you definitely need spark for, hardcode them on the agent's md file",
                  "score": 2,
                  "created_utc": "2026-02-14 22:30:30",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5hto00",
              "author": "aithrowaway22",
              "text": "Can Kimi 2.5 really replace Codex 5.3/GPT 5.2 (on high) / Opus 4.5/4.6 in architecture/orchestrator roles ?  \nEven on LocalLama most people agree that open source models are not on that level for complex tasks.  \n",
              "score": 1,
              "created_utc": "2026-02-15 11:59:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5f7crw",
          "author": "j00stmeister",
          "text": "Cool, good to hear. Quick question tho: do you use it through the API or a ChatGPT plus/pro subscription?  \nWhen I use it using my subscription I get 'The 'gpt-5.3-codex-spark' model is not supported when using Codex with a ChatGPT account.'",
          "score": 1,
          "created_utc": "2026-02-14 23:26:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fcdlg",
              "author": "jpcaparas",
              "text": "I use the $200 Pro subscription. It's only available there... for now. Given the competitive nature these days, I doubt OpenAI will silo it for too long within that tier",
              "score": 1,
              "created_utc": "2026-02-14 23:58:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5fcv5k",
                  "author": "j00stmeister",
                  "text": "Ah good to know, thanks!",
                  "score": 0,
                  "created_utc": "2026-02-15 00:01:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5eeru8",
          "author": "jpcaparas",
          "text": "I'm more interested how it performs with layered subagents, so I'll factor that in too with feedback.",
          "score": 0,
          "created_utc": "2026-02-14 20:45:29",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r32eyj",
      "title": "Â£25~ budget. What is best for me?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r32eyj/25_budget_what_is_best_for_me/",
      "author": "TheSagaciousPanda",
      "created_utc": "2026-02-12 19:10:18",
      "score": 15,
      "num_comments": 40,
      "upvote_ratio": 0.9,
      "text": "I've used claude code for sometime now with a pro subscription but its become frustrating to use hitting session and weekly limits where they keep lowering it. I'm not a developer but i know my usage shouldnt be hitting limits like they are and therefore looking for a change. I also want to be able to use opencode again instead of claude code.\n\nI'm on the waiting list for opencode black which looks decent but no idea when that's coming and there are various subscriptions/ai stacks i can choose from but... I've no clue on what would be best for me.\n\n\\- Primary uses are ricing, debugging or optimising my computer.   \n\\- Secondary use would be asking questions, research for various things  \n\\- Sometimes use to vibecode small apps depending on what i need  \n  \nI'd appreciate any and all advice!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r32eyj/25_budget_what_is_best_for_me/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o519u6j",
          "author": "smile132465798",
          "text": "Codex and some provider that has kimi/minimax. Minimax 2.5 is really a solid sonnet replacement, kimi 2.5 is fast and codex for hard stuff",
          "score": 18,
          "created_utc": "2026-02-12 19:32:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51gudq",
          "author": "shaonline",
          "text": "If you want Opus-esque performance: Codex (via a ChatGPT Plus sub) for sure I mean not even a contest, at least while the rate limits are generous... You can use it through OpenCode \"legally\" as well.",
          "score": 8,
          "created_utc": "2026-02-12 20:06:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53jilr",
          "author": "BingGongTing",
          "text": "Codex until April (when 2x usage ends), then GLM Lite (should have GLM5 by then).\nGemini CLI also has a generous free tier.\nGitHub Copilot also has a free tier.",
          "score": 4,
          "created_utc": "2026-02-13 02:51:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51gy9o",
          "author": "Apprehensive_Half_68",
          "text": "Opus to create PRD, architect docs, and test driven development steps..then GLM 5 to code it up then Opus to grade. Rinse, repeat. Use Antigravity for Claude for free. And GLM for 3 bucks or so.",
          "score": 4,
          "created_utc": "2026-02-12 20:07:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o536wrn",
              "author": "ECrispy",
              "text": "how do you set this up? using different llms for the tasks? do you have to switch llm in opencode, copy/paste previous conversations etc?",
              "score": 3,
              "created_utc": "2026-02-13 01:33:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56wzel",
                  "author": "Apprehensive_Half_68",
                  "text": "Like everything else, I just ask AI to set it up, i'm really that lazy and it does a better job than I ever could.  \"I want to help save money by using x ide as a world-class software architect and y for a regular senior dev to do the actual coding up..research and develop and perfect a series of methodology docs that always keep up to date so i can switch back and forth any time and both x and y can pick up where the other left off. Right now model x costs n.1 USD input and n.2 Output and model y cost n.3 USD input and n.4 USD output per 1m tokens . Research the latest trends in AI task-driven-development theory then adapt it for someone who knows something about code but doesn't really want to even look at it much. it should include all best practices for security, docs, and any other best practices a software development studio would put into place but adapted for a lazy vibe coder. .Put all these magical docs in ./docs/methodology/. Any new ai agent who begins coding should be able to start working in the repo immediately being pointed by [AGENTS.md](http://AGENTS.md) to all the correct files it needs.  Don't over-engineeer or under-engineer this. Use Claude's 'Ultrathink' mode if available or whichever is the best agent available in this current ide and spend a bunch of time thinking about how to improve on this method.  Oh and then make an easy to read pretty user guide in markdown language for me to use this with step by step instructions i can keep in another window to follow along.  Oh and add a paragraph after each step to teach me what you are doing and why so I can learn to do better next time.  What other things should I be asking you that i have forgotten in order to save even more money in my budget?\"\n\nThen before i send it I have them clean up that prompt itself, wipe its memory then paste it.",
                  "score": 1,
                  "created_utc": "2026-02-13 16:51:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o53eqr4",
              "author": "kam1L-",
              "text": "Iâ€™m doing this with Claude as a brain and Gemini free tier 1 as a builder, so far best free combo for small stuff and some coding.",
              "score": 1,
              "created_utc": "2026-02-13 02:21:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o51zchp",
          "author": "SnooHesitations6473",
          "text": "OpenAI plus plan, very generous limits, separated limits for chat app. Top tier models. ",
          "score": 2,
          "created_utc": "2026-02-12 21:34:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53ax4v",
          "author": "DependentGuava2343",
          "text": "try copilot, with agents +subagents it might be a good thing for you, I believe \"copilot pro +\" is around Â£25\\~",
          "score": 2,
          "created_utc": "2026-02-13 01:58:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o515vkq",
          "author": "e979d9",
          "text": "[z.ai](http://z.ai) pro plan. I can share an invite link with you if you wish (10% off for you & me). Personally I'm satisfied with the service, and quite impressed GLM-5 that came out yesterday",
          "score": 3,
          "created_utc": "2026-02-12 19:13:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5237lr",
              "author": "TestTxt",
              "text": "minimax is cheaper and better",
              "score": 2,
              "created_utc": "2026-02-12 21:52:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o55855w",
              "author": "Pipimi",
              "text": "They increased the priced after glm 5 ðŸ¥€. I paid $34 for an annual lite plan",
              "score": 1,
              "created_utc": "2026-02-13 11:06:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5175ku",
          "author": "not_particulary",
          "text": "I just switched from Claude pro to codex, trying that out. It's honestly not a bad idea to just get an API key from some aggregator like openrouter, either.",
          "score": 1,
          "created_utc": "2026-02-12 19:19:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52iaj7",
          "author": "TheSagaciousPanda",
          "text": "Thanks for the replies everyone. \n\nI have antigravity on the freeplan but that burns through rate limits quickly if you use opus so unless im missing something that isn't viable unless you mean getting the google ai pro at 18.99 for the gemini models and antigravity usage. Problem is that people are getting banned from using claude models in opencode now aren't they so thats risky?\n\nI'm thinking chatgpt plus subscription for the codex models in opencode and then use [z.ai](http://z.ai) lite plan for GLM5.0 but I have questions\n\na) Would getting [z.ai](http://z.ai) pro plan be better than the above? What justifies the increased cost for it compared to chatgpt plus subscription currently?  \nb) What would be the recommended models regarding the relevant model/models mentioned for different stages - planning > executing > auditing/fixing/improving\n\n",
          "score": 1,
          "created_utc": "2026-02-12 23:10:34",
          "is_submitter": true,
          "replies": [
            {
              "id": "o52omp0",
              "author": "No_Success3928",
              "text": "A) No\nB) Stick with $20 claude or chatgpt, anything else will be a recipe for misery in your situation",
              "score": 3,
              "created_utc": "2026-02-12 23:46:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o53rfqi",
          "author": "keroro7128",
          "text": "My approach is to utilize various incentives to use high-end service providers on the market, including Claude, Codex, and Gemini, while keeping costs as low as possible. This year's cost is approximately $7.60 USD , or $0.63 per month, which is sufficient for casual users, such as those purely interested or students. The only drawback is Claude's relatively low quota, but I think it's enough for creating and conceiving plans. Codex and Gemini offer quotas equivalent to those for regular plus users.",
          "score": 1,
          "created_utc": "2026-02-13 03:41:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o541zui",
          "author": "Icy-Organization-223",
          "text": "I wrote this before in another post but I'll give you the breakdown. If you do things in smaller increments where the model doesn't have to keep building, fixing, and in turn making the context bloat. Just do small directed prompts by giving a file path makes me work on Claude pro plans all day and opencode with kimi 2.5 non stop all day for about $3-$4 but I mean alot of coding. I have let loose when I am lazy and let the model search around but have found out if you give full file.paths and limit it's scope it's better. Be specific on the prompt and the few things it should do. \n\nIn short The plan modes work real well with tight scope and don't let the context go to far as it gets sent again and again. For every new prompt. Start a new session for each file or few files that are related. It's so much faster as well. I do sometimes tell.it to browse my code and suggest things but that for assessments not getting tasks done.\n\nIf I wanted to get it down to less I would even use dumber models for simple refactors. Many cheaper models interpret and debug pretty well when given something to look at but very few models can write something new well. So preserve the refactor assessment document use cases in opencode to smaller parameter models. Now if your asking to write a whole app with it iterating and with context bloat it will burn tokens like a wild fire. \n\nAlso chatgpt free is excellent for code if you give it a full coding document and ask it to fix something. I use that for free and when I want to assess bugs etc. It usually fixes them. I leave sonnet,haiku,and kimi for more agentic coding. It's all about style. I have fit massive coding in the pro plan. But again you complained about the weekly limit which I can't seem to hit",
          "score": 1,
          "created_utc": "2026-02-13 04:54:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55109r",
          "author": "Dinth",
          "text": "Im in the same situation as op. My main useless are managing my docker compose files and configs for various services, home assistant automations and nix configurations. I do have access to Gemini 3.0 pro from work, and find that quality Sonnet 4.5 answers is literally a several levels above Gemini Pro for my usage. Would OpenAI codex be comparable for my use case to Gemini 3.0 Pro or more like Sonner 4.5? \n\nAlso Iâ€™ve got an ollama server with 16gb gpu, potentially will expand to around 24gb soon. Is that something I can realistically use with my OpenCode? Iâ€™ve done several attempts to use OpenCode with 13b models so far, and it seems that 13b models fail to do even simplest tooling",
          "score": 1,
          "created_utc": "2026-02-13 10:01:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58lp2c",
              "author": "shaonline",
              "text": "GPT/GPT-Codex takes on Opus, they each have their strengths. You can tweak the reasoning effort to save on tokens if you just need a decent executor (GPT-Codex medium).\n\nSmall (local) models will always struggle with proper output formatting (at the end of the day it just sends a big JSON for each command) nevermind actual \"knowledge\", and it's hardly something you'll band-aid with MCP servers.",
              "score": 1,
              "created_utc": "2026-02-13 21:48:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5623po",
          "author": "Queasy_Change4668",
          "text": "[https://cortexai.io/](https://cortexai.io/) , it gives unlmited usage of every ai in the world, Ä± deffinitly suggest you to try it, go and join to the discord and write someone to pay,  [https://discord.gg/VVsYkH9f](https://discord.gg/VVsYkH9f)",
          "score": 1,
          "created_utc": "2026-02-13 14:21:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kt1hl",
              "author": "ChocolateActive",
              "text": "How does this service work? It seems to all be in Turkish. I found the English Discord, but it's still a bit confusing regarding the different services and APIs. Are they able to be used with OpenCode, ClaudeCode, etc., and other harnesses?",
              "score": 1,
              "created_utc": "2026-02-15 21:37:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5c1bjs",
          "author": "HarjjotSinghh",
          "text": "ahhhhh budget hack time.",
          "score": 1,
          "created_utc": "2026-02-14 13:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o516y4d",
          "author": "HarjjotSinghh",
          "text": "clumsy devs making $25 budget look impossible. try i386 mode.",
          "score": 0,
          "created_utc": "2026-02-12 19:18:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53hvrk",
          "author": "Diligent_Speaker4692",
          "text": "As you mention you should do PRD with any good chat then put on you project with claude code. Claude code is good for user intention and  good follow instructions so this is the way.  For a developer my best combo is openspec + codex 5.3 high codex planning and 5.3 codex for implement this work perfect",
          "score": 0,
          "created_utc": "2026-02-13 02:41:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51cs19",
          "author": "[deleted]",
          "text": "[removed]",
          "score": -4,
          "created_utc": "2026-02-12 19:47:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51sevd",
              "author": "touristtam",
              "text": "> were an early stage startup so basically we give claude code max equivalent subscription for the price of the pro plan for now as were early and users stay at same price for 1 year\n\nSorry I might be thick but can you break down what you are trying to say?",
              "score": 3,
              "created_utc": "2026-02-12 21:01:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o52nyvx",
                  "author": "No_Success3928",
                  "text": "Looks like they are reselling claude access?",
                  "score": 2,
                  "created_utc": "2026-02-12 23:42:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o52cs40",
                  "author": "TheSagaciousPanda",
                  "text": "u/Popular-Category711 not sure what your copy paste response is trying to say because you dont even mention what service your trying to offer as a new startup and how your able to get what your claiming",
                  "score": 1,
                  "created_utc": "2026-02-12 22:40:51",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o52ea0s",
          "author": "Rygel_XV",
          "text": "You can also checkout [synthetic.new](http://synthetic.new). They have Kimi 2.5, GLM 4.7 (but plan to host 5 once it is released as open source) and Minimax 2.1 (and will probably also host 2.5 once it is released as open source). \n\nI am using them myself, and I am very happy with their performance. \n\n[https://synthetic.new/?referral=SNJDbFCgSUZso9E](https://synthetic.new/?referral=SNJDbFCgSUZso9E)",
          "score": -1,
          "created_utc": "2026-02-12 22:48:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52p5p6",
              "author": "No_Success3928",
              "text": "OP should also know new signups are on a waitlist for access.",
              "score": 0,
              "created_utc": "2026-02-12 23:49:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o548luj",
                  "author": "Rygel_XV",
                  "text": "Oh, I didn't know about the waitlist. Thank you for mentioning it.",
                  "score": 2,
                  "created_utc": "2026-02-13 05:45:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o52tanz",
              "author": "Shep_Alderson",
              "text": "Seconding synthetic.new. Their customer support has been top notch and Iâ€™m sure they will have the latest Kimi and MiniMax models up soon. Iâ€™m guessing they just need to prove it out and such first.",
              "score": -1,
              "created_utc": "2026-02-13 00:13:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o52h8ue",
          "author": "ch4dev_lab",
          "text": "!RemindMe 1week",
          "score": -1,
          "created_utc": "2026-02-12 23:04:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52hh21",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 7 days on [**2026-02-19 23:04:49 UTC**](http://www.wolframalpha.com/input/?i=2026-02-19%2023:04:49%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/opencodeCLI/comments/1r32eyj/25_budget_what_is_best_for_me/o52h8ue/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FopencodeCLI%2Fcomments%2F1r32eyj%2F25_budget_what_is_best_for_me%2Fo52h8ue%2F%5D%0A%0ARemindMe%21%202026-02-19%2023%3A04%3A49%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201r32eyj)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": -1,
              "created_utc": "2026-02-12 23:06:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o52kjn3",
          "author": "ScorpionOfWar",
          "text": "Been using open-source models more lately for private stuff, I got the 100$ Claude Sub for work.\n\nEnded up trying [Synthetic](https://synthetic.new/?referral=hcHozzHNE8CxVvz)(\\~50% off) and it's been very solid so far, alternatively [Z.ai](https://z.ai/subscribe?ic=KQ77ZVKLB5)(\\~10% off) for just the GLM Models, nice for coding, but kind of unreliable at the moment. They host open-source models and the API is OpenAI-compatible so it just plugs into your CLI or Dev Environment. $20/mo flat for the subscription tier is nice.\n\nWith my ref link to Synthetic and Z you are able to get a rebate.",
          "score": -1,
          "created_utc": "2026-02-12 23:23:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52ovlv",
              "author": "No_Success3928",
              "text": "Synthetic have a wait list for new signups. Perhaps you should mention that?",
              "score": 2,
              "created_utc": "2026-02-12 23:47:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o52q4zs",
                  "author": "ScorpionOfWar",
                  "text": "That is true, they recently got new GPU resources though. So there will be more places available soon",
                  "score": 0,
                  "created_utc": "2026-02-12 23:54:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o53juil",
          "author": "Senior-Cod993",
          "text": "anthopic needs to stop tring to push woke agenda into the models",
          "score": -4,
          "created_utc": "2026-02-13 02:53:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8ohj9",
      "title": "TUI Kanban board for OpenCode",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r8ohj9/tui_kanban_board_for_opencode/",
      "author": "Professional_Past_30",
      "created_utc": "2026-02-19 03:33:36",
      "score": 14,
      "num_comments": 4,
      "upvote_ratio": 0.94,
      "text": "Hey everyone,\n\nIâ€™ve been working on a tool called **opencode-kanban** to manage multiple OpenCode sessions and Git worktrees.\n\nThe project is essentially a mixture of two existing concepts, aiming to fix the friction I felt with both:\n\n1. **The Functionality (vs. Agent of Empires):** Like [*Agent of Empires*](https://github.com/njbrake/agent-of-empires), this manages multiple agents through tmux sessions. However, I built this because I found AoE a bit clunky for my daily workflow. I wanted something more ergonomic - offering more features like task tracking.\n2. **The Workflow (vs. Vibe Kanban):** I loved the visual organization of [*Vibe Kanban*](https://www.vibekanban.com/), but I didn't want to leave the terminal. This project brings that Kanban view (Todo/Doing/Done) directly into the CLI.\n3. **OpenCode-Only:** Most importantly, because this is built *exclusively* for OpenCode, it doesn't suffer from the friction of tools trying to support every agent out there. Tapping directly into the OpenCode API unlocks a much more tailored experience, giving you native features like automatic session attaching and built-in todo tracking.\n\n**The Result:** A TUI-first experience that lets you manage parallel work streams visually, while still allowing you to stick with the native OpenCode TUI inside auto-managed tmux sessions.\n\nhttps://preview.redd.it/24kvmv98fdkg1.png?width=1280&format=png&auto=webp&s=9851e35ad1ddb7aef473b49b067887488f9581dc\n\nRepo: [https://github.com/qrafty-ai/opencode-kanban](https://github.com/qrafty-ai/opencode-kanban)\n\nWould love to hear your thoughts!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r8ohj9/tui_kanban_board_for_opencode/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o68fmz7",
          "author": "xenopticon",
          "text": "super cool, congrats on the launch!",
          "score": 2,
          "created_utc": "2026-02-19 12:56:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68i9zd",
          "author": "Nearby_Tumbleweed699",
          "text": "Funciona como plugin dentro opencode o es una herramienta externa?",
          "score": 2,
          "created_utc": "2026-02-19 13:13:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69elnc",
              "author": "Professional_Past_30",
              "text": "It's a standalone program which manages each \"task\" as a tmux session. A \"task\" is something you want to work on linked to a corresponding git worktree/branch. \n\nTypical workflow:\n1. Open the tool\n2. Create a task by specifying a repo and a branch name. The tool will automatically create a new worktree for you.\n3. Attach and work on a task in a dedicated tmux&open code session.",
              "score": 1,
              "created_utc": "2026-02-19 16:05:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67cehb",
          "author": "MrBansal",
          "text": "Slop",
          "score": -2,
          "created_utc": "2026-02-19 07:13:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6marc",
      "title": "Any difference when using GPT model inside Codex vs OpenCode?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r6marc/any_difference_when_using_gpt_model_inside_codex/",
      "author": "ponury2085",
      "created_utc": "2026-02-16 21:19:33",
      "score": 13,
      "num_comments": 12,
      "upvote_ratio": 0.84,
      "text": "I'm a die-hard fan of OpenCode - because of free model, how easy it is to use subagents, and just because it's nice. But I wonder if anyone finds GPT models better in Codex? I cannot imagine why they could possibly work better there, but maybe models are just trained that way, so they \"know\" the tools etc? Anyone noticed anything like that?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r6marc/any_difference_when_using_gpt_model_inside_codex/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5r8t8v",
          "author": "itsjase",
          "text": "I think both claude code and codex have some magic sauce to work better with their respective models.\n\nI personally think codex + 5.3 codex is way ahead of opencode + 5.3 codex. I'm realising now the harness matters just as much as the model these days.",
          "score": 8,
          "created_utc": "2026-02-16 21:34:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5v8pp0",
              "author": "BodeMan5280",
              "text": "that's interesting... I think it comes down to speed for me. OpenCode seems to just get shit done (yes, ironic pun to GSD). Don't get me wrong, Codex is KILLER at getting shit done, but slower IMO.",
              "score": 1,
              "created_utc": "2026-02-17 14:06:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5r7bv5",
          "author": "TechCynical",
          "text": "This is what people mean when they say \"the harness\". Using it in codex means you get the bare bones experience. Not bad but it just means it isn't fine tuned to work specifically for coding /what you would want for coding at least. \n\nThere's a concern for over engineering but that's why things are open source. Claudecode for example has a lot of changes to its system prompt to work well for everything Claude code will try to do like call mini agents and tools during it's execution. Codex afaik actually has nothing but I could be wrong. GitHub copilot has its own too supposedly tuned for multi model workflows, and opencode has their own as well.\n\nImo all models work better in opencode. Sometimes this changes with select models, but it's a safe bet just to use opencode.",
          "score": 7,
          "created_utc": "2026-02-16 21:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uudx6",
              "author": "Morisander",
              "text": "Would you kindly explain your first and last paragraph? This way it sounds a little likeâ€¦ bullshit?",
              "score": 2,
              "created_utc": "2026-02-17 12:41:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tpxpz",
          "author": "widonext",
          "text": "For me there is a difference, but it works great in opencode, so itâ€™s fine for me",
          "score": 2,
          "created_utc": "2026-02-17 06:45:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tjliv",
          "author": "georgiarsov",
          "text": "I tried codex 5.3 in opencode on release and can confirm it was 100% shit. I couldnâ€™t believe the huge gap between my experience and that of the people using it in codex",
          "score": 1,
          "created_utc": "2026-02-17 05:52:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5u1gbv",
          "author": "Round_Mixture_7541",
          "text": "Yes. AI harnesses built by their respective model producers tend to work better together.",
          "score": 1,
          "created_utc": "2026-02-17 08:32:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5u2uij",
          "author": "Open_Scallion9015",
          "text": "I had this experience myself previously but it seems that since a month or so this gap has narrowed or maybe even completely closed. Personally did not had to urge to use the Codex harness recently at all.",
          "score": 1,
          "created_utc": "2026-02-17 08:45:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vlrc6",
          "author": "HarjjotSinghh",
          "text": "this sucks too much i'd pay to use this version",
          "score": 1,
          "created_utc": "2026-02-17 15:14:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60xxzh",
          "author": "blackbirdweb",
          "text": "Simple answer: 5.3-Codex works a bit better in Codex when it comes to the overall quality of the work. However it is much, much nicer to use in Opencode Desktop on Windows. If you are on Windows then the best way to use native codex is the codex plugin in VSCode. However, Opencode Desktop is just much nicer to use, more transparent, more configurable and honestly just more fun. This might change when OpenAI stops gooning over Mac and finally decide to support the most used business OS in the world with their desktop app. I dislike Windows but it's what we use at work as does almost everybody.",
          "score": 1,
          "created_utc": "2026-02-18 09:38:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rw5q7",
          "author": "nyldn",
          "text": "Latest version of Claude Octopus utilises Codex 5.3 smartly https://github.com/nyldn/claude-octopus",
          "score": -5,
          "created_utc": "2026-02-16 23:35:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5txeq6",
              "author": "KnifeFed",
              "text": "Sir, this is the OpenCode sub.",
              "score": 3,
              "created_utc": "2026-02-17 07:54:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r7015z",
      "title": "Anyone else struggling with Opencode gobbling up ram?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r7015z/anyone_else_struggling_with_opencode_gobbling_up/",
      "author": "Optimal_Strength_463",
      "created_utc": "2026-02-17 07:53:14",
      "score": 12,
      "num_comments": 17,
      "upvote_ratio": 1.0,
      "text": "I absolutely love Opencode over the alternatives due to how easy it was to build my own workflow and memory system for it. However I am continually getting Opencode processes running at 10-20gb which on a MacBook Pro with only 16gb of ram means I canâ€™t run multiple CLIs at once like I used to with Claude.\n\nThereâ€™s plenty of people trying to fix it and thereâ€™s even â€œReady to mergeâ€ PRs on Git like this one: https://github.com/anomalyco/opencode/pull/13594\n\nBut the changelog always seems to be focussing on features and minor fixes rather than sorting out some big fundamental issues that stop Opencode from being a real pro-grade tool.\n\nWould be really interested to hear other peopleâ€™s experiences and maybe workarounds?\n\nNote: I am not the author of that PR, but I did leave a comment in the hope it starts to get some traction",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r7015z/anyone_else_struggling_with_opencode_gobbling_up/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5tzind",
          "author": "jhartumc",
          "text": "Yeah, on my mac air with 16gb i can feel it, even in one instance but with multiple agents inside",
          "score": 3,
          "created_utc": "2026-02-17 08:14:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5u1xoj",
          "author": "Rygel_XV",
          "text": "I have the same issue on windows. I regularly close the opencode instance when the bun instance uses too much RAM.",
          "score": 3,
          "created_utc": "2026-02-17 08:37:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ufn42",
              "author": "nasimhc",
              "text": "I am on windows and facing same issue. Sometimes bun process takes close to 2gb of rams.",
              "score": 3,
              "created_utc": "2026-02-17 10:46:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o68aien",
                  "author": "masterkoster",
                  "text": "2? Try 31..",
                  "score": 1,
                  "created_utc": "2026-02-19 12:21:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5u1z0t",
          "author": "shotgunsparkle",
          "text": "problem i have is some processes linger even after ive closed some jobs",
          "score": 2,
          "created_utc": "2026-02-17 08:37:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5utbpp",
          "author": "krimpenrik",
          "text": "Can you high level describe your memory system? Currently setting something up as wel... I am using markdown with obsidian syntax",
          "score": 2,
          "created_utc": "2026-02-17 12:34:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uwwzh",
              "author": "Optimal_Strength_463",
              "text": "Mixture of Skill, MCP and Plugins to read all of the â€œsessionâ€ messages and store them (assistant responses and thinking blocks). The daemon in the MCP server then works though these to extract memories. The plugin then injects context into my interactions to help give the LLM some ideas of how to solve it within my codebase.\n\nMCP allows the LLM to use its skill to save and retrieve memories when it wants to as well",
              "score": 1,
              "created_utc": "2026-02-17 12:58:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5w1wmu",
          "author": "philosophical_lens",
          "text": "I face the same exact issue with both Claude code and opencode. I once had a single long running Claude code instance that grew to nearly 16GB RAM.",
          "score": 2,
          "created_utc": "2026-02-17 16:34:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5u2odx",
          "author": "aeroumbria",
          "text": "I found that if you terminate the window of opencode without using Ctrl+C/D to terminate the app itself, opencode stays open...",
          "score": 2,
          "created_utc": "2026-02-17 08:44:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u8oo7",
              "author": "touristtam",
              "text": "So the process doesn't get terminated? Is that an issue with Opencode itself?",
              "score": 3,
              "created_utc": "2026-02-17 09:42:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5txpqr",
          "author": "HarjjotSinghh",
          "text": "this macbook's doing its best with 16gb.",
          "score": 2,
          "created_utc": "2026-02-17 07:56:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66r5fx",
          "author": "HarjjotSinghh",
          "text": "oh man opencode just took over my dreams with ram!",
          "score": 1,
          "created_utc": "2026-02-19 04:27:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68agmg",
          "author": "masterkoster",
          "text": "https://preview.redd.it/50we0ohy1gkg1.jpeg?width=3024&format=pjpg&auto=webp&s=fbdc52e109ef9ee7691276232161a0127207b9f4\n\nTell me about it..",
          "score": 1,
          "created_utc": "2026-02-19 12:21:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68amyd",
              "author": "masterkoster",
              "text": "https://preview.redd.it/zk9wsbq62gkg1.jpeg?width=3024&format=pjpg&auto=webp&s=d2138628f3d04f40ab829d5f1e7355ba657d0724",
              "score": 1,
              "created_utc": "2026-02-19 12:22:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5uj873",
          "author": "Keep-Darwin-Going",
          "text": "Memory leak are much harder to verify and have it merged. You cannot just go around trust me bro and merge code.",
          "score": -2,
          "created_utc": "2026-02-17 11:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uptnu",
              "author": "Optimal_Strength_463",
              "text": "Yes, Iâ€™m well aware of that. And while I have profiled it on my machine and seen improvements there are also multiple PRâ€™s around this subject that donâ€™t seem to be progressing or have feedback",
              "score": 1,
              "created_utc": "2026-02-17 12:09:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5usmns",
                  "author": "Keep-Darwin-Going",
                  "text": "Yeah, it is really hard to confirm if that patch solve it that is why it will just go on the back burner until they reach like stability patch timing. \nFrom my point of view is I will be adding new stuff constantly so might as well wait until end of quarter than we go fix all at one go instead of wasting time doing it one at a time. I meant that is how I maintain my own product unless the leak is so bad that it affects most users",
                  "score": -1,
                  "created_utc": "2026-02-17 12:29:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r3lhh0",
      "title": "OpenCode Bar update: Support Codex Spark Tracking and other CLI apps",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/00taozb2l8jg1.png",
      "author": "kargnas2",
      "created_utc": "2026-02-13 10:11:59",
      "score": 11,
      "num_comments": 6,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r3lhh0/opencode_bar_update_support_codex_spark_tracking/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o552iz3",
          "author": "HarjjotSinghh",
          "text": "this tracking is officially the best secret weapon.",
          "score": 2,
          "created_utc": "2026-02-13 10:15:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o552oil",
              "author": "kargnas2",
              "text": "How many languages do you speak",
              "score": 2,
              "created_utc": "2026-02-13 10:17:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o56vl0b",
          "author": "devdnn",
          "text": "Does the bar not pick copilot connected to enterprise via opencode?",
          "score": 1,
          "created_utc": "2026-02-13 16:45:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57ppbc",
          "author": "drop_drang",
          "text": "No linux? Uh ...",
          "score": 1,
          "created_utc": "2026-02-13 19:09:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58c33j",
          "author": "wreinoriginal",
          "text": "Why it is asking me for permissions to access the keychain?",
          "score": 1,
          "created_utc": "2026-02-13 21:01:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ann98",
              "author": "kargnas2",
              "text": "Claude Code and Web Browser for Copiloy",
              "score": 1,
              "created_utc": "2026-02-14 05:36:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}