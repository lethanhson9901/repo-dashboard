{
  "metadata": {
    "last_updated": "2026-02-18 03:09:58",
    "time_filter": "week",
    "subreddit": "opencodeCLI",
    "total_items": 20,
    "total_comments": 209,
    "file_size_bytes": 207127
  },
  "items": [
    {
      "id": "1r7bts8",
      "title": "GLM5 is free for a week",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/jyoyelzc63kg1.jpeg",
      "author": "jpcaparas",
      "created_utc": "2026-02-17 17:02:52",
      "score": 138,
      "num_comments": 14,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r7bts8/glm5_is_free_for_a_week/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5w8ljm",
          "author": "EchoesInBackpack",
          "text": "Would be funny if it works better than on zai paid plan",
          "score": 23,
          "created_utc": "2026-02-17 17:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w9x9j",
              "author": "shaonline",
              "text": "They typically do these type of deals with american providers (I think Kimi was on fireworks for example ?) so it probably will.",
              "score": 8,
              "created_utc": "2026-02-17 17:14:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wcgh7",
                  "author": "Spitfire1900",
                  "text": "ollama/glm-5:cloud has been too",
                  "score": 2,
                  "created_utc": "2026-02-17 17:27:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5wnuso",
              "author": "Noob_l",
              "text": "As a max user of z.ai coding plan, I would not recommend it. It now has less usage than other coding plans like minimax and codex. \nAnd the constant errors and silent changes on users should be red flash to not prolong the subscription. Went from being of the best plans in terms of affordable pricing and usage to one of the worst.\n\n-- not even Claude code fills up your weekly usage in just 5 5hour windows. Z.ai does that to you though.\n\nAnd really a warning to all: it wil read your env file and will delete files even if you said explicitly not to in the same conversation as well as having that as fixed rules in the agents file\n\nDon't use glm 5",
              "score": 3,
              "created_utc": "2026-02-17 18:20:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wqno2",
                  "author": "EchoesInBackpack",
                  "text": "I tried glm5 at openzen, it it was decent and very fast. I thought that it can be my daily driver. Then I bought the sub on zai, and it barely works, not usable at all. I feel scummed.",
                  "score": 4,
                  "created_utc": "2026-02-17 18:32:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ws9l9",
                  "author": "jpcaparas",
                  "text": "same, horrendous drop off in speed after the second day. Why even pay for ultra if you're bogged down to lite inference.",
                  "score": 3,
                  "created_utc": "2026-02-17 18:40:18",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o5z8e86",
                  "author": "Jlocke98",
                  "text": "You can max out the kimi weekly plan at the 20usd tier with 2x 5hr windows",
                  "score": 1,
                  "created_utc": "2026-02-18 02:02:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wpnik",
          "author": "TurnUpThe4D3D3D3",
          "text": "Cerebras for free would be crazy",
          "score": 4,
          "created_utc": "2026-02-17 18:28:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wxc6g",
          "author": "hatepoorpeople",
          "text": "I just cancelled my zai subscription. GLM-5 is completely unusable for me.",
          "score": 2,
          "created_utc": "2026-02-17 19:03:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xc654",
          "author": "FaerunAtanvar",
          "text": "I tried to use ite and I instantly got \"all credits used\"",
          "score": 1,
          "created_utc": "2026-02-17 20:13:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yph42",
          "author": "Euphoric-Doughnut538",
          "text": "Canâ€™t get shit done with the limits",
          "score": 1,
          "created_utc": "2026-02-18 00:22:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5vv6g",
      "title": "Minimax M2.5 is not worth the hype compared to Kimi 2.5 and GLM 5",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r5vv6g/minimax_m25_is_not_worth_the_hype_compared_to/",
      "author": "Resident-Ad-5419",
      "created_utc": "2026-02-16 01:14:20",
      "score": 82,
      "num_comments": 42,
      "upvote_ratio": 0.93,
      "text": "I used opencode with exa; to test the latest GLM 5, Kimi 2.5 and Minimax M2.5, along with Codex 5.3 and Opus 4.6 (in its own cli) to understand how would they work on my prompt. And the results were very disappointing.\n\nDespite all these posts, videos and benchmarks stating how awesome minimax m2.5 is, it failed my test horribly given the same environment and prompt, that the others easily passed.\n\nMinimax kept hallucinating various solutions and situations that didn't make any sense. It didn't properly search online or utilized the available documentation properly. So, I wonder how all those benchmarks claiming minimax as some opus alternative actually made their benchmark.\n\nI saw a few other real benchmarks where Minimax M2.5 actually was way below Haiku 4.5 while GLM 5 and Kimi went above Sonnet 4.5; personally it felt like that as well. So at the increased price points from all these providers, its very interesting. Though neither are on opus or codex level.\n\nI did not test the same prompt with gemini, or couldn't test it, to be more precise due to circumstances. But I have a feeling Gemini 3 Pro would be similar to Kimi and GLM 5, maybe just a bit higher.\n\nWhat is your experience with Minimax compared to GLM and Kimi?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r5vv6g/minimax_m25_is_not_worth_the_hype_compared_to/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5lx2xc",
          "author": "Specialist-Yard3699",
          "text": "GLM still has its old problem - horrible speed. \nKimi25 - low limits. In my opinion, this model is not so good in terms of price-to-quality ratio.\nMinimax25 - fast and a good â€œplan executorâ€/â€œcode searcherâ€. I have worked a lot with Minimax21, and the 25th version is much better and has a lower hallucination rate.",
          "score": 18,
          "created_utc": "2026-02-16 01:26:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5m1i9x",
              "author": "lopydark",
              "text": "where minimax 25",
              "score": 1,
              "created_utc": "2026-02-16 01:55:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5m9h0y",
                  "author": "Specialist-Yard3699",
                  "text": "Official minimax code plan + oh-my-opencode plugin + detailed plan from reasoning model (Glm/gemini3)",
                  "score": 2,
                  "created_utc": "2026-02-16 02:46:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5lyi2e",
              "author": "Resident-Ad-5419",
              "text": "It hallucinated a lot on my prompt, the same one given to opus, codex, kimi and glm and those four worked well for that, while minimax failed horribly. It kept inventing stuff that doesnt even exist. I tested many times just to be sure I wasn't the one hallucinating.",
              "score": -1,
              "created_utc": "2026-02-16 01:35:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5m59di",
          "author": "tripleshielded",
          "text": "Yes, minimax still cant do any difficult work on its own. But m2.5 seems a bit better than m2.1, its a good upgrade.",
          "score": 5,
          "created_utc": "2026-02-16 02:19:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m5ovy",
          "author": "segmond",
          "text": "they all have their strengths, i run all of them locally.   minimax2.5 was able to solve a problem that I couldn't get GLM5 and KimiK2.5 to solve after a few prompts, minimax solved it with the same prompt in one go, and generated almost 4,000 lines of perfect code.  The interesting thing is I couldn't have started a solution to the problem without Kimi K2.5 it has very unique capability that many other models don't.  IMO, they all have strengths and you have to know when to use one for another.  I like Minimax not because it solved my problem but I can also run it much faster.  ",
          "score": 5,
          "created_utc": "2026-02-16 02:21:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5oc5nq",
              "author": "Resident-Ad-5419",
              "text": "Absolutely! Any model that solves your problem is the best model, regardless of whatever anyone says otherwise.",
              "score": 2,
              "created_utc": "2026-02-16 12:59:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5mvemk",
          "author": "Lpaydat",
          "text": "GLM5 is really good. Really really good. Just quite slow sometimes.",
          "score": 5,
          "created_utc": "2026-02-16 05:20:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ngr1l",
          "author": "kshnkvn",
          "text": "Skill issue. I'm not joking. Minimax is beast for use as a subagent, as an executor/researcher/etc.\nYou can't use it straight as opus/got and think that it will act as good, at least because it's small model, really small, it knows much less then competitors so you need to provide proper information and context.",
          "score": 3,
          "created_utc": "2026-02-16 08:28:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nvsu5",
              "author": "Resident-Ad-5419",
              "text": "I don't disagree with you on this point, this is worth a shot! Thank you!",
              "score": 2,
              "created_utc": "2026-02-16 10:50:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5v4vtp",
              "author": "Resident-Ad-5419",
              "text": "After your reply I went back and ran a test with Codex as the main agent and GLM, Kimi, Minimax and Codex Spark as sub agent. \n\nCodex + Codex Spark did the best, even better than codex solo.  \nKimi and GLM afterwards.  \nMinimax still couldn't beat even after instructions and helping hands from codex.\n\nI would like to say it's my skill issue, that I am not skilled enough to handle minimax like you do.",
              "score": 1,
              "created_utc": "2026-02-17 13:45:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5v8nai",
                  "author": "kshnkvn",
                  "text": "Idk what exactly you want me to say, because I literally know nothing about your workflow, stack, tasks.  \nIf M2.5 doesn't suit your needs it's fine, because it's not a general-purpose model, it has it's own limitations, cons and pros. It may be either good or bad depends on your tasks and how you threat it.  ",
                  "score": 1,
                  "created_utc": "2026-02-17 14:05:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5mpois",
          "author": "lundrog",
          "text": "K2.5 is very good on boardwell hardware , with the nvidia nvidia/K.imi-K2.5-NVFP4; assuming you find a provider hosing it. ( allows less memory usage for same performance )",
          "score": 3,
          "created_utc": "2026-02-16 04:37:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mwauu",
          "author": "Medical_Farm6787",
          "text": "To me since Iâ€™m using the free usage on opencode, probably due to the last GLM4.7 infinite thinking loop incident I just completely stopped using it.\n\nMy current workflow can be Kimi K2.5 for any web search related or codebase exploration â€”> switch to Minimax M2.5 for actual coding practices.\n\nBecause Minimax M2.5 did actually fall very short in terms of searching benchmarks you can look that up on their official benchmark results, but almost on par with opus and to me it really does feel like it, the thoughts are more structured than Kimi K2.5 when it comes to coding implementation, Kimi sometimes forgets the rules that I set in AGENTS.md as context window grows, but yet to have any issue when it comes to Minimax M2.5\n\nNot to say that Minimax M2.5 size is way more smaller than GLM so it feels well in my M3 ultra 256gb unified memory at Q6",
          "score": 3,
          "created_utc": "2026-02-16 05:27:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m2d16",
          "author": "czumaa",
          "text": "i think it's just me but now i think Minimax m2.5 just PLAIN LIE to all of us. Is not even close to the other models and have some weird allucinations. I test this models on coding. Just broke the entire project, don't respect the rules file of kilo code, re-do code as \"his\" way without check. is a disaster. i'm with GLM5 and kimi2.5 now. don't know which rely is the best but i think glm5 is a little best sometimes.",
          "score": 2,
          "created_utc": "2026-02-16 02:00:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mnpy1",
          "author": "Xhatz",
          "text": "It's great for it's size and fast, but it's truly NOT as good as they say, clearly. For me it feels like it's just m2.1 but with even less coherence sadly, hallucinations are too high (I can say something and a few messages afterwards it says something else). It also feels more \"lazy\" in a way... My theory is that it's only good at very specific things and just completely bad at everything else.",
          "score": 2,
          "created_utc": "2026-02-16 04:23:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mec9t",
          "author": "DistinctWay9169",
          "text": "MIniMax is pure hype. Tried it and nope, thanks. Broke my codebase many times to allow it to touch it again.",
          "score": 3,
          "created_utc": "2026-02-16 03:18:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n3sdd",
          "author": "chiroro_jr",
          "text": "Thought I was the only one. I'm still using Kimi. The limits and speed are good enough for me",
          "score": 1,
          "created_utc": "2026-02-16 06:30:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nbah4",
          "author": "SphaeroX",
          "text": "I also think that Kimi 2.5 is still unbeatable! But we'll see, the new DeepSeek should be ready soon.",
          "score": 1,
          "created_utc": "2026-02-16 07:37:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nd0vv",
          "author": "ciprianveg",
          "text": "maybe try with a lower temperature like 0.7?",
          "score": 1,
          "created_utc": "2026-02-16 07:53:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nszvi",
          "author": "xmnstr",
          "text": "I found the same. Cleary worse compared to GLM 5 and K2.5. I don't really understand why companies do this when it's so obvious to anyone who tries the model out that it simply does not hold up.",
          "score": 1,
          "created_utc": "2026-02-16 10:24:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nt77k",
          "author": "tricky-oooooo",
          "text": "Well, it's much smaller than both Kimi 2.5 and GLM5. What did you expect?",
          "score": 1,
          "created_utc": "2026-02-16 10:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5numcg",
          "author": "rudingshain",
          "text": "I use it Not for Coding but with opencode in textrelevant task and it works weil",
          "score": 1,
          "created_utc": "2026-02-16 10:40:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o81rh",
          "author": "odrakcir",
          "text": "not sure what to say, I've been using it for the last couple of days to write and fix a bunch of unit tests (react native) and it's been great. I've used it like this: plan mode -> openspec (explore + ff + manual review + implement + verify + archive).",
          "score": 1,
          "created_utc": "2026-02-16 12:31:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5obxcd",
              "author": "Resident-Ad-5419",
              "text": "If you use codex/opus as a driver; then haiku, minimax or any other smaller model can do wonders. Problem is the way they market it as if they are better than codex/opus in benchmarks, which is wrong.",
              "score": 2,
              "created_utc": "2026-02-16 12:58:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5olh5s",
                  "author": "odrakcir",
                  "text": "that is true, but to be honest, that claim makes part of the business haha. Now, what we can't deny is that OS models are getting closer.",
                  "score": 1,
                  "created_utc": "2026-02-16 13:54:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5o90ke",
          "author": "mintybadgerme",
          "text": "In my experience Kimi is definitely the best of the trio. GLM has problems as does Minimax.",
          "score": 1,
          "created_utc": "2026-02-16 12:38:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ooy4q",
          "author": "c0nfluks",
          "text": "I had the exact same experience. Kimi is way better. The benchmarks are cooked. Chinese AI companies are cheating on the exam, basically.",
          "score": 1,
          "created_utc": "2026-02-16 14:13:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5p5u06",
          "author": "l_eo_",
          "text": "MiniMax (both M2.1 and M2.5) for me is great for stable & efficient pipelines. \n\nSpawn researches, assess, return data, etc etc etc. \n\nIt's for me the perfect model for programmatic pipelines and is so far dominating its niche. \n\nI tried many other models and providers, but haven't found anything that could deliver this quality & stability at this cost level. \n\nIf anybody knows of anything that works better for these kind of use cases, please let me know!",
          "score": 1,
          "created_utc": "2026-02-16 15:40:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pv4dk",
          "author": "Alternative-Spray176",
          "text": "M2.5 doesn't follow instructions at all. I asked it to copy the plan it generated to a new file. It started implementing the plan. Retried but no use. I tried it for a few tasks, it doesn't want to listen to the user instructions. That model is hard to use like Google gemini models.",
          "score": 1,
          "created_utc": "2026-02-16 17:37:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ufe3p",
          "author": "Practical_Arm_645",
          "text": "I do have the same feeling. Minimax is almost useless for harder task, but only benefit is the speed. Gemini3pro is also terrible, even worse than the flash. The flash is quite reasonable, similar to sonnet 4.5 level",
          "score": 1,
          "created_utc": "2026-02-17 10:43:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v8bb9",
          "author": "Final-Rush759",
          "text": "It's not magic.  It's a cheap model, works fine for what it is.  It benchmark well with agent tests. It is not at the same level as big models.  It doesn't store as much knowledge as bigger models.",
          "score": 1,
          "created_utc": "2026-02-17 14:03:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w5rdo",
          "author": "StardockEngineer",
          "text": "HARD disagree.\n\nMy results in my tests - MM2.5 frequently implements big large PRDs with efficiency and precision.\n\nKimi often gives a result missing a ton of requirements.\n\nGLM5 for some reason uses a ton of tokens, costing far, far more than MM2.5",
          "score": 1,
          "created_utc": "2026-02-17 16:54:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wugw2",
          "author": "WolfpackBP",
          "text": "It's so good at agentic stuff though! And the price point is so good. \n\nI have found it to be very impressive\n\nKimi was slow and not as good with the agentic stuff at a higher price point \n\nHaven't tried GLM yet",
          "score": 1,
          "created_utc": "2026-02-17 18:50:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yk5ut",
          "author": "Dayclone",
          "text": "It's free right now through Ollama Cloud for a short period but seems to do ok for me. Has it's issues but hey it's free. Can't complain.",
          "score": 1,
          "created_utc": "2026-02-17 23:52:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nc971",
          "author": "Bob5k",
          "text": "also have in mind that minimax has released a guaranteed 100tps m2.5 instances / plans, which also fall under the 10% [reflink promo](https://platform.minimax.io/subscribe/coding-plan?code=HO46LCwAJ5&source=link)   \nfaster iteration means more work done even if quality is 5% lower.",
          "score": -1,
          "created_utc": "2026-02-16 07:46:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r27hm2",
      "title": "GLM-5 is now on OpenCode (via Z.ai coding plan)",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r27hm2/glm5_is_now_on_opencode_via_zai_coding_plan/",
      "author": "jpcaparas",
      "created_utc": "2026-02-11 19:46:29",
      "score": 79,
      "num_comments": 28,
      "upvote_ratio": 0.96,
      "text": "https://preview.redd.it/5pstp85z5xig1.png?width=599&format=png&auto=webp&s=400616601878804d681c97da7fd1c4fbd8c6a48d\n\nRun \\`opencode models --refresh\\`\n\nHN thread: [https://news.ycombinator.com/item?id=46974853](https://news.ycombinator.com/item?id=46974853)\n\nWriteup: [https://extended.reading.sh/glm-5](https://extended.reading.sh/glm-5)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r27hm2/glm5_is_now_on_opencode_via_zai_coding_plan/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o4uw05c",
          "author": "jpcaparas",
          "text": "I'll post some amateur feedback here once I've used it for a bit. Key comparison would be against GLM 4.7 ðŸŒ. I'm mostly interested about speed, tool-calling efficacy, and subagent orchestration.",
          "score": 10,
          "created_utc": "2026-02-11 19:59:05",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4wujae",
              "author": "jpcaparas",
              "text": "My honest thoughts after a few hours of usage:\n\n1. Tool-calling efficacy: at par with K2.5 and Opus 4.6. Doesn't miss. This fucker is smart.\n2. Subagent orchestration: After disabling a couple of MCP servers, it performed well, so I think it *does* struggle quite a bit with middle-of-the-road context bloat. Note that I almost always exhaust my context usage at the end of a session due to heavy research tasks.\n3. Inference: (I'm on ultra, so YMMV), Almost at par with Kimi K2.5 on Synthetic. Not blazing fast, but definitely an improvement over GLM 4.7 on Z.ai.\n\nIf you are keen to try it out, please check out the writeup above first.",
              "score": 11,
              "created_utc": "2026-02-12 02:16:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4xdqg9",
                  "author": "keroro7128",
                  "text": "sorry What is Kimi 4.5ï¼Ÿ",
                  "score": 1,
                  "created_utc": "2026-02-12 04:17:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4uy7b8",
          "author": "Evening-Piglet-7471",
          "text": "rate limitâ€¦.",
          "score": 8,
          "created_utc": "2026-02-11 20:09:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xzrbd",
          "author": "Lpaydat",
          "text": "Thank you bro. I just realized that they drop glm 5 by this post. I can finally use my ultra plan now after leaving it idle for months ðŸ˜†",
          "score": 3,
          "created_utc": "2026-02-12 07:19:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4y06gm",
              "author": "jpcaparas",
              "text": "Oh you'll love GLM-5, you betcha. GLM-4.7 on [Z.ai](http://Z.ai) was such a letdown.",
              "score": 2,
              "created_utc": "2026-02-12 07:23:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o529j0d",
                  "author": "Lpaydat",
                  "text": "It's amazing. GLM4.7 just barely worked for me. But this 5.0 is on another level. I haven't used it for coding tasks yet but reasoning tasks bring me really good results.",
                  "score": 1,
                  "created_utc": "2026-02-12 22:23:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4uynht",
          "author": "jpcaparas",
          "text": "https://preview.redd.it/iz5oaefiaxig1.png?width=1320&format=png&auto=webp&s=19c02cc867568398608906f0aeaefdfedd7d4907\n\nHoly shit it's so bad with subagent orchestration lmao. Even GLM 4.7 wasn't this bad.\n\nFor context, I'm having it do deep research. I'm on the Ultra plan btw.",
          "score": 5,
          "created_utc": "2026-02-11 20:11:53",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4v0hzv",
              "author": "jpcaparas",
              "text": "https://preview.redd.it/vvdxzlm4cxig1.png?width=1384&format=png&auto=webp&s=7d76e99cda91266004a47d8ff4559bc27ec0a7d1\n\nGood reasoning and fact-checking skills.",
              "score": 4,
              "created_utc": "2026-02-11 20:20:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4yq5w8",
                  "author": "Living_Tax1592",
                  "text": "how have you found its context compaction and rot handling? i use ohmyopencode with op4.6 on max  and that context gets ripped through but its compaction and ability to mitigate rot is miles better than 4.5",
                  "score": 1,
                  "created_utc": "2026-02-12 11:30:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4ypowh",
              "author": "Living_Tax1592",
              "text": "have you tried this again after a prompt to \"be more fucking patient\"?",
              "score": 1,
              "created_utc": "2026-02-12 11:26:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4z5ga0",
          "author": "Ai_Pirates",
          "text": "But only max coding plan",
          "score": 2,
          "created_utc": "2026-02-12 13:19:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vwnto",
          "author": "TwisTedUK",
          "text": "Used it via NanoGPT and god damn is it slow",
          "score": 1,
          "created_utc": "2026-02-11 22:58:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4vxysv",
              "author": "jpcaparas",
              "text": "maybe because I'm on glm ultra I get peak male LLM inference?",
              "score": 1,
              "created_utc": "2026-02-11 23:05:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4xp37y",
                  "author": "xmnstr",
                  "text": "Peak male?!",
                  "score": 2,
                  "created_utc": "2026-02-12 05:44:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4vsfyt",
          "author": "SynapticStreamer",
          "text": "Anyone literally unable to get it to work? I keep getting \"rate limit reached.\"\n\nWow, never-mind. Looks like the coding plan literally doesn't even work with it: \"Only supports GLM-4.7 and historical text models\" despite being informed when I got the damn thing that new models would be included.",
          "score": 1,
          "created_utc": "2026-02-11 22:37:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4w7roh",
              "author": "Illustrious-Many-782",
              "text": "Agreed. Pretty crappy. I realize the cost is almost double, so just give different limits for glm-5 ... Problem solved.",
              "score": 3,
              "created_utc": "2026-02-12 00:00:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4wo4qm",
                  "author": "SynapticStreamer",
                  "text": "This seems reasonable. Like, I can't even access the free tier with my token? Like wtf.",
                  "score": 2,
                  "created_utc": "2026-02-12 01:37:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wmvqd",
              "author": "Outrageous-Fan-2775",
              "text": "I'm on the coding plan and I've been using GLM 5 for 3-4 hours now with no rate limits. Could be a tier difference though.",
              "score": 2,
              "created_utc": "2026-02-12 01:30:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4wo1hb",
                  "author": "SynapticStreamer",
                  "text": "Likely. I'm on the cheap ass one.",
                  "score": 2,
                  "created_utc": "2026-02-12 01:37:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4uvc0e",
          "author": "Fearless-Elephant-81",
          "text": "When is synthetic gonna add it :3",
          "score": 0,
          "created_utc": "2026-02-11 19:55:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uvo4d",
              "author": "jpcaparas",
              "text": "I suggest joining their Discord to get the latest updates. It's a great community. ",
              "score": 2,
              "created_utc": "2026-02-11 19:57:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4vbpih",
                  "author": "ahmetegesel",
                  "text": "why downvoted tho lol",
                  "score": 1,
                  "created_utc": "2026-02-11 21:15:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4uv1tk",
          "author": "HarjjotSinghh",
          "text": "that's exactly what i needed: open-source pain in a cli",
          "score": -13,
          "created_utc": "2026-02-11 19:54:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4mzch",
      "title": "OpenCode Zen is dead, but MiniMax M2.5 is the ultimate Opus replacement",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r4mzch/opencode_zen_is_dead_but_minimax_m25_is_the/",
      "author": "pipubx",
      "created_utc": "2026-02-14 15:09:29",
      "score": 58,
      "num_comments": 75,
      "upvote_ratio": 0.74,
      "text": "Everyone is mourning the free version of OpenCode Zen, but the real play is moving to MiniMax M2.5. It's the most reliable alternative to Opus I've found. It's a Real World Coworker that costs $1 an hour and hits SOTA benchmarks (80.2% SWE-Bench). I've seen people complain about M2.1 fixing linting instead of errors, but M2.5 is a massive upgrade in task decomposition. If you want the cheapest, most accurate model for your CLI, this is it. Their RL tech blog is a must-read for anyone looking to optimize their dev workflow.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4mzch/opencode_zen_is_dead_but_minimax_m25_is_the/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5du9el",
          "author": "mintybadgerme",
          "text": "In my, admittedly limited tests, Kimi 2.5 is both cheaper and better at the moment.",
          "score": 22,
          "created_utc": "2026-02-14 18:56:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fyh9j",
              "author": "ideadude",
              "text": "Same m2.5 keeps running into issues it could get around if it slowed down and thought things through, but it's deciding to just rewrite things that are out of scope. Maybe folks who start from scratch with it have better outcomes, but i have to have other models clean up for it when it breaks shit.",
              "score": 4,
              "created_utc": "2026-02-15 02:20:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5hafba",
                  "author": "mintybadgerme",
                  "text": "Not to mention that M 2.5 is a little bit more expensive than Kimi 2.5. Which makes quite a difference if you're doing a fairly complex project. I get some quite Sonnet vibes out of Kimi.",
                  "score": 1,
                  "created_utc": "2026-02-15 08:57:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5gefng",
              "author": "oulu2006",
              "text": "Same",
              "score": 3,
              "created_utc": "2026-02-15 04:13:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5vn1m5",
              "author": "East-Stranger8599",
              "text": "Kimi 2.5 is great, but hallucinates badly without proper context",
              "score": 1,
              "created_utc": "2026-02-17 15:20:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5x196g",
                  "author": "mintybadgerme",
                  "text": "I've found that if you keep things short and sweet, it works very well.",
                  "score": 1,
                  "created_utc": "2026-02-17 19:22:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5d1bzt",
          "author": "Big-Masterpiece-9581",
          "text": "Why is it dead?",
          "score": 13,
          "created_utc": "2026-02-14 16:32:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fp0qp",
              "author": "No_Success3928",
              "text": "Its not, OP is being dramatic ðŸ˜‚",
              "score": 11,
              "created_utc": "2026-02-15 01:17:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5cy4ip",
          "author": "DRBragg",
          "text": "Wait, what happened to opencode zen?",
          "score": 10,
          "created_utc": "2026-02-14 16:16:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d021x",
              "author": "touristtam",
              "text": "No idea the pricing page still list free models: https://opencode.ai/docs/zen#pricing",
              "score": 9,
              "created_utc": "2026-02-14 16:25:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5d56kq",
                  "author": "UseHopeful8146",
                  "text": "If I had to guess they are (or did) rotating models. The free subs change every month or so. At least that was my understanding.",
                  "score": 8,
                  "created_utc": "2026-02-14 16:51:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5np6ch",
                  "author": "sudoer777_",
                  "text": "Apparently OpenCode Zen rate limits them now (not the provider), or at least they are for Kimi K2.5",
                  "score": 1,
                  "created_utc": "2026-02-16 09:49:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ev22f",
          "author": "_Turd_Reich",
          "text": "Another clickbait title.",
          "score": 8,
          "created_utc": "2026-02-14 22:14:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cp71p",
          "author": "Specialist-Yard3699",
          "text": "Maybe not Opus, but itâ€™s really good.\nCancel kimi25 subs, and use only minimax+glm now.",
          "score": 7,
          "created_utc": "2026-02-14 15:30:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5es5zv",
              "author": "skewbed",
              "text": "I would avoid subscribing to inference providers. Just use OpenRouter or something similar like OpenCode Zen.",
              "score": 3,
              "created_utc": "2026-02-14 21:58:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5exg2l",
                  "author": "pires1995",
                  "text": "The [nano-gpt](https://nano-gpt.com/r/kVxQFNRB) is a great option for it. The plan is USD 8 and have almost all open-source models (Kimi, GLM, Minimax). I notice some models not working or taking too long, but for the price worth try it. ",
                  "score": 5,
                  "created_utc": "2026-02-14 22:28:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5f8ui9",
                  "author": "Unlikely_Word_5607",
                  "text": "Isn't the whole point of subscribing to inference providers that they subsidise the costs compared to using the API?",
                  "score": 1,
                  "created_utc": "2026-02-14 23:36:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ehi4t",
          "author": "KnifeFed",
          "text": "> Everyone is mourning the free version of OpenCode Zen\n\ntf are you talking about?",
          "score": 3,
          "created_utc": "2026-02-14 21:00:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fknyu",
          "author": "robberviet",
          "text": "It's great for its size (200b). Not Opus or GPT level but good enough.\nAlso I think you should look at swe-rebench, not swe-bench.",
          "score": 3,
          "created_utc": "2026-02-15 00:49:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cud30",
          "author": "benzflow",
          "text": "How does it compare with Kimi k2.5 and GLM 5?",
          "score": 2,
          "created_utc": "2026-02-14 15:56:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5du3dx",
              "author": "mintybadgerme",
              "text": "Kimi 2.5  is better in my tests.",
              "score": 8,
              "created_utc": "2026-02-14 18:55:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5drkc1",
          "author": "Comrade-Porcupine",
          "text": "I like these open models but I fail to see how $1/hour is better value e.g. the $200/month Codex membership which is basically fully unlimited value.\n\nEthically, yes. And for strictly **API** uses, yes.  I use DeepSeek and others using API tokens and they're dirt cheap and quite effective. But the *coding plans* from GLM and MiniMax and Moonshot are not that awesome of value.",
          "score": 2,
          "created_utc": "2026-02-14 18:43:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fpd75",
              "author": "No_Success3928",
              "text": "Codex Fully unlimited? Not even close.",
              "score": 2,
              "created_utc": "2026-02-15 01:19:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e5u8o",
          "author": "Crafty_Chart1694",
          "text": "until deepseek 4 comes out",
          "score": 2,
          "created_utc": "2026-02-14 19:56:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e91mb",
          "author": "soul105",
          "text": "Kimi K2.5 is still free and available for me",
          "score": 2,
          "created_utc": "2026-02-14 20:14:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5gevds",
              "author": "Wildnimal",
              "text": "Free where?",
              "score": 0,
              "created_utc": "2026-02-15 04:17:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5h9hka",
                  "author": "soul105",
                  "text": "https://preview.redd.it/2usu5t91gmjg1.png?width=544&format=png&auto=webp&s=dee2668095d144a0627e96ba20f32ff7d59cbf81\n\nYou can also check the current list of [free models](https://opencode.ai/docs/zen/#pricing).",
                  "score": 1,
                  "created_utc": "2026-02-15 08:48:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5jhoni",
          "author": "amri2k",
          "text": "kimi 2.5 > minimax 2.5",
          "score": 2,
          "created_utc": "2026-02-15 17:40:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cn1sa",
          "author": "HarjjotSinghh",
          "text": "this m2.5 is basically code's new gym rat - cheap, brutal efficiency.",
          "score": 3,
          "created_utc": "2026-02-14 15:18:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cod4i",
          "author": "idkwtftbhmeh",
          "text": "Minimax M2.5 Falls behind both Kimi K2.5 and GLM5 in every bench, hell even glm7 is in front, trully disappointed with the model",
          "score": 6,
          "created_utc": "2026-02-14 15:25:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f12cv",
              "author": "DinoAmino",
              "text": "Disappointed that a 230B model doesn't score better than models that are 3x and 4x larger? srsly? That's some wildly unrealistic expectations there.",
              "score": 1,
              "created_utc": "2026-02-14 22:49:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5g6hza",
                  "author": "idkwtftbhmeh",
                  "text": "well, I did create my expectations out of the benchs that they announced, which in theory would surpass these models in some cases (doesn't happen)",
                  "score": 1,
                  "created_utc": "2026-02-15 03:15:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5f9ean",
              "author": "Squale279",
              "text": "Bench isnâ€™t the best way to evaluate a llm, try it in real use cases and compare it with other products.",
              "score": 1,
              "created_utc": "2026-02-14 23:39:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5g6ev6",
                  "author": "idkwtftbhmeh",
                  "text": "oh I did, it's quite bad overall to be honest, the speed is great tho",
                  "score": 1,
                  "created_utc": "2026-02-15 03:15:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5d59d3",
              "author": "UseHopeful8146",
              "text": "Iâ€™m sorry, glm 7?",
              "score": 1,
              "created_utc": "2026-02-14 16:51:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5d8g22",
                  "author": "zuk987",
                  "text": "He probably meant 4.7",
                  "score": 4,
                  "created_utc": "2026-02-14 17:07:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5esxea",
              "author": "cri10095",
              "text": "M2.5 is much smaller then the other models",
              "score": 1,
              "created_utc": "2026-02-14 22:02:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5eybxo",
                  "author": "idkwtftbhmeh",
                  "text": "It is indeed, still disappointed, I saw the blog post and benchs and it seems VERY cherrypicked compared to individual researchers like swe-rebench",
                  "score": 2,
                  "created_utc": "2026-02-14 22:33:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5d3swd",
          "author": "touristtam",
          "text": "> Their RL tech blog is a must-read for anyone looking to optimize their dev workflow.\n\nLink please?",
          "score": 3,
          "created_utc": "2026-02-14 16:44:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dmo8h",
          "author": "Both_Ad2330",
          "text": "Hope this gets on AWS Bedrock soon.",
          "score": 1,
          "created_utc": "2026-02-14 18:19:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5f30f1",
          "author": "Moist_Associate_7061",
          "text": "i used minimax 2.5 all day long, and it was not even close kimi k2.5. babysitting is needed..",
          "score": 1,
          "created_utc": "2026-02-14 23:00:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f6o33",
              "author": "johnerp",
              "text": "Which one is better, Iâ€™m not clear.",
              "score": 4,
              "created_utc": "2026-02-14 23:22:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5gzjg9",
          "author": "XtoddscottX",
          "text": "Can it work with images? Cause yeah, if you need to generate simple code these models are okay, but for some frontend tasks itâ€™s better to use model that accept visual input too, and as I know these Chinese models donâ€™t whilst three American big models do.",
          "score": 1,
          "created_utc": "2026-02-15 07:12:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h20vn",
          "author": "wjjia",
          "text": "Honestly, it was about time we stopped relying on OpenCode Zen anyway. Everyone is freaking out over the shutdown, but it was a loss leader from day one. I haven't put M2.5 through the wringer yet, but if that 80.2% SWE-Bench score actually holds up in real-world messy codebases, it's a massive jump. Most of these models talk a big game and then fail the moment you hit a weird dependency issue.",
          "score": 1,
          "created_utc": "2026-02-15 07:35:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h94py",
          "author": "Relative-Honey-4485",
          "text": "The jump from 2.1 to 2.5 is the real conversation here. 2.1 was driving me insane with that linting obsession - fixing my tabs while the actual logic was still broken. If the task decomposition is actually improved, I might give it a shot. Still skeptical about the $1/hr claim though, there is always a catch with token windows.",
          "score": 1,
          "created_utc": "2026-02-15 08:44:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h9e2k",
          "author": "Capital_Standard4603",
          "text": "RIP OpenCode Zen. It was good while it lasted.",
          "score": 1,
          "created_utc": "2026-02-15 08:47:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hdefr",
          "author": "elaytot",
          "text": "Minimax m2.5 is not better! Cant even tell my project was in typescript after it reviewed the whole codebase.. got me bunch of typeerrors",
          "score": 1,
          "created_utc": "2026-02-15 09:26:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hgw56",
          "author": "Yukeyii",
          "text": "Did anyone actually read the RL tech blog OP mentioned? I just skimmed it and the way they are handling reinforcement learning is actually pretty clever if you are into the infra side of things. It explains why the task breakdown feels more \"human\" than the older versions.",
          "score": 1,
          "created_utc": "2026-02-15 10:00:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5llki0",
              "author": "touristtam",
              "text": "Do you have a link, I have no idea what is the RL tech blog that is being mentioned.\n\nIs that: https://www.minimax.io/news/forge-scalable-agent-rl-framework-and-algorithm ?",
              "score": 1,
              "created_utc": "2026-02-16 00:16:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5i21vo",
          "author": "LionelOOK",
          "text": "\"Opus replacement\" is a bold claim. Opus has that specific feel for creative logic that is hard to replicate, but for pure CLI work and bug fixing, I can see MiniMax taking that spot if it is really that cheap.",
          "score": 1,
          "created_utc": "2026-02-15 13:05:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i4otx",
          "author": "Feeling-Whole4574",
          "text": "$1 an hour? I will believe it when I see my invoice at the end of the month.",
          "score": 1,
          "created_utc": "2026-02-15 13:23:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i89ww",
          "author": "Virtual-Path1704",
          "text": "Glad I am not the only one who noticed the linting thing. M2.1 would spend half its energy fixing my indentation instead of actually solving the logic error I was pointing at. If 2.5 fixed that, it is worth the switch.",
          "score": 1,
          "created_utc": "2026-02-15 13:46:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i9i1h",
          "author": "linegel",
          "text": "Their SWE bench is basically fake news due to too heavy reliance on Anthrophic models \n\nCheck updated SWE bench",
          "score": 1,
          "created_utc": "2026-02-15 13:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ib39u",
          "author": "Icy_Net5151",
          "text": "Benchmark obsession needs to stop. SWE-Bench is one thing, but how does it handle a 10-year-old legacy codebase with zero documentation? That is the real test for any \"coworker\" model.",
          "score": 1,
          "created_utc": "2026-02-15 14:03:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5id9c8",
          "author": "ChanningACE",
          "text": "Just switched. It is definitely snappier than 2.1. Not sure if it is \"ultimate\" yet, but it is actually usable for once.",
          "score": 1,
          "created_utc": "2026-02-15 14:15:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ifoi1",
          "author": "Dantenmd",
          "text": "Been looking for a solid Opus alternative since the quality started dipping recently. I will check out that blog post later, thanks for the heads up.",
          "score": 1,
          "created_utc": "2026-02-15 14:29:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vmx38",
          "author": "East-Stranger8599",
          "text": "This is an overstatement, at max it may be weaker cousin of Sonnet 4.5",
          "score": 1,
          "created_utc": "2026-02-17 15:19:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yien6",
          "author": "Conscious-Hair-5265",
          "text": "They gamed the bencharks, MiniMax 2.5 is not as impressive in real life usecases. Check out swe re bench bench mark",
          "score": 1,
          "created_utc": "2026-02-17 23:43:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cpuqt",
          "author": "0Bitz",
          "text": "How well does it work with Oh-My opencodeâ€¦?",
          "score": 1,
          "created_utc": "2026-02-14 15:33:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d61rc",
              "author": "UseHopeful8146",
              "text": "In my experience OmO has the structure to make most of the reasoning relatively simple - you could probably get close to kimi/glm level execution with much smaller models, provided they have tool calling support and decent context window.\n\nIâ€™m still in the process of working on tooling and stuff, but testing for local model execution in Opcode/OmO is on my todo list specifically because I hold that theory at present.",
              "score": 3,
              "created_utc": "2026-02-14 16:55:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5szmkm",
                  "author": "0Bitz",
                  "text": "I tested this out and found it making too many bugs even with detailed prompts of an existing system. GLM seems to work better on my code base at least",
                  "score": 1,
                  "created_utc": "2026-02-17 03:30:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r2psy1",
      "title": "I just got banned from gemini :)",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r2psy1/i_just_got_banned_from_gemini/",
      "author": "Eastern-Guess-1187",
      "created_utc": "2026-02-12 10:17:46",
      "score": 43,
      "num_comments": 73,
      "upvote_ratio": 0.95,
      "text": "I know that is something that they warn. :) I am afraid of being banned from claude too.. so I just use gpt 5.3 codex in opencode now. which models should I use now? and what's your workflow? I am using omo slim.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r2psy1/i_just_got_banned_from_gemini/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o50b3fa",
          "author": "xmnstr",
          "text": "Happened to me too. If you're in the EU please file a DMA/DSA complaint, not allowing third party access to Antigravity is likely a breach of EU law. Additionally, just banning people without any notice and no method for appeal is also a very likely breach of EU consumer laws.\n\nThey are under investigation for this behavior and now is the perfect time to provide more information about their anticompetitive and anticonsumer actions!",
          "score": 15,
          "created_utc": "2026-02-12 16:49:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50n0ah",
              "author": "Villain_99",
              "text": "Whatâ€™s the process ?",
              "score": 5,
              "created_utc": "2026-02-12 17:46:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50tjtt",
                  "author": "xmnstr",
                  "text": "I'm not quite sure yet, but looking into it.",
                  "score": 2,
                  "created_utc": "2026-02-12 18:16:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4yk31e",
          "author": "Skquark",
          "text": "I also got banned from Gemini today, didn't expect that. I've been using the opencode-antigravity-auth plug-in to use Claude with Antigravity because I've been running out of my Claude Max 20x weekly limit way too quickly, and I've been paying for Gemini ultra for $250 a month which I hardly used because I just didn't like Gemini 3 Pro nearly as much as I expected, so I thought it was safe to supplement my usage and take advantage of that subscription. I'm writing a letter of appeal to Google in hopes that they might grant me a pardon, but I don't expect much.\n\nI would cancel my Gemini subscription and pay Anthropic $400 a month instead if I could so I don't have to worry about running out of my Opus limits, but that's not even an option. Apparently it's against their terms of service to have two Max accounts, which is really lame. Now I'm being forced to use codex to supplement my addiction, but it's just not satisfying my itch. I guess I have to lean more on Kimi K2.5 and now GLM-5 as my fallbacks. What's a full-time independent developer to do? I've been going all in to take my commercial project I've been working on for so long to completion, can't slow down now...",
          "score": 6,
          "created_utc": "2026-02-12 10:37:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zqril",
              "author": "bigh-aus",
              "text": "At $1000 every 4 months, I'd consider trying out kimi k2.5 on cloud, and if it's good for your use case drop the money on a mac studio.",
              "score": 8,
              "created_utc": "2026-02-12 15:14:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4zyo86",
              "author": "HistorianIll5959",
              "text": "Canâ€™t get a Max account under friend or family members name and just use your card?",
              "score": 1,
              "created_utc": "2026-02-12 15:52:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50hmbo",
              "author": "HotRelationship1127",
              "text": "I wonder how they knew to ban you. Can you post whatever email announcement they made to you? Maybe there are some clues.",
              "score": 1,
              "created_utc": "2026-02-12 17:20:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55qmdx",
                  "author": "powerfulparadox",
                  "text": "If the prompts and headers from the harness aren't identical, it can be as simple as comparing against expected incoming information. After that, I'm sure there can be behavioral differences that are observable.",
                  "score": 1,
                  "created_utc": "2026-02-13 13:18:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o538y8b",
              "author": "DeathShot7777",
              "text": "Antigravity auth wasnt working for claude models last time i checked. Does it work now? I tried it around a week ago",
              "score": 1,
              "created_utc": "2026-02-13 01:46:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o54cewn",
              "author": "ZeroBraneZ",
              "text": "Brother how TF are you running out of Claude max 20xâ€¦. Like thatâ€™s the first place Iâ€™d look..",
              "score": 1,
              "created_utc": "2026-02-13 06:16:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o561mau",
                  "author": "Skquark",
                  "text": "Dude, I use up 90% of my weekly 20x in about 3-4 days, especially now with Opus 4.6... I'm full time, running 2-4 agents on my giant projects simultaneously, and would be doing more but trying to pace myself and conserve my tokens.. Wish I could tell ya the projects I'm working on obsessively, but keeping my mouth shut until my commercial launches.. Also using Codex Max and Kimi K2.5 at the same time on different sections, but Claude is my main man..",
                  "score": 1,
                  "created_utc": "2026-02-13 14:18:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o59erag",
              "author": "forcaster89",
              "text": "What makes it worth paying 400$ for ai models? Working on multiple projects with almost no revenue , thanks",
              "score": 1,
              "created_utc": "2026-02-14 00:31:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5atwqk",
                  "author": "Skquark",
                  "text": "You're not paying just for AI models, you're paying for an employee with high-level skills that does what you ask and doesn't complain. When you're working full-time with them and break down the cost, you're basically paying a team of super geniuses around $2 an hour. Even if you don't see revenue now, I believe if you keep at it and take it seriously, it pays off in the end, but not if you treat it only as a hobby or curiosity. For a developer that is committed, it's an easily justified cost of business...",
                  "score": 2,
                  "created_utc": "2026-02-14 06:29:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5fqi06",
              "author": "GarageExtreme",
              "text": "You can open two antrhopic accounts and just switch when you fill the quota on one, that's not against ToS",
              "score": 1,
              "created_utc": "2026-02-15 01:26:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ymx96",
              "author": "Desperate-Bath5208",
              "text": "Were you banned for using the Gemini Opencode Plugin or the Antigravity Opencode Plugin?",
              "score": 1,
              "created_utc": "2026-02-12 11:02:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ys7u3",
                  "author": "Skquark",
                  "text": "It was for using the Antigravity opencode plug-in and using it primarily to access Opus 4.6... I also had the Gemini open code plug-in, but I never really used that since I had the Gemini CLI that I could use anyways, but that didn't let you use anthropic sadly, and I caught Gemini 3 way too many times screwing up my code and not understanding what it was doing in my monorepo. I liked the opencode interface better than the Antigravity IDE, especially because it wasn't maintaining a reliable connection with my SSH to work on through their application. Do they really have to be so strict about their terms of services? Seems territorial...",
                  "score": 3,
                  "created_utc": "2026-02-12 11:47:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ytm7o",
                  "author": "Eastern-Guess-1187",
                  "text": "I was using both. ",
                  "score": -1,
                  "created_utc": "2026-02-12 11:58:53",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4yu25c",
              "author": "Eastern-Guess-1187",
              "text": "yeah I am using codex gpt 5.3 but its not like gemini 3 or opus 4.6. it feels it's dumber ",
              "score": 0,
              "created_utc": "2026-02-12 12:02:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4zdm43",
                  "author": "Character_Cod8971",
                  "text": "How can GPT-5.3-Codex be dumber than Gemini 3 or Opus 4.6?",
                  "score": 2,
                  "created_utc": "2026-02-12 14:05:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4yltbg",
              "author": "aitorserra",
              "text": "Why they banned you?",
              "score": 0,
              "created_utc": "2026-02-12 10:52:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ysdq1",
                  "author": "Skquark",
                  "text": "Because apparently they don't like it when you use third-party software for accessing their services unless you're using API key and not your subscription... Same with Anthropic.",
                  "score": 4,
                  "created_utc": "2026-02-12 11:49:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4ywwjz",
              "author": "Key_Mousse_8034",
              "text": "BTW how are you gonna switch between claude accounts? Because I'm thinking of doing the same. Just 2 claude subscriptions ðŸ˜€",
              "score": 0,
              "created_utc": "2026-02-12 12:23:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55t4pu",
                  "author": "unnamedb",
                  "text": "try cc switch",
                  "score": 1,
                  "created_utc": "2026-02-13 13:32:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o58irwn",
                  "author": "Disillusioned_Sleepr",
                  "text": "Cli use shell variables",
                  "score": 1,
                  "created_utc": "2026-02-13 21:34:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ymxf4",
          "author": "Desperate-Bath5208",
          "text": "Were you banned for using the Gemini Opencode Plugin or the Antigravity Opencode Plugin?",
          "score": 2,
          "created_utc": "2026-02-12 11:03:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ytnk0",
              "author": "Eastern-Guess-1187",
              "text": "I was using both bro",
              "score": 3,
              "created_utc": "2026-02-12 11:59:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o52ler8",
          "author": "dyzhdyzh",
          "text": "Got banned too. The only reason I got their subscription was to use their Opus with OpenCode.\nI have Copilot subscription from work, but the context window is crippled on the most models, and I exhaust the non-premium quota in a few days of coding.\nI guess I'll stick to Synthetic's models then. Kimi K2.5 is fairly good. As many others said, somewhere between Sonnet and Opus. Also, GLM-5 will be available soon.",
          "score": 1,
          "created_utc": "2026-02-12 23:27:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54i48f",
              "author": "YayaBruno",
              "text": "I've been using Kimi K2.5 via Nano-GPT, but it's quite slow, How do you see its speed at Synthetic? I've been considering switching to it. Also, how do you find the rate limits? Are they too restrictive?",
              "score": 1,
              "created_utc": "2026-02-13 07:05:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bpvyi",
          "author": "HarjjotSinghh",
          "text": "this is the new gemini game show.",
          "score": 1,
          "created_utc": "2026-02-14 11:36:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5djlru",
          "author": "Euphoric-Doughnut538",
          "text": "Everyone needs to perform charge backs since they are banning the API",
          "score": 1,
          "created_utc": "2026-02-14 18:03:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dtk57",
          "author": "layer4down",
          "text": "This is my nightmare. Everyone asks why I donâ€™t use Opus-4.6 or other SaaS models? Thatâ€™s why. I mean they donâ€™t have a SMB tier offering for power user devs grinding out projects like this?",
          "score": 1,
          "created_utc": "2026-02-14 18:53:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j2pre",
          "author": "evilissimo",
          "text": "Reading this thread, I am so glad that I stopped doing that entirely. I just use my pro account within AG, Jules and Gemini cli and when I need more opus I use my copilot plan on 10$. For OC I have Kimi, MiniMax and GLM coding plans and codex plus which with all of it I canâ€™t use that much at the moment as I am switching around. But I feel like I get fair value out of all my subscriptions.\nAlso using oc zen and open router free models.\n\nI donâ€™t manage to run out of quota anywhere",
          "score": 1,
          "created_utc": "2026-02-15 16:27:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ytjwh",
          "author": "krimpenrik",
          "text": "Use GitHub copilot access to multiple models",
          "score": 1,
          "created_utc": "2026-02-12 11:58:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51wi6z",
              "author": "aydgn",
              "text": "Are you using that way?",
              "score": 1,
              "created_utc": "2026-02-12 21:21:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o54xy8a",
              "author": "oVerde",
              "text": "With only 128k of context window",
              "score": 1,
              "created_utc": "2026-02-13 09:32:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o57xcjf",
              "author": "BoThatch",
              "text": "I've read that Copilot still has issues with opencode in terms of preium request charge. Can you confirm that?\nI want to use Copilot, too. But I'm afraid that my budget will get eaten in no time with any subagent flow/tool calls, when opencode does not set the headers etc correctly.\nThere are still open [issues](https://github.com/anomalyco/opencode/issues/8030) regarding that.",
              "score": 1,
              "created_utc": "2026-02-13 19:47:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dtrm7",
                  "author": "Dazzling-Solution173",
                  "text": "There has been workarounds that are said in the github copilot reddit, basically since they use subagents to gather context it's basically another premium request, however u can just set it up for tool calls or subagents to use the free 0x models to do it while the main agent stays to what you choose.",
                  "score": 1,
                  "created_utc": "2026-02-14 18:54:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4yivea",
          "author": "Sukkii",
          "text": "I did too, yesterday, and am also keen for recommendations. Going the API route (e.g. open router) is just too steep, what other coding plans are available?",
          "score": 1,
          "created_utc": "2026-02-12 10:25:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zdww5",
              "author": "Character_Cod8971",
              "text": "Were you banned for using the Gemini Opencode Plugin or the Antigravity Opencode Plugin?",
              "score": 0,
              "created_utc": "2026-02-12 14:07:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zneni",
                  "author": "Sukkii",
                  "text": "Using opencode-antigravity-auth",
                  "score": 3,
                  "created_utc": "2026-02-12 14:57:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4yr9gu",
          "author": "t1maccapp",
          "text": "Same:\n\nGemini has been disabled in this account for violation of Terms of\n    Service. If you believe this is an error, please contact Google Cloud Support, or email\n    gemini-code-assist-user-feedback@google.com.",
          "score": 1,
          "created_utc": "2026-02-12 11:40:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yt979",
              "author": "benchb",
              "text": "can you explain, which plug in you used and what did you do ?",
              "score": 1,
              "created_utc": "2026-02-12 11:56:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o519q3t",
                  "author": "t1maccapp",
                  "text": "Was using https://github.com/NoeFabris/opencode-antigravity-auth in a pretty chill manner. Like 5-10 requests per day maybe. Both gemini 3 pro and opus 4.5.\n\nI've managed to use opus 4.6 couple of times, until I got banned yesterday.\n\nI don't think I've ever reached the quota limit or even used half of it.",
                  "score": 1,
                  "created_utc": "2026-02-12 19:32:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4z0anv",
              "author": "Eastern-Guess-1187",
              "text": "Emailed and I am waiting for the result",
              "score": 1,
              "created_utc": "2026-02-12 12:46:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o51ae2u",
                  "author": "t1maccapp",
                  "text": "Same, said I'm sorry, won't do ever again. \n\nNot like I care much though, I don't see me leaving opencode, will just probly use Chinese models.",
                  "score": 0,
                  "created_utc": "2026-02-12 19:35:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4zdoxj",
              "author": "Character_Cod8971",
              "text": "Were you banned for using the Gemini Opencode Plugin or the Antigravity Opencode Plugin?",
              "score": 1,
              "created_utc": "2026-02-12 14:06:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o519a3b",
                  "author": "t1maccapp",
                  "text": "For this one https://github.com/NoeFabris/opencode-antigravity-auth",
                  "score": 1,
                  "created_utc": "2026-02-12 19:30:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o52xqzy",
          "author": "cutebluedragongirl",
          "text": "Based, fuck Google",
          "score": 0,
          "created_utc": "2026-02-13 00:38:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yq8my",
          "author": "pinklove9",
          "text": "What kind of a ban is this",
          "score": 0,
          "created_utc": "2026-02-12 11:31:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ytriz",
              "author": "Eastern-Guess-1187",
              "text": "only for gemini / antigravity. but I emailed them and I promised that I won't use anything thats 3rd party :D",
              "score": 1,
              "created_utc": "2026-02-12 12:00:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o53ph35",
                  "author": "StrixGGUY",
                  "text": "you are the amin admin of the subscription? if yes simple add another gmail acc, add to your family sharing, add allow google on sharing, you have new access to antigravity -> always use second shared acc for this, if you get banned simple remove from familys haring and add another one easy and fast \n\nhttps://preview.redd.it/i8rnw7qhl6jg1.png?width=1015&format=png&auto=webp&s=40d911905e7da7342de8ebece4fd693ee36e3df1\n\n",
                  "score": 4,
                  "created_utc": "2026-02-13 03:28:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o51p385",
                  "author": "ItsStrike13",
                  "text": "Any response from them?",
                  "score": 1,
                  "created_utc": "2026-02-12 20:45:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o51ycyx",
                  "author": "aydgn",
                  "text": "Can you use [gemini.google.com](http://gemini.google.com) or it is just for Gemini CLI/Antigravity?",
                  "score": 1,
                  "created_utc": "2026-02-12 21:29:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o521yyt",
          "author": "HarjjotSinghh",
          "text": "what's next when gemini bans you, a new ai god named my boss?",
          "score": 0,
          "created_utc": "2026-02-12 21:47:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53ymeh",
          "author": "lundrog",
          "text": "Synthetic.new has a waiting list but be a good option. Dm me for a referral if interested",
          "score": 0,
          "created_utc": "2026-02-13 04:30:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56rl6b",
              "author": "Halfwalker",
              "text": "I hadn't heard of Synthetic before, just took a look. Their SignUp link is there and looks to be working ?  What does a referral get you ?",
              "score": 0,
              "created_utc": "2026-02-13 16:26:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o57kcnk",
                  "author": "lundrog",
                  "text": "We each get credit to use either $10 or $20 off a month",
                  "score": 0,
                  "created_utc": "2026-02-13 18:44:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o58d3x3",
          "author": "SalvadorTMZ",
          "text": "how do you know if you're banned?",
          "score": 0,
          "created_utc": "2026-02-13 21:06:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58swyo",
          "author": "No_Strategy_6034",
          "text": "Can someone actually explain whats happening? I dont understand what got you all banned, idk what the TOS is, all of your comments skip explaining what is the reason of the Ban...\n\nYou cant use gemini / Claude in opencode? Thats it?",
          "score": 0,
          "created_utc": "2026-02-13 22:24:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ynzt5",
          "author": "MegamillionsJackpot",
          "text": "I use Codex + synthetic.new that works nicely.\n\nI think synthetic.new has done some covert marketing on Reddit, so they get some hate for that, but it is still a nice deal. Hopefully, they will add GLM 5.0 soon .\n\nEdit with some news from synthetic:\n\nJust finished setting up our new B200 GPU cluster, where we'll be running `hf:Kimi-K2.5-NVFP4`. The deploy to flip backends is going out now and should be out within the half hour. ðŸ™‚\n\nWe plan to get a version of GLM 5 up for all of you to try soonâ„¢! We plan to move all our infrastructure to (presumably US based) Blackwells in the coming days which should signfiicantly increase our request capacity. ðŸ™‚ \n\n-# You can check when we're back to US only by running `curl \"https://api.synthetic.new/openai/v1/models?provider=synthetic\" | jq '.data[] | select(.id == \"hf:nvidia/Kimi-K2.5-NVFP4\") | {name, datacenters}'` and waiting for `IL` to flip to `US` ðŸ˜›",
          "score": -5,
          "created_utc": "2026-02-12 11:12:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50p0f2",
              "author": "Crinkez",
              "text": "Your standard and pro price point limits are not linear. Stardard users get ripped off.",
              "score": 0,
              "created_utc": "2026-02-12 17:55:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50pga5",
                  "author": "MegamillionsJackpot",
                  "text": "It's not my prices. I'm really not affiliated with the firm. I just use it.",
                  "score": 0,
                  "created_utc": "2026-02-12 17:57:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o50mqz0",
          "author": "Villain_99",
          "text": "Use kimi or the upcoming glm 5",
          "score": -1,
          "created_utc": "2026-02-12 17:44:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4uuiw",
      "title": "Opencode for all!1!1!1!",
      "subreddit": "opencodeCLI",
      "url": "https://v.redd.it/4doqqb8yqijg1",
      "author": "Extension_Armadillo3",
      "created_utc": "2026-02-14 20:21:29",
      "score": 40,
      "num_comments": 5,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4uuiw/opencode_for_all111/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5khton",
          "author": "HarjjotSinghh",
          "text": "wow that's actually genius ceiling tech",
          "score": 1,
          "created_utc": "2026-02-15 20:39:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tqqd8",
          "author": "InternalFarmer2650",
          "text": "Basierter MediaMarkt",
          "score": 1,
          "created_utc": "2026-02-17 06:52:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5eow8q",
          "author": "jpcaparas",
          "text": "at costco atm. dont give me ideas.",
          "score": 0,
          "created_utc": "2026-02-14 21:40:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ec2kn",
          "author": "HarjjotSinghh",
          "text": "this shop's ceiling looks like a tech-themed skylight.",
          "score": -1,
          "created_utc": "2026-02-14 20:30:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5eepto",
          "author": "soul105",
          "text": "Big Pickle at your service!",
          "score": -1,
          "created_utc": "2026-02-14 20:45:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5cdq7",
      "title": "Built a tool to track OpenCode/Claude Code API usage - Anthropic Pro/Max limits, Copilot, and more",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/yyypov9x8njg1.jpeg",
      "author": "prakersh",
      "created_utc": "2026-02-15 11:28:44",
      "score": 32,
      "num_comments": 6,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r5cdq7/built_a_tool_to_track_opencodeclaude_code_api/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5j98i0",
          "author": "landed-gentry-",
          "text": "Will this be useful if I use the same key (e.g., Anthropic) on different machines? Or will it lose track of the bigger picture.",
          "score": 2,
          "created_utc": "2026-02-15 16:58:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j9vx1",
              "author": "prakersh",
              "text": "Yes,\nIt'll work of you use same key across systems. I've separate monitoring linux server and use claude code on my macbook.",
              "score": 1,
              "created_utc": "2026-02-15 17:01:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5os1qw",
          "author": "tamtaradam",
          "text": "funny the ui is similar to something I vibe coded, are the models cooperating? ;)\n\nhttps://preview.redd.it/70mkwhr5avjg1.png?width=850&format=png&auto=webp&s=2e071c4e57c79fccc6bebb9871e5ee3e52ba097f\n\n",
          "score": 1,
          "created_utc": "2026-02-16 14:30:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pcftq",
              "author": "prakersh",
              "text": "I think skills are or maybe models only. Which skill and models you used",
              "score": 1,
              "created_utc": "2026-02-16 16:11:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5pdsda",
                  "author": "tamtaradam",
                  "text": "yeah I think it was opus-4.5 with frontend-design skill",
                  "score": 1,
                  "created_utc": "2026-02-16 16:17:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r3jbfd",
      "title": "All-in-one subscription that gives both strong reasoning + cheap coding models?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r3jbfd/allinone_subscription_that_gives_both_strong/",
      "author": "minhpro279",
      "created_utc": "2026-02-13 07:55:23",
      "score": 28,
      "num_comments": 21,
      "upvote_ratio": 0.92,
      "text": "Iâ€™ve been using OpenCode with Antigravity, but got banned recently and now Iâ€™m looking for a replacement.\n\nMy ideal setup is simple:\none strong model for reasoning/planning,\none cheaper fast model as the workhorse for implementation,\nand preferably under a single subscription since I donâ€™t want to manage multiple subscription.\n\nIâ€™m considering Cursor, Copilot, Chutes, Synthetic, etc., but would love to hear whatâ€™s actually working well in practice.\n\nIâ€™ve heard opencode burn through premium requests quickly on Copilot, while Chutes/Synthetic donâ€™t really offer a strong planning model ( i miss opus TT kimi 2.5 is good, but not there yet. have not used gpt5.3 )\n\nAnyway if youâ€™re in a similar situation, would love to hear your experience. Any recommendations?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r3jbfd/allinone_subscription_that_gives_both_strong/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o55baw1",
          "author": "dengar69",
          "text": "In looking at GitHub Copilot Pro+ for all the closed models, and NanoGPT for all the open ones.  $47 per month for both.",
          "score": 6,
          "created_utc": "2026-02-13 11:33:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56msk5",
              "author": "Desperate-Bath5208",
              "text": "https://www.reddit.com/r/opencodeCLI/comments/1r2fjyn/opencode_vs_github_copilot_cli_huge_credit_usage/\n\nhttps://github.com/anomalyco/opencode/issues/8030\n\nhttps://github.com/anomalyco/opencode/issues/13360",
              "score": 2,
              "created_utc": "2026-02-13 16:03:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o57bm7o",
                  "author": "dengar69",
                  "text": "Thanks.  Looks like Im keeping my OpenAI plan open for now.",
                  "score": 1,
                  "created_utc": "2026-02-13 18:02:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o54u1d9",
          "author": "Bob5k",
          "text": "Have a note synthetic has Kimi k2.5 via Nvidia which is more preformant than any other source for this model.\nAlso have in mind that they have -20$ discount on pro plan with [reflink](https://synthetic.new/?referral=IDyp75aoQpW9YFt).\n\nOn another note tho, minimax M2.5 is pretty damn powerful and fast aswell and it's available across mm coding plans (with [discount](https://platform.minimax.io/subscribe/coding-plan?code=HO46LCwAJ5&source=link) aswell). \nThis or [glm coding plan](https://z.ai/subscribe?ic=CUEFJ9ALMX) are a solid backup plans which are also quite cheap around to get into.",
          "score": 8,
          "created_utc": "2026-02-13 08:55:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55i5k9",
              "author": "pungggi",
              "text": "Is synthetic from Nvidia? Really?",
              "score": -3,
              "created_utc": "2026-02-13 12:24:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56buhu",
                  "author": "mcowger",
                  "text": "No.  \n\nSynthetic has a â€œturboâ€ variant of K2.5 that uses nvidias NVFP4 format for better performance",
                  "score": 3,
                  "created_utc": "2026-02-13 15:10:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o54t6yf",
          "author": "Desperate-Bath5208",
          "text": "z.ai",
          "score": 8,
          "created_utc": "2026-02-13 08:47:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eb1sd",
              "author": "pablonhc",
              "text": "I feel that with use it becomes less intelligent",
              "score": 1,
              "created_utc": "2026-02-14 20:25:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o54pczw",
          "author": "wallapola",
          "text": "I'm not a bot and this is not an ad. Just sharing my experience since I actually use this. Iâ€™m using synthetic mainly because of the promo and so far itâ€™s been noticeably faster than before. I might even stay after the promo expires since their service is a lot better compared to other providers. I feel more secure using it and I no longer want to explore other AI providers because it takes a lot of time and usually requires paying just to try their plans.\n\nThey recently added US-based servers, and one of them is using NVIDIA GPUs. I donâ€™t really understand all the infra details, but performance-wise itâ€™s definitely faster compared to their previous setup. Latency feels a lot better on my end.\n\nOne thing to note is that I think the standard plan is still on a waitlist right now, while the pro plan is available. If anyone wants to double-check, their discord is probably the best place. The devs are active there and they post updates about infra changes, issues and what theyâ€™re working on.\n\nIf you want to try it with the discounted offer:  \n[https://synthetic.new/?referral=4NNoPUXcb63ZYVK](https://synthetic.new/?referral=4NNoPUXcb63ZYVK)\n\nEdit: Based on what Iâ€™ve seen on their discord, theyâ€™re really focusing on improving and stabilizing the service with their new infra setup and servers. They plan to add glm-5 once the infrastructure can handle more users, since adding it too early would definitely flood the service and cause slowdowns.",
          "score": 3,
          "created_utc": "2026-02-13 08:11:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o59dn7g",
              "author": "sudoer777_",
              "text": "What countries where they using before for servers?",
              "score": 1,
              "created_utc": "2026-02-14 00:24:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o55qvhi",
              "author": "disrupted_bln",
              "text": "currently when you sign up for one of their plans there is a waitlist",
              "score": 1,
              "created_utc": "2026-02-13 13:19:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o59lyec",
                  "author": "elllyphant",
                  "text": "Yes there's a waitlist and it'll take a couple more weeks. We'll email those on the waitlist and announce in our [Discord](https://discord.gg/syntheticlab) when we're ready to take on more new subscribers.  \n  \nThe reason is that we want to ensure users get the experience they are paying for so we're taking time to ensure we can scale properly. Thank you so much for your patience in the meantime and I hope you get to try Synthetic soon!",
                  "score": 2,
                  "created_utc": "2026-02-14 01:15:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o56s20l",
          "author": "jorgejhms",
          "text": "Any news about Opencode Black?",
          "score": 1,
          "created_utc": "2026-02-13 16:28:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b9qob",
          "author": "alp82",
          "text": "Windsurf is a very strong contender. You won't get a better token for money count. Lots of models to choose from.\n\nI don't know Chutes and Synthetic though.",
          "score": 1,
          "created_utc": "2026-02-14 08:59:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o551n6g",
          "author": "Putrid-Pair-6194",
          "text": "My setup.\nWorkhorses: Kimi 2.5 from Moonshot, GLM from Z.ai (pro plan).  \nPlanning and validation: GPT 5.2, 5.3 codex via OpenAI team plan.  \n\nNew Fallback: Gemini 3.0 flash via API (free credits)\n\nI just learned about the Gemini API free credits route and set it up so havenâ€™t used extensively yet. Everything else works well",
          "score": 1,
          "created_utc": "2026-02-13 10:07:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o552ayh",
          "author": "Embarrassed_Bread_16",
          "text": "I'm using 20 USD plan from chutes.ai, it allows for making 5k requests daily, the most I can use is 2k when working on many projects at once, it allows for using open source models, like Kimi, glm, minimax\n\n\nIt has drawback that some models might temporarily be over utilized by people and API will become unresponsive and u need to change to other model, but it happened to me only for half an hour yesterday and I'm subscribed for 2 days\n\n\nI also bought minimax coding plan to try out the m2.5, gotta say it is super fast, but haven't used it enough to compare the quality",
          "score": 0,
          "created_utc": "2026-02-13 10:13:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55brpu",
          "author": "keroro7128",
          "text": "High-order model source: GitHub; low-order model source: Minimaz coding plan.",
          "score": 0,
          "created_utc": "2026-02-13 11:37:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5566xz",
          "author": "amba420",
          "text": "I'm using the synthetic new pro plan and Kimi is quite fast the last 2 days now. \n\nI'm using Kimi for planning and glm4.7 mostly for the work horse part. Works good\n\nBut they have a wait-list for new customers if anyone would like here is my referral:\n\nhttps://synthetic.new/?referral=vVOTagHw7nzmm2b",
          "score": -3,
          "created_utc": "2026-02-13 10:49:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54vmme",
          "author": "ScorpionOfWar",
          "text": "Been using open-source models more lately for private stuff, I got the 100$ Claude Sub for work.\n\nEnded up trying [Synthetic](https://synthetic.new/?referral=hcHozzHNE8CxVvz)(\\~50% off) and it's been very solid so far, alternatively [Z.ai](https://z.ai/subscribe?ic=KQ77ZVKLB5)(\\~10% off) for just the GLM Models, nice for coding, but kind of unreliable at the moment. They host open-source models and the API is OpenAI-compatible so it just plugs into your CLI or Dev Environment. $20/mo flat for the subscription tier is nice.\n\nWith my ref link to Synthetic and Z you are able to get a rebate.",
          "score": -3,
          "created_utc": "2026-02-13 09:10:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4oqi8",
      "title": "Best GUI for OpenCode",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r4oqi8/best_gui_for_opencode/",
      "author": "Character_Cod8971",
      "created_utc": "2026-02-14 16:20:30",
      "score": 25,
      "num_comments": 31,
      "upvote_ratio": 0.93,
      "text": "Is the OpenCode desktop app really the best GUI there is out there for Windows? I tried it for a few days now and it doesn't have Worktrees support and in general doesn't really feel well thought out or treated with much love. What are all of you using? Maybe you use something completely decoupled from OpenCode.....\n\nEDIT: There are workspaces in OpenCode desktop but there are super hidden (Hover the project title, three dots appear to the right of it. Enable workspaces.) and I didnt get them to work yet which is why they don't really exist for me in this app. (https://github.com/anomalyco/opencode/issues/11089)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4oqi8/best_gui_for_opencode/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5d16on",
          "author": "Ok-Connection7755",
          "text": "Openchamber, hands down wins!",
          "score": 15,
          "created_utc": "2026-02-14 16:31:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dst65",
              "author": "cmbtlu",
              "text": "I use Openchamber as well. Itâ€™s the best for mobile right now until a native app is released.",
              "score": 4,
              "created_utc": "2026-02-14 18:49:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5d19us",
              "author": "Ok-Connection7755",
              "text": "But I also tried sidecar, maestro, etc. several nice projects coming up",
              "score": 2,
              "created_utc": "2026-02-14 16:31:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5gsrjw",
              "author": "gsxdsm",
              "text": "Definitely open chamber",
              "score": 1,
              "created_utc": "2026-02-15 06:09:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5d1ifq",
          "author": "BarryTownCouncil",
          "text": "I don't think the gui is at all good, but then also the tui sucks when it comes to copy and paste on nix. And in both I find it impossible to see it's thoughts.\n\nI tried openchamber. Openly vibe coded and boy it shows. Broken in weird and unacceptable ways. That was quite a new experience looking for alternatives and being very disappointed.\n\nCodeNomad seems ok for it, not amazing but worth a look.",
          "score": 7,
          "created_utc": "2026-02-14 16:32:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d7ow8",
              "author": "UseHopeful8146",
              "text": "Also nix user, so far I much prefer codenomad to everything else. The remote feature is really the best part to me - though even with tailscale and and adding the site page as a PWA to Home Screen, it still gives me problems sometimes. Rarely critical as long as Iâ€™m home, but it can be annoying.\n\nIf there were a program that offered all that, ran only as well, and offered stt I would recommend that instead js (manifesting, manifesting)",
              "score": 2,
              "created_utc": "2026-02-14 17:03:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fwhau",
                  "author": "Electronic_Newt_8105",
                  "text": "does codenomad stream properly? i was having issues with most of the GUI options streaming the reasoning properly",
                  "score": 1,
                  "created_utc": "2026-02-15 02:07:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5dunwo",
          "author": "mirza_rizvi",
          "text": "Just run \"opencode web\" instead of \"opencode\"",
          "score": 3,
          "created_utc": "2026-02-14 18:58:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d5wao",
          "author": "Recent-Success-1520",
          "text": "CodeNomad supports worktrees, desktop, web, mobile, remote",
          "score": 5,
          "created_utc": "2026-02-14 16:54:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5di05v",
              "author": "Character_Cod8971",
              "text": "How do you think it compares to OpenChamber?",
              "score": 0,
              "created_utc": "2026-02-14 17:56:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5djqcm",
                  "author": "Recent-Success-1520",
                  "text": "I am biased as I built it. TBH\nI like to see all the details but optionally can make it less verbose.\nI haven't used openchamber, it doesn't work for me on my Intel Mac when I tried recently",
                  "score": 2,
                  "created_utc": "2026-02-14 18:04:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5gssfk",
          "author": "gsxdsm",
          "text": "Openchamber",
          "score": 2,
          "created_utc": "2026-02-15 06:09:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dg7nc",
          "author": "pixeladdie",
          "text": "I haven't needed anything but the TUI.",
          "score": 1,
          "created_utc": "2026-02-14 17:47:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dmzqb",
          "author": "atkr",
          "text": "web / desktop app is still beta, no one should rely on it",
          "score": 1,
          "created_utc": "2026-02-14 18:20:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5do4pz",
              "author": "Character_Cod8971",
              "text": "So what are your recommendations?",
              "score": 1,
              "created_utc": "2026-02-14 18:26:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5fnaa4",
          "author": "mr_ignatz",
          "text": "TIL that Kilo is based on OpenCode underneath",
          "score": 1,
          "created_utc": "2026-02-15 01:05:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gwvob",
          "author": "Outrageous_Client272",
          "text": "I don't think it's the best for coding. But, I've working on OpenWork for non-coding tasks. It's more meant to share you opencode config with non-tech friends, family, and colleagues.\n\nWould love to get feedback on it we launched a month ago and grew to close to 10k stars on github and 70k downloads.\n\n  \nStill pretty early though, so would love some feedback from the community :)",
          "score": 1,
          "created_utc": "2026-02-15 06:47:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h92p0",
          "author": "HarjjotSinghh",
          "text": "yeah workspaces are buried under layers here - wish devs would just fix the ui!",
          "score": 1,
          "created_utc": "2026-02-15 08:44:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5hpbfn",
              "author": "Character_Cod8971",
              "text": "Didn't get them to work.... ðŸ˜­",
              "score": 1,
              "created_utc": "2026-02-15 11:20:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hsvzv",
          "author": "Purple_End4828",
          "text": "Hey OpenCode community,\n\nTL;DR: Iâ€™m building UnLoveable, a self-hosted Loveable/Bolt-style â€œprompt -> docs -> checklist -> agent swarm executesâ€ builder on top of OpenCode. This repo already has a Next.js build UI (Monaco editor, file explorer, diffs, PTY terminal, orchestrator dashboard) talking to a Bun/Hono OpenCode server (SSE orchestrator events, file read/write, PTY websockets, multi-provider LLM). What I need help finishing: a real safe preview/sandbox (not just iframe-a-URL), reliable parallel worker isolation + merging, and better prompt-to-plan output quality so agents produce working code more consistently.\n\nHereâ€™s how my Valentineâ€™s Day went: sitting alone in my apartment in, Georgia, doomscrolling and thinking â€œwhy am I paying for yet another â€˜AI builds your appâ€™ tool when Iâ€™m literally surrounded by an agent runtime?â€\n\nSo I snapped and started building a self-hosted alternative on top of OpenCode.\n\nWhat I built: UnLoveable\n\nA local-first â€œprompt â†’ docs â†’ plan â†’ agent swarm executes while you watchâ€ builder.\n\nItâ€™s not trying to be a magic SaaS website generator. The premise is: generate the planning artifacts first (spec/UI spec/architecture/registry/implementation plan/prompt), then have multiple OpenCode agents chew through the checklist with tests/validation, with a UI that lets you observe + intervene.\n\nCodebase tour (whatâ€™s actually in this repo)\n\n- web/: Next.js (React 19) app with a split-pane build UI (Monaco editor, diff viewer, orchestrator dashboard, SSE event console) + an xterm PTY terminal.\n\n- opencode/: Bun + Hono headless server exposing:\n\n- Orchestrator routes (/orchestrator/...) including SSE stream at /orchestrator/:id/event\n\n- File browser + read/write (/file, /file/content, /file/status)\n\n- PTY over WebSocket (/pty/:id/connect)\n\n- Provider plumbing via Vercel AI SDK (OpenAI + OpenAI-compatible + others)\n\n- workspace/: mounted project directory where generated docs/code live (see docker-compose.yml volume mounts).\n\n- Loop packs/templates: templates/unloveable/, unloveable_loop_v2/ (â€œRalph Wiggum static-context loopâ€: checklist-driven, fresh context per iter, runlogs, validation profiles).\n\nWhatâ€™s working right now\n\n- /build â€œIDEâ€: file explorer + Monaco edit/save via /file/content, diff viewer via /file/status, orchestrator panel for editing generated docs, terminal via /pty.\n\n- â€œSimple Modeâ€ kickoff: start orchestrator â†’ generate docs via pipeline â†’ run checklist executor with configurable workers.\n\n- Real-time-ish updates: the UI listens to orchestrator SSE events and refreshes status/dashboards.\n\nWhere Iâ€™m stuck (and what I want help with)\n\n1.\tâ Preview / sandboxing / â€œrun what we builtâ€\n\n- Current â€œLive Previewâ€ is literally an iframe that loads a URL you paste (web/src/components/live-preview.tsx). Itâ€™s not a sandbox.\n\n- What I want: click â€œPreviewâ€ and it spins up the generated app (Vite/Next/etc.) in an isolated way, then embeds it reliably (ideally same-origin proxied) without CORS/postMessage misery or security footguns.\n\n2) Multi-worker isolation + merging back\n\n- The runner can parallelize tasks; when workers > 1 it tries to use git worktrees (opencode/src/orchestrator/runner.ts + worktree pool/merger).\n\n- I need battle-tested guidance on merge strategy + conflict handling + how to make parallel agents not trample each other (and what to do when the target workspace isnâ€™t a git repo).\n\n3) Output quality (docs + plan â†’ executable tasks)\n\n- Pipeline doc generation is currently a pretty bare prompt that returns JSON schema (opencode/src/orchestrator/pipeline.ts).\n\n- I need stronger prompting + post-processing so the implementation plan becomes â€œagent-executableâ€ more consistently (right granularity, no deprecated libs, fewer dead-end tasks, better validation hooks).\n\nWhy Iâ€™m posting\n\nItâ€™s 3AM energy, but the bones feel real: OpenCode is already the hard part. This UI + orchestration layer is the missing â€œBolt/Loveable experienceâ€ for people who want self-hosted + transparent + hackable.\n\nIf you want to dig in, the most relevant files:\n\n- docker-compose.yml\n\n- web/src/app/build/page.tsx\n\n- web/src/components/live-preview.tsx\n\n- opencode/src/orchestrator/pipeline.ts\n\n- opencode/src/orchestrator/runner.ts\n\n- opencode/src/server/routes/orchestrator.ts\n\n- opencode/src/server/routes/file.ts\n\n- opencode/src/server/routes/pty.ts\n\nHow you can help\n\n- If youâ€™ve solved â€œsafe preview for untrusted/generated web codeâ€ in a product: tell me the architecture youâ€™d use here.\n\n- If youâ€™ve built parallel agent systems: Iâ€™d love opinions on worktree/branch/patch-based workflows + conflict resolution ergonomics.\n\n- If youâ€™re good at prompt-to-plan reliability: help me tighten the pipeline so it produces better specs + checklists.\n\nRepo link: Iâ€™ll drop it once I push a cleaned snapshot (itâ€™s currently living as this local codebase).\n\nIf youâ€™ve ever rage-coded something at 2AM and thought â€œwait, this might actually be useful,â€ please chime in.\n\nThis version does not work yet, but has some much needed architecture changes\n\nhttps://github.com/unloveabledev/UnLoveable-parallel\n\nThis version works but has way too much logic in the frontend, and runs loops in series, so it is kinda slow.\n\nhttps://github.com/unloveabledev/unloveable-series",
          "score": 1,
          "created_utc": "2026-02-15 11:52:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pbhtl",
          "author": "RazerWolf",
          "text": "Is there a GUI tool that works with claude code and openai codex CLIs? Not just opencode.",
          "score": 1,
          "created_utc": "2026-02-16 16:06:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d1ms6",
          "author": "HarjjotSinghh",
          "text": "ohhh worktrees would've saved my soul.",
          "score": 1,
          "created_utc": "2026-02-14 16:33:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d5iw6",
              "author": "AndroidJunky",
              "text": "Hover the project title, three dots appear to the right of it. Enable workspaces.",
              "score": 1,
              "created_utc": "2026-02-14 16:52:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dq8gz",
                  "author": "Character_Cod8971",
                  "text": "Whoaahhh, why do they hide it like this? This feature is so important. They should really place it more prominently somewhere, as they did for the Codex Mac Desktop app",
                  "score": 1,
                  "created_utc": "2026-02-14 18:36:36",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5dc74o",
          "author": "SynapticStreamer",
          "text": "CLI.",
          "score": 0,
          "created_utc": "2026-02-14 17:26:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lxkol",
              "author": "Docs_For_Developers",
              "text": "tell the people",
              "score": 1,
              "created_utc": "2026-02-16 01:29:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5dqoyb",
              "author": "Character_Cod8971",
              "text": "Nah, GUIs are better, and I see what they did with the Codex desktop app for Mac and it seems awesome.",
              "score": -1,
              "created_utc": "2026-02-14 18:38:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5g9gek",
                  "author": "SynapticStreamer",
                  "text": "gross",
                  "score": 0,
                  "created_utc": "2026-02-15 03:37:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r2fjyn",
      "title": "OpenCode vs GitHub Copilot CLI â€” huge credit usage difference for same prompt?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r2fjyn/opencode_vs_github_copilot_cli_huge_credit_usage/",
      "author": "usernameIsRand0m",
      "created_utc": "2026-02-12 01:05:14",
      "score": 23,
      "num_comments": 23,
      "upvote_ratio": 0.96,
      "text": "Trying to figure out if I messed something up in my OpenCode config or if this is just how it works.\n\nIâ€™m on OpenCode 1.1.59.  \nI ran a single prompt. No sub agents.  \nIt cost me 27 credits.\n\nI thought maybe OpenCode was doing extra stuff in the background, so I disabled agents:\n\n    \"permission\": {\n      \"task\": \"deny\"\n    },\n    \"agent\": {\n      \"general\": {\n        \"disable\": true\n      },\n      \"explore\": {\n        \"disable\": true\n      }\n    }\n    \n\nRan the exact same prompt again. Still 27 credits.\n\nFor comparison, I tried the same prompt with GitHub Copilot CLI and it only used 3 credits for basically the same task and output.\n\nNot talking about model pricing here. Iâ€™m specifically wondering if:\n\n* Thereâ€™s some other config Iâ€™m missing that controls how much work OpenCode does per prompt\n* OpenCode is doing extra planning or background steps even with agents disabled\n* Anyone else has seen similar credit usage and figured out what was causing it\n\nBasically, is this normal for OpenCode or am I accidentally paying for extra stuff I donâ€™t need?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r2fjyn/opencode_vs_github_copilot_cli_huge_credit_usage/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o4wl2bh",
          "author": "simap2000",
          "text": "Wonder if each round trip in opencode for every tool call, etc counts as a request vs many tool calls and agents in copilot is like 1?",
          "score": 5,
          "created_utc": "2026-02-12 01:18:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wmgbk",
              "author": "usernameIsRand0m",
              "text": "It was not like this few (maybe 5-6 versions?) versions ago. I am wondering if I am missing something in the config that I need to have.",
              "score": 1,
              "created_utc": "2026-02-12 01:27:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ycer8",
                  "author": "SvenVargHimmel",
                  "text": "Use litellm proxy and run with ---detailed-debug  and point opencode to that  with the proxy configured to point to your llm backend and you can see exactly what it is sending per request. \n\n  \nThen point your Copilot at the same endpoint. \n\n  \nYou can see exactly what's going on. \n\nAnd if you want to test your theory that it used be less expensive a few versions ago , just roll back and repeat \n\n  \n",
                  "score": 5,
                  "created_utc": "2026-02-12 09:22:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4xjj39",
                  "author": "albertortilla",
                  "text": "There were problems in older version (1.1.38 if I am no wrong) regarding this: each tool call counted in GitHub copilot as a new request, which was solved in the next versions... Maybe the problem appeared again... I would try to install an older version and check for the same prompt",
                  "score": 1,
                  "created_utc": "2026-02-12 05:00:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4z25wj",
          "author": "krimpenrik",
          "text": "Same issue saw that I am already using a lot opencode with copilot sub, this month is fucked",
          "score": 3,
          "created_utc": "2026-02-12 12:59:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51at7x",
          "author": "PayTheRaant",
          "text": "Check your small model configuration. This is the model for generating the titles of sessions and messages. You should use a free model for that. \n\nAlso try the same prompt with a free model: if your premium request cost is not zero, then something else is triggering premium requests with a paid model.",
          "score": 3,
          "created_utc": "2026-02-12 19:37:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51c6ap",
              "author": "PayTheRaant",
              "text": "You can also use debug logs to track every single call to the LLM\n\nhttps://opencode.ai/docs/troubleshooting/#journaux",
              "score": 1,
              "created_utc": "2026-02-12 19:44:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o53oh68",
                  "author": "usernameIsRand0m",
                  "text": "So, apart from the above config which I have shared in OP, I have to add small model config?\n\nI'll check the debug logs. Thanks.",
                  "score": 1,
                  "created_utc": "2026-02-13 03:22:28",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o537bmt",
          "author": "itsjase",
          "text": "its a known bug: [https://github.com/anomalyco/opencode/issues/8030](https://github.com/anomalyco/opencode/issues/8030)",
          "score": 3,
          "created_utc": "2026-02-13 01:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xpzyi",
          "author": "Michaeli_Starky",
          "text": "Yep, noticed the same. Switched to Copilot CLI",
          "score": 2,
          "created_utc": "2026-02-12 05:52:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ykxmn",
          "author": "Adorable_Buffalo1900",
          "text": "opencode claude model use chat completions api, but copilot use message api. you need raise a issue for opencode",
          "score": 2,
          "created_utc": "2026-02-12 10:44:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4y2ywi",
          "author": "keroro7128",
          "text": "I've heard that some people are saying they can use the free GPT 5 Mini model to call advanced models ï¼ˆopus 4.6ï¼‰ via a sub-agent without consuming any requests, but some are saying they got their accounts banned for it.",
          "score": 1,
          "created_utc": "2026-02-12 07:49:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51ce6k",
              "author": "PayTheRaant",
              "text": "Normally, switching model for sub agent is considered a new premium request.",
              "score": 2,
              "created_utc": "2026-02-12 19:45:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4yrzcr",
              "author": "usernameIsRand0m",
              "text": "Yes, there are lot of instances of that happening, I have Pro+ account, so there are more than enough requests per month for me.",
              "score": 1,
              "created_utc": "2026-02-12 11:46:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4y7uxq",
          "author": "Tadomeku",
          "text": "The system prompt in Opencode is likely longer than the system prompt in GitHub CLI. YOUR prompt may be simple, but it gets appended to the system prompt in Opencode, along with AGENTS.md, CLAUDE.md, SKILLS, etc.    \n    \nI don't know what GitHub CLI does under the hood but I imagine it's pretty different.",
          "score": 1,
          "created_utc": "2026-02-12 08:37:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51a49q",
              "author": "PayTheRaant",
              "text": "Copilot model is expected to consume ONE premium request per ONE user prompt.\nEverything else that is agent initiated is expected to be included in that initial premium request (all tools, even sub agent) as long as it stays in the same model. \nIn theory, it should not even care about input token cache.\n\nSo this is why having 27 premium requests consumed is considered a big problem.",
              "score": 1,
              "created_utc": "2026-02-12 19:34:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ydnqg",
          "author": "soul105",
          "text": "Noticed the same here.  \nSome business users have the limit for 300 requests and cannot buy more due to company policies, making the problem even bigger.",
          "score": 1,
          "created_utc": "2026-02-12 09:35:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o528g2s",
          "author": "HarjjotSinghh",
          "text": "wow copilot's gonna charge you like a slot machine.",
          "score": 1,
          "created_utc": "2026-02-12 22:18:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53n2tt",
          "author": "Desperate-Bath5208",
          "text": "Happened to me as well, I barely used it and I've consumed $2 worth of quota.",
          "score": 1,
          "created_utc": "2026-02-13 03:13:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xd8l2",
          "author": "jmhunter",
          "text": "The preamble/system prompt is probably a lot juicier w opencode",
          "score": 1,
          "created_utc": "2026-02-12 04:14:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xoc9l",
              "author": "IIALE34II",
              "text": "Billing should be one premium request per user initialized message. Or well there is the per model scaling.",
              "score": 4,
              "created_utc": "2026-02-12 05:38:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4x434i",
          "author": "ok_i_am_nobody",
          "text": "Same issue. \nMoved to pi coding agent for simple tasks.\nHow are you tracking the credits usage?",
          "score": 0,
          "created_utc": "2026-02-12 03:13:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xnagy",
              "author": "usernameIsRand0m",
              "text": "In your settings page, here - [https://github.com/settings/billing/premium\\_requests\\_usage](https://github.com/settings/billing/premium_requests_usage)",
              "score": 0,
              "created_utc": "2026-02-12 05:29:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r5bopx",
      "title": "Oh my opencode vs GSD vs others vs Claude CLI vs Kilo",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r5bopx/oh_my_opencode_vs_gsd_vs_others_vs_claude_cli_vs/",
      "author": "Outrageous_Hawk_789",
      "created_utc": "2026-02-15 10:46:19",
      "score": 20,
      "num_comments": 17,
      "upvote_ratio": 0.92,
      "text": "I know I am comparing oranges and apples but when I compare them I mean their agentic flow/orchestration.  \nI first moved to OmO because then claude code did not do orchestration at all iirc and it was all user dependent  \nBut now when I notice that both Codex and Claude Code do that so well with subagents, while OmO feels like it's running in loops, taking long hours to finish a feature that Claude one-shots it in a single prompt.  \nI'm I have access to Codex, Claude Pro, Kimi 2.5 paid and obviously free, and now im trying out GLM-5 on kilo and its very promising, especially with their orchestration and agents.\n\nI'd love to hear some more workflows and hear about your experience and learn a thing or two.\n\nI am a junior software dev but I in the last year I barely open the IDE anymore. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r5bopx/oh_my_opencode_vs_gsd_vs_others_vs_claude_cli_vs/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5iskra",
          "author": "il_94",
          "text": "I came here to ask the same question and saw your post. Interested in seeing what others have to say.   \n  \nIt feels like the philosophy of the Opencode devs is to keep things separate and not \"bloat\" the TUI. I wonder if/when we will see an orchestration layer (similar to OMO) ship with the TUI or as an official plugin.\n\n",
          "score": 8,
          "created_utc": "2026-02-15 15:38:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5orso9",
              "author": "old_mikser",
              "text": "I'm using superpowers lately (in opencode) and really feel need of higher orchestration layer on top of it (with it in mind).\n\nHope if some kind of official orchestration plugin/feature will be created it will be as much separated from inside workflow, as possible. As all other tools I tried to force to work with superpowers overlapped each other, or have completely different philosophy/approach and contradicted each other. Didn't try many, tho.",
              "score": 1,
              "created_utc": "2026-02-16 14:28:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hvakm",
          "author": "Sea-Sir-2985",
          "text": "i've been through a similar journey... started with claude code, tried opencode for a bit, and came back. the subagent orchestration in claude code is genuinely better than what i've seen in most alternatives, it actually breaks down complex tasks and parallelizes work across agents instead of running everything sequentially in one loop. the main downside is the rate limits on pro which can kill your flow if you're deep in a feature\n\nhaven't tried kilo with GLM-5 yet but the orchestration comparison is interesting... the thing that matters most to me is how well the agent recovers from mistakes, like when it goes down a wrong path does it course correct or just keep looping. claude code handles that pretty well with plan mode where you can review the approach before it starts implementing",
          "score": 4,
          "created_utc": "2026-02-15 12:12:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i7qkv",
          "author": "aeroumbria",
          "text": "OmO is a bit too heavy for my uses...\n\nGSD is very effective and getting better, but the Opencode version is still not fully cleansed of Claudism, and some prompt files are really bloated. However they are implementing deterministic housekeeping ops instead of doing everything with prompts, so that is welcoming. One drawback is that you have to fight with it to work in parallel branches. Sometimes I just respawn a fresh GSD project with every new feature branch instead of dealing with the headaches.\n\nI will not use Claude CLI which regularly breaks inside vscode and is a pain to look at compared to Opencode. I am not a one-shot vibe coder, so change readability is quite important to me. Solves the \"fix this issue no matter the cost or means\" situations.\n\nAn extra workflow that occasionally works for me is generating a concise spec + pass criteria with plan mode or GSD, then dump the plan into a ralph loop to brute force it.",
          "score": 6,
          "created_utc": "2026-02-15 13:42:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j0hhm",
          "author": "jellydn",
          "text": "You could check my setup here: https://ai-tools.itman.fyi/#/ I want to keep my workflow clean and simple.",
          "score": 7,
          "created_utc": "2026-02-15 16:16:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qgdt4",
              "author": "alp82",
              "text": "This is so cool! Exactly the type of setup I'd like to let people share at my new project.",
              "score": 1,
              "created_utc": "2026-02-16 19:15:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hoagv",
          "author": "ReasonableReindeer24",
          "text": "Kilo cli with orchestra is good, support subagent and I can debug with debug mode which other does not have mode like this",
          "score": 3,
          "created_utc": "2026-02-15 11:10:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qcsjf",
          "author": "Blufia118",
          "text": "Iâ€™m tryna under the value of OMO.. it just seem like it burns more tokens with not much improvement",
          "score": 2,
          "created_utc": "2026-02-16 18:58:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5iazau",
          "author": "gonssss",
          "text": "what is gsd?\n\n",
          "score": 1,
          "created_utc": "2026-02-15 14:02:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5id7hh",
              "author": "btull89",
              "text": "https://github.com/gsd-build/get-shit-done",
              "score": 3,
              "created_utc": "2026-02-15 14:15:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5j1uhy",
          "author": "Far-Association2923",
          "text": "I'm curious to know what features people are most interested in right now? Currently working on a desktop app that is cross platform and does include the features the OP mentioned. Even though my app an be utilized by devlopers the main focus is for normies. I want to give them the powerful tools us developers have had access to for years.\n\nI'm starting to believe you can get very good results from these newer cheaper opensource LLMs. They have to be good at tool calling though as there is no way around forcing them to do tasks when they refuse to use tools yoou offer them.",
          "score": 1,
          "created_utc": "2026-02-15 16:23:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j6og7",
          "author": "seaal",
          "text": "I just started using oh my pi a few days ago and have been really enjoying it. \n\nhttps://github.com/can1357/oh-my-pi",
          "score": 1,
          "created_utc": "2026-02-15 16:46:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kox9j",
              "author": "Queasy_Asparagus69",
              "text": "Looks nice. Will have to try it",
              "score": 1,
              "created_utc": "2026-02-15 21:16:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5oqzq5",
              "author": "old_mikser",
              "text": "How is this in comparison to tools listed by OP? Did you use any of them before, or maybe superpowers? If so, what in oh my pi you like more?",
              "score": 1,
              "created_utc": "2026-02-16 14:24:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5k4xms",
          "author": "drinksbeerdaily",
          "text": "Really liking https://github.com/alvinunreal/oh-my-opencode-slim.",
          "score": 1,
          "created_utc": "2026-02-15 19:33:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hv6et",
          "author": "Rybens92",
          "text": "Just use OpenHands CLI or Web or other their offerings.\nIt's the best agentic tool based on some benchmarks I used to look at and I can confirm myself it's very good.",
          "score": -1,
          "created_utc": "2026-02-15 12:11:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i2g8b",
          "author": "atkr",
          "text": "skill issue",
          "score": -5,
          "created_utc": "2026-02-15 13:08:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6fnsp",
      "title": "Opencode with Github Copilot",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r6fnsp/opencode_with_github_copilot/",
      "author": "Charming_Support726",
      "created_utc": "2026-02-16 17:19:30",
      "score": 19,
      "num_comments": 19,
      "upvote_ratio": 0.91,
      "text": "I asked that question in the Copilot sub but got not answer yet. Maybe someone with a similar setup could enlighten me.\n\nHalf time I use Opus (Rest of the time still burning my Azure Credits on codex), but after all this discussions of TOS Violations with Antigravity and CC and some further issues I canceled there.\n\nI read that Opencode is accepted as a 3rd Party Agent with GitHub Copilot. (Hope it's true) So I gave it a go.\n\nStill the context size restriction nags a bit, but I think maybe it is time to work less \"sloppy\". I created some workflow (Skills and Agents) for me to work intensively with subagents. Currently only for creating docs, onboarding projects and creating execution plans.\n\nI checked the billing and verified that my workflow only get charged one premium request per prompt, but in the background tools and subs are consuming a hell of a lot of tokens on single premium request.\n\nAre there any limits I shall take care of? I mean this could be really maxxed out by using the Question-Tool and Subagents etc. Dont wanna risk my companies Github Account.\n\nAny experience or hints ?\n\nEDIT: After someone posted about suspension I searched and found: [https://www.reddit.com/r/GithubCopilot/comments/1r0wimi/if\\_you\\_create\\_a\\_long\\_todo\\_list\\_in\\_agent\\_mode\\_you/](https://www.reddit.com/r/GithubCopilot/comments/1r0wimi/if_you_create_a_long_todo_list_in_agent_mode_you/)  Very Interesting. It seems GHCP is banning people who are excessively using the subagent scheme with tremendously long todo-lists. OMG.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r6fnsp/opencode_with_github_copilot/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5ptqum",
          "author": "devdnn",
          "text": "I believe thereâ€™s an active discussion happening in the GitHub issue section.\n\nIt seems like this might be what you need.\n\nhttps://github.com/anomalyco/opencode/issues/8030",
          "score": 8,
          "created_utc": "2026-02-16 17:30:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5px8yi",
              "author": "Charming_Support726",
              "text": "Thanks. I read the discussions beforehand. So I checked - and the billing is ok as expected.\n\nFor example I build a Subagent / Skill / Template -System that generates up-to-date project docs, when they are not available - or updates them etc. My test this morning spawned  6  parallel subs ( Main, Overview and 5 sub-subs - one per module)  burning approx 500k in half an hour on Opus 4.6. Factor 3. Three-Premium request $0.12 .\n\nI mean, If I enable DCP again and use it seriously it will get even worse. Is there anyone verifying the consumption or is this \"safe\" and \"ban-proof\" because covered by the TOS ?",
              "score": 3,
              "created_utc": "2026-02-16 17:47:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5puoge",
          "author": "ZeSprawl",
          "text": "I believe it's an official integration, based on a Twitter post from OpenCode. I use it regularly and I get a lot of usage out of it via Opus every month, and don't notice it eating up credits.",
          "score": 8,
          "created_utc": "2026-02-16 17:35:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pwg2o",
          "author": "JohnnyDread",
          "text": "Are you able to see the billing? I have not noticed a big difference between using OpenCode and the GitHub CLI as far as cost goes. ",
          "score": 2,
          "created_utc": "2026-02-16 17:43:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pwh6b",
          "author": "EchoesInBackpack",
          "text": "5.2 Codex and 5.3 codex models have twice bigger context windows. Sub-agents might be fine, but using it just to not hit compaction feels annoying\n\ntry opencode models â€”verbose(or something like that) to see their context limits",
          "score": 2,
          "created_utc": "2026-02-16 17:43:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qqux1",
              "author": "Charming_Support726",
              "text": "I know, also get them from Azure (free credits) and using my OpenAI subscription. So I am not afraid to run out of computing power. I really like them, but some tasks run better with Opus, e.g. building some up for the first time or running frontend debugging with playwright and a few more. I wanted to have a provider which acts uncomplicated and realizes access to Claude below API costs. \n\nI did a few test with subagents a few weeks ago. Found out, that saving tokens using Subs isn't that easy. Prompting this thoroughly is a piece of work on its own.\n\nAnyway, I thought it might be a good Idea to structure work and docs a bit better, so that I could easily start new sessions and work in the model's sweet spot below 128k - instead of hitting the compaction border like a vibe coder on regular basis. I gave it a try and found it impressing.\n\n",
              "score": 1,
              "created_utc": "2026-02-16 20:06:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5q6l3u",
          "author": "HarjjotSinghh",
          "text": "this actually feels like magic",
          "score": 2,
          "created_utc": "2026-02-16 18:30:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qjh37",
          "author": "trypnosis",
          "text": "I remember saying this was an issue but fixed in the CLI but not fixed yet on the desktop. Are you cli or desktop?",
          "score": 2,
          "created_utc": "2026-02-16 19:30:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qn0el",
              "author": "Charming_Support726",
              "text": "I am using web - building everything myself since I started a few month ago. Works til now without any issue.",
              "score": 1,
              "created_utc": "2026-02-16 19:47:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5qnm03",
                  "author": "trypnosis",
                  "text": "Never looked at the desktop app but if itâ€™s electron or any other web wrapper then it might be the same problem. Try the cli for a bit. I went from CC Max to Co Pilot pro+ I think I get decent value for money.",
                  "score": 2,
                  "created_utc": "2026-02-16 19:50:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5qo09o",
              "author": "soul105",
              "text": "It's not yet fixed in CLI",
              "score": 1,
              "created_utc": "2026-02-16 19:52:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5quyxg",
          "author": "Legal_Dimension_",
          "text": "Just received a suspension to my account for using copilot with opencode. I'm a heavy user and they don't like that so be careful.",
          "score": 1,
          "created_utc": "2026-02-16 20:26:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qvel7",
              "author": "Charming_Support726",
              "text": "That's bad. Could you tell us more? Explicitly because of Opencode? What is your subscription?\n\nBTW: What do you mean with \"Heavy User\" ?",
              "score": 2,
              "created_utc": "2026-02-16 20:28:30",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5qxrzz",
              "author": "ellensen",
              "text": "Suspension of your github  account or just the copilot subscription?",
              "score": 1,
              "created_utc": "2026-02-16 20:40:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5t1h0p",
              "author": "joshashsyd",
              "text": "Yes I got a 157hr timeout??",
              "score": 1,
              "created_utc": "2026-02-17 03:42:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5wtago",
              "author": "haininhhoang94",
              "text": "i guess you use it with subagents?",
              "score": 1,
              "created_utc": "2026-02-17 18:44:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yt6hd",
          "author": "HarjjotSinghh",
          "text": "copilot + opencode is the real secret sauce now",
          "score": 1,
          "created_utc": "2026-02-18 00:42:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ytw5s",
              "author": "FaerunAtanvar",
              "text": "Care to elaborate?",
              "score": 1,
              "created_utc": "2026-02-18 00:46:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r2mw1u",
      "title": "No time for release notes, let's ship it a daily update /s",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r2mw1u/no_time_for_release_notes_lets_ship_it_a_daily/",
      "author": "soul105",
      "created_utc": "2026-02-12 07:13:01",
      "score": 19,
      "num_comments": 5,
      "upvote_ratio": 0.92,
      "text": "https://preview.redd.it/ya0lig1ak0jg1.png?width=1980&format=png&auto=webp&s=fe92a7273ebb8fbcd3094b39ec310a2f81407aee\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r2mw1u/no_time_for_release_notes_lets_ship_it_a_daily/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o4ya76g",
          "author": "sitkarev",
          "text": "i still have frequent memory leakages",
          "score": 2,
          "created_utc": "2026-02-12 09:00:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56510t",
              "author": "_Deftera_",
              "text": "You should stop drinking alcohol",
              "score": 3,
              "created_utc": "2026-02-13 14:36:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o51ds9s",
          "author": "HarjjotSinghh",
          "text": "this means beta.",
          "score": 1,
          "created_utc": "2026-02-12 19:51:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z95uz",
          "author": "Heavy-Focus-1964",
          "text": "people on this sub seem to often forget that this product is *free.* have you ever heard the term â€œyou get what you pay for?â€",
          "score": -2,
          "created_utc": "2026-02-12 13:41:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zdjhp",
              "author": "soul105",
              "text": "You are right, the product is awesome and it's free.  \n  \nThe intention was to have a moment of fun where the wishes to deliver something faster were stronger than waiting for a good headline for the release notes. Release notes don't need to be boring.",
              "score": 5,
              "created_utc": "2026-02-12 14:05:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r5yjz1",
      "title": "Huge Update: You can now run Shannon (Autonomous AI Pentester) directly on OpenCode! ðŸ›¡ï¸ðŸ’»",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r5yjz1/huge_update_you_can_now_run_shannon_autonomous_ai/",
      "author": "ResponsiblePlant8874",
      "created_utc": "2026-02-16 03:21:35",
      "score": 19,
      "num_comments": 10,
      "upvote_ratio": 0.85,
      "text": "If youâ€™ve been using **OpenCode** for autonomous development but worrying about the security of the code your agents are churning out, this is for you.\n\nA new plugin just dropped that lets you run **Shannon**â€”the fully autonomous AI hackerâ€”directly within your OpenCode environment.\n\n# What is Shannon?\n\nFor those who missed the buzz, Shannon (by KeygraphHQ) is essentially the \"Red Team\" to your \"Blue Team.\" While your other agents are busy building features, Shannonâ€™s only job is to break them. It doesnâ€™t just give you \"alerts\"; it actually identifies and delivers exploits to prove where your vulnerabilities are.\n\n# Why this matters for OpenCode users:\n\nUntil now, Shannon was mostly a standalone powerhouse. With the **opencode-shannon-plugin**, you can now bake security auditing right into your agentic workflow.\n\n* **Security-First Vibe Coding:** Stop treating security as an afterthought.\n* **Autonomous Audits:** Let Shannon scan your PRs and local codebase for exploits before you ever hit \"merge.\"\n* **Zero Friction:** It integrates directly via the OpenCode plugin system.\n\n# How to get it:\n\nThe plugin is hosted on GitHub by **vichhka-git**: ðŸ‘‰[https://github.com/vichhka-git/opencode-shannon-plugin](https://github.com/vichhka-git/opencode-shannon-plugin)\n\n**Quick Install (usually):**\n\n1. Clone/Add the plugin to your `.opencode/plugin/` directory.\n2. Restart OpenCode.\n3. (Check the README for specific environment variables needed for the Shannon core).\n\nHuge props to the dev for making this bridge. It makes the \"full-stack\" agentic dream feel a lot more production-ready.\n\n**Has anyone tried running it against their current projects yet? Curious to see what kind of exploits it's catching in AI-generated code!**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r5yjz1/huge_update_you_can_now_run_shannon_autonomous_ai/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5o4z8s",
          "author": "MaxPhoenix_",
          "text": "The actual purpose of this software differs significantly from the claims of this Reddit post - it says shannon plugin is a tool for scanning PRs and auditing local codebases for vulnerabilities, yet the plugin contains no code analysis capabilities whatsoever - it's purely a network penetration testing framework designed to attack live web applications using tools like nmap, sqlmap, and hydra in a Kali Linux Docker container. If you're looking for static code security auditing or SAST, this isn't it; if you need to pentest deployed applications, it might be useful.\n\nThis post conflates the original Shannon's capabilities (which DO include source code analysis for \"scan your PRs\") with this simplified plugin (which only does black-box network pentesting). The plugin author appears to have built a thin wrapper around Shannon's attack tools without implementing the source analysis capabilities that would make the \"scan your PRs\" claim true. Shrug.",
          "score": 7,
          "created_utc": "2026-02-16 12:08:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5scnep",
              "author": "oulu2006",
              "text": "that's a great assessment thank you for ur service ",
              "score": 1,
              "created_utc": "2026-02-17 01:10:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nddj8",
          "author": "xak47d",
          "text": "Any volunteer to do a security audit on this",
          "score": 4,
          "created_utc": "2026-02-16 07:57:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5o11ma",
              "author": "MaxPhoenix_",
              "text": "I have audited the code and it is legit but a problem with seeking security assurance on the internet is you might not trust my authority.  The docker container runs with direct network access (which seems reasonable but you should be aware of it) but it has no obfuscated code, minified files, hardcoded secrets, suspicious network calls, eval or function usage, nor malicious imports.  EDIT: however, the description here doesn't match the code so even though it's \"clean\", people need to be careful what they are getting into.",
              "score": 4,
              "created_utc": "2026-02-16 11:36:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5o2csv",
                  "author": "gottapointreally",
                  "text": "Thank you for taking a look",
                  "score": 2,
                  "created_utc": "2026-02-16 11:47:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ns4cn",
              "author": "Swimming_Ad_5205",
              "text": "ÐÑ…Ð°Ñ…Ð°Ñ…Ð° Ð¾Ñ‡ÐµÐ½ÑŒ Ñ‚Ð¾Ñ‡Ð½Ð¾",
              "score": 2,
              "created_utc": "2026-02-16 10:16:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5odqli",
          "author": "lundrog",
          "text": "Seems safe ðŸ‘€",
          "score": 3,
          "created_utc": "2026-02-16 13:09:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5scot7",
              "author": "oulu2006",
              "text": "lol\n\n",
              "score": 2,
              "created_utc": "2026-02-17 01:10:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5v75dg",
          "author": "HarjjotSinghh",
          "text": "this just saved my pentesting life!",
          "score": 2,
          "created_utc": "2026-02-17 13:57:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pe4op",
          "author": "ResponsiblePlant8874",
          "text": "this repo will take sometime to continue update",
          "score": 1,
          "created_utc": "2026-02-16 16:18:45",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4v7by",
      "title": "OCMONITOR - a CLI tool to monitor OPENCODE CLI usage",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r4v7by/ocmonitor_a_cli_tool_to_monitor_opencode_cli_usage/",
      "author": "WriterOld3018",
      "created_utc": "2026-02-14 20:36:06",
      "score": 18,
      "num_comments": 3,
      "upvote_ratio": 0.92,
      "text": "https://preview.redd.it/95r6b42ktijg1.png?width=3790&format=png&auto=webp&s=e0b2919618f556d387b59e6071b3bb85890aa3bc\n\nHello opencode community,\n\n5 months ago I madeÂ ocmonitor, an open-source CLI tool to monitor opencode usage. Since yesterday (version 1.2.0+), opencode migrated from storing sessions in JSON files to using a SQLite database. Iâ€™ve updated ocmonitor to support this change.\n\nI also added a hierarchy view to show subagents as part of the parent session, and monitoring of output rate (TPS) to give an indication of model performance.\n\nI would appreciate any feedback or bug reports (preferably via GitHub). PRs and contributions are also welcome.  \n[https://github.com/Shlomob/ocmonitor-share](https://github.com/Shlomob/ocmonitor-share)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4v7by/ocmonitor_a_cli_tool_to_monitor_opencode_cli_usage/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5fclrh",
          "author": "rizal72",
          "text": "I love it! Beautifully done, very useful, very good job!",
          "score": 1,
          "created_utc": "2026-02-14 23:59:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ijgae",
          "author": "altsyst",
          "text": "Nice! Is there a way to inspect the whole context window besides having the number of tokens? I'm looking for a tool allowing me to see easily the whole context.",
          "score": 1,
          "created_utc": "2026-02-15 14:50:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kgktq",
          "author": "HarjjotSinghh",
          "text": "this is unreasonably cool actually!",
          "score": 1,
          "created_utc": "2026-02-15 20:33:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2uylw",
      "title": "MiniMax-M2.5 Now First to Go Live on NetMind (Before the Official Launch), Free for a Limited Time Only",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/gu8hlkc1r2jg1.png",
      "author": "MarketingNetMind",
      "created_utc": "2026-02-12 14:33:08",
      "score": 17,
      "num_comments": 12,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r2uylw/minimaxm25_now_first_to_go_live_on_netmind_before/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4zjtyr",
          "author": "Nexmean",
          "text": "> M2.5 surpasses Claude Opus 4.6 on both SWE-bench Pro and SWE-bench Verified, placing it among the absolute best models for real-world software engineering. \n\nCrazy if true",
          "score": 8,
          "created_utc": "2026-02-12 14:38:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o501c86",
              "author": "getaway-3007",
              "text": "It's always untrue.\n\nBecause imagine someone is selling an axe for $10, now you've built a new axe which outperforms the best(or 2nd best if you think 5.3-codex xhigh is #1) then why would you sell for cheap? You would at least sell for $6 or $7 because it would still be cheaper than $10. \n\nI think the good comparison is Sonnet 4.5. all the open-source models are in that range not the Opus, Gpt-5.2 xhigh, etc",
              "score": 3,
              "created_utc": "2026-02-12 16:04:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o507m1d",
                  "author": "RegrettableBiscuit",
                  "text": "I don't believe that M2.5 will beat Opus 4.6. However, compared to Anthropic's pricing, Chinese models like K2.5 and GLM-5, both of which are superb models, do provide much better performance and quota per cost.\n\n\nThere are a bunch of reasons for that, primarily the need for these companies to take away market share from the big American providers.Â ",
                  "score": 3,
                  "created_utc": "2026-02-12 16:33:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o50f82c",
                  "author": "Nexmean",
                  "text": "> why would you sell for cheap\n\nwhy would you release open weights as well?",
                  "score": 2,
                  "created_utc": "2026-02-12 17:09:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4zlhwj",
              "author": "Embarrassed_Bread_16",
              "text": "ill try and see",
              "score": 1,
              "created_utc": "2026-02-12 14:47:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o52kps7",
          "author": "HarjjotSinghh",
          "text": "how many devs actually need netmind's free cloud?",
          "score": 1,
          "created_utc": "2026-02-12 23:24:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zy48c",
          "author": "jhartumc",
          "text": "There is no free access ",
          "score": 1,
          "created_utc": "2026-02-12 15:49:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50mahi",
              "author": "LittleChallenge8717",
              "text": "try in opencode, free for a week",
              "score": 1,
              "created_utc": "2026-02-12 17:42:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o51460g",
              "author": "MarketingNetMind",
              "text": "So sry abt the inconvenience & thx for spotting this! We have fixed the issue. You are able to use it for completely free now",
              "score": 1,
              "created_utc": "2026-02-12 19:05:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zzxwb",
          "author": "jackai7",
          "text": "If free why Asking to add credit??",
          "score": 0,
          "created_utc": "2026-02-12 15:57:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5148s7",
              "author": "MarketingNetMind",
              "text": "So sry abt the inconvenience & thx for spotting this! We have fixed the issue. You are able to use it for completely free now",
              "score": 2,
              "created_utc": "2026-02-12 19:06:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r32eyj",
      "title": "Â£25~ budget. What is best for me?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r32eyj/25_budget_what_is_best_for_me/",
      "author": "TheSagaciousPanda",
      "created_utc": "2026-02-12 19:10:18",
      "score": 17,
      "num_comments": 40,
      "upvote_ratio": 0.95,
      "text": "I've used claude code for sometime now with a pro subscription but its become frustrating to use hitting session and weekly limits where they keep lowering it. I'm not a developer but i know my usage shouldnt be hitting limits like they are and therefore looking for a change. I also want to be able to use opencode again instead of claude code.\n\nI'm on the waiting list for opencode black which looks decent but no idea when that's coming and there are various subscriptions/ai stacks i can choose from but... I've no clue on what would be best for me.\n\n\\- Primary uses are ricing, debugging or optimising my computer.   \n\\- Secondary use would be asking questions, research for various things  \n\\- Sometimes use to vibecode small apps depending on what i need  \n  \nI'd appreciate any and all advice!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r32eyj/25_budget_what_is_best_for_me/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o519u6j",
          "author": "smile132465798",
          "text": "Codex and some provider that has kimi/minimax. Minimax 2.5 is really a solid sonnet replacement, kimi 2.5 is fast and codex for hard stuff",
          "score": 17,
          "created_utc": "2026-02-12 19:32:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51gudq",
          "author": "shaonline",
          "text": "If you want Opus-esque performance: Codex (via a ChatGPT Plus sub) for sure I mean not even a contest, at least while the rate limits are generous... You can use it through OpenCode \"legally\" as well.",
          "score": 6,
          "created_utc": "2026-02-12 20:06:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53jilr",
          "author": "BingGongTing",
          "text": "Codex until April (when 2x usage ends), then GLM Lite (should have GLM5 by then).\nGemini CLI also has a generous free tier.\nGitHub Copilot also has a free tier.",
          "score": 4,
          "created_utc": "2026-02-13 02:51:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51gy9o",
          "author": "Apprehensive_Half_68",
          "text": "Opus to create PRD, architect docs, and test driven development steps..then GLM 5 to code it up then Opus to grade. Rinse, repeat. Use Antigravity for Claude for free. And GLM for 3 bucks or so.",
          "score": 6,
          "created_utc": "2026-02-12 20:07:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o536wrn",
              "author": "ECrispy",
              "text": "how do you set this up? using different llms for the tasks? do you have to switch llm in opencode, copy/paste previous conversations etc?",
              "score": 3,
              "created_utc": "2026-02-13 01:33:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56wzel",
                  "author": "Apprehensive_Half_68",
                  "text": "Like everything else, I just ask AI to set it up, i'm really that lazy and it does a better job than I ever could.  \"I want to help save money by using x ide as a world-class software architect and y for a regular senior dev to do the actual coding up..research and develop and perfect a series of methodology docs that always keep up to date so i can switch back and forth any time and both x and y can pick up where the other left off. Right now model x costs n.1 USD input and n.2 Output and model y cost n.3 USD input and n.4 USD output per 1m tokens . Research the latest trends in AI task-driven-development theory then adapt it for someone who knows something about code but doesn't really want to even look at it much. it should include all best practices for security, docs, and any other best practices a software development studio would put into place but adapted for a lazy vibe coder. .Put all these magical docs in ./docs/methodology/. Any new ai agent who begins coding should be able to start working in the repo immediately being pointed by [AGENTS.md](http://AGENTS.md) to all the correct files it needs.  Don't over-engineeer or under-engineer this. Use Claude's 'Ultrathink' mode if available or whichever is the best agent available in this current ide and spend a bunch of time thinking about how to improve on this method.  Oh and then make an easy to read pretty user guide in markdown language for me to use this with step by step instructions i can keep in another window to follow along.  Oh and add a paragraph after each step to teach me what you are doing and why so I can learn to do better next time.  What other things should I be asking you that i have forgotten in order to save even more money in my budget?\"\n\nThen before i send it I have them clean up that prompt itself, wipe its memory then paste it.",
                  "score": 1,
                  "created_utc": "2026-02-13 16:51:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o53eqr4",
              "author": "kam1L-",
              "text": "Iâ€™m doing this with Claude as a brain and Gemini free tier 1 as a builder, so far best free combo for small stuff and some coding.",
              "score": 1,
              "created_utc": "2026-02-13 02:21:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o51zchp",
          "author": "SnooHesitations6473",
          "text": "OpenAI plus plan, very generous limits, separated limits for chat app. Top tier models. ",
          "score": 2,
          "created_utc": "2026-02-12 21:34:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53ax4v",
          "author": "DependentGuava2343",
          "text": "try copilot, with agents +subagents it might be a good thing for you, I believe \"copilot pro +\" is around Â£25\\~",
          "score": 2,
          "created_utc": "2026-02-13 01:58:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o515vkq",
          "author": "e979d9",
          "text": "[z.ai](http://z.ai) pro plan. I can share an invite link with you if you wish (10% off for you & me). Personally I'm satisfied with the service, and quite impressed GLM-5 that came out yesterday",
          "score": 2,
          "created_utc": "2026-02-12 19:13:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5237lr",
              "author": "TestTxt",
              "text": "minimax is cheaper and better",
              "score": 2,
              "created_utc": "2026-02-12 21:52:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o55855w",
              "author": "Pipimi",
              "text": "They increased the priced after glm 5 ðŸ¥€. I paid $34 for an annual lite plan",
              "score": 1,
              "created_utc": "2026-02-13 11:06:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5175ku",
          "author": "not_particulary",
          "text": "I just switched from Claude pro to codex, trying that out. It's honestly not a bad idea to just get an API key from some aggregator like openrouter, either.",
          "score": 1,
          "created_utc": "2026-02-12 19:19:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52iaj7",
          "author": "TheSagaciousPanda",
          "text": "Thanks for the replies everyone. \n\nI have antigravity on the freeplan but that burns through rate limits quickly if you use opus so unless im missing something that isn't viable unless you mean getting the google ai pro at 18.99 for the gemini models and antigravity usage. Problem is that people are getting banned from using claude models in opencode now aren't they so thats risky?\n\nI'm thinking chatgpt plus subscription for the codex models in opencode and then use [z.ai](http://z.ai) lite plan for GLM5.0 but I have questions\n\na) Would getting [z.ai](http://z.ai) pro plan be better than the above? What justifies the increased cost for it compared to chatgpt plus subscription currently?  \nb) What would be the recommended models regarding the relevant model/models mentioned for different stages - planning > executing > auditing/fixing/improving\n\n",
          "score": 1,
          "created_utc": "2026-02-12 23:10:34",
          "is_submitter": true,
          "replies": [
            {
              "id": "o52omp0",
              "author": "No_Success3928",
              "text": "A) No\nB) Stick with $20 claude or chatgpt, anything else will be a recipe for misery in your situation",
              "score": 3,
              "created_utc": "2026-02-12 23:46:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o53rfqi",
          "author": "keroro7128",
          "text": "My approach is to utilize various incentives to use high-end service providers on the market, including Claude, Codex, and Gemini, while keeping costs as low as possible. This year's cost is approximately $7.60 USD , or $0.63 per month, which is sufficient for casual users, such as those purely interested or students. The only drawback is Claude's relatively low quota, but I think it's enough for creating and conceiving plans. Codex and Gemini offer quotas equivalent to those for regular plus users.",
          "score": 1,
          "created_utc": "2026-02-13 03:41:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o541zui",
          "author": "Icy-Organization-223",
          "text": "I wrote this before in another post but I'll give you the breakdown. If you do things in smaller increments where the model doesn't have to keep building, fixing, and in turn making the context bloat. Just do small directed prompts by giving a file path makes me work on Claude pro plans all day and opencode with kimi 2.5 non stop all day for about $3-$4 but I mean alot of coding. I have let loose when I am lazy and let the model search around but have found out if you give full file.paths and limit it's scope it's better. Be specific on the prompt and the few things it should do. \n\nIn short The plan modes work real well with tight scope and don't let the context go to far as it gets sent again and again. For every new prompt. Start a new session for each file or few files that are related. It's so much faster as well. I do sometimes tell.it to browse my code and suggest things but that for assessments not getting tasks done.\n\nIf I wanted to get it down to less I would even use dumber models for simple refactors. Many cheaper models interpret and debug pretty well when given something to look at but very few models can write something new well. So preserve the refactor assessment document use cases in opencode to smaller parameter models. Now if your asking to write a whole app with it iterating and with context bloat it will burn tokens like a wild fire. \n\nAlso chatgpt free is excellent for code if you give it a full coding document and ask it to fix something. I use that for free and when I want to assess bugs etc. It usually fixes them. I leave sonnet,haiku,and kimi for more agentic coding. It's all about style. I have fit massive coding in the pro plan. But again you complained about the weekly limit which I can't seem to hit",
          "score": 1,
          "created_utc": "2026-02-13 04:54:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55109r",
          "author": "Dinth",
          "text": "Im in the same situation as op. My main useless are managing my docker compose files and configs for various services, home assistant automations and nix configurations. I do have access to Gemini 3.0 pro from work, and find that quality Sonnet 4.5 answers is literally a several levels above Gemini Pro for my usage. Would OpenAI codex be comparable for my use case to Gemini 3.0 Pro or more like Sonner 4.5? \n\nAlso Iâ€™ve got an ollama server with 16gb gpu, potentially will expand to around 24gb soon. Is that something I can realistically use with my OpenCode? Iâ€™ve done several attempts to use OpenCode with 13b models so far, and it seems that 13b models fail to do even simplest tooling",
          "score": 1,
          "created_utc": "2026-02-13 10:01:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58lp2c",
              "author": "shaonline",
              "text": "GPT/GPT-Codex takes on Opus, they each have their strengths. You can tweak the reasoning effort to save on tokens if you just need a decent executor (GPT-Codex medium).\n\nSmall (local) models will always struggle with proper output formatting (at the end of the day it just sends a big JSON for each command) nevermind actual \"knowledge\", and it's hardly something you'll band-aid with MCP servers.",
              "score": 1,
              "created_utc": "2026-02-13 21:48:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5623po",
          "author": "Queasy_Change4668",
          "text": "[https://cortexai.io/](https://cortexai.io/) , it gives unlmited usage of every ai in the world, Ä± deffinitly suggest you to try it, go and join to the discord and write someone to pay,  [https://discord.gg/VVsYkH9f](https://discord.gg/VVsYkH9f)",
          "score": 1,
          "created_utc": "2026-02-13 14:21:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kt1hl",
              "author": "ChocolateActive",
              "text": "How does this service work? It seems to all be in Turkish. I found the English Discord, but it's still a bit confusing regarding the different services and APIs. Are they able to be used with OpenCode, ClaudeCode, etc., and other harnesses?",
              "score": 1,
              "created_utc": "2026-02-15 21:37:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5c1bjs",
          "author": "HarjjotSinghh",
          "text": "ahhhhh budget hack time.",
          "score": 1,
          "created_utc": "2026-02-14 13:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o516y4d",
          "author": "HarjjotSinghh",
          "text": "clumsy devs making $25 budget look impossible. try i386 mode.",
          "score": 0,
          "created_utc": "2026-02-12 19:18:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53hvrk",
          "author": "Diligent_Speaker4692",
          "text": "As you mention you should do PRD with any good chat then put on you project with claude code. Claude code is good for user intention and  good follow instructions so this is the way.  For a developer my best combo is openspec + codex 5.3 high codex planning and 5.3 codex for implement this work perfect",
          "score": 0,
          "created_utc": "2026-02-13 02:41:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51cs19",
          "author": "[deleted]",
          "text": "[removed]",
          "score": -4,
          "created_utc": "2026-02-12 19:47:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51sevd",
              "author": "touristtam",
              "text": "> were an early stage startup so basically we give claude code max equivalent subscription for the price of the pro plan for now as were early and users stay at same price for 1 year\n\nSorry I might be thick but can you break down what you are trying to say?",
              "score": 3,
              "created_utc": "2026-02-12 21:01:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o52nyvx",
                  "author": "No_Success3928",
                  "text": "Looks like they are reselling claude access?",
                  "score": 2,
                  "created_utc": "2026-02-12 23:42:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o52cs40",
                  "author": "TheSagaciousPanda",
                  "text": "u/Popular-Category711 not sure what your copy paste response is trying to say because you dont even mention what service your trying to offer as a new startup and how your able to get what your claiming",
                  "score": 1,
                  "created_utc": "2026-02-12 22:40:51",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o52ea0s",
          "author": "Rygel_XV",
          "text": "You can also checkout [synthetic.new](http://synthetic.new). They have Kimi 2.5, GLM 4.7 (but plan to host 5 once it is released as open source) and Minimax 2.1 (and will probably also host 2.5 once it is released as open source). \n\nI am using them myself, and I am very happy with their performance. \n\n[https://synthetic.new/?referral=SNJDbFCgSUZso9E](https://synthetic.new/?referral=SNJDbFCgSUZso9E)",
          "score": -3,
          "created_utc": "2026-02-12 22:48:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52p5p6",
              "author": "No_Success3928",
              "text": "OP should also know new signups are on a waitlist for access.",
              "score": 0,
              "created_utc": "2026-02-12 23:49:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o548luj",
                  "author": "Rygel_XV",
                  "text": "Oh, I didn't know about the waitlist. Thank you for mentioning it.",
                  "score": 2,
                  "created_utc": "2026-02-13 05:45:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o52tanz",
              "author": "Shep_Alderson",
              "text": "Seconding synthetic.new. Their customer support has been top notch and Iâ€™m sure they will have the latest Kimi and MiniMax models up soon. Iâ€™m guessing they just need to prove it out and such first.",
              "score": -1,
              "created_utc": "2026-02-13 00:13:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o52h8ue",
          "author": "ch4dev_lab",
          "text": "!RemindMe 1week",
          "score": -1,
          "created_utc": "2026-02-12 23:04:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52hh21",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 7 days on [**2026-02-19 23:04:49 UTC**](http://www.wolframalpha.com/input/?i=2026-02-19%2023:04:49%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/opencodeCLI/comments/1r32eyj/25_budget_what_is_best_for_me/o52h8ue/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FopencodeCLI%2Fcomments%2F1r32eyj%2F25_budget_what_is_best_for_me%2Fo52h8ue%2F%5D%0A%0ARemindMe%21%202026-02-19%2023%3A04%3A49%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201r32eyj)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": -1,
              "created_utc": "2026-02-12 23:06:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o52kjn3",
          "author": "ScorpionOfWar",
          "text": "Been using open-source models more lately for private stuff, I got the 100$ Claude Sub for work.\n\nEnded up trying [Synthetic](https://synthetic.new/?referral=hcHozzHNE8CxVvz)(\\~50% off) and it's been very solid so far, alternatively [Z.ai](https://z.ai/subscribe?ic=KQ77ZVKLB5)(\\~10% off) for just the GLM Models, nice for coding, but kind of unreliable at the moment. They host open-source models and the API is OpenAI-compatible so it just plugs into your CLI or Dev Environment. $20/mo flat for the subscription tier is nice.\n\nWith my ref link to Synthetic and Z you are able to get a rebate.",
          "score": -1,
          "created_utc": "2026-02-12 23:23:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52ovlv",
              "author": "No_Success3928",
              "text": "Synthetic have a wait list for new signups. Perhaps you should mention that?",
              "score": 2,
              "created_utc": "2026-02-12 23:47:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o52q4zs",
                  "author": "ScorpionOfWar",
                  "text": "That is true, they recently got new GPU resources though. So there will be more places available soon",
                  "score": 0,
                  "created_utc": "2026-02-12 23:54:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o53juil",
          "author": "Senior-Cod993",
          "text": "anthopic needs to stop tring to push woke agenda into the models",
          "score": -5,
          "created_utc": "2026-02-13 02:53:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4vefc",
      "title": "Holy shit, Codex-5.3-Spark on OpenCode is FAST!",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r4vefc/holy_shit_codex53spark_on_opencode_is_fast/",
      "author": "jpcaparas",
      "created_utc": "2026-02-14 20:44:22",
      "score": 15,
      "num_comments": 16,
      "upvote_ratio": 0.76,
      "text": "Will provide some detailed feedback soon, but for those on the fence:\n\nEVERYTHING IS **INSTANT**. IT IS THE REAL THING!\n\n*\"I could smell colors, I could feel sounds.\"*\n\n  \nUpdate: I'm going back to Plus. The limited weekly cap and compaction issues are simply to hard to justify for the $200 price tag.\n\nhttps://preview.redd.it/fhp62ppcskjg1.png?width=1504&format=png&auto=webp&s=0413284d29b14420a50bf01cfa5e494de0abacc3\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4vefc/holy_shit_codex53spark_on_opencode_is_fast/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5ef77e",
          "author": "jpcaparas",
          "text": "https://preview.redd.it/wsb29jnnvijg1.png?width=402&format=png&auto=webp&s=7d31b23879ab602f5b04840c78cfa635516a71f2\n\nAs you've probably already read, the context length is only 128K, so you'll have to leverage subagents where possible to break down bulky tasks.",
          "score": 10,
          "created_utc": "2026-02-14 20:47:49",
          "is_submitter": true,
          "replies": [
            {
              "id": "o5g7tqd",
              "author": "franz_see",
              "text": "Yep. Youâ€™d have to create a task list and keep delegating to a codex spark powered subagent to maximize it.\n\nTbh, cant imagine any other agent being able to maximize it as much as opencode. I could be wrong though.",
              "score": 2,
              "created_utc": "2026-02-15 03:25:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5g94fs",
                  "author": "jpcaparas",
                  "text": "No, you're right. That's why only use OpenCode and Claude Code these days. I requested a refund from OpenAI a few minutes ago for the atrocious weekly limits of Spark.",
                  "score": 2,
                  "created_utc": "2026-02-15 03:34:34",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5efbak",
          "author": "HarjjotSinghh",
          "text": "okay first post ever? how's that instant thing work?",
          "score": 5,
          "created_utc": "2026-02-14 20:48:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ei7hq",
              "author": "ExtentOdd",
              "text": "Try Cerebras, you will feel that constant thing",
              "score": 5,
              "created_utc": "2026-02-14 21:04:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5g7yaw",
                  "author": "franz_see",
                  "text": "I have cerebras. I get rate limited a lot with its GLM 4.7 though ðŸ˜… but itâ€™s super fast! That being said, itâ€™s also super fast at consuming tokens! ðŸ˜…",
                  "score": 3,
                  "created_utc": "2026-02-15 03:26:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5epsr9",
          "author": "jpcaparas",
          "text": "Okay, some early thoughts:\n\n- Auto compaction is horrible with Spark\n- It's very capable and very snappy, just avoid hitting the context window limits. \n- Your only noticeable bottleneck are external API calls responses\n- Spark is better used a hardcoded model on subagents instead of being the main model, ie use Opus 4.6,  Codex-5.3 or Kimi K2.5 as the orchestrator and have most if not all subagents use Spark.",
          "score": 9,
          "created_utc": "2026-02-14 21:45:37",
          "is_submitter": true,
          "replies": [
            {
              "id": "o5hto00",
              "author": "aithrowaway22",
              "text": "Can Kimi 2.5 really replace Codex 5.3/GPT 5.2 (on high) / Opus 4.5/4.6 in architecture/orchestrator roles ?  \nEven on LocalLama most people agree that open source models are not on that level for complex tasks.  \n",
              "score": 1,
              "created_utc": "2026-02-15 11:59:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5evrtz",
              "author": "segmond",
              "text": "how do you set up k2.5 as orchestrator and subagents to use spark?",
              "score": 1,
              "created_utc": "2026-02-14 22:18:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5extvm",
                  "author": "jpcaparas",
                  "text": "say for example, you have a slash command and that slash command invokes subagents: dont hardcode the model on the md file of the slash command, use ctrl + p to do model selection, but for the subagents that you definitely need spark for, hardcode them on the agent's md file",
                  "score": 4,
                  "created_utc": "2026-02-14 22:30:30",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5f7crw",
          "author": "j00stmeister",
          "text": "Cool, good to hear. Quick question tho: do you use it through the API or a ChatGPT plus/pro subscription?  \nWhen I use it using my subscription I get 'The 'gpt-5.3-codex-spark' model is not supported when using Codex with a ChatGPT account.'",
          "score": 1,
          "created_utc": "2026-02-14 23:26:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fcdlg",
              "author": "jpcaparas",
              "text": "I use the $200 Pro subscription. It's only available there... for now. Given the competitive nature these days, I doubt OpenAI will silo it for too long within that tier",
              "score": 1,
              "created_utc": "2026-02-14 23:58:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5fcv5k",
                  "author": "j00stmeister",
                  "text": "Ah good to know, thanks!",
                  "score": 0,
                  "created_utc": "2026-02-15 00:01:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5eeru8",
          "author": "jpcaparas",
          "text": "I'm more interested how it performs with layered subagents, so I'll factor that in too with feedback.",
          "score": 0,
          "created_utc": "2026-02-14 20:45:29",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r22cml",
      "title": "CodeNomad v0.10.3 Released - Viewer for Changes, Git Diff and workspace files",
      "subreddit": "opencodeCLI",
      "url": "https://v.redd.it/b7mh9h879wig1",
      "author": "Recent-Success-1520",
      "created_utc": "2026-02-11 16:42:26",
      "score": 15,
      "num_comments": 2,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r22cml/codenomad_v0103_released_viewer_for_changes_git/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4uqbnj",
          "author": "idkwtftbhmeh",
          "text": "Amazing project, ty\n\n",
          "score": 2,
          "created_utc": "2026-02-11 19:31:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bcxnp",
          "author": "HarjjotSinghh",
          "text": "this looks like a terminal's new favorite snack!",
          "score": 1,
          "created_utc": "2026-02-14 09:31:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6marc",
      "title": "Any difference when using GPT model inside Codex vs OpenCode?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r6marc/any_difference_when_using_gpt_model_inside_codex/",
      "author": "ponury2085",
      "created_utc": "2026-02-16 21:19:33",
      "score": 11,
      "num_comments": 11,
      "upvote_ratio": 0.79,
      "text": "I'm a die-hard fan of OpenCode - because of free model, how easy it is to use subagents, and just because it's nice. But I wonder if anyone finds GPT models better in Codex? I cannot imagine why they could possibly work better there, but maybe models are just trained that way, so they \"know\" the tools etc? Anyone noticed anything like that?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r6marc/any_difference_when_using_gpt_model_inside_codex/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5r8t8v",
          "author": "itsjase",
          "text": "I think both claude code and codex have some magic sauce to work better with their respective models.\n\nI personally think codex + 5.3 codex is way ahead of opencode + 5.3 codex. I'm realising now the harness matters just as much as the model these days.",
          "score": 9,
          "created_utc": "2026-02-16 21:34:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5v8pp0",
              "author": "BodeMan5280",
              "text": "that's interesting... I think it comes down to speed for me. OpenCode seems to just get shit done (yes, ironic pun to GSD). Don't get me wrong, Codex is KILLER at getting shit done, but slower IMO.",
              "score": 1,
              "created_utc": "2026-02-17 14:06:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5r7bv5",
          "author": "TechCynical",
          "text": "This is what people mean when they say \"the harness\". Using it in codex means you get the bare bones experience. Not bad but it just means it isn't fine tuned to work specifically for coding /what you would want for coding at least. \n\nThere's a concern for over engineering but that's why things are open source. Claudecode for example has a lot of changes to its system prompt to work well for everything Claude code will try to do like call mini agents and tools during it's execution. Codex afaik actually has nothing but I could be wrong. GitHub copilot has its own too supposedly tuned for multi model workflows, and opencode has their own as well.\n\nImo all models work better in opencode. Sometimes this changes with select models, but it's a safe bet just to use opencode.",
          "score": 6,
          "created_utc": "2026-02-16 21:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uudx6",
              "author": "Morisander",
              "text": "Would you kindly explain your first and last paragraph? This way it sounds a little likeâ€¦ bullshit?",
              "score": 2,
              "created_utc": "2026-02-17 12:41:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tjliv",
          "author": "georgiarsov",
          "text": "I tried codex 5.3 in opencode on release and can confirm it was 100% shit. I couldnâ€™t believe the huge gap between my experience and that of the people using it in codex",
          "score": 1,
          "created_utc": "2026-02-17 05:52:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tpxpz",
          "author": "widonext",
          "text": "For me there is a difference, but it works great in opencode, so itâ€™s fine for me",
          "score": 1,
          "created_utc": "2026-02-17 06:45:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5u1gbv",
          "author": "Round_Mixture_7541",
          "text": "Yes. AI harnesses built by their respective model producers tend to work better together.",
          "score": 1,
          "created_utc": "2026-02-17 08:32:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5u2uij",
          "author": "Open_Scallion9015",
          "text": "I had this experience myself previously but it seems that since a month or so this gap has narrowed or maybe even completely closed. Personally did not had to urge to use the Codex harness recently at all.",
          "score": 1,
          "created_utc": "2026-02-17 08:45:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vlrc6",
          "author": "HarjjotSinghh",
          "text": "this sucks too much i'd pay to use this version",
          "score": 1,
          "created_utc": "2026-02-17 15:14:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rw5q7",
          "author": "nyldn",
          "text": "Latest version of Claude Octopus utilises Codex 5.3 smartly https://github.com/nyldn/claude-octopus",
          "score": -5,
          "created_utc": "2026-02-16 23:35:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5txeq6",
              "author": "KnifeFed",
              "text": "Sir, this is the OpenCode sub.",
              "score": 2,
              "created_utc": "2026-02-17 07:54:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}