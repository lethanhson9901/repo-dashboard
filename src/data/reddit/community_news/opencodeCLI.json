{
  "metadata": {
    "last_updated": "2026-02-08 08:47:11",
    "time_filter": "week",
    "subreddit": "opencodeCLI",
    "total_items": 20,
    "total_comments": 187,
    "file_size_bytes": 192104
  },
  "items": [
    {
      "id": "1qtqx2p",
      "title": "OpenCode Bar 2.1: Now with CLI + Per-Provider Subscription Tracking",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/sxdtdls302hg1.png",
      "author": "kargnas2",
      "created_utc": "2026-02-02 09:54:58",
      "score": 78,
      "num_comments": 17,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qtqx2p/opencode_bar_21_now_with_cli_perprovider/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o35kmbo",
          "author": "Possible-Text8643",
          "text": "mac only?",
          "score": 5,
          "created_utc": "2026-02-02 13:49:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38ikf2",
              "author": "trenescese",
              "text": "I think you'd need something like https://yasb.dev/ to run a widget like that on windows?",
              "score": 1,
              "created_utc": "2026-02-02 22:12:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37banq",
          "author": "Specialist-Yard3699",
          "text": "Looks nice. No Linux in plans?",
          "score": 2,
          "created_utc": "2026-02-02 18:49:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dbqhl",
              "author": "buggytheking",
              "text": "I've made something similar on Linux. Check it out here and lemme know what else to add.https://github.com/OmegAshEnr01n/GnomeCodexBar",
              "score": 3,
              "created_utc": "2026-02-03 16:54:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ddfr6",
                  "author": "Specialist-Yard3699",
                  "text": "Will test today. Any plans for opencode-zen/hype-chinese providers(zai/minimax/kimi)?",
                  "score": 1,
                  "created_utc": "2026-02-03 17:02:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34t6uv",
          "author": "jellydn",
          "text": "Nice. Thanks, I will give it a try soon :)",
          "score": 1,
          "created_utc": "2026-02-02 10:28:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34xfkk",
          "author": "Financial_Reward2512",
          "text": "Any similar product where we can use Claude Code with multiple Provider and show this bar as well.?",
          "score": 1,
          "created_utc": "2026-02-02 11:06:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35f0ob",
              "author": "United_Bandicoot1696",
              "text": "Quotio",
              "score": 1,
              "created_utc": "2026-02-02 13:17:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o36ptdn",
          "author": "_w_8",
          "text": "neat!! how are you using claude code sub in opencode still though?",
          "score": 1,
          "created_utc": "2026-02-02 17:11:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37hzpv",
          "author": "0sko59fds24",
          "text": "Anthropic still bans users using CC in opencode right",
          "score": 1,
          "created_utc": "2026-02-02 19:19:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bw028",
          "author": "VlaadislavKr",
          "text": "How to connect gemini quota based?",
          "score": 1,
          "created_utc": "2026-02-03 12:20:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3c0gnu",
          "author": "renan_william",
          "text": "Works on Intel Mac or just Silicon? ",
          "score": 1,
          "created_utc": "2026-02-03 12:51:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3grey4",
          "author": "Powerful_Associate85",
          "text": "Iâ€™m using Ollama on OpenCode, but thereâ€™s no support?",
          "score": 1,
          "created_utc": "2026-02-04 03:12:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3grint",
              "author": "kargnas2",
              "text": "What do you want to track of Ollama?",
              "score": 1,
              "created_utc": "2026-02-04 03:12:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3gs8zh",
                  "author": "Powerful_Associate85",
                  "text": "I mean Ollama cloud plan, for multiple models",
                  "score": 1,
                  "created_utc": "2026-02-04 03:17:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3hj1mu",
          "author": "touristtam",
          "text": "Side note: I like the review bot you are using. Any more info on that?",
          "score": 1,
          "created_utc": "2026-02-04 06:21:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41w6mc",
          "author": "Character_Cod8971",
          "text": "Please make it for Windows aswell",
          "score": 1,
          "created_utc": "2026-02-07 08:55:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwiy50",
      "title": "My mobile setup",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/3wgo2hmdwnhg1.jpeg",
      "author": "tamtaradam",
      "created_utc": "2026-02-05 11:34:45",
      "score": 76,
      "num_comments": 17,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qwiy50/my_mobile_setup/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3q2uc4",
          "author": "redoubledit",
          "text": "If you want an alternative to Termius, look at [Termix](https://github.com/Termix-SSH/Termix). If you run a VPS already, itâ€™s easy to set Termix up so you get the Termius stuff but open source.",
          "score": 7,
          "created_utc": "2026-02-05 14:46:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qevy4",
              "author": "tamtaradam",
              "text": "hostinger vps already serves web console access, but I really like termius nativeness, especially when using external keyboard",
              "score": 1,
              "created_utc": "2026-02-05 15:45:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3pb1tv",
          "author": "PersonalityOne2559",
          "text": "Why not use a laptop atp?",
          "score": 4,
          "created_utc": "2026-02-05 11:59:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pbs8r",
              "author": "ahmetegesel",
              "text": "because",
              "score": 4,
              "created_utc": "2026-02-05 12:04:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3pf2fs",
                  "author": "Michaeli_Starky",
                  "text": "Because",
                  "score": 4,
                  "created_utc": "2026-02-05 12:28:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3pgyrc",
              "author": "tamtaradam",
              "text": "I'd say multiple reasons:\n\n\\- laptops are so 2025 ;)\n\n\\- I take ipad on my private trips anyway\n\n\\- I don't want to use company laptop for private stuff",
              "score": 2,
              "created_utc": "2026-02-05 12:41:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3pylo6",
                  "author": "bigh-aus",
                  "text": "I totally get that. If I was to travel with a laptop, I'd have my work laptop, personal laptop, iPad, work phone, personal phone. \n\n That said, my personal laptop is a Lenovo X1 Nano because it's so light.",
                  "score": 0,
                  "created_utc": "2026-02-05 14:24:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3p91vr",
          "author": "_d1re",
          "text": "Can you tell us how to do this?",
          "score": 1,
          "created_utc": "2026-02-05 11:43:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3p9gx2",
              "author": "tamtaradam",
              "text": "similar setup is covered in this video [https://www.youtube.com/watch?v=FEDiAHzS0zw](https://www.youtube.com/watch?v=FEDiAHzS0zw)",
              "score": 2,
              "created_utc": "2026-02-05 11:46:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3pnerx",
          "author": "Few-Mycologist-8192",
          "text": "nice set up , i have the same keybbord K380 logi; right?",
          "score": 1,
          "created_utc": "2026-02-05 13:21:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q19v3",
          "author": "sdexca",
          "text": "Noice, I am going to build a similar setup soon, iPhone 16 PM + netcup VPS 1000 G12 + termius + opencode + antigravity auth plugin / openai chagpt auth + opus/codex.",
          "score": 1,
          "created_utc": "2026-02-05 14:38:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3r7uv4",
          "author": "asmkgb",
          "text": "wow that's amazing to see, i just bought this exact keyboard too hahaha\n\ni love that we are now able to do great stuff on the go with just internet",
          "score": 1,
          "created_utc": "2026-02-05 17:59:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3wg08r",
          "author": "Aerion23",
          "text": "I have something similar, but i am missing the ability to quickly check the codebase like I can do with an code editor. Is there not already a opencode mobile app in the works?",
          "score": 1,
          "created_utc": "2026-02-06 13:48:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3wtgnr",
          "author": "Recent-Success-1520",
          "text": "Ditch the keyboard too with CodeNomad https://github.com/NeuralNomadsAI/CodeNomad",
          "score": 1,
          "created_utc": "2026-02-06 14:58:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ztvf2",
          "author": "NullVoidXNilMission",
          "text": "white mode eww",
          "score": 1,
          "created_utc": "2026-02-06 23:56:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o416r74",
          "author": "flexrc",
          "text": "Did you try running it directly in terminus?",
          "score": 1,
          "created_utc": "2026-02-07 05:08:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pk1vt",
          "author": "atkr",
          "text": "go to bluetooth settings, connect the keyboard? (please not this model ðŸ˜‚) There is nothing to it",
          "score": 0,
          "created_utc": "2026-02-05 13:01:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu44yh",
      "title": "Notes after using Claude Code and OpenCode side by side",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qu44yh/notes_after_using_claude_code_and_opencode_side/",
      "author": "Arindam_200",
      "created_utc": "2026-02-02 19:00:16",
      "score": 72,
      "num_comments": 40,
      "upvote_ratio": 0.93,
      "text": "Iâ€™ve been using Claude Code pretty heavily for day-to-day work. Itâ€™s honestly one of the first coding agents Iâ€™ve trusted enough for real production tasks.\n\nThat said, once you start using it *a lot*, some tradeoffs show up.\n\nCost becomes noticeable. Model choice matters more than you expect. And because itâ€™s a managed tool, you donâ€™t really get to see or change how the agent works under the hood. You mostly adapt your workflow to it.\n\nOut of curiosity, I started testing OpenCode (Got Hyped up from X & reddit TBH). Didnâ€™t realize how big it had gotten until recently. The vibe is very different.\n\nClaude Code feels guarded and structured. It plans carefully, asks before doing risky stuff, and generally prioritizes safety and predictability.\n\nOpenCode feels more like raw infrastructure. You pick the model per task. It runs commands, edits files, and you validate by actually running the code. More control, less hand-holding.\n\nBoth got the job done when I tried real tasks (multi-file refactors, debugging from logs). Neither â€œfailed.â€ The difference was *how* they worked, not whether they could.\n\nIf you want something managed and predictable, Claude Code is great. If you care about flexibility, cost visibility, and owning the workflow, OpenCode is interesting.\n\nI wrote up a longer comparison [here](https://www.tensorlake.ai/blog/opencode-the-best-claude-code-alternative) if anyone wants the details.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qu44yh/notes_after_using_claude_code_and_opencode_side/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o38ob1f",
          "author": "Guinness",
          "text": "CC has horrible TUI issues, bad framerates, and oh god the bug where it constantly jumps to the top of the history scrollback.\n\nCC introduced me to all of these tools after years of development with nano/vi{m}, but it was so buggy I branched out to cline, and eventually to opencode.\n\nOpencode is clearly the winner right now. It feels like CC was written by an LLM. Like....it works? But its buggy as fuck.",
          "score": 14,
          "created_utc": "2026-02-02 22:41:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bdv87",
              "author": "ReporterCalm6238",
              "text": "I mean opencode was also written by LLMs",
              "score": 3,
              "created_utc": "2026-02-03 09:45:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fs2jr",
                  "author": "priestoferis",
                  "text": "The difference is that CC engineers clearly had no idea about TUI. There was an interview couple of months back where some lead said they didn't know if this could be made in a TUI. Like what is it you can't make in a TUI? I wouldn't pick a TUI for video rendering, but you _can_ if you want to.\n\nTbh, they almost couldn't considering how they try doing rendering :D",
                  "score": 2,
                  "created_utc": "2026-02-03 23:54:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3afs82",
              "author": "Accomplished-Toe7014",
              "text": "Well Anthropic was seemingly proud when they said that 100% of their code is written by AIs",
              "score": 5,
              "created_utc": "2026-02-03 04:47:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o38rmsv",
              "author": "Positive-Badger6588",
              "text": "the TUI has issues but ive kept tryng to go back to opencode just to find out the performance and harness is just much better optimized on cc. I have it hooked up to a nvim wrapper so all the diffs show up in nvim for me. the TUI outside of general explanation just becomes a bit irrelavant for me.",
              "score": 1,
              "created_utc": "2026-02-02 22:58:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37ijy1",
          "author": "hey_ulrich",
          "text": "The only feature from CC that I miss is that CC can run bash commands in the background and easily check its logs. That's it. For everything else, OpenCode is superior. Once you set your custom modes, and add plugins, it's awesome.",
          "score": 22,
          "created_utc": "2026-02-02 19:22:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37wb6o",
              "author": "nmiljkovic89",
              "text": "There is an opencode pty plugin for that",
              "score": 10,
              "created_utc": "2026-02-02 20:27:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37zkdh",
                  "author": "hey_ulrich",
                  "text": "Thanks, I'll check it out",
                  "score": 1,
                  "created_utc": "2026-02-02 20:42:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o393m7x",
                  "author": "MyriadAsura",
                  "text": "Care to share a link brother?",
                  "score": 1,
                  "created_utc": "2026-02-03 00:03:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3l9fzf",
              "author": "mdrahiem",
              "text": "Also being able to type multi line in CC ðŸ˜…",
              "score": 0,
              "created_utc": "2026-02-04 20:01:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o38qwbj",
          "author": "poop_harder_please",
          "text": "is anyone still using the oauth authentication method? Anthropic sort of backpedaled on banning accounts for using oauth with external providers, but I'm worried about taking the risk on my account until it's a sure thing",
          "score": 2,
          "created_utc": "2026-02-02 22:54:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38skkd",
              "author": "Positive-Badger6588",
              "text": "wait, are they blocking access or banning people form if using it? blocking users sounds kind of retarded lol",
              "score": 2,
              "created_utc": "2026-02-02 23:03:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3capjw",
                  "author": "poop_harder_please",
                  "text": "Apparently full blocks. But they mightâ€™ve lightened up after the backlash and the counter positioning of OAIâ€™s plans being third-party-harness-friendly",
                  "score": 2,
                  "created_utc": "2026-02-03 13:51:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3aq3sp",
              "author": "Fickle_Permi",
              "text": "Yeah I use it with no problem. Iâ€™m not a heavy user though. I maybe hit the daily limit once a week.",
              "score": 2,
              "created_utc": "2026-02-03 06:05:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o38sogz",
              "author": "FunnyRocker",
              "text": "I'd really like to know this also",
              "score": 1,
              "created_utc": "2026-02-02 23:04:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dw774",
              "author": "james__jam",
              "text": "Was there any official announcement that indicated backpedaling? Or is it more of an observation that theyâ€™re not actively banning people now?",
              "score": 1,
              "created_utc": "2026-02-03 18:27:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3f38aa",
              "author": "Keep-Darwin-Going",
              "text": "When did they back pedal? I see people complaining about getting banned regularly",
              "score": 1,
              "created_utc": "2026-02-03 21:47:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fqyoy",
                  "author": "poop_harder_please",
                  "text": "[https://x.com/trq212/status/2009689816468992334?s=20](https://x.com/trq212/status/2009689816468992334?s=20)",
                  "score": 1,
                  "created_utc": "2026-02-03 23:48:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3kfoxz",
              "author": "IntrepidLawfulness42",
              "text": "Yep, I'm using it daily, no issues the last two weeks. Pro account, hitting the 5 hour session limits regularly.",
              "score": 1,
              "created_utc": "2026-02-04 17:46:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o431usb",
                  "author": "poop_harder_please",
                  "text": "thanks, that's super helpful.",
                  "score": 2,
                  "created_utc": "2026-02-07 14:29:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o38zizc",
          "author": "ianxiao",
          "text": "The only i miss when moving from CC is /rewind . Opencode has something less powerful /undo but itâ€™s tedious to use and buggy.",
          "score": 3,
          "created_utc": "2026-02-02 23:41:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3au8fx",
              "author": "aeroumbria",
              "text": "I hate it that a lot of their functions are locked to CLI and not available in the VSCode extention, but their CLI glitches out like crazy in IDE terminals, and I dislike infinite scrolling CLI in IDEs.",
              "score": 1,
              "created_utc": "2026-02-03 06:40:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37nviq",
          "author": "ellensen",
          "text": "I have connected my subscriptions to opencode, seems to give me the same control over cost as if using the subscription by the provider directly without opencode?",
          "score": 2,
          "created_utc": "2026-02-02 19:47:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3aovzm",
          "author": "cafesamp",
          "text": "I know thereâ€™s mixed opinions on how opinionated your workflow should be with these tools, but Iâ€™ve found Superpowers w/ OpenCode (you can use it with Claude Code too, but itâ€™s more redundant there) to be actually really awesome in making coding with OpenCode a more structured experience.\n\nThe main thing OpenCode can really fail on is picking back up on things if you need to run out of usage and/or need to switch models/providers.  OpenCodeâ€™s flow is already so rigid (with task management being handled in the conversation), and Superpowers expects the full lifecycle of its skills to complete a task, and doesnâ€™t wrap up things if you get interrupted.\n\nThat being said, I like using Superpowers in CC with Opus for brainstorming and planning, and then having it write to files that I can pick up and continue to work with in OpenCode with Codex for implementation/testing/review.\n\nPerfect?  No, but great mileage out of two $20/mo subscriptions.",
          "score": 2,
          "created_utc": "2026-02-03 05:55:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oms90",
              "author": "Top-Chain001",
              "text": "interesting, I too like super powers because it really puts you in the driver seat asking you confirmations on the data flow diagrams is this what you're talking about etc I would love to hear your comparison with codex or GPt 5.2 directly versus using something like superpowers and also your opinion on if you tried GSd",
              "score": 1,
              "created_utc": "2026-02-05 08:16:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3bjmsu",
          "author": "Tushar_BitYantriki",
          "text": "The only thing lagging in OpenCode is the lack of hooks in any language, like Claude code supports.\n\nWhat's the point of binding it to JS?\n\nI had posted a migration guide and a skill on this sub to move from Claude Code to OpenCode. But I am yet to find a clean way to migrate my Python+shell hooks from Claude code.\n\nFor me, hooks are a crucial part of my workflow, and I have collected a lot over the months, going from simple grep and regex matches in a shell script to AST-based DDD-enforcing via Python's tree-sitter.\n\nIt seems that OpenCode was made by the JS folks, for the JS folks.",
          "score": 2,
          "created_utc": "2026-02-03 10:39:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o392wv3",
          "author": "Western_Objective209",
          "text": "what can opencode actually do (besides use different models) that claude code can't, like what actually makes it more flexible? Claude code supports plugins, MCPs, and skills, in your extended write up:\n\n> Extensibility: [Claude Code] Managed core with limited extension of agent internals, \t[OpenCode] Open source, extensible with internal tools\n\nLike you don't even seem to understand how to extend claude code and \"open source\" is not an extension model",
          "score": 3,
          "created_utc": "2026-02-03 00:00:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3avs9f",
          "author": "Holiday_Degree_7721",
          "text": "opencode have critical issues with memory leaking, thats why I moved to cc",
          "score": 1,
          "created_utc": "2026-02-03 06:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bf865",
          "author": "raydou",
          "text": "for me the only thing i'm missing is the integration of CC rules files in OpenCode. I like the way it's surgical and don't load the context so much. \nI made a PR for this but it seems that OpenCode team only check the PRs of their buddies. No review nothing on mine since 2 weeks.. \nAt this rythm PR won't be mergeable and I would have to reedit all the changes ..",
          "score": 1,
          "created_utc": "2026-02-03 09:58:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3cfsb0",
          "author": "SpecKitty",
          "text": "I like running both - one implements, the other reviews. I manage with Spec Kitty.",
          "score": 1,
          "created_utc": "2026-02-03 14:19:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dc97r",
          "author": "buggytheking",
          "text": "Loved this",
          "score": 1,
          "created_utc": "2026-02-03 16:56:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hngsj",
          "author": "illusionst",
          "text": "There have been multiple times when Iâ€™ve given the same job to both and Claude failed.\n- Claude fails at maintaining context or sometimes not reading important stuff from claude.md\n- Asked claude to configure a MCP server, it had no idea what MCPâ€™s are, it searched the web, got confused and gave me instructions on how to do it.\n\n- Opencode remembered my instructions and did not hallucinate \n- Opencode searched the specific page and configured the MCP server in less than a minute. \n\nClaude Code comes with a lot of safety guard rails which make it less effective than OpenCode. \n\nIâ€™ve completely moved on to opencode.",
          "score": 1,
          "created_utc": "2026-02-04 06:59:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hse7c",
          "author": "dd768110",
          "text": "If I use a CC, will my account be banned?",
          "score": 1,
          "created_utc": "2026-02-04 07:42:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iwnkj",
          "author": "Comprehensive-Age155",
          "text": "Iâ€™m using OpenCode as backbone of my Saas product. Its architecture and extensibility is unbeatable.",
          "score": 1,
          "created_utc": "2026-02-04 13:17:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qi6iw",
          "author": "FriendAgile5706",
          "text": "Unless im doing something wrong I much prefer planmode on claude code vs opencode",
          "score": 1,
          "created_utc": "2026-02-05 16:00:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37tu8w",
          "author": "vixalien",
          "text": "I feel like OpenCode is much more ambitious than CC, not in a good way. For example, when you ask it why it did something in a certain way, instead of explaining why, it will just undo the change.\n\nOpenCode also seems to use dangerous commands more often, especially with git. It commits everything, and when you ask it to revert, it will happily git reset everything, including any uncommitted changes YOU (not OpenCode) had made.",
          "score": 1,
          "created_utc": "2026-02-02 20:15:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3eqwcg",
              "author": "gsxdsm",
              "text": "Opencode isnâ€™t making decisions. The models are",
              "score": 3,
              "created_utc": "2026-02-03 20:50:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o389fir",
              "author": "Arindam_200",
              "text": "Interesting\n\nI haven't personally faced this problem. But I'll give this a try",
              "score": 1,
              "created_utc": "2026-02-02 21:29:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qvo002",
      "title": "Kilo CLI 1.0 just launched - built on OpenCode as its open-source foundation",
      "subreddit": "opencodeCLI",
      "url": "https://blog.kilo.ai/p/kilo-cli",
      "author": "alokin_09",
      "created_utc": "2026-02-04 13:03:23",
      "score": 63,
      "num_comments": 27,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qvo002/kilo_cli_10_just_launched_built_on_opencode_as/",
      "domain": "blog.kilo.ai",
      "is_self": false,
      "comments": [
        {
          "id": "o3izcov",
          "author": "StephenAfamO",
          "text": "I'm curious, what's the difference between this and using opencode directly?",
          "score": 34,
          "created_utc": "2026-02-04 13:32:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3j271i",
              "author": "abeecrombie",
              "text": "Looks like it integrates back with the other kilo apps on your desktop \n\nAnd maybe they added some agents. I did find previously that kilo worked with many other open models. \n\nBut curious to hear the answer as well.",
              "score": 4,
              "created_utc": "2026-02-04 13:48:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3l9fcy",
                  "author": "touristtam",
                  "text": "> Looks like it integrates back with the other kilo apps on your desktop\n\nThat's a bit heavy, isn't it?",
                  "score": 2,
                  "created_utc": "2026-02-04 20:01:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jyk24",
              "author": "Coldshalamov",
              "text": "Orchestration, compaction. \n\nI really love the idea of kilo code but tbh when I run it with my GLM key it makes weird mistakes, makes the same code in triplicate sometimes, and Iâ€™d never let it touch an important codebase because itâ€™s always broken.\n\nIâ€™ll use it to bootstrap a project and get it off the ground because itâ€™ll run a long time, maybe itâ€™s better with a smarter model, but I really wish it had better handoff between modes. I think maybe better task handling or logging would help, it seems like itâ€™ll lose really important details across modes and GLM isnâ€™t smart enough not to do the same thing over and over without checking.",
              "score": 2,
              "created_utc": "2026-02-04 16:27:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3j0prc",
          "author": "Chrisnba24",
          "text": "Nice rebrand of opencode, it took a lot time to change the naming it seems",
          "score": 23,
          "created_utc": "2026-02-04 13:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3j1k2w",
              "author": "branik_10",
              "text": "lmao",
              "score": 6,
              "created_utc": "2026-02-04 13:44:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jnnvc",
          "author": "landed-gentry-",
          "text": "What value does the \"Kilo platform\" add?",
          "score": 9,
          "created_utc": "2026-02-04 15:36:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3p3081",
              "author": "TestTxt",
              "text": "It adds to their company valuation",
              "score": 3,
              "created_utc": "2026-02-05 10:51:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jb43d",
          "author": "Aggressive-Habit-698",
          "text": "I don't get it. Why rebrand OC instead of a oc plugin or create easily a CLI on your on like octo?\n\nKilo love copy ðŸ˜º ?",
          "score": 7,
          "created_utc": "2026-02-04 14:35:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jo4gv",
              "author": "WatchMySixWillYa",
              "text": "Exactly. It would be a lot more beneficial to contribute to their codebase and add support using some plugin. Fragmentation, on the other hand, can cause them a lot of headaches in the long run.",
              "score": 5,
              "created_utc": "2026-02-04 15:39:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ou6lw",
              "author": "bludgeonerV",
              "text": "Makes for better ads for them to spam across reddit.",
              "score": 2,
              "created_utc": "2026-02-05 09:28:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jb3gy",
          "author": "Charming_Support726",
          "text": "What is it good for? I already wondered when I saw Kilo aggressively do their ads when starting as copy-of-cline.",
          "score": 5,
          "created_utc": "2026-02-04 14:35:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j07rs",
          "author": "ReasonableReindeer24",
          "text": "No thinking for model on kilo ðŸ˜ž, kilo need add this on their cli",
          "score": 1,
          "created_utc": "2026-02-04 13:37:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ja726",
          "author": "jackai7",
          "text": "Does it have all the agents like orchestor/code/debug/ask as they were in the extension??",
          "score": 1,
          "created_utc": "2026-02-04 14:30:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yxjhd",
              "author": "dsvost",
              "text": "Not seeing them. Just opencode's Plan and Build ones.",
              "score": 1,
              "created_utc": "2026-02-06 21:05:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jtu9y",
          "author": "trypnosis",
          "text": "Never heard of Kilo so I went to check them out. \n\nSeems like some kind of model provider. With plug in",
          "score": 1,
          "created_utc": "2026-02-04 16:05:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kyxny",
          "author": "jkz88",
          "text": "Have they fixed opencode freezing up after a few prompts? It looks so good but I could never get it to work once it started to make a few edits. Or maybe it's not happening for everyone?",
          "score": 1,
          "created_utc": "2026-02-04 19:12:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n0hg6",
              "author": "toadi",
              "text": "I update each time to the newest version. I also use it daily. Never happened to me.",
              "score": 1,
              "created_utc": "2026-02-05 01:27:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o45w2fh",
                  "author": "Ordinary-You8102",
                  "text": "its updated automatically",
                  "score": 1,
                  "created_utc": "2026-02-07 23:20:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o45w32w",
              "author": "Ordinary-You8102",
              "text": "never happened to me",
              "score": 1,
              "created_utc": "2026-02-07 23:20:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3mddxg",
          "author": "atkr",
          "text": "weak and feeble, like",
          "score": 1,
          "created_utc": "2026-02-04 23:18:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mdnb5",
          "author": "atkr",
          "text": "marketing is cringy AF, just going over how good opencode is without giving them enough credit",
          "score": 1,
          "created_utc": "2026-02-04 23:20:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mswqb",
          "author": "Demien19",
          "text": "Clone-slop, now every company will release own clone and confuse people even more",
          "score": 1,
          "created_utc": "2026-02-05 00:44:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40yfhs",
          "author": "itsabhiyan",
          "text": "I'm not into kilo ecosystem or whatever. So I find no benefit to kilocode cli. just tried it out. and nothing different.\n\nI see no reason to use it tbh.",
          "score": 1,
          "created_utc": "2026-02-07 04:08:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45wdxo",
          "author": "Ordinary-You8102",
          "text": "the fact that I cant see a github repo link here/on the blog says a lot already",
          "score": 1,
          "created_utc": "2026-02-07 23:22:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nguua",
          "author": "bobthearsonist",
          "text": "sweet! cline+opencode=kilo. did you keep the mobile server?",
          "score": 0,
          "created_utc": "2026-02-05 03:00:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qw108w",
      "title": "Thank you dax and opencode team",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qw108w/thank_you_dax_and_opencode_team/",
      "author": "jmhunter",
      "created_utc": "2026-02-04 21:07:30",
      "score": 57,
      "num_comments": 3,
      "upvote_ratio": 0.96,
      "text": "I just wanted to give a big thumbs up and thank you to dax and the team.. what a great product.. I do use claude primarily, but this has been my goto harness since since like march 2025.. i really appreciate your guys work and improvements.. The project rocks... no buts, just thanks!\n\nI will continue to support by buying at least $20 in credits a month from zen.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qw108w/thank_you_dax_and_opencode_team/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o3onsed",
          "author": "Complex_Initial_8309",
          "text": "Last time I checked, they mentioned, on their Zen page and dax's video about Zen, that they are not profiting from it.",
          "score": 5,
          "created_utc": "2026-02-05 08:26:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qq97y",
          "author": "Spirited-Milk-6661",
          "text": "Same, it's been my daily driver for months now. The cost transparency alone makes it a game-changer.",
          "score": 2,
          "created_utc": "2026-02-05 16:38:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lprai",
          "author": "Free-Stretch1980",
          "text": "Is 20 usd a month enough ? What models do you use ?",
          "score": 3,
          "created_utc": "2026-02-04 21:20:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwszbc",
      "title": "Gpt 5.3 codex dropped",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/r15af3dwwphg1.jpeg",
      "author": "ReasonableReindeer24",
      "created_utc": "2026-02-05 18:18:24",
      "score": 49,
      "num_comments": 28,
      "upvote_ratio": 0.79,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qwszbc/gpt_53_codex_dropped/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3rf2xj",
          "author": "Xeon06",
          "text": "Nothing more idiotic than sharing a screenshot of the top of a blog post instead of the URL\n\nhttps://openai.com/index/introducing-gpt-5-3-codex/",
          "score": 67,
          "created_utc": "2026-02-05 18:33:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yaleb",
              "author": "rmaxdev",
              "text": "I do this at work, nobody opens links",
              "score": 0,
              "created_utc": "2026-02-06 19:11:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ywlzg",
                  "author": "duboispourlhiver",
                  "text": "are you working in the shorts department of YouTube ?",
                  "score": 3,
                  "created_utc": "2026-02-06 21:01:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o47n6r8",
                  "author": "gnaarw",
                  "text": "A Max at a client of mine does this too. Time to shame him again on Monday. Headlines are not enough information",
                  "score": 1,
                  "created_utc": "2026-02-08 06:26:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3roc4r",
          "author": "Timo_schroe",
          "text": "Its available for me in Opencode after reauth",
          "score": 9,
          "created_utc": "2026-02-05 19:15:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3s8p51",
              "author": "Aggravating_Win2960",
              "text": "Hi, can you share the exact the command? I tried /connect inside opencode and als 'opencode auth login' in terminal/ghostty but I only get the GPT-5.2 Codex model.  \nps: have latest 1.1.51 version of opencode",
              "score": 2,
              "created_utc": "2026-02-05 20:51:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3saeb4",
                  "author": "Timo_schroe",
                  "text": "Opencode auth logout after that Login, Opus 4.6 available too now ! (I use pro at openai and max at anthrophic). Ah and I use oh-my-opencode. Maybe they bring an other with plugin (?)",
                  "score": 2,
                  "created_utc": "2026-02-05 20:59:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ru5eh",
          "author": "drinksbeerdaily",
          "text": "Really impressed with it so far. I might finally drop Claude 5x and get by with two ChatGPT plus subs.",
          "score": 5,
          "created_utc": "2026-02-05 19:42:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3sqi7d",
              "author": "Impossible_Secret80",
              "text": "Opus 4.6 with a 1-million token context window is out also today :)",
              "score": 1,
              "created_utc": "2026-02-05 22:18:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3sssxf",
                  "author": "Chris266",
                  "text": "1 mil only in API...",
                  "score": 2,
                  "created_utc": "2026-02-05 22:29:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ys49v",
              "author": "Ordinary-You8102",
              "text": "Why two subs?",
              "score": 1,
              "created_utc": "2026-02-06 20:38:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3z3hih",
                  "author": "drinksbeerdaily",
                  "text": "Usage limits on one plus plan isn't enough.",
                  "score": 1,
                  "created_utc": "2026-02-06 21:35:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3s2840",
          "author": "web_assassin",
          "text": "I'm trying to save it as my opencode default model but it's not having any effect. I can set it with /models though.   \"model\": \"openai/gpt-5.3-codex\", is what i'm using in the opencode.json",
          "score": 3,
          "created_utc": "2026-02-05 20:20:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3usldg",
          "author": "Fit-Mulberry-8611",
          "text": "Is it also 25% faster in opencode",
          "score": 2,
          "created_utc": "2026-02-06 05:46:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rm1q6",
          "author": "MegamillionsJackpot",
          "text": "I can see it in Codex CLI, but not in Opencode. Anyone see it ?",
          "score": 1,
          "created_utc": "2026-02-05 19:04:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3s0zke",
              "author": "MegamillionsJackpot",
              "text": "Sorted. Used codex to set it up as agent in opencode",
              "score": 2,
              "created_utc": "2026-02-05 20:14:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3vnaqk",
                  "author": "oulu2006",
                  "text": "I might have to do that as well",
                  "score": 1,
                  "created_utc": "2026-02-06 10:24:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3tgmyt",
              "author": "MattU2000",
              "text": "do opencode auth logout and then opencode auth login.",
              "score": 1,
              "created_utc": "2026-02-06 00:40:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3vn9tt",
                  "author": "oulu2006",
                  "text": "that's interesting, from iterm still didn't work for me and shot gpt5.2 only \n\n    opencode auth logout + \n    opencode auth login",
                  "score": 3,
                  "created_utc": "2026-02-06 10:24:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wxwm5",
          "author": "JuiceBoxJonny",
          "text": "https://preview.redd.it/gsqmpae36whg1.jpeg?width=1170&format=pjpg&auto=webp&s=2a728f5ec5192e98d238e3bdefd90d2e14127821\n\nYall seriously use ts ðŸ¤”",
          "score": 0,
          "created_utc": "2026-02-06 15:20:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsyzso",
      "title": "GPT 5.2 for difficult things and Kimi K2.5 for everything else seems to be the move, what the cheapest way to get there?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qsyzso/gpt_52_for_difficult_things_and_kimi_k25_for/",
      "author": "SweatyHands247",
      "created_utc": "2026-02-01 13:51:45",
      "score": 44,
      "num_comments": 32,
      "upvote_ratio": 0.98,
      "text": "Once the free period of Kimi K2.5 is finished, what's the cheapest, fast and private way to access it?\n\n  \nWe'll also want GPT access to tactically use it when necessary. What's the most cost effective way for this.\n\n  \nAnyone got OpenCode Black 20 access? Will that do the job? I imagine it'll get you pretty far for K2.5, but what about with some GPT sprinkled in. \n\nOr maybe Black 20 and a Chutes sub? \n\nAny other ideas?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qsyzso/gpt_52_for_difficult_things_and_kimi_k25_for/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2yy0fp",
          "author": "Simple_Split5074",
          "text": "Probably ChatGPT Plus (or maybe GH Copilot) and synthetic - nanogpt I found sadly cannot do Kimi K2.5 properly (otherwise probably the best deal out there if you want multiple models) and chutes I never had good experience with. To be fair, I have not yet signed up for synthetic.\n\nFWIW, the thinking goes that glm5 and DS4 might appear in the next two weeks (before CNY), picture might look different again then.",
          "score": 12,
          "created_utc": "2026-02-01 13:57:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zivvx",
              "author": "AlergDeNebun",
              "text": "Same, on nano-gpt, my experience so far is that it's quite slow and K2.5 tool calling is just broken 80% of the time, so unfortunately unusable for real work.\n\nThis being said, if they fix these issues and maybe improve the speed a bit (I'm OK with it being slower, but not THAT slow) it's a GREAT deal. But as it stands, if these issues are not resolved by the time my first month is up, I'm unsubscribing.",
              "score": 3,
              "created_utc": "2026-02-01 15:47:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zkoxz",
                  "author": "Simple_Split5074",
                  "text": "It also just flat out fails to respond at all quite often, according rto the discord they are trying to fix it",
                  "score": 2,
                  "created_utc": "2026-02-01 15:55:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2yxqw7",
          "author": "Recent-Success-1520",
          "text": "Depending upon your usage - Codex $20 + Nano-GPt  $8",
          "score": 5,
          "created_utc": "2026-02-01 13:56:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zjp2q",
              "author": "AlergDeNebun",
              "text": "It WOULD be a great deal, but here is a screenshot of me using Nano-GPT and trying to summarize some random files in a directory (it's my go to test for several reasons, just throw a bunch of text files together, some related some not, and ask the models to do things).\n\nYou can see how much it took, and at the end the tool calling failed.\n\nI get similar failures for Kimi K2.5 .\n\nSo my experience so far: very cheap, HUGE limits, but also slow and unreliable to the point of being unusable in the real world, for anything other than asking direct questions.\n\nhttps://preview.redd.it/g0yqofemmwgg1.png?width=1859&format=png&auto=webp&s=d10f14f2f650ca9212c628ee527a80cee62debe6",
              "score": 3,
              "created_utc": "2026-02-01 15:50:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34k7fx",
                  "author": "TastyIndividual6772",
                  "text": "I had this tool call failures on multiple plugins/clis on multiple Chinese models. I havenâ€™t had those inside kimi cli tho.",
                  "score": 1,
                  "created_utc": "2026-02-02 09:01:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2z1gvy",
              "author": "Ang_Drew",
              "text": "nano gpt seems to be good offer. what is the 60k message a month mean?\n\ndid tool call considered 1 messages?",
              "score": 1,
              "created_utc": "2026-02-01 14:17:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zkt8d",
                  "author": "Simple_Split5074",
                  "text": "My usage logs suggest they do count as a message",
                  "score": 2,
                  "created_utc": "2026-02-01 15:56:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30jpzz",
          "author": "drinksbeerdaily",
          "text": "Is kimi 2.5 really good enough to replace opus 4.5 for planning or debugging?",
          "score": 2,
          "created_utc": "2026-02-01 18:35:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32iu1t",
              "author": "Grand-Management657",
              "text": "No, I would not say so. In my experience, Opus 4.5 for planning, Kimi K2.5 for execution, and then review with GPT 5.2. Gets 3 LLMs to look at the same problem while being economical since most of the output happens on K2.5. \n\nK2.5 is not on the same level as Opus or GPT 5.2 but close to Sonnet 4.5 performance. I wrote more about it in my post here: https://www.reddit.com/r/opencodeCLI/s/MJoHjOdZGq",
              "score": 2,
              "created_utc": "2026-02-02 00:30:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36fq8i",
                  "author": "trypnosis",
                  "text": "Man after my own heart I never let model x review the work of model x. Model y should be used to review model x.\n\nI have not had much luck with kimi over the weekend  on synthetic but I hear they are now hosting it them selves so it is meant to be faster so will be trying that from tomorrow.",
                  "score": 1,
                  "created_utc": "2026-02-02 16:25:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3oyx2i",
              "author": "Key_Mousse_8034",
              "text": "As for the k2.5 model, it honestly feels like a downgraded Opus 4.5. It hallucinates, fails to complete tasks (while reporting success), and is generally lazy. I wouldn't rely on Kimi k2.5 for anything critical",
              "score": 1,
              "created_utc": "2026-02-05 10:14:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o32i9n5",
          "author": "Grand-Management657",
          "text": "Gpt 5.2 or Opus 4.5 as orchestrator, Kimi K2.5 for execution. Run K2.5 through synthetic and $20 codex sub for orchestration (run it in xhigh). It's pretty much the most economical combination while retaining performance. I wrote about the economics in my post here: https://www.reddit.com/r/opencodeCLI/s/MJoHjOdZGq",
          "score": 2,
          "created_utc": "2026-02-02 00:27:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31tmk5",
          "author": "oknowton",
          "text": "I think you're on the right track.  Chutes and Synthetic are two of the subscriptions I have active right now.  I'm having success with Kimi K2.5 on both, but I am not a professional programmer writing code 8 hours a day, and my coding work is pretty simple compared to what most people in here are doing.\n\nSynthetic is about 3x faster right Chutes, at least for Kimi K2, but Chutes has bigger quotas at a fraction of the price.\n\n> Or maybe Black 20 and a Chutes sub? \n\nI think OpenCode Black is too new to pin down.  They haven't published any information about how their quotas will work, and they probably won't have any of that nailed down until after the service goes live for everyone.\n\nOpenAI seems friendly towards the OpenCode community, and I'm always hearing people say how plentiful their quotas are compared to Anthropic.  Pairing a $20 Codex subscription with a $3 or $10 Chutes subscription seems like a good place to start.",
          "score": 3,
          "created_utc": "2026-02-01 22:14:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33th7b",
          "author": "TreeBearr",
          "text": "I've had the best luck with synthetic for the good open weight models. The inference can be a lil slow sometimes but the cost and privacy is worth. I use it with opencode and OpenWebUI.\n\nI think they're pretty generous usage limits for the $20 plan. 135 requests per 5 hours and the requests are fractional if they are tool calls or small agent runs. I'll insert a ss of the billing page.  \n\n\nIf u get referred you get some credit :3\n\n[https://synthetic.new/?referral=7JlVOLCkmEQv5oI](https://synthetic.new/?referral=7JlVOLCkmEQv5oI)\n\nhttps://preview.redd.it/v9co646ak0hg1.png?width=934&format=png&auto=webp&s=425697296c73a8656b541efbff6e16e6de94e4a2",
          "score": 2,
          "created_utc": "2026-02-02 05:08:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o340flg",
          "author": "bduyng",
          "text": "That combo makes sense â€” GPT-5.2 for heavy lifting and Kimi K2.5 for cheap grunt work seems solid ðŸ‘",
          "score": 1,
          "created_utc": "2026-02-02 06:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34p4o2",
          "author": "LordEli",
          "text": "i've had a lot of success with just kimi k2.5 alone but wondering how rate limits work",
          "score": 1,
          "created_utc": "2026-02-02 09:49:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35h7oy",
          "author": "pbalIII",
          "text": "One angle worth considering: task-based routing instead of just model tiers.\n\nK2.5 at $0.60/1M input and $3/1M output is already cheap. GPT 5.2 is ~10x more expensive per token. The math flips depending on what you're actually doing.\n\nFor agentic work with lots of tool calls and iterations, K2.5 handles the volume. For single-shot complex reasoning where you need one good answer, GPT 5.2 pays for itself in fewer retries.\n\nSo rather than Black 20 + another sub, you could route at the task level: classify incoming requests by complexity and let a cheap router model decide. OpenRouter and similar services support this natively. Cuts costs 30-40% without degrading quality on the tasks that matter.",
          "score": 1,
          "created_utc": "2026-02-02 13:30:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iewp2",
          "author": "Keep-Darwin-Going",
          "text": "How are you all setting this up? Opencode do not have a way to auto switch between model. Are you all planning first then switch to code the switch the model?",
          "score": 1,
          "created_utc": "2026-02-04 11:11:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2z77hs",
          "author": "Bob5k",
          "text": "[synthetic](https://synthetic.new/?referral=IDyp75aoQpW9YFt) as a provider for kimi / glm / minimax is a nobrainer, especially when you also consider reliable infrastructure and zero data retency as important feature.",
          "score": -1,
          "created_utc": "2026-02-01 14:49:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2z83ek",
              "author": "Funny-Advertising238",
              "text": "Literally anything is better than synthetic don't listen to this guyÂ ",
              "score": 3,
              "created_utc": "2026-02-01 14:53:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2z9nv9",
                  "author": "lundrog",
                  "text": "Not sure why you hate them but performance is good and you get good value for a monthly. If you can find me a subscription for the same or less with the usage I need that performs as well im all ears ðŸ˜‚ (not nano gpt ) they were called out on Reddit via a provider for credit fraud",
                  "score": 4,
                  "created_utc": "2026-02-01 15:01:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2zknxz",
                  "author": "red_rolling_rumble",
                  "text": "It would be more convincing if you would explain how itâ€™s bad!",
                  "score": 2,
                  "created_utc": "2026-02-01 15:55:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o35okb7",
                  "author": "blankeos",
                  "text": "Like what? :o",
                  "score": 1,
                  "created_utc": "2026-02-02 14:11:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zae4t",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-02-01 15:05:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zl7ro",
              "author": "Simple_Split5074",
              "text": "As opposed to spamming referral links...",
              "score": 1,
              "created_utc": "2026-02-01 15:57:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qyhiyt",
      "title": "Bringing Claude Codeâ€™s Agent Teams to Open Code via MCP",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qyhiyt/bringing_claude_codes_agent_teams_to_open_code/",
      "author": "vicdotso",
      "created_utc": "2026-02-07 16:02:17",
      "score": 44,
      "num_comments": 19,
      "upvote_ratio": 0.95,
      "text": "https://reddit.com/link/1qyhiyt/video/2a0tm3voc3ig1/player\n\nAfter Anthropic shipped Agent Teams in Claude Code, I got curious about how the coordination layer worked under the hood. After some back and forth with claude and a little reverse engineering, the coordination layer turns out to be a clever mix of tmux + file locks and undocumented cli arguments.\n\nSo I pulled it apart and reimplemented it as a standalone MCP server. Any MCP client can use it now, including\n\nopencode as seen in the demo video.\n\nHere's what the server exposes:\n\n\\- Team + spawning: create teams, spawn Claude Code teammates into tmux panes, graceful and forced shutdown.\n\n\\- Task coordination: ownership, status tracking, dependency graphs with cycle detection.\n\n\\- Messaging: DMs, broadcast, long-polling inbox, shutdown/plan-approval protocol.\n\n\\- Concurrency safety: file locks on inboxes and tasks, atomic config writes.\n\nRepo: [github.com/cs50victor/claude-code-teams-mcp](http://github.com/cs50victor/claude-code-teams-mcp)\n\nIt's early (v0.1.0) and I'd love as much feedback as possible specifically around tighter opencode integrations.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qyhiyt/bringing_claude_codes_agent_teams_to_open_code/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o442pfq",
          "author": "james__jam",
          "text": "And here i was wondering how long i need to wait before somebody ports it over to opencode ðŸ˜…\n\nThank you kind sir! ðŸ˜",
          "score": 10,
          "created_utc": "2026-02-07 17:35:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o444be8",
              "author": "vicdotso",
              "text": "ðŸ«¡  \nPRs , Github Issues and PRs are all welcome.",
              "score": 2,
              "created_utc": "2026-02-07 17:43:23",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o461r5z",
              "author": "DeExecute",
              "text": "Itâ€™s not ported to opencode it still spawn claude codeâ€¦",
              "score": 2,
              "created_utc": "2026-02-07 23:57:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o44hlg4",
          "author": "rothnic",
          "text": "This looks cool, I started a bit on this when they first announced it but didn't have time to fully explore their implementation, but i was working on it to avoid the dependency on claude code entirely. Is that not the idea with this?\n\nIdeally, you'd spawn an opencode server associated with a project/directory if there already isn't one available when starting up opencode since that avoids any mcp server duplication and your plugin can interact with any sessions, events, etc. You want a plugin that hooks into all the relevant events for making sure all the expected interactions work as expected and you can recover from any odd states.\n\nIf trying to have this work outside of the opencode server process, it's possible but i think it'll be more difficult. There are some other projects doing something similar and there is a ton of adapter work. Gastown, for example is one of those. ",
          "score": 3,
          "created_utc": "2026-02-07 18:48:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44ivfi",
              "author": "vicdotso",
              "text": "currently thinkering on an integration to support claude code and the opencode server natively. it's all just json files and tmux so it might be possilbe. will post an update here once i do.",
              "score": 2,
              "created_utc": "2026-02-07 18:54:57",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o46zwu6",
              "author": "philosophical_lens",
              "text": "Yeah, opencode server + sdk would be the ideal implementation I think.",
              "score": 1,
              "created_utc": "2026-02-08 03:30:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4668ug",
          "author": "RelationshipAny1889",
          "text": "Any chance we can have this implemented to use only Open Code? Without having to use the Cloud Code at all.",
          "score": 3,
          "created_utc": "2026-02-08 00:24:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47792z",
              "author": "vicdotso",
              "text": "not sure how challenging this would be but i'm working on this.",
              "score": 2,
              "created_utc": "2026-02-08 04:20:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o44pc2w",
          "author": "idkwtftbhmeh",
          "text": "Amazing, thanks a lot",
          "score": 2,
          "created_utc": "2026-02-07 19:27:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45gx1k",
          "author": "nadimtuhin",
          "text": "Looks cool, how is the token usage ?",
          "score": 2,
          "created_utc": "2026-02-07 21:55:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o477fxv",
              "author": "vicdotso",
              "text": "more agents / teammates  , more tokens",
              "score": 1,
              "created_utc": "2026-02-08 04:21:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o461hmi",
          "author": "DeExecute",
          "text": "I am wondering if there isnâ€™t better way without MCP as everyone is moving away from MCPs for good reasons.",
          "score": 2,
          "created_utc": "2026-02-07 23:56:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4775p8",
              "author": "vicdotso",
              "text": "you could technically use a cli - [https://github.com/cs50victor/mcpx](https://github.com/cs50victor/mcpx) or [https://github.com/philschmid/mcp-cli](https://github.com/philschmid/mcp-cli) ",
              "score": 1,
              "created_utc": "2026-02-08 04:19:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o473az9",
          "author": "MakesNotSense",
          "text": "The closest thing to Agent Teams in OpenCode right now, is my PR [https://github.com/anomalyco/opencode/pull/7756](https://github.com/anomalyco/opencode/pull/7756)\n\nDuring it's development I identified that the Parent-Child caller system needs a redesign in order to become multi-caller. A proper agent team can't occur without multi-caller support.\n\nThis is because currently, when an agent that isn't the Parent persists a session with a subagent, the subagent thinks the calling agent is the Parent.\n\nWhat that looks like:\n\nPrimary A tasks subagent A. Subagent A tasks Subagent B. Primary A tries to communicate with Subagent B, and Subagent B thinks it's talking to Subagent A.\n\nTo coordinate a team of agents, which are communicating, and orchestrating each others actions, you need them to be able to identify who they're talking to/with.\n\nI have a workaround system where agents write relay-files, where a sub-skill helps them know the relay-file protocol. It's part of a larger Agentic Collaboration Framework.\n\nI think a proper OpenCode implementation for Agent Teams needs first, to merge my PR, second to revamp the caller system so that it supports multi-callers.\n\nIt is getting tiresome that Anthropic releases something, and people think it's innovative, say they should copy it, but something better is already available for OpenCode, but the community just isn't paying attention or pitching in.\n\nI have a complete roadmap for agentic collaboration in development for OpenCode. The PR is just one small part of it. A critical part, but a small part. There's so much more that needs doing, and I'm just one newbie vibe coder who develops purely to build the tools that I need for litigation.\n\nEdit: decided to turn this comment into a GitHub Issue. [https://github.com/anomalyco/opencode/issues/12661](https://github.com/anomalyco/opencode/issues/12661) (\\[FEATURE\\]: Add Agent Teams EquivalentÂ #12661)",
          "score": 3,
          "created_utc": "2026-02-08 03:53:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43v801",
          "author": "buggytheking",
          "text": "Sick.... I'll take a look. If it works well we need to put this in the main code. Are you working on something for that?",
          "score": 1,
          "created_utc": "2026-02-07 16:58:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43xcft",
              "author": "vicdotso",
              "text": "currently trying some approaches to see how feasible it is to integrate directly with the opencode server",
              "score": 1,
              "created_utc": "2026-02-07 17:08:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o450asf",
          "author": "idkwtftbhmeh",
          "text": "Is there a way to run this on windows?",
          "score": 1,
          "created_utc": "2026-02-07 20:25:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o479h2k",
              "author": "vicdotso",
              "text": "should just work with any mcp client, open an issue if it doesn't",
              "score": 2,
              "created_utc": "2026-02-08 04:36:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o45aa9n",
          "author": "MarcoHoudini",
          "text": "It is similar to how oh my opencode does it. Maybe you could check there for inspiration. I wander maybe cc team got inspired by it and we are not backport but source)",
          "score": 1,
          "created_utc": "2026-02-07 21:20:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qt06bj",
      "title": "OpenCode Bar 2.0: It auto-detects all your AI providers from OpenCode. Zero setup.",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/qe8z8tsfawgg1.png",
      "author": "kargnas2",
      "created_utc": "2026-02-01 14:41:05",
      "score": 37,
      "num_comments": 13,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qt06bj/opencode_bar_20_it_autodetects_all_your_ai/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2zfpw3",
          "author": "Putrid-Pair-6194",
          "text": "Cool. But Mac only? Any chance of a windows or a command line version?",
          "score": 8,
          "created_utc": "2026-02-01 15:31:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34dyi7",
              "author": "digitalfreshair",
              "text": "yes, linux would be awesome. Or a cli would be even better",
              "score": 3,
              "created_utc": "2026-02-02 08:01:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34qnki",
                  "author": "kargnas2",
                  "text": "I released CLI version but maybe it doesn't work at Linux yet: [https://www.reddit.com/r/opencodeCLI/comments/1qtqx2p/opencode\\_bar\\_21\\_now\\_with\\_cli\\_perprovider/](https://www.reddit.com/r/opencodeCLI/comments/1qtqx2p/opencode_bar_21_now_with_cli_perprovider/)",
                  "score": 1,
                  "created_utc": "2026-02-02 10:04:24",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zjqox",
          "author": "mylittlecumprincess",
          "text": "It says \"err\" in the menubar using the latest Opencode on Mac \n\nhttps://preview.redd.it/palduvkzmwgg1.jpeg?width=342&format=pjpg&auto=webp&s=c2c6c6cb560740eac711d5e17a2d7b4465a23ade\n\n",
          "score": 2,
          "created_utc": "2026-02-01 15:51:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zr11h",
              "author": "kargnas2",
              "text": "adding a log viewer!",
              "score": 3,
              "created_utc": "2026-02-01 16:25:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zrjsn",
          "author": "aimericg",
          "text": "I am honestly not having much trouble with this. I don't find the setup to be that hard.",
          "score": 2,
          "created_utc": "2026-02-01 16:27:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zx6hm",
              "author": "thatsnot_kawaii_bro",
              "text": "...Then don't need to use it?",
              "score": 1,
              "created_utc": "2026-02-01 16:53:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o318c3a",
                  "author": "KHALIMER0",
                  "text": "![gif](giphy|dW0KIk9KCsWBy|downsized)",
                  "score": 2,
                  "created_utc": "2026-02-01 20:30:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zz26s",
          "author": "Ok_Proposal_1290",
          "text": "I LOVE THIS, but if it isn't too much work, is there some way it could support windows or alternatively linux?",
          "score": 2,
          "created_utc": "2026-02-01 17:01:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o30js2p",
          "author": "touristtam",
          "text": "I have been using https://github.com/nguyenphutrong/quotio with mitigated success. \n\n---\n\nI'd suggest that you create a homebrew recipe; I know I cannot install that outside homebrew/appstore so the dmg is a non starter unfortunately (work is a b***)",
          "score": 2,
          "created_utc": "2026-02-01 18:35:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34nngw",
              "author": "kargnas2",
              "text": "nice idea",
              "score": 1,
              "created_utc": "2026-02-02 09:35:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zb396",
          "author": "lundrog",
          "text": "Hmm interesting, how do we make it support claude code as well? I use that often with other providers",
          "score": 1,
          "created_utc": "2026-02-01 15:09:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3otlvp",
          "author": "Character_Cod8971",
          "text": "No Windows? Need this on Windows, so I can use it",
          "score": 1,
          "created_utc": "2026-02-05 09:23:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qt428t",
      "title": "Using an AI Agent (opencode) To Teach Me Rust and Itâ€™s Kinda Blowing My Mind",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qt428t/using_an_ai_agent_opencode_to_teach_me_rust_and/",
      "author": "feursteiner",
      "created_utc": "2026-02-01 17:07:00",
      "score": 36,
      "num_comments": 46,
      "upvote_ratio": 0.93,
      "text": "Iâ€™ve been learning Rust with an AI agent through OpenCode, and itâ€™s honestly way cooler than I expected.\n\nComing from a TypeScript-heavy background, I thought Rust would break my brain, but the AI keeps mapping concepts to stuff I already know. Itâ€™s structured, but flexible enough that I can reshape the whole plan whenever I get stuck or suddenly decide to deep-dive ownership at 2am.\n\nIt uses a pyramid-style method where each layer builds on the last, and I can expand it as I go. The repo basically becomes a living skill tree. Also, I get to ask all the â€œdumbâ€ questions Iâ€™d never ask a human. No judgment. Just explanations until it finally clicks.\n\nLearning at my own pace, on my own time, has been way more comfortable, and honestly the speed is kind of wild. Rust went from intimidating to fun way faster than I expected.\n\nEdit:   \ntook down the link before, but happy to share it again, thanks for the support y'all!  \n[https://github.com/feuersteiner/learning-rust](https://github.com/feuersteiner/learning-rust)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qt428t/using_an_ai_agent_opencode_to_teach_me_rust_and/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o3012hq",
          "author": "Maasu",
          "text": "Covered async yet?",
          "score": 9,
          "created_utc": "2026-02-01 17:11:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30500e",
              "author": "coffee_brew69",
              "text": "the agent might delete itself on that part",
              "score": 12,
              "created_utc": "2026-02-01 17:29:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o307ol9",
                  "author": "feursteiner",
                  "text": "haha I see that x) I had to cycle through a couple of models to find one that is actually a good value for money. I am using github copilot as the provider (wide variety, and feels cheaper). Opus was best, but is most expensive, currently with codex-5.2",
                  "score": 2,
                  "created_utc": "2026-02-01 17:41:19",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30ee1c",
          "author": "debba_",
          "text": "I totally agree. Iâ€™m also using a *learn-by-doing* approach with Rust, with the help of OpenCode. On top of that, KIMI K2.5 Free with Zen is a really nice bonus.\n\nIn just one week, I managed to ship a first beta of a side project of mine: a lightweight database tool with a clean, pleasant UX.\n\nIf you want to take a look:  \n[https://github.com/debba/tabularis](https://github.com/debba/tabularis)",
          "score": 8,
          "created_utc": "2026-02-01 18:11:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30it3b",
              "author": "feursteiner",
              "text": "Tauri is the goat!",
              "score": 1,
              "created_utc": "2026-02-01 18:31:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o30veop",
          "author": "Ok_Layer2715",
          "text": "Hey, i would appreciate if you give me more details, as what you have written to opencode from first and what is the pyramid method",
          "score": 2,
          "created_utc": "2026-02-01 19:28:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31lmv6",
              "author": "feursteiner",
              "text": "absolutely! I can first refer you to the agents md (feel free to star the repo, please and thank you haha) and you can see everything. feel free to ask me any questions about it too!  \n[https://github.com/feuersteiner/learning-rust](https://github.com/feuersteiner/learning-rust)",
              "score": 1,
              "created_utc": "2026-02-01 21:35:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31okdl",
                  "author": "Ok_Layer2715",
                  "text": "Nice, i have checked both of them and they are awesome specially your repo hahah\nBut the thing that i cant understand till now is the pyramid method",
                  "score": 2,
                  "created_utc": "2026-02-01 21:49:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33dppo",
          "author": "mrpoopybruh",
          "text": "Its wild. I am currently binding them into cards on a big canvas. I will be in the matrix within days, if not hours. Not kidding -- I'm literally coding up a green on black style layer lol",
          "score": 2,
          "created_utc": "2026-02-02 03:26:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o364qtl",
              "author": "feursteiner",
              "text": "dude the matrix theme is my fav on opencode haha",
              "score": 1,
              "created_utc": "2026-02-02 15:34:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o36pp5y",
                  "author": "mrpoopybruh",
                  "text": "LOL me too. Man, I gotta say though, the CLI is slick, but the actual Rest API is very complex and its taking forever to get even something crappy up. Gotta give it to the opencode team on how slick the CLI is.",
                  "score": 2,
                  "created_utc": "2026-02-02 17:11:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3499i5",
          "author": "levu304",
          "text": "i think you should approach through napi-rs first",
          "score": 2,
          "created_utc": "2026-02-02 07:18:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o364urq",
              "author": "feursteiner",
              "text": "can you explain more please ? I am intrigued",
              "score": 1,
              "created_utc": "2026-02-02 15:34:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o34xf4w",
          "author": "chiroro_jr",
          "text": "I used to do this too when learning something new. I tell the AI to generate tests. Then I write code that tries to pass those tests. If it's too hard, I ask for a hint. When the tests pass I asked the AI to look at the code so that it tell me if I could have implemented a better solution or written more idiomatic code depending on the ecosystem. It's pretty cool.",
          "score": 2,
          "created_utc": "2026-02-02 11:06:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3651p9",
              "author": "feursteiner",
              "text": "exactly the same process! someone should make product around this... this is what school should look like in the future ...",
              "score": 2,
              "created_utc": "2026-02-02 15:35:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o308wxl",
          "author": "web_assassin",
          "text": "I'm advancing my Git skills with opencode and loving it. It doesn't give me snarky replies to my dumb questions.",
          "score": 1,
          "created_utc": "2026-02-01 17:46:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o309ufg",
              "author": "feursteiner",
              "text": "yeah, exactly. it's sad to see places like reddit turn like that, where people don't appreciate other's learning journeys and just pile on them... sad. Good luck to you too u/web_assassin !",
              "score": 2,
              "created_utc": "2026-02-01 17:51:06",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o30f6ey",
              "author": "vertigo235",
              "text": "Stackoverflow prevented so many eager people from learning, you only really learned from it if a previous person took some serious heat for asking a simple question. \n\nGone are those days!",
              "score": 2,
              "created_utc": "2026-02-01 18:14:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o30hn16",
                  "author": "feursteiner",
                  "text": "I re-posted this same exact post on another subreddit and gotten so much hate in 2 minutes I deleted the post...",
                  "score": 1,
                  "created_utc": "2026-02-01 18:25:56",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o30ik60",
                  "author": "web_assassin",
                  "text": "Hah yeah sacrificial lambs. The online haters are losing their jobs. So sad!",
                  "score": 1,
                  "created_utc": "2026-02-01 18:30:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30ejob",
          "author": "vertigo235",
          "text": "This is the way",
          "score": 1,
          "created_utc": "2026-02-01 18:12:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31lrbg",
              "author": "feursteiner",
              "text": "damn straight!! exciting times for learning!",
              "score": 1,
              "created_utc": "2026-02-01 21:36:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o30frws",
          "author": "antifeixistes",
          "text": "Could you share a bit more about the process, how did you set it up to learn rust from it? Thx",
          "score": 1,
          "created_utc": "2026-02-01 18:17:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30imc2",
              "author": "feursteiner",
              "text": "so it was a process, first I tried to setup just a readme with a curriculum (opus generated that I think, or gpt5.2), but then I went and setup the agents\\[.\\]md. I knew I wanted to have different level of answers depending on how much detail I want, so I setup the \"pyramid method\" which is how news articles are written.  \nthen I started slowly to scaffold what a lesson is and what an exercise is, then added an \"ex-00\" which just gives me basic syntax to learn, and other exercises to teach the concepts.  \nI found myself learning by analogy (bun vs cargo, memory management in C...) so I told the agents file about my background so that it explain conxepts in a relevant manner.  \nanyhow, it's a moving process, but I hitnk it's getting better as I advance in lessons, happy to give you more detail if you want (pyramid method again haha).",
              "score": 3,
              "created_utc": "2026-02-01 18:30:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3254p2",
                  "author": "antifeixistes",
                  "text": "Thanks! Also saw your other reply with the repo. Will check that out. Thanks a lot!",
                  "score": 2,
                  "created_utc": "2026-02-01 23:14:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30sqzq",
          "author": "larowin",
          "text": "Do you think you could explain a borrow checker without help yet?",
          "score": 1,
          "created_utc": "2026-02-01 19:16:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30svt1",
              "author": "feursteiner",
              "text": "oh yeah def haha",
              "score": 2,
              "created_utc": "2026-02-01 19:16:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o30teox",
                  "author": "feursteiner",
                  "text": "basically a variable's value can be borrowed, i.e. if a = 5, I can declare b that points to the value so to speak. I am allowed to do operations on b (multiple reads), if it's a mut (an actual variable), I can only have one mutation reference at a time. and finally, when a goes out of scope, it's freed from memory. and we can't have borrows outside the scope of a, htat's called hanging.. how did I do ? haha",
                  "score": 1,
                  "created_utc": "2026-02-01 19:19:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o31e3bm",
          "author": "Michaeli_Starky",
          "text": "Books. Use them for learning.",
          "score": 1,
          "created_utc": "2026-02-01 20:59:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ru1bk",
              "author": "25Violet",
              "text": "Books are only good up to a certain point. Their main flaw is that they are one-sided. You can only read what's in it, and if you have any questions about X and the book doesn't elaborate further, or their explanation still does not really make you understand, you are on your own. One of the good things about AI is that you can ask it to dumb it down as much as you need until you finally understand.",
              "score": 1,
              "created_utc": "2026-02-05 19:41:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o31ldm8",
              "author": "feursteiner",
              "text": "thanks granpa.",
              "score": 0,
              "created_utc": "2026-02-01 21:34:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31mjhw",
                  "author": "Michaeli_Starky",
                  "text": "You're welcome.",
                  "score": 1,
                  "created_utc": "2026-02-01 21:40:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o328e6c",
          "author": "haobes",
          "text": "how",
          "score": 1,
          "created_utc": "2026-02-01 23:32:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtweb2",
      "title": "OpenCode Swarm Plugin",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qtweb2/opencode_swarm_plugin/",
      "author": "Outrageous-Fan-2775",
      "created_utc": "2026-02-02 14:23:53",
      "score": 35,
      "num_comments": 12,
      "upvote_ratio": 0.97,
      "text": "I created a swarm plugin for OpenCode that I've been rigorously testing on my own and I think its in a good enough state to get additional feedback. Github link is below but all you have to do is add the plugin to your OpenCode config and NPM will download the latest package for you automatically.\n\n[https://github.com/zaxbysauce/opencode-swarm](https://github.com/zaxbysauce/opencode-swarm)  \n[https://www.npmjs.com/package/opencode-swarm](https://www.npmjs.com/package/opencode-swarm)\n\nGeneral idea is that of perspective management. When you code with the traditional Plan/Build method in OpenCode, you are forcing a slightly different perspective on the LLM but in the end it is still a perspective borne of the same exact training set. My intent was to collate genuinely different data sets by calling different models for each agent.\n\nA single architect guides the entire process. This is your most capable LLM be it local or remote. Its job is to plan the project, collate all intake, and ensure the project proceeds as planned. The architect knows to break the task down into domains and then solicit Subject Matter Expert input from up to 3 domains it has detected. So if you are working on a python app, it would ask for input from a Python SME. This input is then collated, plan adjusted, and implementation instructions are sent to the coding agent one task at a time. The architect knows that it is the most capable LLM and writes all instructions for the lowest common denominator. All code changes are sent to an independent auditor and security agent for review. Lastly, the Test Engineer writes robust testing frameworks and scripts and runs them against the code base.\n\nIf there are any issues with any of these phases they will be sent back to the architect who will interpret and adjust fire. The max number of iterations the architect is allowed to roll through is configurable, I usually leave it at 5.\n\nClaude put together a pretty good readme on the github so take a look at that for more in depth information. Welcoming all feedback. Thanks!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qtweb2/opencode_swarm_plugin/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o3gca9y",
          "author": "stephen_S27",
          "text": "It looks like oh-my-opencode to me, we also have multi agents with different roles",
          "score": 3,
          "created_utc": "2026-02-04 01:46:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3gh1y5",
              "author": "Outrageous-Fan-2775",
              "text": "For sure similar, I used your project as a reference when I needed to understand agent calls better. Along with oh-my-opencode-slim, froggy, and a few other agents. I actually built the swarm as a full on application before I ever knew about OpenCode. When I saw what you could do with plugins I decided to just move the entire idea to OpenCode instead. Following your project readme's instruction to just ask an LLM about it, I put both our project readmes in GPT 5.2 and this was the TLDR.\n\n**Choose OpenCode Swarm when you care about correctness, control, and repeatability.**  \nIt enforces an architect-planned, phase-gated workflow with mandatory QA before code merges and persists project state to disk so work can be resumed deterministically. Best for complex tasks where you want traceability, predictable outcomes, and protection against agent drift or context loss.\n\n**Choose Oh-My-OpenCode when you care about speed, tooling breadth, and ecosystem power.**  \nIt provides a rich library of prebuilt agents, LSP/AST tooling, and strong community support to accelerate development workflows. Best when you want maximum productivity and flexibility and are comfortable trading strict process control for capability and convenience.\n\nAs an aside, one of my constraints was that I needed to use entirely local resources, which limited how many agents I could call. Parallel agents drastically slow down inference on consumer hardware. I needed to build in hard requires for serial operation.",
              "score": 2,
              "created_utc": "2026-02-04 02:13:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3bualp",
          "author": "touristtam",
          "text": "How does it compare to https://github.com/joelhooks/swarm-tools?",
          "score": 2,
          "created_utc": "2026-02-03 12:08:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bzj86",
              "author": "Outrageous-Fan-2775",
              "text": "Somewhat similar. I haven't seen anything that's doing a 1 for 1 copy. joelhooks has a single coordinator that can spawn multiple parallel workers to decompose a project and allow for parallelization. Along with project memory. Mine has project memory as well, but the point is quality, not speed. Swarm-tools doesn't elicit perspectives from other models and it doesn't give the sub agents different roles. So in the end, the quality will be whatever your coordinator is capable of outputting.\n\nWith opencode-swarm, every agent can be a truly different outlook born of vastly different training data and methods. This more closely replicates actual software development methodologies, where you have team members with different jobs and very different backgrounds all working together.   \n  \nHeterogeneous perspectives on a problem are almost always better than homogeneous. In my opinion anyway, all of this is up for interpretation and at the end of the day whatever fits your needs is the best option regardless of how it does it.",
              "score": 3,
              "created_utc": "2026-02-03 12:45:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3c1nj9",
                  "author": "touristtam",
                  "text": "Thank you for taking the time to provide a complete answer. Always useful when so many plugins are being brought out. Make sure you are sharing it on the Discord.",
                  "score": 2,
                  "created_utc": "2026-02-03 12:58:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ihyis",
          "author": "Outrageous_Fan7685",
          "text": "In linux , its working fine but in windows using opencode portable that loads its config from ../users/user/.opencode/opencode json, the plugin is loaded but impossible to load the opencode-swarm.json",
          "score": 2,
          "created_utc": "2026-02-04 11:36:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3in1jn",
              "author": "Outrageous-Fan-2775",
              "text": "That is certainly odd, I can take a look. Do you have any logs or anything? I run entirely in Windows, I've never even tried it in any Linux distro, but plugins are mostly OS agnostic. I have it running on 4-5 Windows 11 boxes currently with no problems. But I also use the full install of OpenCode.",
              "score": 1,
              "created_utc": "2026-02-04 12:14:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ipkt4",
                  "author": "Outrageous_Fan7685",
                  "text": "No logs at all but using opencode portable. I know the conf is not loaded because the architect still using claude while i have configured it to use step3.5 in my local machine. Do you know which path you created the the json conf ?\nTo debug i run:\nOpencode debug agent architect",
                  "score": 2,
                  "created_utc": "2026-02-04 12:32:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35zs6w",
          "author": "Outrageous-Fan-2775",
          "text": "Below is an example of what the architect sends to the coding agent.\n\nImplementing via u/coder...\n\nTASK: Fix issues identified by Auditor and Security Reviewer in Phase 3.\n\nFiles:\n\n1. \\`knowledge-vault/src-tauri/src/services/vector\\_store.rs\\`\n\n2. \\`knowledge-vault/src-tauri/Cargo.toml\\`\n\n3. \\`knowledge-vault/src-tauri/src/services/embeddings.rs\\`\n\n\n\nFIXES REQUIRED:\n\n1. \\*\\*VectorStore\\*\\*:\n\n   \\- \\`search\\`: Use \\`metric(lancedb::DistanceType::Cosine)\\` (or similar API for 0.13) to ensure Cosine similarity.\n\n   \\- \\`add\\_chunks\\`: Normalize vectors to unit length before insertion (required for Cosine with some indexes, good practice).\n\n   \\- \\`delete\\_by\\_file\\`: Sanitize \\`file\\_path\\` in filter string. Replace \\`'\\` with \\`''\\` or use parameters if supported. Let's do string escaping for now as 0.13 sql parser might be simple.\n\n   \\- \\`delete\\_by\\_file\\` signature: ensure it returns Result.\n\n   \n\n2. \\*\\*Cargo.toml\\*\\*:\n\n   \\- Change \\`arrow\\` version from \\`\"52\"\\` to \\`\"52.0.0\"\\` (or just \"52\" if it was strictly \"52\" before, but auditor said it's pre-release. Actually arrow 52.0.0 is released. Let's stick to \"52.0.0\" to be explicit, or \"52.2.0\"). Wait, lancedb 0.13 depends on arrow 52. Let's check lancedb docs/repo. Lancedb 0.13 uses arrow 52. So \"52\" is fine, but maybe auditor was confused by \"52\" vs \"52.0\". Let's use \\`arrow = \"52.0\"\\`.\n\n\n\n3. \\*\\*Embeddings\\*\\*:\n\n   \\- Validate \\`base\\_url\\`: Ensure it starts with http/https.\n\n   \\- Validate response vector dimensions (must be 768).\n\n\n\n4. \\*\\*Security\\*\\*:\n\n   \\- Sanitize \\`file\\_path\\` in \\`delete\\_by\\_file\\`.\n\n\n\nCONTEXT: We are polishing Phase 3 before moving to Phase 4.\n\nACCEPTANCE: Code compiles, fixes implementation gaps.",
          "score": 1,
          "created_utc": "2026-02-02 15:09:57",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o4785rv",
          "author": "TransitionSlight2860",
          "text": "what do you think about the idea of a swarm system? like, is it really boosting your work?",
          "score": 1,
          "created_utc": "2026-02-08 04:26:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35zfjz",
          "author": "Outrageous-Fan-2775",
          "text": "All model settings are done in the opencode-swarm config JSON. if you leave an agent out, it will fall back to its defaults. an example is below of entirely remote setup. you can have as many swarms as you want, and opencode will add Architects with the swarm name appended. so I have Remote and Local. Remote is all API calls with paid plans, Local is entirely my own locally hosted options. I generally leave the architect out so it is always whatever model is selected in OpenCode.  \n  \n\"coder\": { \"model\": \"kimi-for-coding/k2p5\" },   \n\"explorer\": { \"model\": \"zai-coding-plan/glm-4.7\" },   \n\"\\_sme\": { \"model\": \"nvidia/openai/gpt-oss-120b\" },   \n\"\\_qa\": { \"model\": \"nvidia/nvidia/nemotron-3-nano-30b-a3b\" },   \n\"test\\_engineer\": { \"model\": \"zai-coding-plan/glm-4.7-flash\" }\n\nthe underscore tells it to use that model for all calls to that section. you can break it out even further if you wanted to though. for instance \\_qa covers code auditing and security testing. you could break it down so there was a different model for both of those steps",
          "score": 0,
          "created_utc": "2026-02-02 15:08:12",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwwr9w",
      "title": "Codex multi-account plugin (now w/ Codex 5.3 + dashboard)",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/5y8gndp8lqhg1.jpeg",
      "author": "ZookeepergameFit4082",
      "created_utc": "2026-02-05 20:34:58",
      "score": 33,
      "num_comments": 7,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qwwr9w/codex_multiaccount_plugin_now_w_codex_53_dashboard/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3saxct",
          "author": "techsavage",
          "text": "Looks great, could you do the same for Claude OAuth accounts? I know itâ€™s use at your own risk but thereâ€™s definitely people looking for that too",
          "score": 6,
          "created_utc": "2026-02-05 21:02:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3w137f",
          "author": "usrnammit",
          "text": "What's the use case for having this many Codex accounts? Is it just to use a bunch of trial accounts?",
          "score": 2,
          "created_utc": "2026-02-06 12:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3y1r4u",
              "author": "Character_Cod8971",
              "text": "Exactly, same question",
              "score": 1,
              "created_utc": "2026-02-06 18:29:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3sp8kx",
          "author": "Donnybonny22",
          "text": "Doesnt opencode already do this ? Sorry I am new to opencode",
          "score": 1,
          "created_utc": "2026-02-05 22:11:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vvvpb",
              "author": "IISomeOneII",
              "text": "one account yes\nmulti account nope",
              "score": 1,
              "created_utc": "2026-02-06 11:38:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3uoetc",
          "author": "Square-Nebula-9258",
          "text": "Yoooo bro that's whatÂ I wanted today to do",
          "score": 1,
          "created_utc": "2026-02-06 05:14:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y1pb4",
          "author": "Character_Cod8971",
          "text": "Why should you connect multiple ChatGPT accounts? What are the benefits?",
          "score": 0,
          "created_utc": "2026-02-06 18:29:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtrh0d",
      "title": "Synthetic AI Issues.",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/t5ijzb5462hg1.jpeg",
      "author": "NiceDescription804",
      "created_utc": "2026-02-02 10:27:19",
      "score": 31,
      "num_comments": 59,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qtrh0d/synthetic_ai_issues/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o34yn00",
          "author": "sewer56lol",
          "text": "Synthetic documents which models are hosted where on this page https://dev.synthetic.new/docs/api/models , including their parameters.\n\nThey usually self-host the best open models, and proxy the rest to Fireworks or TogetherAI.\n\nThe Synthetic folks have been setting up self hosted K2.5 over the weekend, but it's not trivial. It's a huge ass model, accepts vision (first for them), and securing the extra hardware has been tough. The folks on the Discord have been pretty transparent about this, and are actively around everyday.\n\nLikewise, Fireworks and TogetherAI both had problems hosting K2.5 on their own, as evidenced by your lack of good performance on these proxied requests. This shouldn't be too surprising, it's a 1T param model after all.\n\nHave some patience.\n\nEdit: Synthetic is self hosting K2.5 as of 5 minutes ago. M2.1 will be proxied away to make compute room.\n[Funny timing]",
          "score": 14,
          "created_utc": "2026-02-02 11:17:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34zc1o",
          "author": "FyreKZ",
          "text": "Similar experience, but I'm sure it'll improve soon.",
          "score": 4,
          "created_utc": "2026-02-02 11:23:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o351jwl",
          "author": "Simple_Split5074",
          "text": "What I find more interesting is that according to the plan you should get 1350 requests every 5 hours but your limit will reset in more than 10h?",
          "score": 3,
          "created_utc": "2026-02-02 11:42:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o39ug13",
              "author": "exploriann",
              "text": "I am currently using pro plan 60$, limits Will be reset every 5 hours, you can clearly track the consumption and time until the next reset in their website.",
              "score": 2,
              "created_utc": "2026-02-03 02:34:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3awe9f",
                  "author": "thebraukwood",
                  "text": "Whyâ€™s this guys screenshot show otherwise then? Genuine question",
                  "score": 1,
                  "created_utc": "2026-02-03 06:59:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34tuy8",
          "author": "alovoids",
          "text": "thank you so much, I'm about to try synthetic. now I won't do that",
          "score": 4,
          "created_utc": "2026-02-02 10:34:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35w1rp",
              "author": "harrypham2000",
              "text": "lol still worth it though, you can use other models like MiniMax or DeepSeek, still dope for the price",
              "score": 5,
              "created_utc": "2026-02-02 14:50:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o34uulx",
          "author": "Xera1",
          "text": "I signed up because the free Kimi through opencode kept timing out and it's been great for me. It's not as fast as Anthropic's but it's at least as fast as Gemini through AG, and it's the new hotness so it's probably getting absolutely hammered.",
          "score": 2,
          "created_utc": "2026-02-02 10:43:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o354bis",
          "author": "philosophical_lens",
          "text": "What are some alternatives? Iâ€™m currently on the Z.AI subscription but looking to try something new.",
          "score": 2,
          "created_utc": "2026-02-02 12:03:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35wa15",
              "author": "harrypham2000",
              "text": "maybe celebras but their coding plan already sold out",
              "score": 2,
              "created_utc": "2026-02-02 14:52:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o381phf",
                  "author": "ResponsibilityOk1306",
                  "text": "64k context limit for glm",
                  "score": 1,
                  "created_utc": "2026-02-02 20:52:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o356ttk",
          "author": "P1zz4-T0nn0",
          "text": "Worked fine before the weekend. Today K2.5 is unusable. It just stops working, can't finish one response.",
          "score": 2,
          "created_utc": "2026-02-02 12:22:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36z2gu",
              "author": "sudoer777_",
              "text": "Kimi K2.5 Free has been a lot buggier today for me on OpenCode Zen than the previous days so there might be a provider issue involved (Fireworks probably)",
              "score": 2,
              "created_utc": "2026-02-02 17:54:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o381yuf",
                  "author": "ResponsibilityOk1306",
                  "text": "kimi is hosted on synthetic. its very fast on fireworks.",
                  "score": 1,
                  "created_utc": "2026-02-02 20:54:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o381dwt",
          "author": "ResponsibilityOk1306",
          "text": "I also fell for it, but just $10 via api, payg. slow, that my apps timeout after 15 minutes. hitting error 429 frequently. this is just poor service.\n\nFireworks is blazing fast, happy to use them.",
          "score": 2,
          "created_utc": "2026-02-02 20:51:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hghvu",
              "author": "touristtam",
              "text": "What's the downside of using Fireworks directly instead of going through another ~~reseller~~ provider like Synthetic?",
              "score": 1,
              "created_utc": "2026-02-04 06:01:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3529nk",
          "author": "dbkblk",
          "text": "I don't understand your problem. Every time I use it, it's quite fast to answer.  \nIt happens that it could rarely get stuck for some seconds, but not much more than when I was using Claude.  \nEDIT: Ok, probably because I use GLM.",
          "score": 5,
          "created_utc": "2026-02-02 11:47:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o352txp",
              "author": "NiceDescription804",
              "text": "GLM and minimax are doing great but I subscribed for Kimi.",
              "score": 2,
              "created_utc": "2026-02-02 11:52:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o357rkm",
                  "author": "dbkblk",
                  "text": "I've barely tried Kimi, so that's why our experiences differ. But as other said, it has just been deployed. They may are encountering issues with deployment?",
                  "score": 1,
                  "created_utc": "2026-02-02 12:29:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34tx68",
          "author": "jrsa2012",
          "text": "For me it is working just fine.",
          "score": 3,
          "created_utc": "2026-02-02 10:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3566pp",
          "author": "Ok_Direction4392",
          "text": "I also signed up on synthetic recently to use Kimi K2.5 mainly. Only had one failed request so far today, otherwise it's been running solid for a few hours.",
          "score": 1,
          "created_utc": "2026-02-02 12:17:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o359qs3",
          "author": "ryudice",
          "text": " thanks, I was considering it as well, Iâ€™ll just stick to the kimi subscription for now",
          "score": 1,
          "created_utc": "2026-02-02 12:43:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35bejv",
          "author": "dyzhdyzh",
          "text": "I subscribed to them yesterday evening. Solely because of Kimi K2.5. Zero issues both yesterday and this morning.",
          "score": 1,
          "created_utc": "2026-02-02 12:54:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o365snq",
              "author": "dyzhdyzh",
              "text": "I stand corrected. It **is** quite slow right now. Only ~30 requests in the last two hours. I see 30+ second delays between tool calls and token output of ~1-3 tokens per second.",
              "score": 1,
              "created_utc": "2026-02-02 15:39:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3awvwn",
                  "author": "thebraukwood",
                  "text": "A lot of new users because of k2.5 but from everything Iâ€™ve seen of the Synthetic team I believe theyâ€™ll iron out the issues as soon as possible. People need to be more understanding now adays",
                  "score": 1,
                  "created_utc": "2026-02-03 07:03:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35e8wf",
          "author": "blankeos",
          "text": "Really? Damn.. I was gonna get one.",
          "score": 1,
          "created_utc": "2026-02-02 13:12:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35lab5",
          "author": "Josh8972",
          "text": "I've been using Synthetic for a couple of weeks now and switched to Kimi K2.5 when it became available. No problems/issues here.",
          "score": 1,
          "created_utc": "2026-02-02 13:53:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hfm1a",
              "author": "touristtam",
              "text": "What plan are you on?",
              "score": 1,
              "created_utc": "2026-02-04 05:54:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35ojl1",
          "author": "harrypham2000",
          "text": "problem is not with Synthetic hosted models, problem is their provider, sometimes I met this and figured out that most of it caused by their provider for the models like TogetherAI and Fireworks, you could check at [status.synthetic.new](http://status.synthetic.new)",
          "score": 1,
          "created_utc": "2026-02-02 14:11:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35rjad",
              "author": "NiceDescription804",
              "text": "https://preview.redd.it/mcojc7qxc3hg1.jpeg?width=1280&format=pjpg&auto=webp&s=66334182b0d209ff78f1d0cff70d8ccb1b17a68d\n\nWE'RE BACK TO SELF HOSTING.",
              "score": 1,
              "created_utc": "2026-02-02 14:27:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o382xmn",
                  "author": "ResponsibilityOk1306",
                  "text": "either they are lying, or something is wrong. fireworks has been very fast for me, and synthetic very slow. maybe their account is rate limited, or something.",
                  "score": 1,
                  "created_utc": "2026-02-02 20:58:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35r0n7",
          "author": "gonssss",
          "text": "same for me, fucking slow",
          "score": 2,
          "created_utc": "2026-02-02 14:24:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36jon5",
          "author": "annakhouri2150",
          "text": "The limits are not fake, what are you talking about. Maybe you can't reach them â€” due to errors and slowness â€” using Kimi K2.5, but they've got dozens of other very good, competent models available on their API that you can use, and you absolutely can fully saturate the API call limits. I've done it regularly for months since I subscribed. They're absolutely not fake.\n\nRegarding the errors and slowness of K2.5, this is a temporary thing, due to the huge influx of new users they've had, plus the fact that K2.5 was just released. They're actively working on securing more compute, and communicating very actively in their Discord with users about the state of things and listening to complaints (when they're not sleeping, or pulling all-nighters). I've used K2T (K2.5's predecessor) with them since I joined and it has been rock solid. K2.5 will get there eventually too.\n\nI don't think it's fair to call this fake.",
          "score": 1,
          "created_utc": "2026-02-02 16:43:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34tiaf",
          "author": "indian_geek",
          "text": "Have you tried other models? Considering Kimi 2.5 is new, it could potentially be an issue with this model.",
          "score": 1,
          "created_utc": "2026-02-02 10:31:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34ubyj",
              "author": "NiceDescription804",
              "text": "It's not a black or white situation they're advertising serving models, but leaving behind details that it's not usable AT ALL. \n\nI don't have any objections on being up front with consideration. \nBut oh my god did they advertise the living shit out of Kimi k2.5.",
              "score": 2,
              "created_utc": "2026-02-02 10:38:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o36am0r",
                  "author": "indian_geek",
                  "text": "They just posted an update on their discord regarding Kimi",
                  "score": 1,
                  "created_utc": "2026-02-02 16:01:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34v1ux",
          "author": "LittleChallenge8717",
          "text": "Kimi model is't hosted in their gpu's that's reason (they have fireworks provider currently, they plan to host kimi in the next week , currently I agree it has issues on kimi, but glm and minimax works great)",
          "score": 1,
          "created_utc": "2026-02-02 10:45:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34w6m1",
              "author": "NiceDescription804",
              "text": "It's dishonest though to know they can't ensure reliability and just hammer away the ad posts with all these bots.",
              "score": 4,
              "created_utc": "2026-02-02 10:55:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o352huo",
                  "author": "dbkblk",
                  "text": "Which bots are you talking about?",
                  "score": 2,
                  "created_utc": "2026-02-02 11:49:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o354fp1",
          "author": "Bob5k",
          "text": "have in mind that synthetic is actually rerouting kimi k2.5 to fireworks which is having problems - so you're blaming the wrong company for the problems because of your incompetence / ignorance.   \nalso - fireworks have fixed this and kimi is just as usable as it was before weekend hit.",
          "score": 0,
          "created_utc": "2026-02-02 12:04:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o382dp7",
              "author": "ResponsibilityOk1306",
              "text": "I am not sure about this. over the past few days, I have tested both synthetic and fireworks. I haven't experienced any issue on fireworks so far, no errors, super fast answers, etc. on Synthetic, after 15 minutes there is still no reply, when fireworks can get it done in 3 minutes max.",
              "score": 1,
              "created_utc": "2026-02-02 20:55:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o383b85",
                  "author": "Bob5k",
                  "text": "but you do realize that till today at approx 2pm berlin time they've been routing kimi through fireworks? so how fireworks can be worse than fireworks if it's direct routing? :)",
                  "score": 1,
                  "created_utc": "2026-02-02 21:00:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qvscxp",
      "title": "Your ZSH shell, but with an AI scratchpad",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qvscxp/your_zsh_shell_but_with_an_ai_scratchpad/",
      "author": "a_alberti",
      "created_utc": "2026-02-04 15:59:02",
      "score": 30,
      "num_comments": 20,
      "upvote_ratio": 0.94,
      "text": "[Knight Rider spinner for Zsh Line Editor \\(ZLE\\)](https://i.redd.it/31sfhr5c2ihg1.gif)\n\n  \n  \nI made a zsh plugin that lets you iterate with an AI agent directly in your prompt until the command looks right (powered by \\`opencode\\`).\n\nYou keep your scratch notes, refine them line by line, and the agent keeps rewriting the command in place. Nothing gets executed for you; it just helps you draft.\n\nExtras:\n\n\\- #? explainer mode (â€œwhat does this command do?â€), answer formatted in MarkDown\n\n\\- a gorgeous Knight Rider spinner while it thinks\n\nRepo: [https://github.com/alberti42/Zsh-Opencode-Tab](https://github.com/alberti42/Zsh-Opencode-Tab)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qvscxp/your_zsh_shell_but_with_an_ai_scratchpad/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o3jt84z",
          "author": "brunogbasto",
          "text": "This great! Thatâ€™s basically what I used Warp for before I switched to Ghostty.",
          "score": 7,
          "created_utc": "2026-02-04 16:02:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ju3dm",
              "author": "a_alberti",
              "text": "Thanks! Give it a try, itâ€™s still new, but Iâ€™m already using it daily and itâ€™s been genuinely useful.\n\nAnd if the defaults donâ€™t match your style, you can tweak the agent prompts to make it behave exactly how you want. I tried to keep them sensible and not bloated. Iâ€™d really love feedback from the community on what to improve.",
              "score": 2,
              "created_utc": "2026-02-04 16:06:39",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3jvonq",
              "author": "a_alberti",
              "text": "And btw, that was the point: you get a similar â€œdraft + iterate + explainâ€ loop, but the plugin lives in plain zsh, so it works in Ghostty/iTerm/Terminal (and my favorite WezTerm). Youâ€™re not locked into one terminal UI like Warp.\n\nPS: Iâ€™ve never tried Warp personally, so this isnâ€™t a judgment about Warp, just a note about portability of the core idea.",
              "score": 2,
              "created_utc": "2026-02-04 16:13:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3oivoj",
                  "author": "drinksbeerdaily",
                  "text": "Throwing shade on Kitty smh. Kidding. Great little tool. Iterations and user confirmation before execution is good UX.",
                  "score": 1,
                  "created_utc": "2026-02-05 07:40:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3lfo9v",
          "author": "Xeon06",
          "text": "I quite like that. I've been using something called sgpt for a while now but was looking for something a little nicer recently. Well done!\n\nEdit: Hm, can't seem to get it to work. Added to my .zshrc\n\n    â¯ echo $plugins\n    git zsh-opencode-tab\n\nBut I get file tab complete when I try say `# ping google.com`",
          "score": 1,
          "created_utc": "2026-02-04 20:32:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n8btd",
              "author": "toadi",
              "text": "When I read the documentation I knew this tab would be an issue. I have fzf plugin installed that used tab too. Would be handy if you could change that key.\n\nFor me that plugin is: Aloxaf/fzf-tab\n\nI will not replace that plugin with the llm one even though I like it a lot.",
              "score": 1,
              "created_utc": "2026-02-05 02:12:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ofyci",
                  "author": "a_alberti",
                  "text": "I use both fzf and Aloxaf/fzf-tab, and the plugin is compatible with both. I need to understand why it breaks for you and put a remedy. I am busy right now but I will come back.",
                  "score": 1,
                  "created_utc": "2026-02-05 07:13:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3owegn",
              "author": "a_alberti",
              "text": "I put a comment below explaining to toadi what is likely the reason. Try to load zsh-opencode-tab as last one after fzf-tab, and other plugins binding to TAB (i.e., Ctrl-I).",
              "score": 1,
              "created_utc": "2026-02-05 09:50:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3q2frb",
                  "author": "Xeon06",
                  "text": "I echo'd my $plugins above, only git and yours, does it require fzf-tab?",
                  "score": 1,
                  "created_utc": "2026-02-05 14:44:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3p5gjc",
          "author": "ezhupa99",
          "text": "good job, i really like the idea, can you make it windows compatible?",
          "score": 1,
          "created_utc": "2026-02-05 11:13:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45unw7",
              "author": "a_alberti",
              "text": "Thanks for the positive feedback. Yeah, I plan to work more in the coming days / weeks on furhter extensions / improvements. I have a few ideas. I think the most important thing  is to fine-tune the interaction with the agent(s), to have a smooth experience when you iteratively build your shell command on the command line.\n\nI am also considering supporting not just single-shot calls to the agent but a per-shell chat with the agent. Possibly introducing some extra magic commands to start a new session. Maybe also to open the current session with opencode. I imagine the situation when one starts a dummy exchange, and then it develops into something interesting and worth preserving.\n\nBut coming back to your question: I don't have Windows. I will at some point test it on Windows too, but I cannot promise this will happen in the next few days. Did you try to run it with WSL or WSL2? It should work, but I did not test it myself.",
              "score": 1,
              "created_utc": "2026-02-07 23:12:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3plrd0",
          "author": "atkr",
          "text": "Why make it zsh specific?",
          "score": 1,
          "created_utc": "2026-02-05 13:11:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45v8co",
              "author": "a_alberti",
              "text": "Because I use zsh. There is a large community around zsh. Do you have other ideas? I am sure the same concept can be ported to many other shells.\n\nHowever, this implementation is heavily integrated / tailored to zsh, in the sense that it develops the Knight Rider animation bar in the Zsh Line Editor (something that is quite painful to control, but it gives the best experience of a nicely integrated tool).\n\nWere you thinking of bash?",
              "score": 1,
              "created_utc": "2026-02-07 23:15:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3juhz2",
          "author": "Otherwise_Wave9374",
          "text": "This is such a nice UX idea, drafting commands with an agent but keeping execution manual is exactly the right safety line.\n\nDo you have any plans to add optional \"constraints\" presets (like no sudo, no rm, only read-only) so the agent suggestions stay within bounds? I have been reading more about safe tool use patterns for agents here: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-02-04 16:08:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jv3nu",
              "author": "a_alberti",
              "text": "There are already constraints like no sudo. Also, if the operation is dangerous, the agent proposes to you a dry-run version, and the dangerous command is given commented out.. so even if you are distracted and press enter, you only get a dry-run / preview of the command.",
              "score": 3,
              "created_utc": "2026-02-04 16:11:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3jwcfx",
                  "author": "a_alberti",
                  "text": "Also more on practical safety: the agent prompt has \"no tools,\" and opencode permissions are set to deny (so even if the model tried, it can't invoke privileged actions through opencode).\n\nThe plugin itself never executes generated commands; it only inserts text into your shell buffer. You still choose whether to run it. And dangerous commands are commented out.",
                  "score": 1,
                  "created_utc": "2026-02-04 16:16:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qtn8lt",
      "title": "I thought Kimi 2.5 was exaggerated by Chinese people with their patriotism.",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qtn8lt/i_thought_kimi_25_was_exaggerated_by_chinese/",
      "author": "Ok-Regret-4013",
      "created_utc": "2026-02-02 06:13:52",
      "score": 28,
      "num_comments": 74,
      "upvote_ratio": 0.69,
      "text": "Yesterday I subscribed to Synthetic.   \n  \n It was disappointing, because of, I guess, there were issues with the weekend or server migration.  \n  \nBut today it is so good. It is fast and smart. This model is not exaggerated, and the billing is quite reasonable.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qtn8lt/i_thought_kimi_25_was_exaggerated_by_chinese/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o34kvf5",
          "author": "NiceDescription804",
          "text": "I actually subscribed to synthetic unlike these bots\n\nhttps://preview.redd.it/zb9yczoyr1hg1.jpeg?width=1280&format=pjpg&auto=webp&s=e8908aa52df0682c2a89f83212248297574fb7d4\n\nThe limits are not what's advertised. Kimi k2.5 time to first token is 30 seconds in some cases. No support or response. The models cut off randomly. So unstable.",
          "score": 22,
          "created_utc": "2026-02-02 09:07:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36zmmd",
              "author": "elllyphant",
              "text": "I'm sorry for the poor experience :( We had overwhelming usage, but as of 3AM PST today, (6 hours ago), we're back to self-hosting kimi k2.5. I'm here to help & support, please let me know what you need!",
              "score": 5,
              "created_utc": "2026-02-02 17:56:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o34laxv",
              "author": "Select-Service-5023",
              "text": "Iâ€™ve had similar experiences. They are kinda suffering from success and are trying to get more computer. Itâ€™s definitely a slight bummer. When they get it self-hosted with enough capacity Iâ€™m sure those issues will disappear",
              "score": 6,
              "created_utc": "2026-02-02 09:12:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34lfuc",
                  "author": "NiceDescription804",
                  "text": "No it's not a slight bummer. \nIt's not delivering what I'm paying for deeming the subscription useless. \nI'm gonna be posting about this so people don't fall into the marketing like I did.",
                  "score": 10,
                  "created_utc": "2026-02-02 09:13:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o34lbuw",
                  "author": "Select-Service-5023",
                  "text": "Compute* lol",
                  "score": 3,
                  "created_utc": "2026-02-02 09:12:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o36huft",
              "author": "annakhouri2150",
              "text": "Join the Discord, they're actively working on it, making announcements about it, responding to people about it.",
              "score": 2,
              "created_utc": "2026-02-02 16:35:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o373up4",
              "author": "sudoer777_",
              "text": "Not to be a shill for Synthetic, but I've also been having issues with OpenCode Zen with Kimi K2.5, and it's probably due to the massive influx of users with both services still working on stabilizing the infrastructure and relying on Fireworks which is the source of the problems, since this model only came out a few days ago",
              "score": 1,
              "created_utc": "2026-02-02 18:15:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dq63r",
              "author": "RudyRobichaux",
              "text": "I've not had any issues with synthetic, and I'm not a bot. Though they are currently hosting it on their own hardware, and from their discord it's been a learning experience, so I think quality may vary. I had subscribed to them a few months ago, and they refunded me due to performance issues unprompted, so you may want to reach out to them. That being said, I'm enjoying them so much here is my referral code https://synthetic.new/?referral=fvJqmyHGVGdftwm.",
              "score": 0,
              "created_utc": "2026-02-03 18:00:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o342ugg",
          "author": "SamatIssatov",
          "text": "So much advertising. Why buy Kimi when you can buy Codex for the same price? I don't understand these advertisers, they're always deceiving people.",
          "score": 13,
          "created_utc": "2026-02-02 06:22:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o374m2r",
              "author": "sudoer777_",
              "text": "Kimi is open weight, and I don't want OpenAI/Anthropic to have my business and monopolize the AI industry",
              "score": 4,
              "created_utc": "2026-02-02 18:19:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o34al2h",
              "author": "RegrettableBiscuit",
              "text": "It's the same price, but not the same usage limits.Â ",
              "score": 6,
              "created_utc": "2026-02-02 07:30:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34f5rl",
                  "author": "SamatIssatov",
                  "text": "We're talking about Codex, not Claude. And it's very economical. I program all day long and it's enough for me. You can't compare Codex/Claude with GLM/Kimi. If you need them for repetitive tasks, then yes, GLM/Kimi may be justified.",
                  "score": 1,
                  "created_utc": "2026-02-02 08:13:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o36il3f",
              "author": "annakhouri2150",
              "text": "1. You get API usage, as the other person said, no weird OAuth hacks to use the service in anything other than Codex CLI, that could get you blocked (like what Anthropic did to OpenCode)\n2. They don't train on, or even ever store, your prompts or outputs (check their Privacy Policy and TOS)\n3. They're extremely open, active, and responsive in their Discord, so you always know what's going on\n4. Since they're only running open weight models, if you get used to a model, but want to switch providers, you can actually do that, instead of being locked into GPT\n5. They won't secretly replace a model with a differently-RL'd one under your nose, but keep the ID the same\n6. They won't secretly quantize a model under load and then gaslight you telling you it's not different.\n\nCall me a bot if you want. I'm not. Y'all are just cynical. Which is understandable, given the, you know, general AI landscape, but I feel like Synthetic is, if anything, an oasis from that.",
              "score": 2,
              "created_utc": "2026-02-02 16:38:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o348bc4",
              "author": "CYTR_",
              "text": "I also don't know why people would choose a Kimi API if the price is similar. On the other hand, personally, I really want to rent a B300 server and run K2.5 on it. \n\nIn this way: we have the benefits of GDPR regulation and a powerful model. I can't wait to try this out as soon as I have the time.",
              "score": 1,
              "created_utc": "2026-02-02 07:09:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34odxv",
                  "author": "RnRau",
                  "text": "You don't really need Nvidia hardware to just run inference. Surely non-nvidia inference options are cheaper? Sure if you are training as well... nvidia is THE option, but just common garden variety inference, nvidia might not be the most cost effective?",
                  "score": 1,
                  "created_utc": "2026-02-02 09:42:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o36klam",
              "author": "telewebb",
              "text": "I'm sorry, I most likely don't understand your statement. Did you just ask \"why use OpenCode when you can purchase a subscription to a different product like codex\"? Or \"why use pay-as-you-go with any combination of middle when you could buy a subscription to one walled garden of models\"? Statements slightly exaggerated to make sure I'm communicating what I think the question is.",
              "score": 1,
              "created_utc": "2026-02-02 16:47:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3454bd",
              "author": "Ok-Regret-4013",
              "text": "I think other providers are not good, but Synthetic is okay. If you can afford the high prices of Codex and Anthropic, it is good. But to me, I use custom prompts and agent workflows, so fast and smart balance and keeping my rules is better for me.",
              "score": 1,
              "created_utc": "2026-02-02 06:41:53",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o34p84z",
              "author": "charmander_cha",
              "text": "Particularly because I'd rather give my money to the Chinese than to the Nazis of this century.",
              "score": 1,
              "created_utc": "2026-02-02 09:50:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o349642",
          "author": "elllyphant",
          "text": "Thank you for sharing and thank you sooo much for your patience.   \nBoth Synthetic founders have been working all weekend to get Kimi K2.5 faster. It's still in progress but we'll update in Discord when the work is done. We really value having a powerful product at a competitive price (and your privacy).",
          "score": 10,
          "created_utc": "2026-02-02 07:17:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34gmm8",
              "author": "Simple_Split5074",
              "text": "Can you give an indication on tps (and maybe time to first token)?Â ",
              "score": 1,
              "created_utc": "2026-02-02 08:26:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3725ay",
                  "author": "elllyphant",
                  "text": "https://preview.redd.it/a23ey4m8g4hg1.png?width=2724&format=png&auto=webp&s=627e91464480bdc2f13f0f50ca0a377ac0d005e4\n\nhmm is this helpful? if not I'll ask Matt again   \n(this is from 6 hours ago)",
                  "score": 3,
                  "created_utc": "2026-02-02 18:08:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3b2u8a",
          "author": "Snoo_57113",
          "text": "It is good, but they were overwhelmed over the weekend in opencode, I think it is good to pay moonshot since that money helps to fund future models.\n\nI still use minimax it is a little slow, but very reliable. I had tasks that run overnight and it finishes it in a couple hours. No better feeling than to wake up with a big chunk of code ready for review.",
          "score": 2,
          "created_utc": "2026-02-03 07:57:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o345xis",
          "author": "jpcaparas",
          "text": "It's pretty awesome but I find it tiring to actively campaign for people to not get it from [Kimi.com](http://Kimi.com) because they don't know the implications of doing so.",
          "score": 2,
          "created_utc": "2026-02-02 06:48:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o349jqp",
              "author": "Crowley-Barns",
              "text": "What are the implications?\n\nAnd why do you campaignâ€¦? Like, as an act of human goodwill??\n\nI wanna know what Kimi did to you!",
              "score": 2,
              "created_utc": "2026-02-02 07:20:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34bepo",
                  "author": "Crowley-Barns",
                  "text": "Oh. \n\nHmm. Iâ€¦ kinda donâ€™t care haha. As long as theyâ€™re open about doing it. \n\nI donâ€™t have anything highly confidential so Iâ€™m okay contributing in a small way to advancement.",
                  "score": 2,
                  "created_utc": "2026-02-02 07:37:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o34an1g",
                  "author": "RegrettableBiscuit",
                  "text": "They train models on your data.Â ",
                  "score": 2,
                  "created_utc": "2026-02-02 07:30:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o348jky",
          "author": "Select-Service-5023",
          "text": "*HAVE* to rep synthetic.new here... just join the discord and nose around. That alone will be the ONLY evidence for or against it you will need.",
          "score": 2,
          "created_utc": "2026-02-02 07:11:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34kqsn",
              "author": "FyreKZ",
              "text": "Seems like a pretty good deal, what does a request count as? 135 requests every 5 hours? Is that 135 prompts or individual API calls?",
              "score": 2,
              "created_utc": "2026-02-02 09:06:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34l097",
                  "author": "Select-Service-5023",
                  "text": "Website shows exact. But itâ€™s simple. A message = one request. Tool calls are I think 0.1 requests, and super small messages under 2048 are 0.2 requests. No weekly, just 5 hour window. I cannot stress how much usage this really is in practice.",
                  "score": 3,
                  "created_utc": "2026-02-02 09:09:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34plp7",
          "author": "trypnosis",
          "text": "I used kimi 2.5 over the weekend and it was terrible.\n\nAre you saying I should go back and try again?",
          "score": 1,
          "created_utc": "2026-02-02 09:54:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34spaf",
              "author": "manojlds",
              "text": "Nah, I found GLM 4.7 better.",
              "score": 1,
              "created_utc": "2026-02-02 10:23:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34suz4",
                  "author": "trypnosis",
                  "text": "From synthetic or other provider. Iâ€™m keen on using synthetic as I just got the sub.",
                  "score": 1,
                  "created_utc": "2026-02-02 10:25:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o376dq4",
                  "author": "sudoer777_",
                  "text": "Based on my usage so far, I've found that Kimi K2.5 has better reasoning and was less likely to get confused by stupid things while debugging but tends to be worse at following instructions, but I've only used it for a couple days so far",
                  "score": 1,
                  "created_utc": "2026-02-02 18:27:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35hsu2",
          "author": "DistinctWay9169",
          "text": "Is it worth it paying synthetic instead of kimi plan on kimi.com?\n\n",
          "score": 1,
          "created_utc": "2026-02-02 13:33:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d70tk",
              "author": "Funny-Advertising238",
              "text": "Definitely pay kimi directly. For what it's worth I've tried from many different providers, kimi performed the best. Not one failed tool call while the others all had issues.Â ",
              "score": 1,
              "created_utc": "2026-02-03 16:32:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d7vn7",
                  "author": "DistinctWay9169",
                  "text": "The only thing I did not like is that their $20 plan is bad; it is easy to hit limits and have weekly limits. GLM 4.7 is not that behind Kimi, and their $6 plan gives us much better limits and no weekly limits. Just waiting for Claude to release the Sonnet 5 that will be better than Kimi, as it will be basically Opus 4.5 but cheaper, so I would rather pay $20 for Claude than $19 for Kimi.",
                  "score": 1,
                  "created_utc": "2026-02-03 16:36:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35yolr",
          "author": "wallapola",
          "text": "Is nano-gpt good enough for kimi k2.5 or is [chutes.ai](http://chutes.ai) already fine? Iâ€™ve heard the synthetic provider is one of the best, but itâ€™s expensive and I canâ€™t really justify paying $20 when I can get claude code pro for the same price.",
          "score": 1,
          "created_utc": "2026-02-02 15:04:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o370826",
              "author": "elllyphant",
              "text": "you can try it for $12/mo this month via Â [https://synthetic.new/?saleType=moltbot](https://synthetic.new/?saleType=moltbot)",
              "score": 1,
              "created_utc": "2026-02-02 17:59:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3714rc",
                  "author": "wallapola",
                  "text": "Okay but how about for following months?",
                  "score": 1,
                  "created_utc": "2026-02-02 18:03:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36vbxj",
          "author": "bigh-aus",
          "text": "Yeah, it's models like this that make me think going forward the American AI company's value is in running the models, less the model themselves.  China is certainly setting the bar high for what you can run locally if you have the $.  Need more OSS releases from the USA / Europe, but I know that goes against their valuation.    \n  \nJust wait until the local hardware catches up! eg a Mac Studio M7 Ulta 1TB unified ram :p",
          "score": 1,
          "created_utc": "2026-02-02 17:37:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38gb8o",
          "author": "cutebluedragongirl",
          "text": "If you are using API, you should always use the official provider.",
          "score": 1,
          "created_utc": "2026-02-02 22:01:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3axwuk",
          "author": "knowoneknows",
          "text": "synthetic + opencode is the way to go, best experience so far with kimi k2.5. I was using K2.5 (free) through Kilo Code for another project and it was impressive when it worked but damn slow. Synthetic experience is much better with opencode - Charm Crush is pretty nice too but opencode is the best experience so far.\n\nEdit:\n\nIt's buggy, especially running parallel agents. It's also really slow at times?",
          "score": 1,
          "created_utc": "2026-02-03 07:12:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3g0ocb",
              "author": "epyctime",
              "text": "what's buggy, synthetic? first i eard of it",
              "score": 1,
              "created_utc": "2026-02-04 00:41:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3g708h",
                  "author": "knowoneknows",
                  "text": "Both opencode and Kimi k2.5 through synthetic",
                  "score": 1,
                  "created_utc": "2026-02-04 01:16:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qwqv23",
      "title": "OpenCode Bar 2.3.2: Now tracks OpenCode + Codex, Intel Mac support, new providers",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qwqv23/opencode_bar_232_now_tracks_opencode_codex_intel/",
      "author": "kargnas2",
      "created_utc": "2026-02-05 17:03:06",
      "score": 26,
      "num_comments": 3,
      "upvote_ratio": 0.88,
      "text": "Quick update since 2.1.1:\n\n**Backed by OP.GG**\n- Since I'm the Founder OP.GG, I decided to move this repo to OP.GG's repository, because many of our members use this.\n\n**Now tracks both OpenCode AND Codex**\n- Native Codex client support with ~/.codex/auth.json fallback\n- See all your AI coding usage in one menu bar app\n- It distinguishes the account id, so you can see every account\n\n**New Providers**\n- Chutes AI\n- Synthetic\n- Z.AI Coding Plan (GLM 4.7)\n- Native Gemini CLI Auth\n- Native Codex Auth\n\n**Platform**\n- Intel Macs (x86) now supported\n- Brew installation\n\n**Install:**\n\nbrew tap opgginc/opencode && brew install opencode-bar\n\nGitHub: [https://github.com/opgginc/opencode-bar](https://github.com/opgginc/opencode-bar)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qwqv23/opencode_bar_232_now_tracks_opencode_codex_intel/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o3rjf3g",
          "author": "BERLAUR",
          "text": "Looking forward to a CLI Linux version!Â ",
          "score": 7,
          "created_utc": "2026-02-05 18:52:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3s3yst",
          "author": "Possible-Text8643",
          "text": "windows pretty pwease?",
          "score": 3,
          "created_utc": "2026-02-05 20:29:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tnszz",
          "author": "Nearby_Tumbleweed699",
          "text": "This is fantastic. I'll add it right now.",
          "score": 1,
          "created_utc": "2026-02-06 01:23:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwacgd",
      "title": "Cheapest Provider",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qwacgd/cheapest_provider/",
      "author": "FutureIncrease",
      "created_utc": "2026-02-05 03:34:05",
      "score": 24,
      "num_comments": 18,
      "upvote_ratio": 0.96,
      "text": "Whatâ€™s the cheapest way to get access to MiniMax 2.1/Kimi K2.5?\n\nI use CC Max (x20) for work. Interested in switching but not sure I can afford other solutions since Iâ€™ve heard the Max plan is heavily subsidized.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qwacgd/cheapest_provider/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o3nqh2s",
          "author": "devdnn",
          "text": "If cost is your sole consideration and youâ€™re not experimenting with other models, GitHub Copilot Pro+ for $39.99 or Copilot Pro with $10 is an incredible offer, set you budget limit for $29, itâ€™s still worth it if thatâ€™s within your budget.\n\nItâ€™s charged on per request, quality prompts and not vibe coding will take very far.\n\nThe only limitation I have come across from not going to direct company is the context size, but that is hugely mitigated using the subagents and MD files for memory.\n\nI stuck to it for 2 weeks and use it as my only go to and now itâ€™s my daily driver.",
          "score": 10,
          "created_utc": "2026-02-05 03:59:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3o65o6",
              "author": "IIALE34II",
              "text": "I've had issues with multiple agents hitting rate limits with Copilot. Other than that it's a very good experience. Dirt cheap for what you get too.",
              "score": 1,
              "created_utc": "2026-02-05 05:49:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3oqqwp",
              "author": "jixbo",
              "text": "Don't they have the context limited?\nChat gpt plus is around 20 quid and fairly good limits to work with codex 5.2.",
              "score": 1,
              "created_utc": "2026-02-05 08:54:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3o5xq8",
          "author": "DJDannySteel",
          "text": "Antigravity auth plugin, Gemini auth plugin, free usage from kilo and codex etc plugins, and boom bam. Or open router on the 10 dollar hide usage allowances",
          "score": 6,
          "created_utc": "2026-02-05 05:48:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tve45",
          "author": "joe__n",
          "text": "[chutes.ai](http://chutes.ai) is my goto for these models",
          "score": 3,
          "created_utc": "2026-02-06 02:08:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3uso45",
          "author": "Ok_Cricket9353",
          "text": "You can use https://chutes.ai They have API plans with daily requests limits and PAYG. their 10-20$ plan will be sufficient. They even have 3$ plan for lower needs with 300 requests per day.\nYou will get many models, privacy centred TEE models. There are many models to choose from.",
          "score": 2,
          "created_utc": "2026-02-06 05:46:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ole2b",
          "author": "MaxPhoenix_",
          "text": "\"what's the cheapest minimax/kimi\":  \nedit: removed minimax - thay model is another nanny model absolutely useless.  \ndirect kimi-2.5 (kimi.com): $19/mo for 2000-3500 requests per week (7day rolling cycle) (reported)  \ndirect [z.ai](http://z.ai) glm even though you didn't ask it's worth it: $6/mo for 120 requests per 5hr\n\n\"other solutions\":  \ngithub copilot (github.com) $10/mo for 300 premium requests (best deal on opus-4.5 flat rate!)  \nuse AMPcode (ampcode.com/free): FREE mode gives $10 of credit a day that includes opus-4.5 supposedly  \nuse OPENCODE zen: right now these are FREE: minimax-m2.1(trash), glm-4.7, kimi-2.5, big pickle, trinity large preview  \nuse KILO code: right now these are FREE: minimax-m2.1(trash), glm-4.7, corethink, giga potato, arcee ai..  \nyou can also less models nearly limitless (qwen code and gemini cli) or [openrouter.ai](http://openrouter.ai) free models that hit throttle/limits\n\nEDIT: explaining why to not \"just use the free kimi/minimax(trash)/glm?\" -> because they are slow and run into throttle issues and timeout and they train on your sessions.  if you aren't paying, you are the product.",
          "score": 4,
          "created_utc": "2026-02-05 08:03:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o4xjr",
          "author": "Shep_Alderson",
          "text": "Itâ€™s not the absolute cheapest but Iâ€™ve really enjoyed synthetic.new. $20/mo for very useable 5 hour limits. Their customer service is also amazing. I had a billing issue when I downgraded from their $60 plan to their $20 plan. I was supposed to get the remainder of my billing period at the same $60 limits, but when I renewed to continue on the $20 plan, it cut me down to the $20 plan limits.\n\nI emailed their support email in the contacts page and that evening the cofounder emailed me, apologized for the issue, corrected the billing for the remaining month and gave me a $40 credit so I could have an extra month of their Pro plan at the standard price before my downgrade kicked in. The fact that they not only fixed the remaining billing period issue, but also gave me credits for my trouble, really speaks volumes to me. I doubt Iâ€™ll go anywhere else for running the open weight models.",
          "score": 2,
          "created_utc": "2026-02-05 05:40:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ombjc",
              "author": "ZeSprawl",
              "text": "I agree about [synthetic.new](http://synthetic.new) but they are on a waitlist right now: \n\nhttps://preview.redd.it/z07ru8yswmhg1.png?width=1290&format=png&auto=webp&s=acc4869edc3ec157c15276af77a550023579479a",
              "score": 4,
              "created_utc": "2026-02-05 08:12:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3phisw",
                  "author": "Shep_Alderson",
                  "text": "Oops. I wonder if an invite code would get people past it. Iâ€™ll have to test later.",
                  "score": 1,
                  "created_utc": "2026-02-05 12:44:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3p1ptw",
          "author": "Dangerous-Relation-5",
          "text": "Amp code gives you $10/day in Opus credits if you turn on Ads. I think Minimax is still free in Opencode",
          "score": 1,
          "created_utc": "2026-02-05 10:40:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3wmqnz",
          "author": "TechieRathor",
          "text": "In my opinion the cheapest way would be to use Zen (https://opencode.ai/docs/zen/) service by OpenCode for trying different models.\n\nAre you really able to use all limit of CC Max subscription ? Frankly speaking I spent/wasted lot of time trying/using different models for coding a couple of months back but then I realised it'a waste of time there is not Model better than Antropic models when it comes to coding, plus I also saw that I am not able to use even 50% of CC Max (x20) so I downgraded it to CC Max (x5) which is sufficient enough plus I bought the Max subscription of GLM coding API which I use with Open Code as I don't want to get into the hassle of usage based pricing.  ",
          "score": 1,
          "created_utc": "2026-02-06 14:24:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zw633",
          "author": "krogel-web-solutions",
          "text": "Been happy with synthetic, but TPS varies quite a bit.",
          "score": 1,
          "created_utc": "2026-02-07 00:10:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3opssp",
          "author": "wallapola",
          "text": "Yeah, if the goal is purely cheapest, there are definitely options, but in my experience that usually comes with tradeoffs like throttling, random slowdowns, or timeouts. Iâ€™m currently using synthetic with opencode mostly because of the promo. At that price, it feels reasonable. What I like about it is that the devs are actually active and transparent. Theyâ€™re on discord, issues get acknowledged and you can see what theyâ€™re working on instead of guessing why a model suddenly feels worse.\n\nOnce the promo is over, Iâ€™ll probably reassess again, especially if other Kimi or GLM providers improve. But for now itâ€™s been a decent balance of cost and stability for my usage.  \n  \nIf anyone wants to try it, this is the link I used for the discounted offer:  \n[https://synthetic.new/?referral=4NNoPUXcb63ZYVK](https://synthetic.new/?referral=4NNoPUXcb63ZYVK)",
          "score": 1,
          "created_utc": "2026-02-05 08:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ockkv",
          "author": "exploriann",
          "text": "Definitely recommend synthetic.new, their service and price are really great. They have a discord community which is very active. You can start with a standard plan (20$).\n\nI have been using their service for 2 weeks, it's good.",
          "score": 0,
          "created_utc": "2026-02-05 06:43:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3oeefp",
          "author": "Bob5k",
          "text": "Synthetic if you care about stability and availability (the quota size). No other provider is close to their pro plan with basically unlimited model requests per 5h \n\nI'm using cc max 20 at work and synthetic for anything else. \nAlso first month can be cheaper with [reflink](https://synthetic.new/?referral=IDyp75aoQpW9YFt)",
          "score": 0,
          "created_utc": "2026-02-05 06:59:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nnxsb",
          "author": "stevilg",
          "text": "Nano is a pretty cheap way to get all of them open source models at $8/month ( I think this link  [https://nano-gpt.com/r/R7pbqiXX](https://nano-gpt.com/r/R7pbqiXX) will give a slight discount). The fact that the just measure the quantity of messages on the subscription and not the tokens means heavy context coding goes a long way. Its far from blazing fast, but it gets the job done.",
          "score": -1,
          "created_utc": "2026-02-05 03:43:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3opg3z",
          "author": "NoTomatillo1141",
          "text": "Would recommend [Synthetic.new](http://Synthetic.new)\n\nNo, the self's not going to give his referral link for it.",
          "score": 0,
          "created_utc": "2026-02-05 08:42:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qv3s16",
      "title": "CodeNomad v0.9.4 Released - Context manipulation, Session search, Themes and more",
      "subreddit": "opencodeCLI",
      "url": "https://v.redd.it/lw48vkltdchg1",
      "author": "Recent-Success-1520",
      "created_utc": "2026-02-03 20:48:39",
      "score": 23,
      "num_comments": 6,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qv3s16/codenomad_v094_released_context_manipulation/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3h5ah5",
          "author": "cschulze1977",
          "text": "Love this tool, have been using allot over the last few days. I was trying the new version and was getting a permission error on linux about a missing file \"resources/opencode-config/.gitignore\" (when selecting a folder in the app). Manually creating the file fixes it",
          "score": 2,
          "created_utc": "2026-02-04 04:39:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ixtgh",
              "author": "Recent-Success-1520",
              "text": "Thanks for using it.\nThe issue was due to a bug introduced in opencode v1.1.50. if you install v1.1.49 the problem should go away.\nI have raised a PR for opencode to fix it",
              "score": 1,
              "created_utc": "2026-02-04 13:23:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3p8vwb",
          "author": "Possible-Text8643",
          "text": "this uses electron right? i know the mac version of this tool, or at least  something very similar.\n\nMy main issue is how heavy the tool is, since its electron based, any plans to mitigate that in the future?",
          "score": 1,
          "created_utc": "2026-02-05 11:42:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pfe67",
              "author": "Recent-Success-1520",
              "text": "We have both Electron and Tauri apps.\nExcept for storage size memory usage is almost same",
              "score": 1,
              "created_utc": "2026-02-05 12:30:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3pyd55",
                  "author": "Possible-Text8643",
                  "text": "tried a lot to get superset to work on windows but i think the best option will be to use this",
                  "score": 1,
                  "created_utc": "2026-02-05 14:22:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qwjr76",
      "title": "AI Consumption Tracker 1.2.0: Windows app with zero config for opencode users",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/3rn1tpj83ohg1.png",
      "author": "Rygel_XV",
      "created_utc": "2026-02-05 12:17:34",
      "score": 23,
      "num_comments": 5,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qwjr76/ai_consumption_tracker_120_windows_app_with_zero/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3v5xiu",
          "author": "npittas",
          "text": "Great, but still you have a severe issue asking users to add base\\_url to the auth.json file. auth.json should not be used changed, add base\\_url in opencode.json instead, and add instructions on how to do so. Also not having codex and github-copilot tracking is a no go for me, but good job.",
          "score": 2,
          "created_utc": "2026-02-06 07:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vp3c6",
              "author": "Rygel_XV",
              "text": "Thank you for your feedback. I have just released [1.3.2](https://github.com/rygel/AIConsumptionTracker/releases/tag/v1.3.2) and it should fix the base\\_url issue. I am working on github-copilot next.",
              "score": 1,
              "created_utc": "2026-02-06 10:41:10",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3wgf67",
              "author": "Rygel_XV",
              "text": "I released [1.5.0](https://github.com/rygel/AIConsumptionTracker/releases/tag/v1.5.0) which supports Github Copilot.",
              "score": 1,
              "created_utc": "2026-02-06 13:50:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3qjyld",
          "author": "Several-System1535",
          "text": "Windows in 2026  \nlmao",
          "score": 3,
          "created_utc": "2026-02-05 16:09:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41x1he",
          "author": "Character_Cod8971",
          "text": "I can't get it to show me my OpenAI usage; please fix it",
          "score": 1,
          "created_utc": "2026-02-07 09:03:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qund3z",
      "title": "GLM 4.7 has terrible logic",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qund3z/glm_47_has_terrible_logic/",
      "author": "reficulgr",
      "created_utc": "2026-02-03 09:19:20",
      "score": 20,
      "num_comments": 14,
      "upvote_ratio": 0.92,
      "text": "People had been praising GLM as being on-par with Sonnet or Opus, but it is lagging very severely behind. I have been fighting it for almost an hour now to convince it that 2002 does not come after 2010.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qund3z/glm_47_has_terrible_logic/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o3bmk8k",
          "author": "bonnmos",
          "text": "ðŸ¤£ðŸ¤£ðŸ¤£ A  few days back it said \"This is too much for me ðŸ˜¥ , I can't handle\". Sometime it goes crazy with chinese characters..\nðŸ¤£ðŸ¤£",
          "score": 6,
          "created_utc": "2026-02-03 11:05:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3brtqi",
              "author": "aimericg",
              "text": "Does that for me too hehe. Sometimes it just switches everything to chinese, how my supposed to understand if it starts asking me question in chinese??",
              "score": 2,
              "created_utc": "2026-02-03 11:49:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3e4v9u",
              "author": "mdrahiem",
              "text": "I just came here to post if anyone sees the thinking is in Chinese and I realised that I am not alone ðŸ˜‚",
              "score": 2,
              "created_utc": "2026-02-03 19:06:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3bbia0",
          "author": "Heavy-Focus-1964",
          "text": "an hour? i think this reflects worse on you than GLM",
          "score": 9,
          "created_utc": "2026-02-03 09:21:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3citia",
          "author": "pbalIII",
          "text": "GLM-4.7 has a known gap on pure logic puzzles... someone ran lineage-bench on it recently and got ~60% accuracy on 8-node graphs where Qwen3-32B and OLMo-3 both hit 90%+. Even on trivial 4-node problems it tops out around 80%.\n\nThe benchmarks that made it look Sonnet-tier are heavily weighted toward code generation and agentic tool use, not abstract reasoning. For date comparisons and arithmetic, you're basically hitting its weakest surface.\n\naeroumbria's multi-model approach in the comments is the pragmatic fix. Route logic-heavy tasks to a model that doesn't fumble basic orderings.",
          "score": 3,
          "created_utc": "2026-02-03 14:35:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bebbx",
          "author": "jpcaparas",
          "text": "I've given up on it. slow af inference too",
          "score": 4,
          "created_utc": "2026-02-03 09:49:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3buum2",
          "author": "Ok-Regret-4013",
          "text": "I subscribed to GLM, but it was terrible. I would rather subscribe to Sonnet before experimenting.",
          "score": 2,
          "created_utc": "2026-02-03 12:12:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bh7lm",
          "author": "aeroumbria",
          "text": "I don't let any model go solo because they all have weaknesses. But three models with DIFFERENT weaknesses cross-checking each other is better than one master model going solo",
          "score": 1,
          "created_utc": "2026-02-03 10:16:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ej3ea",
          "author": "mintybadgerme",
          "text": "Kimi 2.5. Thank me later. :)",
          "score": 1,
          "created_utc": "2026-02-03 20:13:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gaqam",
          "author": "SynapticStreamer",
          "text": "I don't think anyone is making the claim that GLM is better, or on par with Sonnet or Opus. But GLM **will** fix things for me that both Sonnet and Opus miss. Some pretty obvious things that I have explicitly asked both Sonnet and Opus to fix, they've said they did, but they actually didn't. GLM was able to identify the problem, and fix it on the first time, and it **actually** fixed the problem, didn't just say \"hey, I fixed this.\"\n\nIt's a **different** tool, made in a **different** way and people expect it to perform well in the **same metrics** you use to classify another tool. It's just plain stupid.\n\nPeople want to throw \"do this thing\" at an LLM agent and expect it to extrapolate extremely detailed context and perform that task perfectly, and you just can't do that with GLM.\n\nIt's the difference between automatic (Anthropic) and manual (GLM) engines. People are going to fight ad infinitum about which is better. Some people prefer the convenience of the Anthropic model and will generally **never** be able to use a manual transmission because they learned on automatic, and it just is what it is.\n\nIf you're willing to take the time to learn how to respond appropriately to the model, and to learn what it needs and expects, GLM is a great model, most especially at the price point it's being offered for.",
          "score": 1,
          "created_utc": "2026-02-04 01:37:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xnyw0",
          "author": "Fklife45",
          "text": "yep slow and terrible logic, but whe i tried in claude code, it fel a bit smarter",
          "score": 1,
          "created_utc": "2026-02-06 17:24:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41snx5",
          "author": "Smart-Cap-2216",
          "text": "å› ä¸ºä»–å¤§å°ä»…ä»…åªæœ‰300b",
          "score": 1,
          "created_utc": "2026-02-07 08:21:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dvutq",
          "author": "fabricio3g",
          "text": "Skill issue. For the price, itâ€™s amazing. On Linux, for some reason, it runs fast, but on Windows itâ€™s too slow with opencode. I believe it was designed to be used with the Claude CLI.",
          "score": 1,
          "created_utc": "2026-02-03 18:26:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3e5glx",
              "author": "mdrahiem",
              "text": "I see the latency issue too and I am on mac. I dont think its just designed for Claude Code, If it was designed to work with Claude Code, why would they write OpenCode documentation on their website.",
              "score": 1,
              "created_utc": "2026-02-03 19:09:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}