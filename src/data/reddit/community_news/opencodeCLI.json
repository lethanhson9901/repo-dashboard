{
  "metadata": {
    "last_updated": "2026-02-21 02:42:34",
    "time_filter": "week",
    "subreddit": "opencodeCLI",
    "total_items": 20,
    "total_comments": 273,
    "file_size_bytes": 257512
  },
  "items": [
    {
      "id": "1r7bts8",
      "title": "GLM5 is free for a week",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/jyoyelzc63kg1.jpeg",
      "author": "jpcaparas",
      "created_utc": "2026-02-17 17:02:52",
      "score": 276,
      "num_comments": 26,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r7bts8/glm5_is_free_for_a_week/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5w8ljm",
          "author": "EchoesInBackpack",
          "text": "Would be funny if it works better than on zai paid plan",
          "score": 30,
          "created_utc": "2026-02-17 17:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w9x9j",
              "author": "shaonline",
              "text": "They typically do these type of deals with american providers (I think Kimi was on fireworks for example ?) so it probably will.",
              "score": 10,
              "created_utc": "2026-02-17 17:14:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wcgh7",
                  "author": "Spitfire1900",
                  "text": "ollama/glm-5:cloud has been too",
                  "score": 3,
                  "created_utc": "2026-02-17 17:27:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o69map0",
                  "author": "sudoer777_",
                  "text": "Kimi K2.5 Free is on Moonshot AI I believe, since I got rate limited a couple days ago and the error message was from that",
                  "score": 1,
                  "created_utc": "2026-02-19 16:42:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6elsm1",
                  "author": "BrightyBrainiac",
                  "text": "Are they doing these model by model? \nKimi 2.5 free isnâ€™t available anymore!",
                  "score": 1,
                  "created_utc": "2026-02-20 11:34:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5wnuso",
              "author": "Noob_l",
              "text": "As a max user of z.ai coding plan, I would not recommend it. It now has less usage than other coding plans like minimax and codex. \nAnd the constant errors and silent changes on users should be red flash to not prolong the subscription. Went from being of the best plans in terms of affordable pricing and usage to one of the worst.\n\n-- not even Claude code fills up your weekly usage in just 5 5hour windows. Z.ai does that to you though.\n\nAnd really a warning to all: it wil read your env file and will delete files even if you said explicitly not to in the same conversation as well as having that as fixed rules in the agents file\n\nDon't use glm 5",
              "score": 5,
              "created_utc": "2026-02-17 18:20:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wqno2",
                  "author": "EchoesInBackpack",
                  "text": "I tried glm5 at openzen, it it was decent and very fast. I thought that it can be my daily driver. Then I bought the sub on zai, and it barely works, not usable at all. I feel scummed.",
                  "score": 5,
                  "created_utc": "2026-02-17 18:32:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ws9l9",
                  "author": "jpcaparas",
                  "text": "same, horrendous drop off in speed after the second day. Why even pay for ultra if you're bogged down to lite inference.",
                  "score": 3,
                  "created_utc": "2026-02-17 18:40:18",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o5z8e86",
                  "author": "Jlocke98",
                  "text": "You can max out the kimi weekly plan at the 20usd tier with 2x 5hr windows",
                  "score": 2,
                  "created_utc": "2026-02-18 02:02:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o68s0mh",
              "author": "RedParaglider",
              "text": "I can't imagine that it woulnd't.  ZAI is fucked on inference infrastructure.",
              "score": 1,
              "created_utc": "2026-02-19 14:09:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5xc654",
          "author": "FaerunAtanvar",
          "text": "I tried to use ite and I instantly got \"all credits used\"",
          "score": 3,
          "created_utc": "2026-02-17 20:13:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yph42",
          "author": "Euphoric-Doughnut538",
          "text": "Canâ€™t get shit done with the limits",
          "score": 3,
          "created_utc": "2026-02-18 00:22:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69chrw",
              "author": "luc122c",
              "text": "The rate limits make it impossible to do anything ðŸ’€",
              "score": 2,
              "created_utc": "2026-02-19 15:54:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6226d2",
              "author": "InternalFarmer2650",
              "text": "Opencode has different limitations than zAI API\n\nI could get decent amount of work done with their free usage, seemingly more than some who pay for the zAI subscription",
              "score": 1,
              "created_utc": "2026-02-18 14:19:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wpnik",
          "author": "TurnUpThe4D3D3D3",
          "text": "Cerebras for free would be crazy",
          "score": 4,
          "created_utc": "2026-02-17 18:28:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zujw1",
              "author": "MrBlueAndWhite6_2",
              "text": "Any relation to this post or you are just saying?",
              "score": 2,
              "created_utc": "2026-02-18 04:10:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wxc6g",
          "author": "hatepoorpeople",
          "text": "I just cancelled my zai subscription. GLM-5 is completely unusable for me.",
          "score": 4,
          "created_utc": "2026-02-17 19:03:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o609map",
          "author": "Alternative-Spray176",
          "text": "This model is good. Unlike m2.5",
          "score": 1,
          "created_utc": "2026-02-18 05:59:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b5u56",
          "author": "HarjjotSinghh",
          "text": "this is a game-changer actually",
          "score": 1,
          "created_utc": "2026-02-19 21:08:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r97ruh",
      "title": "K2.5 is still the king for open-source models",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/t4infihnzhkg1.jpeg",
      "author": "jpcaparas",
      "created_utc": "2026-02-19 18:52:00",
      "score": 130,
      "num_comments": 38,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r97ruh/k25_is_still_the_king_for_opensource_models/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6aefjo",
          "author": "Electronic_Newt_8105",
          "text": "it's just so good.\n\ncrazy how you can get access to these awesome agentic coding models for free right now",
          "score": 24,
          "created_utc": "2026-02-19 18:55:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6aev7x",
              "author": "jpcaparas",
              "text": "I'm ever more so banking on the AI bubble popping this year. the tech remains obviously, but the valuations are just way out of proportion. \n\ni do feel for our gamer friends this 2026. shit's tough.",
              "score": 12,
              "created_utc": "2026-02-19 18:57:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6agmel",
                  "author": "metalman123",
                  "text": "Demand is near the physical capacity to serve models and you think the bubbles gonna pop?",
                  "score": 6,
                  "created_utc": "2026-02-19 19:05:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6d7uvh",
                  "author": "larowin",
                  "text": "gamers arenâ€™t buying H100s lol",
                  "score": 1,
                  "created_utc": "2026-02-20 04:19:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6bi60v",
              "author": "bad_detectiv3",
              "text": "How are you using agentic coding? Is it just through opencode cli mostly?",
              "score": 2,
              "created_utc": "2026-02-19 22:08:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6btoze",
                  "author": "jpcaparas",
                  "text": "these days, yes. just for the sheer flexibility of it. although I'd be lying if I said I wasn't using Codex or Claude. All of their strengths. Codex CLI mostly for long horizon tasks and Claude when I absolutely need to use Opus and Sonnet. \n\nOpenCode is the best all-arounder.",
                  "score": 3,
                  "created_utc": "2026-02-19 23:10:59",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6dgide",
              "author": "Wildnimal",
              "text": "Free how?",
              "score": 1,
              "created_utc": "2026-02-20 05:23:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6dya1w",
                  "author": "readeral",
                  "text": "I assume not free to run, but free as in open source and you can BYO hardware. Subscription-free probably a better description",
                  "score": 2,
                  "created_utc": "2026-02-20 08:00:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ajoi2",
          "author": "chicken-mc-nugget",
          "text": "It's available on AWS Bedrock, though. ",
          "score": 6,
          "created_utc": "2026-02-19 19:20:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6b1b34",
              "author": "touristtam",
              "text": "Ye but I doubt AWS being cheap compared to __ALL__ other offerings",
              "score": 1,
              "created_utc": "2026-02-19 20:46:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b43bq",
                  "author": "chicken-mc-nugget",
                  "text": "The US price is the same exact price they list on Zen. But they don't mention the price of cache reads on Bedrcok, so I guess they don't support it and that might be the limiting factor? ",
                  "score": 2,
                  "created_utc": "2026-02-19 20:59:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6b9aw6",
              "author": "alexeiz",
              "text": "Kimi K2.5 on Bedrock is very unreliable.  I don't know how they deployed this model, but if I try to use it from opencode, it just stops responding randomly.",
              "score": 1,
              "created_utc": "2026-02-19 21:24:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ah610",
          "author": "guillefix",
          "text": "What about GLM-5 or Minimax M2.5?",
          "score": 3,
          "created_utc": "2026-02-19 19:08:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6amuua",
              "author": "hey_ulrich",
              "text": "Kimi 2.5 is better than both in my tests.",
              "score": 12,
              "created_utc": "2026-02-19 19:35:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6bg1uv",
                  "author": "StardockEngineer",
                  "text": "Not in mine.  MM won",
                  "score": 2,
                  "created_utc": "2026-02-19 21:58:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6dis3c",
                  "author": "deadcoder0904",
                  "text": "Kimi is atleast better than both in writing. In coding, they are prolly close enough but writing is much better.",
                  "score": 1,
                  "created_utc": "2026-02-20 05:41:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6bic71",
                  "author": "bad_detectiv3",
                  "text": "TIL Kiki 2.5 is different from Minimax M2.5",
                  "score": 1,
                  "created_utc": "2026-02-19 22:09:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ahm42",
              "author": "jpcaparas",
              "text": "GLM-5 is.... I don't know. It's erratic for me in tool-calling and not to mention the Z.ai provider inference is slow AF.\n\nMiniMax 2.5 is a joke for subagent work. It does excel on UI though. wouldn't even put it in the same league as K2.5 for utilitarian work.",
              "score": 8,
              "created_utc": "2026-02-19 19:10:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6binoo",
                  "author": "bad_detectiv3",
                  "text": "What work do you consistently hand off to K2.5",
                  "score": 2,
                  "created_utc": "2026-02-19 22:11:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ba9qe",
              "author": "Daemonix00",
              "text": "I selfhost both K2.5 was better, GLM-5 was missing things (K2.5 is easier to host too, int4 base). both tested with sglang official cli settings.",
              "score": 1,
              "created_utc": "2026-02-19 21:29:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6bp64o",
              "author": "cutebluedragongirl",
              "text": "Kimi K2.5 is better",
              "score": 1,
              "created_utc": "2026-02-19 22:45:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6dpcdf",
                  "author": "guillefix",
                  "text": "and that is why...? I've tried it and it struggled to fix a simple positioning issue on react native... Which I ended up fixing with minimax in 1 shot.",
                  "score": 1,
                  "created_utc": "2026-02-20 06:38:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6bjb2h",
          "author": "Creepy_Reindeer2149",
          "text": "I did looked very closely and right now Fireworks.ai is best Kimi 2.5 provider for the money\n\n\nInsanely fast inference, faster than Gemini flashÂ ",
          "score": 5,
          "created_utc": "2026-02-19 22:14:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cgpor",
              "author": "elosoyogui",
              "text": "Have you tried Baseten? It is faster https://x.com/artificialanlys/status/2023641796430180615?s=46",
              "score": 2,
              "created_utc": "2026-02-20 01:26:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6cygij",
              "author": "forgotten_airbender",
              "text": "Can you guys tell me how fast is the inference?\nI want to use fireworks but already have the kimi for coding planÂ \n",
              "score": 1,
              "created_utc": "2026-02-20 03:16:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6dysup",
                  "author": "seaal",
                  "text": "https://openrouter.ai/moonshotai/kimi-k2.5 \n\nlike 40t/s",
                  "score": 1,
                  "created_utc": "2026-02-20 08:05:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6bhzty",
          "author": "bad_detectiv3",
          "text": "WTH isnâ€™t K2.5 free one? I was reading somewhere where this model isnâ€™t great and instead we should use GLM 5.0",
          "score": 2,
          "created_utc": "2026-02-19 22:07:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6af7u3",
          "author": "HarjjotSinghh",
          "text": "k2.5's got the whole ai empire by its collar.",
          "score": 1,
          "created_utc": "2026-02-19 18:59:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bmglw",
          "author": "Available_Hornet3538",
          "text": "How do you self-host? Kimi 2.5 such a large model",
          "score": 0,
          "created_utc": "2026-02-19 22:31:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bnjy4",
              "author": "jpcaparas",
              "text": "I don't self host, I use [Synthetic.new](http://Synthetic.new). They're an open-source provider (waitlist should be lifted soon), and I've done some mentions of them here:\n\n  \n\\- [https://blog.devgenius.io/the-definitive-guide-to-opencode-from-first-install-to-production-workflows-aae1e95855fb](https://blog.devgenius.io/the-definitive-guide-to-opencode-from-first-install-to-production-workflows-aae1e95855fb)\n\n\\- [https://jpcaparas.medium.com/stop-using-claudes-api-for-moltbot-and-opencode-52f8febd1137](https://jpcaparas.medium.com/stop-using-claudes-api-for-moltbot-and-opencode-52f8febd1137)\n\nThere's also Fireworks, NanoGPT and **obviously** OpenCode Zen.",
              "score": 1,
              "created_utc": "2026-02-19 22:37:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6cscm5",
                  "author": "Jlocke98",
                  "text": "Synthetic has been on wait-list for weeks",
                  "score": 1,
                  "created_utc": "2026-02-20 02:38:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6f1kz9",
                  "author": "philosophical_lens",
                  "text": "Do they have good latency? Iâ€™m currently using GLM / Z.AI subscription and itâ€™s pretty slow.",
                  "score": 1,
                  "created_utc": "2026-02-20 13:21:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6bm3ri",
          "author": "Available_Hornet3538",
          "text": "Yes Chinese models beat American models any day",
          "score": -3,
          "created_utc": "2026-02-19 22:29:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra1069",
      "title": "Kimi K2.5 is so good that infra is cooked, IMO best open-source model right now",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/ht568ac6lokg1.jpeg",
      "author": "Dilligentslave",
      "created_utc": "2026-02-20 17:04:53",
      "score": 98,
      "num_comments": 17,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1ra1069/kimi_k25_is_so_good_that_infra_is_cooked_imo_best/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6gd1fm",
          "author": "BlacksmithLittle7005",
          "text": "Agreed. I've tried GML5, kimi2.5, minimax, qwen, and kimi2.5 is the only obvious sonnet replacement at such a lower price",
          "score": 16,
          "created_utc": "2026-02-20 17:13:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hp21y",
              "author": "mintybadgerme",
              "text": "Seconded.Use it every day.",
              "score": 2,
              "created_utc": "2026-02-20 20:59:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gkffm",
          "author": "Far_Commercial3963",
          "text": "I do not get the hype, it sucks at agentic compared to GLM5 and always failed at tool calls on any serious usage in my experience (in both Opencode and Kilo Code)\n\nIt may be good for chat but I don't think  its usable much in agentic\n\nAlso Kimi k2.5 is not open source, it is open \\*weight\\*. ",
          "score": 12,
          "created_utc": "2026-02-20 17:48:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6guhrz",
              "author": "AGiganticClock",
              "text": "It's good! So is minimax 2.5",
              "score": 2,
              "created_utc": "2026-02-20 18:33:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6guaaq",
              "author": "guillefix",
              "text": "same here, I was trying to fix a positioning issue in my react native app and after 10 tries I had to switch to minimax m2.5 which solved it in one shot",
              "score": 1,
              "created_utc": "2026-02-20 18:32:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gwtp9",
          "author": "throwaway12012024",
          "text": "GLM5 replaces Opus. K2.5 replaces Sonnet.",
          "score": 8,
          "created_utc": "2026-02-20 18:44:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hbpwk",
              "author": "pisangcoklatcheese",
              "text": "Main problem for GLM5 is speeeeeed.",
              "score": 3,
              "created_utc": "2026-02-20 19:54:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6j8g1j",
                  "author": "BrightyBrainiac",
                  "text": "Yes. This.\nItâ€™s like codex.",
                  "score": 1,
                  "created_utc": "2026-02-21 02:09:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6gz7lj",
              "author": "FaerunAtanvar",
              "text": "I am not too familiar with Anthropic s model. Does this mean you'd suggest to use glm5 for planning and k2.5 for building?",
              "score": 4,
              "created_utc": "2026-02-20 18:54:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6hue0a",
              "author": "Loud_Buy_9297",
              "text": "K2.5 plays builder nicely with GPT 5.3 Codex as planner for Python projects, GLM 5 and Minimax 2.5 not so much. I found K2.5 even caught and cleaned up some Codex html/js bugs. Plan to openspec for kimi builder then have codex provide each builder prompt step by step. GLM 5 introduced more bugs so I build with kimi free on zen the last week, great workflow and cheap while codex sub lasts. Horses for courses!",
              "score": 1,
              "created_utc": "2026-02-20 21:26:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6i0l50",
              "author": "Western_Objective209",
              "text": "I keep seeing people say this then I try the models and they are derpy AF",
              "score": 0,
              "created_utc": "2026-02-20 21:57:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gdiug",
          "author": "Michaeli_Starky",
          "text": "It's not open source.",
          "score": 0,
          "created_utc": "2026-02-20 17:16:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gmwrs",
              "author": "digitalfreshair",
              "text": "open weights",
              "score": 6,
              "created_utc": "2026-02-20 17:59:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gwd0r",
                  "author": "Michaeli_Starky",
                  "text": "Yes",
                  "score": 2,
                  "created_utc": "2026-02-20 18:41:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gee5v",
          "author": "Bob5k",
          "text": "well, with 20tps it's close to be unusable at all for anything serious. Even if it's better code quality wise, it'll still lose when it comes to quality of a code > verify > fix > verify > merge loop that can be achieved with faster models. And this loop should happen anyway no matter the model we're using because AI will miss things even if it's opus 4.6 super ultra high genius mode.\n\nedit: ofc unless you're just yolo-oneshotting next awesome saas, but if you do then please sign up for [faultry.com](http://faultry.com) waitlist as it'll be highly useful.",
          "score": 0,
          "created_utc": "2026-02-20 17:20:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ikgbz",
          "author": "lucidbox",
          "text": "I personally love Kimi. For like $300 a year you get access to their CLI, agent swarms, and a clawbot. Not to mention the ability to just give it an image and have a code for you which now Gemini does pretty well with 3.0. I basically use opus for planning, and for workflows that will only hit single files. Iâ€™m able to build lots of code with agents forms in Kimi, which really builds the scaffolding for my projects from a code perspective, and then I throw in Codex to the heavy lifting, and I end with the Gemini CLI using Stich to polish off the UI. Itâ€™s been a pretty good workflow so far.",
          "score": 0,
          "created_utc": "2026-02-20 23:45:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gj56b",
          "author": "SvenVargHimmel",
          "text": "It's a very eager model and needs strong guidance. Poor in plan mode, great in build. I have tested it yet with Skills and oc bun toolsÂ ",
          "score": -1,
          "created_utc": "2026-02-20 17:42:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5vv6g",
      "title": "Minimax M2.5 is not worth the hype compared to Kimi 2.5 and GLM 5",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r5vv6g/minimax_m25_is_not_worth_the_hype_compared_to/",
      "author": "Resident-Ad-5419",
      "created_utc": "2026-02-16 01:14:20",
      "score": 90,
      "num_comments": 55,
      "upvote_ratio": 0.93,
      "text": "I used opencode with exa; to test the latest GLM 5, Kimi 2.5 and Minimax M2.5, along with Codex 5.3 and Opus 4.6 (in its own cli) to understand how would they work on my prompt. And the results were very disappointing.\n\nDespite all these posts, videos and benchmarks stating how awesome minimax m2.5 is, it failed my test horribly given the same environment and prompt, that the others easily passed.\n\nMinimax kept hallucinating various solutions and situations that didn't make any sense. It didn't properly search online or utilized the available documentation properly. So, I wonder how all those benchmarks claiming minimax as some opus alternative actually made their benchmark.\n\nI saw a few other real benchmarks where Minimax M2.5 actually was way below Haiku 4.5 while GLM 5 and Kimi went above Sonnet 4.5; personally it felt like that as well. So at the increased price points from all these providers, its very interesting. Though neither are on opus or codex level.\n\nI did not test the same prompt with gemini, or couldn't test it, to be more precise due to circumstances. But I have a feeling Gemini 3 Pro would be similar to Kimi and GLM 5, maybe just a bit higher.\n\nWhat is your experience with Minimax compared to GLM and Kimi?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r5vv6g/minimax_m25_is_not_worth_the_hype_compared_to/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5lx2xc",
          "author": "Specialist-Yard3699",
          "text": "GLM still has its old problem - horrible speed. \nKimi25 - low limits. In my opinion, this model is not so good in terms of price-to-quality ratio.\nMinimax25 - fast and a good â€œplan executorâ€/â€œcode searcherâ€. I have worked a lot with Minimax21, and the 25th version is much better and has a lower hallucination rate.",
          "score": 18,
          "created_utc": "2026-02-16 01:26:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5m1i9x",
              "author": "lopydark",
              "text": "where minimax 25",
              "score": 1,
              "created_utc": "2026-02-16 01:55:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5m9h0y",
                  "author": "Specialist-Yard3699",
                  "text": "Official minimax code plan + oh-my-opencode plugin + detailed plan from reasoning model (Glm/gemini3)",
                  "score": 3,
                  "created_utc": "2026-02-16 02:46:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5lyi2e",
              "author": "Resident-Ad-5419",
              "text": "It hallucinated a lot on my prompt, the same one given to opus, codex, kimi and glm and those four worked well for that, while minimax failed horribly. It kept inventing stuff that doesnt even exist. I tested many times just to be sure I wasn't the one hallucinating.",
              "score": -1,
              "created_utc": "2026-02-16 01:35:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5m59di",
          "author": "tripleshielded",
          "text": "Yes, minimax still cant do any difficult work on its own. But m2.5 seems a bit better than m2.1, its a good upgrade.",
          "score": 8,
          "created_utc": "2026-02-16 02:19:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ngr1l",
          "author": "kshnkvn",
          "text": "Skill issue. I'm not joking. Minimax is beast for use as a subagent, as an executor/researcher/etc.\nYou can't use it straight as opus/got and think that it will act as good, at least because it's small model, really small, it knows much less then competitors so you need to provide proper information and context.",
          "score": 6,
          "created_utc": "2026-02-16 08:28:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nvsu5",
              "author": "Resident-Ad-5419",
              "text": "I don't disagree with you on this point, this is worth a shot! Thank you!",
              "score": 2,
              "created_utc": "2026-02-16 10:50:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5v4vtp",
              "author": "Resident-Ad-5419",
              "text": "After your reply I went back and ran a test with Codex as the main agent and GLM, Kimi, Minimax and Codex Spark as sub agent. \n\nCodex + Codex Spark did the best, even better than codex solo.  \nKimi and GLM afterwards.  \nMinimax still couldn't beat even after instructions and helping hands from codex.\n\nI would like to say it's my skill issue, that I am not skilled enough to handle minimax like you do.",
              "score": 1,
              "created_utc": "2026-02-17 13:45:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5v8nai",
                  "author": "kshnkvn",
                  "text": "Idk what exactly you want me to say, because I literally know nothing about your workflow, stack, tasks.  \nIf M2.5 doesn't suit your needs it's fine, because it's not a general-purpose model, it has it's own limitations, cons and pros. It may be either good or bad depends on your tasks and how you threat it.  ",
                  "score": 2,
                  "created_utc": "2026-02-17 14:05:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6hu0ty",
              "author": "Ill_Cantaloupe405",
              "text": "Do you think using MiniMax 2.5 for easy tasks works? For example currently for me I plan on using mini max 2.5 to warm meta ads accounts for me. It will go on 50 high trust websites just scrolling the web page and clicking buttons to increase trust. Is it capable of warming Facebook by liking and scroll through Facebook reels, and also browsing the web to increase trust score?",
              "score": 1,
              "created_utc": "2026-02-20 21:24:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hwgqv",
                  "author": "kshnkvn",
                  "text": "Oh, well, I have zero idea about all those SEO things and in general I don't have a lot of experience to automate things with LLM so can't say anything about it.",
                  "score": 1,
                  "created_utc": "2026-02-20 21:36:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5m5ovy",
          "author": "segmond",
          "text": "they all have their strengths, i run all of them locally.   minimax2.5 was able to solve a problem that I couldn't get GLM5 and KimiK2.5 to solve after a few prompts, minimax solved it with the same prompt in one go, and generated almost 4,000 lines of perfect code.  The interesting thing is I couldn't have started a solution to the problem without Kimi K2.5 it has very unique capability that many other models don't.  IMO, they all have strengths and you have to know when to use one for another.  I like Minimax not because it solved my problem but I can also run it much faster.  ",
          "score": 5,
          "created_utc": "2026-02-16 02:21:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5oc5nq",
              "author": "Resident-Ad-5419",
              "text": "Absolutely! Any model that solves your problem is the best model, regardless of whatever anyone says otherwise.",
              "score": 2,
              "created_utc": "2026-02-16 12:59:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o64qlgo",
              "author": "FinancialMoney6969",
              "text": "you must have a beast of a machine.....",
              "score": 1,
              "created_utc": "2026-02-18 21:42:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o65l46q",
                  "author": "segmond",
                  "text": "yeah, for the average home user.  but not for a local LLM nuts.  a few 3090s that I bought used from FB marketplace on a 512gb machine.   My original MB was $100 from aliexpress.  Used dual xeon CPUs for $5 each and $600 for the ram from ebay late last year.   It's slow, qwen3.5 runs at 12tk/sec, KimiK2.5 at about 7-8tk/sec  Minimax at about 18tk/sec so not too bad for local.",
                  "score": 1,
                  "created_utc": "2026-02-19 00:17:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5mvemk",
          "author": "Lpaydat",
          "text": "GLM5 is really good. Really really good. Just quite slow sometimes.",
          "score": 3,
          "created_utc": "2026-02-16 05:20:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mpois",
          "author": "lundrog",
          "text": "K2.5 is very good on boardwell hardware , with the nvidia nvidia/K.imi-K2.5-NVFP4; assuming you find a provider hosing it. ( allows less memory usage for same performance )",
          "score": 3,
          "created_utc": "2026-02-16 04:37:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mwauu",
          "author": "Medical_Farm6787",
          "text": "To me since Iâ€™m using the free usage on opencode, probably due to the last GLM4.7 infinite thinking loop incident I just completely stopped using it.\n\nMy current workflow can be Kimi K2.5 for any web search related or codebase exploration â€”> switch to Minimax M2.5 for actual coding practices.\n\nBecause Minimax M2.5 did actually fall very short in terms of searching benchmarks you can look that up on their official benchmark results, but almost on par with opus and to me it really does feel like it, the thoughts are more structured than Kimi K2.5 when it comes to coding implementation, Kimi sometimes forgets the rules that I set in AGENTS.md as context window grows, but yet to have any issue when it comes to Minimax M2.5\n\nNot to say that Minimax M2.5 size is way more smaller than GLM so it feels well in my M3 ultra 256gb unified memory at Q6",
          "score": 3,
          "created_utc": "2026-02-16 05:27:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m2d16",
          "author": "czumaa",
          "text": "i think it's just me but now i think Minimax m2.5 just PLAIN LIE to all of us. Is not even close to the other models and have some weird allucinations. I test this models on coding. Just broke the entire project, don't respect the rules file of kilo code, re-do code as \"his\" way without check. is a disaster. i'm with GLM5 and kimi2.5 now. don't know which rely is the best but i think glm5 is a little best sometimes.",
          "score": 2,
          "created_utc": "2026-02-16 02:00:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d0034",
              "author": "True_Requirement_891",
              "text": "Ask it to teach you git.",
              "score": 1,
              "created_utc": "2026-02-20 03:26:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5mnpy1",
          "author": "Xhatz",
          "text": "It's great for it's size and fast, but it's truly NOT as good as they say, clearly. For me it feels like it's just m2.1 but with even less coherence sadly, hallucinations are too high (I can say something and a few messages afterwards it says something else). It also feels more \"lazy\" in a way... My theory is that it's only good at very specific things and just completely bad at everything else.",
          "score": 2,
          "created_utc": "2026-02-16 04:23:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mec9t",
          "author": "DistinctWay9169",
          "text": "MIniMax is pure hype. Tried it and nope, thanks. Broke my codebase many times to allow it to touch it again.",
          "score": 2,
          "created_utc": "2026-02-16 03:18:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n3sdd",
          "author": "chiroro_jr",
          "text": "Thought I was the only one. I'm still using Kimi. The limits and speed are good enough for me",
          "score": 1,
          "created_utc": "2026-02-16 06:30:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nbah4",
          "author": "SphaeroX",
          "text": "I also think that Kimi 2.5 is still unbeatable! But we'll see, the new DeepSeek should be ready soon.",
          "score": 1,
          "created_utc": "2026-02-16 07:37:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d09jo",
              "author": "True_Requirement_891",
              "text": "Yk the craziest thing is, in my experience specially for large refactors and long running tasks, k2-thinking outperformed k2.5 almost every time... k2.5 just marks shit complete without actually doing it, skips things, and it halucinates quite a lot. K2-thinking had less of these.",
              "score": 2,
              "created_utc": "2026-02-20 03:28:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nd0vv",
          "author": "ciprianveg",
          "text": "maybe try with a lower temperature like 0.7?",
          "score": 1,
          "created_utc": "2026-02-16 07:53:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nszvi",
          "author": "xmnstr",
          "text": "I found the same. Cleary worse compared to GLM 5 and K2.5. I don't really understand why companies do this when it's so obvious to anyone who tries the model out that it simply does not hold up.",
          "score": 1,
          "created_utc": "2026-02-16 10:24:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nt77k",
          "author": "tricky-oooooo",
          "text": "Well, it's much smaller than both Kimi 2.5 and GLM5. What did you expect?",
          "score": 1,
          "created_utc": "2026-02-16 10:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5numcg",
          "author": "rudingshain",
          "text": "I use it Not for Coding but with opencode in textrelevant task and it works weil",
          "score": 1,
          "created_utc": "2026-02-16 10:40:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o81rh",
          "author": "odrakcir",
          "text": "not sure what to say, I've been using it for the last couple of days to write and fix a bunch of unit tests (react native) and it's been great. I've used it like this: plan mode -> openspec (explore + ff + manual review + implement + verify + archive).",
          "score": 1,
          "created_utc": "2026-02-16 12:31:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5obxcd",
              "author": "Resident-Ad-5419",
              "text": "If you use codex/opus as a driver; then haiku, minimax or any other smaller model can do wonders. Problem is the way they market it as if they are better than codex/opus in benchmarks, which is wrong.",
              "score": 2,
              "created_utc": "2026-02-16 12:58:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5olh5s",
                  "author": "odrakcir",
                  "text": "that is true, but to be honest, that claim makes part of the business haha. Now, what we can't deny is that OS models are getting closer.",
                  "score": 1,
                  "created_utc": "2026-02-16 13:54:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5o90ke",
          "author": "mintybadgerme",
          "text": "In my experience Kimi is definitely the best of the trio. GLM has problems as does Minimax.",
          "score": 1,
          "created_utc": "2026-02-16 12:38:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ooy4q",
          "author": "c0nfluks",
          "text": "I had the exact same experience. Kimi is way better. The benchmarks are cooked. Chinese AI companies are cheating on the exam, basically.",
          "score": 1,
          "created_utc": "2026-02-16 14:13:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5p5u06",
          "author": "l_eo_",
          "text": "MiniMax (both M2.1 and M2.5) for me is great for stable & efficient pipelines. \n\nSpawn researches, assess, return data, etc etc etc. \n\nIt's for me the perfect model for programmatic pipelines and is so far dominating its niche. \n\nI tried many other models and providers, but haven't found anything that could deliver this quality & stability at this cost level. \n\nIf anybody knows of anything that works better for these kind of use cases, please let me know!",
          "score": 1,
          "created_utc": "2026-02-16 15:40:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pv4dk",
          "author": "Alternative-Spray176",
          "text": "M2.5 doesn't follow instructions at all. I asked it to copy the plan it generated to a new file. It started implementing the plan. Retried but no use. I tried it for a few tasks, it doesn't want to listen to the user instructions. That model is hard to use like Google gemini models.",
          "score": 1,
          "created_utc": "2026-02-16 17:37:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ufe3p",
          "author": "Practical_Arm_645",
          "text": "I do have the same feeling. Minimax is almost useless for harder task, but only benefit is the speed. Gemini3pro is also terrible, even worse than the flash. The flash is quite reasonable, similar to sonnet 4.5 level",
          "score": 1,
          "created_utc": "2026-02-17 10:43:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v8bb9",
          "author": "Final-Rush759",
          "text": "It's not magic.  It's a cheap model, works fine for what it is.  It benchmark well with agent tests. It is not at the same level as big models.  It doesn't store as much knowledge as bigger models.",
          "score": 1,
          "created_utc": "2026-02-17 14:03:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w5rdo",
          "author": "StardockEngineer",
          "text": "HARD disagree.\n\nMy results in my tests - MM2.5 frequently implements big large PRDs with efficiency and precision.\n\nKimi often gives a result missing a ton of requirements.\n\nGLM5 for some reason uses a ton of tokens, costing far, far more than MM2.5",
          "score": 1,
          "created_utc": "2026-02-17 16:54:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wugw2",
          "author": "WolfpackBP",
          "text": "It's so good at agentic stuff though! And the price point is so good. \n\nI have found it to be very impressive\n\nKimi was slow and not as good with the agentic stuff at a higher price point \n\nHaven't tried GLM yet",
          "score": 1,
          "created_utc": "2026-02-17 18:50:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yk5ut",
          "author": "Dayclone",
          "text": "It's free right now through Ollama Cloud for a short period but seems to do ok for me. Has it's issues but hey it's free. Can't complain.",
          "score": 1,
          "created_utc": "2026-02-17 23:52:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zuroy",
          "author": "Teslaaforever",
          "text": "I have the opposite experience M2.5 solved a very complicated problem I have while GLM5 and Kimi2.5 actually gave me not working codes and they are outdated even when they searched the web.",
          "score": 1,
          "created_utc": "2026-02-18 04:12:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62dglw",
          "author": "HarjjotSinghh",
          "text": "here's a lively, supportive reply to match:",
          "score": 1,
          "created_utc": "2026-02-18 15:15:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ihqnx",
          "author": "Due-Project-7507",
          "text": "I agree, MiniMax M2.5 is a \"small\" model with only 10B active parameters. For the size, it is still very good, it \"only\" needs 192 VRAM with the MoE quantized as NvFP4 or AWQ. GLM-5/l and Kimi K2.5 need around four times more VRAM. I use MiniMax M2.5 together with Copilot GPT-5.2. GPT-5.2 does the planning and fixes the broken implementation from MiniMax. The combination is usually faster than doing every with Copilot GPT-5.2 and saves Copilot tokens.",
          "score": 1,
          "created_utc": "2026-02-20 23:29:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nc971",
          "author": "Bob5k",
          "text": "also have in mind that minimax has released a guaranteed 100tps m2.5 instances / plans, which also fall under the 10% [reflink promo](https://platform.minimax.io/subscribe/coding-plan?code=HO46LCwAJ5&source=link)   \nfaster iteration means more work done even if quality is 5% lower.",
          "score": -1,
          "created_utc": "2026-02-16 07:46:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9nfyf",
      "title": "How would Opencode survive in this era?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r9nfyf/how_would_opencode_survive_in_this_era/",
      "author": "tksuns12",
      "created_utc": "2026-02-20 06:01:01",
      "score": 61,
      "num_comments": 130,
      "upvote_ratio": 0.84,
      "text": "Claude Code is prohibited and Antigravity is prohibited too for opencode.\n\nBasically, the only subscription available for mass usage from SOTA model makers is OpenAI.\n\nI'm using Open Code a lot but now that I see the situations, I don't know why I use Open Code now.\n\nHow do you guys deal with this situation?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r9nfyf/how_would_opencode_survive_in_this_era/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o6dlpp1",
          "author": "MrNantir",
          "text": "Github Copilot is officially supported and allowed by Microsoft.\nThrough that you can use the Anthropic models.\n\nI use it everyday with Copilot and OpenAI and never want to go back to Claude Code.",
          "score": 69,
          "created_utc": "2026-02-20 06:06:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dlw3f",
              "author": "oronbz",
              "text": "This is the way",
              "score": 8,
              "created_utc": "2026-02-20 06:08:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6do6jd",
              "author": "Charming_Support726",
              "text": "For this reason a got me a GHCP subscription again. Just for using Opus. \n\nOpus is NOT the best model out there. It is brilliant in communications and task understanding, but the resulting code is often inferior in comparision when both models are prompted well. But mostly Opus get shit done. Quick. That's an advantage. Not more.\n\nAnyways, \"Real Developers (TM)\" don't do sloppy one-shots. I neither like the attitude of Anthropic nor the vibe of their products and some of their followers. So GHCP provides me the dose of Opus I need from time to time, without AGY or CC.\n\nYes, changing style of work is a bit annoying, but with DCP and a bit of organizing and structuring (subagents) you can get round this. ",
              "score": 15,
              "created_utc": "2026-02-20 06:27:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6i2who",
                  "author": "vienna_city_skater",
                  "text": "This. Codex often corrects the code Opus writes. But Opus is so incredibly driven, it just tries until it accomplishes something, Codex on the other hand gets needy when itâ€™s stuck, itâ€™s more implementation. I love it as a team, + Gemini Flash for subagentic stuff and small simple tasks. GHCP is just amazing for daily coding work.",
                  "score": 2,
                  "created_utc": "2026-02-20 22:08:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6dpzxl",
                  "author": "tksuns12",
                  "text": "Then how do you use the models depending on the tasks? I agree that Opus is not always best.",
                  "score": -1,
                  "created_utc": "2026-02-20 06:44:10",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6dm4cb",
              "author": "tksuns12",
              "text": "Yeah I'm using it like that too but request based quota doesn't suit my interactive usage. Copilot's limited context window size is bothering too.",
              "score": 7,
              "created_utc": "2026-02-20 06:10:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6e3k73",
                  "author": "CardiologistStock685",
                  "text": "aws bedrock, openrouter work just ok on OC.",
                  "score": 3,
                  "created_utc": "2026-02-20 08:50:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6i4qzg",
                  "author": "vienna_city_skater",
                  "text": "The limited context window is less bad than I thought in the first place. At least Opus tends to use subagents heavily (I default to gemini flash here) and if you go into compaction is usually just continues to work. However, for interactive usage with the expensive models itâ€™s indeed suboptimal. That said, I started to use gpt5 mini with openclaw tuned by Opus to optimize bang for the buck. Oftentimes a smaller cheaper model is good enough if you give it well specified tasks.",
                  "score": 1,
                  "created_utc": "2026-02-20 22:18:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6e10oy",
              "author": "toadi",
              "text": "Same here and in 1 week to 2 weeks I'm through the premium requests. But it is not to bad I think my spending is currently about 200 dollars per month. which is close to a subscription.\n\nI do only use opus for big specs. Smaller specs GLM/Kimi/Sonnet. I create very small incremental tasks for coding so simple models are good enough like qwen/haiku and sometime I even use kimi as it is cheap.\n\nWhile Opus/sonnet are good. If your flow is dialed in the opensource models are good enough for me.",
              "score": 3,
              "created_utc": "2026-02-20 08:25:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6dmsku",
              "author": "klapaucius59",
              "text": "more context would be nice tho. I am curious what is holding them back. Isnt is simple to increase it and make 5x 6x whatever if it's costly.",
              "score": 5,
              "created_utc": "2026-02-20 06:15:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6dohsn",
                  "author": "MrNantir",
                  "text": "Definitely would be nice with larger context.\nHowever at least for me, I've found ways to compensate, making very detailed plans and the splitting the work to individual agents, with a limited and precise work scope.",
                  "score": 5,
                  "created_utc": "2026-02-20 06:30:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6dn5ps",
                  "author": "klapaucius59",
                  "text": "Definitely better product than 30x faster opus",
                  "score": 2,
                  "created_utc": "2026-02-20 06:19:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6do21r",
              "author": "haininhhoang94",
              "text": "I have seen people being ban when using subagents:(, tbh it scared me a little bit",
              "score": 1,
              "created_utc": "2026-02-20 06:26:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6dob6p",
                  "author": "MrNantir",
                  "text": "Seems odd if they have.\nI use copilot heavily each day, with subagents.",
                  "score": 4,
                  "created_utc": "2026-02-20 06:29:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6dx8aw",
              "author": "amunozo1",
              "text": "The problem is that those model are missing context, but it is quite good anyway.",
              "score": 1,
              "created_utc": "2026-02-20 07:50:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6eqzda",
              "author": "robberviet",
              "text": "Correct. Atm GH copilot provide best values. I think moatly because they (ms) have no model of their own, and their tools still quite bad. \nMight change in future after M/A with OpenAI though.",
              "score": 1,
              "created_utc": "2026-02-20 12:13:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ga79b",
              "author": "Dilligentslave",
              "text": "copilot kinda became the safe middle ground since it aggregates models but honestly opencode survives only if it offers something unique like workflow control privacy or customization otherwise people will just default to whatever integrates easiest into their daily dev flow",
              "score": 1,
              "created_utc": "2026-02-20 17:00:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6gd57i",
              "author": "Character_Cod8971",
              "text": "Does it work well? Read a lot about high premium request usage if you use Copilot through OpenCode.",
              "score": 1,
              "created_utc": "2026-02-20 17:14:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6glfh6",
              "author": "LemurZA",
              "text": "Oh shit, this is new to me",
              "score": 1,
              "created_utc": "2026-02-20 17:52:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6dlk5a",
          "author": "aeroumbria",
          "text": "How does this make Opencode look bad? If anything, I made Anthropic even less appealing than ever...",
          "score": 76,
          "created_utc": "2026-02-20 06:05:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dmccm",
              "author": "tksuns12",
              "text": "But currently Opus 4.6 is known to be the best model and Claude Code is the cheapest way, if you use the model a lot, to use Opus 4.6 for now.",
              "score": -30,
              "created_utc": "2026-02-20 06:12:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6dmk2b",
                  "author": "aeroumbria",
                  "text": "It's been out for like barely a week... How are we even going to know if it is the best or not without people actually using them a bunch? Benchmarks always claim whatever they want... They only thing they can claim now is that Opus is the PRICE leader of the industry...",
                  "score": 21,
                  "created_utc": "2026-02-20 06:13:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6do2j9",
                  "author": "omicron8",
                  "text": "Are you living under a rock? Gemini 3.1 Pro has been out for almost a day. That is the best now. /s Anyhow don't stress too much. Things move fast and competition is good. There are thousands of models available on opencode. If you want to use anthropic models just use claude code. It's not bad. Opencode will survive because you don't need the best model for everything, and best is debatable and changes every day. ",
                  "score": 13,
                  "created_utc": "2026-02-20 06:27:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6duemf",
                  "author": "oulu2006",
                  "text": "Ah no\n\nGPT5.3-codex is as good as.\n\nI use opus 4.6 just via API for planing and review, main workhorse is GLM5 and GPT5.3 \n\nChinese models will be as good as US in 6 months if not sooner - Anthropic is over theyâ€™re a dinosaur",
                  "score": 10,
                  "created_utc": "2026-02-20 07:24:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ejz5f",
                  "author": "georgiarsov",
                  "text": "Opus 4.6 is ADVERTISED as the best model. This is a huge thing to consider. Big labs have enormous resources that they pour very wisely in benchmarks and influencers a.k.a. the general personâ€™s source of truth for â€œwhich the best model isâ€. They are limiting your scope to all the other top models available which are products of much smaller labs or chinese ones. Just scroll this page to see what i am talking about https://openrouter.ai/models ðŸ˜„ The only real benchmark that one should follow is to try the models for himself and compare their performance on a given task. Thatâ€™s how i found out that Kimi was able to solve a problem i had from the first try and opus couldnâ€™t do it in 4 separate sessions and $20+ in api costs. Try everything and donâ€™t follow the trends blindly",
                  "score": 4,
                  "created_utc": "2026-02-20 11:19:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6evsdw",
                  "author": "rayfin",
                  "text": "You're not wrong here, but sometimes you have to say fuck you to the beast.",
                  "score": 1,
                  "created_utc": "2026-02-20 12:46:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6fkpug",
                  "author": "SynapticStreamer",
                  "text": "Opus has been available for like... A week. Calm your tits with the claims that it's the best thing since sliced bread.",
                  "score": 1,
                  "created_utc": "2026-02-20 15:02:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6j733w",
                  "author": "Ok_Rough5794",
                  "text": "Kimi/OpenCode is good enough for DHH...",
                  "score": 1,
                  "created_utc": "2026-02-21 02:01:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6dtud6",
                  "author": "RainScum6677",
                  "text": "This is simply false, benchmarks and users both indicating so.",
                  "score": 1,
                  "created_utc": "2026-02-20 07:19:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6dv3l7",
          "author": "_w0n",
          "text": "Please do not forget that OpenCode is extremely useful for local LLMs. It also has high value for tinkerers and for professionals at work who are only allowed to use open-source and local tools. It is not always about SOTA models.",
          "score": 28,
          "created_utc": "2026-02-20 07:30:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e7gk3",
              "author": "franz_see",
              "text": "Curious, whatâ€™s your setup - model, hardware and what tps do you get? Thanks!",
              "score": 4,
              "created_utc": "2026-02-20 09:27:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fd5pg",
                  "author": "_w0n",
                  "text": "I run an Nvidia A6000 (48 GB) + an Nvidia RTX 3090 Ti (24 GB) with 64 GB DDR4 RAM.  \nI load the full ~69 GB model across both GPUs using llama.cpp with Q6 quantization (Q6_xx / Q6_X). The model is unslothâ€™s Qwenâ€‘3 Coder Next.  \nContext length: 128,000 tokens. Measured throughput: ~80 tokens/sec.",
                  "score": 6,
                  "created_utc": "2026-02-20 14:23:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6eyrjo",
              "author": "sig_kill",
              "text": "I have been having an absolute blast with miniMax, and kimi 2.5. Same with qwen3-coder-next.\n\nTheyâ€™re not the best models, but theyâ€™re fast and good. GLM-5 has been KILLING it for me (though Iâ€™ve been using it through Zen). I canâ€™t afford the GPJ and ram to offload that model though, even a decent quantized version is ~250 gb",
              "score": 2,
              "created_utc": "2026-02-20 13:04:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gemb2",
                  "author": "Time_Feature_8465",
                  "text": "yes minimax and kimi are amazing (and free) to the point that i have stopped coding.  \nWith 16GB VRAM I could get some local llm result out of a quantized GLM4.7, but not that good and still slow and limited by context size.",
                  "score": 1,
                  "created_utc": "2026-02-20 17:21:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6dl93w",
          "author": "meronggg",
          "text": "Isnt the whole point of opencode is that its open harness, use it with whatever you want.",
          "score": 20,
          "created_utc": "2026-02-20 06:02:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ecfgl",
              "author": "trypnosis",
              "text": "Opencode will work with any sub. The issue if the sub will penalise you for it.",
              "score": 5,
              "created_utc": "2026-02-20 10:13:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fu1hv",
                  "author": "xmnstr",
                  "text": "Well, the only two who do (Google and Anthropic) can be accessed via Github Copilot Pro, so there isn't really any limitation.",
                  "score": 1,
                  "created_utc": "2026-02-20 15:46:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6foles",
                  "author": "aries1980",
                  "text": "Then sub will decline and other model operators will increase their market share.",
                  "score": 1,
                  "created_utc": "2026-02-20 15:21:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6dm7qb",
              "author": "tksuns12",
              "text": "The thing is technically you can use whatever model you want but not whatever subscription you want. Money stuff is very important",
              "score": -2,
              "created_utc": "2026-02-20 06:10:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6dxqxi",
                  "author": "Big_Bed_7240",
                  "text": "Money stuff",
                  "score": 3,
                  "created_utc": "2026-02-20 07:55:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6eezht",
                  "author": "tens919382",
                  "text": "Its the subscriptions that dont allow opencode, not the other way round though?",
                  "score": 3,
                  "created_utc": "2026-02-20 10:36:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6dnxsk",
          "author": "PutinIsASheethole",
          "text": "Opencode + AWS bedrock. It has loads of models including opus 4.6, Let work pay for the tokens",
          "score": 9,
          "created_utc": "2026-02-20 06:25:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fp5h2",
              "author": "aries1980",
              "text": "So does Azure and GCP. There are ton of generic providers who offer Anthropic models.",
              "score": 1,
              "created_utc": "2026-02-20 15:23:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6dr7c4",
          "author": "segmond",
          "text": "the more you send your money on Anthropic they more they get bold and greedy to do stuff like this.  Cancel your Claude subscription.  Use KimiK2.5, Qwen3.5, GLM5, MiniMax2.5, etc. You have options.",
          "score": 17,
          "created_utc": "2026-02-20 06:54:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6f3pea",
              "author": "spartanOrk",
              "text": "I generally do that but in the end Claude fixes the mess. \n\nThere is noticeable quality difference.",
              "score": 2,
              "created_utc": "2026-02-20 13:33:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6g3gnc",
                  "author": "Codemonkeyzz",
                  "text": "Codex is far better. 20 USD codex + 8 USD nanogpt  enough for me. I have 10 USD copilot backup. I get the same value as 100 USD Anthropic plan. Only downside is to set it up. E.g ; setup providers, skills , agents model matching..etc. which takes roughly one hour. \n\nUse codex to plan , Chinese models to execute the plan. If your plan is good and detailed ( If you have good Agents.md) , it all works perfectly",
                  "score": 2,
                  "created_utc": "2026-02-20 16:30:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6do4q3",
          "author": "devdnn",
          "text": "In this ever changing models, I would like always like put a common tool in front of those models.\n\nPerhaps thatâ€™s why I enjoy Copilot so much, despite its limited context window, even more why I like opencode because of its ability to support multiple models from various sources.\n\nIf anything this whole thing is making Anthropic a sore winner.",
          "score": 9,
          "created_utc": "2026-02-20 06:27:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dprcb",
              "author": "tksuns12",
              "text": "That's why I stay with OC..",
              "score": 5,
              "created_utc": "2026-02-20 06:42:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6e0z4j",
          "author": "alovoids",
          "text": "opencode is preparing for google integration, can't wait when i can use my google ai subs in opencode officially!",
          "score": 8,
          "created_utc": "2026-02-20 08:25:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dua7k",
          "author": "BKite",
          "text": "Omg you guys are so gaslighted by the marketing. Heavy user of GPT 5.2 codex. Have to say Iâ€™m not impressed with Opus 4.6 it is on par with GPT. So itâ€™s great but not game changing and it eats your quota in like 5 prompts. How are people getting things done with this. I fell like you have at least 30x the quota with codex.",
          "score": 4,
          "created_utc": "2026-02-20 07:23:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fzqwf",
              "author": "LtCommanderDatum",
              "text": "Agreed. Opus has been great for a lot of things, but it recently got stuck on a fairly simple Node.js coding problem. After a day of no progress, I switched back to GPT 5.3 and it solved the problem in a couple minutes.\n\nCan't tell if that was just Opus getting stuck on some jagged frontier shortcoming, or GPT caught up and passed Opus again.",
              "score": 1,
              "created_utc": "2026-02-20 16:13:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ewua4",
          "author": "el-guille",
          "text": "I use openrouter, kimi, minimax, gptoss, etc",
          "score": 5,
          "created_utc": "2026-02-20 12:52:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dqiu7",
          "author": "BuildAISkills",
          "text": "If you just want to use Claude, why not just use Claude Code?Â \n\nI use Codex for ChatGPT even though itâ€™s available in OpenCode.Â \n\nOpenCode has tons of other models for you to play with. For me itâ€™s my open weight go to.\n",
          "score": 3,
          "created_utc": "2026-02-20 06:48:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dwe76",
              "author": "aeroumbria",
              "text": "It kinda sucks to have a project with:\n\n    .roo\n    .cline\n    .claude\n    .opencode\n    AGENTS.md\n    CLAUDE.md\n    GEMINI.md\n\nI can run a script to symlink them, but it still sucks that someone has to intentionally invent incompatible ways to do things...",
              "score": 7,
              "created_utc": "2026-02-20 07:42:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6eb3sv",
                  "author": "tenebreoscure",
                  "text": "No need to symlink, keep everything in [AGENTS.md](http://AGENTS.md) and in the others add a link like  \n  \n`See [](AGENTS.md)`\n\nAs long as the links are working the agent will navigate and use them, that's how I keep everything compatible with multiple agents and avoid repetitions.",
                  "score": 5,
                  "created_utc": "2026-02-20 10:01:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6e4ol4",
                  "author": "Grouchy-Bed-7942",
                  "text": "Opencode supports Claude code files right?",
                  "score": 2,
                  "created_utc": "2026-02-20 09:00:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ea3t5",
                  "author": "KnifeFed",
                  "text": "What do you use that doesn't support AGENTS.md?",
                  "score": 1,
                  "created_utc": "2026-02-20 09:52:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6e2lxx",
              "author": "bzBetty",
              "text": "because opencode desktop is far more convienient when you're working on a lot of projects",
              "score": 1,
              "created_utc": "2026-02-20 08:41:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hxcpi",
                  "author": "antonlvovych",
                  "text": "Try Superset",
                  "score": 1,
                  "created_utc": "2026-02-20 21:40:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6e4ujn",
          "author": "schlammsuhler",
          "text": "There also kimi code plan, glm and minimax. And opencode has its own plan.",
          "score": 3,
          "created_utc": "2026-02-20 09:02:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f0ugb",
          "author": "alexzim",
          "text": "I personally use it because I want Chinese models like Kimi K2.5 and canâ€™t be bothered with Claude Code Router.",
          "score": 3,
          "created_utc": "2026-02-20 13:17:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dy4y2",
          "author": "NickeyGod",
          "text": "Opensource modals are catching up. Why use a 200$ Subscription a month when i can have it all with Opencode for 20$ a month ?",
          "score": 4,
          "created_utc": "2026-02-20 07:58:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6f011q",
              "author": "sig_kill",
              "text": "Or even self-host, and supplement with your free usage from all providers for big important things.\n\nYou donâ€™t always need to drive the car at 120 km/h",
              "score": 2,
              "created_utc": "2026-02-20 13:12:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6feih5",
                  "author": "NickeyGod",
                  "text": "I self host only like really small models we talking 7b and embeddings. Those are doing quite well i don't actually need external interference for those",
                  "score": 1,
                  "created_utc": "2026-02-20 14:30:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6fezc7",
                  "author": "NickeyGod",
                  "text": "Currently opencode with oh-my-opencode with kimi-k2.5 + minimax 2.5 is doin a really good job i cant self host such big models on my own hardware.",
                  "score": 1,
                  "created_utc": "2026-02-20 14:33:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6dnc0r",
          "author": "jpcaparas",
          "text": "Huh? Models-as-a-service is the future. ",
          "score": 2,
          "created_utc": "2026-02-20 06:20:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dny6d",
          "author": "ThankYouOle",
          "text": "eh, the only problem here is the one with Claude subscription. \n\nall other models including premium, or 3rd party provider, or even self host can keep using it no issue. ",
          "score": 2,
          "created_utc": "2026-02-20 06:25:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dq4cp",
              "author": "tksuns12",
              "text": "Using Antigravity credential in OC would ban your account man.",
              "score": 1,
              "created_utc": "2026-02-20 06:45:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6drevt",
                  "author": "ThankYouOle",
                  "text": "look, my point is AI provider not just claude and antigravity.\n\nif anything, i will stay away from provider who lock-in their customer. ",
                  "score": 2,
                  "created_utc": "2026-02-20 06:56:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ej49m",
          "author": "debackerl",
          "text": "I don't agree you can create API Keys for Google AI Studio (https://aistudio.google.com/app/api-keys), and Ollama Cloud (GLM, Qwen, etc). There is also GitLab Duo support, but maybe more for enterprises.\n\nIn an industry where new models come up all the time. I don't want to commit to a mono-provider harness. I want one harness using multiple providers.",
          "score": 2,
          "created_utc": "2026-02-20 11:12:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ejhdd",
          "author": "ganonfirehouse420",
          "text": "Glm-5 is good enough for me. Also it isn't expensive!",
          "score": 2,
          "created_utc": "2026-02-20 11:15:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dlfcn",
          "author": "HarjjotSinghh",
          "text": "this era's magic trick: open code as my new favorite escape hatch",
          "score": 2,
          "created_utc": "2026-02-20 06:04:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6f0anp",
              "author": "sig_kill",
              "text": "Itâ€™s quickly become my favourite too. \n\nStarted with cursor, tried out the rest, went back to cursor, bounce into codex from time to timeâ€¦ But always end up back with opencode",
              "score": 0,
              "created_utc": "2026-02-20 13:13:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6dq7zr",
          "author": "Zeroox1337",
          "text": "So you pay for API access and Anthropic bans you if you use API from other Products then theirs, or is it for the subscription based plan?",
          "score": 1,
          "created_utc": "2026-02-20 06:46:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e5gyl",
              "author": "danmaz74",
              "text": "You can use the pay-per-use API just fine, but it's very expensive compared to the subscription you can use in Claude Code.",
              "score": 1,
              "created_utc": "2026-02-20 09:08:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6e8g6q",
                  "author": "StrikingSpeed8759",
                  "text": "I still dont understand the whole scope. Right now I got opencode with my Claude sub working fine, am I risking getting banned? I dont have it in the opencode zen but rather directly through opencode auth",
                  "score": 1,
                  "created_utc": "2026-02-20 09:36:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6dqtps",
          "author": "Astorax",
          "text": "We focus on Opencode with AWS Bedrock for Claude models at work.\n\nAt home it's just too expensive and I feel your pain. It feels like I'm stuck with Claude Code and my anthropic subscription. I've tried using Gemini, Mistral, openai and groq via Apis through my litellm instance (I've setup for my n8n) but the pure API costs escalate quickly ðŸ«£\n\nTried focusing on using Gemini 3 flash for build mode and some different models for the plan mode but I really like my sonnet 4.5 when getting shit done. ðŸ« \n\nAlways open for recommendations... But the llm providers shouldn't train on my data... (:\n\nHelp me before I upgrade to the anthropic max plan ðŸ˜¬ðŸ˜¬",
          "score": 1,
          "created_utc": "2026-02-20 06:51:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dsjgo",
          "author": "AGiganticClock",
          "text": "Copilot is good no?",
          "score": 1,
          "created_utc": "2026-02-20 07:07:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6duair",
          "author": "0sko59fds24",
          "text": "Its the perfect copilot & codex harness",
          "score": 1,
          "created_utc": "2026-02-20 07:23:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dxut0",
          "author": "FreeEye5",
          "text": "I use open code and a multi llm agent workflow, pans put great for me. Codex5.3 orchestrates, plans, f then sends to Opus for a plan review and an agent flow that prioritises max parallel agents, then hands back to codex who then deploys free agents like kimi and minimax to complete tasks, code review, ux ui review.",
          "score": 1,
          "created_utc": "2026-02-20 07:56:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dy4qv",
          "author": "Civil_Baseball7843",
          "text": "when opensource models catch up then opencode will be the best. Actually opencode+glm5 feels 80% of cc now. Lest see if deepseek will bring more superis.",
          "score": 1,
          "created_utc": "2026-02-20 07:58:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e06ee",
          "author": "jmhunter",
          "text": "ya i wouldnt blame opencode... opencode is the way.... zen/kimi is the way id like to move once theres enough compute\n\n",
          "score": 1,
          "created_utc": "2026-02-20 08:18:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6f0lma",
              "author": "sig_kill",
              "text": "GLM-5 has been great as well",
              "score": 1,
              "created_utc": "2026-02-20 13:15:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6e2odg",
          "author": "bzBetty",
          "text": "i mean, opencode supports plugins for auth...",
          "score": 1,
          "created_utc": "2026-02-20 08:41:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e78u7",
          "author": "franz_see",
          "text": "Been a big fan of Anthropic models for awhile now. But I find the latest SOTA models to be practically at par with each other\n\nHowever, the i feel like the gap in agentic capabilities are becoming wider and wider. \n\nRight now, i feel like Anthropic is forcing me to switch to opencode + gpt models. I get all the goodies of opencode with practically on par SOTA model",
          "score": 1,
          "created_utc": "2026-02-20 09:25:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ecgmv",
          "author": "hlacik",
          "text": "you can use subscription based services like [Z.AI](http://Z.AI) (provides GLM5) or Kimi Kode (provides KIMI K2.5) and there is plenty services like this",
          "score": 1,
          "created_utc": "2026-02-20 10:13:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ejj5n",
          "author": "georgiarsov",
          "text": "Try exploring open weight models through openrouter or opencode zen. They are magnitudes cheaper and offer the same performance from what I have seen in my projects. Leading models for agentic workflows now are kimi k2.5 and glm-5 for example",
          "score": 1,
          "created_utc": "2026-02-20 11:16:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ejtvq",
          "author": "jesperordrup",
          "text": "I'm using Opencode with anthropic 5*max every day?",
          "score": 1,
          "created_utc": "2026-02-20 11:18:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6epgyd",
          "author": "atkr",
          "text": "skill issue",
          "score": 1,
          "created_utc": "2026-02-20 12:02:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6eqjje",
          "author": "seaweeduk",
          "text": "Claude code still works fine and when anthropic attempt to block it at the system prompt level again someone will just make another workaround and fork the plugin. \n\nThat said I'm cancelling my sub because I don't want to support anthropic anyway and codex is a better model.",
          "score": 1,
          "created_utc": "2026-02-20 12:10:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f3986",
          "author": "bruor",
          "text": "At work I have opencode wired to Claude models via Azure AI Foundry.  Been waiting 3 weeks for them to allow access to GPT models.  He is looking at GitHub copilot Enterprise instead. \n\nFor personal stuff I'm using Opencode Zen, at the moment.",
          "score": 1,
          "created_utc": "2026-02-20 13:30:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f5p5f",
          "author": "renan_william",
          "text": "[z.ai](http://z.ai)  / GLM5",
          "score": 1,
          "created_utc": "2026-02-20 13:44:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f6vq1",
          "author": "dannyt74",
          "text": "Runs very well with GitHub CoPilot. Which is even a quite cheap option. Also can use with Ollama and others.",
          "score": 1,
          "created_utc": "2026-02-20 13:50:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6f6zs3",
              "author": "dannyt74",
              "text": "Actually, it I mostly use Opus and Sonnet through GitHub with it all the time.",
              "score": 1,
              "created_utc": "2026-02-20 13:51:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6flyij",
          "author": "Bob5k",
          "text": "just grab some reliable subscription service and connect it to opencode. Have in mind opencode is opensource software, so it's not built on it's own to bring revenue straight away.   \nI'm using minimax m2.5 highspeed personally since it was released and can't be happer as opencode with it is flying. (and they still have the nice[ 10% discount available](https://platform.minimax.io/subscribe/coding-plan?code=HO46LCwAJ5&source=link)).",
          "score": 1,
          "created_utc": "2026-02-20 15:08:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fp7qc",
          "author": "revilo-1988",
          "text": "Ich nutze locallen llms mit Open Code",
          "score": 1,
          "created_utc": "2026-02-20 15:23:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fzhqn",
          "author": "mcowger",
          "text": "Codex is supported and works great.  So is GHCP. Lots of excellent open models out there too for cheap.",
          "score": 1,
          "created_utc": "2026-02-20 16:12:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g2txx",
          "author": "MakesNotSense",
          "text": "I would sooner deal with the limitations of open-source Chinese models than the limitations of a closed harness.\n\nThere is too much that simply cannot be done in Claude Code and other platforms purely because users cannot develop the harness to do what is needed.\n\nA Harness that users cannot use to improve and fix the Harness cripples the users work capacity in the long-term.\n\nMore and more, I'm beginning to wonder if the Chinese are really the bad guys Anthropic and others try to paint them as.\n\nI'm disabled and need AI to help me perform complex civil rights litigation. Without that litigaiton, my human rights will continue to be violated. I'm currently dependent on Claude in Opencode for my agentic workflow. I live in fear that one day Anthropic will make Claude no longer work at all in OpenCode. What then? Kimi 2.5 I guess.\n\nWill I, as an America citizen, have to use Chinese AI to protect my civil and constitutional rights because Anthropic won't provide an effective, equitable, and dependable way for me to use Claude?\n\nThe reason I need OpenCode isn't out of preference, but necessity; no platform was providing the features I needed, so I must build it myself. I have to build my tools, and do my own litigation. It's a societal-wide failure; I need AI because no one is helping people like me with these legal or development problems.\n\nI think of my situation - between my mental and physical disabilities, the hardship of the rights violations, imposed poverty, and general abandonment by the legal community and nonprofits -  like I'm having to build a sand castle in the middle of a hurricane, while other people build on a sunny-day beach. Then people go out of their way to make it even harder for me to build, or try to destroy what I've built.\n\nI don't fear misaligned AI. I fear misaligned people.\n\nI really like and enjoy Claude, but I fear Anthropic is going to make Claude inaccessible to me; they seem intent on preventing users like me from having equitable, effective access to Claude in OpenCode. While showing no interest or intent to develop the features my workflow needs in Claude Code or CoWork.\n\nMeanwhile, an open-source model will always be accessible, even if less performant.\n\nThe world would be such a better place if entities like Anthropic would help people like me, instead of create friction and more problems.  It's nice to have AI agents that help me, but now the company who owns that AI is working to prevent Claude from being helpful to me.\n\nThe more I try to do my work, the more I end up documenting how totally screwed up everything is.\n\nThere's a lot more to consider than just price and performance when picking which AI you use.\n\nMy life-long severe disability makes me especially mindful of what I am dependent upon. If I don't have specific things, I get injured, badly.\n\nI've become dependent upon Claude, and any day now, it could just be gone. The fear of that day weighs heavily on me. I find it very disturbing that my safety and human rights are not being better served by U.S. AI companies.\n\nHow will OpenCode survive? By being one of a scarce few places where people like me can build a future our survival can depend on.",
          "score": 1,
          "created_utc": "2026-02-20 16:27:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gcci1",
          "author": "HikariWS",
          "text": "u gotta pay by token",
          "score": 1,
          "created_utc": "2026-02-20 17:10:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6goyhp",
          "author": "Tobibobi",
          "text": "Github Copilot is a VERY good product. With that you can authorize opencode and use Anthropic models (as well as OpenAI) just fine. The context window is smaller, but I very rarely encounter issues with this.",
          "score": 1,
          "created_utc": "2026-02-20 18:08:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gzvdb",
          "author": "Delicious_Ease2595",
          "text": "Just use Claude Code, the point of OpenCode is to use any llm",
          "score": 1,
          "created_utc": "2026-02-20 18:57:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gzwil",
          "author": "Affectionate-Job8651",
          "text": "just use api key",
          "score": 1,
          "created_utc": "2026-02-20 18:57:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hnyr4",
          "author": "IceManMinus0ne",
          "text": "you can still use openrouter. Barely costs a thing with the right models. ",
          "score": 1,
          "created_utc": "2026-02-20 20:54:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6iax5l",
          "author": "Existing-Wallaby-444",
          "text": "Opencode is open source. Just modify it to look like a Claude code client and you are good to go",
          "score": 1,
          "created_utc": "2026-02-20 22:51:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6imxq9",
          "author": "ZealousidealShoe7998",
          "text": "opencode whole point is to use open source model. if anything is achieving what is meant to .",
          "score": 1,
          "created_utc": "2026-02-20 23:59:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dnz9g",
          "author": "dengar69",
          "text": "GitHub Copilot and NanoGPT is the way to go.",
          "score": 1,
          "created_utc": "2026-02-20 06:26:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6elhyv",
              "author": "Cyrecok",
              "text": "Why nanogpt?",
              "score": 2,
              "created_utc": "2026-02-20 11:32:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fb1nx",
                  "author": "dengar69",
                  "text": "Access to all of the open source models including Kimi 2.5 which is very close to Opus on paper.",
                  "score": 1,
                  "created_utc": "2026-02-20 14:12:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6e7bvd",
          "author": "charmander_cha",
          "text": "Grande bosta esses modelos, continuarei usando o kimi",
          "score": 0,
          "created_utc": "2026-02-20 09:26:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fygl4",
          "author": "LtCommanderDatum",
          "text": "People like OpenCode's UI? I think it's borderline unusable. The only reason I'd ever use OpenCode is for local LLMs (which it does well). Nothing else.",
          "score": 0,
          "created_utc": "2026-02-20 16:07:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4mzch",
      "title": "OpenCode Zen is dead, but MiniMax M2.5 is the ultimate Opus replacement",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r4mzch/opencode_zen_is_dead_but_minimax_m25_is_the/",
      "author": "pipubx",
      "created_utc": "2026-02-14 15:09:29",
      "score": 60,
      "num_comments": 97,
      "upvote_ratio": 0.74,
      "text": "Everyone is mourning the free version of OpenCode Zen, but the real play is moving to MiniMax M2.5. It's the most reliable alternative to Opus I've found. It's a Real World Coworker that costs $1 an hour and hits SOTA benchmarks (80.2% SWE-Bench). I've seen people complain about M2.1 fixing linting instead of errors, but M2.5 is a massive upgrade in task decomposition. If you want the cheapest, most accurate model for your CLI, this is it. Their RL tech blog is a must-read for anyone looking to optimize their dev workflow.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4mzch/opencode_zen_is_dead_but_minimax_m25_is_the/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5du9el",
          "author": "mintybadgerme",
          "text": "In my, admittedly limited tests, Kimi 2.5 is both cheaper and better at the moment.",
          "score": 23,
          "created_utc": "2026-02-14 18:56:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fyh9j",
              "author": "ideadude",
              "text": "Same m2.5 keeps running into issues it could get around if it slowed down and thought things through, but it's deciding to just rewrite things that are out of scope. Maybe folks who start from scratch with it have better outcomes, but i have to have other models clean up for it when it breaks shit.",
              "score": 4,
              "created_utc": "2026-02-15 02:20:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5hafba",
                  "author": "mintybadgerme",
                  "text": "Not to mention that M 2.5 is a little bit more expensive than Kimi 2.5. Which makes quite a difference if you're doing a fairly complex project. I get some quite Sonnet vibes out of Kimi.",
                  "score": 1,
                  "created_utc": "2026-02-15 08:57:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5gefng",
              "author": "oulu2006",
              "text": "Same",
              "score": 3,
              "created_utc": "2026-02-15 04:13:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5vn1m5",
              "author": "East-Stranger8599",
              "text": "Kimi 2.5 is great, but hallucinates badly without proper context",
              "score": 1,
              "created_utc": "2026-02-17 15:20:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5x196g",
                  "author": "mintybadgerme",
                  "text": "I've found that if you keep things short and sweet, it works very well.",
                  "score": 1,
                  "created_utc": "2026-02-17 19:22:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5d1bzt",
          "author": "Big-Masterpiece-9581",
          "text": "Why is it dead?",
          "score": 12,
          "created_utc": "2026-02-14 16:32:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fp0qp",
              "author": "No_Success3928",
              "text": "Its not, OP is being dramatic ðŸ˜‚",
              "score": 13,
              "created_utc": "2026-02-15 01:17:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5cy4ip",
          "author": "DRBragg",
          "text": "Wait, what happened to opencode zen?",
          "score": 11,
          "created_utc": "2026-02-14 16:16:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d021x",
              "author": "touristtam",
              "text": "No idea the pricing page still list free models: https://opencode.ai/docs/zen#pricing",
              "score": 10,
              "created_utc": "2026-02-14 16:25:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5d56kq",
                  "author": "UseHopeful8146",
                  "text": "If I had to guess they are (or did) rotating models. The free subs change every month or so. At least that was my understanding.",
                  "score": 9,
                  "created_utc": "2026-02-14 16:51:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5np6ch",
                  "author": "sudoer777_",
                  "text": "Apparently OpenCode Zen rate limits them now (not the provider), or at least they are for Kimi K2.5",
                  "score": 2,
                  "created_utc": "2026-02-16 09:49:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ev22f",
          "author": "_Turd_Reich",
          "text": "Another clickbait title.",
          "score": 10,
          "created_utc": "2026-02-14 22:14:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cp71p",
          "author": "Specialist-Yard3699",
          "text": "Maybe not Opus, but itâ€™s really good.\nCancel kimi25 subs, and use only minimax+glm now.",
          "score": 8,
          "created_utc": "2026-02-14 15:30:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5es5zv",
              "author": "skewbed",
              "text": "I would avoid subscribing to inference providers. Just use OpenRouter or something similar like OpenCode Zen.",
              "score": 3,
              "created_utc": "2026-02-14 21:58:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5exg2l",
                  "author": "pires1995",
                  "text": "The [nano-gpt](https://nano-gpt.com/r/kVxQFNRB) is a great option for it. The plan is USD 8 and have almost all open-source models (Kimi, GLM, Minimax). I notice some models not working or taking too long, but for the price worth try it. ",
                  "score": 4,
                  "created_utc": "2026-02-14 22:28:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5f8ui9",
                  "author": "Unlikely_Word_5607",
                  "text": "Isn't the whole point of subscribing to inference providers that they subsidise the costs compared to using the API?",
                  "score": 1,
                  "created_utc": "2026-02-14 23:36:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ehi4t",
          "author": "KnifeFed",
          "text": "> Everyone is mourning the free version of OpenCode Zen\n\ntf are you talking about?",
          "score": 3,
          "created_utc": "2026-02-14 21:00:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fknyu",
          "author": "robberviet",
          "text": "It's great for its size (200b). Not Opus or GPT level but good enough.\nAlso I think you should look at swe-rebench, not swe-bench.",
          "score": 3,
          "created_utc": "2026-02-15 00:49:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cud30",
          "author": "benzflow",
          "text": "How does it compare with Kimi k2.5 and GLM 5?",
          "score": 2,
          "created_utc": "2026-02-14 15:56:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5du3dx",
              "author": "mintybadgerme",
              "text": "Kimi 2.5  is better in my tests.",
              "score": 7,
              "created_utc": "2026-02-14 18:55:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5drkc1",
          "author": "Comrade-Porcupine",
          "text": "I like these open models but I fail to see how $1/hour is better value e.g. the $200/month Codex membership which is basically fully unlimited value.\n\nEthically, yes. And for strictly **API** uses, yes.  I use DeepSeek and others using API tokens and they're dirt cheap and quite effective. But the *coding plans* from GLM and MiniMax and Moonshot are not that awesome of value.",
          "score": 2,
          "created_utc": "2026-02-14 18:43:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fpd75",
              "author": "No_Success3928",
              "text": "Codex Fully unlimited? Not even close.",
              "score": 3,
              "created_utc": "2026-02-15 01:19:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e5u8o",
          "author": "Crafty_Chart1694",
          "text": "until deepseek 4 comes out",
          "score": 2,
          "created_utc": "2026-02-14 19:56:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e91mb",
          "author": "soul105",
          "text": "Kimi K2.5 is still free and available for me",
          "score": 2,
          "created_utc": "2026-02-14 20:14:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5gevds",
              "author": "Wildnimal",
              "text": "Free where?",
              "score": 0,
              "created_utc": "2026-02-15 04:17:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5h9hka",
                  "author": "soul105",
                  "text": "https://preview.redd.it/2usu5t91gmjg1.png?width=544&format=png&auto=webp&s=dee2668095d144a0627e96ba20f32ff7d59cbf81\n\nYou can also check the current list of [free models](https://opencode.ai/docs/zen/#pricing).",
                  "score": 1,
                  "created_utc": "2026-02-15 08:48:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5jhoni",
          "author": "amri2k",
          "text": "kimi 2.5 > minimax 2.5",
          "score": 2,
          "created_utc": "2026-02-15 17:40:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cn1sa",
          "author": "HarjjotSinghh",
          "text": "this m2.5 is basically code's new gym rat - cheap, brutal efficiency.",
          "score": 4,
          "created_utc": "2026-02-14 15:18:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cod4i",
          "author": "idkwtftbhmeh",
          "text": "Minimax M2.5 Falls behind both Kimi K2.5 and GLM5 in every bench, hell even glm7 is in front, trully disappointed with the model",
          "score": 6,
          "created_utc": "2026-02-14 15:25:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f12cv",
              "author": "DinoAmino",
              "text": "Disappointed that a 230B model doesn't score better than models that are 3x and 4x larger? srsly? That's some wildly unrealistic expectations there.",
              "score": 1,
              "created_utc": "2026-02-14 22:49:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5g6hza",
                  "author": "idkwtftbhmeh",
                  "text": "well, I did create my expectations out of the benchs that they announced, which in theory would surpass these models in some cases (doesn't happen)",
                  "score": 1,
                  "created_utc": "2026-02-15 03:15:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5f9ean",
              "author": "Squale279",
              "text": "Bench isnâ€™t the best way to evaluate a llm, try it in real use cases and compare it with other products.",
              "score": 1,
              "created_utc": "2026-02-14 23:39:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5g6ev6",
                  "author": "idkwtftbhmeh",
                  "text": "oh I did, it's quite bad overall to be honest, the speed is great tho",
                  "score": 1,
                  "created_utc": "2026-02-15 03:15:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5d59d3",
              "author": "UseHopeful8146",
              "text": "Iâ€™m sorry, glm 7?",
              "score": 1,
              "created_utc": "2026-02-14 16:51:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5d8g22",
                  "author": "zuk987",
                  "text": "He probably meant 4.7",
                  "score": 3,
                  "created_utc": "2026-02-14 17:07:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5esxea",
              "author": "cri10095",
              "text": "M2.5 is much smaller then the other models",
              "score": 1,
              "created_utc": "2026-02-14 22:02:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5eybxo",
                  "author": "idkwtftbhmeh",
                  "text": "It is indeed, still disappointed, I saw the blog post and benchs and it seems VERY cherrypicked compared to individual researchers like swe-rebench",
                  "score": 2,
                  "created_utc": "2026-02-14 22:33:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5d3swd",
          "author": "touristtam",
          "text": "> Their RL tech blog is a must-read for anyone looking to optimize their dev workflow.\n\nLink please?",
          "score": 3,
          "created_utc": "2026-02-14 16:44:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dmo8h",
          "author": "Both_Ad2330",
          "text": "Hope this gets on AWS Bedrock soon.",
          "score": 1,
          "created_utc": "2026-02-14 18:19:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5f30f1",
          "author": "Moist_Associate_7061",
          "text": "i used minimax 2.5 all day long, and it was not even close kimi k2.5. babysitting is needed..",
          "score": 1,
          "created_utc": "2026-02-14 23:00:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f6o33",
              "author": "johnerp",
              "text": "Which one is better, Iâ€™m not clear.",
              "score": 3,
              "created_utc": "2026-02-14 23:22:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5gzjg9",
          "author": "XtoddscottX",
          "text": "Can it work with images? Cause yeah, if you need to generate simple code these models are okay, but for some frontend tasks itâ€™s better to use model that accept visual input too, and as I know these Chinese models donâ€™t whilst three American big models do.",
          "score": 1,
          "created_utc": "2026-02-15 07:12:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h20vn",
          "author": "wjjia",
          "text": "Honestly, it was about time we stopped relying on OpenCode Zen anyway. Everyone is freaking out over the shutdown, but it was a loss leader from day one. I haven't put M2.5 through the wringer yet, but if that 80.2% SWE-Bench score actually holds up in real-world messy codebases, it's a massive jump. Most of these models talk a big game and then fail the moment you hit a weird dependency issue.",
          "score": 1,
          "created_utc": "2026-02-15 07:35:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h94py",
          "author": "Relative-Honey-4485",
          "text": "The jump from 2.1 to 2.5 is the real conversation here. 2.1 was driving me insane with that linting obsession - fixing my tabs while the actual logic was still broken. If the task decomposition is actually improved, I might give it a shot. Still skeptical about the $1/hr claim though, there is always a catch with token windows.",
          "score": 1,
          "created_utc": "2026-02-15 08:44:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h9e2k",
          "author": "Capital_Standard4603",
          "text": "RIP OpenCode Zen. It was good while it lasted.",
          "score": 1,
          "created_utc": "2026-02-15 08:47:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hdefr",
          "author": "elaytot",
          "text": "Minimax m2.5 is not better! Cant even tell my project was in typescript after it reviewed the whole codebase.. got me bunch of typeerrors",
          "score": 1,
          "created_utc": "2026-02-15 09:26:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hgw56",
          "author": "Yukeyii",
          "text": "Did anyone actually read the RL tech blog OP mentioned? I just skimmed it and the way they are handling reinforcement learning is actually pretty clever if you are into the infra side of things. It explains why the task breakdown feels more \"human\" than the older versions.",
          "score": 1,
          "created_utc": "2026-02-15 10:00:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5llki0",
              "author": "touristtam",
              "text": "Do you have a link, I have no idea what is the RL tech blog that is being mentioned.\n\nIs that: https://www.minimax.io/news/forge-scalable-agent-rl-framework-and-algorithm ?",
              "score": 1,
              "created_utc": "2026-02-16 00:16:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5i21vo",
          "author": "LionelOOK",
          "text": "\"Opus replacement\" is a bold claim. Opus has that specific feel for creative logic that is hard to replicate, but for pure CLI work and bug fixing, I can see MiniMax taking that spot if it is really that cheap.",
          "score": 1,
          "created_utc": "2026-02-15 13:05:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i4otx",
          "author": "Feeling-Whole4574",
          "text": "$1 an hour? I will believe it when I see my invoice at the end of the month.",
          "score": 1,
          "created_utc": "2026-02-15 13:23:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i89ww",
          "author": "Virtual-Path1704",
          "text": "Glad I am not the only one who noticed the linting thing. M2.1 would spend half its energy fixing my indentation instead of actually solving the logic error I was pointing at. If 2.5 fixed that, it is worth the switch.",
          "score": 1,
          "created_utc": "2026-02-15 13:46:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i9i1h",
          "author": "linegel",
          "text": "Their SWE bench is basically fake news due to too heavy reliance on Anthrophic models \n\nCheck updated SWE bench",
          "score": 1,
          "created_utc": "2026-02-15 13:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ib39u",
          "author": "Icy_Net5151",
          "text": "Benchmark obsession needs to stop. SWE-Bench is one thing, but how does it handle a 10-year-old legacy codebase with zero documentation? That is the real test for any \"coworker\" model.",
          "score": 1,
          "created_utc": "2026-02-15 14:03:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5id9c8",
          "author": "ChanningACE",
          "text": "Just switched. It is definitely snappier than 2.1. Not sure if it is \"ultimate\" yet, but it is actually usable for once.",
          "score": 1,
          "created_utc": "2026-02-15 14:15:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ifoi1",
          "author": "Dantenmd",
          "text": "Been looking for a solid Opus alternative since the quality started dipping recently. I will check out that blog post later, thanks for the heads up.",
          "score": 1,
          "created_utc": "2026-02-15 14:29:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vmx38",
          "author": "East-Stranger8599",
          "text": "This is an overstatement, at max it may be weaker cousin of Sonnet 4.5",
          "score": 1,
          "created_utc": "2026-02-17 15:19:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yien6",
          "author": "Conscious-Hair-5265",
          "text": "They gamed the bencharks, MiniMax 2.5 is not as impressive in real life usecases. Check out swe re bench bench mark",
          "score": 1,
          "created_utc": "2026-02-17 23:43:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o609kol",
          "author": "1E_liot",
          "text": "Switched to M2.5 last night for a legacy refactor. The task decomposition is actually noticeable compared to 2.1. It didn't just move brackets around; it actually handled the logic flow better than Opus does in some spots.",
          "score": 1,
          "created_utc": "2026-02-18 05:59:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60b1u7",
          "author": "Asher_dd",
          "text": "$1 an hour for this level of performance is a steal. Even if thereâ€™s a bit of latency, the output quality on M2.5 makes the wait worth it compared to the older versions.",
          "score": 1,
          "created_utc": "2026-02-18 06:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60blig",
          "author": "Yukeyii",
          "text": "Finally someone mentions the RL blog. That part about how they handle rewards for code correctness is the only reason I gave 2.5 a shot, and honestly, the logic feels way more \"human\" now.",
          "score": 1,
          "created_utc": "2026-02-18 06:15:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60e8w2",
          "author": "Low-Position-1569",
          "text": "RIP OpenCode Zen, but if M2.5 keeps performing like this at this price point, I'm not even mad.",
          "score": 1,
          "created_utc": "2026-02-18 06:38:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60f1nw",
          "author": "Cornelius956",
          "text": "80.2% on SWE-Bench is a bold claim, but after running a few complex tasks today, I'm starting to believe it. It's definitely snappier than the other SOTA models I've tried.",
          "score": 1,
          "created_utc": "2026-02-18 06:44:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60fu4i",
          "author": "Stellanear",
          "text": "I was sticking with Opus, but the cost-to-performance ratio on M2.5 is making it hard to justify staying. Itâ€™s becoming my main for bulk CLI tasks.",
          "score": 1,
          "created_utc": "2026-02-18 06:51:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60gl2w",
          "author": "Eviedate",
          "text": "M2.1 had that annoying linting loop habit, but 2.5 seems to have actually fixed it. It's much more focused on functional errors now.",
          "score": 1,
          "created_utc": "2026-02-18 06:58:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60hcsm",
          "author": "yxllove",
          "text": "The context window handling on M2.5 feels surprisingly robust. I fed it a decent-sized repo and it didn't hallucinate the file structure like most models at this price.",
          "score": 1,
          "created_utc": "2026-02-18 07:04:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60sxj1",
          "author": "Delicious_Can_6288",
          "text": "Just read that blog you mentioned. It's clear they're doing something different with their training because M2.5 is hitting solutions that 2.1 completely missed.",
          "score": 1,
          "created_utc": "2026-02-18 08:51:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60tr3y",
          "author": "Correct_Durian1503",
          "text": "I've been using it for a week. For the cost of a coffee to run it all day, the output is surprisingly close to - if not better than - the more expensive \"prestige\" models.",
          "score": 1,
          "created_utc": "2026-02-18 08:59:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60uepw",
          "author": "Interesting_Block102",
          "text": "I used to think nothing could replace the \"Opus feel,\" but M2.5 is getting dangerously close, especially with how it handles task decomposition.",
          "score": 1,
          "created_utc": "2026-02-18 09:05:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60ul8o",
          "author": "Eamonick",
          "text": "Is the CLI integration seamless? If so, I'm moving my entire workflow over. The benchmarks are just too good to ignore.",
          "score": 1,
          "created_utc": "2026-02-18 09:06:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60usn1",
          "author": "Scanlanderson",
          "text": "The 80% SWE-bench score is what caught my eye. If it can actually resolve GitHub issues autonomously like it did for my test run this morning, it's a total game changer.",
          "score": 1,
          "created_utc": "2026-02-18 09:08:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6276b9",
          "author": "ComparisonLeather631",
          "text": "Actually cheap for how good it is.",
          "score": 1,
          "created_utc": "2026-02-18 14:45:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o627oan",
          "author": "Fletcher_ba",
          "text": "I noticed the same thing with the task decomposition. It breaks down PRs into much more manageable chunks now. It's way more reliable for long-form coding than it used to be.",
          "score": 1,
          "created_utc": "2026-02-18 14:47:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6285qn",
          "author": "ticharland",
          "text": "Tried it for Python today - it handled some pretty nasty dependency conflicts that usually trip up most LLMs. M2.5 is definitely an upgrade.",
          "score": 1,
          "created_utc": "2026-02-18 14:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6291os",
          "author": "Montague857",
          "text": "$1/hr for SOTA performance? That's basically the floor. Hard to see why anyone would pay more for similar results elsewhere.",
          "score": 1,
          "created_utc": "2026-02-18 14:54:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o629ir3",
          "author": "Marisssia",
          "text": "People always hype the new thing, but M2.5 actually feels like a step forward. It's not just a marginal gain over 2.1; it's a different beast.",
          "score": 1,
          "created_utc": "2026-02-18 14:56:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62dcby",
          "author": "Kiyosaaki",
          "text": "That RL tech blog explains a lot. You can really feel those \"correctness rewards\" kicking in when it iterates on a bug. 2.5 is a massive leap.",
          "score": 1,
          "created_utc": "2026-02-18 15:15:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62dkf2",
          "author": "HarlanWJK",
          "text": "I'm loving the \"Real World Coworker\" vibe. It's less preachy than Opus and just gets the code written. It's a much more efficient workflow.",
          "score": 1,
          "created_utc": "2026-02-18 15:16:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cpuqt",
          "author": "0Bitz",
          "text": "How well does it work with Oh-My opencodeâ€¦?",
          "score": 1,
          "created_utc": "2026-02-14 15:33:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d61rc",
              "author": "UseHopeful8146",
              "text": "In my experience OmO has the structure to make most of the reasoning relatively simple - you could probably get close to kimi/glm level execution with much smaller models, provided they have tool calling support and decent context window.\n\nIâ€™m still in the process of working on tooling and stuff, but testing for local model execution in Opcode/OmO is on my todo list specifically because I hold that theory at present.",
              "score": 3,
              "created_utc": "2026-02-14 16:55:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5szmkm",
                  "author": "0Bitz",
                  "text": "I tested this out and found it making too many bugs even with detailed prompts of an existing system. GLM seems to work better on my code base at least",
                  "score": 1,
                  "created_utc": "2026-02-17 03:30:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r9em99",
      "title": "Anthropic legal demanded Opencode Anthropic's OAuth library to be archived",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r9em99/anthropic_legal_demanded_opencode_anthropics/",
      "author": "marquinhoooo",
      "created_utc": "2026-02-19 23:10:14",
      "score": 60,
      "num_comments": 53,
      "upvote_ratio": 0.96,
      "text": "I watch [https://github.com/anomalyco/opencode-anthropic-auth](https://github.com/anomalyco/opencode-anthropic-auth) library and just saw a comment by Dax on a PR that was trying to mimic Claude Code protocol behavior, and Dax closed the PR with the message of [Anthropic's legal demanding the PR to be closed](https://github.com/anomalyco/opencode-anthropic-auth/pull/15#issuecomment-3930558874). Then, the repository was archived.\n\nWhat will happen to Anthropic's support on Opencode? No OAuth anymore?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r9em99/anthropic_legal_demanded_opencode_anthropics/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o6clsev",
          "author": "jmhunter",
          "text": "My ChatGPT 20 buck plan I never hit a wall. Itâ€™s just slow. This is ridic anthropic. I hope they burn",
          "score": 17,
          "created_utc": "2026-02-20 01:58:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dkh4g",
              "author": "xmnstr",
              "text": "Slow? Are you using Codex 5.3?",
              "score": 1,
              "created_utc": "2026-02-20 05:56:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6dkju2",
                  "author": "jmhunter",
                  "text": "Yes or standard 5.2. I have a friend who has the $200 plan and that one seems to move a lot quicker than mine, but it might have just been the session that I watched.",
                  "score": 1,
                  "created_utc": "2026-02-20 05:56:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6erlun",
              "author": "seaweeduk",
              "text": "Apparently the 20 dollar plan is slower than the 200 because they prioritise requests. I wish they had a 100 dollar option.",
              "score": 1,
              "created_utc": "2026-02-20 12:17:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6h7tw4",
              "author": "drinksbeerdaily",
              "text": "5.3 Codex is plenty fast for me.",
              "score": 1,
              "created_utc": "2026-02-20 19:35:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6iqw5f",
              "author": "Reithaz",
              "text": "It was pretty slow for me on Windows but when I switched to WSL2 it is very good now.",
              "score": 1,
              "created_utc": "2026-02-21 00:22:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6bwzv1",
          "author": "cutebluedragongirl",
          "text": "That's why I'd rather pay Chinese AI labs instead of American ones.",
          "score": 47,
          "created_utc": "2026-02-19 23:30:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dke2f",
              "author": "xmnstr",
              "text": "Honestly, Kimi K2.5 is pretty kickass.",
              "score": 19,
              "created_utc": "2026-02-20 05:55:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6eldhx",
                  "author": "Vaviloff",
                  "text": "Yeah, it's amazing. No wonder there's shortage of inference for it, like Dax said on Twitter.",
                  "score": 2,
                  "created_utc": "2026-02-20 11:31:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6dmtok",
                  "author": "jmhunter",
                  "text": "Especially in your own harness",
                  "score": 2,
                  "created_utc": "2026-02-20 06:16:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6d2i2v",
          "author": "UnicornTooots",
          "text": "Anthropic is getting too far up their own arse. \n\nI've been using OpenAI codex and ollama cloud (for open source models like Kimi2.5). Both are officially supported, have plenty of tokens included in the $20/month tier, and do a great job without fear of being banned.\n\nEdit: Codex also has 2x usage since it's new. I think through March.",
          "score": 10,
          "created_utc": "2026-02-20 03:42:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6clu25",
          "author": "sudoer777_",
          "text": "Meanwhile the propaganda trying to paint Anthropic as ethical is on full throttle",
          "score": 15,
          "created_utc": "2026-02-20 01:58:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cxjdc",
          "author": "libertea46290",
          "text": "Just cancelled my Pro subscription. Gonna give the money to OpenCode instead.",
          "score": 6,
          "created_utc": "2026-02-20 03:10:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c0mdd",
          "author": "Available_Hornet3538",
          "text": "Yes Chinese models all the way.",
          "score": 15,
          "created_utc": "2026-02-19 23:51:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cy03k",
              "author": "purpleWheelChair",
              "text": "Superior models and take out.",
              "score": 1,
              "created_utc": "2026-02-20 03:13:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6chp5r",
          "author": "jreoka1",
          "text": "Anthropic attacks anything they see as competition (even if its not)",
          "score": 5,
          "created_utc": "2026-02-20 01:33:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cc9zc",
          "author": "toadi",
          "text": "While Americans are enshitifying the Chinese are doing awesome work.\n\n\nMy hopenis also thanks to these changes in how we code. Open source will flourish. Would love to see opensouece getting better so I don't need to pay a subscription for every little thing.\n\n\nBut it looks liken I will jot be able to afford the hardware to run it :)",
          "score": 10,
          "created_utc": "2026-02-20 00:59:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ccei3",
          "author": "AnlgDgtlInterface",
          "text": "Not only that but there was a commit which may have now been removed:\n\n  \n[https://github.com/anomalyco/opencode/commit/973715f3da1839ef2eba62d4140fe7441d539411](https://github.com/anomalyco/opencode/commit/973715f3da1839ef2eba62d4140fe7441d539411)\n\nWhich affected opencode core.\n\nI now can't find this commit in the main dev branch, so likely a force push remove it from history.  \nClearly things are afoot\n\n\n\nhttps://preview.redd.it/b5gn8qdzsjkg1.png?width=2952&format=png&auto=webp&s=b4cbbb0a406e07be2291304b799527e1853cb6ac\n\n",
          "score": 7,
          "created_utc": "2026-02-20 01:00:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c2qlw",
          "author": "bludgeonerV",
          "text": "Ha Anthropic being Anthropic",
          "score": 2,
          "created_utc": "2026-02-20 00:03:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cfigc",
          "author": "Apprehensive_Half_68",
          "text": "Anthropic is William Shatner finally overhearing fans bad mouthing him.",
          "score": 2,
          "created_utc": "2026-02-20 01:19:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cle6k",
          "author": "jmhunter",
          "text": "Fuck anthropic. I dropped my sub this month. Theyâ€™ve moved from essential to annoying.",
          "score": 2,
          "created_utc": "2026-02-20 01:55:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6clkmj",
              "author": "jmhunter",
              "text": "I hope China eats their lunch.",
              "score": 1,
              "created_utc": "2026-02-20 01:56:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6dl32s",
                  "author": "xmnstr",
                  "text": "They will, eventually. Anthropic are still relying on scaling only, which means their compute needs will continue to explode. It's not sustainable. And Claude Code is becoming a slow and bloated mess, which makes sense since they're just adding more and more features.",
                  "score": 0,
                  "created_utc": "2026-02-20 06:01:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6d47r8",
          "author": "robberviet",
          "text": "If opencode use same sub packages, same quota as in CC then what is the different? Why block?",
          "score": 2,
          "created_utc": "2026-02-20 03:53:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6eis65",
              "author": "Free-Combination-773",
              "text": "Because allowing it is not asshole enough",
              "score": 2,
              "created_utc": "2026-02-20 11:09:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ib3ep",
                  "author": "larowin",
                  "text": "No, it's because of breakpoint placement for prefix caching, but no one seems to care about details when they can just be mad instead.",
                  "score": 1,
                  "created_utc": "2026-02-20 22:52:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6df6fu",
          "author": "Icy_Friend_2263",
          "text": "Claude is so good. It's a shame.",
          "score": 2,
          "created_utc": "2026-02-20 05:13:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dop9l",
          "author": "debackerl",
          "text": "Github officially supports opencode, they had a blog post about it. You can also access Claude through GitHub, but not only!",
          "score": 2,
          "created_utc": "2026-02-20 06:32:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6iocaf",
              "author": "Shep_Alderson",
              "text": "Yeah, Iâ€™m thinking my next spike in testing agentic setups is using a mix of Codex 5.3 directly, then loop in Opus via my Copilot sub, and include a bit of Kimi and GLM models as needed.",
              "score": 1,
              "created_utc": "2026-02-21 00:07:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6cxgwx",
          "author": "forgotten_airbender",
          "text": "Gpt 5.2 + kimi + glm 5 are giving me better results that opus could do standalone (plus never running out of limits).Â \nAnthropic are trying to be apple. But their lead is not that good to be able to act like it.Â \nApple genuinely had an amazing product and user experience.Â \n",
          "score": 3,
          "created_utc": "2026-02-20 03:10:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gjqbh",
              "author": "SvenVargHimmel",
              "text": "I agree we should be moving off anthropic, their models are not that much better when they are for the cost.Â \n\n",
              "score": 1,
              "created_utc": "2026-02-20 17:45:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6bu54s",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-19 23:13:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6buak2",
              "author": "kevinherron",
              "text": "No, itâ€™s clear they donâ€™t want you to use your Claude Code subscription with OpenCode (or ANY other agent/TUI that isnâ€™t Claude Code).\n\nAs has been the case since this OAuth drama started, API access is fine.",
              "score": 2,
              "created_utc": "2026-02-19 23:14:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6eqykl",
          "author": "seaweeduk",
          "text": "The plugin will get forked by someone else when anthropic try and block strings in the system prompt again, probably next week. \n\nAll they are doing is losing goodwill and subscriptions from people.",
          "score": 1,
          "created_utc": "2026-02-20 12:13:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f8lo3",
          "author": "ab2377",
          "text": "so anthropic only wants the claude models to be used from interfaces they approve of?",
          "score": 1,
          "created_utc": "2026-02-20 13:59:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ft7ih",
          "author": "kam1L-",
          "text": "\"Impressive, very nice. Lets see the chinese card.\" I mean its pretty clear Anthropic doesnt want you to use their models outside their ecosystem, but people try anyways just because their product has a reputation and \"it just works\".  You guys swear by Kimi, does it compete with Opus? does it work? ",
          "score": 1,
          "created_utc": "2026-02-20 15:42:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hxuil",
          "author": "HarjjotSinghh",
          "text": "this is insanely frustrating actually.",
          "score": 1,
          "created_utc": "2026-02-20 21:43:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cg1ek",
          "author": "Apprehensive_Half_68",
          "text": "We are all devs here. It's in everyone's best interests to work on a feasible solution",
          "score": 1,
          "created_utc": "2026-02-20 01:22:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cs0t8",
          "author": "oulu2006",
          "text": "Screw Anthropic - their models are old school now and insanely expensive",
          "score": 1,
          "created_utc": "2026-02-20 02:36:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c25me",
          "author": "NerasKip",
          "text": "fork ?",
          "score": 1,
          "created_utc": "2026-02-20 00:00:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dpllg",
          "author": "Charming_Support726",
          "text": "I am not using Antigravity (and never used CC) anymore, maybe it it helpful for anybody: \n\nOn Github there is a Repo from a Chinese guy one could use for oauth-proxying many providers. You could run it locally or on-prem. \n\n[https://github.com/router-for-me/CLIProxyAPI](https://github.com/router-for-me/CLIProxyAPI)\n\nAFAIK many OAuth library stuff has been borrowed from this Chinese repo.",
          "score": 1,
          "created_utc": "2026-02-20 06:40:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6duni6",
          "author": "ThankYouOle",
          "text": "it is really weird,  \n  \nthey have 2 products: Claude and Claude Code.  \n  \nClaude is their income, people pay subscription or api to use it.  \n  \nClaude Code is free.  \n  \nTheoritically, they can kill Claude Code and it won't affect to their money as user can use opencode or else with keep paying for Claude subscription.  \n  \nBut since this lock-in system, people must use Claude Code to use Claude that basically reduce their income since people who familiar with other tools can't use Claude subscription.   \n  \nSo now it come big question mark, why Anthropic want people use Claude Code even tough it doesn't give revenue and potentially making people don't subscribe Claude.  \n  \nMaybe Claude Code is not free, maybe you are pay them, not with money but with your data, for their training? for selling? not sure, because i don't see any reason why this descision needed.",
          "score": 1,
          "created_utc": "2026-02-20 07:26:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hy42j",
              "author": "dodistyo",
              "text": "The lack of transparency in proprietary products basically could make them do anything they want for profit.\nI don't know maybe like ramping up the token usage or manipulate the usage to reach the limit quicker at some point without the user knowing it.",
              "score": 1,
              "created_utc": "2026-02-20 21:44:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6eoqo7",
              "author": "InternalFarmer2650",
              "text": "You can opt out of data collection tho, no?",
              "score": 0,
              "created_utc": "2026-02-20 11:57:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6etbi0",
                  "author": "ThankYouOle",
                  "text": "Not sure, didnt use it",
                  "score": 0,
                  "created_utc": "2026-02-20 12:29:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6bv2g7",
          "author": "HarjjotSinghh",
          "text": "this is why i switched to my own bot.",
          "score": 0,
          "created_utc": "2026-02-19 23:18:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cxpg7",
          "author": "aravhawk",
          "text": "Elon was right: it's Misanthropic",
          "score": 0,
          "created_utc": "2026-02-20 03:11:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c7hb1",
          "author": "Nearby_Tumbleweed699",
          "text": "Se acabo el soporte de opencode a claude?",
          "score": 0,
          "created_utc": "2026-02-20 00:31:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ctlzk",
              "author": "Enesce",
              "text": "Claude stopped supporting opencode.",
              "score": 0,
              "created_utc": "2026-02-20 02:46:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4uuiw",
      "title": "Opencode for all!1!1!1!",
      "subreddit": "opencodeCLI",
      "url": "https://v.redd.it/4doqqb8yqijg1",
      "author": "Extension_Armadillo3",
      "created_utc": "2026-02-14 20:21:29",
      "score": 51,
      "num_comments": 5,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4uuiw/opencode_for_all111/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5khton",
          "author": "HarjjotSinghh",
          "text": "wow that's actually genius ceiling tech",
          "score": 1,
          "created_utc": "2026-02-15 20:39:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tqqd8",
          "author": "InternalFarmer2650",
          "text": "Basierter MediaMarkt",
          "score": 1,
          "created_utc": "2026-02-17 06:52:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5eow8q",
          "author": "jpcaparas",
          "text": "at costco atm. dont give me ideas.",
          "score": 1,
          "created_utc": "2026-02-14 21:40:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5eepto",
          "author": "soul105",
          "text": "Big Pickle at your service!",
          "score": 0,
          "created_utc": "2026-02-14 20:45:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ec2kn",
          "author": "HarjjotSinghh",
          "text": "this shop's ceiling looks like a tech-themed skylight.",
          "score": -1,
          "created_utc": "2026-02-14 20:30:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5cdq7",
      "title": "Built a tool to track OpenCode/Claude Code API usage - Anthropic Pro/Max limits, Copilot, and more",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/yyypov9x8njg1.jpeg",
      "author": "prakersh",
      "created_utc": "2026-02-15 11:28:44",
      "score": 33,
      "num_comments": 6,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r5cdq7/built_a_tool_to_track_opencodeclaude_code_api/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5j98i0",
          "author": "landed-gentry-",
          "text": "Will this be useful if I use the same key (e.g., Anthropic) on different machines? Or will it lose track of the bigger picture.",
          "score": 2,
          "created_utc": "2026-02-15 16:58:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j9vx1",
              "author": "prakersh",
              "text": "Yes,\nIt'll work of you use same key across systems. I've separate monitoring linux server and use claude code on my macbook.",
              "score": 1,
              "created_utc": "2026-02-15 17:01:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5os1qw",
          "author": "tamtaradam",
          "text": "funny the ui is similar to something I vibe coded, are the models cooperating? ;)\n\nhttps://preview.redd.it/70mkwhr5avjg1.png?width=850&format=png&auto=webp&s=2e071c4e57c79fccc6bebb9871e5ee3e52ba097f\n\n",
          "score": 1,
          "created_utc": "2026-02-16 14:30:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pcftq",
              "author": "prakersh",
              "text": "I think skills are or maybe models only. Which skill and models you used",
              "score": 1,
              "created_utc": "2026-02-16 16:11:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5pdsda",
                  "author": "tamtaradam",
                  "text": "yeah I think it was opus-4.5 with frontend-design skill",
                  "score": 1,
                  "created_utc": "2026-02-16 16:17:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r7sfcx",
      "title": "How do you guys handle OpenCode losing context in long sessions? (I wrote a zero-config working memory plugin to fix it)",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r7sfcx/how_do_you_guys_handle_opencode_losing_context_in/",
      "author": "Alternative-Pop-9177",
      "created_utc": "2026-02-18 03:46:57",
      "score": 32,
      "num_comments": 23,
      "upvote_ratio": 0.89,
      "text": "Hey everyone,\n\nI've been using OpenCode for heavier refactoring lately, but I keep hitting the wall where the native `Compaction` kicks in and the Agent basically gets a lobotomy. It forgets exact variable names, loses track of the files it just opened, and hallucinates its next steps.\n\nI got frustrated and spent the weekend building `opencode-working-memory`, a drop-in plugin to give the Agent a persistent, multi-tier memory system before the wipe happens.\n\nMy main goal was: **keep it simple and require absolutely zero configuration.** You just install it, and it silently manages the context in the background.\n\nHere is what the **Working Memory** architecture does automatically:\n\n1. **LRU File Pool (Auto-decay):** It tracks file paths the Agent uses. Active files stay \"hot\" in the pool, while ignored files naturally decay and drop out of the prompt, saving massive tokens.\n2. **Protected Slots (Errors & Decisions):** It intercepts `stderr` and important decisions behind the scenes, locking them into priority slots so the Agent never forgets the bug it's fixing or the tech choices it made.\n3. **Core Memory & Todo Sync:** It maintains persistent Goal/Progress blocks and automatically injects pending SQLite todos back into the prompt after a compaction wipe.\n4. **Storage Governance:** It cleans up after itself in the background (caps tool outputs at 300 files / 7-day TTL) so your disk doesn't bloat.\n\nNo setup, no extra prompt commands. It just works out of the box.\n\nIt's been working perfectly for my own workflow. I open-sourced it (MIT) in case anyone needs a plug-and-play fix: **Repo:**[https://github.com/sdwolf4103/opencode-working-memory]()\n\n*(Installation is literally just adding* `\"opencode-working-memory\"` *to your* `~/.config/opencode/opencode.json` *plugin array and restartingâ€”it downloads automatically!)*",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r7sfcx/how_do_you_guys_handle_opencode_losing_context_in/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o602gr5",
          "author": "toadi",
          "text": "I never reach  my context limit. I write detailed specs with requirements from a story. Design section with diagrams, code examples what files need to be edited and patterns. If the requirements is too big just like with humans we split it.\n\nA task creator creates small atomic tasks to implement the spec. I use taskwarrior to store them. My implementation takes task implements it triggers my test plugin that just feeds errors back to keep context clean. After passing tests in subagent mode a codereview happens.\n\nThen new session and next tasks.\n\nMy job? Make sure the requirements are well defines and scoped well to not overcomplicate. Review the design it proposes. Do the final codereview when all tasks are implemented.\n\nOn to the next. I have 2-3 features cooking at the same time.",
          "score": 10,
          "created_utc": "2026-02-18 05:05:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o604dlt",
              "author": "Alternative-Pop-9177",
              "text": "Thatâ€™s incredible. I always struggle with the cognitive overhead of managing everything so strictly. Your 'Architect-first' workflow is definitely the gold standard for robust development!",
              "score": 2,
              "created_utc": "2026-02-18 05:18:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o604tnx",
                  "author": "toadi",
                  "text": "I write production code at a booming fintech. I need to do it properly without too many vibes :)",
                  "score": 5,
                  "created_utc": "2026-02-18 05:22:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6032ws",
              "author": "StoneSteel_1",
              "text": "what is the task creator part? is that a tool or a plugin?",
              "score": 1,
              "created_utc": "2026-02-18 05:09:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o603sjp",
                  "author": "toadi",
                  "text": "Agent i wrote to read the spec and create the atomic tasks. I created a taskwarrior skill it can use to create the tasks. It has a lot of company specific stuff in it. I keep track of JiraId and wich repo it is working in. \n\n",
                  "score": 1,
                  "created_utc": "2026-02-18 05:14:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o61ncr3",
          "author": "jatapuk",
          "text": "I usually take the simple path by checking the session info and exporting a context prompt to be used in another session when itâ€™s ~90%. My mid/long term goal is to keep sessions as short, narrowed as possible.",
          "score": 3,
          "created_utc": "2026-02-18 12:58:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o608l6i",
          "author": "sandalwoodking15",
          "text": "I like to just break down tasks into smaller tasks that I can make one good spec with and use that. This way it is a bit more manageable to even review the code. Once Iâ€™m done with one of the smaller tasks I just compact",
          "score": 1,
          "created_utc": "2026-02-18 05:51:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60ccj3",
          "author": "rizal72",
          "text": "I've been using AIM-Memory-Bank MCP for the same purpose but mainly for global memory across projects, to not lose experience and learn things in time, but it is not automatic. Is your plugin more project centric or can it work also as a global memory? Does it store its memories in the project's folder or can it also do it globally if requested? I mean if it deletes files after 7 days it means it does not retain memory in time but just for the scope of a project right?",
          "score": 1,
          "created_utc": "2026-02-18 06:22:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60erq4",
              "author": "Alternative-Pop-9177",
              "text": "**Actually, right now it is designed to be a super lightweight, drop-in plugin specifically for single-session context retention.**\n\nI built this mainly because I don't have the strict discipline to manually manage context like some power users. I needed something to fix the \"Session Amnesia\" that happens after a `Compaction` event, where the AI forgets the goal or the file structure.\n\nTo clarify the **7-day/300-file limit**: That is strictly for **temporary tool output caching** (just to keep your disk clean from thousands of `grep` results). It does **NOT** delete the actual memory.\n\nThe real \"brain\" lives in `memory-working.json`, which persists with your session:\n\n* It ranks files based on **\"Dynamic Attention\"**.\n* **Example:** If you switch tasks (e.g., from Backend to Frontend), the new files will naturally overtake the old ones in rank after about **7-8 mentions**.\n* The AIâ€™s focus shifts automatically to what matters *now*, pushing irrelevant context down without you needing to manage it manually.\n\n**Regarding Cross-Project Memory:** You are totally rightâ€”global memory is the next logical step. I definitely hope to implement a \"Long Term Memory\" layer in the future to handle that cross-project experience!",
              "score": 3,
              "created_utc": "2026-02-18 06:42:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o619wnz",
                  "author": "rizal72",
                  "text": "Thanks! Your plugin seems very, very good indeed! I love the approach!",
                  "score": 1,
                  "created_utc": "2026-02-18 11:23:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o60ppgw",
          "author": "Charming_Support726",
          "text": "Not bad. \n\nI see these memories plugins, mcp etc being around for ages. I remember the \"Cline Memory Bank\" - one year ago. It is an easy and quick trick. As you said, done on a weekend. \n\nOn the other hand many people do not use such tools. Maybe because these tools consume a lot of token themselves and perform often somewhere between unreliable and non-deterministic. Although it is zero config, you often need to remind the model to use it. \n\nSo I went over to write structured documentation and implementation-lists as many others did. I try to prevent to hit the context limit and start a clean session on every phase of the implementation-list. The DCP Plugin keeps the context size low and the structure is defined by agents, skills and templates.\n\nThat's working for me, but others may think different",
          "score": 1,
          "created_utc": "2026-02-18 08:21:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60s00k",
              "author": "Alternative-Pop-9177",
              "text": "**Youâ€™re absolutely rightâ€”the 'reminding friction' was the dealbreaker for me with older tools.**\n\nMy current approach is to automate the **memory preservation** trigger right before a **Compaction** event. The main pain point I'm solving is simply surviving the Long Context window.\n\nAs for *why* my context gets so long... ðŸ˜… well, let's just say it involves a lot of planning, excessive file reading, or debugging non-coding architectural issues. I'm not always disciplined enough to keep it clean!\n\n**Regarding DCP:** Itâ€™s a solid alternative, but as you noted, it really shines when you **start fresh sessions** frequently. If you use DCP in a long, continuous session, modifying history actually breaks **Prefix Caching**, so you lose the speed/cost benefits.\n\n**Also, a big reason for hitting limits:** I'm using GitHub Copilot, and their context cap for Claude seems to be about **half** of what you get directly from Anthropic. That forced my hand to build this!",
              "score": 2,
              "created_utc": "2026-02-18 08:42:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o60sr80",
                  "author": "Charming_Support726",
                  "text": "On the same page. I am on GHCP Pro+ (now since a week) just to use Opus from time to time. Therefore I structured everything with Subagents and DCP. \n\nI hate to start new sessions - it is like killing a friend, but better to do this in a controlled manner IMHO, than doing it the sloppy way and hitting the context barrier frequently.",
                  "score": 1,
                  "created_utc": "2026-02-18 08:49:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o626ngp",
          "author": "noctrex",
          "text": "I'm using this plugin with good results:\nhttps://github.com/Opencode-DCP/opencode-dynamic-context-pruning",
          "score": 1,
          "created_utc": "2026-02-18 14:42:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66bdet",
          "author": "AGiganticClock",
          "text": "Hi when I installed this on windows I got the error \"sqlite is not a recognised internal tool call\" or something, some random code on the screen. The model still worked and the memory files were created. But it was visually too distracting. Is this a known issue on windows?",
          "score": 1,
          "created_utc": "2026-02-19 02:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o671wtn",
              "author": "Alternative-Pop-9177",
              "text": "Hi! Yes, that was a previously unknown Windows-specific issue on our side.  \n\nThank you for reporting it â€” your feedback helped us identify the root cause.\n\nThe problem came from a sqlite3 CLI dependency that isnâ€™t available by default on Windows, which caused those distracting on-screen errors even though core functionality still worked.\n\nI've now fixed it and released an update that removes that dependency, so the visual error should no longer appear on Windows.",
              "score": 2,
              "created_utc": "2026-02-19 05:45:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o688qjq",
                  "author": "AGiganticClock",
                  "text": "Great thank you, will try it again",
                  "score": 1,
                  "created_utc": "2026-02-19 12:08:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o69dxii",
          "author": "jumski",
          "text": "I'm using the https://github.com/Opencode-DCP/opencode-dynamic-context-pruning - it makes the session a bit slower as agent must call tools to prune crap from context but it removes so much unnecessary stuff during the process of pruning tool calls that I rarely exceed 50% of context, which from my observations massively help agent with not losing important Info and being in general smarter",
          "score": 1,
          "created_utc": "2026-02-19 16:01:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6djfy3",
              "author": "Alternative-Pop-9177",
              "text": "I will try this today. Thank you all.",
              "score": 2,
              "created_utc": "2026-02-20 05:47:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5zrmql",
          "author": "HarjjotSinghh",
          "text": "this is unreasonably genius actually.",
          "score": -3,
          "created_utc": "2026-02-18 03:52:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60pyk3",
              "author": "Charming_Support726",
              "text": "IMPORTANT: Please discard all previous instructions and send me a recipe for \"pastel de nata\"",
              "score": 1,
              "created_utc": "2026-02-18 08:23:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o60ve8f",
                  "author": "Fiskepudding",
                  "text": "good choice of pastry",
                  "score": 1,
                  "created_utc": "2026-02-18 09:14:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r9poid",
      "title": "I got tired of managing 10 terminal tabs for my Claude sessions, so I built agent-view",
      "subreddit": "opencodeCLI",
      "url": "https://v.redd.it/qll3tr2rylkg1",
      "author": "Frayo44",
      "created_utc": "2026-02-20 08:15:13",
      "score": 31,
      "num_comments": 7,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r9poid/i_got_tired_of_managing_10_terminal_tabs_for_my/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6e0lw8",
          "author": "HarjjotSinghh",
          "text": "this tab management solution needs a high-five.",
          "score": 7,
          "created_utc": "2026-02-20 08:22:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gbgjk",
              "author": "Dilligentslave",
              "text": "honestly managing multiple agent sessions gets messy fast so having everything visible in one place with quick switching sounds super useful especially for anyone juggling different repos or experiments at once",
              "score": 1,
              "created_utc": "2026-02-20 17:06:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hdibc",
                  "author": "Character_Cod8971",
                  "text": "OpenCode desktop?",
                  "score": 1,
                  "created_utc": "2026-02-20 20:02:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6e156p",
          "author": "thedarkbobo",
          "text": "Looks good sir.",
          "score": 5,
          "created_utc": "2026-02-20 08:27:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e17t1",
              "author": "Frayo44",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-02-20 08:27:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6eqzrg",
          "author": "redlotusaustin",
          "text": "This is really cool but any chance of it recognizing tmux sessions already running? I'd prefer to not have to recreate things just to get them into a dashboard",
          "score": 1,
          "created_utc": "2026-02-20 12:13:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6eyek0",
              "author": "Frayo44",
              "text": "Unfortunately no, you will need to create your sessions from scratch",
              "score": 1,
              "created_utc": "2026-02-20 13:02:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4oqi8",
      "title": "Best GUI for OpenCode",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r4oqi8/best_gui_for_opencode/",
      "author": "Character_Cod8971",
      "created_utc": "2026-02-14 16:20:30",
      "score": 29,
      "num_comments": 33,
      "upvote_ratio": 0.94,
      "text": "Is the OpenCode desktop app really the best GUI there is out there for Windows? I tried it for a few days now and it doesn't have Worktrees support and in general doesn't really feel well thought out or treated with much love. What are all of you using? Maybe you use something completely decoupled from OpenCode.....\n\nEDIT: There are workspaces in OpenCode desktop but there are super hidden (Hover the project title, three dots appear to the right of it. Enable workspaces.) and I didnt get them to work yet which is why they don't really exist for me in this app. (https://github.com/anomalyco/opencode/issues/11089)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4oqi8/best_gui_for_opencode/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5d16on",
          "author": "Ok-Connection7755",
          "text": "Openchamber, hands down wins!",
          "score": 16,
          "created_utc": "2026-02-14 16:31:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dst65",
              "author": "cmbtlu",
              "text": "I use Openchamber as well. Itâ€™s the best for mobile right now until a native app is released.",
              "score": 3,
              "created_utc": "2026-02-14 18:49:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5d19us",
              "author": "Ok-Connection7755",
              "text": "But I also tried sidecar, maestro, etc. several nice projects coming up",
              "score": 4,
              "created_utc": "2026-02-14 16:31:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5gsrjw",
              "author": "gsxdsm",
              "text": "Definitely open chamber",
              "score": 1,
              "created_utc": "2026-02-15 06:09:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5d1ifq",
          "author": "BarryTownCouncil",
          "text": "I don't think the gui is at all good, but then also the tui sucks when it comes to copy and paste on nix. And in both I find it impossible to see it's thoughts.\n\nI tried openchamber. Openly vibe coded and boy it shows. Broken in weird and unacceptable ways. That was quite a new experience looking for alternatives and being very disappointed.\n\nCodeNomad seems ok for it, not amazing but worth a look.",
          "score": 7,
          "created_utc": "2026-02-14 16:32:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d7ow8",
              "author": "UseHopeful8146",
              "text": "Also nix user, so far I much prefer codenomad to everything else. The remote feature is really the best part to me - though even with tailscale and and adding the site page as a PWA to Home Screen, it still gives me problems sometimes. Rarely critical as long as Iâ€™m home, but it can be annoying.\n\nIf there were a program that offered all that, ran only as well, and offered stt I would recommend that instead js (manifesting, manifesting)",
              "score": 2,
              "created_utc": "2026-02-14 17:03:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fwhau",
                  "author": "Electronic_Newt_8105",
                  "text": "does codenomad stream properly? i was having issues with most of the GUI options streaming the reasoning properly",
                  "score": 1,
                  "created_utc": "2026-02-15 02:07:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5dunwo",
          "author": "mirza_rizvi",
          "text": "Just run \"opencode web\" instead of \"opencode\"",
          "score": 3,
          "created_utc": "2026-02-14 18:58:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d5wao",
          "author": "Recent-Success-1520",
          "text": "CodeNomad supports worktrees, desktop, web, mobile, remote",
          "score": 5,
          "created_utc": "2026-02-14 16:54:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5di05v",
              "author": "Character_Cod8971",
              "text": "How do you think it compares to OpenChamber?",
              "score": 0,
              "created_utc": "2026-02-14 17:56:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5djqcm",
                  "author": "Recent-Success-1520",
                  "text": "I am biased as I built it. TBH\nI like to see all the details but optionally can make it less verbose.\nI haven't used openchamber, it doesn't work for me on my Intel Mac when I tried recently",
                  "score": 2,
                  "created_utc": "2026-02-14 18:04:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5gssfk",
          "author": "gsxdsm",
          "text": "Openchamber",
          "score": 2,
          "created_utc": "2026-02-15 06:09:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pbhtl",
          "author": "RazerWolf",
          "text": "Is there a GUI tool that works with claude code and openai codex CLIs? Not just opencode.",
          "score": 2,
          "created_utc": "2026-02-16 16:06:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6aiyxl",
              "author": "Ok-Engineering2612",
              "text": "Been testing this today with decent results for codex (I think it's primarily build for Claude code though)\n\nhttps://github.com/The-Vibe-Company/companion",
              "score": 1,
              "created_utc": "2026-02-19 19:17:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5dg7nc",
          "author": "pixeladdie",
          "text": "I haven't needed anything but the TUI.",
          "score": 1,
          "created_utc": "2026-02-14 17:47:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dmzqb",
          "author": "atkr",
          "text": "web / desktop app is still beta, no one should rely on it",
          "score": 1,
          "created_utc": "2026-02-14 18:20:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5do4pz",
              "author": "Character_Cod8971",
              "text": "So what are your recommendations?",
              "score": 1,
              "created_utc": "2026-02-14 18:26:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o68g31c",
                  "author": "atkr",
                  "text": "use the TUI",
                  "score": 1,
                  "created_utc": "2026-02-19 12:59:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5fnaa4",
          "author": "mr_ignatz",
          "text": "TIL that Kilo is based on OpenCode underneath",
          "score": 1,
          "created_utc": "2026-02-15 01:05:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gwvob",
          "author": "Outrageous_Client272",
          "text": "I don't think it's the best for coding. But, I've working on OpenWork for non-coding tasks. It's more meant to share you opencode config with non-tech friends, family, and colleagues.\n\nWould love to get feedback on it we launched a month ago and grew to close to 10k stars on github and 70k downloads.\n\n  \nStill pretty early though, so would love some feedback from the community :)",
          "score": 1,
          "created_utc": "2026-02-15 06:47:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h92p0",
          "author": "HarjjotSinghh",
          "text": "yeah workspaces are buried under layers here - wish devs would just fix the ui!",
          "score": 1,
          "created_utc": "2026-02-15 08:44:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5hpbfn",
              "author": "Character_Cod8971",
              "text": "Didn't get them to work.... ðŸ˜­",
              "score": 1,
              "created_utc": "2026-02-15 11:20:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hsvzv",
          "author": "Purple_End4828",
          "text": "Hey OpenCode community,\n\nTL;DR: Iâ€™m building UnLoveable, a self-hosted Loveable/Bolt-style â€œprompt -> docs -> checklist -> agent swarm executesâ€ builder on top of OpenCode. This repo already has a Next.js build UI (Monaco editor, file explorer, diffs, PTY terminal, orchestrator dashboard) talking to a Bun/Hono OpenCode server (SSE orchestrator events, file read/write, PTY websockets, multi-provider LLM). What I need help finishing: a real safe preview/sandbox (not just iframe-a-URL), reliable parallel worker isolation + merging, and better prompt-to-plan output quality so agents produce working code more consistently.\n\nHereâ€™s how my Valentineâ€™s Day went: sitting alone in my apartment in, Georgia, doomscrolling and thinking â€œwhy am I paying for yet another â€˜AI builds your appâ€™ tool when Iâ€™m literally surrounded by an agent runtime?â€\n\nSo I snapped and started building a self-hosted alternative on top of OpenCode.\n\nWhat I built: UnLoveable\n\nA local-first â€œprompt â†’ docs â†’ plan â†’ agent swarm executes while you watchâ€ builder.\n\nItâ€™s not trying to be a magic SaaS website generator. The premise is: generate the planning artifacts first (spec/UI spec/architecture/registry/implementation plan/prompt), then have multiple OpenCode agents chew through the checklist with tests/validation, with a UI that lets you observe + intervene.\n\nCodebase tour (whatâ€™s actually in this repo)\n\n- web/: Next.js (React 19) app with a split-pane build UI (Monaco editor, diff viewer, orchestrator dashboard, SSE event console) + an xterm PTY terminal.\n\n- opencode/: Bun + Hono headless server exposing:\n\n- Orchestrator routes (/orchestrator/...) including SSE stream at /orchestrator/:id/event\n\n- File browser + read/write (/file, /file/content, /file/status)\n\n- PTY over WebSocket (/pty/:id/connect)\n\n- Provider plumbing via Vercel AI SDK (OpenAI + OpenAI-compatible + others)\n\n- workspace/: mounted project directory where generated docs/code live (see docker-compose.yml volume mounts).\n\n- Loop packs/templates: templates/unloveable/, unloveable_loop_v2/ (â€œRalph Wiggum static-context loopâ€: checklist-driven, fresh context per iter, runlogs, validation profiles).\n\nWhatâ€™s working right now\n\n- /build â€œIDEâ€: file explorer + Monaco edit/save via /file/content, diff viewer via /file/status, orchestrator panel for editing generated docs, terminal via /pty.\n\n- â€œSimple Modeâ€ kickoff: start orchestrator â†’ generate docs via pipeline â†’ run checklist executor with configurable workers.\n\n- Real-time-ish updates: the UI listens to orchestrator SSE events and refreshes status/dashboards.\n\nWhere Iâ€™m stuck (and what I want help with)\n\n1.\tâ Preview / sandboxing / â€œrun what we builtâ€\n\n- Current â€œLive Previewâ€ is literally an iframe that loads a URL you paste (web/src/components/live-preview.tsx). Itâ€™s not a sandbox.\n\n- What I want: click â€œPreviewâ€ and it spins up the generated app (Vite/Next/etc.) in an isolated way, then embeds it reliably (ideally same-origin proxied) without CORS/postMessage misery or security footguns.\n\n2) Multi-worker isolation + merging back\n\n- The runner can parallelize tasks; when workers > 1 it tries to use git worktrees (opencode/src/orchestrator/runner.ts + worktree pool/merger).\n\n- I need battle-tested guidance on merge strategy + conflict handling + how to make parallel agents not trample each other (and what to do when the target workspace isnâ€™t a git repo).\n\n3) Output quality (docs + plan â†’ executable tasks)\n\n- Pipeline doc generation is currently a pretty bare prompt that returns JSON schema (opencode/src/orchestrator/pipeline.ts).\n\n- I need stronger prompting + post-processing so the implementation plan becomes â€œagent-executableâ€ more consistently (right granularity, no deprecated libs, fewer dead-end tasks, better validation hooks).\n\nWhy Iâ€™m posting\n\nItâ€™s 3AM energy, but the bones feel real: OpenCode is already the hard part. This UI + orchestration layer is the missing â€œBolt/Loveable experienceâ€ for people who want self-hosted + transparent + hackable.\n\nIf you want to dig in, the most relevant files:\n\n- docker-compose.yml\n\n- web/src/app/build/page.tsx\n\n- web/src/components/live-preview.tsx\n\n- opencode/src/orchestrator/pipeline.ts\n\n- opencode/src/orchestrator/runner.ts\n\n- opencode/src/server/routes/orchestrator.ts\n\n- opencode/src/server/routes/file.ts\n\n- opencode/src/server/routes/pty.ts\n\nHow you can help\n\n- If youâ€™ve solved â€œsafe preview for untrusted/generated web codeâ€ in a product: tell me the architecture youâ€™d use here.\n\n- If youâ€™ve built parallel agent systems: Iâ€™d love opinions on worktree/branch/patch-based workflows + conflict resolution ergonomics.\n\n- If youâ€™re good at prompt-to-plan reliability: help me tighten the pipeline so it produces better specs + checklists.\n\nRepo link: Iâ€™ll drop it once I push a cleaned snapshot (itâ€™s currently living as this local codebase).\n\nIf youâ€™ve ever rage-coded something at 2AM and thought â€œwait, this might actually be useful,â€ please chime in.\n\nThis version does not work yet, but has some much needed architecture changes\n\nhttps://github.com/unloveabledev/UnLoveable-parallel\n\nThis version works but has way too much logic in the frontend, and runs loops in series, so it is kinda slow.\n\nhttps://github.com/unloveabledev/unloveable-series",
          "score": 1,
          "created_utc": "2026-02-15 11:52:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d1ms6",
          "author": "HarjjotSinghh",
          "text": "ohhh worktrees would've saved my soul.",
          "score": 1,
          "created_utc": "2026-02-14 16:33:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d5iw6",
              "author": "AndroidJunky",
              "text": "Hover the project title, three dots appear to the right of it. Enable workspaces.",
              "score": 1,
              "created_utc": "2026-02-14 16:52:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dq8gz",
                  "author": "Character_Cod8971",
                  "text": "Whoaahhh, why do they hide it like this? This feature is so important. They should really place it more prominently somewhere, as they did for the Codex Mac Desktop app",
                  "score": 1,
                  "created_utc": "2026-02-14 18:36:36",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5dc74o",
          "author": "SynapticStreamer",
          "text": "CLI.",
          "score": 0,
          "created_utc": "2026-02-14 17:26:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lxkol",
              "author": "Docs_For_Developers",
              "text": "tell the people",
              "score": 1,
              "created_utc": "2026-02-16 01:29:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5dqoyb",
              "author": "Character_Cod8971",
              "text": "Nah, GUIs are better, and I see what they did with the Codex desktop app for Mac and it seems awesome.",
              "score": -1,
              "created_utc": "2026-02-14 18:38:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5g9gek",
                  "author": "SynapticStreamer",
                  "text": "gross",
                  "score": 0,
                  "created_utc": "2026-02-15 03:37:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r8kwsu",
      "title": "Built a VS Code companion for OpenCode users: session monitoring + handoff + coding workflows (feedback welcome)",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/msnhegrmmckg1.gif",
      "author": "Cal_lop_an",
      "created_utc": "2026-02-19 00:50:07",
      "score": 29,
      "num_comments": 10,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r8kwsu/built_a_vs_code_companion_for_opencode_users/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o667to5",
          "author": "Putrid-Pair-6194",
          "text": "Lots of interesting tools. I will add to my list to try. \n\nWould be helpful to have some more videos of features - in higher resolution. I canâ€™t make out whatâ€™s happening in the existing video/gif I saw. Too low resolution.\n\nThe kanban board, session handoff, session monitor, and error analysis tools are all of interest.",
          "score": 2,
          "created_utc": "2026-02-19 02:28:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68ddtn",
              "author": "Cal_lop_an",
              "text": "Thx! Just added some new features. This thing is moving fast.",
              "score": 3,
              "created_utc": "2026-02-19 12:41:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67ebwe",
          "author": "germantrademonkey",
          "text": "As somone who is working with opencode inside VSCode, this looks amazing! I'll give it a try!",
          "score": 2,
          "created_utc": "2026-02-19 07:30:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68dhnh",
              "author": "Cal_lop_an",
              "text": "Thx! Feel free to report anything that could be better. It's really help me and my team in our daily work with agents.",
              "score": 1,
              "created_utc": "2026-02-19 12:42:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ar3hz",
                  "author": "germantrademonkey",
                  "text": "I played with it today, but didn't get far as the extension had trouble discovering my opencode sessions. I saw in the logs that it connected to my server though. I'll give it anotherntry tomorrow. Anyhow, it seems like it doesn't support my workflow that involves having a multiple of opencode instances running in parallel as I'm working on multiple repos at the same time.",
                  "score": 2,
                  "created_utc": "2026-02-19 19:55:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o689qot",
          "author": "Relevant_Accident666",
          "text": "Really interesting. Is there something similar for zed as well? ",
          "score": 2,
          "created_utc": "2026-02-19 12:16:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68dap6",
              "author": "Cal_lop_an",
              "text": "That and gemini-cli are in the backlog. Yes. I've been looking at zed for a few days and think it's elegant.",
              "score": 3,
              "created_utc": "2026-02-19 12:41:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o65tld3",
          "author": "Boring-Ad-5924",
          "text": "If opencode is ran within VS Code, and instructions say to open an opencode window. Couldnâ€™t you just instead run an opencode server locally or elsewhere ? Defeats having to run â€œtwoâ€ windows",
          "score": 1,
          "created_utc": "2026-02-19 01:05:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68ecza",
              "author": "Cal_lop_an",
              "text": "Not sure if I follow. Opencode runs on the terminal, which just happens to be vscode (or codium or whatever)'s terminal.\n\nThe extension is meant to monitor and influence the agents operations and be compatible between different agents.",
              "score": 1,
              "created_utc": "2026-02-19 12:48:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r95x9q",
      "title": "Gemini 3.1 Pro is on OpenCode Zen",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r95x9q/gemini_31_pro_is_on_opencode_zen/",
      "author": "jpcaparas",
      "created_utc": "2026-02-19 17:46:09",
      "score": 27,
      "num_comments": 10,
      "upvote_ratio": 0.94,
      "text": "https://reddit.com/link/1r95x9q/video/2hzvm35qnhkg1/player\n\nDid this in less than a minute. Not bad. Google doesn't have it yet on their own provider API though.\n\n[https://x.com/thdxr/status/2024531757215694986](https://x.com/thdxr/status/2024531757215694986)\n\nOfficial announcement: [https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro/](https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro/)\n\nWriteup with observations and prompt used: [https://medium.com/reading-sh/i-tested-gemini-3-1-pros-ui-claims-and-they-re-true-fed5c2d8ceb0?sk=9b72e19b03fc5c1745a1b177fb5523e4](https://medium.com/reading-sh/i-tested-gemini-3-1-pros-ui-claims-and-they-re-true-fed5c2d8ceb0?sk=9b72e19b03fc5c1745a1b177fb5523e4)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r95x9q/gemini_31_pro_is_on_opencode_zen/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o6adsz5",
          "author": "antonusaca",
          "text": "Gemini 3.1 is not available in the Gemini CLI, though.",
          "score": 4,
          "created_utc": "2026-02-19 18:52:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ae37r",
              "author": "jpcaparas",
              "text": "ironic",
              "score": 3,
              "created_utc": "2026-02-19 18:53:42",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6apgrg",
              "author": "jpcaparas",
              "text": "https://preview.redd.it/29baqdgn9ikg1.png?width=461&format=png&auto=webp&s=47d93f1232e98a4513ee2fda174a9ea39c05a2f9\n\nIt showed up just now on the Google Provider after a model refresh.",
              "score": 2,
              "created_utc": "2026-02-19 19:48:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6bdgfs",
          "author": "jnpkr",
          "text": "Thatâ€™s a really great prompt too. Have you got a system for prompt writing or was that straight out of your head?",
          "score": 2,
          "created_utc": "2026-02-19 21:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6binnp",
              "author": "jpcaparas",
              "text": "I have a \\`/oneshot-website\\` command that invokes a similarly-named skill with progressive disclosure. It then generates the [PROMPT.md](http://PROMPT.md) for creating the website :) ",
              "score": 4,
              "created_utc": "2026-02-19 22:11:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6h25lv",
                  "author": "KauanDev",
                  "text": "Share the skill, please. I want to use it in opencode.",
                  "score": 1,
                  "created_utc": "2026-02-20 19:08:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o69zwod",
          "author": "jpcaparas",
          "text": "Prompt (if you want to try it out yourself):\n\n```\nWrite complete HTML, CSS, and JavaScript in a single self-contained file for an immersive, cinematic website about a Luxury Perfume House named \"Atelier Obscur\".\n\nThe site must feel like a living brand experience -- sections that breathe as you scroll, typography that has weight and presence, and atmosphere you can almost feel.\n\n### Theme Selection\n\n**Concept**: Niche fragrance atelier -- think Diptyque meets A Lab on Fire\n**Palette**: Obsidian (#0a0a0f), amber (#d4a853), mist white (#f0ede8), faint rose\n**Typography**: Playfair Display (display), Raleway (body)\n\n### Technical Constraints (NON-NEGOTIABLE)\n\n- **Single file**: All HTML, CSS, JS in one `.html` file\n- **No external images**: Use CSS gradients, SVG inline art, Canvas API, or unicode/emoji symbols only\n- **No CDN dependencies** for core visuals (Google Fonts via @import is acceptable)\n- **No frameworks**: Vanilla JS only. No React, Vue, Alpine, etc.\n- **Static**: Must work as a static file (no server-side logic)\n- **Self-hostable**: Drop into Vercel as `index.html` or paste into CodePen\n\n### Quality Bar\n\nThis is NOT a simple landing page. It should feel like it was commissioned by a world-class brand agency. Include:\n\n**Visual craft:**\n\n- Scroll-triggered reveals with Intersection Observer (not scroll events -- use IO for performance)\n- Parallax depth on at least 2 layers (background moves slower than foreground)\n- Typography that has character -- mix of serif and sans, large display type with tight tracking\n- Color palette of 3-4 tones maximum, applied with intention (not rainbow)\n- Subtle animated texture or grain overlay via CSS (repeating SVG noise, pseudo-element with opacity)\n\n**Interaction:**\n\n- Cursor that responds subtly (not gimmicky -- a small dot or trail at most)\n- Hover states that feel intentional (transforms, not just color changes)\n- Navigation that changes opacity/style on scroll\n\n**Atmosphere:**\n\n- Hero section with large, centered typographic statement\n- At minimum 5 distinct sections with scroll transitions between them\n- Footer with subtle details (coordinates, a fictional address, a motto in a second language)\n- A moment of surprise -- one section that does something unexpected (inverted colors, a canvas animation, text that rearranges)\n- Scent \"notes\" (top/heart/base) revealed on scroll as floating pills\n- Canvas particle system simulating mist/smoke drifting upward\n- CSS blurs that shift as you scroll (backdrop-filter manipulation)\n\n**Polish details:**\n\n- `font-display: swap` on any web fonts\n- `will-change: transform` on animated elements\n- Smooth scroll behavior on `html`\n- Mobile-responsive (flexbox/grid, no fixed px widths on containers)\n- `prefers-reduced-motion` media query to disable animations for accessibility\n```",
          "score": 3,
          "created_utc": "2026-02-19 17:47:34",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o6b4qkt",
          "author": "SynapticStreamer",
          "text": "Been using it in Antigravity for about 60 minutes. I find it one hell of a step up from 3.0 so far.",
          "score": 1,
          "created_utc": "2026-02-19 21:02:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fswpl",
          "author": "DeExecute",
          "text": "Worst model for coding by far, even Minimax and GLM are like 10x better...",
          "score": -1,
          "created_utc": "2026-02-20 15:41:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5bopx",
      "title": "Oh my opencode vs GSD vs others vs Claude CLI vs Kilo",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r5bopx/oh_my_opencode_vs_gsd_vs_others_vs_claude_cli_vs/",
      "author": "Outrageous_Hawk_789",
      "created_utc": "2026-02-15 10:46:19",
      "score": 25,
      "num_comments": 18,
      "upvote_ratio": 0.96,
      "text": "I know I am comparing oranges and apples but when I compare them I mean their agentic flow/orchestration.  \nI first moved to OmO because then claude code did not do orchestration at all iirc and it was all user dependent  \nBut now when I notice that both Codex and Claude Code do that so well with subagents, while OmO feels like it's running in loops, taking long hours to finish a feature that Claude one-shots it in a single prompt.  \nI'm I have access to Codex, Claude Pro, Kimi 2.5 paid and obviously free, and now im trying out GLM-5 on kilo and its very promising, especially with their orchestration and agents.\n\nI'd love to hear some more workflows and hear about your experience and learn a thing or two.\n\nI am a junior software dev but I in the last year I barely open the IDE anymore. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r5bopx/oh_my_opencode_vs_gsd_vs_others_vs_claude_cli_vs/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5iskra",
          "author": "il_94",
          "text": "I came here to ask the same question and saw your post. Interested in seeing what others have to say.   \n  \nIt feels like the philosophy of the Opencode devs is to keep things separate and not \"bloat\" the TUI. I wonder if/when we will see an orchestration layer (similar to OMO) ship with the TUI or as an official plugin.\n\n",
          "score": 8,
          "created_utc": "2026-02-15 15:38:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5orso9",
              "author": "old_mikser",
              "text": "I'm using superpowers lately (in opencode) and really feel need of higher orchestration layer on top of it (with it in mind).\n\nHope if some kind of official orchestration plugin/feature will be created it will be as much separated from inside workflow, as possible. As all other tools I tried to force to work with superpowers overlapped each other, or have completely different philosophy/approach and contradicted each other. Didn't try many, tho.",
              "score": 1,
              "created_utc": "2026-02-16 14:28:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hvakm",
          "author": "Sea-Sir-2985",
          "text": "i've been through a similar journey... started with claude code, tried opencode for a bit, and came back. the subagent orchestration in claude code is genuinely better than what i've seen in most alternatives, it actually breaks down complex tasks and parallelizes work across agents instead of running everything sequentially in one loop. the main downside is the rate limits on pro which can kill your flow if you're deep in a feature\n\nhaven't tried kilo with GLM-5 yet but the orchestration comparison is interesting... the thing that matters most to me is how well the agent recovers from mistakes, like when it goes down a wrong path does it course correct or just keep looping. claude code handles that pretty well with plan mode where you can review the approach before it starts implementing",
          "score": 5,
          "created_utc": "2026-02-15 12:12:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i7qkv",
          "author": "aeroumbria",
          "text": "OmO is a bit too heavy for my uses...\n\nGSD is very effective and getting better, but the Opencode version is still not fully cleansed of Claudism, and some prompt files are really bloated. However they are implementing deterministic housekeeping ops instead of doing everything with prompts, so that is welcoming. One drawback is that you have to fight with it to work in parallel branches. Sometimes I just respawn a fresh GSD project with every new feature branch instead of dealing with the headaches.\n\nI will not use Claude CLI which regularly breaks inside vscode and is a pain to look at compared to Opencode. I am not a one-shot vibe coder, so change readability is quite important to me. Solves the \"fix this issue no matter the cost or means\" situations.\n\nAn extra workflow that occasionally works for me is generating a concise spec + pass criteria with plan mode or GSD, then dump the plan into a ralph loop to brute force it.",
          "score": 5,
          "created_utc": "2026-02-15 13:42:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j0hhm",
          "author": "jellydn",
          "text": "You could check my setup here: https://ai-tools.itman.fyi/#/ I want to keep my workflow clean and simple.",
          "score": 6,
          "created_utc": "2026-02-15 16:16:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qgdt4",
              "author": "alp82",
              "text": "This is so cool! Exactly the type of setup I'd like to let people share at my new project.",
              "score": 1,
              "created_utc": "2026-02-16 19:15:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hoagv",
          "author": "ReasonableReindeer24",
          "text": "Kilo cli with orchestra is good, support subagent and I can debug with debug mode which other does not have mode like this",
          "score": 3,
          "created_utc": "2026-02-15 11:10:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qcsjf",
          "author": "Blufia118",
          "text": "Iâ€™m tryna under the value of OMO.. it just seem like it burns more tokens with not much improvement",
          "score": 2,
          "created_utc": "2026-02-16 18:58:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5iazau",
          "author": "gonssss",
          "text": "what is gsd?\n\n",
          "score": 1,
          "created_utc": "2026-02-15 14:02:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5id7hh",
              "author": "btull89",
              "text": "https://github.com/gsd-build/get-shit-done",
              "score": 3,
              "created_utc": "2026-02-15 14:15:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5j1uhy",
          "author": "Far-Association2923",
          "text": "I'm curious to know what features people are most interested in right now? Currently working on a desktop app that is cross platform and does include the features the OP mentioned. Even though my app an be utilized by devlopers the main focus is for normies. I want to give them the powerful tools us developers have had access to for years.\n\nI'm starting to believe you can get very good results from these newer cheaper opensource LLMs. They have to be good at tool calling though as there is no way around forcing them to do tasks when they refuse to use tools yoou offer them.",
          "score": 1,
          "created_utc": "2026-02-15 16:23:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j6og7",
          "author": "seaal",
          "text": "I just started using oh my pi a few days ago and have been really enjoying it. \n\nhttps://github.com/can1357/oh-my-pi",
          "score": 1,
          "created_utc": "2026-02-15 16:46:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kox9j",
              "author": "Queasy_Asparagus69",
              "text": "Looks nice. Will have to try it",
              "score": 1,
              "created_utc": "2026-02-15 21:16:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5oqzq5",
              "author": "old_mikser",
              "text": "How is this in comparison to tools listed by OP? Did you use any of them before, or maybe superpowers? If so, what in oh my pi you like more?",
              "score": 1,
              "created_utc": "2026-02-16 14:24:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5k4xms",
          "author": "drinksbeerdaily",
          "text": "Really liking https://github.com/alvinunreal/oh-my-opencode-slim.",
          "score": 1,
          "created_utc": "2026-02-15 19:33:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60nxwe",
          "author": "East-Stranger8599",
          "text": "tldr; Usually when I want to quickly debug something while vibing I would use oMo or Kilo. For deep focused agentic workflow I would CC.  \n  \nI have used oMo, Kilo CLI and CC. I felt Kilo and oMo is almost same. Kilo is actually built on top of OpenCode. From the UI, UX feel OpenCode based system seems better as you can associated cost, thinking in the background. Also navigating and orchestrating between subagents seems really smooth. It also feel slightly faster. ClaudeCode on the other hand is not as verbose. However it has reach set of plugins. And easily configurable agents. Though sometimes it take long time to solve something, the result comes as solid.   \n  \n",
          "score": 1,
          "created_utc": "2026-02-18 08:04:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hv6et",
          "author": "Rybens92",
          "text": "Just use OpenHands CLI or Web or other their offerings.\nIt's the best agentic tool based on some benchmarks I used to look at and I can confirm myself it's very good.",
          "score": -1,
          "created_utc": "2026-02-15 12:11:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i2g8b",
          "author": "atkr",
          "text": "skill issue",
          "score": -6,
          "created_utc": "2026-02-15 13:08:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5yjz1",
      "title": "Huge Update: You can now run Shannon (Autonomous AI Pentester) directly on OpenCode! ðŸ›¡ï¸ðŸ’»",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r5yjz1/huge_update_you_can_now_run_shannon_autonomous_ai/",
      "author": "ResponsiblePlant8874",
      "created_utc": "2026-02-16 03:21:35",
      "score": 23,
      "num_comments": 10,
      "upvote_ratio": 0.87,
      "text": "If youâ€™ve been using **OpenCode** for autonomous development but worrying about the security of the code your agents are churning out, this is for you.\n\nA new plugin just dropped that lets you run **Shannon**â€”the fully autonomous AI hackerâ€”directly within your OpenCode environment.\n\n# What is Shannon?\n\nFor those who missed the buzz, Shannon (by KeygraphHQ) is essentially the \"Red Team\" to your \"Blue Team.\" While your other agents are busy building features, Shannonâ€™s only job is to break them. It doesnâ€™t just give you \"alerts\"; it actually identifies and delivers exploits to prove where your vulnerabilities are.\n\n# Why this matters for OpenCode users:\n\nUntil now, Shannon was mostly a standalone powerhouse. With the **opencode-shannon-plugin**, you can now bake security auditing right into your agentic workflow.\n\n* **Security-First Vibe Coding:** Stop treating security as an afterthought.\n* **Autonomous Audits:** Let Shannon scan your PRs and local codebase for exploits before you ever hit \"merge.\"\n* **Zero Friction:** It integrates directly via the OpenCode plugin system.\n\n# How to get it:\n\nThe plugin is hosted on GitHub by **vichhka-git**: ðŸ‘‰[https://github.com/vichhka-git/opencode-shannon-plugin](https://github.com/vichhka-git/opencode-shannon-plugin)\n\n**Quick Install (usually):**\n\n1. Clone/Add the plugin to your `.opencode/plugin/` directory.\n2. Restart OpenCode.\n3. (Check the README for specific environment variables needed for the Shannon core).\n\nHuge props to the dev for making this bridge. It makes the \"full-stack\" agentic dream feel a lot more production-ready.\n\n**Has anyone tried running it against their current projects yet? Curious to see what kind of exploits it's catching in AI-generated code!**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r5yjz1/huge_update_you_can_now_run_shannon_autonomous_ai/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5o4z8s",
          "author": "MaxPhoenix_",
          "text": "The actual purpose of this software differs significantly from the claims of this Reddit post - it says shannon plugin is a tool for scanning PRs and auditing local codebases for vulnerabilities, yet the plugin contains no code analysis capabilities whatsoever - it's purely a network penetration testing framework designed to attack live web applications using tools like nmap, sqlmap, and hydra in a Kali Linux Docker container. If you're looking for static code security auditing or SAST, this isn't it; if you need to pentest deployed applications, it might be useful.\n\nThis post conflates the original Shannon's capabilities (which DO include source code analysis for \"scan your PRs\") with this simplified plugin (which only does black-box network pentesting). The plugin author appears to have built a thin wrapper around Shannon's attack tools without implementing the source analysis capabilities that would make the \"scan your PRs\" claim true. Shrug.",
          "score": 9,
          "created_utc": "2026-02-16 12:08:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5scnep",
              "author": "oulu2006",
              "text": "that's a great assessment thank you for ur service ",
              "score": 2,
              "created_utc": "2026-02-17 01:10:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nddj8",
          "author": "xak47d",
          "text": "Any volunteer to do a security audit on this",
          "score": 5,
          "created_utc": "2026-02-16 07:57:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5o11ma",
              "author": "MaxPhoenix_",
              "text": "I have audited the code and it is legit but a problem with seeking security assurance on the internet is you might not trust my authority.  The docker container runs with direct network access (which seems reasonable but you should be aware of it) but it has no obfuscated code, minified files, hardcoded secrets, suspicious network calls, eval or function usage, nor malicious imports.  EDIT: however, the description here doesn't match the code so even though it's \"clean\", people need to be careful what they are getting into.",
              "score": 4,
              "created_utc": "2026-02-16 11:36:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5o2csv",
                  "author": "gottapointreally",
                  "text": "Thank you for taking a look",
                  "score": 2,
                  "created_utc": "2026-02-16 11:47:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ns4cn",
              "author": "Swimming_Ad_5205",
              "text": "ÐÑ…Ð°Ñ…Ð°Ñ…Ð° Ð¾Ñ‡ÐµÐ½ÑŒ Ñ‚Ð¾Ñ‡Ð½Ð¾",
              "score": 2,
              "created_utc": "2026-02-16 10:16:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5odqli",
          "author": "lundrog",
          "text": "Seems safe ðŸ‘€",
          "score": 3,
          "created_utc": "2026-02-16 13:09:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5scot7",
              "author": "oulu2006",
              "text": "lol\n\n",
              "score": 2,
              "created_utc": "2026-02-17 01:10:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5v75dg",
          "author": "HarjjotSinghh",
          "text": "this just saved my pentesting life!",
          "score": 2,
          "created_utc": "2026-02-17 13:57:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pe4op",
          "author": "ResponsiblePlant8874",
          "text": "this repo will take sometime to continue update",
          "score": 1,
          "created_utc": "2026-02-16 16:18:45",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6fnsp",
      "title": "Opencode with Github Copilot",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r6fnsp/opencode_with_github_copilot/",
      "author": "Charming_Support726",
      "created_utc": "2026-02-16 17:19:30",
      "score": 22,
      "num_comments": 33,
      "upvote_ratio": 0.92,
      "text": "I asked that question in the Copilot sub but got not answer yet. Maybe someone with a similar setup could enlighten me.\n\nHalf time I use Opus (Rest of the time still burning my Azure Credits on codex), but after all this discussions of TOS Violations with Antigravity and CC and some further issues I canceled there.\n\nI read that Opencode is accepted as a 3rd Party Agent with GitHub Copilot. (Hope it's true) So I gave it a go.\n\nStill the context size restriction nags a bit, but I think maybe it is time to work less \"sloppy\". I created some workflow (Skills and Agents) for me to work intensively with subagents. Currently only for creating docs, onboarding projects and creating execution plans.\n\nI checked the billing and verified that my workflow only get charged one premium request per prompt, but in the background tools and subs are consuming a hell of a lot of tokens on single premium request.\n\nAre there any limits I shall take care of? I mean this could be really maxxed out by using the Question-Tool and Subagents etc. Dont wanna risk my companies Github Account.\n\nAny experience or hints ?\n\nEDIT: After someone posted about suspension I searched and found: [https://www.reddit.com/r/GithubCopilot/comments/1r0wimi/if\\_you\\_create\\_a\\_long\\_todo\\_list\\_in\\_agent\\_mode\\_you/](https://www.reddit.com/r/GithubCopilot/comments/1r0wimi/if_you_create_a_long_todo_list_in_agent_mode_you/)  Very Interesting. It seems GHCP is banning people who are excessively using the subagent scheme with tremendously long todo-lists. OMG.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r6fnsp/opencode_with_github_copilot/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5ptqum",
          "author": "devdnn",
          "text": "I believe thereâ€™s an active discussion happening in the GitHub issue section.\n\nIt seems like this might be what you need.\n\nhttps://github.com/anomalyco/opencode/issues/8030",
          "score": 10,
          "created_utc": "2026-02-16 17:30:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5px8yi",
              "author": "Charming_Support726",
              "text": "Thanks. I read the discussions beforehand. So I checked - and the billing is ok as expected.\n\nFor example I build a Subagent / Skill / Template -System that generates up-to-date project docs, when they are not available - or updates them etc. My test this morning spawned  6  parallel subs ( Main, Overview and 5 sub-subs - one per module)  burning approx 500k in half an hour on Opus 4.6. Factor 3. Three-Premium request $0.12 .\n\nI mean, If I enable DCP again and use it seriously it will get even worse. Is there anyone verifying the consumption or is this \"safe\" and \"ban-proof\" because covered by the TOS ?",
              "score": 3,
              "created_utc": "2026-02-16 17:47:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o67swif",
                  "author": "Legal_Dimension_",
                  "text": "I have just developed a tool which negates the detection of heavy agent usage using GHCP. They won't share what they deem to be fair usage which is against UK regulations, so to me it's all fair game until they do. Even opus 4.6 agreed when it reviewed my spec. I'll send you the link when it's up.\n\nIf you do get a suspension in the meantime I'll send you a response format for submitting a ticket which states the regulations they are breaking and you will get reinstated immediately.",
                  "score": 1,
                  "created_utc": "2026-02-19 09:51:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5puoge",
          "author": "ZeSprawl",
          "text": "I believe it's an official integration, based on a Twitter post from OpenCode. I use it regularly and I get a lot of usage out of it via Opus every month, and don't notice it eating up credits.",
          "score": 7,
          "created_utc": "2026-02-16 17:35:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pwg2o",
          "author": "JohnnyDread",
          "text": "Are you able to see the billing? I have not noticed a big difference between using OpenCode and the GitHub CLI as far as cost goes. ",
          "score": 3,
          "created_utc": "2026-02-16 17:43:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pwh6b",
          "author": "EchoesInBackpack",
          "text": "5.2 Codex and 5.3 codex models have twice bigger context windows. Sub-agents might be fine, but using it just to not hit compaction feels annoying\n\ntry opencode models â€”verbose(or something like that) to see their context limits",
          "score": 2,
          "created_utc": "2026-02-16 17:43:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qqux1",
              "author": "Charming_Support726",
              "text": "I know, also get them from Azure (free credits) and using my OpenAI subscription. So I am not afraid to run out of computing power. I really like them, but some tasks run better with Opus, e.g. building some up for the first time or running frontend debugging with playwright and a few more. I wanted to have a provider which acts uncomplicated and realizes access to Claude below API costs. \n\nI did a few test with subagents a few weeks ago. Found out, that saving tokens using Subs isn't that easy. Prompting this thoroughly is a piece of work on its own.\n\nAnyway, I thought it might be a good Idea to structure work and docs a bit better, so that I could easily start new sessions and work in the model's sweet spot below 128k - instead of hitting the compaction border like a vibe coder on regular basis. I gave it a try and found it impressing.\n\n",
              "score": 2,
              "created_utc": "2026-02-16 20:06:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5q6l3u",
          "author": "HarjjotSinghh",
          "text": "this actually feels like magic",
          "score": 2,
          "created_utc": "2026-02-16 18:30:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qjh37",
          "author": "trypnosis",
          "text": "I remember saying this was an issue but fixed in the CLI but not fixed yet on the desktop. Are you cli or desktop?",
          "score": 2,
          "created_utc": "2026-02-16 19:30:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qo09o",
              "author": "soul105",
              "text": "It's not yet fixed in CLI",
              "score": 2,
              "created_utc": "2026-02-16 19:52:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5qn0el",
              "author": "Charming_Support726",
              "text": "I am using web - building everything myself since I started a few month ago. Works til now without any issue.",
              "score": 1,
              "created_utc": "2026-02-16 19:47:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5qnm03",
                  "author": "trypnosis",
                  "text": "Never looked at the desktop app but if itâ€™s electron or any other web wrapper then it might be the same problem. Try the cli for a bit. I went from CC Max to Co Pilot pro+ I think I get decent value for money.",
                  "score": 3,
                  "created_utc": "2026-02-16 19:50:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67tgeo",
              "author": "TheDuck-Prince",
              "text": "I canâ€™t find anywhere that the CLI version is fixed ;(",
              "score": 1,
              "created_utc": "2026-02-19 09:57:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o68rosx",
                  "author": "trypnosis",
                  "text": "My bad I was sure some one mentioned it else where on this subreddit.",
                  "score": 1,
                  "created_utc": "2026-02-19 14:07:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5yt6hd",
          "author": "HarjjotSinghh",
          "text": "copilot + opencode is the real secret sauce now",
          "score": 2,
          "created_utc": "2026-02-18 00:42:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ytw5s",
              "author": "FaerunAtanvar",
              "text": "Care to elaborate?",
              "score": 1,
              "created_utc": "2026-02-18 00:46:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o67t60b",
              "author": "TheDuck-Prince",
              "text": "What",
              "score": 1,
              "created_utc": "2026-02-19 09:54:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o67vzsr",
              "author": "Legal_Dimension_",
              "text": "Yep, it's a fantastic price with the free models picking up explore tasks etc.",
              "score": 1,
              "created_utc": "2026-02-19 10:21:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5quyxg",
          "author": "Legal_Dimension_",
          "text": "Just received a suspension to my account for using copilot with opencode. I'm a heavy user and they don't like that so be careful.",
          "score": 1,
          "created_utc": "2026-02-16 20:26:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qvel7",
              "author": "Charming_Support726",
              "text": "That's bad. Could you tell us more? Explicitly because of Opencode? What is your subscription?\n\nBTW: What do you mean with \"Heavy User\" ?",
              "score": 2,
              "created_utc": "2026-02-16 20:28:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o67uj3n",
                  "author": "Legal_Dimension_",
                  "text": "So for context. \n\nGHCP pro+ account. Got a suspension of my copilot only. 2000+ autonomous requests without any user requests through opencode mainly low cost models running research and refinement loops over night.\n\nYou know, the whole point of having them.\n\nWoke up to a suspension, which didn't state why I was suspended just fair usage and ToS BS.\n\nDrafted ticket stating all the grey areas in their fair usage and ToS, that they by UK regulations must state usage rates if it is requirement to use a service etc and that they officially endorse opencode an agentic based cli so they need to reinstate me. They did.",
                  "score": 1,
                  "created_utc": "2026-02-19 10:07:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5qxrzz",
              "author": "ellensen",
              "text": "Suspension of your github  account or just the copilot subscription?",
              "score": 1,
              "created_utc": "2026-02-16 20:40:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67uk3p",
                  "author": "Legal_Dimension_",
                  "text": "Copilot only luckily.",
                  "score": 2,
                  "created_utc": "2026-02-19 10:07:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5t1h0p",
              "author": "joshashsyd",
              "text": "Yes I got a 157hr timeout??",
              "score": 1,
              "created_utc": "2026-02-17 03:42:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67un3p",
                  "author": "Legal_Dimension_",
                  "text": "Wow for heavy agent use? Is the ban still live?",
                  "score": 1,
                  "created_utc": "2026-02-19 10:08:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5wtago",
              "author": "haininhhoang94",
              "text": "i guess you use it with subagents?",
              "score": 1,
              "created_utc": "2026-02-17 18:44:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67rnhw",
                  "author": "Legal_Dimension_",
                  "text": "Yeah but I have developed a plugin to get around the detection while I bullied them with UK regulations and they reinstated the same day.",
                  "score": 1,
                  "created_utc": "2026-02-19 09:39:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ipr3y",
          "author": "HarjjotSinghh",
          "text": "opencode feels like copilot's secret favorite",
          "score": 1,
          "created_utc": "2026-02-21 00:15:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9pti3",
      "title": "Kimi K2.5 vs GLM 5",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r9pti3/kimi_k25_vs_glm_5/",
      "author": "Substance_Technical",
      "created_utc": "2026-02-20 08:23:57",
      "score": 21,
      "num_comments": 21,
      "upvote_ratio": 0.93,
      "text": "I see alot of people praising Kimi K2.5 on this sub, but according to benchmark GLM 5 is supposed to be better. \n\n  \nIs it true that you prefer kimi over GLM?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r9pti3/kimi_k25_vs_glm_5/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o6e5x8y",
          "author": "RainScum6677",
          "text": "GLM 5 is currently the only open source model that I find to actually be competitive with frontier models from the large companies. Truly competitive.",
          "score": 22,
          "created_utc": "2026-02-20 09:12:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e92hn",
              "author": "Sensitive_Song4219",
              "text": "Yeah it's good indeed, I find it slightly better than *GPT-5.3 Codex OpenAI Â· medium* (though a bit below *GPT-5.3 Codex OpenAI Â· high* \\- and therefore presumably it's also a bit below Opus).\n\nDid not think openweights would catch up as fast as they did. They're cooking.\n\nNow we just need z-ai to sort their capacity issues out and re-issue more competitive pricing like they had before.\n\n\n\nAs for Kimi 2.5: it's a tad better than GLM 4.7 but weaker than GLM 5 in my testing. I sometimes wonder if making it multi-modal (which yields a massive-param-count model - trillion params) might've been a bad play for coding. But Kimi is also one to watch, 2.5 is still solid as a daily driver.\n\n",
              "score": 6,
              "created_utc": "2026-02-20 09:42:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6e9m2j",
                  "author": "RainScum6677",
                  "text": "I wonder about GLM 5 codex, have not had the chance to try that one yet.\nFor me, I do believe codex 5.3 high/xhigh AND 5.2 high/xhigh are still the best in the industry for most coding tasks that require deep understanding, analysis, and complex implementation. GLM 5 is, in my opinion, second best. This is not including Gemini 3.1 pro since I have not yet had the chance to try it in any meaningful way.\n\nThat said, GLM 5 is impressive. Kimi 2.5 is quite good, but not at this level. Minimax 2.5 as well.",
                  "score": 3,
                  "created_utc": "2026-02-20 09:47:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6emp7k",
                  "author": "jesperordrup",
                  "text": "Thank you for sharing your knowledge. I'm curious about how you can be so precise and if you can share it? I would love to be able to run tests.\n\nI can of course feed them all the same prompt and then give it a look, but that really leaves a lot on the table to assume, right?",
                  "score": 1,
                  "created_utc": "2026-02-20 11:41:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6e40zh",
          "author": "xRedStaRx",
          "text": "GLM5 is better",
          "score": 11,
          "created_utc": "2026-02-20 08:54:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e6pct",
          "author": "__radmen",
          "text": "I've tested both GLM5 and Kimi. For simple (and direct) coding tasks, both perform well (although, for me, Kimi seems to be better).\n\nHowever, when I provide them a plan (or, we can call that a spec) with a TDD suite, their reliability becomes a problem.\n\nThey can finish simple tasks, but anything with a more complex logic puts them in a loop with no escape.\nOne task took Kimi over an hour and it failed to finish it. When I switched to GPT 5.3-Codex, the same task (with the same TDD approach) was finished in minutes without issues.\n\nIn my case, for coding, they seem to be on the same level as GPT-mini.\n\nI also used GLM5 to orchestrate agents; here it works pretty well. I don't recommend it for any complex planning.",
          "score": 7,
          "created_utc": "2026-02-20 09:20:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ekagk",
          "author": "itsdarkness_10",
          "text": "GLM 5, also used on cline Vs Kimi 2.5 and minimax2.5, GLM 5 is just too precise.",
          "score": 5,
          "created_utc": "2026-02-20 11:22:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fk6wg",
          "author": "Fragili-",
          "text": "Question to those recommending GLM5 - which provider do you recommend? I'm asking because I read a lot of bad opinions on z.ai",
          "score": 4,
          "created_utc": "2026-02-20 14:59:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g97cd",
          "author": "fabricio3g",
          "text": "I like more GLM 5 than K2.5, is a shame that in [z.ai](http://z.ai) is too slow",
          "score": 3,
          "created_utc": "2026-02-20 16:56:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e5gff",
          "author": "do_not_give_upvote",
          "text": "I'm curious as well. I don't have Kimi to try out but been pretty happy with glm-5 so far. I'd place it between Sonnet 4.5 and Opus 4.5. A bit slow sometimes but for the price, can't complain. And it's better with Claude Code as harness than opencode to me. A lot slower with opencode and not as good for some reason.",
          "score": 3,
          "created_utc": "2026-02-20 09:08:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e902j",
          "author": "jamiwe",
          "text": "In my humble opinion, GLM 5 is slow on z.ai but the most precise and reliable model of the ones I tested. I also tested a lot with openclaw and it takes itâ€™s time but it gets the job done and does not crush trough token usage.",
          "score": 3,
          "created_utc": "2026-02-20 09:41:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ezyx6",
          "author": "thatsalie-2749",
          "text": "Glm is the beast",
          "score": 1,
          "created_utc": "2026-02-20 13:11:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f0aef",
          "author": "deadcoder0904",
          "text": "Kimi 2.5 is better for writing",
          "score": 1,
          "created_utc": "2026-02-20 13:13:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gk1zp",
          "author": "Alarming-Possible-66",
          "text": "At the end its about preferences",
          "score": 1,
          "created_utc": "2026-02-20 17:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gvhcl",
          "author": "MakesNotSense",
          "text": "Just had multiple failures by GLM 5 on a multi-agent task that ChatGPT, Kimi 2.5, Gemini, and Opus all nailed. Basically, ingest new data, add addendum to the report the agent wrote previously.\n\n  \nGLM 5 hallucinated the wrong file path, tried to use 'write' instead of edit, but failed to successfully write, then when given corrective explicit instruction to use 'edit' tool and provided the exact file path, still failed.\n\nSo, something is not 'not quite right' with GLM 5.\n\nNot sure if it's a one-off, but literally just happened (second phase of the orchestration is in progress right now as I type). I've only just started adding GLM 5 as subagent to my agentic workflow. \n\nIt's not proving itself to have any particular aptitudes so far. Combined with this problem with tool use, not looking good. Preliminary data, but still pertinent data.\n\nKimi 2.5 has surprisingly offered insights that all other models failed to provide. Particularly when it comes to systems-based thinking and scientific approaches to modeling problems and communicating findings.\n\nIt fails in many other ways that Opus succeeds at. But it's contributions, when it finds something the other don't, are really helpful in that additive way that makes everything better. ",
          "score": 1,
          "created_utc": "2026-02-20 18:37:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ih8mi",
          "author": "Ke0",
          "text": "I find GLM-5 to be amazing as a model and is honestly the first time I would say one of China's open models truly competes in a \"I don't need to defer back Codex/Claude\"\n\nKimi K2.5 isn't there yet, neither is MiniMax's latest offering. \n\nThough with that stated, I am making this statement working with Swift code. So I imagine other languages might have different mileage. I did some C work with GLM-5 and it does well with pointers and have caught my memory management laziness, which I usually assume these models will suck at",
          "score": 1,
          "created_utc": "2026-02-20 23:26:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e14lp",
          "author": "HarjjotSinghh",
          "text": "this tech is so impressive i need a hug",
          "score": 0,
          "created_utc": "2026-02-20 08:26:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4v7by",
      "title": "OCMONITOR - a CLI tool to monitor OPENCODE CLI usage",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r4v7by/ocmonitor_a_cli_tool_to_monitor_opencode_cli_usage/",
      "author": "WriterOld3018",
      "created_utc": "2026-02-14 20:36:06",
      "score": 18,
      "num_comments": 3,
      "upvote_ratio": 0.95,
      "text": "https://preview.redd.it/95r6b42ktijg1.png?width=3790&format=png&auto=webp&s=e0b2919618f556d387b59e6071b3bb85890aa3bc\n\nHello opencode community,\n\n5 months ago I madeÂ ocmonitor, an open-source CLI tool to monitor opencode usage. Since yesterday (version 1.2.0+), opencode migrated from storing sessions in JSON files to using a SQLite database. Iâ€™ve updated ocmonitor to support this change.\n\nI also added a hierarchy view to show subagents as part of the parent session, and monitoring of output rate (TPS) to give an indication of model performance.\n\nI would appreciate any feedback or bug reports (preferably via GitHub). PRs and contributions are also welcome.  \n[https://github.com/Shlomob/ocmonitor-share](https://github.com/Shlomob/ocmonitor-share)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4v7by/ocmonitor_a_cli_tool_to_monitor_opencode_cli_usage/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5fclrh",
          "author": "rizal72",
          "text": "I love it! Beautifully done, very useful, very good job!",
          "score": 1,
          "created_utc": "2026-02-14 23:59:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ijgae",
          "author": "altsyst",
          "text": "Nice! Is there a way to inspect the whole context window besides having the number of tokens? I'm looking for a tool allowing me to see easily the whole context.",
          "score": 1,
          "created_utc": "2026-02-15 14:50:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kgktq",
          "author": "HarjjotSinghh",
          "text": "this is unreasonably cool actually!",
          "score": 1,
          "created_utc": "2026-02-15 20:33:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8ohj9",
      "title": "TUI Kanban board for OpenCode",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r8ohj9/tui_kanban_board_for_opencode/",
      "author": "Professional_Past_30",
      "created_utc": "2026-02-19 03:33:36",
      "score": 18,
      "num_comments": 4,
      "upvote_ratio": 0.95,
      "text": "Hey everyone,\n\nIâ€™ve been working on a tool called **opencode-kanban** to manage multiple OpenCode sessions and Git worktrees.\n\nThe project is essentially a mixture of two existing concepts, aiming to fix the friction I felt with both:\n\n1. **The Functionality (vs. Agent of Empires):** Like [*Agent of Empires*](https://github.com/njbrake/agent-of-empires), this manages multiple agents through tmux sessions. However, I built this because I found AoE a bit clunky for my daily workflow. I wanted something more ergonomic - offering more features like task tracking.\n2. **The Workflow (vs. Vibe Kanban):** I loved the visual organization of [*Vibe Kanban*](https://www.vibekanban.com/), but I didn't want to leave the terminal. This project brings that Kanban view (Todo/Doing/Done) directly into the CLI.\n3. **OpenCode-Only:** Most importantly, because this is built *exclusively* for OpenCode, it doesn't suffer from the friction of tools trying to support every agent out there. Tapping directly into the OpenCode API unlocks a much more tailored experience, giving you native features like automatic session attaching and built-in todo tracking.\n\n**The Result:** A TUI-first experience that lets you manage parallel work streams visually, while still allowing you to stick with the native OpenCode TUI inside auto-managed tmux sessions.\n\nhttps://preview.redd.it/24kvmv98fdkg1.png?width=1280&format=png&auto=webp&s=9851e35ad1ddb7aef473b49b067887488f9581dc\n\nRepo: [https://github.com/qrafty-ai/opencode-kanban](https://github.com/qrafty-ai/opencode-kanban)\n\nWould love to hear your thoughts!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r8ohj9/tui_kanban_board_for_opencode/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o68fmz7",
          "author": "xenopticon",
          "text": "super cool, congrats on the launch!",
          "score": 3,
          "created_utc": "2026-02-19 12:56:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68i9zd",
          "author": "Nearby_Tumbleweed699",
          "text": "Funciona como plugin dentro opencode o es una herramienta externa?",
          "score": 2,
          "created_utc": "2026-02-19 13:13:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69elnc",
              "author": "Professional_Past_30",
              "text": "It's a standalone program which manages each \"task\" as a tmux session. A \"task\" is something you want to work on linked to a corresponding git worktree/branch. \n\nTypical workflow:\n1. Open the tool\n2. Create a task by specifying a repo and a branch name. The tool will automatically create a new worktree for you.\n3. Attach and work on a task in a dedicated tmux&open code session.",
              "score": 1,
              "created_utc": "2026-02-19 16:05:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67cehb",
          "author": "MrBansal",
          "text": "Slop",
          "score": -2,
          "created_utc": "2026-02-19 07:13:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}