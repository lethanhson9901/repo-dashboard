{
  "metadata": {
    "last_updated": "2026-02-27 09:08:44",
    "time_filter": "week",
    "subreddit": "opencodeCLI",
    "total_items": 20,
    "total_comments": 256,
    "file_size_bytes": 293029
  },
  "items": [
    {
      "id": "1re6jhd",
      "title": "OpenCode launches low cost OpenCode Go @ $10/month",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/hpxb0za5dllg1.jpeg",
      "author": "jpcaparas",
      "created_utc": "2026-02-25 07:17:06",
      "score": 326,
      "num_comments": 132,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1re6jhd/opencode_launches_low_cost_opencode_go_10month/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7aduu4",
          "author": "justDeveloperHere",
          "text": "Will be cool to be an \"Open\" and show some limits numbers.",
          "score": 56,
          "created_utc": "2026-02-25 07:23:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7aegbh",
              "author": "toadi",
              "text": "Agree. At my company we use github for the models. They have premium requests. You can upgrade get x3 more requests. you just don't know what that means. never told how many there are to start with so I have no clue.",
              "score": 6,
              "created_utc": "2026-02-25 07:28:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7afgdv",
                  "author": "oplianoxes",
                  "text": "It clearly says that.it starts with 300, X3 is 900",
                  "score": 18,
                  "created_utc": "2026-02-25 07:37:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7azc0p",
                  "author": "rothnic",
                  "text": "GitHub is one of the few that is super transparent about it. It just has a monthly number rather than a time based reset during the month. Iirc it is 1200 requests per month, no matter how many tokens or tool calls it takes to complete the request.",
                  "score": 7,
                  "created_utc": "2026-02-25 10:42:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7b68f4",
                  "author": "keroro7128",
                  "text": "Pro=300, Pro + =1500",
                  "score": 4,
                  "created_utc": "2026-02-25 11:41:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7aeckm",
          "author": "jpcaparas",
          "text": "https://preview.redd.it/064i2a50fllg1.png?width=2534&format=png&auto=webp&s=32281eb52d6d4d022ee198a455434ac881e431c7\n\nJust these models for now:\n\n‚Ä¢ Kimi K2.5\n\n‚Ä¢ GLM-5\n\n‚Ä¢ MiniMax M2.5",
          "score": 57,
          "created_utc": "2026-02-25 07:27:48",
          "is_submitter": true,
          "replies": [
            {
              "id": "o7c4jt4",
              "author": "xmnstr",
              "text": "Solid choices from the Opencode team, I have to say.",
              "score": 16,
              "created_utc": "2026-02-25 15:04:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ck2gq",
                  "author": "jpcaparas",
                  "text": "Dax is a huge fan of K2.5. He's raved about it multiple times. I actually think it's his daily driver.",
                  "score": 8,
                  "created_utc": "2026-02-25 16:16:50",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7aizm8",
              "author": "AdamSmaka",
              "text": "they were free so far",
              "score": 9,
              "created_utc": "2026-02-25 08:10:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bdtzp",
                  "author": "SidneyBae",
                  "text": "The only free one is minimax 2.5 now, and the free one often hit limit ",
                  "score": 5,
                  "created_utc": "2026-02-25 12:36:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7akjfa",
                  "author": "wokkieman",
                  "text": "Exactly, context matters. It's nice to see there are some free an cheaper options. Every budget and purpose something. For some things Opus is really not required.",
                  "score": 5,
                  "created_utc": "2026-02-25 08:24:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7afsdz",
              "author": "GasSea1599",
              "text": "please provide link",
              "score": 4,
              "created_utc": "2026-02-25 07:40:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ageae",
                  "author": "jpcaparas",
                  "text": "You'll need to go through the Zen page first at [https://opencode.ai/zen](https://opencode.ai/zen), work your way to the billing section, and then you'll see Go. It doesn't seem to have its own standalone URL.\n\nStep by step guide here with some deets about the models: [https://reading.sh/opencode-go-gives-you-three-frontier-models-for-10-a-month-9fa091be6fd1?sk=fdc57ad14073b8a3f3d919a5d4b6cbcf](https://reading.sh/opencode-go-gives-you-three-frontier-models-for-10-a-month-9fa091be6fd1?sk=fdc57ad14073b8a3f3d919a5d4b6cbcf)\n\nhttps://preview.redd.it/88r3bvrbillg1.png?width=1192&format=png&auto=webp&s=b6ae8c49f62810db0f3ac7d5117f289ff7c35a86",
                  "score": 13,
                  "created_utc": "2026-02-25 07:46:28",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7b01ou",
              "author": "gnaarw",
              "text": "\"just\" üò≥",
              "score": 3,
              "created_utc": "2026-02-25 10:48:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7beujn",
                  "author": "One_Pomegranate_367",
                  "text": "I've been personally paying for all three, and I will gladly welcome canceling all three of those subscriptions. \n\nMain reason is because each model is only good at certain things, and when I pay for these subscriptions, they're much cheaper than Claude. ",
                  "score": 3,
                  "created_utc": "2026-02-25 12:42:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7f8f7l",
              "author": "saggassa",
              "text": "minimax is already free(i hope it stays like that)  \ni tried glm last weekend and was weird to use\n\n  \nminimax is doing great for me, oneshoting almost everything",
              "score": 1,
              "created_utc": "2026-02-25 23:52:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7adis1",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 22,
          "created_utc": "2026-02-25 07:20:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ajc7f",
              "author": "Sheepza",
              "text": "Indeed",
              "score": 2,
              "created_utc": "2026-02-25 08:13:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7apc1h",
          "author": "Huge-Refrigerator95",
          "text": "10$ is pretty cheap and good, but be clear about the number of requests even if they are low, I don't mind, just be clear, don't be like other tools that we're scared to use because we'll reach the limit before pressing enter",
          "score": 10,
          "created_utc": "2026-02-25 09:09:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7apoe1",
              "author": "Bob5k",
              "text": "this is the problem with ollama cloud aswell. it says there's 'some' 5h and weekly cap but they don't say what roughly it is. So at least some part of the market would just hesitate to subscribe because they just don't know what to expect.  \nFrom the other side 10$ is pretty cheap and running newest openweight models, sounds.. interesting limits-wise.",
              "score": 3,
              "created_utc": "2026-02-25 09:13:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7apv45",
                  "author": "Huge-Refrigerator95",
                  "text": "Of course, running a business is not easy at all, you'll have to be sure of the demand on the servers and maybe they need to have a \"priority\" pass during heavy load, I guess fireworks is there sponsor so maybe they want to return the favor but adding it to zen\n\n  \nI mean tell me you get 10 requests per hour much better than \"good\" usage!\n\n  \nBest of luck opencode, your forever supporters",
                  "score": 2,
                  "created_utc": "2026-02-25 09:14:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7as9uq",
          "author": "TreeBearr",
          "text": "yea okay I thought synthetic was good, for $10/month this is awesome!!\n\nI've been running it for the past hour and a half or so and am at 60% of the 5 hour rolling limit. The pay as you go api pricing is solid.\n\nhttps://preview.redd.it/maru4etu1mlg1.png?width=897&format=png&auto=webp&s=34f92c74bae1031dfc406e486f6003318db177e9\n\nInference is very nice, especially m2.5",
          "score": 15,
          "created_utc": "2026-02-25 09:37:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7axdx2",
              "author": "gkon7",
              "text": "Not looking good actually. You'll hit the monthly limit in 15 hours of your coding.",
              "score": 24,
              "created_utc": "2026-02-25 10:24:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7h5zy5",
                  "author": "TreeBearr",
                  "text": "Yea you're right, I don't think it's a good plan for someone doing a ton of serious coding. Though I might recommend it to someone who is new to the tools and wants to get started with opencode asap.\n\nSynthetic was my fav for a hot minute but the jury is still out on their new plans and they've been kinda slow to add the new models.\n\nu/Far_Commercial3963 mentioned Chutes which looks interesting tho",
                  "score": 2,
                  "created_utc": "2026-02-26 07:19:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7bg3lv",
                  "author": "HebelBrudi",
                  "text": "Still an insane subsidy over paying for token!",
                  "score": 1,
                  "created_utc": "2026-02-25 12:50:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7auesk",
              "author": "Professional-Cup916",
              "text": "24% weekly for 1.5 hours? Really?\nLooks terrible.",
              "score": 17,
              "created_utc": "2026-02-25 09:57:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7aydfr",
                  "author": "SOBER-128",
                  "text": "And already at 11% of the monthly quota. Going to run out of quota in just a couple of days at this rate.",
                  "score": 8,
                  "created_utc": "2026-02-25 10:33:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7b6qcf",
              "author": "ForeverDuke2",
              "text": "11% mothly usage in 1.5 hr is bad",
              "score": 7,
              "created_utc": "2026-02-25 11:44:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7izrvn",
              "author": "GarauGarau",
              "text": "How can I recover this information? Is it a plugin?",
              "score": 1,
              "created_utc": "2026-02-26 15:20:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lizvf",
                  "author": "TreeBearr",
                  "text": "The usage meters? In a browser login to [opencode.ai](http://opencode.ai) and go to Zen. Then it's under billing.\n\n  \nFor anyone else who's confused about where to sign up for the Go plan that's also where I found it.",
                  "score": 2,
                  "created_utc": "2026-02-26 22:29:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7l2aoi",
              "author": "HebelBrudi",
              "text": "I do actually like the trend that tools provide inference providers. There are so many slop providers. I am glad the bar gets raised for open weight.",
              "score": 1,
              "created_utc": "2026-02-26 21:08:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7avx49",
          "author": "Magnus114",
          "text": "Anyone have a feeling how the usage compares with claude 20 USD plan?",
          "score": 6,
          "created_utc": "2026-02-25 10:11:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aehpm",
          "author": "Jeidoz",
          "text": "https://preview.redd.it/nvgxfp82fllg1.png?width=1061&format=png&auto=webp&s=0dd939724ddc474963537e917d54ca79a943fe00\n\nIt does not says any numbers about limits... I personally feel that NanoGPT with 8$ would be better (providing same and extra/more models)...",
          "score": 16,
          "created_utc": "2026-02-25 07:29:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7aez6z",
              "author": "nonpre10tious",
              "text": "Yeah I wish they had transparent limits - as a side note, tool calling has always been buggy for me on nanogpt, making it difficult to use claude code or opencode. Hopefully this doesn‚Äôt fave that same pitfall",
              "score": 13,
              "created_utc": "2026-02-25 07:33:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ain1g",
                  "author": "HornyEagles",
                  "text": "Not to mention the inference is very slow too and is known to time out occasionally. Other than that the community is welcoming and limits are generous",
                  "score": 5,
                  "created_utc": "2026-02-25 08:06:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7anco5",
              "author": "alovoids",
              "text": "nanogpt is cheap but I can't bear the speed. too slow. I'm impatient üòî",
              "score": 3,
              "created_utc": "2026-02-25 08:51:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7arqtz",
              "author": "evnix",
              "text": "impossible to code with nanoGPT, not sure what it is, its like 30% requests get repeatedly sent to GPT-2 which is enough to kill coding exxperience, probably to save costs. but I wont complain, its nice for roleplay, minimal image generation for the low price, ¬†If you are looking for a NanoGPT referral link with ongoing discount like I was, you can use mine:¬†[https://nano-gpt.com/r/wdD9Gnti](https://nano-gpt.com/r/wdD9Gnti)",
              "score": 5,
              "created_utc": "2026-02-25 09:32:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7cznyi",
              "author": "RandsFlute",
              "text": "I just paid for the $8 yesterday to try it out, it is not worth it for opencode, at all\nTried it with kimi 2.5 thinking and glm 5, requests just failed, it wasn't even slow they just failed after a couple requests.\nTried the same conversation with zen kimi 2.5 and it worked flawlessly.\nI liked the idea of nanogpt because they clearly don't care about nsfw and I want to turn opencode into a slutty code assistant. But their service sucks for it. May be good for sillytavern but opencode is unusable there.",
              "score": 2,
              "created_utc": "2026-02-25 17:28:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7dribc",
                  "author": "ExcellentDeparture71",
                  "text": "u/RandsFlute so what do you recommend?",
                  "score": 1,
                  "created_utc": "2026-02-25 19:34:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ah1z5",
          "author": "verkavo",
          "text": "A reminder for the community - new models/vendors/plans usually provide the best bang for the buck, because they reserve capacity for the launch event. Get it while it lasts.",
          "score": 12,
          "created_utc": "2026-02-25 07:52:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ar2p4",
              "author": "geckothegeek42",
              "text": "That's not a good sign, glm-5 is basically lobotomized on go right now. It can't do anything. Infinite spirals, broken tool calls, garbled text. Atleast I haven't technically lost any money yet",
              "score": 5,
              "created_utc": "2026-02-25 09:26:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7asg4z",
                  "author": "Resident-Ad-5419",
                  "text": "I got the same feeling. The GLM inside the opencode go is nerfed compared to the GLM on the Z.AI.",
                  "score": 6,
                  "created_utc": "2026-02-25 09:39:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7gde4f",
                  "author": "SelectionCalm70",
                  "text": "How's the overall limit in go plan?",
                  "score": 1,
                  "created_utc": "2026-02-26 03:45:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7an0um",
          "author": "onafoggynight",
          "text": "What happened to open code black?",
          "score": 5,
          "created_utc": "2026-02-25 08:48:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7aq1bk",
              "author": "jpcaparas",
              "text": "![gif](giphy|6T5uDarA77yOBWS5Ii|downsized)",
              "score": 7,
              "created_utc": "2026-02-25 09:16:32",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7apzhw",
              "author": "InternalFarmer2650",
              "text": "Stuck in Whitelist hell, i subscribed like a month ago and have yet to get access / money billed on my card. \n\nSo i kinda wonder why they offer this new sub if they can't even whitelist the people that \"applied\" for the other one",
              "score": 2,
              "created_utc": "2026-02-25 09:16:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7b3c2i",
                  "author": "Outrageous_Style_300",
                  "text": "yep same üòÇ is there even a way to get off that waitlist? I never heard anything",
                  "score": 1,
                  "created_utc": "2026-02-25 11:17:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ap0y5",
          "author": "Resident-Ad-5419",
          "text": "So I got the subscription on my personal email after reading this thread. It was not appearing with the account that has my custom domain. Performance feels similar between the free models and their outputs. But at least the rate limiting seems a bit less aggressive so far. The free versions would rate limit faster.",
          "score": 5,
          "created_utc": "2026-02-25 09:06:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7asr4c",
          "author": "Resident-Ad-5419",
          "text": "I have a feeling the limits are around $4.5 for 5 hour rolling, $10-15 for weekly and $30-40 for monthly. Cannot confirm yet though, need to spend more time to figure out.  \n\\---  \nThe glm 5 version inside this model seems to be heavily nerfed (I'm assuming same for all other models). The same query given to the [Z.AI](http://Z.AI) Coding plan finished a response instantly while the one in Opencode Go just went into a thinking frenzy for minutes and wasted bunch of token.",
          "score": 4,
          "created_utc": "2026-02-25 09:42:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cxkv2",
          "author": "alexx_kidd",
          "text": "Can we use the API with Go in other apps also?",
          "score": 3,
          "created_utc": "2026-02-25 17:18:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aw8o0",
          "author": "klippers",
          "text": "I love opencode but wouldn't nanoGPT or Synthetic.ai subscription",
          "score": 3,
          "created_utc": "2026-02-25 10:14:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7grfc4",
              "author": "HenryTheLion_12",
              "text": "Nanogpt - too slow for coding. Synthetic - they changed their pricing structure yesterday. They are a good provider though.",
              "score": 1,
              "created_utc": "2026-02-26 05:20:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7h4f65",
                  "author": "klippers",
                  "text": "Oh that's great to know, thank you",
                  "score": 1,
                  "created_utc": "2026-02-26 07:05:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7axv0j",
          "author": "SOBER-128",
          "text": "https://preview.redd.it/5hg0maa2bmlg1.png?width=1155&format=png&auto=webp&s=1f37717b82d055f6d9224879c03c29c01ebfb916\n\nTried it. The rolling usage quota seems fine, but weekly and monthly limits are very restrictive. I'll run out of weekly/monthly quota after a couple of days with basically any kind of programming work.  \n  \nQuota usage seems to depend on the token count and the model's API usage price, not just on the number of requests. Requests with large contexts or more generated tokens deplete the quota faster. The requests show up in the Zen usage history as usual with some per-request costs. My request history page shows that I've used $1.38 worth of requests with the Go subscription, and I'm already at 6% of my monthly quota. This means for $10 per month I get the equivalent of around $20 in pay-as-you-go credit. Not sure if it's worth it.",
          "score": 3,
          "created_utc": "2026-02-25 10:29:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bkmod",
          "author": "thermal-runaway",
          "text": "What is up with all these subscription models and not having a reasonable middle tier? They‚Äôre either dirt cheap, $10-20, or $100+. I‚Äôm not making money off of my work, I just find it fun, so I can‚Äôt justify $100, but I exhaust my cheap subscriptions 3-5 days into the week. I‚Äôd happily pay someone $40-50 for a single plan that comfortably covers a week of casual use",
          "score": 3,
          "created_utc": "2026-02-25 13:18:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bupk1",
              "author": "mikkel01",
              "text": "Check out Github Copilot Pro+ ($39 per month)",
              "score": 2,
              "created_utc": "2026-02-25 14:14:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7fs771",
          "author": "jempezen",
          "text": "GLM5 est inutilisable pour l'instant. J'ai pris l‚Äôabonnement pour l'utiliser et la il part compl√©tement en vrille. J'ai √©t√© habitu√© √† lui via la version free et la version compl√®te de zai et j'√©tais compl√©tement satisfait. La lui donner acc√®s √† un projet en cours serait du suicide... ",
          "score": 3,
          "created_utc": "2026-02-26 01:42:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ikkh1",
              "author": "jempezen",
              "text": "GLM5 a nouveau fonctionnel mais avec ce plan il n'a pas la vision, c'est un mod√®le brider",
              "score": 1,
              "created_utc": "2026-02-26 14:03:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7an4u1",
          "author": "alovoids",
          "text": "I'm having decent speed with kimi and minimax. haven't tried glm. hopefully they're 'quick' enough",
          "score": 2,
          "created_utc": "2026-02-25 08:49:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aq2fx",
          "author": "NickeyGod",
          "text": "Well the question is right now what is generous and also there is other providers with more models that are equal in price. I mean its fine for what it has 10$ is not really of an ask if you really want to support them in their efforts go for it its fair",
          "score": 2,
          "created_utc": "2026-02-25 09:16:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aufj8",
          "author": "foolsgold1",
          "text": "\"generous\".. wtf does that mean?",
          "score": 2,
          "created_utc": "2026-02-25 09:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dort9",
          "author": "Anticode-Labs",
          "text": "$20 gets you Gpt plus with codex",
          "score": 2,
          "created_utc": "2026-02-25 19:21:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7eelq5",
          "author": "Just_Lingonberry_352",
          "text": "So are these models hosted in the US? Where does it host the actual models from?",
          "score": 2,
          "created_utc": "2026-02-25 21:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f3rq1",
          "author": "0xDezzy",
          "text": "The GLM-5 model is HEAVILY nerfed to be honest. It's messing up on outputs as well as doing things in a very stupid manner. ",
          "score": 2,
          "created_utc": "2026-02-25 23:26:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hoic6",
              "author": "Less_Ad_1505",
              "text": "Confirm! Had some issues with GLM-5, but Kimi and MiniMax work well",
              "score": 2,
              "created_utc": "2026-02-26 10:15:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7bkto2",
          "author": "trypnosis",
          "text": "To be honest this is moot for me as I won‚Äôt use AIs hosted outside the US and/or EU.",
          "score": 4,
          "created_utc": "2026-02-25 13:19:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7dcbr7",
              "author": "Depart_Into_Eternity",
              "text": "Same",
              "score": 1,
              "created_utc": "2026-02-25 18:24:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7g8dj2",
              "author": "not_particulary",
              "text": "You worry about foreign intelligence?",
              "score": 1,
              "created_utc": "2026-02-26 03:14:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7nsz70",
                  "author": "trypnosis",
                  "text": "Does it not worry you?",
                  "score": 1,
                  "created_utc": "2026-02-27 06:55:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7an4f5",
          "author": "AGiganticClock",
          "text": "Very cool, these are great models. Will wait a bit to hear about limits and speed/ratelimits",
          "score": 1,
          "created_utc": "2026-02-25 08:49:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c20w6",
          "author": "Lost-Ad-2259",
          "text": "the rod of morality at its finest",
          "score": 1,
          "created_utc": "2026-02-25 14:52:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7e2i74",
          "author": "Permit-Historical",
          "text": "Is it possible to use it outside of opencode?",
          "score": 1,
          "created_utc": "2026-02-25 20:25:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ggy2e",
          "author": "gameguy56",
          "text": "Need qwen3.5 then I'm gonna jump right in",
          "score": 1,
          "created_utc": "2026-02-26 04:07:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7iotsz",
          "author": "MorningFew1574",
          "text": "Instead of coding specifically, it can be used for something like Openclaw?",
          "score": 1,
          "created_utc": "2026-02-26 14:26:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7j79kj",
          "author": "SuperElephantX",
          "text": "So no more free MiniMax M2.5?",
          "score": 1,
          "created_utc": "2026-02-26 15:55:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jan45",
          "author": "clad87",
          "text": "What about the MCP web\\_search and image\\_analysis servers?\n\n",
          "score": 1,
          "created_utc": "2026-02-26 16:11:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jbdhv",
              "author": "jpcaparas",
              "text": "I just have a synthetic.new search and minimax mcp do that for me. separate subs. minimax vision is quite good",
              "score": 1,
              "created_utc": "2026-02-26 16:14:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7jbmnw",
                  "author": "clad87",
                  "text": "I use minimax vision too but it's very slow, like 40s for a result with a prompt",
                  "score": 1,
                  "created_utc": "2026-02-26 16:15:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7k2hxj",
          "author": "Available_Pass_7155",
          "text": "Has anyone tried it? With that subscription, do you notice everything runs faster?\n\n",
          "score": 1,
          "created_utc": "2026-02-26 18:19:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kix0z",
          "author": "mdrahiem",
          "text": "GLM 5.0 is unreachable here.. I get an error",
          "score": 1,
          "created_utc": "2026-02-26 19:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aifvw",
          "author": "revilo-1988",
          "text": "Mehr Details zu den Limits und so und das Abo ist gebucht",
          "score": 0,
          "created_utc": "2026-02-25 08:05:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7be4j9",
          "author": "Swimming_Ad_5205",
          "text": "–≠—ç—ç—Ö \n–ï—â—ë –±—ã –∏—Ö —Ä—Ñ –æ–ø–ª–∞—Ç–∏—Ç—å ) –±—ã–ª–æ –±—ã –≤–æ–æ–±—â–µ –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ",
          "score": -1,
          "created_utc": "2026-02-25 12:37:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7d7rxq",
              "author": "Competitive_Ad_2192",
              "text": "go away, there‚Äôs no vodka here!",
              "score": 0,
              "created_utc": "2026-02-25 18:04:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7aneis",
          "author": "Less_Ad_1505",
          "text": "https://preview.redd.it/6eg3j3xytllg1.png?width=720&format=png&auto=webp&s=deb828f47263566e7934bd2e5d1b562ed8ddb777\n\n",
          "score": 0,
          "created_utc": "2026-02-25 08:51:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7e4t1a",
          "author": "No-Friend7851",
          "text": "Considering they literally built censorship right into their software ‚Äî so even on my local model it was wasting tokens checking if I'm writing \"bad\" code ‚Äî yeah, no thanks. Hope they go bankrupt.",
          "score": -1,
          "created_utc": "2026-02-25 20:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7al0mu",
          "author": "ImMaury",
          "text": "Problem is, open source models suck.",
          "score": -10,
          "created_utc": "2026-02-25 08:29:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ang34",
              "author": "mintybadgerme",
              "text": "Confidently, and massively, incorrect. :)",
              "score": 9,
              "created_utc": "2026-02-25 08:52:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7an8o2",
              "author": "alovoids",
              "text": "i think the keys are to be more patient and thorough :))",
              "score": 1,
              "created_utc": "2026-02-25 08:50:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rb65ak",
      "title": "I made a TUI tool to use OpenCode basically for free using NVIDIA's NIM API's and finds the fastest available model in real-time.",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/mr10u1qdoxkg1.gif",
      "author": "AgeFirm4024",
      "created_utc": "2026-02-21 23:37:25",
      "score": 258,
      "num_comments": 74,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rb65ak/i_made_a_tui_tool_to_use_opencode_basically_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6p7205",
          "author": "a_alberti",
          "text": "OK. It works very nicely for me. Kudos for the clean implementation.\n\nCan someone explain here in the chat why Nvidia allows such loopholes? What is the business model? Do they want us to try out their models and then be hooked to a subscription or paid APIs?\n\nIt is too good to be true. Please help me understand. Thanks!",
          "score": 19,
          "created_utc": "2026-02-22 01:30:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6q07q1",
              "author": "ElectronicBend6984",
              "text": "Cut out the middleman. They sell the brain and providing free options for that layer of consumption makes the hardware more valuable. Nvidia got to where they are by funding research, no reason to change that strategy.",
              "score": 10,
              "created_utc": "2026-02-22 04:47:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6x4c46",
                  "author": "Latter-Parsnip-5007",
                  "text": "Wtf man, its an free online service. I want yall to understand ONE thing about the internet. IF ITS FREE, YOU ARE THE PRODUCT. Insta, facebook, youtube even opencode free models. They all exist to collect your user behavior. Either to sell it directly to ad firms, to use it to improve their sellable product or to tailor ads themselves. ",
                  "score": 4,
                  "created_utc": "2026-02-23 08:17:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6snemo",
              "author": "Pleasant_Thing_2874",
              "text": "It likely is more a way to spike up usage in various data centers while they continue to seek funding and to look good when the shareholder meetings come around.  They're all still in building/financing mode so most of them are likely profit negative (and intentional) but when they can go \"Look, our usage is 75% concurrent on average, which means when we put in our pricing plans and start pushing for revenue we should expect to bring in X amount of revenue daily/weekly/monthly\"....that's how you get people to invest millions upon millions more.   The entire AI data center investing/infastructure sector is built on sticks and duct tape and this is just another row to help keep it going.",
              "score": 3,
              "created_utc": "2026-02-22 16:30:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6p76xd",
              "author": "AgeFirm4024",
              "text": "Thanks a lot ! üòä",
              "score": 2,
              "created_utc": "2026-02-22 01:30:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6pdywn",
          "author": "MaxPhoenix_",
          "text": "I actually built an automated benchmarking tool very similar to this recently for free models across OpenCode, Kilocode, OpenRouter, and NVIDIA. You're right about the NVIDIA loophole - now that they swapped the non-renewing 1000-credit cap for per-minute rate limits, it's a great resource.\n\nThat said, I'd highly recommend adding a quality/eval filter, rather than just sorting by latency and uptime. I found that optimizing for speed over quality is a trap with these free tiers. Many of the fastest models will hallucinate, fail tool calls, or completely ignore instructions. For example, my benchmarks showed models like Minimax-2.5 consistently mutilating code (e.g., quietly changing hyphens to underscores in variables).\n\nIf you adjust your agent/tool to evaluate and filter models based on actual coding performance and instruction-following - not just ping times - this could be incredibly useful. Right now, sending users to the top 20 fastest models might just result in a lot of frustrating debugging sessions.",
          "score": 11,
          "created_utc": "2026-02-22 02:14:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6pm6we",
              "author": "omicron8",
              "text": "I have been working on something similar. Do you have a repo you wouldn't mind sharing?",
              "score": 4,
              "created_utc": "2026-02-22 03:09:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6rkppm",
              "author": "AgeFirm4024",
              "text": "you're absolutely right. Quality filtering is the next priority. How do you think it can be done without spamming tests to Nvidia servers ?",
              "score": 3,
              "created_utc": "2026-02-22 13:09:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6seide",
                  "author": "Vmvgsar",
                  "text": "Can you just include results for some of the coding benchmarks for all the models they offer, and allow the user to sort by this or to show only those either above some value or in top 5/10?",
                  "score": 2,
                  "created_utc": "2026-02-22 15:51:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6pkyxt",
          "author": "ochonueve89",
          "text": "Doesn't this risk nvidia-nim banning the user due to spamming their servers with requests?",
          "score": 9,
          "created_utc": "2026-02-22 03:00:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rl1yy",
              "author": "AgeFirm4024",
              "text": "I implemented a little feature to change ping frequency, i'll optimize it today, you can change ping updates with W/X keyboard touch.  \n  \nThe 40 req/min rate limit is enforced at the API gateway level, so even pinging all 44 models in parallel stays within their intended usage. Each ping is a single legit API call, not scraping or abuse. NVIDIA explicitly encourages this kind of dev usage in their docs. So I guess we're good !",
              "score": 3,
              "created_utc": "2026-02-22 13:12:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6pcz4n",
          "author": "dengar69",
          "text": "This looks great.  Are you able to apply this to NanoGPT?  The models there are very inconsistent and would be nice to check beforehand.  Thanks,",
          "score": 6,
          "created_utc": "2026-02-22 02:08:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rledr",
              "author": "AgeFirm4024",
              "text": "added to the update list, don't hesitate to add issues on github next time :)",
              "score": 2,
              "created_utc": "2026-02-22 13:14:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6t99xk",
                  "author": "dengar69",
                  "text": "I entered one last night.  The model doesn‚Äôt get picked when it opens OpenCode.  It reverts to my last choice.",
                  "score": 2,
                  "created_utc": "2026-02-22 18:10:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6s0k38",
              "author": "Embarrassed_Bread_16",
              "text": "does it provide free models?",
              "score": 2,
              "created_utc": "2026-02-22 14:43:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6t91fd",
                  "author": "dengar69",
                  "text": "Yes",
                  "score": 2,
                  "created_utc": "2026-02-22 18:09:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6pxv2y",
          "author": "keroro7128",
          "text": "Awesome üëç",
          "score": 4,
          "created_utc": "2026-02-22 04:30:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rlf57",
              "author": "AgeFirm4024",
              "text": "thanks bro :)",
              "score": 2,
              "created_utc": "2026-02-22 13:14:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6q8bkw",
          "author": "ForeverDuke2",
          "text": "Amazing, thanks for this bro\n\nI hope the opensource community picks it up and keeps contributing",
          "score": 3,
          "created_utc": "2026-02-22 05:52:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rlitm",
              "author": "AgeFirm4024",
              "text": "thanks ! i'll try to update it a lot so it's always up to date with all current free ai available :)",
              "score": 2,
              "created_utc": "2026-02-22 13:15:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6qinmu",
          "author": "c0nfluks",
          "text": "Cool tool. Do you have an ETA on the openclaw implementation? Thanks",
          "score": 2,
          "created_utc": "2026-02-22 07:25:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rloc6",
              "author": "AgeFirm4024",
              "text": "i almost made it work yesterday, just need to fix a few config files and try some models (some models on the list are really lame for openclaw, i had a model that gave me arabic, tha√Ø, and non sense english in the same message ü§£)",
              "score": 2,
              "created_utc": "2026-02-22 13:16:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o752qkf",
                  "author": "c0nfluks",
                  "text": "Can you let us know when youve implemented it?",
                  "score": 1,
                  "created_utc": "2026-02-24 14:26:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6qu3ym",
          "author": "Embarrassed_Bread_16",
          "text": "Thx for implementing my suggestion, great, now I think it's gonna be great that u or someone else create a blog like explanation of the Nvidia service and what the experience with it is like, I'm looking at u community üòâ",
          "score": 2,
          "created_utc": "2026-02-22 09:14:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rlqw7",
              "author": "AgeFirm4024",
              "text": "thanks ! i'll let you know, i'm planning on doing that when the tool is a bit more mature and finished :) ",
              "score": 2,
              "created_utc": "2026-02-22 13:16:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6r1k63",
          "author": "mintybadgerme",
          "text": "How can I get this going with opencode desktop for Windows?",
          "score": 2,
          "created_utc": "2026-02-22 10:26:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rlzw8",
              "author": "AgeFirm4024",
              "text": "I'll add that in a future update, good point :) i guess you can do it manually for now. One amazing (but risky thing) I do all the time : you run a opencode or claude code instance in your root folder and just ask it to install it, 99% of the time, it works. But I'll add it in a future update anyway :)",
              "score": 1,
              "created_utc": "2026-02-22 13:18:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rqrlc",
                  "author": "mintybadgerme",
                  "text": "Yeah, I'm trying not to be risky. :)",
                  "score": 1,
                  "created_utc": "2026-02-22 13:48:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6oy5df",
          "author": "rizal72",
          "text": "you changed the name! I remember checking it yesterday and it had another name :) However, nice job! Don't bother for stupid comments ;)",
          "score": 2,
          "created_utc": "2026-02-22 00:34:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6p0f7m",
              "author": "AgeFirm4024",
              "text": "yeah i called it nimping, since it was just a tui that pings NIM, but I plan to add other sources of free AI providers in it so i renamed it :) Thanks a lot ! don't worry I don't care about stupid comments lol",
              "score": 2,
              "created_utc": "2026-02-22 00:47:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6pasm1",
                  "author": "rizal72",
                  "text": "May I suggest to add opencode Zen and Openrouter? I use their free models a lot, mixed with paid ones in my multi agents settings with oh-my-opencode-slim, to witch I am a contributor ;)",
                  "score": 5,
                  "created_utc": "2026-02-22 01:54:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rvxh8",
          "author": "AgeFirm4024",
          "text": "UPDATE : I added an auto updater and a warning to inform you about new versions available, please reinstall the package :) npm i -g free-coding-models",
          "score": 1,
          "created_utc": "2026-02-22 14:17:43",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o6uw6ad",
          "author": "AgeFirm4024",
          "text": "I made a Discord sever ! [https://discord.com/invite/5MbTnDC3Md](https://discord.com/invite/5MbTnDC3Md)",
          "score": 1,
          "created_utc": "2026-02-22 23:04:58",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o6w0fo2",
          "author": "JensInJapan",
          "text": "That's really cool! I just tested it out.   \nIt would be super nice if we could pause with space button to have it not jump around any for us slow readers üòÇ",
          "score": 1,
          "created_utc": "2026-02-23 03:01:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6y54lg",
              "author": "AgeFirm4024",
              "text": "Oh, you can sort models with keyboard touches so it doesnt move. try hitting T, R, M, (keys are white letters in the title)",
              "score": 1,
              "created_utc": "2026-02-23 13:29:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o73k4cu",
                  "author": "JensInJapan",
                  "text": "Oh how silly of me! You're right! üòÇ Thanks.",
                  "score": 1,
                  "created_utc": "2026-02-24 07:25:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x6sra",
          "author": "RelationshipAny1889",
          "text": "Where can I find this opencode theme?",
          "score": 1,
          "created_utc": "2026-02-23 08:41:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6y5b8c",
              "author": "AgeFirm4024",
              "text": "it's called matrix, it's already in opencode, but my iterm2 has custom colors too on top of it \\^\\^ so i'm not sure it'll look 100% like mine",
              "score": 1,
              "created_utc": "2026-02-23 13:30:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73nnao",
          "author": "Villain_99",
          "text": "Will definitely try it",
          "score": 1,
          "created_utc": "2026-02-24 07:58:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ffej5",
          "author": "Primary_Prune_8351",
          "text": "Remindme! 15 hours",
          "score": 1,
          "created_utc": "2026-02-26 00:30:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ffkwu",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 15 hours on [**2026-02-26 15:30:22 UTC**](http://www.wolframalpha.com/input/?i=2026-02-26%2015:30:22%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/opencodeCLI/comments/1rb65ak/i_made_a_tui_tool_to_use_opencode_basically_for/o7ffej5/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FopencodeCLI%2Fcomments%2F1rb65ak%2Fi_made_a_tui_tool_to_use_opencode_basically_for%2Fo7ffej5%2F%5D%0A%0ARemindMe%21%202026-02-26%2015%3A30%3A22%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201rb65ak)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-02-26 00:31:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7l3hk3",
          "author": "Level_Peak_7110",
          "text": "Btw why Kimi2.5 is not listed Free anymore on opencode?",
          "score": 1,
          "created_utc": "2026-02-26 21:13:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l45o3",
              "author": "AgeFirm4024",
              "text": "weird, i'm checking that now.",
              "score": 1,
              "created_utc": "2026-02-26 21:17:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6p1eoc",
          "author": "No_Yard9104",
          "text": "Nvidia's website is broken.  You can't verify your account at [build.nvidia.com](http://build.nvidia.com) because their sms otp stuff to verify you is broken and has been for months and months.  There's hundreds of community discussions about it on Nvidia's developer forum.  It's been over 6 months for me and I've been trying every few weeks.  And without a verified account, you can't generate an API key.  \n\nThe most valuable company in the history of the world has been sitting on this bug for over 6 months.  If they wanted it fixed, it would be fixed.  Instead, the nvidia employees just tell people to email support for manual verification.  And they don't bother at all with doing that unless it's for a higher-profile user.  ",
          "score": 1,
          "created_utc": "2026-02-22 00:54:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6pfei5",
              "author": "Top-Faithlessness758",
              "text": "I just did it 10 minutes ago, it works.",
              "score": 2,
              "created_utc": "2026-02-22 02:24:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6p1rcl",
              "author": "AgeFirm4024",
              "text": "Where are you from? I discovered the existence of NIM yesterday and I created an API key and working very well. Maybe try with vpn but i guess you already tried",
              "score": 1,
              "created_utc": "2026-02-22 00:56:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6p5xp7",
                  "author": "No_Yard9104",
                  "text": "I'm from the US.  I've tried with and without VPN.  I tried using a friend's number who lives across the pond.  I've tried multiple browsers in case it's something bugged in the front-end.  Nothing has worked.",
                  "score": 1,
                  "created_utc": "2026-02-22 01:22:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6p397v",
              "author": "eltigre_rawr",
              "text": "YMMV but I just registered and verified through SMS OTP",
              "score": 1,
              "created_utc": "2026-02-22 01:05:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6p5pxh",
                  "author": "No_Yard9104",
                  "text": "What country are you from?  What browser did you use?  I've been trying to get an API key from them for months.  So if there's a different way to do things, I'll try anything.  ",
                  "score": 1,
                  "created_utc": "2026-02-22 01:21:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6p9ool",
          "author": "HarjjotSinghh",
          "text": "this is the future now.",
          "score": 1,
          "created_utc": "2026-02-22 01:47:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ot5x3",
          "author": "oxygen_addiction",
          "text": "Thanks for ruining NIM even further, asshole. I've reported your repo to Nvidia. Hopefully they ban your IP range.",
          "score": -16,
          "created_utc": "2026-02-22 00:03:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ou5jq",
              "author": "AgeFirm4024",
              "text": "NVIDIA's free tier is publicly documented, requires account creation, and has explicit rate limits. I automated the discovery of public endpoints. If that 'ruins' anything, take it up with NVIDIA's product team who designed it this way not me.",
              "score": 10,
              "created_utc": "2026-02-22 00:09:50",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6ouanb",
              "author": "UnifiedFlow",
              "text": "Weirdo",
              "score": 4,
              "created_utc": "2026-02-22 00:10:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6owvsz",
              "author": "Due-Dragonfruit2984",
              "text": "You must be fun at parties",
              "score": 3,
              "created_utc": "2026-02-22 00:26:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6p20ku",
                  "author": "No_Yard9104",
                  "text": "Lol...  He doesn't get invited to parties...",
                  "score": 2,
                  "created_utc": "2026-02-22 00:57:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1ra1069",
      "title": "Kimi K2.5 is so good that infra is cooked, IMO best open-source model right now",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/ht568ac6lokg1.jpeg",
      "author": "Dilligentslave",
      "created_utc": "2026-02-20 17:04:53",
      "score": 127,
      "num_comments": 22,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1ra1069/kimi_k25_is_so_good_that_infra_is_cooked_imo_best/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6gd1fm",
          "author": "BlacksmithLittle7005",
          "text": "Agreed. I've tried GML5, kimi2.5, minimax, qwen, and kimi2.5 is the only obvious sonnet replacement at such a lower price",
          "score": 23,
          "created_utc": "2026-02-20 17:13:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hp21y",
              "author": "mintybadgerme",
              "text": "Seconded.Use it every day.",
              "score": 5,
              "created_utc": "2026-02-20 20:59:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6jonaq",
              "author": "toadi",
              "text": "Think I agree. They are sonnet replacements but Opus still rules... I don't need it often but sometimes I use it when I'm on a hard spec...",
              "score": 1,
              "created_utc": "2026-02-21 03:54:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gwtp9",
          "author": "throwaway12012024",
          "text": "GLM5 replaces Opus. K2.5 replaces Sonnet.",
          "score": 13,
          "created_utc": "2026-02-20 18:44:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hbpwk",
              "author": "pisangcoklatcheese",
              "text": "Main problem for GLM5 is speeeeeed.",
              "score": 9,
              "created_utc": "2026-02-20 19:54:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6j8g1j",
                  "author": "BrightyBrainiac",
                  "text": "Yes. This.\nIt‚Äôs like codex.",
                  "score": 1,
                  "created_utc": "2026-02-21 02:09:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6gz7lj",
              "author": "FaerunAtanvar",
              "text": "I am not too familiar with Anthropic s model. Does this mean you'd suggest to use glm5 for planning and k2.5 for building?",
              "score": 4,
              "created_utc": "2026-02-20 18:54:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6hue0a",
              "author": "Loud_Buy_9297",
              "text": "K2.5 plays builder nicely with GPT 5.3 Codex as planner for Python projects, GLM 5 and Minimax 2.5 not so much. I found K2.5 even caught and cleaned up some Codex html/js bugs. Plan to openspec for kimi builder then have codex provide each builder prompt step by step. GLM 5 introduced more bugs so I build with kimi free on zen the last week, great workflow and cheap while codex sub lasts. Horses for courses!",
              "score": 1,
              "created_utc": "2026-02-20 21:26:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6i0l50",
              "author": "Western_Objective209",
              "text": "I keep seeing people say this then I try the models and they are derpy AF",
              "score": 1,
              "created_utc": "2026-02-20 21:57:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gkffm",
          "author": "Far_Commercial3963",
          "text": "I do not get the hype, it sucks at agentic compared to GLM5 and always failed at tool calls on any serious usage in my experience (in both Opencode and Kilo Code)\n\nIt may be good for chat but I don't think  its usable much in agentic\n\nAlso Kimi k2.5 is not open source, it is open \\*weight\\*. ",
          "score": 12,
          "created_utc": "2026-02-20 17:48:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6guhrz",
              "author": "AGiganticClock",
              "text": "It's good! So is minimax 2.5",
              "score": 3,
              "created_utc": "2026-02-20 18:33:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6guaaq",
              "author": "guillefix",
              "text": "same here, I was trying to fix a positioning issue in my react native app and after 10 tries I had to switch to minimax m2.5 which solved it in one shot",
              "score": 1,
              "created_utc": "2026-02-20 18:32:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6jy2lh",
              "author": "deadcoder0904",
              "text": "Its better for writing at least. Haven't tested it solely for coding.",
              "score": 1,
              "created_utc": "2026-02-21 05:03:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6k56yq",
          "author": "BitterAd6419",
          "text": "Only decent model for open source.",
          "score": 1,
          "created_utc": "2026-02-21 06:00:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gdiug",
          "author": "Michaeli_Starky",
          "text": "It's not open source.",
          "score": -1,
          "created_utc": "2026-02-20 17:16:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gmwrs",
              "author": "digitalfreshair",
              "text": "open weights",
              "score": 7,
              "created_utc": "2026-02-20 17:59:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gwd0r",
                  "author": "Michaeli_Starky",
                  "text": "Yes",
                  "score": 2,
                  "created_utc": "2026-02-20 18:41:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gee5v",
          "author": "Bob5k",
          "text": "well, with 20tps it's close to be unusable at all for anything serious. Even if it's better code quality wise, it'll still lose when it comes to quality of a code > verify > fix > verify > merge loop that can be achieved with faster models. And this loop should happen anyway no matter the model we're using because AI will miss things even if it's opus 4.6 super ultra high genius mode.\n\nedit: ofc unless you're just yolo-oneshotting next awesome saas, but if you do then please sign up for [faultry.com](http://faultry.com) waitlist as it'll be highly useful.",
          "score": -3,
          "created_utc": "2026-02-20 17:20:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ikgbz",
          "author": "lucidbox",
          "text": "I personally love Kimi. For like $300 a year you get access to their CLI, agent swarms, and a clawbot. Not to mention the ability to just give it an image and have a code for you which now Gemini does pretty well with 3.0. I basically use opus for planning, and for workflows that will only hit single files. I‚Äôm able to build lots of code with agents forms in Kimi, which really builds the scaffolding for my projects from a code perspective, and then I throw in Codex to the heavy lifting, and I end with the Gemini CLI using Stich to polish off the UI. It‚Äôs been a pretty good workflow so far.",
          "score": 0,
          "created_utc": "2026-02-20 23:45:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jpbwb",
              "author": "toadi",
              "text": "Would love that kimi but it seems you can only use a google account to login.",
              "score": 1,
              "created_utc": "2026-02-21 03:59:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6jsuqg",
          "author": "xoStardustt",
          "text": "What is kimi? Never heard of it",
          "score": 0,
          "created_utc": "2026-02-21 04:24:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gj56b",
          "author": "SvenVargHimmel",
          "text": "It's a very eager model and needs strong guidance. Poor in plan mode, great in build. I have tested it yet with Skills and oc bun tools¬†",
          "score": -1,
          "created_utc": "2026-02-20 17:42:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdocla",
      "title": "Found a way to touch grass and use Mac terminal from my iPhone so I can be vibecoding and live a balanced life",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/seyx8usijhlg1.jpeg",
      "author": "eureka_boy",
      "created_utc": "2026-02-24 18:30:13",
      "score": 80,
      "num_comments": 79,
      "upvote_ratio": 0.76,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rdocla/found_a_way_to_touch_grass_and_use_mac_terminal/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7798av",
          "author": "d2xdy2",
          "text": "I‚Äôll be a hater here. That‚Äôs not a balanced life.",
          "score": 21,
          "created_utc": "2026-02-24 20:26:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gzkev",
              "author": "veegaz",
              "text": "Wasn't OP being sarcastic",
              "score": 2,
              "created_utc": "2026-02-26 06:24:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o779ycv",
              "author": "eureka_boy",
              "text": "Hahaha this is the only balanced life i know of",
              "score": 3,
              "created_utc": "2026-02-24 20:29:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o77bidy",
                  "author": "d2xdy2",
                  "text": "Being extreme, I‚Äôd liken it a coke addict having a bump in their pocket for a quick pick me up vs binging the whole eight ball at home. \n\nYou‚Äôre still doing the thing. I‚Äôll never be onboard with this idea that being so attached to vibe coding is healthy or balanced",
                  "score": 6,
                  "created_utc": "2026-02-24 20:36:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76prac",
          "author": "Dudmaster",
          "text": "With opencode web you wouldn't need a terminal",
          "score": 10,
          "created_utc": "2026-02-24 18:56:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76q7lj",
              "author": "eureka_boy",
              "text": "well all my code is in my local mac, I still prefer a cli interface over a generic chat interface on the web. ",
              "score": -2,
              "created_utc": "2026-02-24 18:58:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o76sxz7",
                  "author": "Dudmaster",
                  "text": "\"web\" name might be misleading, it's a server that would be on your mac, the interface also has shell access",
                  "score": 3,
                  "created_utc": "2026-02-24 19:10:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76llww",
          "author": "drinksbeerdaily",
          "text": "IPhones doesn't have SSH clients? I see all these vibe coded ways to connect to claude, codex, opencode and I wonder what the point is. I use termux + mosh over VPN on my Android phone, and not really missing anything.",
          "score": 13,
          "created_utc": "2026-02-24 18:38:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76muhh",
              "author": "eureka_boy",
              "text": "yes if you have already done the VPN setup then sure go ahead with what you have. I hate setting up vpn, tailscale stuff because it is just too much. Also ssh is kind of slow if you don't setup right but I used webrtc here, so it should be significantly faster because now you are directly connecting instead of seperate vpn network in between. \n\nFYI: this is only for Mac and iPhone",
              "score": -17,
              "created_utc": "2026-02-24 18:43:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o76u4y7",
                  "author": "onil34",
                  "text": "brother what. How is installing tailscale more of a hassle than WRITING AN ENTIRE APP",
                  "score": 25,
                  "created_utc": "2026-02-24 19:16:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76z01v",
                  "author": "drinksbeerdaily",
                  "text": "How is ssh slow? We're talking kilobytes of text here. If you don't like tailscale, just use wireguard directly. I always use a vpn when remotely connecting home, because it's way more secure than exposing your service to the public internet will ever be.",
                  "score": 4,
                  "created_utc": "2026-02-24 19:38:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o773qve",
                  "author": "Delicious_Ease2595",
                  "text": "Tailscale is so simple not sure how you find it \"just too much\".",
                  "score": 3,
                  "created_utc": "2026-02-24 20:00:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77adhe",
          "author": "AltruisticRip5151",
          "text": "If you‚Äôre just trying to reach your own machines, Tailscale is the more robust default.\n\nMacky is clever, but it relies on a central signaling service to broker a direct WebRTC session and a custom Mac app, which adds extra third-party trust and attack surface compared to standard SSH over a private mesh.\n\nYou don‚Äôt need to toggle Tailscale on or off either. Traffic only goes over it if you point at app at a hostname. It runs 24/7 on my iPhone without issues even between restarts and updates.",
          "score": 4,
          "created_utc": "2026-02-24 20:31:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7n42wv",
              "author": "Vast-Information-779",
              "text": "love tailscale, it gives you access to all your server ports as well which really helpful for remote web dev. tailscale + termius + tmux is soo op for accessing servers from phone  . i use it for managing my home lab and honestly thats the primary way i access my homelab, no need to fuck around with tunnels and stuff",
              "score": 1,
              "created_utc": "2026-02-27 03:52:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o77dho7",
          "author": "getfitdotus",
          "text": "https://github.com/chriswritescode-dev/opencode-manager this is much more phone ui friendly. Plus all the integration GIT, tts , file editing",
          "score": 4,
          "created_utc": "2026-02-24 20:46:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7amebn",
              "author": "Potential-Leg-639",
              "text": "This!\nThat‚Äòs open source as it should be for OpenCode related stuff",
              "score": 2,
              "created_utc": "2026-02-25 08:42:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7bbirm",
              "author": "smvueno",
              "text": "Wooow thanks for sharing, this is really cool!! üòÅ Mobile first ftw!!",
              "score": 2,
              "created_utc": "2026-02-25 12:20:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o76zy60",
          "author": "rySeeR4",
          "text": "You vibe coded a shitty app and dont know anything about opencode web and how to expose it and just want the attention, take my down vote",
          "score": 5,
          "created_utc": "2026-02-24 19:42:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o772fz9",
              "author": "drinksbeerdaily",
              "text": "Charging $29 for a closed-source vibe-coded terminal relay when tailscale ssh exists for free is a tough sell.",
              "score": 7,
              "created_utc": "2026-02-24 19:54:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o77mh65",
                  "author": "onil34",
                  "text": "HE IS CHARGING 29$ FOR THIS?",
                  "score": 3,
                  "created_utc": "2026-02-24 21:27:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o79v9oc",
          "author": "lol_idk_234",
          "text": "Yo bring this to windows and make a IntelliJ plugin üôè",
          "score": 2,
          "created_utc": "2026-02-25 04:54:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7a4fa4",
          "author": "Ancient_Topic_6416",
          "text": "I have been doing this for free with termius+tailscale. Why make it complicated? Hmmm\n\n\nEdit: And pricey at 29/month.",
          "score": 2,
          "created_utc": "2026-02-25 06:03:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a8l8c",
              "author": "eureka_boy",
              "text": "$29/lifetime",
              "score": 1,
              "created_utc": "2026-02-25 06:37:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o771vgi",
          "author": "layer4down",
          "text": "A lot of shade in this thread bro never mind all that. Keep doin you and maybe post over in r/LocalLLama for like-minded folks who like local everything and reinventing wheels for the thrill of it.",
          "score": 3,
          "created_utc": "2026-02-24 19:51:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o772ac4",
              "author": "eureka_boy",
              "text": "thanks bro!",
              "score": 1,
              "created_utc": "2026-02-24 19:53:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o778gvp",
          "author": "MKU64",
          "text": "What is it with all the shade in here? Nice job! I made one myself and regardless of what everyone says it‚Äôs the journey to productivity that matters and what will actually make you continue coding anywhere.",
          "score": 4,
          "created_utc": "2026-02-24 20:22:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77y39k",
              "author": "ImagineBeingPoorLmao",
              "text": "It's the same app every day.",
              "score": 3,
              "created_utc": "2026-02-24 22:21:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o779t93",
              "author": "eureka_boy",
              "text": "yeah thanks bro! Yes I don‚Äôt know why people are taking offence to a side project lol",
              "score": 2,
              "created_utc": "2026-02-24 20:28:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76rkm6",
          "author": "jamesrossdev",
          "text": "I have been wanting to make something like this that allows me to connect to my local pc terminal from my phone, without having to walk 10 meters to my computer. \n\nReally cool seeing you do it and I guess I'll get working on it as well. Btw I also hate doing the VPN setup. It's nice in the long run but tedious to set up",
          "score": 1,
          "created_utc": "2026-02-24 19:04:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76ufik",
              "author": "onil34",
              "text": "Tailscale is literally \n1. install \n2. tailscale up\n3. Log in \n4. Done",
              "score": 5,
              "created_utc": "2026-02-24 19:17:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76veta",
                  "author": "jamesrossdev",
                  "text": "I should also add that I prefer making my own tools, but I guess I'll try out Tailscale too. Thanks!",
                  "score": -1,
                  "created_utc": "2026-02-24 19:21:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76vo8u",
                  "author": "eureka_boy",
                  "text": "sure bro, then you need a seperate app in your iphone for ssh. To connect, first you need to turn on the tailscale thing each time in your phone and then go to some ssh app in app store which costs $ if you want a good one. Also tailscale just raised a massive Series C funding of $160M (https://tailscale.com/blog/series-c), you think it is going to be free forever?",
                  "score": -1,
                  "created_utc": "2026-02-24 19:23:11",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o76ssvz",
              "author": "eureka_boy",
              "text": "yesss if you love networking stuff, this should be a fun thing to work on!",
              "score": 1,
              "created_utc": "2026-02-24 19:10:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76z0kr",
          "author": "ciprianveg",
          "text": "I like it! Can it be ported to android?",
          "score": 1,
          "created_utc": "2026-02-24 19:38:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77146y",
              "author": "drinksbeerdaily",
              "text": "Sir, there are FREE, battletested and open source alternatives to let you easily connect remotely to a terminal on Linux or MacOS.",
              "score": 4,
              "created_utc": "2026-02-24 19:48:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7723l4",
                  "author": "eureka_boy",
                  "text": "okie this is not simple ssh app, this is an app that allows you to establish a direct p2p webrtc connection between your mac and iphone. This just makes accessing your mac terminal much simpler and easier to setup. \n\nIf you want ssh, you need to first setup VPN so you can access it from anywhere. Which i hate to setup",
                  "score": 1,
                  "created_utc": "2026-02-24 19:52:46",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o76ziqs",
              "author": "eureka_boy",
              "text": "my google play dev account is being weird so might take a few weeks to release the app on android",
              "score": 2,
              "created_utc": "2026-02-24 19:40:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o77no8u",
          "author": "andrewchen5678",
          "text": "I built something similar but entirely web based with mobile optimized interfaces that is completely open source that can run anything on the terminal [https://github.com/andrewtheguy/mywebterm](https://github.com/andrewtheguy/mywebterm)",
          "score": 1,
          "created_utc": "2026-02-24 21:32:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o796bbo",
          "author": "Otherwise_Bee_7330",
          "text": "the balls to charge for slop these days ü•µ",
          "score": 1,
          "created_utc": "2026-02-25 02:22:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7a0xt2",
          "author": "Alberion",
          "text": "For anyone else who is looking for a similar thing to this (with no other depencencies), I spun up a free OCI tier instance and ssh into the instance from phone with JuiceSSH.\n\nhttps://preview.redd.it/pa4d9bu2vklg1.png?width=1080&format=png&auto=webp&s=b410afd4e11b40b459bb8412aa8b702285c731ac",
          "score": 1,
          "created_utc": "2026-02-25 05:35:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ab9w7",
          "author": "Far_Complex8736",
          "text": "How are you going to verify the changes incase it is a front end or a backend change?",
          "score": 1,
          "created_utc": "2026-02-25 07:00:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aehk0",
          "author": "Procrastinator9Mil",
          "text": "Is this E2E encrypted?",
          "score": 1,
          "created_utc": "2026-02-25 07:29:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7afphq",
              "author": "eureka_boy",
              "text": "Yes: https://macky.dev/#architecture",
              "score": 1,
              "created_utc": "2026-02-25 07:40:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7bev6k",
          "author": "NerdistRay",
          "text": "Tailscale",
          "score": 1,
          "created_utc": "2026-02-25 12:42:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7en64d",
          "author": "IncomeAsOutcome",
          "text": "Tailscale + Terminus on Android.",
          "score": 1,
          "created_utc": "2026-02-25 22:01:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g27p8",
          "author": "throwaway12012024",
          "text": "why not just use opencode web in your computer + tailscale to access it in your iphone?",
          "score": 1,
          "created_utc": "2026-02-26 02:39:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gwj9p",
              "author": "soundslikeinfo",
              "text": "Here's how to set up OpenCode web and access it via Tailscale:\n\n1. Start OpenCode Web\n\nopencode web\n\nThis starts on [127.0.0.1](http://127.0.0.1) by default.\n\n2. Make it accessible over network\n\nTo use with Tailscale, bind to all interfaces and set a password:\n\nopencode web --host [0.0.0.0](http://0.0.0.0) \\--port 4096\n\nSet a password for security:\n\nexport OPENCODE\\_SERVER\\_PASSWORD=\"your-password\"\n\nopencode web --host [0.0.0.0](http://0.0.0.0)\n\n3. Connect via Tailscale\n\n1. Ensure Tailscale is running on both machines\n\n2. Find your Tailscale IP: tailscale ip -4\n\n3. Access from another machine at http://<tailscale-ip>:4096\n\n4. Optional: Enable HTTPS with Caddy or nginx reverse proxy\n\nFor encrypted traffic over Tailscale, you can set up a simple reverse proxy, or just use Tailscale's built-in encryption (it already encrypts all traffic).\n\nNote: Tailscale's network is already encrypted, so the OPENCODE\\_SERVER\\_PASSWORD provides app-level authentication. Make sure to set a strong password if exposing to network access.",
              "score": 2,
              "created_utc": "2026-02-26 05:59:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kfh7c",
          "author": "sullenisme",
          "text": "long ass nails",
          "score": 1,
          "created_utc": "2026-02-26 19:19:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76w4a7",
          "author": "momentary_blip",
          "text": "Lol just use exe.dev",
          "score": 0,
          "created_utc": "2026-02-24 19:25:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76wup4",
              "author": "eureka_boy",
              "text": "huh? are you a bot? ",
              "score": 1,
              "created_utc": "2026-02-24 19:28:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o79tdku",
                  "author": "momentary_blip",
                  "text": "why would i be a bot",
                  "score": 1,
                  "created_utc": "2026-02-25 04:41:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rac56z",
      "title": "I made a little CLI tool to check for available Nvidia NIM Free coding LLM models",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/s5ib33h2pqkg1.gif",
      "author": "AgeFirm4024",
      "created_utc": "2026-02-21 00:10:16",
      "score": 69,
      "num_comments": 28,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rac56z/i_made_a_little_cli_tool_to_check_for_available/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6krcuv",
          "author": "sorvendral",
          "text": "You filmed that with you oven?",
          "score": 12,
          "created_utc": "2026-02-21 09:31:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6l0zkk",
              "author": "AgeFirm4024",
              "text": "Absolutely ! üòÇ i have to update that gif it‚Äôs horrible i must admit.",
              "score": 3,
              "created_utc": "2026-02-21 11:05:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75l1bw",
                  "author": "oVerde",
                  "text": "Actually at first I thought this was a CRT/Matrix kind of filter for opencode",
                  "score": 1,
                  "created_utc": "2026-02-24 15:54:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6k4i59",
          "author": "BuildAISkills",
          "text": "Now this is actually useful. Thanks¬†",
          "score": 3,
          "created_utc": "2026-02-21 05:55:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jdkpr",
          "author": "planetearth80",
          "text": "Doesn‚Äôt nvidia provide one-time allocation of free API credits (typically 1,000 credits initially). Trying to understand where are the free coding models",
          "score": 2,
          "created_utc": "2026-02-21 02:41:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jfiry",
              "author": "AgeFirm4024",
              "text": "NVIDIA phased out fixed API credits (like the old 1,000 initial ones) early 2025 for NIM API Catalog it‚Äôs now rate-limit based for free eval/prototyping (40 RPM per model, varies by load).\n\nSo, that's why I made that tool, most big models are down due to over use i guess, but it's free and without credits limits, i guess ! from my understanding. Correct me if i'm wrong",
              "score": 3,
              "created_utc": "2026-02-21 02:54:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6l8e7j",
          "author": "labdoe",
          "text": "Nice, it would be useful if the llms are sorted based on avg latency within each tier.",
          "score": 2,
          "created_utc": "2026-02-21 12:12:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n1pdn",
              "author": "AgeFirm4024",
              "text": "ok, i'll do that :) ",
              "score": 2,
              "created_utc": "2026-02-21 18:21:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ipgqr",
          "author": "HarjjotSinghh",
          "text": "this is genius - nvidia just gave us free llms!",
          "score": 2,
          "created_utc": "2026-02-21 00:14:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7iezux",
              "author": "SnowBoy_00",
              "text": "evaluation purposes only. The maximum context length of their models is butchered to \\~4000 tokens.",
              "score": 1,
              "created_utc": "2026-02-26 13:32:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6jccf7",
          "author": "crmfan",
          "text": "What stack made the app, very nice!",
          "score": 1,
          "created_utc": "2026-02-21 02:34:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jfn6l",
              "author": "AgeFirm4024",
              "text": "I made the CLI tool with claude code, and finished it with free GLM5 from nvidia NIM haha :) thanks !",
              "score": 1,
              "created_utc": "2026-02-21 02:54:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kpymm",
          "author": "Embarrassed_Bread_16",
          "text": "now i would add litellm proxy for automatically choosing best available model",
          "score": 1,
          "created_utc": "2026-02-21 09:17:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6l4v09",
              "author": "AgeFirm4024",
              "text": "Planned in an update",
              "score": 4,
              "created_utc": "2026-02-21 11:41:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6lcxo0",
                  "author": "Embarrassed_Bread_16",
                  "text": "very cool",
                  "score": 2,
                  "created_utc": "2026-02-21 12:48:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6muh4y",
          "author": "x8code",
          "text": "Perfect use case for a monitoring TUI. Excellent work",
          "score": 1,
          "created_utc": "2026-02-21 17:46:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n1pze",
              "author": "AgeFirm4024",
              "text": "thanks !",
              "score": 1,
              "created_utc": "2026-02-21 18:21:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ofgr7",
          "author": "AgeFirm4024",
          "text": "**UPDATE : i just renamed it to \"free-coding-models\"**  \n**and I updated it to a new version with sorting, much better TUI, and automatic opencode config, the new repo is**¬†[**https://github.com/vava-nessa/free-coding-models**](https://github.com/vava-nessa/free-coding-models)\n\n`npm i -g free-coding-models`",
          "score": 1,
          "created_utc": "2026-02-21 22:41:41",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o6qiylq",
          "author": "HarjjotSinghh",
          "text": "this is unreasonably cool actually.",
          "score": 1,
          "created_utc": "2026-02-22 07:28:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v16yq",
          "author": "HarjjotSinghh",
          "text": "this nvidia nim tool is gonna be the hype.",
          "score": 1,
          "created_utc": "2026-02-22 23:33:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v9mll",
          "author": "Big-Masterpiece-9581",
          "text": "So has everyone else evidently",
          "score": 1,
          "created_utc": "2026-02-23 00:21:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6mem8e",
          "author": "aeonixx",
          "text": "I tried to configure this in OpenCode, but I'm not getting a response. Am I doing something wrong? I got the API key from build.nvidia.com, and inserted it into the Nvidia provider in OpenCode. But nothing.",
          "score": 0,
          "created_utc": "2026-02-21 16:26:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n1txq",
              "author": "AgeFirm4024",
              "text": "hmm, I'll release a big update that will install it automatically, i'll let you know when it's out !",
              "score": 2,
              "created_utc": "2026-02-21 18:22:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6odo43",
                  "author": "aeonixx",
                  "text": "Thank you! I'll keep an eye out for it",
                  "score": 1,
                  "created_utc": "2026-02-21 22:31:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rba64a",
      "title": "Best bang for your bucks plan?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rba64a/best_bang_for_your_bucks_plan/",
      "author": "CantFindMaP0rn",
      "created_utc": "2026-02-22 02:41:38",
      "score": 66,
      "num_comments": 58,
      "upvote_ratio": 0.95,
      "text": "From my research so far, this is what I've gathered:\n\n1. Github Copilot's $40 plan -> Codex/Opus/Sonnet, but metered per request instead of per token (~~can try to saturate context window for maximum value~~ Don't think you need to try for Claude models when it's 128k already lmao)\n2. Codex -> It's free right now, but not sure if $20 per month is worth it\n3. Kimi 2.5 -> Workhorse?\n4. MiniMax/GLM -> Even dumber workhorses that can serve as subagents?\n5. Zen -> Pay per API calls is pretty pricey, but can help in a pinch\n\nNot counting Antigravity due to reportedly very low limits\n\nPS. I'm keeping my Claude 5x Max plan for when I need to one shot stuff at work/detailed planning.\n\nEdit: Got all your comments into a nice summary here, courtesy of Claude Sonnet lol. Hope it proves useful for those who might be wondering the same thing (since the agantic AI landscape shifts so effing fast)\n\n# Plan Ranking (as of Feb 22, 2026)\n\n|Rank|Plan|Mentions|Sentiment|Key Signal|\n|:-|:-|:-|:-|:-|\n|1|Opencode Black/Zen|5|‚úÖ Positive|Best value; multi-model; cheap entry|\n|2|Codex Plan|4|‚úÖ Strongly Positive|\"The best\"; 272K context; top performance|\n|3|Alibaba Cloud (Qwen)|2|‚úÖ Positive|$5-10/mo; relaxed quotas; multi-model|\n|4|[Chutes.ai](http://Chutes.ai)|5|‚ö†Ô∏è Mixed+|Cheap; unreliable for real-time use|\n|5|Copilot|5|‚ö†Ô∏è Mixed|Broad access; 100K context limit|\n|6|Minimax|3|‚úÖ Positive|Best secondary/budget execution plan|\n|7|OpenRouter API|2|‚úÖ Positive|Fair PAYG pricing; transparent|\n|8|Ollama Cloud|2|‚û°Ô∏è Neutral|Good quotas; slow under load|\n|9|ChatGPT Plus|2|‚û°Ô∏è Neutral|Needed for Codex 5.3 only|\n|10|[Synthetic.new](http://Synthetic.new)|2|‚ö†Ô∏è Mixed|Over-capacity; low community validation|\n|11|[Z.AI](http://Z.AI) Coding Plan|1|‚û°Ô∏è Neutral|No signal|\n|12|Claude Max/Pro|3|‚ùå Negative|Expensive; session limits; weak coding|\n|13|Kilocode API|2|‚ùå Negative|Accused proxy/copycat; skip for OpenRouter|",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rba64a/best_bang_for_your_bucks_plan/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o6rkq7t",
          "author": "hotairplay",
          "text": "I don't see it mentioned here, but Alibaba Cloud Coding Plan (Qwen) is $5/mo first month then $10/mo normal.\n\nBut it provides not only Qwen family models, but Kimi K2.5 and GLM 4.7 as well! The quota is very relaxed..i think it is one of the best value subscriptions.",
          "score": 8,
          "created_utc": "2026-02-22 13:09:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6scuek",
              "author": "CutEmpty3551",
              "text": "You mentioned that there is an AI plan on Alibaba Cloud that supports Kimi K2.5 and GLM 4.7. I tried to find it, but I can currently only see the Qwen service available.  \n  \nCould you please provide the link to the specific service you are using?",
              "score": 1,
              "created_utc": "2026-02-22 15:44:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6upivd",
              "author": "drbobb",
              "text": "I don't see any non-Qwen models there.",
              "score": 1,
              "created_utc": "2026-02-22 22:28:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7i68fz",
              "author": "West-Ad-2051",
              "text": "I'm testing it right now and have some thoughts after two days of using the Pro plan for $50 ($25 after getting a coupon). The price is pretty good for what you get, but from what I've understood and talked to Alibaba support, you get only one endpoint and it's Singapore/global. I don't know if they change your main server based on API calls (as their panel has options for Singapore/US/China), but you by yourself can't really change it (correct me if I'm wrong, please).\n\nSpeed is meh. I was using Synthetic before, but after changes in subscriptions to weird subscription packs, they lost me on that. But comparing them to each other, Synthetic wasn't ultra-fast, but Alibaba seems to be even slower sometimes.\n\nAnd there is some weird behavior with Kimi for sure, some weird loops that I didn't get with any other provider and didn't even think that Kimi can go for loops like this. I'm hoping that it will be a little bit better after some time.\n\nQwen's models in this subscription seem solid. Can't say anything on MiniMax and GLM because I didn't use them too much yet, but one subagent run MiniMax and for now didn't go into some strange loops.\n\n",
              "score": 1,
              "created_utc": "2026-02-26 12:39:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7j41c8",
                  "author": "dyzhdyzh",
                  "text": "I'm in the same boat as you. Looking for a different provider since Synthetic announced the changes.\nI subscribed to Alibaba Cloud yesterday and tested how the same models perform and was very disappointed. All of the big three (Kimi K2.5, Qwen3.5, and GLM-5) have trouble following instructions, leak tool calls, get stuck in loops, and sometimes hang indefinitely.",
                  "score": 1,
                  "created_utc": "2026-02-26 15:40:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6qkz36",
          "author": "Optimal_Strength_463",
          "text": "Personally I got on the Opencode Black plan pretty quickly and that‚Äôs been one of the best value for me. Especially with Gemini 3.1 on there. I also have a Codex plan and the limits are insane and 5.3 is pretty wild.\n\nI‚Äôm stopping my Claude Max plan at the end of the cycle as it seems Opus is brilliant at communicating what it does, but it never quite solves the problem and makes weird choices. When you read the thinking output it‚Äôs like ‚Äúwow, this thing thinks like a lead developer‚Äù but when you see the solution you realise it‚Äôs better at communicating than coding.\n\nGemini 3.1 however seems to think and blab on about tool selection incessantly but created something amazing that wasn‚Äôt even on my radar and solves my problem in both a technically superior way and is about 50x cheaper to run.\n\nCodex is somewhere between Gemini and Claude and with the insane limits at the moment is a true workhorse.\n\nThen if you‚Äôre into the ‚Äúrun 20 Opencode instances 24/7‚Äù kind of crowd then having Kimi 2.5 on your Black plan do the grunt work means you‚Äôll struggle to hit the limit of a Zen&Codex Max plan.\n\nIf you have less than $50 a month budget I‚Äôd get the cheapest Codex plan and top the rest up with Kimi credit or the cheapest Zen Black plan.",
          "score": 6,
          "created_utc": "2026-02-22 07:47:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6sasab",
              "author": "BodeMan5280",
              "text": ".... how do you justify so many plans?! I find that multiple different coding assistants are helpful, but $200/month helpful? And MULTIPLE? Unless you have a crazy budget im just wondering if your power usage is generating income and if the speed is truly worth the return?\n\nI have ChatGPT Plus and two free acvpints: Gemini Pro and Copilot Pro through my '.edu' account. Claude is too expensive and rate limits are just... yuck. Curious if any MAX plans are really worthwhile and im just a baby vibe coder lol",
              "score": 2,
              "created_utc": "2026-02-22 15:34:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6tib1k",
                  "author": "Optimal_Strength_463",
                  "text": "Yeah, fair point. I spend about ¬£750 a month on AI plans and make about ¬£12-18k in revenue. Most, if not all, is directly attributed the work those plans are used on.\n\nI also regularly max out all those plans 4 days into a 7 day limit and am trying to find ways to make them last longer hence the suggestions about Kimi etc.\n\nI work for myself now and do about 40-50 hours a week and drink a lot of coffee and have ADHD and Autism, so having a hive of developers working for me that are a bit dopey but don‚Äôt talk back or moan about snacks in the fridge is heaven compared to a previous role being a Director with a software org of 300+ people.",
                  "score": 3,
                  "created_utc": "2026-02-22 18:51:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6qjvj7",
          "author": "ganonfirehouse420",
          "text": "I went with the openrouter api. Pricing seems fair.",
          "score": 2,
          "created_utc": "2026-02-22 07:37:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r0xla",
          "author": "deadcoder0904",
          "text": "Minimax 2.5 & GLM 5 are not bad.\n\nKimi 2.5 too.\n\nCodex is the best. Combine that with Antigravity.\n\nBut yeah buy the Chinese models. They are cheaper. And free on OpenCode for example.",
          "score": 2,
          "created_utc": "2026-02-22 10:20:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70oyx1",
              "author": "rothnic",
              "text": "Can't use antigravity... pretty much everyone is getting banned at this point. And they don't pro-rate the month. ",
              "score": 1,
              "created_utc": "2026-02-23 20:54:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o72omnd",
                  "author": "deadcoder0904",
                  "text": "Just get a new a/c. Everyone who got banned got banned because of OpenClaw. Don't use that on ur new a/c. Simple.",
                  "score": 1,
                  "created_utc": "2026-02-24 03:26:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6qcouo",
          "author": "soul105",
          "text": "chutes.ai provides Kimi K2.5, GLM5 and others",
          "score": 4,
          "created_utc": "2026-02-22 06:30:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qlv9d",
              "author": "wallapola",
              "text": "How was the experience in terms of speed and reliability with chutes? Is it fine?",
              "score": 1,
              "created_utc": "2026-02-22 07:55:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6qtkey",
                  "author": "shadow1609",
                  "text": "Absolute catastrophe - imo only usable for async bots/automations with heavy retries and model fallback. You will need both. Nightmare for coding.",
                  "score": 3,
                  "created_utc": "2026-02-22 09:09:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6qvztc",
                  "author": "soul105",
                  "text": "I have been using it for a few days for personal projects, mostly Kimi K2.5 TEE.\n\nIt works very well, a bit faster when compared to the previous free offer from OpenCode Zen few weeks ago. The cost benefit is awesome, you get way more than you paid for, and I'm not sure how can they make money on it.\n\nIf you consider you get 300 calls/day for only $3 you cannot expect it being the favorite tool for vibe coders, that's why it gets so much hate.",
                  "score": 2,
                  "created_utc": "2026-02-22 09:32:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6pucga",
          "author": "cmbtlu",
          "text": "Only 1 and 2 are actually good value right now. 3 and 4‚Äôs models break down when solving actual real world problems and not building basic apps. \n\nCopilot gets you Opus 4.6 with more usage than a standard Claude subscription.\n\nSmartest model right now is probably Codex 5.3 but it‚Äôs not in Copilot currently so you‚Äôll need a ChatGPT subscription.",
          "score": 2,
          "created_utc": "2026-02-22 04:05:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6pyl35",
              "author": "keroro7128",
              "text": "copilot has this CodeX 5.3 model.ü§£",
              "score": 4,
              "created_utc": "2026-02-22 04:35:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6qklrg",
              "author": "albertortilla",
              "text": "The problem with copilot is that the context is limited to 100k",
              "score": 0,
              "created_utc": "2026-02-22 07:43:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6r3sjl",
                  "author": "armindvd2018",
                  "text": "Codex limits are 272K",
                  "score": 1,
                  "created_utc": "2026-02-22 10:47:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6x82jw",
                  "author": "Latter-Parsnip-5007",
                  "text": "Not a problem I ever ran into. Your tasks are too big or you work in the main agent",
                  "score": 1,
                  "created_utc": "2026-02-23 08:54:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6pmm64",
          "author": "Desperate-Bath5208",
          "text": "Z.AI Coding Plan",
          "score": 3,
          "created_utc": "2026-02-22 03:11:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qkw1p",
              "author": "dreamkast06",
              "text": "Not anymore due to weekly limits and reduced quotas. They also removed the ability to see when the 5 hours resets, so pretty obvious they are doing something fishy. The way they cache changed recently too so many requests are counting as a new prompt.\n\nMinimax is still good for the $10 as a backup for me.",
              "score": 2,
              "created_utc": "2026-02-22 07:46:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6qsxt4",
                  "author": "Tadomeku",
                  "text": "Opencode tells me when the timer resets? It literally says your window will reset at 11:00 or whatever. It shows up on Red.  \n   \nI use GLM myself. Happy with it.",
                  "score": 4,
                  "created_utc": "2026-02-22 09:03:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ql5vx",
          "author": "gmakkar9",
          "text": "I have been using Ollama Cloud recently. Good quotas but sometimes they are resource constrained and the model won't respond fast.",
          "score": 1,
          "created_utc": "2026-02-22 07:49:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qr7wq",
          "author": "felixgar",
          "text": "I am using Claude‚Äôs Opus 4.6 for planning and Minimax for execution and reviewing due to low cost. It works fine so far, but this split is mostly due to the extreme low session limit on Claude pro plan. Two plans done by opus and the limit is reached :/ Anyone ideas for a better split?",
          "score": 1,
          "created_utc": "2026-02-22 08:46:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6si250",
          "author": "hicder",
          "text": "i'm thinking of the following setup:\n* $20/mo Codex plan\n* $20/mo Synthetic plan\n\nUsecase:\n* Use GPT-5.3-Codex for planning. optionally specify Kimi K2.5 (through Synthetic) for the Explore subagent\n* Use Kimi K2.5 for implementation",
          "score": 1,
          "created_utc": "2026-02-22 16:06:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6skd7d",
              "author": "CantFindMaP0rn",
              "text": "Since I'm already on the Claude 5x Max plan, I don't think I'm going to spend that much more for \"dumb workhorse\" implementation subagent.\n\nThat being said, I'll sign up for Github Copilot's Pro+ to see how much more I can push it before hitting the limit (I can work around that 128k context window with better prompting strategy/skills/compaction), and rotating between whatever free models Zen and nvidia are offering at the time for my implementation subagent.  \n  \nMaybe if I can still one-shot with Copilot, I'd drop Claude entirely and get Minimax/Kimi plans for subagents instead.",
              "score": 1,
              "created_utc": "2026-02-22 16:16:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6slael",
          "author": "tonio_i",
          "text": "Got good use out of Codex $20 plan. But slower compared to Antropic plans, but far more generous quotas and nice context window.¬†\n\nAlso tried the Moonshot Kimi 2.5 plan, bargained for $5 (regular $20). Worth the $5 but not the $20, far lower quality and quota compared to the $20 Codex.\n\n\nCopilot is also nice with all the models being offered, lower context but still worth it. Very generous quotas.\n\n\nNice way that I found to maximize the usage is to use one provider for the primary agent and another provider for subagent.",
          "score": 1,
          "created_utc": "2026-02-22 16:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tg3n5",
          "author": "gasmanc",
          "text": "What‚Äôs the difference between the codex plan and ChatGPT plus????",
          "score": 1,
          "created_utc": "2026-02-22 18:41:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6tn0as",
              "author": "dynacx",
              "text": "Nothing, they're the same picture.\n\nI think it's just the AI made a mistake when summarising.",
              "score": 1,
              "created_utc": "2026-02-22 19:14:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7aeduw",
          "author": "crxssrazr93",
          "text": "Is the Codex plan different from ChatGPT Plus plan? If I have a ChatGPT Plus plan, can I purchase a Codex plan also?",
          "score": 1,
          "created_utc": "2026-02-25 07:28:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bsdbp",
          "author": "ecofield",
          "text": "I also tried abacus routeLLM  for 7$ and it basically used 90% of time claude behind and burned the 14K credits( they use credits not tokens) in less than an hour.\n\nI want my money back üò¢",
          "score": 1,
          "created_utc": "2026-02-25 14:01:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ciuzj",
          "author": "Virtual-Honeydew6228",
          "text": "Codex plan is the GOAT at the moment",
          "score": 1,
          "created_utc": "2026-02-25 16:11:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pulcy",
          "author": "lundrog",
          "text": "Don't forget ollama cloud. Synthetic.new is good but over capacity or i would share a referral.",
          "score": -1,
          "created_utc": "2026-02-22 04:07:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qlxfk",
              "author": "wallapola",
              "text": "But they change how they count tool calls and it is so limiting ü•≤",
              "score": 2,
              "created_utc": "2026-02-22 07:56:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o704weq",
                  "author": "Simple_Split5074",
                  "text": "Just looked at it. 500 per day? Useless ",
                  "score": 2,
                  "created_utc": "2026-02-23 19:19:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6pre3p",
          "author": "NaturalRedditMotion",
          "text": "What I do is the $10 copilot plan along with $10 plan from chutes. That way I have access to all sota models via copilot and have access to all open source models via chutes. You can opt for the $20 plan from chutes and that will get you 5000 requests per day if the $10 plan wouldn‚Äôt work for you. I do the planning via sonnet and implementing the plan via kimi k2.5. This setup works for me.",
          "score": 0,
          "created_utc": "2026-02-22 03:44:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6udslg",
              "author": "robercleverson",
              "text": "Doesn't chutes serve quantized versions of those models?",
              "score": 1,
              "created_utc": "2026-02-22 21:28:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6q4ctm",
          "author": "MorningFew1574",
          "text": "Add Kilocode api into the mix...",
          "score": 0,
          "created_utc": "2026-02-22 05:19:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r45jf",
              "author": "armindvd2018",
              "text": "Kilo?\n\nThose thieves! They copy Roocode and Cline like-for-like! They use Openrouter behind their thin proxy! So we will use the source, not the proxy!",
              "score": 1,
              "created_utc": "2026-02-22 10:50:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6udkhz",
                  "author": "robercleverson",
                  "text": "And kilo code cli is nothing more than a sloppy reskin of opencode",
                  "score": 2,
                  "created_utc": "2026-02-22 21:27:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6pmfgw",
          "author": "No_Success3928",
          "text": "Synthetic, hands down üëç",
          "score": -5,
          "created_utc": "2026-02-22 03:10:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pjb15",
          "author": "HarjjotSinghh",
          "text": "ah hell yeah, saturate context window and call me!",
          "score": -6,
          "created_utc": "2026-02-22 02:49:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra62db",
      "title": "Is it possible that I've been using OpenCode for over a month now and these are the stats?",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/xq4tulukipkg1.jpeg",
      "author": "TheOnlyArtz",
      "created_utc": "2026-02-20 20:10:32",
      "score": 55,
      "num_comments": 31,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1ra62db/is_it_possible_that_ive_been_using_opencode_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6hlrmb",
          "author": "cri10095",
          "text": "Which models did you used mostly?",
          "score": 6,
          "created_utc": "2026-02-20 20:43:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hlwot",
              "author": "TheOnlyArtz",
              "text": "5.2-codex and 5.3-codex",
              "score": 7,
              "created_utc": "2026-02-20 20:44:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6hn8g3",
                  "author": "cri10095",
                  "text": "Wow high tier!",
                  "score": 5,
                  "created_utc": "2026-02-20 20:50:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6luz1u",
                  "author": "WalrusCritical773",
                  "text": "what do you mean, codex 5.3 is not available in API so cost stats are meaningless because u're using ChatGPT Plus..?",
                  "score": 1,
                  "created_utc": "2026-02-21 14:44:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6hnub0",
          "author": "thedarkbobo",
          "text": "lul I had like 100M once I started with deepseek, now I dont even count",
          "score": 4,
          "created_utc": "2026-02-20 20:53:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kvjdr",
              "author": "FriendlyUser_",
              "text": "this is the way. Im already on 60$ this month‚Ä¶ 1,5B tokens or what ever üòÖ",
              "score": 1,
              "created_utc": "2026-02-21 10:12:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6kz6db",
                  "author": "thedarkbobo",
                  "text": "Nice,am on two plans but will keep glm only",
                  "score": 1,
                  "created_utc": "2026-02-21 10:48:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o757xeu",
              "author": "ab2377",
              "text": "i haven't used it so i have no idea what you guys are talking about, do you get subscription or api keys for deepseek and use opencode? how much does it cost, is api keys good deal for deepseek?",
              "score": 1,
              "created_utc": "2026-02-24 14:52:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6qd0yi",
          "author": "Limp_Carpet1444",
          "text": "![gif](giphy|YmQLj2KxaNz58g7Ofg)",
          "score": 3,
          "created_utc": "2026-02-22 06:33:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hg217",
          "author": "HarjjotSinghh",
          "text": "this openai powerhouse is running my finances? wow.",
          "score": 5,
          "created_utc": "2026-02-20 20:15:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kjy2w",
          "author": "yuckygpt",
          "text": "is this a native feature in opencode?",
          "score": 2,
          "created_utc": "2026-02-21 08:17:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kk7qa",
              "author": "TheOnlyArtz",
              "text": "Yes \\`opencode stats\\`",
              "score": 3,
              "created_utc": "2026-02-21 08:20:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6kkaa1",
                  "author": "yuckygpt",
                  "text": "ahh, I meant the caching",
                  "score": 1,
                  "created_utc": "2026-02-21 08:21:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6nx3hb",
          "author": "touristtam",
          "text": "Been using opencode for a while now:\n\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ‚îÇ                       OVERVIEW                         ‚îÇ\n    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n    ‚îÇSessions                                          1,847 ‚îÇ\n    ‚îÇMessages                                        100,207 ‚îÇ\n    ‚îÇDays                                                173 ‚îÇ\n    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ‚îÇ                    COST & TOKENS                       ‚îÇ\n    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n    ‚îÇTotal Cost                                     $7059.44 ‚îÇ\n    ‚îÇAvg Cost/Day                                     $40.81 ‚îÇ\n    ‚îÇAvg Tokens/Session                                 3.5M ‚îÇ\n    ‚îÇMedian Tokens/Session                            915.6K ‚îÇ\n    ‚îÇInput                                           2777.5M ‚îÇ\n    ‚îÇOutput                                            41.0M ‚îÇ\n    ‚îÇCache Read                                      3398.9M ‚îÇ\n    ‚îÇCache Write                                      227.4M ‚îÇ\n    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
          "score": 2,
          "created_utc": "2026-02-21 21:02:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rdkjd",
              "author": "UseMoreBandwith",
              "text": "my 'Avg Tokens/Session' is 27.6M ,   \nand only 22 sessions.  \n  \nYou seem to use lots of sessions.",
              "score": 1,
              "created_utc": "2026-02-22 12:15:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s763t",
                  "author": "touristtam",
                  "text": "I do. Two reasons for that: Opencode seems have had memory leaks at time and Opencode lacks a mechanism to re-load the config/custom tools/skills that I would be working on.",
                  "score": 2,
                  "created_utc": "2026-02-22 15:17:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6o08u8",
              "author": "KaMaFour",
              "text": "Bro... This is the stage you start thinking about buying a B200...",
              "score": 0,
              "created_utc": "2026-02-21 21:19:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6hj4mz",
          "author": "soul105",
          "text": "Yes, its possible.",
          "score": 1,
          "created_utc": "2026-02-20 20:30:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hjc5z",
              "author": "TheOnlyArtz",
              "text": "That's insane isn't it?",
              "score": 0,
              "created_utc": "2026-02-20 20:31:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6hw3av",
          "author": "dodistyo",
          "text": "what provider?",
          "score": 1,
          "created_utc": "2026-02-20 21:34:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6limbc",
          "author": "SnooGiraffes625",
          "text": "1.5 months 99M 2000m cached\n\nhttps://preview.redd.it/4z0tcaotnukg1.jpeg?width=4000&format=pjpg&auto=webp&s=1d526b44b2ea63d8508e39b3adcbbc4efbf5aa23",
          "score": 1,
          "created_utc": "2026-02-21 13:28:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nz98y",
          "author": "KaMaFour",
          "text": "https://preview.redd.it/bvb227xtywkg1.png?width=493&format=png&auto=webp&s=408a77a9ab15bfef873490ff5c00b43cc9a78bd6\n\ndunno man, costsaving works for me",
          "score": 1,
          "created_utc": "2026-02-21 21:14:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xd3hc",
              "author": "Latter-Parsnip-5007",
              "text": "Local inference be like 0.00$ 17,3M out. Qwen coder is soooo good",
              "score": 1,
              "created_utc": "2026-02-23 09:44:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7bgniv",
          "author": "fahimalizain",
          "text": "https://preview.redd.it/1rxkmk3a1nlg1.png?width=818&format=png&auto=webp&s=8f472d51de7a7417d949fac7371539e9f7cae693\n\nMostly kimi-k2.5 for me.",
          "score": 1,
          "created_utc": "2026-02-25 12:54:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6igzob",
          "author": "Empty-Sandwich-7092",
          "text": "How do I find these costs? Is this some kind of special feature? And where can I see the stats?. And  how are you using opencode?\n\nHow I get this costs? It's something special? and how I can see that stats?How do I find these costs? Is this some kind of special feature? And where can I see the stats?How do I find these costs? Is this some kind of special feature? And where can I see the stats?",
          "score": 0,
          "created_utc": "2026-02-20 23:25:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jwfmc",
              "author": "TheOnlyArtz",
              "text": "`opencode stats`",
              "score": 4,
              "created_utc": "2026-02-21 04:51:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6mw596",
          "author": "HarjjotSinghh",
          "text": "my brain just upgraded to gold standard mode.",
          "score": 0,
          "created_utc": "2026-02-21 17:54:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rce8jw",
      "title": "Built an open-source Telegram client for OpenCode CLI ‚Äî now dogfooding it daily from my phone",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rce8jw/built_an_opensource_telegram_client_for_opencode/",
      "author": "Less_Ad_1505",
      "created_utc": "2026-02-23 10:46:21",
      "score": 52,
      "num_comments": 11,
      "upvote_ratio": 0.97,
      "text": "OpenCode CLI has become my primary dev tool, and I want to give a huge shoutout to its authors for building such an incredible piece of software. The models seem to handle context and logic particularly well in it, especially when using the Plan agent first and then switching to Build.\n\nEven before Openclaw became popular, I kept thinking how useful it would be to access OpenCode from my phone. I noticed OpenCode has a server mode, which meant building a custom client was totally doable. Initially, I just wanted to write a simple Telegram bot for my own needs. But, as it usually goes, I got carried away, added more features, and eventually decided to open-source the project.\n\nI definitely won't call it \"fully functional\" yet - there are still rough edges. However, it currently has enough features to be used for actual development.\n\nHere is what works right now:\n\n* Switching between projects and sessions.\n* Selecting the agent, model, and variant (reasoning effort).\n* Tracking the agent's progress on a task.\n* Receiving code diffs directly in the chat as text files.\n\nIronically, I'm now at the point where I use the bot to write code for the bot itself. It‚Äôs a pretty great feeling to lie on the couch, watch a TV series, and casually send dev tasks to the agent via Telegram on my phone.\n\nI plan to keep actively developing the project since I use it daily. If anyone wants to try it out, the repo is here: [https://github.com/grinev/opencode-telegram-bot](https://github.com/grinev/opencode-telegram-bot)\n\nI would be really grateful for any feedback, thoughts, or suggestions!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rce8jw/built_an_opensource_telegram_client_for_opencode/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o6y66na",
          "author": "throwaway12012024",
          "text": "i am doing exactly the same! Great project! I am sad bc just discovered your work now so i am a bit in the sunken cost thing.",
          "score": 1,
          "created_utc": "2026-02-23 13:36:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o729fdr",
          "author": "bradjones6942069",
          "text": "Open code has become my daily driver as well. I would love to try this out",
          "score": 1,
          "created_utc": "2026-02-24 01:57:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74yj13",
              "author": "bradjones6942069",
              "text": "Been trying it out since last night. Works great.",
              "score": 1,
              "created_utc": "2026-02-24 14:04:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75oe9h",
          "author": "oVerde",
          "text": "Is the bot publicly available? So anyone would run agents on my machine?",
          "score": 1,
          "created_utc": "2026-02-24 16:09:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75pesn",
              "author": "Less_Ad_1505",
              "text": "No, the basic idea is that the bot only works with one allowed userId for security purposes.",
              "score": 1,
              "created_utc": "2026-02-24 16:14:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75pxyt",
                  "author": "oVerde",
                  "text": "thanks",
                  "score": 1,
                  "created_utc": "2026-02-24 16:16:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xk26d",
          "author": "HarjjotSinghh",
          "text": "this is how you turn phone dev dreams real!",
          "score": 0,
          "created_utc": "2026-02-23 10:50:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xlrh2",
              "author": "Less_Ad_1505",
              "text": "You can also run this bot on a remote server to have a personal assistant right in Telegram. It won't be as proactive as Openclaw, but you can still do a lot with it and see exactly what's happening in detail",
              "score": 1,
              "created_utc": "2026-02-23 11:06:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xp91n",
          "author": "revilo-1988",
          "text": "Mal ne Frage was ist den so n√ºtzlich um es vom Handy aus zu machen?",
          "score": -2,
          "created_utc": "2026-02-23 11:37:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfdadw",
      "title": "Stay away from  synthetic.new",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rfdadw/stay_away_from_syntheticnew/",
      "author": "Codemonkeyzz",
      "created_utc": "2026-02-26 15:34:01",
      "score": 48,
      "num_comments": 45,
      "upvote_ratio": 0.82,
      "text": "I saw this provider a lot in reddit. Some guys keep promoting it and i got hooked.  20 USD a month, x3 Claude Usage ,  no weekly limits.  Too good to be true. However,  there are a problems with the provider: \n\n 1.  **Standard Plan 5 hour  limit is  x3 of Claude Pro Plan:**   Maybe this is correct in theory, but in practice not at all.  Maybe due to caching or another reason,   the plan hits the limit pretty quickly.  Also I believe Chinese models can be inefficient with the tool calling  hence,   Standard Plan 5 hour limit is same as  Codex/Claude  20 USD plan.  \n  \n2. **Impractial Usage:**  Since  for a regular coding task you will hit  5 hour limit pretty quickly on their standard model , having no weekly limit has no advantages for the developers at all.  The existing plan is actually made for the abusers , which is funny cause the provider keep complaining about some accounts abusing their system while  they are the one actually allowing it in the first place.  Cause the provider is  for  bots  not for regular developer.\n\n3. **Price Increase:**  They increased the price  from  20 USD to **30 USD** for standard plan last night . Their ratioanle is  \"They need a lot of compute\".  But the reason for the need for compute is  that, their bad planning.  There's no way an everyday  coder/user can abuse this system,  you need to be  24/7 online, which means this for bots and bots are abusing it but they want everyone to pay for it. \n\n**4. Delayed model release:**   Even  opencode was serving GLM5 , Minimax M2.5 and Kimi K2.5  for **free.** And as of today, they are still not serving  GLM5 and Minimax M2.5  only  K2.5.  They are using the same excuse ;   shorteage of compute/GPUs.\n\n  \nI already cancelled my subscription. Just shariing this so that , you don't fall for their false advertisement on reddit as i did. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rfdadw/stay_away_from_syntheticnew/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7ja996",
          "author": "FlyingDogCatcher",
          "text": "I have had their pro sub for a while. Even with the pricing changes still one of the best values you can get and has been a stable and fast provider.\n\nI like synthetic.new quite a lot. I understand their marketing but it doesn't make a ton of sense to directly compare them to claude. They are hosting free OSS models for you.\n\nMy prediction by the end of the year is that the buffet-style AI plans from the big guys are either going to get killed or massively nerfed as they try to make the the numbers make sense. Synth doesn't have the luxury of bottomless investment money so they are making that adjustment now",
          "score": 16,
          "created_utc": "2026-02-26 16:09:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jcbxd",
              "author": "Codemonkeyzz",
              "text": "I am just explaining myself from a developer point of view. I believe synthetic  current pricing make s a lot of sense for openclaw and bots, but it is definitely not developer friendly. ",
              "score": -1,
              "created_utc": "2026-02-26 16:18:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7jckfj",
                  "author": "FlyingDogCatcher",
                  "text": "If you say so. Been working great for me.",
                  "score": 5,
                  "created_utc": "2026-02-26 16:19:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7jd630",
                  "author": "ZeSprawl",
                  "text": "If anything the new pricing makes openclaw far less attractive, and has made it better for devs. I use it for daily software dev, and I voluntarily converted my plan yesterday from pro to two packs, and coded for 6+ hours without hitting a limit.",
                  "score": 3,
                  "created_utc": "2026-02-26 16:22:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7j434t",
          "author": "dengar69",
          "text": "Thanks.  I was looking into this but for $30 I'll just pay $9 more and get Github Copilot+.",
          "score": 10,
          "created_utc": "2026-02-26 15:41:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7j4x61",
              "author": "Codemonkeyzz",
              "text": "That's what I did. Using codex +Copilot pro + nanogpt .  Good value of money. 38 USD. \n\nI think synthetic only makes sense with openclaw or something. Not for development",
              "score": 4,
              "created_utc": "2026-02-26 15:44:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7jeei0",
                  "author": "sucksesss",
                  "text": "can we use an AI model like Opus from Copilot Pro+ in OpenCode using an API key?",
                  "score": 1,
                  "created_utc": "2026-02-26 16:28:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ld95b",
              "author": "Legal_Dimension_",
              "text": "Just be aware of the GitHub copilot models, they don't like agent MCP/tool calls and can fail alot.",
              "score": 1,
              "created_utc": "2026-02-26 22:00:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lt1c7",
                  "author": "FlyingDogCatcher",
                  "text": "They use the same models you can get anywhere else",
                  "score": 1,
                  "created_utc": "2026-02-26 23:21:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7jc3qc",
          "author": "Infinite_Grab_7315",
          "text": "I agree too. I used to be subscribed. But they have changed their plan terms twice already in a single month. They really shot themselves in the foot by over promising in my opinion. \n\nThey also introduced a weird 500 tool call limit a day which is super easy to hit",
          "score": 4,
          "created_utc": "2026-02-26 16:17:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jck0v",
              "author": "Codemonkeyzz",
              "text": "Yeah i forgot to mention that 500 tool limit introduction  in my post. ",
              "score": 1,
              "created_utc": "2026-02-26 16:19:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ljl4e",
          "author": "GoranKrampe",
          "text": "I have been using Synthetic quite a lot and I find them super friendly and honest. So the title here irks me the wrong way \"stay away\" etc. Also, I do not think they have been doing much advertising (I may be wrong) but rather users have recommended them (I have done it a few times in comments). It is a very small company and they are bending over backwards to make the leading open weight models work well from the harnesses we like the most like Claude Code and Opencode etc. I started with the $20 which IMHO was much harder to run out of than the Anthropic Pro subscription. Then I upped to the $60 option which gave 10x and I never ever came close running out. But as Synthetic have clearly admitted, it was a non sustainable plan. I think the new plan scheme with \"packs\" is a good idea and I have every trust in that they are honestly trying to find a model that works well.  \n  \nAnyway, I am super happy with their service, they are extremely helpful and very transparent in what they do. I use mainly Kimi K2.5, Minimax M2.5 and a bit of Deepseek 3.2/GLM4.7. And I use it purely for coding. Just my 2c, felt this thread was... oddly negative given mine and many others experiences. :) ",
          "score": 10,
          "created_utc": "2026-02-26 22:31:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7llqhy",
              "author": "commandedbydemons",
              "text": "Full agree.",
              "score": 2,
              "created_utc": "2026-02-26 22:42:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7m5u16",
              "author": "Farm_Boss826",
              "text": "Fully agree, I am in the pro plan and never came even close to max it out for normal development task.",
              "score": 0,
              "created_utc": "2026-02-27 00:32:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7mhv30",
              "author": "Terryble_",
              "text": "I was on their $20 plan and even I thought it was pretty generous before they moved to their current pricing model.\n\nI'm also baffled by this post because the Synthetic team has been very honest about the way they operate. They're also responsive in their Discord, so it's hard to think badly of them.\n\nThey're also helpful in their GitHub. I contributed a fix to their harness, [Octofriend](https://github.com/synthetic-lab/octofriend), and they were responsive all throughout the process of getting the PR merged.\n\nI still have a lot of trust in Synthetic at this point in time. From my perspective, they're still one of the good ones, so I have no reason to stop using them. ",
              "score": 0,
              "created_utc": "2026-02-27 01:40:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7mythu",
              "author": "Codemonkeyzz",
              "text": "My post wasn't about they being dishonest or unfriendly. They are honest and friendly but they have a very bad business plan. It has been bad all along and they are not making any efforts to improve it.  As i mentioned,  their existing plans are not developer friendly,  they make their plans attractive for bot users, and yet complain the very same bot users are abusing their system.\n\nTheir recent  announcements, their recent  pricing changes  and everything proves my  point.",
              "score": -2,
              "created_utc": "2026-02-27 03:19:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7j7b3b",
          "author": "LittleChallenge8717",
          "text": "sadly it's true, initially it was great, Did't like new system at all",
          "score": 3,
          "created_utc": "2026-02-26 15:55:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7javue",
          "author": "exploriann",
          "text": "It used to be good, but they introduced free tool calling daily limit then they hiked the price then removed the 10x pro plan, and reduced the concurrency to 1 per model on the 30$ plan, not worth it anymore.",
          "score": 6,
          "created_utc": "2026-02-26 16:12:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jatcf",
          "author": "Juan_Ignacio",
          "text": "I agree. For me, the best value-for-money plans right now are Codex or Minimax. I really like Kimi as well, but its limits don‚Äôt make much sense to me.\n\nI‚Äôm also paying for Chutes, even though it‚Äôs not very popular because of the NanoGPT controversy and some other issues. Overall, the limits are actually pretty decent, but it has a serious speed problem with several of its providers. That said, if you‚Äôre using it in the background, switching between providers, or running agents like oh-my-opencode, it‚Äôs not that bad. However, if you need fast results, it‚Äôs definitely not a good option.\n\nIt‚Äôll be interesting to see what the limits look like for the new Open Zen Go subscription. I saw in another Reddit thread that, according to someone‚Äôs calculations, it comes out to around $20/month in Minimax/Kimi/GPT equivalent usage.",
          "score": 3,
          "created_utc": "2026-02-26 16:11:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jc08u",
              "author": "Codemonkeyzz",
              "text": "\"even though it‚Äôs not very popular because of the NanoGPT controversy and some other issues.\"\n\nHaven't heard about Chutes.  What's the controversy and what issues ?",
              "score": 1,
              "created_utc": "2026-02-26 16:17:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7jd7hn",
                  "author": "Juan_Ignacio",
                  "text": "Chutes is a provider that hosts its own models. Besides their direct subscription, you might also be using them indirectly through OpenRouter or other platforms.\n\nA few months ago, I think Chutes publicly accused NanoGPT of using stolen credit cards or engaging in unfair competition (or something along those lines), but as far as I know, there wasn‚Äôt much solid evidence provided.\n\nI only followed the situation loosely, though. I mostly heard about it because people in the NanoGPT subreddit and Discord *really* dislike Chutes lol.",
                  "score": 3,
                  "created_utc": "2026-02-26 16:22:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7jlpu3",
              "author": "Ang_Drew",
              "text": "well, some ppl posted about chutes controversy üòÖ\nthey said chutes quantized the model and being slow and hallucinating \n\ni honestly dont use it a lot that time (it was Nov 2025 btw), and from price point glm coding plan is more sexy to me back then..\n\nnow a new reddit popped said synt is not worth it..\n\ni think synthetic is pretty generous calculating tool calling 0.1 request. it makes other similar competitor (copilot, nano gpt, chutes, etc.. you name it) looks not worth it.. (at least that what i feel! about the pricing point)\n\nin practice it might not last long enough like the OP mentioned (chinese model inefficient in tool calling), but it's still good deal, and if they can last longer run than the big labs (openai, anthropic), it could be a good news for us actually.. we can still \"vibe code\" amidst chaos üòÇ\n\nim looking forward to synthetic tho.. im with codex now because the model is so freaking good..",
              "score": 1,
              "created_utc": "2026-02-26 17:01:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jc2eb",
          "author": "RiskyBizz216",
          "text": "their subscription was a bad deal at $20 too.\n\nI stayed on the pay-as-you-go because its a little faster than openrouter.\n\nollama cloud is a good alternative if you want to stay in the $20 price range. they also have a free tier [https://ollama.com/pricing](https://ollama.com/pricing)\n\nor you could just sign up for NVidia NIM and get a free api key [https://build.nvidia.com/settings/api-keys](https://build.nvidia.com/settings/api-keys)\n\nor you can sign up to iFlow and get a free api key\n\n[https://platform.iflow.cn/en/models](https://platform.iflow.cn/en/models)",
          "score": 2,
          "created_utc": "2026-02-26 16:17:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jg172",
              "author": "flobblobblob",
              "text": "I've seen NVidia NIM mentioned a few times, sounds too good to be true so what is the catch? Why isn't everyone just doing that?",
              "score": 3,
              "created_utc": "2026-02-26 16:35:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7jkep0",
                  "author": "mdn0",
                  "text": "Everyone is doing that of course. So you have to wait, wait, wait...",
                  "score": 2,
                  "created_utc": "2026-02-26 16:55:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7jo7vz",
                  "author": "RiskyBizz216",
                  "text": "Very generous of NVidia to give it to us for free, but it can be unreliable. If you're using the high demand models like glm5/ kimi k2.5 then it will be slow. But other models like llama ones are pretty snappy.\n\nIt all depends on your use case.",
                  "score": 1,
                  "created_utc": "2026-02-26 17:13:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7lxx42",
          "author": "jerieljan",
          "text": "So their landing page still says:\n> Subscribe for $20/month, and use models in our app\n\nBut going to pricing with 1 subscription pack (default) indicates $1/day - $30/mo\n\nHeh, they clearly didn't update those pricing changes fully.",
          "score": 1,
          "created_utc": "2026-02-26 23:48:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mgfl1",
          "author": "klippers",
          "text": "Thank you for the heads up. I was actually going to sign up this weekend. \n\nI'm now looking at nano-gpt",
          "score": 1,
          "created_utc": "2026-02-27 01:32:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mnjgl",
          "author": "Maleficent_Radish807",
          "text": "I was also on their pay-per-use and on the wait list for the subscription.\n\nMy initial testing is that models like GLM 4.7 or Kimi 2.5 are running at half the token per second then directly from z.ai or moonshot.\n\nAnyone else experiencing that?",
          "score": 1,
          "created_utc": "2026-02-27 02:13:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jdass",
          "author": "qmrelli",
          "text": "I can also recommend you to try [Dvina Code](https://dvina.ai/p/dvinacode/dvinacode), it has more usage of Opus 4.6, you can test it yourself in free plan, also if you wanna upgrade to pro I can give you 1 month free plus membership code, DM if you want that.",
          "score": 1,
          "created_utc": "2026-02-26 16:23:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jn5mw",
          "author": "Bob5k",
          "text": "have in mind opencode was serving those for free for a few days. now they charge you the standard price.  \namongst all other providers for now, synthetic is one of a very few that hosts opensource models in US (mainly) while setting up 0 data retention overall and maintaining a quite good overall quality of the model for everyone while also keeping no weekly cap at all. As 5h caps might be low or high, i don't care but what will stop you from progressing with your projects is a weekly cap suddenly hit after 3 days. and then you're done.   \ncan't be compared to gh copilot or anything monthly based quota as the quota amount provided by synthetic will be indefinitely higher than gh copilot pro.   \nalso the reasoning is not 'they need more gpu' but rather 'they need to make the plan fair for everyone' - if you'd read the announcement correctly then it'd be probably clear (or copy paste it into some ai to explain to you). It still makes a ton of sense for developers as you still get 135 prompts per 5h which is really hard to cap these out working on some serious projects and not just pushing idiotic prompts blindly around to force the ai to progress. And also it makes total sense for me to set the change so I as a dev can receive 80tps+ on kimi k2.5 because the infra doesn't struggle with openclaw bots just abusing the system to the very limit. And not gonna lie - their previous limit of 1.35k prompts per 5h sounded great on paper, but i always wondered how they can sustain it as I as dev even running parallel sessions was able to maye reach 400 prompts usage per 5h max - and yet some people abused the pro plan to very last token. ",
          "score": 1,
          "created_utc": "2026-02-26 17:08:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7k0lng",
              "author": "Infinite_Grab_7315",
              "text": "More than this i feel that current users and terms were not honoured. I was happy paying 20$ but a 50% increase I am not willing to\n\n135 calls with 0.1 toolcall made sense for devs. Since my total tool calls were less\n\nIf I start an explore task on opencode it takes up 40-50 tool call easily. So I can explore at max 1 times. It is same for things like review or any subagents or normal coding. After 500 it is counted as normal which is also easy to hit. Before I would have to make ~600 every 5hr which was generous \n\nAll in all I am not fan of their changes. But what worries me is that if they can change limits like this they will do it again in future and it keeps getting worse for us",
              "score": 2,
              "created_utc": "2026-02-26 18:10:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7k7ve9",
                  "author": "Bob5k",
                  "text": "Nah i don't think anything will be changed there anytime soon. Also they have monthly subscription and not annual so after all if you're unhappy - you can just unsubscribe and move on. For me legacy plan i have now seems to be just fine for real world usecases when I'm running Kimi K2.5. they don't host any annual plan which feels fair as if any changes are done then you can just move on. \n\nI'll just mention annual plans that messed up their userbase way way more that changed recently - Trae (totally messed up any usage within 100$ range), glm reducing quota usage by 33% and adding weekly caps, Kimi code being unusable for any serious dev under 79$ range because of weekly cap aswell. \nNot defending synthetic completely but they still have most balanced subscription model out there that feels just fair considering both sides of the business. Have in mind for past year everyone was running ai inference at a loss so it's not a big surprise companies are changing their plans and basically this is sort of an end of\n1. Using single provider all the time as terms will change \n2. Cheap vibecoding using sota or top opensource models at 10$ / monthly with almost unlimited usage.\n\nPeople will need to realize that running ai is expensive and nobody will throw own money at the business forever for peeps to just run their openclaw instances. And tbh for any sort of serious development price range within 40/60$ / month is still kinda occupied mainly by synthetic if you'd want a reliable provider for 5-8-10h of work daily.",
                  "score": 2,
                  "created_utc": "2026-02-26 18:43:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7n6rg0",
          "author": "New-Fuel-2735",
          "text": "Immediately unsubbed when i discover alibaba coding plan. It rendered any other sub right now obsolete",
          "score": 1,
          "created_utc": "2026-02-27 04:09:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7nbozz",
              "author": "Codemonkeyzz",
              "text": "Wasn't aware of their plan. Just had a look , it looks like a great deal. Will definitely try it",
              "score": 0,
              "created_utc": "2026-02-27 04:42:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1refkcr",
      "title": "kimi k2.5 vs glm-5 vs minimax m2.5 pros and cons",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1refkcr/kimi_k25_vs_glm5_vs_minimax_m25_pros_and_cons/",
      "author": "tomdohnal",
      "created_utc": "2026-02-25 15:02:02",
      "score": 47,
      "num_comments": 27,
      "upvote_ratio": 0.98,
      "text": "in your own subjective experience, which of these models are best for what types of tasks?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1refkcr/kimi_k25_vs_glm5_vs_minimax_m25_pros_and_cons/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7c7h9a",
          "author": "rodabi",
          "text": "Out of the three I was really unimpressed with k2.5 and minimax. But GLM-5 feels like the real deal. It can actually do tool calls properly, and it's been pretty successful at larger changes. I'd rate it similar to Sonnet 4.6",
          "score": 32,
          "created_utc": "2026-02-25 15:18:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cstuh",
          "author": "SvenVargHimmel",
          "text": "GLM5 is pretty solid. \n\nKimi k2.5 is the eager junior but it gets it right but fails in more complex reasoning planning scenarios and on larger codebases.  \n\nMinimax m2.5 - i don't bother with. Kimi k.25  does build steps much better. \n\nNone of these models are good in planning mode for anything that is non trivial or unconventional. \n\n  \nMay workflow is:  Plan Mode ( Gemini Pro 3.1 ) , Build Mode (Kimi k.2.5) \n\nIf am relying on the open source model strictly then I \n\nin Plan Mode (Kimi | Minimax ) generate a spec file  exit and the start build mode with Kimi 2.5 referencing the spec file. \n\nI have not tried 1.) ralph loops yet with any of these models",
          "score": 9,
          "created_utc": "2026-02-25 16:56:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d1z4a",
          "author": "forgotten_airbender",
          "text": "Kimi 2.5 and glm 5x¬†\nBoth have their own strengths. Glm‚Äôs coding ability is better, kimi writes better + tool calling + orchestration.¬†\nDidnt find minimax 2.5 that good tbh. Okay as a fallback model but not as a main coding agent.¬†",
          "score": 7,
          "created_utc": "2026-02-25 17:38:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ggl4e",
              "author": "lundrog",
              "text": "This",
              "score": 1,
              "created_utc": "2026-02-26 04:05:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7caqes",
          "author": "seymores",
          "text": "I have not use GLM but paid for MiniMax and Kimi recently. MiniMax is the dumb one consistently.\nI did not expect much from Kimi but it gets code done quickly and correctly whilst MiniMax is a waste of time and money for coding.\nI am heavy Codex user so I evaluated on the basis of Codex 5.3 as the benchmark.",
          "score": 12,
          "created_utc": "2026-02-25 15:34:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cbpfg",
              "author": "Potential-Leg-639",
              "text": "All are selling quite quantized versions for sure",
              "score": 0,
              "created_utc": "2026-02-25 15:38:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7emamq",
          "author": "Key_Credit_525",
          "text": "all of them just smart anough to implement detailed plan given them by Opus",
          "score": 3,
          "created_utc": "2026-02-25 21:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c79qc",
          "author": "Honest-Ad-6832",
          "text": "Well, glm just screwed me over by doing git checkout without stashing changes, and there were a lot of changes. So there's that I guess. Still, it¬†is smart. And slow.¬†\n\n\nYesterday both mm and kimi failed to debug something which codex oneshotted.¬†\n\n\nMinimax is fast but not to bright. Best for chores.¬†\n\n\nGlm feels smartest but the speed is annoying.¬†\n\n\nHaven't used kimi much.",
          "score": 3,
          "created_utc": "2026-02-25 15:17:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cdtn8",
              "author": "DasBlueEyedDevil",
              "text": "Claude did the same shit to me, tbh.  More than once actually.  I had to build in a hard stop because the bastard kept ignoring the md and trying to do it anyway.",
              "score": 4,
              "created_utc": "2026-02-25 15:48:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7caxmh",
              "author": "Bob5k",
              "text": "no opensource model is even remotely close to any sort of debugging vs codex5.3xhigh or opus4.6high. This is probably the reason that for serious development you'd like to combine those with something like codex / claude 20$ subscriptions (or just amp and use amp free if you have access to it / add a few $ for really really complex issues).   \nminimax in highspeed variant improved the overall delivery speed significantly tho, as both TTFT is super low and 100tps+ makes a serious difference, as usually in my usecases it's \\~2/2.5x as fast as kimi via their official api endpoints in both cases. ",
              "score": 2,
              "created_utc": "2026-02-25 15:34:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7cem85",
                  "author": "Honest-Ad-6832",
                  "text": "It was high too... League above free models for sure. I do like and use free models a lot. But I feel much more confident with codex",
                  "score": 2,
                  "created_utc": "2026-02-25 15:52:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7hdbw5",
                  "author": "xmnstr",
                  "text": "I use the $10 Github Copilot Pro sub. Get the big models, no risk of getting banned. Enough for all the planning/debugging/research/reviewing I need.",
                  "score": 1,
                  "created_utc": "2026-02-26 08:27:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7juokw",
              "author": "deadcoder0904",
              "text": "Codex for code. Kimi or Claude for writing.",
              "score": 1,
              "created_utc": "2026-02-26 17:43:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7c8ceg",
          "author": "Bob5k",
          "text": "considering majority of accessible providers, especially when we talk about budget friendly:  \n\\- kimi via their coding plan is a big joke, but can be 'abused' for 0.99$ subscriptions. However they're on 3x usage quota right now till 'who knows when' and it's a joke as even lightweight work can cap out 5h quota in 1.5h and each 5h quota is 20% of weekly quota. You can rotate a few accounts tho easily  \n\\- glm via official api seems to be probably smartest out of these 3, best when it comes to actual frontend design and picking up logic tasks quite well. The main issue is that official api is generally slow / very slow for 95% of the time (and im EU based...) and other providers are having either quite low quota allowance or doesn't host it at all. Can't wait till synthetic provides it still (btw when they open the waitlist gates remember about [referral discount](https://synthetic.new/?referral=IDyp75aoQpW9YFt))  \n**have in mind that both kimi and glm via official api have weekly caps** which are quite.. low in both cases. GLM is better than kimi tho.  \n\\- minimax m2.5 via the minimax provider is my go-to for now, as it's reliable enough to develop 95% of usual work (i run amp free when i really need opus 4.6 for some super complex debugging) and also it has the -highspeed variant which is insanely fast as it provides constant 100tps. So for my fast paced workflow of ideate > plan > develop > review > fix > merge it's great, as minimax's speed vs kimi / glm both as a model and provider is a big win here. Also they still host [10% discount via reflinks aswell ](https://platform.minimax.io/subscribe/coding-plan?code=HO46LCwAJ5&source=link)\\- can recommend. Also **there is no weekly cap** and they say it's 100 / 300 / 1000 / 2000 prompts per 5h but the main pros of minimax system are:  \n\\- their 5h windows are fixed (00 - 05 am / 5 - 10 / 10 - 3pm / 3-8 / 8 - 11:59:59 pm) > which might sound as not like a big deal but you can basically start working at 8am, code for 2h and even if you'd get anywhere close to the cap - the limit resets at 10 so you can move forward with your work. It's IMO **way better system** than rolling 5h window as you know when to expect a reset so can potentially plan upfront.   \n\\- their system says as prompts, but what they don't say on the pricing page is that each prompt is at least 15-20 model calls - so even the 10$ (9 with discount) plan allows to do a **ton of coding**. I'm right now at 40$ highspeed plan, spinning 3 agents all the time and using their api for one of my SaaS and can't cap it out really through the day.   \nHave in mind that with 3 agents and highspeed minimax usually the human in the loop is worst piece of the whole system and workflow. :) ",
          "score": 3,
          "created_utc": "2026-02-25 15:22:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7chfjw",
          "author": "External_Ad1549",
          "text": "glm-5 supposed to be good but running slow if you are taking from coding plan however glm 4.7 does the job with good speed, minimax m2.5 starts well but with increase in context model little bit degrades",
          "score": 2,
          "created_utc": "2026-02-25 16:04:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cjpzo",
          "author": "ThingRexCom",
          "text": "GLM-5 is a clear winner for me. I use it for agentic coding, and it delivers solid results (especially when organized as a team of AI developers).\n\nI tried kimi 2.5k, but it produced a garbage stream of characters during \"thinking\" and never recovered.\n\nNote: I had Z.AI GLM Coding Max-Monthly¬†Plan, but the inference performance was very poor, and I switched to DeepInfra API (still using GLM-5).",
          "score": 2,
          "created_utc": "2026-02-25 16:15:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7exw8k",
          "author": "tsimmist",
          "text": "Not much experience from glm5, but m2.5 vs k2.5 - I would pick k2.5 all the time (although the m2.5 coding plan has higher value than what kimi offers) \n\nWith omos - kimi is a lot more better in orchestrating to my experience, m2.5 in other hands - slower but seems code better. But sometime - m2.5 has its own personality that not 100% following my instruction (still doing the job in merit but just not as I planned to be) - it could be pros or cons depends on outcome‚Ä¶",
          "score": 2,
          "created_utc": "2026-02-25 22:54:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7frh8e",
              "author": "SingingRooster95",
              "text": "Does omos refer to Oh-My-OpenCode-slim?",
              "score": 1,
              "created_utc": "2026-02-26 01:38:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7g4fsv",
                  "author": "tsimmist",
                  "text": "Yes",
                  "score": 2,
                  "created_utc": "2026-02-26 02:52:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7cft3z",
          "author": "dengar69",
          "text": "How is the token speed through Zen?  I tried Nano but it‚Äôs not reliable at all.",
          "score": 1,
          "created_utc": "2026-02-25 15:57:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7co4kq",
          "author": "JohnnyDread",
          "text": "Kimi and MiniMax would be useful if they were faster. I've tried to use GLM-5 and it does well for a while, if a bit slow, but then eventually starts going insane - rampant tool-use errors, spewing gibberish or just stopping mid-thought or task for no reason and I have to abandon the session. Promising, but not ready for prime time yet.",
          "score": 1,
          "created_utc": "2026-02-25 16:35:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7feix9",
          "author": "0xDezzy",
          "text": "If you're asking because of OpenCode Go, then GLM-5 is lobotomized . Haven't tried Kimi k2.5 or Minimax yet. ",
          "score": 1,
          "created_utc": "2026-02-26 00:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fzolo",
          "author": "aeroumbria",
          "text": "It seems to be quite sensitive to the workflow and control style you use...\n\nNot much experience with minimax yet. As for Kimi and GLM, Kimi is more reliable on \"fragile\" tasks where failed tool calls can derail the whole tasks (e.g. frameworks like GSD where each step must produce artifacts that later steps depend on). It sometimes decide to stop where it is not supposed to, but it is fairly easy to fix. GLM seems to be more intelligent and can solve more complex problems \"from scratch\" (basically using bare prompts), but it does not seem to be very reliable with tool calls, and will eventually start hallucinating tools or generating nonsensical texts if the task goes on for too long.\n\nI don't think I have enough evidence to tell which one works better generally, but for now I would prefer Kimi for orchestrated workflows and GLM for adhoc / interactive use. When trying popular prompting frameworks, GSD works better with Kimi (GLM derails in fully automated tasks), and OAC works better with GLM (Kimi impersonates user and fills user questions automatically).",
          "score": 1,
          "created_utc": "2026-02-26 02:25:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hvker",
          "author": "Ivankax28",
          "text": "Kimi K2.5",
          "score": 1,
          "created_utc": "2026-02-26 11:18:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jpnl4",
          "author": "Codemonkeyzz",
          "text": "Kimi k2.5 > glm 5 > minimax m2.5",
          "score": 1,
          "created_utc": "2026-02-26 17:20:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c4wj5",
          "author": "HarjjotSinghh",
          "text": "this is the closest to perfection yet!",
          "score": 0,
          "created_utc": "2026-02-25 15:06:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbilk6",
      "title": "Looking for the best and cheapest plan for opencode",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rbilk6/looking_for_the_best_and_cheapest_plan_for/",
      "author": "Technical_Map_5676",
      "created_utc": "2026-02-22 10:33:30",
      "score": 45,
      "num_comments": 60,
      "upvote_ratio": 0.96,
      "text": "Hey :) \n\nI don't want to vibe code. I mainly used AI to save myself the trouble of looking through the documentation or to discuss errors and ideas.    \nI want to use opencode because I don't want a vendor lock and I like the idea to use any model that ich want.   \nI would also like to use an open source model, but I can't decide for a plan. \n\nWhat's the best open source model for opencode ? Is NanoGPT with the 8 dollar plan good ? Maybe [https://z.ai/subscribe](https://z.ai/subscribe) ? \n\nOr pay only my real use with a api key from [https://openrouter.ai](https://openrouter.ai) or [https://opencode.ai/docs/zen](https://opencode.ai/docs/zen)\n\nThank you for sharing your experiences. :)\n\nLg ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rbilk6/looking_for_the_best_and_cheapest_plan_for/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o6r32fv",
          "author": "kegelo",
          "text": "GitHub Copilot : [https://github.com/features/copilot/plans](https://github.com/features/copilot/plans) \\- 10 USD for 300 requests / month for the best models\n\nNot open source models though",
          "score": 20,
          "created_utc": "2026-02-22 10:40:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rbbgb",
              "author": "c0nfluks",
              "text": "Chutes: [https://chutes.ai/pricing](https://chutes.ai/pricing) \\- $3/month for 300 requests/day. Access to 60+ Open-source models (including Kimi K2.5, Minimax 2.5, GLM5, Qwen3.5 and more). Completely private (TEE).",
              "score": 7,
              "created_utc": "2026-02-22 11:57:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rd11o",
                  "author": "bizz_koot",
                  "text": "https://www.reddit.com/r/chutesAI/s/DP61DSx4FS\n\nQuite bad experiences it seems",
                  "score": 12,
                  "created_utc": "2026-02-22 12:11:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rcici",
                  "author": "kegelo",
                  "text": "very interesting thanks",
                  "score": 1,
                  "created_utc": "2026-02-22 12:07:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6r8bz3",
              "author": "LINGLING55581",
              "text": "But 300 requests per month is not really much is it?",
              "score": 4,
              "created_utc": "2026-02-22 11:30:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6retvp",
                  "author": "BERLAUR",
                  "text": "It's 300 \"tasks\", any task is up to N requests. You could go for 20-30 minutes in a session and still be on the first \"task\". It's not very transparent but quite doable.¬†",
                  "score": 4,
                  "created_utc": "2026-02-22 12:26:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6r9bga",
                  "author": "kegelo",
                  "text": "Given the OP usage, should be fine for them",
                  "score": 2,
                  "created_utc": "2026-02-22 11:39:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6r3d7s",
              "author": "Technical_Map_5676",
              "text": "and Microsoft is fine to use it with opencode ?",
              "score": 2,
              "created_utc": "2026-02-22 10:43:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6r4lne",
                  "author": "ArifNiketas",
                  "text": "Yes, when Anthropic stopped subscription usage on third-party CLI tools, GitHub came out with official support for OpenCode.",
                  "score": 15,
                  "created_utc": "2026-02-22 10:55:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6r7xdh",
              "author": "ahmetegesel",
              "text": "I really hate the fact that claude models are low context window with GH Copilot",
              "score": 1,
              "created_utc": "2026-02-22 11:26:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6rb64u",
              "author": "touristtam",
              "text": "I have this and it goes pretty quick on what I would call moderate usage. YMMV",
              "score": 1,
              "created_utc": "2026-02-22 11:55:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6uvjnl",
              "author": "pmv143",
              "text": "Copilot is solid if you‚Äôre fine with closed models and predictable monthly usage. If you specifically want open source + flexibility, API pay per use with a good open model can be cheaper long term, especially if your usage isn‚Äôt constant. The main thing to watch is how providers bill idle time or minimum instance uptime.",
              "score": 1,
              "created_utc": "2026-02-22 23:01:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6r77kf",
              "author": "charmander_cha",
              "text": "Quero modelo de c√≥digo aberto,  que sao baratos e eu poderei me planejar a longo prazo com mais paz",
              "score": 1,
              "created_utc": "2026-02-22 11:19:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6r8fyx",
                  "author": "ChatGPTisOP",
                  "text": "+1000 open source models aren't (only) about pricing, but also about having an exit strategy, avoiding future vendor lock-in, supporting better alternatives to proprietary models, etc",
                  "score": 1,
                  "created_utc": "2026-02-22 11:31:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ujmvs",
          "author": "intwiz",
          "text": "just use NVIDIA, free API key, access to tons of OSS models (GLM, Kimi, Qwen3.5, etc.), unlimited usage with a very generous 40 requests per minute rate limit\n\nthe provider can be a little slow at times but for lightweight workflows such as yours, it should be well suited. go to build.nvidia.com",
          "score": 7,
          "created_utc": "2026-02-22 21:58:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rq7yn",
          "author": "RainScum6677",
          "text": "The best value is probably copilot. Be it the 10$ or 40$ sub.\n\nThe best overall? In my opinion that's the 200$ codex, with a new 100$ possibly taking Ng the crown soon.\n\nThat said, you can combine all sorts of subs for great value, depending on your needs and the type of work you do.",
          "score": 4,
          "created_utc": "2026-02-22 13:44:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r31kc",
          "author": "armindvd2018",
          "text": "NanoGPT has rate limiting; they advertise 2000 requests a day, but every time I pass 300-400, the models keep failing. They are not honest about their true limits. So i cancelled my subscription after first month. \n\nIf you're looking at a cheap option, maybe Chutes would be a good one. \n\nAlso, opencode always has something free, so you don't need to pay anything.",
          "score": 7,
          "created_utc": "2026-02-22 10:40:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rhafe",
              "author": "drbobb",
              "text": "I don't see anything about using Chutes with a tool like OpenCode, and on their site they don't seem to even mention integration with coding tools. Any hints?",
              "score": 1,
              "created_utc": "2026-02-22 12:45:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rit0k",
                  "author": "armindvd2018",
                  "text": "opencode auth login\n\nhttps://preview.redd.it/gk4j31aym1lg1.jpeg?width=1039&format=pjpg&auto=webp&s=3692ea4526cabdee0c53ac8dded2676bd8eff1c9\n\nIf you don't like to see all models just add it as custom provider with few models that you like",
                  "score": 1,
                  "created_utc": "2026-02-22 12:56:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6r4ci6",
          "author": "nunodonato",
          "text": "I'm using z.ai super happy with it",
          "score": 6,
          "created_utc": "2026-02-22 10:52:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rbv0b",
              "author": "Illustrious-Many-782",
              "text": "Are you on the grandfathered or the post-February account?",
              "score": 1,
              "created_utc": "2026-02-22 12:01:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rcm3i",
                  "author": "nunodonato",
                  "text": "Pre February. But I would do it again today anyway.¬†",
                  "score": 0,
                  "created_utc": "2026-02-22 12:08:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rbriu",
          "author": "beardedNoobz",
          "text": "[Z.ai](http://Z.ai) used to be the go to plan if you want a cheap one. It had very good bang for the buck value, but nowadays they seems to unable to properly serve their users. And with GLM 5 such a big models, their subscription will no longer cheap.         \n\nI still use it though, their old model 4.7, 4.6 and 4.5 still stable enough. But I plan to add new subscription or migrate from them entirely.",
          "score": 2,
          "created_utc": "2026-02-22 12:00:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r4kts",
          "author": "HarjjotSinghh",
          "text": "i'd switch to nano if my wallet's that broke - still way better than vendor debt.",
          "score": 4,
          "created_utc": "2026-02-22 10:54:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r2ucl",
          "author": "mdn0",
          "text": "nanogpt has changed its subscription last week - it is much more limited now. opencode is just not intended to be used in their new limits.",
          "score": 2,
          "created_utc": "2026-02-22 10:38:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r36r4",
              "author": "Technical_Map_5676",
              "text": "m√§h, 8$ ..to good to be true ",
              "score": 0,
              "created_utc": "2026-02-22 10:41:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rm77q",
          "author": "McKing_07",
          "text": "what do you guys think about synthetic.new?",
          "score": 2,
          "created_utc": "2026-02-22 13:19:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6sh216",
              "author": "Vict1232727",
              "text": "The problem is there‚Äôs a waitlist currently. I got lucky and got in before it, but there‚Äôs high demand and they have infra issues (normal when demand explodes like this) but yeah, they don‚Äôt even have GLM5/ mm2.5 running yet",
              "score": 1,
              "created_utc": "2026-02-22 16:02:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6w69kj",
              "author": "inventivepotter",
              "text": "I got in and like their service, they're fast serving llm requests.",
              "score": 0,
              "created_utc": "2026-02-23 03:39:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rzemd",
          "author": "AbbreviationsMany728",
          "text": "Have been using minimax m2.5 as my go-to sub. 10 bucks and the limits are generous as fuck. but am also thinking of getting ollama cloud considering the planning and reasoning of it isn't that good, it needs great prompts. I have found Minimax to be better than even free versions of GLM5, but Kimi K2.5 has been the best to me in even free versions and that is why thinking of shifting to [https://ollama.com/pricing](https://ollama.com/pricing) to try all the free models but still haven't tried it so idk.",
          "score": 2,
          "created_utc": "2026-02-22 14:37:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6vwqcu",
              "author": "bright_wal",
              "text": "Use nvidia nim developer access ? Its free.",
              "score": 0,
              "created_utc": "2026-02-23 02:39:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zkrnp",
          "author": "giuliastro",
          "text": "Why not using OpenCode Zen?\nFree usage of GLM-5, Minimax M2.5, Trinity.\nIn my experience GLM-5 is quite good for coding.",
          "score": 1,
          "created_utc": "2026-02-23 17:47:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ra0g7",
          "author": "Bob5k",
          "text": "minimax while using [10% discount](https://platform.minimax.io/subscribe/coding-plan?code=HO46LCwAJ5&source=link)is cheapest and most generous, no weekly limit and efficiently even the 100 prompts plan as long as you're not spinning 5 opencode sessions as a time is really good and tricky to cap out.",
          "score": 1,
          "created_utc": "2026-02-22 11:45:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rf8tz",
              "author": "Technical_Map_5676",
              "text": "sounds good.  is minimax a good model for opencode ?",
              "score": 0,
              "created_utc": "2026-02-22 12:29:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rhhyt",
                  "author": "Bob5k",
                  "text": "i moved all my workflow into minimax - the highspeed variant is insane. Might be a bit worse inn terms of coming up with assumptions so it does require proper prompting around to set it towards the right direction (here wispr flow or other dictation tools help, wispr also has a [free month for referrals](https://wisprflow.ai/r/MICHA%C5%81428)), but once it's set it's insanely good. And the speed matters as you'll be able to code and review while other tools will be at task 6/12 . Especially now when glm is slow and Kimi is super slow no matter the provider.",
                  "score": 2,
                  "created_utc": "2026-02-22 12:46:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6wrv54",
                  "author": "look",
                  "text": "Minimax 2.5 is very good for coding. For personal projects I use agents a lot like it sounds you do: more interactive, kind of ‚Äúpartial pair programming‚Äù while I multitask between things.\n\nI‚Äôm currently running a mix of Minimax (default for build mode), GLM5 (default for plan mode), and Kimi 2.5 (more ad hoc/mix) on Opencode. \n\nAt work, I run Claude Code with Opus 4.6 1M context in unlimited fast mode API, and I feel my home Opencode/GLM/Mini/Kimi setup is in the same ballpark (for how I use it at least), and it‚Äôs effectively free in comparison.\n\nI also just started experimenting with the Chutes $3/month plan, and it seems promising for slightly more async workflows. It‚Äôs a little slower, and sometimes needs a kick, but a great price for the times you are mostly swapping between it and other work.\n\nWhen I want a faster, more interactive flow, I use Opencode Zen pay-as-you-go API with Minimax or Kimi. At $0.15/$1.20 for Minimax, my bill never gets large. I‚Äôm tracking about $10/mo now altogether and I use them a few hours a day on average.",
                  "score": 1,
                  "created_utc": "2026-02-23 06:22:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rzwjz",
          "author": "UseMoreBandwith",
          "text": "'good' depends on how you use it.  \nWith the right instructions, the free and even some local models are good enough.  \nMany people don't know how to prompt though.\n\n",
          "score": 1,
          "created_utc": "2026-02-22 14:39:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tpwcc",
          "author": "MakesNotSense",
          "text": "OpenCode has some 'free' models. They can be slow due to demand, or not accessible. But if you don't use AI much, it's free. \n\nYou can also use OpenCode Zen. Basically, it's pay as you go, but in $20 increments. So, pay $20, then use it up over time. If your usage is low, then you can end up paying less than you do for a subscription. Especially if you use cost-effective but smart models like Kimi k2.5. \n\nIf you need more usage, GitHub Copilot seems to be getting popular. Subscription provides an amount of usage per month, and past that pay as you go.",
          "score": 1,
          "created_utc": "2026-02-22 19:28:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wr6qq",
          "author": "sudoer777_",
          "text": "Right this very moment the cheapest plan is the free models on OpenCode Zen, which have been there for about a month",
          "score": 1,
          "created_utc": "2026-02-23 06:16:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r3n95",
          "author": "No-Profession-734",
          "text": "What‚Äôs wrong with 'vibe coding' if you focus on a solid architecture first and follow up with an in-depth review?\n\n\n\nPersonally, I‚Äôd stick with paid models unless your needs are very basic. I‚Äôd be worried that free models might be more harmful than helpful when it comes to aggregating documentation.",
          "score": 0,
          "created_utc": "2026-02-22 10:46:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r41au",
              "author": "Technical_Map_5676",
              "text": "nothing is wrong with it ...It's just that I enjoy writing code myself. ",
              "score": 8,
              "created_utc": "2026-02-22 10:49:46",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6r503f",
              "author": "Specialist_Garden_98",
              "text": "Don't think he said anything was wrong with vibe coding?",
              "score": 4,
              "created_utc": "2026-02-22 10:58:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6r2x46",
          "author": "SphaeroX",
          "text": "you alway pay an extra fee for openrouter, keep this in mind",
          "score": 0,
          "created_utc": "2026-02-22 10:39:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rt8vl",
              "author": "Delicious_Ease2595",
              "text": "Extra fee?",
              "score": 0,
              "created_utc": "2026-02-22 14:02:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6sbnwe",
                  "author": "SphaeroX",
                  "text": "https://preview.redd.it/v6vnnxmwf2lg1.png?width=1141&format=png&auto=webp&s=4ccea4b983ece454e1c94601d221e455bf5198fc\n\n[https://openrouter.ai/pricing](https://openrouter.ai/pricing)",
                  "score": 1,
                  "created_utc": "2026-02-22 15:38:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6r8934",
          "author": "Snoo_57113",
          "text": "Minimax",
          "score": 0,
          "created_utc": "2026-02-22 11:29:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uuylr",
          "author": "pmv143",
          "text": "If you want flexibility and no lock-in, API-based pay-per-use is usually safer than flat monthly plans unless you‚Äôre very heavy usage. For open source coding models, people are having decent results with DeepSeek Coder, Code Llama, and some of the newer Qwen variants depending on context length needs. The main tradeoff is latency and hosting cost.",
          "score": 0,
          "created_utc": "2026-02-22 22:58:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uw7jd",
          "author": "pmv143",
          "text": "We‚Äôve been experimenting with a runtime that behaves more like Lambda for LLMs. The goal is simple. scale to zero, restore fast, and align billing to actual execution time instead of idle uptime. We‚Äôre looking for a handful of people running open-source models who want to benchmark their workload against this approach. Happy to run your model on H100s and share detailed metrics.",
          "score": 0,
          "created_utc": "2026-02-22 23:05:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vyf12",
          "author": "Disillusioned_Sleepr",
          "text": "Z.ai (glm) or minimax",
          "score": 0,
          "created_utc": "2026-02-23 02:49:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rea3gg",
      "title": "Potential limits of OpenCode Go plan",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rea3gg/potential_limits_of_opencode_go_plan/",
      "author": "alovoids",
      "created_utc": "2026-02-25 10:53:01",
      "score": 43,
      "num_comments": 22,
      "upvote_ratio": 0.97,
      "text": "Been looking at my OpenCode dashboard and here's the usage so far:\n\nTotal today: $0.44\n\nRolling (5-hour cycle): 11% (resets in \\~2 hours)\n\nWeekly: 4% (resets in 4d 13h, likely Monday)\n\nMonthly: 2% (resets in 27d 21h)\n\nIf today's usage is the only one so far, the limits seem to be:\n\nRolling (5h): $4.00\n\nWeekly: $11.00\n\nMonthly: $22.00\n\nAlso worth noting: among the three models, from cheapest to most expensive it's Minimax M2.5, Kimi K2.5, GLM 5. So choose your model wisely based on your needs and budget.\n\nThese are just indicative findings from my own dashboard. What's been your experience with the OpenCode Go plan so far? Do these numbers match what you're seeing?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rea3gg/potential_limits_of_opencode_go_plan/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7b8mys",
          "author": "Bob5k",
          "text": "i'd still pop that minimax for 9$ with [reflink](https://platform.minimax.io/subscribe/coding-plan?code=HO46LCwAJ5&source=link) has basically 1.5-2k requests per 5h window and no weekly cap. have that in mind. \n\nalso surprisingly codex on plus plan (the 20$ one) has quite high 5h caps recently.",
          "score": 9,
          "created_utc": "2026-02-25 11:59:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bnxme",
              "author": "Pleasant_Thing_2874",
              "text": "Their usage is 2xed atm I think until beginning of April.  So while we have some time with the plus plan being fairly usable don't get too used to it.",
              "score": 7,
              "created_utc": "2026-02-25 13:37:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7c3m06",
                  "author": "Bob5k",
                  "text": "yeah, but in meantime it can be used aswell. Not sure how many people would retain as customers once 2nd april hits lol ",
                  "score": 2,
                  "created_utc": "2026-02-25 14:59:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bhr54",
          "author": "trypnosis",
          "text": "Thanks for sharing but if they won‚Äôt share the limits then I will stick with synthetic.new for open source models.\n\nMight be bigger payment but at least I know what I‚Äôm getting.",
          "score": 6,
          "created_utc": "2026-02-25 13:01:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7blesx",
              "author": "alovoids",
              "text": "cheers! everyone is free to choose which provider they want :)",
              "score": 4,
              "created_utc": "2026-02-25 13:23:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bo1ur",
                  "author": "trypnosis",
                  "text": "Very true, and customer feedback is key to a successful business. \n\nSo I hope they tune in to Reddit and see that there are people who want to give OpenCode money like my self. \n\nIf it‚Äôs for AI, usage transparency and server locations are key. \n\nThis is super fresh so I‚Äôm sure lots of iterations are to come and I hope the feedback on here is considered.",
                  "score": 2,
                  "created_utc": "2026-02-25 13:38:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7bxvqf",
              "author": "Ok_Direction4392",
              "text": "Similar situation here, I was on Synthetic but switched to Ollama Cloud. Interested to see how OpenCode Go's offering develops.",
              "score": 2,
              "created_utc": "2026-02-25 14:30:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bz3hh",
                  "author": "trypnosis",
                  "text": "I think they are positioned to grab the biggest slice of the market. \n\nAnd the quality of these open source models are catching up with the big three. \n\nI think this has potential to do better for them than black.\n\nAssuming transparency and server locations are resolved. \n\nOn a side note why did you leave synthetic and what does 3 models mean on Ollama?(never seen there cloud offering before)",
                  "score": 3,
                  "created_utc": "2026-02-25 14:37:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7b2wuo",
          "author": "Glittering-Call8746",
          "text": "How's this compared to kimi coding plan ?",
          "score": 5,
          "created_utc": "2026-02-25 11:13:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7b4rqn",
              "author": "alovoids",
              "text": "the boosted usage (3x) of moderato plan seems to be lower than claude pro plan",
              "score": -2,
              "created_utc": "2026-02-25 11:29:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7b57w8",
                  "author": "Glittering-Call8746",
                  "text": "Claude pro opus 4.6 enough usage though? Lol",
                  "score": 0,
                  "created_utc": "2026-02-25 11:32:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rajn6x",
      "title": "cocoindex-code - super light weight MCP that understand and searches codebase that just works on opencode",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rajn6x/cocoindexcode_super_light_weight_mcp_that/",
      "author": "Whole-Assignment6240",
      "created_utc": "2026-02-21 06:14:04",
      "score": 40,
      "num_comments": 43,
      "upvote_ratio": 0.98,
      "text": "I built a a super light-weight, effective embedded MCP that understand and searches your codebase that just works (AST-based) ! Using¬†CocoIndex¬†- an Rust-based ultra performant data transformation engine. No blackbox. Works for opencode or any coding agent. Free, No API needed.\n\n* Instant token saving by 70%.\n* 1 min setup¬†- Just claude/codex mcp add works!\n\n[https://github.com/cocoindex-io/cocoindex-code](https://github.com/cocoindex-io/cocoindex-code)\n\nWould love your feedback! Appreciate a star ‚≠ê if it is helpful!\n\nTo get started:\n\n\\`\\`\\`  \nopencode mcp add  \n\\`\\`\\`\n\n* Enter MCP server name: `cocoindex-code`\n* Select MCP server type: `local`\n* Enter command to run: `uvx --prerelease=explicit --with cocoindex>=1.0.0a16 cocoindex-code@latest`\n\n`Or use opencode.json`:\n\n    {\n      \"$schema\": \"https://opencode.ai/config.json\",\n      \"mcp\": {\n        \"cocoindex-code\": {\n          \"type\": \"local\",\n          \"command\": [\n            \"uvx\",\n            \"--prerelease=explicit\",\n            \"--with\",\n            \"cocoindex>=1.0.0a16\",\n            \"cocoindex-code@latest\"\n          ]\n        }\n      }\n    }",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rajn6x/cocoindexcode_super_light_weight_mcp_that/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o6kfpjq",
          "author": "mrfreez44",
          "text": "How does it behave when switching branch? Or using worktrees?",
          "score": 3,
          "created_utc": "2026-02-21 07:36:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wd7sx",
              "author": "Whole-Assignment6240",
              "text": "hey thanks a lot this is really a great question!!\n\nCurrently the index is just kept up-to-date with the workspace. When you switch branch etc. it'll always follow the latest in your workspace and update incrementally. The index is automatically updated when the MCP starts, and before any query.  \n  \nIf you have multiple worktrees, and when you load the MCP in different worktrees, it's indexing each worktree independently.  \n\n\nWe are having plans in future doing advanced optimization on this too!",
              "score": 2,
              "created_utc": "2026-02-23 04:27:10",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6rmtt4",
              "author": "mrfreez44",
              "text": "No one?\nIndexing for the first time is great, but that must be updated for each feature, handling merge conflicts, rebases, branche switches...",
              "score": 1,
              "created_utc": "2026-02-22 13:23:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kzhc5",
          "author": "Miserable-Cow3117",
          "text": "How this compare to Serena or grepai? I've already tried both and I'm not really impressed by the results",
          "score": 3,
          "created_utc": "2026-02-21 10:51:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6obkq4",
              "author": "Whole-Assignment6240",
              "text": "this is AST based. i'm not familar with grepai - from high level looks like it is not?",
              "score": 1,
              "created_utc": "2026-02-21 22:19:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6k7bmu",
          "author": "HarjjotSinghh",
          "text": "oh my god this is genius - stealing my ideas already.",
          "score": 2,
          "created_utc": "2026-02-21 06:19:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k7fou",
              "author": "Whole-Assignment6240",
              "text": "give it a spin! would love your feedback! ",
              "score": 1,
              "created_utc": "2026-02-21 06:20:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kaiag",
          "author": "debackerl",
          "text": "Very nice! I just found https://github.com/postrv/narsil-mcp yesterday. They also offer code search based on embeddings (their neural_search tool).\n\nHow do you split your chunks? Is it based on an AST?",
          "score": 2,
          "created_utc": "2026-02-21 06:48:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ketiw",
              "author": "mrfreez44",
              "text": "This MCP seems over-engineered: what are the use cases?? 90 tools? Will it exhaust the model context?",
              "score": 4,
              "created_utc": "2026-02-21 07:28:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6odz1f",
                  "author": "debackerl",
                  "text": "Yes and no. Keeping BM25, IDF and embedding indexes all benefit from constantly watching files. Once you have embeddings, it's easy to find duplicate codes logic. I suppose that the call graph also benefit from file watching. However, yes, the security scans have probably little synergies.\n\nHowever, it's split in categories, and you can simply enable the categories that you want. So I don't see a problem ti have one tool with feature flags.\n\nI see more of a problem having 10 tools watching my files, and all reading those same files in parallel as I edit them.",
                  "score": 2,
                  "created_utc": "2026-02-21 22:33:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6kb8qt",
              "author": "Whole-Assignment6240",
              "text": "yes! AST based. tree-sitter :) ",
              "score": 3,
              "created_utc": "2026-02-21 06:54:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ka625",
          "author": "Professional_Past_30",
          "text": "Looks really cool! How does the embedding model work underneath?",
          "score": 1,
          "created_utc": "2026-02-21 06:44:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kb78u",
              "author": "Whole-Assignment6240",
              "text": "sentence transformer, also supports ollama and 100+ cloud providers if you have a preference!",
              "score": 2,
              "created_utc": "2026-02-21 06:54:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kjbjc",
          "author": "Delyzr",
          "text": "No php support ?",
          "score": 1,
          "created_utc": "2026-02-21 08:11:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6obx1a",
              "author": "Whole-Assignment6240",
              "text": "it does support php! [https://cocoindex.io/docs/ops/functions#supported-languages](https://cocoindex.io/docs/ops/functions#supported-languages) this is built on top of cocoindex which supports php, i'll update the docs. thanks!",
              "score": 2,
              "created_utc": "2026-02-21 22:21:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kr6dp",
          "author": "Character_Cod8971",
          "text": "How can I tell the model to always use this instead of its built-in tools. I found models to not use any MCP server except if I specifically tell them which is not what I want to do every time I make a prompt.",
          "score": 1,
          "created_utc": "2026-02-21 09:30:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kz6k7",
              "author": "Miserable-Cow3117",
              "text": "Put clear instructions in your agents.md file",
              "score": 1,
              "created_utc": "2026-02-21 10:48:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6l1ccq",
                  "author": "Character_Cod8971",
                  "text": "Do you have an example?",
                  "score": 1,
                  "created_utc": "2026-02-21 11:09:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6oc6nu",
              "author": "Whole-Assignment6240",
              "text": "yes! add skill for it. happy to help with a skill too!",
              "score": 1,
              "created_utc": "2026-02-21 22:23:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7jc9kb",
                  "author": "Character_Cod8971",
                  "text": "But they won't use their skills either. Like either I tell them to use the MCP tools I provide them or I tell them to use the skill that tells them to use the MCP tools. It would just be another layer of abstraction.",
                  "score": 1,
                  "created_utc": "2026-02-26 16:18:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6l0sq2",
          "author": "landed-gentry-",
          "text": "Saves tokens, but at what cost? There's a reason SOTA agentic harnesses aren't using these tools already.",
          "score": 1,
          "created_utc": "2026-02-21 11:03:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6m27wr",
              "author": "Mlaz72",
              "text": "ofc they want you to spend more tokens then you need to",
              "score": 1,
              "created_utc": "2026-02-21 15:23:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6myn1g",
                  "author": "landed-gentry-",
                  "text": "Any model provider that isn't competing on token efficiency is going to get left in the dust by their competitors.",
                  "score": 2,
                  "created_utc": "2026-02-21 18:06:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6pqnl9",
          "author": "Hot_Dig8208",
          "text": "I tried cocoindex months ago. Its useful when you have a monorepo project. It will make the agent search faster. \n\nThe downside of cocoondex is it use postrgres to store the data. I need to run locally since my project is huge. I can‚Äôt use free postgress instance for this.",
          "score": 1,
          "created_utc": "2026-02-22 03:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6proir",
              "author": "Whole-Assignment6240",
              "text": "hi there, thanks for the feedback!  the new one no longer use postgres! it is embedded, please check it out :)  [https://github.com/cocoindex-io/cocoindex-code](https://github.com/cocoindex-io/cocoindex-code)  \n",
              "score": 1,
              "created_utc": "2026-02-22 03:46:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ptlhb",
                  "author": "Hot_Dig8208",
                  "text": "Is it brand new app on top of cocoindex ? Glad to hear now it is embedded, but how ?",
                  "score": 1,
                  "created_utc": "2026-02-22 04:00:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6svyhw",
          "author": "ThatNickGuyyy",
          "text": "I‚Äôll test drive this at work tomorrow!\n\nI work a huge legacy PHP codebase and most of these tools don‚Äôt work the greatest. This seems promising! \n\nWill provide feedback!",
          "score": 1,
          "created_utc": "2026-02-22 17:08:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6vx4rt",
              "author": "Whole-Assignment6240",
              "text": "thank you so much!! are you already sending PR to the repo? love your work!!",
              "score": 1,
              "created_utc": "2026-02-23 02:41:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6swaho",
          "author": "chrismo80",
          "text": "currently under test, need to watch token usage if it makes a difference.  \n  \nbut my agent already thanked for the search tool, seems to be handy for him.",
          "score": 1,
          "created_utc": "2026-02-22 17:10:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6vx7bd",
              "author": "Whole-Assignment6240",
              "text": "thank you so much for the feedback!!",
              "score": 1,
              "created_utc": "2026-02-23 02:42:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6yv516",
          "author": "KevinNitroG",
          "text": "Is this similar to [VectorCode](https://github.com/Davidyz/VectorCode)?",
          "score": 1,
          "created_utc": "2026-02-23 15:47:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o738sj1",
              "author": "Whole-Assignment6240",
              "text": "great question! i don't know vectorcode well and how well it performs  \ncocoindex-code does realtime indexing so when your codebase changes it updates the index with only what's changed.",
              "score": 1,
              "created_utc": "2026-02-24 05:48:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o75cj2q",
          "author": "PunjabiMunda90",
          "text": "could this be published as a docker image? That way wouldn't have to install deps etc.",
          "score": 1,
          "created_utc": "2026-02-24 15:14:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kq7ga",
          "author": "Mlaz72",
          "text": "https://preview.redd.it/qfkh89iqetkg1.png?width=2014&format=png&auto=webp&s=24ef175eeac1b2a8e5dea01909e30e4afec18336\n\nwhen I asked model to check if this MCP works correctly, it seems it installed some stuff I did not have on my machine. So  I am not sure this is working out of box, and leaving here for you to investigate. I hope my assumption is incorrect but I decided to share this with you anyway. ",
          "score": 1,
          "created_utc": "2026-02-21 09:20:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6l3pdk",
              "author": "Mlaz72",
              "text": "seems got it finally work when I used \\`kilo add mcp\\`\n\nhttps://preview.redd.it/3qm9s5zt2ukg1.png?width=2792&format=png&auto=webp&s=8791f29dff9111c00fdb5cfd1c20fe9af6542378\n\n",
              "score": 1,
              "created_utc": "2026-02-21 11:31:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6mo4w7",
                  "author": "Docs_For_Developers",
                  "text": "Why are you using Kilo for your IDE? just curious i tried it a while ago and thought it was meh just another IDE I don't know if they changed anything?",
                  "score": 1,
                  "created_utc": "2026-02-21 17:13:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ogft2",
                  "author": "Whole-Assignment6240",
                  "text": "amazing!! thanks a lot for sharing the result :)",
                  "score": 1,
                  "created_utc": "2026-02-21 22:47:10",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6oc3sl",
              "author": "Whole-Assignment6240",
              "text": "thanks a lot!! i'll add kilo to the documentation!",
              "score": 1,
              "created_utc": "2026-02-21 22:22:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rfsbdm",
      "title": "Estimate of OpenCode Go Limits - I think its about 60M/mo, 30M/w, 12M/5hr",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rfsbdm/estimate_of_opencode_go_limits_i_think_its_about/",
      "author": "mcowger",
      "created_utc": "2026-02-27 00:58:53",
      "score": 36,
      "num_comments": 5,
      "upvote_ratio": 0.91,
      "text": "I paid the $10 just to see what the performance and limits look like.\n\nPerformance is average - no problems, but also not amazed.\n\nI recorded every single request I made for the first day in my [proxy](https://github.com/mcowger/plexus) \\- a total of 207 requests.\n\nBased on the token counts and the reported '% used' on the website:\n\n\\* Monthly: 60M tokens or 1150 requests  \n\\* Weekly: 30M tokens or 575 requests  \n\\* Rolling: 12M tokens or 225 requests\n\nThe numbers come out to within about 1% of those round numbers, so I think its pretty reasonable.  Its not clear if they count by requests or tokens.\n\nAssuming you consume all 60M tokens, with M2.5, thats about $18 worth of inference.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rfsbdm/estimate_of_opencode_go_limits_i_think_its_about/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7mhm9w",
          "author": "alovoids",
          "text": "these token estimates make sense. i also made some rough calculation in https://www.reddit.com/r/opencodeCLI/s/xY1i1xl2S6\n\nEDIT: to make it easier for anyone who sees this post, the current estimates ranged from $18-$22",
          "score": 8,
          "created_utc": "2026-02-27 01:39:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7n129l",
          "author": "SelectionCalm70",
          "text": "That's actually a generous limit for a 10 dollar plan t h",
          "score": 2,
          "created_utc": "2026-02-27 03:33:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nvfr1",
          "author": "lemon07r",
          "text": "to put into perspective. $20 droid plan is 20m x 2.5 in GLM 5 tokens. so, 50m tokens a month. this makes opencode go better IF your minimax usage is counted the same as glm usage. if it's not.. you get 20m x 8.3 in minimax tokens for $20 on droid plan. That's 167m minimax tokens. So I guess droid plan is better value if you wanted to use minimax? If OP or someone else could test if GLM 5 counts as the same amount of usage as minimax that would be nice. ",
          "score": 2,
          "created_utc": "2026-02-27 07:17:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mbfag",
          "author": "HarjjotSinghh",
          "text": "still counting on this one, genius!",
          "score": 2,
          "created_utc": "2026-02-27 01:03:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7negyq",
          "author": "MorningFew1574",
          "text": "Thanks for the analysis, it helps. I'm currently trying to decide whether to opt for Go plan or not. Just unsubscribed from GLM 5 pro quarterly plan due to price hike.",
          "score": 1,
          "created_utc": "2026-02-27 05:01:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfm8dq",
      "title": "OpenCode-Swarm v6.11 Release",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rfm8dq/opencodeswarm_v611_release/",
      "author": "Outrageous-Fan-2775",
      "created_utc": "2026-02-26 20:59:12",
      "score": 31,
      "num_comments": 22,
      "upvote_ratio": 0.94,
      "text": "I posted a few weeks ago about a very early build of my OpenCode plugin. I've iterated on it every day multiple times a day since then until we are here now with version 6.11. See below for a general guide on what it is and why it could help you. This comparison was built using Perplexity Computer over multiple iterations doing extensive market research on other plugins and capabilities.  \n  \nI've been working on opencode-swarm for a while now and figured I'd share what it actually does and why it exists.\n\nThe short version: most multi-agent coding tools throw a bunch of agents at your codebase in parallel and hope for the best. That works fine for demos. It falls apart on real projects where a bad merge or a missed security hole costs you a week of debugging.\n\nopencode-swarm does the opposite. One task at a time. Every task goes through a full QA gauntlet before the next one starts. Syntax validation (tree-sitter across 9 languages), static security analysis (63+ OWASP rules), placeholder/slop detection, secret scanning, lint, build check, then a reviewer on a *different model* than the coder, then a test engineer that writes both verification AND adversarial tests against your code. Only after all of that passes does the plan move forward.\n\nThe agents aren't generic workers either. There are 9 of them with actual permission boundaries. The Explorer can't write code. The SME can't execute anything. The Critic only reviews plans. The Architect owns the plan and delegates everything. Nobody touches what they shouldn't.\n\nSome stuff that took a lot of iteration to get right:\n\n* **Critic gate**: the plan gets reviewed by a separate agent before any code gets written. Prevents the most expensive failure mode, which is perfectly executing a bad plan\n* **Heterogeneous models**: coder and reviewer run on different LLMs on purpose. Different models have different blind spots, and this catches stuff single-model setups miss\n* **Retrospectives**: at the end of each phase, execution metrics (revisions, rejections, test failures) and lessons learned get captured and injected into the architect's prompt for the next phase. The swarm actually learns from its own mistakes within a project\n* **Everything persists**: plan.json, [context.md](http://context.md), evidence bundles, phase history. Kill your terminal, come back tomorrow, pick up exactly where you left off\n* **4,008 tests** on the plugin itself. Not the projects it builds. On the framework\n\nThe tradeoff is real. It's slower than parallel approaches. If you want 5 agents banging out code simultaneously, this isn't that. But if you've ever had an AI tool generate something that looked right, passed a vibe check, and then blew up in production... that's the problem this solves.\n\n**How it compares to other stuff out there**\n\nThere's a lot of multi-agent tooling floating around right now so here's how I see the landscape:\n\n**Swarm Tools (opencode-swarm-plugin)** is the closest competitor and honestly a solid project. Their focus is speed through parallelism: break a task into subtasks, spawn workers, file reservations to avoid conflicts. They also have a learning system that tracks what strategies worked. Where we differ is philosophy. Their workers are generic and share the same model. Mine are specialized with different models on purpose. They have optional bug scanning after the fact. I have 15+ QA gates that run on every single task before it moves on. If you want fast, go Swarm Tools. If you want verified, this is the one.\n\n**Get Shit Done (GSD)** is more of a meta-prompting and spec-driven framework than a true multi-agent system. It's great at what it does: interviews you, builds a detailed spec, then executes phase by phase. It recently added parallel wave execution and subagent orchestration. But it doesn't have a persistent QA pipeline, no security scanning, no heterogeneous models, and no evidence system. GSD is a planning tool that got good at execution. opencode-swarm is a verification system that happens to plan and execute.\n\n**Oh My OpenCode** gets a lot of attention because of the RPG theming and the YouTube coverage. Six agents with fun names, easy to set up, approachable. But when you look under the hood it's basically prompt engineering. No persistent state between sessions. No QA pipeline. No security analysis. No test suite on the plugin itself. It's a good entry point if you've never tried multi-agent coding, but it's not something I'd trust on a production codebase.\n\n**Claude Code Agent Teams** is native to Claude Code, which is a big advantage since there's no plugin to install. Peer-to-peer messaging between agents is cool architecturally. But it's still experimental with known limitations: no session resumption, no built-in QA, no evidence trail. Running multiple Opus-class agents in parallel also gets expensive fast with zero guarantees on output quality.\n\n**Codex multi-agent** gives you a nice macOS GUI and git worktree isolation so agents don't step on each other. But the workflow is basically \"agents do stuff in parallel branches, you manually review and merge.\" That's just branch management with extra steps. No automated QA, no verification, no persistence beyond conversation threads.\n\nThe common thread across all of these: none of them answer the question \"how do you know the AI's output is actually correct?\" They coordinate agents. They don't verify their work. That's the gap opencode-swarm fills.\n\nMIT licensed: [https://github.com/zaxbysauce/opencode-swarm](https://github.com/zaxbysauce/opencode-swarm)\n\nHappy to answer questions about the architecture or any of the design decisions.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rfm8dq/opencodeswarm_v611_release/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7lagwc",
          "author": "bitcoinbookmarks",
          "text": "Looks interesting and promising, but this made for commercial models. Can you add some easy swith or instruction how to fine tune it for one (max two) local models? At least I see time limits, maybe some agents redundant for local use with one model only... It will be cool if it will be possible to easy use with local model.",
          "score": 4,
          "created_utc": "2026-02-26 21:47:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7le6j2",
              "author": "Outrageous-Fan-2775",
              "text": "Its actually already built for and fully capable of local only. I can post the config when I get home. One of my requirements was that it run just as well fully locally.",
              "score": 3,
              "created_utc": "2026-02-26 22:05:01",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7lx48x",
              "author": "Outrageous-Fan-2775",
              "text": "I posted this above, but it aligns perfectly with your question.\n\nFor an example, I have one swarm called \"laptop\" that I run on my Ryzen AI Max 395+ 128gb system when I'm traveling. It only uses two models. GPT OSS 20b and some variant of Qwen coder. GPT serves as the architect, reviewer, etc while Qwen serves as the critic, coder, and test engineer. When at home I run 3 swarms, Mega, Paid, and Local. Mega has all the expensive top of the line models, Paid is one step down, and Local is all running here locally. I usually kick a project off with Mega and then let one of the other swarms take over. Or if its a smaller project the other two swarms can handle it themselves.\n\nTLDR I would recommend a minimum of two models NOT by the same company (i.e. dont do Qwen3.5 and also Qwen3 Coder. The plugin prompts are about as good as they can be, but no prompt will solve a blind spot caused by training data. Only a different data set can solve that.",
              "score": 1,
              "created_utc": "2026-02-26 23:44:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7o6gma",
                  "author": "bitcoinbookmarks",
                  "text": "I'm looking forward to try it, but a bug doesn't allow me to install your tool. I'm already using something like this with orchestrator agent and sequence subagents calls, but simplified. Works best.\n\nYou mentioned below that you feed cloud models with description how to make plan for your tool. Can you please also share this? Maybe include in repo and update periodically when tool change behavior?",
                  "score": 1,
                  "created_utc": "2026-02-27 08:58:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ldgiy",
          "author": "VVocach",
          "text": "Cool, ill test it tomorrow morning",
          "score": 2,
          "created_utc": "2026-02-26 22:01:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lifuc",
          "author": "Fit-Palpitation-7427",
          "text": "Full vibe coder building internal apps for the company, seems like I‚Äôm the perfect ginny pig with 15+ apps ATM",
          "score": 2,
          "created_utc": "2026-02-26 22:26:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lmtol",
              "author": "Outrageous-Fan-2775",
              "text": "Yeah you definitely want quality over speed for your situation. And this plugin values quality over everything else.",
              "score": 1,
              "created_utc": "2026-02-26 22:48:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lio1x",
          "author": "Soft_Syllabub_3772",
          "text": "Ill check it out!",
          "score": 2,
          "created_utc": "2026-02-26 22:27:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lusgr",
          "author": "BestUsernameLeft",
          "text": "Looks very promising. I just got opencode + oh-my-opencode running in a container and hooked up to Zen AI. But I spent more $$ than I want to yesterday evening. So, two questions. \n\n1. What's the effort to get this running in a container? \n2. Can I set up fallback models or otherwise configure to adjust between expensive models and free/local models?",
          "score": 2,
          "created_utc": "2026-02-26 23:31:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lwoyh",
              "author": "Outrageous-Fan-2775",
              "text": "1. Little to none. Same config process as oh my opencode. Add plugin to opencode.json and then create the opencode-swarm.json file for the config.  \n2. Yep. You can set up a single swarm, multiple swarms, a swarm that is all one model, and there is a defaults file that it will fall back to if necessary, although by default it just falls back to whatever model you set as the orchestrator. The most important part you want to be absolutely sure of is that the antagonistic agents are using different models. So Coder and Reviewer, Architect and Critic. You can prompt engineer all you want but if its the same model it will make the same mistakes. Two models trained on two different data sets will find problems that homogeneous setups miss.\n\nFor an example, I have one swarm called \"laptop\" that I run on my Ryzen AI Max 395+ 128gb system when I'm traveling. It only uses two models. GPT OSS 20b and some variant of Qwen coder. GPT serves as the architect, reviewer, etc while Qwen serves as the critic, coder, and test engineer. When at home I run 3 swarms, Mega, Paid, and Local. Mega has all the expensive top of the line models, Paid is one step down, and Local is all running here locally. I usually kick a project off with Mega and then let one of the other swarms take over. Or if its a smaller project the other two swarms can handle it themselves.\n\nThe last piece that is critical is generating an implementation plan BEFORE you get into OpenCode. I use Perplexity, Gemini, ChatGPT, Claude, QwenChat, etc via web chat to bounce ideas off each other until I can generate a single agreed upon implementation plan that is written specifically for the swarm workflow. I then pop it in the directory and tell the architect to implement. This saves a huge amount of API calls just nailing down the plan itself without doing any actual work.",
              "score": 1,
              "created_utc": "2026-02-26 23:41:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ly9ox",
                  "author": "BestUsernameLeft",
                  "text": "That's great advice (I'm doing similar on a more basic and inexpensive level) and great answers, I'll be kicking the tires soon!",
                  "score": 2,
                  "created_utc": "2026-02-26 23:50:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7mun8s",
          "author": "Weird-Negotiation-27",
          "text": "Very good, I like this kind of project, it‚Äôs an improvement to our work. But honestly, I find its documentation extremely verbose without getting anywhere.\n\nI‚Äôm not a vibe coder, I‚Äôm a software engineer, and I had to read it three times and still only figured out how to use it by, well‚Ä¶ using it.\n\nThe project seems very good at first glance, but being good is not enough if people simply don‚Äôt know how to use it, what it‚Äôs for, how it actually works‚Ä¶ But even worse, what it is. Again, I, a technical professional, had to read it three times and still didn‚Äôt understand.\n\nA vibe coder or someone seriously entering the field will try to read it three times and won‚Äôt have the knowledge to explore it on their own, they‚Äôll just give up.\n\nAt the moment, my concerns are more about communication than technical aspects. I need to test it much more, I‚Äôll integrate it into the workflow of smaller projects at my company and see how it performs.\n\nI liked the suggestion of using models from different companies for tasks like QA, that perspective is usually ignored in this kind of workflow and you were spot on there, congratulations. There‚Äôs no point in asking the GLM to verify whether the code it wrote is good, it‚Äôs the same as asking me if the code I wrote is good, my answer will be ‚Äúobviously, I wrote it.‚Äù\n\nNow a question: I know they‚Äôre different proposals but the end goal is the same, how do you position yourself in relation to the GitHub Spec Kit? Yours feels much more ‚Äúvibe coder vibe‚Äù (sorry for the pun), Spec Kit involves a lot of manual action and direct user inference, yours seems more automated, fine, different proposals. But have you compared the final results both methods produce? It seems like something interesting to analyze.\n\nIn any case, I hope to see this project evolve further over time. Thank you for your dedication to this development.",
          "score": 2,
          "created_utc": "2026-02-27 02:54:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7mwnv1",
              "author": "Outrageous-Fan-2775",
              "text": "Valid points. I did use AI to write the readmes. I'm a back end engineer by trade, but being lazy and just letting AI write things for me is sometimes just the order of the day.\n\nI can definitely describe the project, reasons I went certain directions, or answer any questions you have if you want to post them here or just DM me.\n\nAs for Spec Kit, its basically a semi automated method to do what I already do when I build out specs. I bounce between Perplexity, ChatGPT, Claude, Gemini, QwenChat, Deepseek, all via their web chat and build a single implementation plan they all agree on. I give all of them a gitingest of my swarm plugin so they understand the workflow and can build the implementation plan specifically for it. AFAIK spec kit stops when it comes to creating actual code, which is where this plugin picks it up. It is about creating the highest quality code possible, but it can still be impacted by garbage in garbage out. If you give it a terrible plan that makes no sense it will try its best with the Critic to turn it into gold, but it may just be straw.",
              "score": 1,
              "created_utc": "2026-02-27 03:06:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7n4vel",
          "author": "RainScum6677",
          "text": "Looking good. I'm working with huge code bases with some very convoluted and sometimes outdated flows(.NET 4.6-4.8, c#7), and need to deal with problematic parts of these code bases on a daily basis.\n\nQuestion: can you estimate how token efficient this system is? It looks like it might be costly to run.\n\nAlso, any way of introducing existing memory/context retention systems into the flow alongside/instead of the specified approach?\n\nVery interesting to try in workflow. Great work!",
          "score": 2,
          "created_utc": "2026-02-27 03:57:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7n6h3x",
              "author": "Outrageous-Fan-2775",
              "text": "I actually went back and forth with Perplexity about this a few days ago. The below was the result.  \n  \nShort version: it uses about 3-5x more tokens per task than base OpenCode or Claude Code. Every task goes through architect, coder, reviewer, and test engineer instead of one agent doing everything, so yeah, more tokens.\n\nBut that doesn't tell the full story.\n\nThe QA gates (syntax checking, SAST, secret scanning, build verification, placeholder detection) all run locally. No LLM calls. That stuff is free. Meanwhile Claude Code users are regularly posting about burning 10% of their weekly quota on a single plan-mode message because context just spirals.\n\nSerial execution helps too. Only one agent is loaded at a time. Claude Code's Agent Teams run at 7x overhead according to Anthropic's own docs because every teammate keeps its own full context window open.\n\nThe retrospective system also pays for itself over time. The swarm learns from past mistakes so you get fewer rework cycles, which is where most people actually waste tokens.\n\nWhere it genuinely costs more: simple stuff. A one-line typo fix still runs through the full pipeline. That's overkill and I know it.\n\nQuick comparison:\n\n* Base OpenCode/Claude Code: 1x (no review, no testing, no security scanning)\n* GSD: roughly 1x (single agent, good context isolation, but no verification)\n* Oh-My-OpenCode: 2-3x (subagents with lean context, less enforcement)\n* Claude Code Agent Teams: 7x (per Anthropic's docs)\n* opencode-swarm: 3-5x (code comes out reviewed, tested, and security scanned)\n\nThe way I think about it: what matters is cost per correct line of code, not cost per task. If you're spending tokens on rework because nothing got reviewed, you're paying anyway. The swarm just moves that cost upfront into verification instead of after the fact into debugging.",
              "score": 2,
              "created_utc": "2026-02-27 04:07:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7n84d3",
                  "author": "RainScum6677",
                  "text": "I appreciate this approach. \nUp until now, for most tasks complex tasks I've had to run,\nNone of the existing systems did better than using basic plan mode with a capable model(the longest part of the flow), revising cycles, then execution with close guidance and mostly manually reviewing(with some agent assistance thrown in).\nBut this is slow. It takes time, it's a bottleneck. And obviously it has some built in weaknesses that are difficult to handle.\n\nWill try your system. Thank you.",
                  "score": 2,
                  "created_utc": "2026-02-27 04:18:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7nu9in",
          "author": "disruptz",
          "text": "from the diag logger.\n\n---\n\nYou‚Äôre not looking at a remove failure ‚Äî this log is from:\n\nnpm install -g opencode-swarm\n\n‚Ä¶and it failed during postinstall:\n\nbun run copy-grammars\n\nModule not found \"scripts/copy-grammars.ts\"\n\nSo the package install is half-written, and npm also can‚Äôt clean it up because of EPERM (Windows file lock / permissions) in the global node_modules path.\n\nWhat‚Äôs going wrong (from your log)\n\nopencode-swarm@6.11.0 postinstall runs bun run copy-grammars (line 83‚Äì92).\n\nThat script points at scripts/copy-grammars.ts but it can‚Äôt be found (line 92).\n\nnpm then tries cleanup and hits EPERM: operation not permitted, rmdir ...\\zod\\...\n\n\n---",
          "score": 1,
          "created_utc": "2026-02-27 07:06:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7o0avt",
          "author": "ProvidenceXz",
          "text": "I don't know why but my AI slop alarm didn't go off on this post. I found a reason to try opencode.",
          "score": 1,
          "created_utc": "2026-02-27 08:00:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7l35z8",
          "author": "HarjjotSinghh",
          "text": "you nailed my dev soulmate now.",
          "score": 1,
          "created_utc": "2026-02-26 21:12:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l4ko4",
              "author": "Outrageous-Fan-2775",
              "text": "lol. It can still be annoying with AI slop, but the guardrails are so tight in this latest version that the occurrences are much fewer and further between. Plus the checks are balances are so rigorous its very hard for any of that to ever make it through to the final product.",
              "score": 2,
              "created_utc": "2026-02-26 21:18:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7mnbgi",
                  "author": "Ang_Drew",
                  "text": "hey FYI, he is bot.. dont take it seriously",
                  "score": 1,
                  "created_utc": "2026-02-27 02:12:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rewp99",
      "title": "I got tired of rate limits, so I wired 80+ free models together",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rewp99/i_got_tired_of_rate_limits_so_i_wired_80_free/",
      "author": "rroj671",
      "created_utc": "2026-02-26 01:33:19",
      "score": 29,
      "num_comments": 22,
      "upvote_ratio": 0.83,
      "text": "Built a small routing layer that sits in front of OpenCode and automatically switches between 80+ free model endpoints.\n\nIt monitors latency and failures in real time and fails over when a provider gets slow or rate limited. It auto-selects the fastest healthy model at request time.\n\nhttps://preview.redd.it/p8ht0jf19qlg1.png?width=2858&format=png&auto=webp&s=ca5121d68b2b9eccc02c68a5dcc4c3b638c042fa\n\nhttps://preview.redd.it/dx8onhxksqlg1.png?width=2516&format=png&auto=webp&s=54e45da07ecd3c919094a0f670f64052e9de35ac\n\n  \n`npm install -g modelrelay`\n\n`modelrelay onboard`\n\nSource: [https://github.com/ellipticmarketing/modelrelay](https://github.com/ellipticmarketing/modelrelay)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rewp99/i_got_tired_of_rate_limits_so_i_wired_80_free/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7gn1de",
          "author": "Ang_Drew",
          "text": "how about the result consistency?",
          "score": 6,
          "created_utc": "2026-02-26 04:48:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7htlar",
              "author": "rroj671",
              "text": "You can pin a model and it will serve the same one for all requests until you unpin it or it goes down.",
              "score": 2,
              "created_utc": "2026-02-26 11:01:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7iyyxs",
                  "author": "Ang_Drew",
                  "text": "that's actually cool..",
                  "score": 1,
                  "created_utc": "2026-02-26 15:17:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7fsp7a",
          "author": "ELPascalito",
          "text": "So you're just hammering the free Nvidia API? Got it",
          "score": 10,
          "created_utc": "2026-02-26 01:45:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hu1te",
              "author": "rroj671",
              "text": "Hammering?",
              "score": 1,
              "created_utc": "2026-02-26 11:05:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7h6y6m",
          "author": "DJGreenHill",
          "text": "This is why we can‚Äôt have nice things",
          "score": 6,
          "created_utc": "2026-02-26 07:27:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hbqq9",
          "author": "Deep_Traffic_7873",
          "text": "great, this project will pop the AI bubble",
          "score": 4,
          "created_utc": "2026-02-26 08:12:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nnj3v",
          "author": "Glittering-Call8746",
          "text": "Anyone outside of USA able to get otp from build.nvidia.com ?",
          "score": 2,
          "created_utc": "2026-02-27 06:10:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gur26",
          "author": "crmfan",
          "text": "Doesnt it lose all the context when switching randomly",
          "score": 2,
          "created_utc": "2026-02-26 05:45:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7h2zj5",
              "author": "Delyzr",
              "text": "Your client sents the context each time. If you ar  e always using the same model, it might have a copy of your context from last call and might be able to reuse part of it. This is called a cache hit. If it doesn't, its input tokens.\n\nIf you look at something like openrouter you might be switching provider every request, so there goes your serverside cache.\n\nEven if always using the same provider/model directly, you might switch physical machines in their cloud from one moment to the next, and the new machine doesn't have your context cached.",
              "score": 5,
              "created_utc": "2026-02-26 06:53:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7h3le5",
              "author": "mdn0",
              "text": "No. They are stateless.",
              "score": 1,
              "created_utc": "2026-02-26 06:58:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7h2bov",
          "author": "MrMrsPotts",
          "text": "I need something like this but I am worried that switching like this will damage the quality of the code you get.",
          "score": 1,
          "created_utc": "2026-02-26 06:47:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7h34qy",
              "author": "Enesce",
              "text": "Oh it will absolutely butcher it. Not all models are equal. But that's besides the point üòÖ",
              "score": 3,
              "created_utc": "2026-02-26 06:54:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7h8s3m",
          "author": "Timo_schroe",
          "text": "So like the openrouter free endpoint ?",
          "score": 1,
          "created_utc": "2026-02-26 07:44:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hgmoi",
              "author": "rroj671",
              "text": "Yes, openrouter is one of the providers. You can just add more.",
              "score": 1,
              "created_utc": "2026-02-26 08:59:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7hrmio",
          "author": "FormalAd7367",
          "text": "If we want cheap/free, we could just switch between those Nvidia models and Deepseek.  My company uses Deepseek API to keep cost down (we were on Gemini API and Open Ai).",
          "score": 1,
          "created_utc": "2026-02-26 10:44:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jghyf",
          "author": "oxygen_addiction",
          "text": "People like you are why we can't have free shit.",
          "score": 1,
          "created_utc": "2026-02-26 16:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jgwx6",
              "author": "rroj671",
              "text": "How is this package hurting others? Your usage would be the same. The only thing automated is switching models that go offline.",
              "score": 1,
              "created_utc": "2026-02-26 16:39:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7jiqw7",
                  "author": "oxygen_addiction",
                  "text": "You are hammering all models and enabling non-techies to do the same. Fine for personal usage, but the more people use packages like this, the more free services degrade.\n\nSometimes it's better to keep things to yourself.",
                  "score": 1,
                  "created_utc": "2026-02-26 16:47:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7mtg51",
          "author": "bradjones6942069",
          "text": "Thank you for this",
          "score": 1,
          "created_utc": "2026-02-27 02:47:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7frjai",
          "author": "HarjjotSinghh",
          "text": "this is the future of ai magic, baby!",
          "score": -4,
          "created_utc": "2026-02-26 01:38:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfwlzk",
      "title": "I have 2,004 AI skills installed. Here's how I reduced my startup context from ~80K tokens to ~255 tokens (99.7% reduction)",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rfwlzk/i_have_2004_ai_skills_installed_heres_how_i/",
      "author": "blacksiders",
      "created_utc": "2026-02-27 04:15:48",
      "score": 28,
      "num_comments": 18,
      "upvote_ratio": 0.79,
      "text": "I've been collecting skill packs for OpenCode/Claude Code and hit 2,004 skills across 34 categories (ai-ml, security, devops, game-dev, etc.).\n\nThe problem: AI agents use a¬†[3-level progressive disclosure system](https://opencode.ai/docs/skills)¬†to load skills. Level 1 loads the¬†`name`¬†\\+¬†`description`¬†of¬†**every**¬†skill into the system prompt at startup. With 2,004 skills, that's¬†**\\~80,000 tokens consumed before I even type a prompt**¬†\\- roughly 40% of a 200K context window.\n\n# The fix: SkillPointer\n\nIt's not a plugin or library. It's an¬†**organizational pattern**¬†that works with native skills:\n\n1. Move all 2,004 raw skills to a hidden vault directory (outside the agent's scan path)\n2. Replace them with 35 lightweight \"category pointer\" skills\n3. Each pointer tells the AI: \"use¬†`list_dir`¬†and¬†`view_file`¬†to browse the vault and find the exact skill you need\"\n\nResult:\n\n||Before|After|\n|:-|:-|:-|\n|Startup tokens|\\~80,000|\\~255|\n|Skills accessible|2,004|2,004|\n|Reduction|\\-|99.7%|\n\nThe AI still accesses every skill - it just discovers them on-demand using file tools it already has, instead of loading all descriptions at startup.\n\n# How I verified this\n\n* Measured actual YAML frontmatter sizes from all 2,004 [SKILL.md](http://SKILL.md) files\n* Confirmed the¬†`<available_skills>`¬†loading behavior in¬†[OpenCode docs](https://opencode.ai/docs/skills)¬†and¬†[Claude Code docs](https://docs.anthropic.com/en/docs/claude-code/skills)\n* Real data from my own environment, not theoretical numbers\n\n# Repo\n\n[github.com/blacksiders/SkillPointer](https://github.com/blacksiders/SkillPointer)\n\nIncludes a zero-dependency Python setup script that auto-categorizes your skills and generates the pointers.\n\nHappy to answer questions about the approach. I know \"it's just skills organizing skills\" - that's literally the point. The value is in the pattern, not the tech. savings in scale.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rfwlzk/i_have_2004_ai_skills_installed_heres_how_i/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o7nx2hq",
          "author": "Wrenky",
          "text": "Okay so I absolutely love this from a technical standpoint, I've seen several mini patterns like this and written a few- but mostly for chaining not condensing context!\n\nHowever, this feels pretty perfectly situated for rag or even just a database lookup right?  Then again, I don't think you can actually best text lookup at this scale, if you are willing to be militant on organization and policing descriptions\n\nWell done lmao",
          "score": 3,
          "created_utc": "2026-02-27 07:31:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o3qxy",
              "author": "blacksiders",
              "text": "Haha, thank you! You absolutely nailed the dilemma, but you actually read my mind, I'm not skipping vector search entirely, I‚Äôm just layering it.\n\nI use FAISS+Langchain and a custom Qdrant MCP at different layers under the hood (since Cursor/VS Code have built-in indexers by default anyway).\n\nThe problem with pure RAG/Vector DB lookups at the¬†*top*¬†level is semantic failure. If I ask a pure RAG setup to 'Build a stretchy IK arm', it pulls the IK node and all connected if you know those DBS can have up to 700+ vrctors, but misses the 'Naming Conventions' file because the mathematical overlap just isn't there.\n\nSo I use SkillPointer as a strict text router to force a custom dependency map first. This mixed, layer-by-layer approach saves massive tokens. Once inside the routed skill, the AI can trigger the Qdrant MCP for semantic search if it actually needs to deep-dive into the docs.\n\nAs you probably know, MCP is a context vampire and gives you 'auto-compact', where you loosing details and moving you closer to hallucinations. If you have hundreds of tools/skills active, MCP will literally eat 80% of your context window just defining the JSON schemas.\n\nBy putting SkillPointer's strict text routing in front, I shield the AI from that MCP bloat. The AI only loads the schema for the¬†*one*¬†tool it actually needs at that exact moment, rather than choking on the entire protocol dictionary.\n\nIn the end, it's all about ruthlessly optimizing context limits to prevent hallucinations in complex tasks. ",
              "score": 1,
              "created_utc": "2026-02-27 08:32:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7o0syb",
          "author": "Outrageous_Corgi7553",
          "text": "Ever get misroutes where a skill could fit like 2-3 categories? How does the agent decide which pointer to hit?\"",
          "score": 3,
          "created_utc": "2026-02-27 08:05:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o52dr",
              "author": "blacksiders",
              "text": "Great question! Right now, the Heuristic Engine just assigns it to the first matching category in my priority dictionary. Code of idea is open and you can see examples and improve them for your specific needs.\n\nBut the real safeguard is the AI itself. Because the Pointers are incredibly token-cheap (just a few sentences), the agent will naturally trigger multiple pointers simultaneously if a task is ambiguous. But it also can skip most of them, depends on approach and rules llm follows.\n\nIf I ask for a 'secure React native login', the AI will hit the Security, Mobile, and Web-Dev pointers all at once, scan those three specific hidden vaults, and find the skill wherever the script dropped it. The gateways are so cheap it can afford to double-check, main thing if you have -60 token avarage description for all 2000 skills in context, it's just massive.  \n  \nTo be clear, I‚Äôm not trying to sell this as a universal ‚Äòbest‚Äô pattern or some magic skill standard. I‚Äôm just sharing one concrete way I‚Äôve been optimizing context usage in a messy, real pipeline so the agent doesn‚Äôt drown in skills metadata and start hallucinating. It‚Äôs a tradeoff tool for my use case, not a new religion.",
              "score": 1,
              "created_utc": "2026-02-27 08:45:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7nzb86",
          "author": "Rizarma",
          "text": "i need to invest my time with this tool",
          "score": 2,
          "created_utc": "2026-02-27 07:51:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7o0eq9",
          "author": "Revolutionary_Sir140",
          "text": "Wow",
          "score": 2,
          "created_utc": "2026-02-27 08:01:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7o2rav",
          "author": "rapkannibale",
          "text": "Thanks for sharing. Does this make sense if you only have 6 skills? At what volume of skills would you say this is really worth it?",
          "score": 2,
          "created_utc": "2026-02-27 08:23:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o5v9g",
              "author": "blacksiders",
              "text": "With 6 skills it‚Äôs not worth the complexity. This pattern is for when you have hundreds+ hyper‚Äëspecific skills (e.g., full Maya/Unity pipelines, maybe even skipping MCP and driving DCCs via Python/commandPort), and the skills metadata itself starts to bloat context and cause hallucinations. Each skill eats \\~60 tokens on average in a fresh session, so once you hit hundreds or thousands you can quickly do the math and see why I'm dig into it.",
              "score": 1,
              "created_utc": "2026-02-27 08:52:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7o4qt7",
          "author": "PreparationAny8816",
          "text": "Cloudflare released code mode to target this issue: [Code Mode](https://blog.cloudflare.com/code-mode-mcp/)",
          "score": 2,
          "created_utc": "2026-02-27 08:42:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o65zm",
              "author": "blacksiders",
              "text": "Nice! Code Mode is super interesting, thanks for linking it. It‚Äôs tackling the same core problem (too much MCP/API surface in context).",
              "score": 1,
              "created_utc": "2026-02-27 08:55:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7nfzuq",
          "author": "Michaeli_Starky",
          "text": "Why would you install so many useless stuff? These bot posts are tiring.",
          "score": 4,
          "created_utc": "2026-02-27 05:12:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7niaa3",
              "author": "blacksiders",
              "text": "Not a bot, just a Technical Director working in VFX/Game Dev pipelines.\n\nIn a real enterprise pipeline, 'skills' aren't just generic 'how to code Python' prompts. They are hyper-specific, multi-disciplinary rules. If I need an AI agent to build a single character rig in Maya via MCP, it requires a dozen different dependency documents (Naming Conventions, Stretchy IK math, Twist Extraction rules, Joint Orientations, Unity Export settings).\n\nWhen you map an entire studio's pipeline into agent skills, you hit hundreds or thousands of files instantly. This tool prevents the AI from choking on that metadata. If you only use AI for simple single-file web apps, you definitely don't need this yet. But at scale, flat structures break.",
              "score": 8,
              "created_utc": "2026-02-27 05:29:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7n9u5d",
          "author": "xak47d",
          "text": "The models have been ignoring most skills anyway. They are mostly useless",
          "score": 3,
          "created_utc": "2026-02-27 04:30:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7nhuoj",
              "author": "blacksiders",
              "text": "The reason models ignore skills is¬†***exactly***¬†because of the architecture flaw I'm fixing here.\n\nBy hiding the raw skills behind a dynamic routing layer (SkillPointer), the AI only loads the exact dependency bundle it needs (e.g., Unity Naming + IK Rigging + Twist setup) in absolute isolation. When the context is clean, the model obeys the skill 100% of the time. The skills aren't useless, the default retrieval mechanism is.",
              "score": 1,
              "created_utc": "2026-02-27 05:26:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7nkhjh",
                  "author": "Codemonkeyzz",
                  "text": "Lol. You are a bot .  It seems you replied to wrong threads. Your reply to the other thread posted here, and your reply for this thread posted to the other thread.",
                  "score": -9,
                  "created_utc": "2026-02-27 05:46:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7n8jr4",
          "author": "HarjjotSinghh",
          "text": "this is unreasonably cool actually - my mind needs this.",
          "score": 1,
          "created_utc": "2026-02-27 04:21:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7nwfod",
              "author": "t4a8945",
              "text": "You made me think of Neo in the Matrix \"I know kung-fu\". Bro just loaded the skill in its context.¬†",
              "score": 2,
              "created_utc": "2026-02-27 07:26:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ra9ebs",
      "title": "OpenCode iPhone App",
      "subreddit": "opencodeCLI",
      "url": "https://v.redd.it/nnlydahc5qkg1",
      "author": "KnifeDev",
      "created_utc": "2026-02-20 22:18:30",
      "score": 27,
      "num_comments": 13,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1ra9ebs/opencode_iphone_app/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6i4vzg",
          "author": "KnifeDev",
          "text": "Download via test flight https://testflight.apple.com/join/cvQaEf6s \n\nGitHub: https://github.com/DNGriffin/whispercode",
          "score": 4,
          "created_utc": "2026-02-20 22:19:16",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o6ibw9x",
          "author": "KnifeDev",
          "text": "Important note: this is an OpenCode client app, you still must run the server from your dev machine: ‚Äòopencode serve --hostname 0.0.0.0 --cors app-local://localhost‚Äô and then add your server‚Äôs ip in the phone app.",
          "score": 3,
          "created_utc": "2026-02-20 22:56:28",
          "is_submitter": true,
          "replies": [
            {
              "id": "o6jxf9z",
              "author": "c0nfluks",
              "text": "Genuinely curious to know what‚Äôs the advantage of this compared to simply ssh into your dev machine?",
              "score": 2,
              "created_utc": "2026-02-21 04:58:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6jylq0",
                  "author": "KnifeDev",
                  "text": "Main benefit is WhisperKit speech-to-text. iPhone keyboard‚Äôs built in speech to text is terrible. WhisperKit is the best (it‚Äôs why ChatGPT mic input is so good). \n\nThe OpenCode desktop app is pretty great compared to TUI even on desktop, and I think the phone version of it is far superior to SSH‚Äôd TUI on phones. \n\nAlso a minor thing: the app has some keyboard customizations that are nice. \n\nI‚Äôm working on wiring up push notifications \nand considering Dynamic Island but trying to not make it over the top.",
                  "score": 5,
                  "created_utc": "2026-02-21 05:07:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6i5l4o",
          "author": "HarjjotSinghh",
          "text": "how'd you turn cli into phone magic?",
          "score": 2,
          "created_utc": "2026-02-20 22:22:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i5us3",
              "author": "KnifeDev",
              "text": "OpenCode Desktop uses Tauri which supports IOS. So porting was easy. \n\nAn official app is on their roadmap, hoping this community port can help them get there.",
              "score": 1,
              "created_utc": "2026-02-20 22:24:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6l1zya",
          "author": "RIP26770",
          "text": "This is really cool! Might port this to Android!",
          "score": 2,
          "created_utc": "2026-02-21 11:15:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lfib5",
          "author": "oVerde",
          "text": "Okay, now make it find OpenCode in local network",
          "score": 2,
          "created_utc": "2026-02-21 13:07:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xm0rv",
          "author": "TheDataQuokka",
          "text": "Nice work dude! Ill give it ago! Did you make it with opencode? Also what models do you normally use? I am coming over from claude code atm",
          "score": 2,
          "created_utc": "2026-02-23 11:08:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6y91ob",
              "author": "KnifeDev",
              "text": "Yes! I started using the app to build itself for extra test time. I use its speech to text primarily on mobile. \n\nMainly use GPT 5.3 xhigh",
              "score": 1,
              "created_utc": "2026-02-23 13:52:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xkbkh",
          "author": "oVerde",
          "text": "I noticed it does not have a way to fill servers‚Äô password",
          "score": 1,
          "created_utc": "2026-02-23 10:53:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6y88jj",
              "author": "KnifeDev",
              "text": "This is true. It‚Äôs also a gap in the official desktop/web product but it‚Äôs more important for mobile security. \n\nAdding it to the roadmap",
              "score": 2,
              "created_utc": "2026-02-23 13:48:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rboyul",
      "title": "First \"vibe coding\" experiment. With OpenCode.",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1rboyul/first_vibe_coding_experiment_with_opencode/",
      "author": "fbochicchio",
      "created_utc": "2026-02-22 15:41:00",
      "score": 27,
      "num_comments": 4,
      "upvote_ratio": 0.97,
      "text": "Hi all, today I run my first experiment at vibe coding. \n\nI don't know if it still can be called \"vibe coding\", since I am a veteran software engineer ( at first year in college, we still used punch cards, and I wrote my dissertation with Wordstar  ). \n\nI used OpenCode with the default agents ( mostly Build ) and the default LLM ( Big Pickle ). I must say I am impressed.  I managed in a couple hours to implement from scratch a small game with Rust+Ratatui, just giving interactive directions on what I wanted ( no coding suggestions or such ) and running the resulting program to see if it worked ( 95% it worked at first attempt, the remaining 5% it was able to fix the issues at second attempt ).\n\nAt work, we cannot use these tools extensively, because we cannot expose our company software to the internet for obvious reasons, so we just use LLMs to search for ideas and suggestions on how to do things with technology we are not familiar with. Which is a pity, since tools like these  would speed up development significantly. I work for a large international company, which probably can and will build its own AI infrastructure (or rent something with the proper legal restrictions in place) . But as many big companies it will move slow, and maybe I will retire first.\n\nWell, I can say I have lived almost the whole arc of human software programming, from punch cards to AI coding agents ;-)  \n\nI wish my younger collegues lots of fun with these new toys and don't worry, there will be always work for people willing to use their brain and their experience to try and use new tools.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rboyul/first_vibe_coding_experiment_with_opencode/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o6sdwcs",
          "author": "Otherwise_Wave9374",
          "text": "Love this. Vibe coding with agents feels like a new kind of REPL where you steer with intent and tests instead of syntax. The local-only constraint at work is real though, it is the biggest blocker for a lot of teams adopting AI agents beyond toy projects. If you are curious, I have seen some decent approaches for \"local-first\" agent setups and auditability discussed here: https://www.agentixlabs.com/blog/",
          "score": 7,
          "created_utc": "2026-02-22 15:48:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tojke",
          "author": "greeneyedguru",
          "text": "Big pickle is extremely competent, and free right now (not even an account required, it literally just works on opencode install)",
          "score": 2,
          "created_utc": "2026-02-22 19:21:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wwn91",
          "author": "somePadestrian",
          "text": "you can run some of these models locally and that way your company code stays local.",
          "score": 1,
          "created_utc": "2026-02-23 07:04:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xk5fi",
              "author": "fbochicchio",
              "text": "Will try, but at home  I lack the right hardware. Managed to install ollama with gemma3 on a PC with an old  NVIDIA card and  16GM of RAM but it looks like the agents do not know how to use it ( opencode connects with ollama ok, but then fail to emit any output ).\n\nAt work we have an unused server I could use (used to run wmsphere with abiut 20 VM ) , but I have to covince middle management to let me  use it to run some open LLM more performant than gemma3 ( was thinking of gwen-code ).\n\nAs I said, I am pretty sure that my company is moving the first steps toward ensuring an AI framework useful for our tasks, but if I want to work  with these new tools  before I retire, iI have to take some shortcuts.",
              "score": 2,
              "created_utc": "2026-02-23 10:51:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rbruti",
      "title": "CodeNomad v0.11.4 Released - Mobile full screen and lots of UX updates",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/99jdrz2qz2lg1.png",
      "author": "Recent-Success-1520",
      "created_utc": "2026-02-22 17:31:08",
      "score": 27,
      "num_comments": 4,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1rbruti/codenomad_v0114_released_mobile_full_screen_and/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6t2uh2",
          "author": "Otherwise_Wave9374",
          "text": "These UX improvements matter a ton for agentic CLIs, especially the context meter and better diffs. Patch review is where things still fall apart for me with agents.\n\nDo you have any roadmap for \"checkpoints\" or resumable runs, like being able to replay from a tool call boundary with the same context and artifacts?\n\nAlso, Ive been collecting posts on making AI agents more reliable (logs, evals, guardrails) here: https://www.agentixlabs.com/blog/",
          "score": 6,
          "created_utc": "2026-02-22 17:41:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6t8ou1",
              "author": "Recent-Success-1520",
              "text": "I am not sure what \"checkpoints\" are and how it would work like.  \nCodeNomad does support Fork a session from a message boundary so you can fork in new session and run again. \n\nI am available in OpenCode Discord  > Forums > CodeNomad chat for discussion. I am always open to new suggestions and ideas.\n\n  \n",
              "score": 1,
              "created_utc": "2026-02-22 18:07:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6wdlj5",
          "author": "MorningFew1574",
          "text": "You guys are doing a great job. I regret leaving Code nomad after seeing opencode come up with their own desktop version. Opencode lacks the UX and I haven't seen major updates from them....",
          "score": 1,
          "created_utc": "2026-02-23 04:29:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wqwjl",
              "author": "Recent-Success-1520",
              "text": "Easy to come back, you can resume all your work from the desktop app in CodeNomad without any migration needed",
              "score": 1,
              "created_utc": "2026-02-23 06:13:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}