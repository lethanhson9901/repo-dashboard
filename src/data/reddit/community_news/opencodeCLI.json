{
  "metadata": {
    "last_updated": "2026-01-31 08:47:09",
    "time_filter": "week",
    "subreddit": "opencodeCLI",
    "total_items": 20,
    "total_comments": 195,
    "file_size_bytes": 242483
  },
  "items": [
    {
      "id": "1qrjgfn",
      "title": "Opencode v1.1.47 and auto updates",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/9l7wwnsu6kgg1.png",
      "author": "pi314ever",
      "created_utc": "2026-01-30 21:59:22",
      "score": 98,
      "num_comments": 17,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qrjgfn/opencode_v1147_and_auto_updates/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2pgo7m",
          "author": "MySkadi",
          "text": "I understand your feeling, i was a victim of 1.1.37 version bug where every tool call and subagent activities does cost me my copilot premium request, which reduce all of my 300 premium request at once, fortunately at least the objective is achieved, but at what cost..\n\nYou can turn off the autoupdate from global opencode.json config",
          "score": 12,
          "created_utc": "2026-01-31 00:36:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qgk51",
          "author": "philosophical_lens",
          "text": "I think they should split into two releases - main and dev. Their current high velocity releases should stay on the dev branch, and they should also offer a main branch which lags behind by a week or so until it‚Äôs confirmed stable.",
          "score": 9,
          "created_utc": "2026-01-31 04:12:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ors93",
          "author": "Lyuseefur",
          "text": "![gif](giphy|iFnBpFxFGetn8BH0vZ)",
          "score": 10,
          "created_utc": "2026-01-30 22:22:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pqx38",
          "author": "Psidium",
          "text": "You shouldn‚Äôt be running any ai coding tools barebones anyway. Create a sandbox and let it lose there. The models themselves can hallucinate dangerous commands, it‚Äôs just inherent to the medium.",
          "score": 2,
          "created_utc": "2026-01-31 01:35:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2prrjd",
              "author": "pi314ever",
              "text": "While I agree with that and do sandboxing, the issue is that the vast majority of vulnerable users will probably not look that far into it. The people who don't know about the risks of auto updates are likely the same people who aren't aware of sandboxing as best practice.",
              "score": 1,
              "created_utc": "2026-01-31 01:40:30",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2r5b7p",
              "author": "gbladeCL",
              "text": "Is there a recommended sandbox? I am looking at opencode-devcontainers",
              "score": 1,
              "created_utc": "2026-01-31 07:27:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2oou60",
          "author": "Historical-Internal3",
          "text": "![gif](giphy|fT3OUK1DTJAI76ZF0i)",
          "score": 6,
          "created_utc": "2026-01-30 22:08:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2orr0x",
          "author": "Heavy-Focus-1964",
          "text": "most likely passed an empty string in to the release message generator because there were no commit hashes produced. harmless edge case. \n\nif this is enough to rattle your confidence maybe the breakneck speed and reckless abandon of AI programming is not for you",
          "score": 3,
          "created_utc": "2026-01-30 22:22:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p8ud5",
              "author": "carlanwray",
              "text": "Right? If it doesn't reseamble a seive, leaking everything everywhere it's too old school. üòÑ",
              "score": 3,
              "created_utc": "2026-01-30 23:53:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2procs",
          "author": "mrpoopybruh",
          "text": "like just use it in a sandbox like ya supposed to!",
          "score": 1,
          "created_utc": "2026-01-31 01:39:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qtk0m",
              "author": "ProfessionNo3952",
              "text": "Could you tell please in which way?",
              "score": 1,
              "created_utc": "2026-01-31 05:46:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2r62p6",
                  "author": "RegrettableBiscuit",
                  "text": "Docker is a good option.¬†",
                  "score": 1,
                  "created_utc": "2026-01-31 07:34:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2qb1fq",
          "author": "Ok_Road_8710",
          "text": "The default settings just let the agent rm rf your entire PC, so",
          "score": 1,
          "created_utc": "2026-01-31 03:36:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p5qqu",
          "author": "alovoids",
          "text": "lol",
          "score": 1,
          "created_utc": "2026-01-30 23:36:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pcq5i",
          "author": "doodirock",
          "text": "Dude relax",
          "score": 0,
          "created_utc": "2026-01-31 00:15:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pm9hz",
          "author": "neamtuu",
          "text": "Clown. What are you afraid of? Check the files for yourself if you think of a security breach and come up with a conclusion. Stop assuming uncertain checkable realities.",
          "score": -4,
          "created_utc": "2026-01-31 01:08:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqmdcs",
      "title": "Kimi is FREE for a limited time in OpenCode CLI!",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qqmdcs/kimi_is_free_for_a_limited_time_in_opencode_cli/",
      "author": "jpcaparas",
      "created_utc": "2026-01-29 21:55:41",
      "score": 93,
      "num_comments": 34,
      "upvote_ratio": 0.98,
      "text": "https://preview.redd.it/2t904wj91dgg1.png?width=581&format=png&auto=webp&s=ec530b8a251fccf7440a64d426c59d2e846c50d0\n\nYou heard that right boys and gals!\n\nEdit: Kimi K2.5 *specifically*.\n\nEdit 2: Check out the benchmarks and capabilities [here](https://generativeai.pub/moonshots-kimi-k2-5-can-spawn-100-ai-agents-to-do-your-work-5c7f0bd90a88?sk=f68e605853d40ad859c9d5507b9e0749).\n\nEdit 3: [Dax](https://x.com/thdxr/status/2016993820940943623?s=20) stands by Kimi K2.5, says it's at par with Opus 4.5.\n\nEdit 4: Here's my longform, non-paywalled review after trying it out for the last 24 hours (with a solid recommendation from OpenCode's co-creator, Dax):\n\n(Obviously, try it out for free first before you make the switch to a paid provider, either with Zen, Chutes, NanoGPT, or Synthetic)\n\n‚û°Ô∏è¬†[Stop using Claude‚Äôs API for Moltbot (and OpenCode)](https://jpcaparas.medium.com/stop-using-claudes-api-for-moltbot-and-opencode-52f8febd1137)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qqmdcs/kimi_is_free_for_a_limited_time_in_opencode_cli/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2hzt1j",
          "author": "jpcaparas",
          "text": "https://preview.redd.it/a4d728zubdgg1.png?width=628&format=png&auto=webp&s=3d5a1573eefed1803f4df89a7a9daf21d63e17a0\n\nThis is all the validation you need right now.",
          "score": 17,
          "created_utc": "2026-01-29 22:55:05",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2j52wk",
              "author": "Sensitive_Song4219",
              "text": "For debugging I'm finding Kimi 2.5 a definite step down from Opus / Codex-High. Feels more like Sonnet/Codex-Medium/GLM4.7 in that regard, at least in the projects I've tried it in so far. (I don't think I can do without at least a small subscription to a frontier-model - at least not quite yet!)\n\nBut  *Kimi K2.5 Free* via OpenCode-Zen is quite fast, I assume Moonshot's doing the hosting for them. Impressive. (Edit: poster below correctly mentions the X post says *fireworks* is hosting - they're doing a good job!)",
              "score": 6,
              "created_utc": "2026-01-30 02:39:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2kqio6",
                  "author": "lofuller",
                  "text": "Fireworks is providing the inference (Dax confirmed)",
                  "score": 2,
                  "created_utc": "2026-01-30 09:42:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2jl483",
                  "author": "jmhunter",
                  "text": "ya, agree...",
                  "score": 1,
                  "created_utc": "2026-01-30 04:12:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2jfpna",
              "author": "philosophical_lens",
              "text": "I‚Äôm looking forward to trying K2.5, but that post is marketing, not validation.",
              "score": 4,
              "created_utc": "2026-01-30 03:40:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ht1mo",
          "author": "Metalwell",
          "text": "I just had signed up for free tier in moonshot, imma cancel it now",
          "score": 5,
          "created_utc": "2026-01-29 22:20:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iao1l",
          "author": "baldreus",
          "text": "It‚Äôs insanely cheap on chutes too.  300 requests a day on the lowest tier for 3$ a month.  You can‚Äôt beat free but the advantage with chutes is privacy if you care (which may be a concern if pitching a provider at work)",
          "score": 4,
          "created_utc": "2026-01-29 23:52:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2idghp",
              "author": "minato_shikamaru",
              "text": "Is chutes a good provider, some people are telling me not to use third party providers because they don't run well.¬†",
              "score": 3,
              "created_utc": "2026-01-30 00:08:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2in5cv",
                  "author": "baldreus",
                  "text": "I've only been using it for a couple of days since this model came out, so take this with a grain of salt. As long as the model is hot loaded (in other words it‚Äôs actively being used by others), it seems to be pretty responsive. If the model is cold and hasn't been used for a while, it does take some time for it to spin up instances. Yes there could be occasions when you have to wait a bit but in my experience 98% of the time it's been fast. Let's wait and see what happens when everyone piles on once the hype reaches critical mass. \n\nThat said it's definitely way better than free tier openrouter models that are insanely rate limited and slow - so, so far it's been a great experience.",
                  "score": 4,
                  "created_utc": "2026-01-30 01:00:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2i8an9",
          "author": "ECrispy",
          "text": "how do I find the free model? /connect doesnt list it",
          "score": 2,
          "created_utc": "2026-01-29 23:40:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2k4tr2",
              "author": "deadcoder0904",
              "text": "https://github.com/anomalyco/opencode/issues/11261\n\ntl;dr do `opencode models --refresh` before launching `opencode`",
              "score": 10,
              "created_utc": "2026-01-30 06:31:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2k591k",
                  "author": "ECrispy",
                  "text": "thanks",
                  "score": 1,
                  "created_utc": "2026-01-30 06:35:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ik91x",
              "author": "____HakunaMatata____",
              "text": "/models",
              "score": 1,
              "created_utc": "2026-01-30 00:44:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2idiek",
          "author": "hexa01010",
          "text": "Sweet! I agree this model is awesome first time we have real SOTA with open models finally!\n\nI think Kimi also increased their quotas a lot I went from like 10% yesterday to 1% today on the weekly",
          "score": 2,
          "created_utc": "2026-01-30 00:08:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2j3w8d",
              "author": "Hoak-em",
              "text": "Yeah they switched to token-based (instead of API-request based) and have a limited-time 3x for the higher-tier plans (I'm not sure if the lower-tier one has it too). Go ham with it now I figure -- also it's fast :3",
              "score": 1,
              "created_utc": "2026-01-30 02:33:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2hsr7s",
          "author": "krimpenrik",
          "text": "Nice",
          "score": 1,
          "created_utc": "2026-01-29 22:19:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hukiu",
          "author": "SilverMethor",
          "text": "Nice!",
          "score": 1,
          "created_utc": "2026-01-29 22:28:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hwjb4",
          "author": "MeasurementPlenty514",
          "text": "I was a mega-slut for mimo 2 when free when I needed to just munch and gobble tokens.",
          "score": 1,
          "created_utc": "2026-01-29 22:38:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2idfma",
          "author": "ianxiao",
          "text": "So no thinking ?",
          "score": 1,
          "created_utc": "2026-01-30 00:08:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2jhwkf",
          "author": "LtCommanderDatum",
          "text": "Error: 401 Unauthorized",
          "score": 1,
          "created_utc": "2026-01-30 03:52:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ji0te",
              "author": "jpcaparas",
              "text": "It got reddit hugged lol",
              "score": 3,
              "created_utc": "2026-01-30 03:53:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2jm7n6",
          "author": "kr_roach",
          "text": "Provider of Kimi 2.5 free model is moonshot ai??",
          "score": 1,
          "created_utc": "2026-01-30 04:19:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l79cz",
              "author": "mizzuri",
              "text": "Fireworks AI",
              "score": 1,
              "created_utc": "2026-01-30 12:02:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2n0qfa",
          "author": "neamtuu",
          "text": "https://preview.redd.it/8q48vc28vigg1.png?width=2151&format=png&auto=webp&s=eca507bf5d5424e206f2cbd499ed7cd8be1234f8\n\n‚Ä¢ OpenCode 1.1.45 - oh yeah. it works so well. Fuck off.",
          "score": 1,
          "created_utc": "2026-01-30 17:32:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2i6mpr",
          "author": "kpgalligan",
          "text": "Just bought zen credits. Have to see if I can use some extra gas here...",
          "score": 1,
          "created_utc": "2026-01-29 23:31:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2i73n0",
              "author": "jpcaparas",
              "text": "The token pricing itself will get you a lot of range for that fill.",
              "score": 1,
              "created_utc": "2026-01-29 23:33:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ij5a0",
                  "author": "kpgalligan",
                  "text": "I went to look at the dashboard after poking around for a while, and the chart struggled to show <$1 of usage :)\n\nWill try to use free, but not sure it'll matter all that much. I'm not exactly hammering it.",
                  "score": 2,
                  "created_utc": "2026-01-30 00:38:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ic1yk",
          "author": "atkr",
          "text": "why advertise it?",
          "score": -2,
          "created_utc": "2026-01-30 00:00:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlunsk",
      "title": "GLM 4.7 removed from the free models",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/uenlpakmbcfg1.jpeg",
      "author": "marmoure",
      "created_utc": "2026-01-24 18:28:00",
      "score": 81,
      "num_comments": 42,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qlunsk/glm_47_removed_from_the_free_models/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1han0e",
          "author": "kapiteinklapkaak",
          "text": "GLM 4.7 was free on OpenCode for a limited time so the ycan test the model. That's basicly it",
          "score": 13,
          "created_utc": "2026-01-24 19:24:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2g714g",
              "author": "Background-Leg-6840",
              "text": "I just got GLM and everything else back\n\nhttps://preview.redd.it/lbc9tqa8tbgg1.png?width=617&format=png&auto=webp&s=87b9264de0a62654109b24f1d6e7daa321886213",
              "score": 1,
              "created_utc": "2026-01-29 17:48:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1j3dhk",
          "author": "robberviet",
          "text": "Also MiniMax. Damn it was good.",
          "score": 10,
          "created_utc": "2026-01-25 00:39:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ld3gl",
              "author": "tigerbrowneye",
              "text": "And it‚Äôs super fast and very capable. Rushing through my issues‚Ä¶",
              "score": 1,
              "created_utc": "2026-01-25 09:51:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1o38xp",
              "author": "luongnv-com",
              "text": "Yeah, that‚Äôs my go to opencode too",
              "score": 1,
              "created_utc": "2026-01-25 18:54:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1h7uqq",
          "author": "disgruntledempanada",
          "text": "The ride is over.\n\nMight pick up a Zai subscription (it's super cheap) but as a hobbyist just tinkering with stuff and using it very sporadically I think my interest just took a hit.",
          "score": 19,
          "created_utc": "2026-01-24 19:12:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1l3qt3",
              "author": "ProfessorSpecialist",
              "text": "I bought the lite subscription for zai and must say i dont know what i expected. Its dirt cheap, and its impressive what it can achieve at its best. But on average its painfully slow and stupid af. It regularly fails read write toolcalls, corrupts files even at 20% context usage and is absolutely garbage at debugging. But to be fair, i use opus at work, so my point of reference is skewed.",
              "score": 5,
              "created_utc": "2026-01-25 08:28:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ral46",
                  "author": "Embarrassed_Egg2711",
                  "text": "I bought a higher end sub and been very happy so far.",
                  "score": 1,
                  "created_utc": "2026-01-26 04:01:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1m35g5",
              "author": "shooshmashta",
              "text": "You lost interest because it stopped being free?",
              "score": 1,
              "created_utc": "2026-01-25 13:21:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1mjdzw",
                  "author": "slayyou2",
                  "text": "Consumers in a nutshell",
                  "score": 1,
                  "created_utc": "2026-01-25 14:51:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1nwhv3",
                  "author": "whimsicaljess",
                  "text": "there's a certain tradeoff with these models.\n\nif you have a very capable model, it becomes almost always worth using even at relatively high cost because saving even a small percentage of human labor is worth a lot.\n\nif you have a very stupid model, it becomes almost never worth using even at a relatively low cost because it doesn't meaningfully decrease (or worse, increases) the amount of human labor required to complete a task. \n\nGLM4.7 on z.ai is, imo, in the second category. it might be worth using when free for very low priority tasks that you don't particularly care about. but for anything else, it's not worth trying- and it's certainly not worth paying for.",
                  "score": 1,
                  "created_utc": "2026-01-25 18:26:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1gyid1",
          "author": "Background-Leg-6840",
          "text": "Why is this? I am also missing Grok Code Fast. They went away after I updated the open-code desktop app.\n\nhttps://preview.redd.it/myc7k5lfccfg1.png?width=290&format=png&auto=webp&s=8bf441c2cc0e7cb6aa76838f351bb2d3c6ea3b89",
          "score": 6,
          "created_utc": "2026-01-24 18:32:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eqad6",
              "author": "Just-Veterinarian397",
              "text": "Big Pickle receives too much requests. So it is not usable at all.  \nGPT-5 Nano's quality is so so but even it became paid",
              "score": 2,
              "created_utc": "2026-01-29 13:42:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hh290",
                  "author": "Background-Leg-6840",
                  "text": "Amen, it should not take an hour for one request \n\nhttps://preview.redd.it/aaz0iphkvcgg1.png?width=274&format=png&auto=webp&s=b6035ad3394a0f9306c6765200d4891d5442f719",
                  "score": 1,
                  "created_utc": "2026-01-29 21:23:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2g6wic",
              "author": "Background-Leg-6840",
              "text": "I just got everything back \n\nhttps://preview.redd.it/nvjdkhn4tbgg1.png?width=617&format=png&auto=webp&s=720a88b8820b9cd64bd8fadee5ac5f8b591acf0a",
              "score": 1,
              "created_utc": "2026-01-29 17:48:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1kr25p",
              "author": "No_Success3928",
              "text": "Elons getting rid of that one too",
              "score": -1,
              "created_utc": "2026-01-25 06:41:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1hw8vc",
          "author": "smile132465798",
          "text": "There used to be zero free models. At least we still had Big Pickle. Now I‚Äôm just waiting for the next DeepSeek drop next month.",
          "score": 4,
          "created_utc": "2026-01-24 21:04:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2a9l8z",
              "author": "Dank-13",
              "text": "I read somewhere that it is as good as opus 4.5, with multiple tools and a highly coding-focused approach. idk if that good of a model will be open-source but if it comes then at least it's gonna be better than glm 4.7... I hope they'll provide it at a good price like Zai",
              "score": 1,
              "created_utc": "2026-01-28 20:44:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1k22gm",
          "author": "Disastrous-Mix6877",
          "text": "What about grok code fast? And minimax? They‚Äôre all gone now?",
          "score": 2,
          "created_utc": "2026-01-25 03:53:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1hwury",
          "author": "InfraScaler",
          "text": "Yup, it's gone. The good thing though is that the GLM Coding Plan goes as cheap as $2.40/mo if you pay for a year (or $3 if you pay monthly). \n\nAlso, with another subscriber's link you get 10% extra credits. Here's mine if anyone wants those extra 10% credits: [https://z.ai/subscribe?ic=WBMQNQBVIS](https://z.ai/subscribe?ic=WBMQNQBVIS)",
          "score": 5,
          "created_utc": "2026-01-24 21:07:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1i6yqc",
              "author": "DistinctWay9169",
              "text": "It is $6 monthly, $3 is only the first time.",
              "score": 3,
              "created_utc": "2026-01-24 21:55:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ia0wg",
                  "author": "InfraScaler",
                  "text": "True! But if you get the full year it becomes effectively $2.40/mo for 12 months, which is insane.",
                  "score": 6,
                  "created_utc": "2026-01-24 22:10:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ia61u",
                  "author": "WPDumpling",
                  "text": "If you buy an entire year of the cheapest plan, the first-time discount makes it work out so you're effectively paying $2.40/month",
                  "score": 2,
                  "created_utc": "2026-01-24 22:10:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1j8v07",
          "author": "lmagusbr",
          "text": "Not only that, but Big Pickle is being throttled now too.\n\nI think it's because of all the people using Clawdbot.",
          "score": 1,
          "created_utc": "2026-01-25 01:09:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1k5cup",
          "author": "wiroaj",
          "text": "Is GLM 4.7 a viable alternative to Gemini 3? I'm currently using Claude Opus 4.5 via Google Antigravity, but due to strict quota limits, I tried switching to Gemini 3. Unfortunately, I found the performance disappointing compared to Opus. Would GLM 4.7 be a better fit?",
          "score": 1,
          "created_utc": "2026-01-25 04:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1l3mec",
              "author": "shaonline",
              "text": "GLM 4.7 is an ok Sonnet 4.5 replacement, nowhere near Opus though.",
              "score": 3,
              "created_utc": "2026-01-25 08:27:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1le1f2",
              "author": "No_Success3928",
              "text": "Glm is less likely to lay waste to your codebase üòÜ",
              "score": 1,
              "created_utc": "2026-01-25 09:59:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ljb1q",
          "author": "Deep_Category_5013",
          "text": "The encoding capabilities of this model are quite good.",
          "score": 1,
          "created_utc": "2026-01-25 10:46:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29r6wk",
          "author": "Boring-Ad-5924",
          "text": "Is it worth paying for OpenCode Zen? Or should I sub somewhere else?",
          "score": 1,
          "created_utc": "2026-01-28 19:22:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2av4mw",
              "author": "marmoure",
              "text": "Don't, I paid for the 20$ offer one prompt 0.5$   \nI would go for the GLM 3$ subscription.",
              "score": 1,
              "created_utc": "2026-01-28 22:18:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2cuntc",
                  "author": "alexeiz",
                  "text": "Aren't you paying for API tokens with Opencode Zen?  It's not a subscription.  It's similar to Cline and Kilocode (or Openrouter).  Opencode Black is a subscription though.",
                  "score": 1,
                  "created_utc": "2026-01-29 04:50:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2kq33m",
                  "author": "Squale279",
                  "text": "Hi,\nCould you please explain to me how to use GLM subscription with opencode?",
                  "score": 1,
                  "created_utc": "2026-01-30 09:38:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1h1h5w",
          "author": "Suitable-Program-181",
          "text": "2 days ago if im not wrong.\n\nGroq and gpt nano where so bad not even free should be shared again. Big pickle is solid if you know what you need or have clear instructions. If they keep it free will be solid.",
          "score": 1,
          "created_utc": "2026-01-24 18:44:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h6cvv",
              "author": "sevindi",
              "text": "Big Pickle is GLM 4.6",
              "score": 5,
              "created_utc": "2026-01-24 19:05:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1iajxt",
                  "author": "RiskyBizz216",
                  "text": "I don't think it is. Big Pickle doesn't have a reasoning channel (or <think> tags) but GLM 4.6 does.\n\nI suspect its GPT 4o\n\nTested (kinda confirmed):  \nIt says its training cutoff is 2024. \n\nAnd if you straight up ask it - it'll claim to be Claude Sonnet 3.5 \n\nI told it \"Claude Sonnet 3.5 was deprecated, not hosted anywhere online\"\n\nIt had an identity crisis, and went down a list of 2024 models until it honed in on GPT-4o.",
                  "score": 0,
                  "created_utc": "2026-01-24 22:12:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1h8f9o",
                  "author": "Suitable-Program-181",
                  "text": "Im not sure, I read that yesterday and thats why I tested it and I cant complaint.\n\nI have an lenovo laptop with athlon silver cpu... tiny 10 is laggy, mint is laggy so i asked pickle just for fun if we can cook something and went straight for vulkan lol \n\nI know is not a premium model from a premium provider but gpt 5 and grok are \"premium\" providers and the difference was huge. Gpt I had to keep asking him to do something besides talking and grok feels like a free model charged as premium... classic elon scams.",
                  "score": -1,
                  "created_utc": "2026-01-24 19:14:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1isql3",
          "author": "ZeSprawl",
          "text": "It‚Äôs removed but the pay version of GLM 4.7 is like 5x the speed, and pretty cheap.",
          "score": 1,
          "created_utc": "2026-01-24 23:44:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr6u36",
      "title": "I tried Kimi K2.5 with OpenCode it's really good",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qr6u36/i_tried_kimi_k25_with_opencode_its_really_good/",
      "author": "orucreiss",
      "created_utc": "2026-01-30 14:24:39",
      "score": 69,
      "num_comments": 40,
      "upvote_ratio": 0.93,
      "text": "Been testing Kimi For Coding (K2.5) with OpenCode and I am impressed. The model handles code really well and the context window is massive (262K tokens).\n\nIt actually solved a problem I could not get Opus 4.5 to solve which surprised me.\n\nHere is my working config: [https://gist.github.com/OmerFarukOruc/26262e9c883b3c2310c507fdf12142f4](https://gist.github.com/OmerFarukOruc/26262e9c883b3c2310c507fdf12142f4)\n\n# Important fix\n\nIf you get thinking is enabled but reasoning\\_content is missing - the key is adding the interleaved option with \"field\": \"reasoning\\_content\". That's what makes it work.\n\nHappy to help if anyone has questions!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qr6u36/i_tried_kimi_k25_with_opencode_its_really_good/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2mllgx",
          "author": "RegrettableBiscuit",
          "text": "The more I use it, the more impressed I am. GLM 4.7 seemed good initially, but as I kept using it, I noticed issues with more complex tasks. But if you put K2.5 and Sonnet 4.5 in front of me and asked me to tell which is which based on how well they work, I probably would need a bit of time to figure it out, if I could at all.¬†",
          "score": 19,
          "created_utc": "2026-01-30 16:25:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mrtkx",
              "author": "Grand-Management657",
              "text": "Right now I am not doing super complex tasks, mostly telling K2.5 to convert an old wordpress site to modern static site. It handles it very well. I spun up 18 subagents at once to do each page individually and it executed without any errors. Did a much better job at the UI than GLM 4.7 IMO. But GLM's UI design was never its strong suite. With better prompting and skills, I probably wouldn't notice much of a difference between K2.5 and GLM 4.7. But at the same time K2.5's raw intelligence due to its training and parameter size just makes it so much smarter IMO.",
              "score": 5,
              "created_utc": "2026-01-30 16:52:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2mqbzb",
          "author": "epicfilemcnulty",
          "text": "Lots of folks praising this model, and I guess it does deliver for their use cases (particularly, I'd assume that it should be good for TS/JS and Python coding), but I've tried it several times with my codebase, which is C + Lua mix and pretty complex, and while it usually comes up with a pretty decent plan, but the execution is bad -- it looses focus, it changes function signatures but forgets to update the invocation calls, and so on. Opus nails the same task with the same prompt. But it is really fast, that's true.",
          "score": 4,
          "created_utc": "2026-01-30 16:46:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2msd91",
              "author": "Grand-Management657",
              "text": "Exactly you hit the nail on the head. I found it very good in TS/JS environements but I hear reviews from those who use it for other languages or libraries and it falls short. Have you tried to use Opus as your planner and K2.5 as your executor? I am curious if that would yield better results for you.",
              "score": 5,
              "created_utc": "2026-01-30 16:55:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2n2qqa",
                  "author": "epicfilemcnulty",
                  "text": "Have not tried this approach yet, will give it a shot. I'd very much love to improve its performance on my codebase, because it's much cheaper than Opus, it's fast and it's open weights.",
                  "score": 2,
                  "created_utc": "2026-01-30 17:41:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2oj4jg",
              "author": "thatsnot_kawaii_bro",
              "text": "It's the usual cycle:\n\n*Hype up model X as the second coming of christ. Say it's the real deal compared to previous models*\n\nWeeks/months later:\n\n*Hype up new model as the second coming of christ, say that X was overhyped but this is the real deal*",
              "score": 4,
              "created_utc": "2026-01-30 21:40:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ofsxf",
              "author": "frasiersbrotherniles",
              "text": "I know benchmarking is kind of broken but it would be very interesting to see a rating of each model's competency at different languages. Do you know if anyone tries to evaluate that?",
              "score": 2,
              "created_utc": "2026-01-30 21:25:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2olyfo",
                  "author": "epicfilemcnulty",
                  "text": "No, unfortunately, I don't know if anyone is working on that. I'd be very interested to see it, though, but I think it's not a trivial task to do, if we are talking about a thorough benchmark -- last time I looked at some of python benchmarks I was not impressed at all, usually it's just a set of one-shot tasks. On one hand, it does make sense -- if you ask a model to create a function that does X, you can actually verify if the implementation is correct. But it's much harder to create a benchmark that would include complex tasks like code refactoring involving multiple files -- particularly when it comes to assessing the results... But I was not actually following this benchmarking area lately, maybe there is something like this already... My approach is empirical -- I just try different models with my real codebase and see how they perform. But of course that is not a \"real\" benchmarking.",
                  "score": 2,
                  "created_utc": "2026-01-30 21:54:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2p5xuj",
              "author": "Federal-Initiative18",
              "text": "I have been using it with C# mainly with no issues and the code looks much better than Sonnet 4.5",
              "score": 2,
              "created_utc": "2026-01-30 23:37:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2n6299",
          "author": "jmhunter",
          "text": "I think it's really great that OpenCode was able to get it for free for a period for us.   \n  \nSo far it works fairly well, but it seems to kind of fizzle after one task, it reminds me of Sonnet 3.5. You will definitely have to keep an eye on your task management. It does not seem to have its own. We probably need a good agent harness/opening prompt/system prompt for this?   \n  \nI have not tried it with something like Beads and see if it can keep an eye on that. But it does actively engage with Serena it seems to be fairly good at recognizing tools and utilizing them.   \n  \nI made a video about some changes I made on a personal use project and it did an OK job but now that I've messed with it some more and done some IT tasks with it I recognize that it kind of fizzles after one task and comes back to the user. I'd be curious to hear from people who use hooks like Ralph Wiggum.\n\n[https://youtu.be/vWylCQtQ1Bs?si=2VqriQL\\_yMlNKJ1c](https://youtu.be/vWylCQtQ1Bs?si=2VqriQL_yMlNKJ1c)",
          "score": 4,
          "created_utc": "2026-01-30 17:56:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mjvg4",
          "author": "xmnstr",
          "text": "I have the same experience, very impressed! Got the $20 subscription for $3.49 and cancelled my Cursor subscription immediately. This is so much better, and the limits are insane. I can't get over how fast it is!",
          "score": 6,
          "created_utc": "2026-01-30 16:17:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2nlvdg",
              "author": "MarvNC",
              "text": "If you have a lot of time on your hands you can get it to $0.99. Pretty fun honestly.",
              "score": 2,
              "created_utc": "2026-01-30 19:05:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pjomh",
                  "author": "xmnstr",
                  "text": "Well I guess I need to start new conversations. Mine hit a point where I needed to share to my socials to get it lower. Not worth it.",
                  "score": 1,
                  "created_utc": "2026-01-31 00:53:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2mnc33",
              "author": "bigh-aus",
              "text": "can you tell me more about the $3.49 sub?",
              "score": 4,
              "created_utc": "2026-01-30 16:33:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mwe1x",
                  "author": "shaonline",
                  "text": "You need to haggle with the web chatbot on kimi's website to knock the price down, it's the \"Moderato\" sub.",
                  "score": 8,
                  "created_utc": "2026-01-30 17:13:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2n6u7q",
                  "author": "slothkenny",
                  "text": "I couldn‚Äôt get it to go below 4 bucksüò≠",
                  "score": 2,
                  "created_utc": "2026-01-30 17:59:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ml2wr",
          "author": "cartazio",
          "text": "patch the default prompt to be more chill and the reasoning will work better¬†",
          "score": 3,
          "created_utc": "2026-01-30 16:23:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nzfpg",
          "author": "throwaway12012024",
          "text": "tried w/opencode. This model is so slow, almost codex-level slow. Still hard to beat opus codex for planning and flash for coding.",
          "score": 3,
          "created_utc": "2026-01-30 20:07:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ncfer",
          "author": "Aggravating_Bad4163",
          "text": "It really looks good. I tried it with opencode and it just worked fine.",
          "score": 2,
          "created_utc": "2026-01-30 18:24:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2nerhd",
              "author": "orucreiss",
              "text": "yeah feels smotth with opencode",
              "score": 1,
              "created_utc": "2026-01-30 18:34:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ng6so",
          "author": "Visual_Weather_7937",
          "text": "Hello! I can't understand: why do I need such a config if I can simply choose from the list of Kimi 2.5 models in OC?",
          "score": 2,
          "created_utc": "2026-01-30 18:40:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2njg7t",
              "author": "orucreiss",
              "text": "its because i am using [https://github.com/code-yeongyu/oh-my-opencode](https://github.com/code-yeongyu/oh-my-opencode) and i want to customize an agent (Atlas) to use the model. ",
              "score": 0,
              "created_utc": "2026-01-30 18:54:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nsiou",
          "author": "uttkarsh26",
          "text": "Json parse errors are not good, but nonetheless pretty solid so far\n\nDoes misunderstand sometime if not being explicit",
          "score": 2,
          "created_utc": "2026-01-30 19:35:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2o1dtx",
          "author": "Putrid-Pair-6194",
          "text": "Tried it for the first time today using a monthly subscription, which I got for $3.49. Could have been lower but I got tired of haggling.\n\nI don‚Äôt have enough usage yet for feedback on quality. But speed was very fast compared to other models I use in opencode. Leaves GLM 4.7 in the dust.",
          "score": 2,
          "created_utc": "2026-01-30 20:16:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2o998n",
              "author": "funzbag",
              "text": "How did you get that low price?",
              "score": 2,
              "created_utc": "2026-01-30 20:54:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ooshn",
                  "author": "Putrid-Pair-6194",
                  "text": "They encourage negotiation with their online bot. Start telling the bot innovative ways you will promote their service to other people. After about 7 back and forth chats, I got down to $3.49 for the first month.",
                  "score": 3,
                  "created_utc": "2026-01-30 22:08:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2p1c5b",
          "author": "OffBoyo",
          "text": "Opus has been terrible as of late so not very suprising. Test alongside 5.2 Xhigh",
          "score": 2,
          "created_utc": "2026-01-30 23:12:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pl3mo",
          "author": "Finn55",
          "text": "How big is the context window? For local and hosted (does it have a difference?). I‚Äôm using MiniMax 2.1 Q6 GGUF Unsloth, and I‚Äôm ok with it but the 200k context is difficult to work with for longer sessions and larger repos",
          "score": 2,
          "created_utc": "2026-01-31 01:01:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mw1o3",
          "author": "Aardvark_Says_What",
          "text": "not for me. it just fucked up my svelte / css stack and couldn't unfuck it. \n\nthank Linus for git.",
          "score": 3,
          "created_utc": "2026-01-30 17:11:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r2l1a",
          "author": "npittas",
          "text": "For me kimi for coding works fine without the interleave option, but I cannot make the normal kimi API key to work for the non coding models, the normal [Moonshot.ai](http://Moonshot.ai) API. That is the one that shows the \"reasoning\\_content is missing\" error. I had not needed to make any changes to the opencode.json at all to make kimi for coding work. But the [moonshot.ai](http://moonshot.ai) API, well, nothing...  \nIf anyone has any idea, that would be awsome.  \nMy experience with kimi 2.5 is far superior that expected, and I am actively using it along side opus. And it is fast enough, that I can relly on it and even let it run as main for clawdbot!",
          "score": 1,
          "created_utc": "2026-01-31 07:02:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r35v2",
          "author": "Queasy_Asparagus69",
          "text": "not really; I got the $20 plan and it can't figure out how to do a simple website oath; been going for an hour trying to make the login work....",
          "score": 1,
          "created_utc": "2026-01-31 07:08:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lw80a",
          "author": "pokemonplayer2001",
          "text": "The sadness I feel for people scrambling to post their experience with things is accumulating.\n\nCongrats u/orucreiss, here's your participant ribbon.",
          "score": -28,
          "created_utc": "2026-01-30 14:27:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m1mzq",
              "author": "Dyhart",
              "text": "Some people don't have others to talk to about this kind of stuff so this is their way to connect with people. No need to talk others down",
              "score": 11,
              "created_utc": "2026-01-30 14:54:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2m0ybj",
              "author": "disgruntledempanada",
              "text": "Please talk to a therapist or ask Kimi to act like one.",
              "score": 4,
              "created_utc": "2026-01-30 14:50:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2m1cze",
                  "author": "pokemonplayer2001",
                  "text": "No.",
                  "score": -8,
                  "created_utc": "2026-01-30 14:52:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qmt0ib",
      "title": "Sharing my OpenCode config",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qmt0ib/sharing_my_opencode_config/",
      "author": "filipbalada",
      "created_utc": "2026-01-25 19:38:43",
      "score": 68,
      "num_comments": 19,
      "upvote_ratio": 0.96,
      "text": "I‚Äôve put together an OpenCode configuration with custom agents, skills, and commands that help with my daily workflow. Thought I‚Äôd share it in case it‚Äôs useful to anyone.üòä\n\nhttps://github.com/flpbalada/my-opencode-config\n\nI‚Äôd really appreciate any feedback on what could be improved. Also, if you have any agents or skills you‚Äôve found particularly helpful, I‚Äôd be curious to hear about them. üòä Always looking to learn from how others set things up.\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qmt0ib/sharing_my_opencode_config/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o1olvwh",
          "author": "msrdatha",
          "text": "Respect and Thanks, for taking time to prepare and share these configuration details. \n\nIt has lot of valuable contents, and I am sure, will need to spend at least a week even to fully understand some of them on how to use.\n\nReally appreciate the thought on \"Always looking to learn\" part. Just my two cents, based on my experience and the config - you seems to be using ollama. If you would like to try with llama.cpp, it might give an edge over ollama. It seems much better optimized in using system resources, and also gets support for newer models much sooner than ollama.",
          "score": 9,
          "created_utc": "2026-01-25 20:16:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sjpm4",
              "author": "filipbalada",
              "text": "Thank you! I'd love to hear more of your ideas and thoughts. Great point about llama.cpp. I'll definitely give it a try. The better resource optimization sounds promising, especially for experimenting with newer models. :))",
              "score": 4,
              "created_utc": "2026-01-26 09:54:38",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o28eq7y",
              "author": "spaceSpott",
              "text": "Isn't vllm better than Llama.cpp? Honest question",
              "score": 1,
              "created_utc": "2026-01-28 15:52:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o29hu1d",
                  "author": "msrdatha",
                  "text": "I did not try running vllm yet. My understanding is that vllm performs better, when there are multiple gpus. On single system (Mac or 1 GPU Linux) llama.cpp is more optimized. Please correct me if you happened to have more experience on this.",
                  "score": 1,
                  "created_utc": "2026-01-28 18:41:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1olqoy",
          "author": "lundrog",
          "text": "Maybe a readme?",
          "score": 5,
          "created_utc": "2026-01-25 20:15:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sk061",
              "author": "filipbalada",
              "text": "Good point. :)) Since this was just my personal config I decided to share, I hadn't thought about documentation initially. But you're right. I've now added a readme and marked the repository with an MIT license. Thanks! üôè",
              "score": 2,
              "created_utc": "2026-01-26 09:57:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1opu9e",
          "author": "jmreicha",
          "text": "How do you interact with and leverage the agent configs? Looks like some interesting personas for specific use cases.",
          "score": 2,
          "created_utc": "2026-01-25 20:33:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sks4t",
              "author": "filipbalada",
              "text": "Thank you for your question. :)) I'm still trying to find the ideal workflow to automate. What I've found useful is to start developing a feature with the requirements analyzer agent and save the content/context to a GitHub issue. After I've designed the requirements document, I use it with the \"plan\" agent. Then I loop over the implementation. I personally still handle pull requests, the implementation, and try to keep them as small as possible so that my team can easily review them. During development, I use all agents like reviewer, deep thinker, and code simplifier based on what I consider I need. I also use the git commit agent a lot, that leverages shell scripts so that committing is quite fast. :)) ",
              "score": 2,
              "created_utc": "2026-01-26 10:04:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1z9j1i",
                  "author": "silopolis",
                  "text": "I very much like this idea of keeping the issue in the loop to both control and document the process! üëç",
                  "score": 2,
                  "created_utc": "2026-01-27 07:48:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1p9fyy",
          "author": "bagvix",
          "text": "Thank you. I am a beginner to opencode will learn it by looking your config files.",
          "score": 2,
          "created_utc": "2026-01-25 21:58:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sj8vv",
              "author": "filipbalada",
              "text": "Feel free to share your thoughts or ideas :)) ",
              "score": 2,
              "created_utc": "2026-01-26 09:50:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1qmwsh",
          "author": "jellydn",
          "text": "Could you explain why and how you set up your system? Like mine here http://ai-tools.itman.fyi/",
          "score": 2,
          "created_utc": "2026-01-26 01:54:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1pcwfj",
          "author": "gia_rox",
          "text": "Are the skills there all created by the skill creator agent? Just wondering the usefulness of it",
          "score": 1,
          "created_utc": "2026-01-25 22:13:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sl1xw",
              "author": "filipbalada",
              "text": "I personally design and maintain the prompts, but the skill creator helps a lot. Before OpenCode existed, I maintained a personal 'second brain'. A collection of notes on programming patterns, product management frameworks, and psychology principles... I found it valuable to transform these notes into skills that the AI can execute (skill creator helped a lot), making my accumulated knowledge actionable during coding sessions. :)) ",
              "score": 2,
              "created_utc": "2026-01-26 10:06:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1x6kxi",
          "author": "robertmachine",
          "text": "it seems the agents/skill-creator.md is for claude",
          "score": 1,
          "created_utc": "2026-01-26 23:58:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlqj0q",
      "title": "Benchmarking with Opencode (Opus,Codex,Gemini Flash & Oh-My-Opencode)",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/7c0g6rjkjbfg1.png",
      "author": "tisDDM",
      "created_utc": "2026-01-24 15:54:37",
      "score": 62,
      "num_comments": 31,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qlqj0q/benchmarking_with_opencode_opuscodexgemini_flash/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1g9k85",
          "author": "kshnkvn",
          "text": "Subagents are not about saving context in general, but about avoiding context bloat.  \nWhen each agent and subagent strictly performs its specific task and each of them has only the information they need in their context, the quality of generation is much higher.",
          "score": 11,
          "created_utc": "2026-01-24 16:43:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h5hc8",
              "author": "tisDDM",
              "text": "I agree to a certain point. That's the theory. \n\nIn the last two week I read a hell of a lot of logs. If you see how easy the context bloats, that's crazy. Every tool call counts because it is resending the full conversation history. I've seen models cross checking sub agents results causing more context bloat than doing it themselves. And much more. Nearly impossible to switch it off by prompting.",
              "score": 0,
              "created_utc": "2026-01-24 19:02:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ipjve",
                  "author": "aeroumbria",
                  "text": "My personal theory is that every handover point (whether it is task delegation / return or context compaction) is a potential point of failure, and whichever method that can minimise handover failure risks will work better. This kind of aligns with my observation that most orchestrator workflows seem to offer no benefit over simple plan -> build, unless the orchestrator is passing continuously  curated plans instead of ad-hoc task prompts to the subagent.",
                  "score": 1,
                  "created_utc": "2026-01-24 23:28:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1idpws",
                  "author": "kshnkvn",
                  "text": ">Every tool call counts because it is resending the full conversation history.\n\nNope. Only final output. Moreover, you can write a subagent so that it does not respond at all, just performs some action. And that's it.\n\n>I've seen models cross checking sub agents results\n\nSounds like a prompt issue, nothing more. I occasionally encounter this problem, but it can be solved.",
                  "score": 1,
                  "created_utc": "2026-01-24 22:27:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1g968z",
          "author": "nicklazimbana",
          "text": "which one was the most powerfull",
          "score": 3,
          "created_utc": "2026-01-24 16:41:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h0zth",
              "author": "tisDDM",
              "text": "Well - depends on what you need. This benchmark doesn't tell which one performs best. It tells which one is the most efficient. But it might help you pick the right tool for your task.\n\nFrom that point of view I would go with Codex if it delivers a fine solution for the task. If you're running on API and need something efficient, Gemini Flash with DCP is great. If money and quota don't matter, Opus with DCP is the expensive swiss army knife.",
              "score": 2,
              "created_utc": "2026-01-24 18:42:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1hln08",
                  "author": "Michaeli_Starky",
                  "text": "Gemini Flash and Pro are unusable. They go crazy very fast on real world tasks with massive context looping some dumb ass tokens\n\nhttps://preview.redd.it/rhpnytpuucfg1.png?width=410&format=png&auto=webp&s=11109819d1796bb8e842b5fb38302b094e22f93c",
                  "score": 1,
                  "created_utc": "2026-01-24 20:14:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1gi728",
          "author": "Groundbreaking-Mud79",
          "text": "I never really see any usefulness in a tool like Oh-my-opencode. I don‚Äôt know why, but to me it just seems like it consumes a lot more tokens for very little improvement.",
          "score": 4,
          "created_utc": "2026-01-24 17:21:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hm8iq",
              "author": "Michaeli_Starky",
              "text": "It's as good as the task given. You need to be mindful when to use it over the normal Plan or even directly build modes.",
              "score": 1,
              "created_utc": "2026-01-24 20:17:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1kg6sj",
                  "author": "Groundbreaking-Mud79",
                  "text": "I'd love to hear your use cases! I'm also learning how to use these tools. Do you have any examples?",
                  "score": 1,
                  "created_utc": "2026-01-25 05:22:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1g7vr7",
          "author": "Big-Coyote-3622",
          "text": "Thanks, quantitative analysis approach is something I was looking for evaluating some of my modification to opencode/omo as well, especially as sometimes I use deepseek/glm/minimax/qwen models‚Ä¶ I think another good metric especially for calculating efficiency is token/api cost per full test run, if I find some time I will try to do a PR",
          "score": 2,
          "created_utc": "2026-01-24 16:35:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h2st2",
              "author": "tisDDM",
              "text": "I'd really appriciate that. Especially comparisions with all the other models outthere, a result db, testing UI Code and averaging over multiple runs are missing.",
              "score": 1,
              "created_utc": "2026-01-24 18:50:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1hmi61",
              "author": "Michaeli_Starky",
              "text": "The analysis is statistically incorrect considering the huge number of factors.",
              "score": 1,
              "created_utc": "2026-01-24 20:18:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gdojq",
          "author": "MissingHand",
          "text": "So based on this, what‚Äôs in #1,2, and 3 spot",
          "score": 2,
          "created_utc": "2026-01-24 17:01:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h1vxw",
              "author": "tisDDM",
              "text": "As said before: If it does the trick - codex is great and most efficient and Gemini Flash is cheap and got lots of quota over antigravity.",
              "score": 1,
              "created_utc": "2026-01-24 18:46:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1hn6du",
          "author": "Michaeli_Starky",
          "text": "Solving the task once is statistically incorrect even for deterministic computational environments. LLMs are non-deterministic by their nature. You would need 100+ runs per each category at VERY least for a proper study.",
          "score": 2,
          "created_utc": "2026-01-24 20:21:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1hjff7",
          "author": "MakesNotSense",
          "text": "I think we need more efforts to create systems for data collection and quantitative testing by users. Too many projects are vibing their way to a good idea, and falling short because testing and data collection remain a complex challenge. \n\n  \nAn automated agentic system that collects, analyzes, and reports on agent and model performance seems like it should be a top priority for OpenCode. \n\nWe already have most of the tools; session data, session read/search/info tools in OMO. With direct integration into OpenCode, and a workflow, you could tasks something like Grok fast or Gemini Flash to churn through a dataset to extract and consolidate information on actual workloads. \n\nImagine, you have a release, users produce data, return data to main dev, gets processed by a main dev agentic workflow, report produced, report used to generate spec, spec used to implement PR. The optimization of a project gets automated by real-world performance. Even the data collection system itself could be built for automated optimization.",
          "score": 1,
          "created_utc": "2026-01-24 20:04:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1iazsi",
          "author": "Codemonkeyzz",
          "text": "I used  Oh-my-opencode plugin before then later i dropped it.  Opencode's default  Plan and Build agents seems a lot more efficient  in terms of token and cost. I wonder  what exactly  oh-my-opencode plugin does well ? It's obviouslly not efficient with time and token cost, so  is  it about accuracy ? Does it have some prompts that produce more accurate output ?",
          "score": 1,
          "created_utc": "2026-01-24 22:14:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1n8hlf",
          "author": "rothnic",
          "text": "I at first thought oh my opencode was useful, but just keep running into what you are seeing. The most useful part of it is more in regard to some of the tooling that is added to help keep an agent running. Ralph loop, todo continuation, etc. I also noticed that it doesn't seem to spawn subagents very often and when it does it seems to not handle it that well.\n\nPersonally, I think there isn't a ton to get out of an orchestrating agent being the orchestrator because the orchestrator is inherently error prone and likely to suffer the same issues as using an agent regularly. Eventually as the context grows, the agent is less likely to do the things you tell are important. It just isn't ever going to happen consistently when the agent is managing everything. It starts great, but eventually will break down.\n\nI've been working with various orchestrators that leverage beads and deterministic state to execute workflows around centrally planned and broken down work and get much better results from this approach. I've also been working on my own state-machine based orchestration layer as well that is getting close to being usable. Basically, the idea is to have a state machine based orchestration layer that orchestrates the agents and when needed can leverage an agent to recover from bad states, etc.",
          "score": 1,
          "created_utc": "2026-01-25 16:45:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wwx89",
          "author": "Express-Peace-4002",
          "text": "That's my guy!",
          "score": 1,
          "created_utc": "2026-01-26 23:09:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zcaub",
          "author": "datosweb",
          "text": "Buen√≠simo el benchmark, se agradece el laburo de postear los datos crudos. Me qued√© pensando en la latencia de **Gemini Flash**. Es un ca√±o para tareas de bajo costo, pero a veces siento que en el razonamiento l√≥gico de *multistep* se queda un poco atr√°s comparado con Opus, sobre todo cuando el context window se empieza a llenar de basura.\n\nMe llama la atenci√≥n que los resultados de *tokens por segundo* var√≠en tanto en el test de c√≥digo complejo. ¬øUsaste algun tipo de **quantizaci√≥n** espec√≠fica para correr local o es todo via API pura? Pregunto porque a veces el cuello de botella no es el modelo en s√≠, sino c√≥mo el CLI maneja el streaming de los buffers de memoria cuando la rsta es muy larga.\n\nCapaz me equivoco, pero me da la sensasi√≥n de que en ciertos lenguajes menos comunes el Flash alucina un toque m√°s que el restoo. ¬øNotaste algun patron de errores sint√°cticos en Python vs Rust, o maso menos performan igual en tdos los casos?",
          "score": 1,
          "created_utc": "2026-01-27 08:13:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zfhzg",
              "author": "tisDDM",
              "text": "I did not perform so many tests - this benchmark I have written to test my subagent plugin. The comparison of the models I did out of pure curiosity. \n\nAll models are used from cloud offerings. \n\n\\- Flash from Gemini API\n\n\\- Codex from Azure\n\n\\- Opus from  Google Antigravity (this might have caused some delay)\n\nIt is fairly easy to use smaller models - even Devstral2Small - if the tasks are cut in smaller pieces. (\"If it fails, the task was to big\") This is what I deterministically did with my subagent plugin. \n\nFlash is great ( and so are Codex-Mini or Devstral2) if the task is small enough. Never expect it to get it like Opus. It is not made for reasoning about bigger complex tasks",
              "score": 1,
              "created_utc": "2026-01-27 08:42:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27mpaq",
          "author": "ArgenEgo",
          "text": "TOS",
          "score": 1,
          "created_utc": "2026-01-28 13:37:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq4vxu",
      "title": "Kimi K2.5, a Sonnet 4.5 alternative for a fraction of the cost",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qq4vxu/kimi_k25_a_sonnet_45_alternative_for_a_fraction/",
      "author": "Grand-Management657",
      "created_utc": "2026-01-29 10:19:28",
      "score": 60,
      "num_comments": 82,
      "upvote_ratio": 0.76,
      "text": "\n\nYes you read the title correctly. Kimi K2.5 is THAT good. \n\nI would place it around Sonnet 4.5 level quality. It‚Äôs great for agentic coding and uses structured to-do lists similar to other frontier models, so it‚Äôs able to work autonomously like Sonnet or Opus.\n\nIt's thinking is very methodical and highly logical, so its not the best at creative writing but the tradeoff is that it is very good for agentic use.\n\nThe move from K2 -> K2.5 brought multimodality, which means that you can drive it to self-verify changes. Prior to this, I used antigravity almost exclusively because of its ability to drive the browser agent to verify its changes. This is now a core agentic feature of K2.5. It can build the app, open it in a browser, take a screenshot to see if it rendered correctly, and then loop back to fix the UI based on what it \"saw\". Hookup playwright or vercel's browser-agent and you're good to go.\n\nNow like I said before, I would still classify Opus 4.5 as superior outside of JS or TS environments. If you are able to afford it you should continue using Opus, especially for complex applications.¬†\n\nBut for many workloads the best economical and capable pairing would be Opus as an orchestrator/planner + Kimi K2.5 as workers/subagents. This way you save a ton of money while getting 99% of the performance (depending on your workflow).  \n\n\n\\+ You don't have to be locked into a single provider for it to work.\n\n\\+ Screw closed source models.\n\n\\+ Spawn hundreds of parallel agents like you've always wanted WITHOUT despawning your bank account.\n\n\n\n*Btw this is coming from someone who very much disliked GLM 4.7 and thought it was benchmaxxed to the moon*\n\n\n\n# Get Started\n\n  \nThere are plenty of providers for open source models and only one for claude (duh)\n\n[Nano-GPT](https://nano-gpt.com/invite/mNibVUUH)\n\nA provider aggregator. Essentially routing all of your requests to a provider in their network. This is by far the most cost effective way to drive opencode, claude code, vscode (insiders), or any other harness. For the cost of a one extremely large cup of coffee, $8/month, you get 60,000 requests/month. That is $0.00013 per request regardless of input or output size. To put that into perspective, Sonnet 4.5 would cost you $0.45 for a request of 100k in/1k out (small-medium codebase) and not taking caching into account. Sonnet is 3,461x more expensive.\n\nAlso you can use Opus 4.5 through nano-gpt at API rates like I do to drive the orchestrator and then my subscription covers K2.5 subagents.\n\nCheap AF, solid community, founders are very active and helpful  \n  \nMy referral for 5% off web: [https://nano-gpt.com/invite/mNibVUUH](https://nano-gpt.com/invite/mNibVUUH)\n\n  \n[**Synethetic.new**](https://synthetic.new/?referral=KBL40ujZu2S9O0G)\n\nThis is what I would recommend for anyone needing maximum security and lightning fast inference. It costs a premium of $20/month ($10 with my referral), but compared to claude pro plan's usage limit, its a bargain. 135 requests/5hrs with tool calls only counting as 0.1 requests. This is the best plan for professionals and you can hook it up with practically any tool like claude code and opencode. Within a 10 hour period, you can use up to 270 requests which comes out to $0.002. Sonnet 4.5 is 225x more expensive.\n\nCheap, fast speed, $60/month plan gets you 1,350 requests/5hr, data not trained on \n\nMy referral for $10 or $20 off: [https://synthetic.new/?referral=KBL40ujZu2S9O0G](https://synthetic.new/?referral=KBL40ujZu2S9O0G)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qq4vxu/kimi_k25_a_sonnet_45_alternative_for_a_fraction/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2dz8i6",
          "author": "Hozukr",
          "text": "Marketing hype is really strong with this one. Running away as fast as possible.",
          "score": 25,
          "created_utc": "2026-01-29 10:34:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dzeg0",
              "author": "Repulsive_Educator61",
              "text": "I tried it and it's somewhere between sonnet 4.5 and opus 4.5\n\ndefinitely not benchmaxxed",
              "score": 20,
              "created_utc": "2026-01-29 10:35:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2f3p92",
                  "author": "kr_roach",
                  "text": "Is it fast? Im using GLM 4.7 but its so slow",
                  "score": 2,
                  "created_utc": "2026-01-29 14:51:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2dzqfu",
                  "author": "Grand-Management657",
                  "text": "That is similar to my findings but I'm not sure if its really better or on par with Sonnet 4.5  \nI think it may just come down to preference or its performance in the harness you use. I was very surprised it wasn't benchmaxxed unlike GLM 4.7. I tried to love that model but nope.",
                  "score": 1,
                  "created_utc": "2026-01-29 10:38:42",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ef7mp",
              "author": "annakhouri2150",
              "text": "Nah, K2.5 is actually this good imo",
              "score": 3,
              "created_utc": "2026-01-29 12:36:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2fb5se",
              "author": "randvoo12",
              "text": "No hype tbh, quality of work produced is comparable to Opus not even Sonnet.",
              "score": 3,
              "created_utc": "2026-01-29 15:26:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2g8bdh",
              "author": "RegrettableBiscuit",
              "text": "I've started running it in opencode today, and it's a strong model. I would put it above GLM4.7 and at least in the same general ballpark as Sonnet 4.5.\n\n\nThere's lots of hype, and I find it increasingly difficult to tell real grassroots support from manufactured hype (like why TF is synthetic mentioned in every post, is that real or just BS), but it is a genuinely good model.¬†",
              "score": 3,
              "created_utc": "2026-01-29 17:54:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mlw54",
                  "author": "Grand-Management657",
                  "text": "I'm not familiar with the hype around synthetic. Its just one of the only providers that I found that is subscription based, has privacy, gets around \\~60tok/s on K2.5, and pretty cheap compared to Sonnet 4.5. There's not really any other options and if you know any, feel free to link it for everyone.",
                  "score": 1,
                  "created_utc": "2026-01-30 16:26:39",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2l5xdy",
              "author": "zarrasvand",
              "text": "Yeah, and it is distilling Claude under the hood anyway so not a lot of new things to see here...",
              "score": 1,
              "created_utc": "2026-01-30 11:52:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2dze6l",
              "author": "Grand-Management657",
              "text": "Which part is hype? Please elaborate.",
              "score": 1,
              "created_utc": "2026-01-29 10:35:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2dztzr",
                  "author": "Repulsive_Educator61",
                  "text": "I would say only [synthetic.new](http://synthetic.new) part is hype, not kimi k2.5\n\nseeing lots of posts here about [synthetic.new](http://synthetic.new), could be their marketing team, i can't confirm",
                  "score": 5,
                  "created_utc": "2026-01-29 10:39:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ewxfg",
          "author": "rokicool",
          "text": "Yesterday I tried their 'native' subscription (via kimi.com) - Moderato ($20 per month).\n\nI spent 5 hours allowance within 30 min. This tier of subscription seems useless.\n\nThe next tier is $40... I will be working for 1 hour and 4 hours cooldown. Useless as well.\n\nSo, the only tier that gives access (for one thread of work!) is $200. And... Why spending the same amount for something that barely imitates the original (Anthropic) when the original costs the same?\n\nI don't understand why people call it 'cheap'. It is on par with Anthropic's subscriptions.\n\nhttps://preview.redd.it/vri1l76sqagg1.png?width=2582&format=png&auto=webp&s=09f5ad25b14b14f9d6b96c11b1ccc81957ea66a7\n\n  \nUPD: There were some changes to the Console interface and I looks different and shows different metrics. And IF they are relevant, I have a lot of allowance with my $20 subscription. \n\nSorry for jumping to conclusions.",
          "score": 11,
          "created_utc": "2026-01-29 14:17:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2exv9x",
              "author": "Grand-Management657",
              "text": "Its more expensive through the moonshot subscription compared to the ones I linked in the post. From what I remember, \"Moderato\" allows 2048 requests per week. Nano-gpt allows 15,000 requests per week. Also nano is $8 instead of $20. If you get two nano subs for $16, you will get almost \\~15x the usage of \"moderato\" for less.\n\nMy referral to nano if you want to give it a try: [https://nano-gpt.com/invite/mNibVUUH](https://nano-gpt.com/invite/mNibVUUH)",
              "score": 5,
              "created_utc": "2026-01-29 14:21:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2fx86f",
                  "author": "rokicool",
                  "text": "Thank your for your research.\n\nUnfortunately, I remember complains about sluggishness of nano-gpt and wanted to test 'original' provider. And despite the really impressive outcome of the Kimi2.5 model I find the Kimi Subscriptions useless.\n\nUPD: Since there are some changes to the Console interface and it looks much more logical and promising now... I should admit that my previous assumption 'everything is useless' might be wrong. Time will show!",
                  "score": 1,
                  "created_utc": "2026-01-29 17:04:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2hoe1q",
                  "author": "Western_Objective209",
                  "text": "is nano-gpt legit? seems like it automatically creates an anonymous account, even takes XMR for payments",
                  "score": 1,
                  "created_utc": "2026-01-29 21:58:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2fw5h2",
              "author": "rokicool",
              "text": "It is getting ridiculous. I managed to spend week allowance of $20 subscription within 1-1.5 hour(s) of OpenCode development.\n\nhttps://preview.redd.it/h1fip0d8kbgg1.png?width=2290&format=png&auto=webp&s=9936622b91898f936ee0670068a9c67a40a9e6c1\n\nAre you sure you would call something like $20 an hour as 'cheap'?\n\nUPD: \n\nIt seems to me that they were changing the interface while I was bitching. Now, after several hours it look  1% and 11%. \n\nSo, I might got it wrong. And it might be cheap.",
              "score": 2,
              "created_utc": "2026-01-29 16:59:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mjejg",
                  "author": "Grand-Management657",
                  "text": "That's where you're messing up, use synthetic as your provider and you will get more limits. Kimi was limited to 2048 requests/week last I checked. Synthetic is 135/5hrs or 1350/5hr on the pro plan.\n\n[https://synthetic.new/?referral=KBL40ujZu2S9O0G](https://synthetic.new/?referral=KBL40ujZu2S9O0G)",
                  "score": 2,
                  "created_utc": "2026-01-30 16:15:31",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2gjutd",
              "author": "chiroro_jr",
              "text": "What do y'all be doing really?",
              "score": 1,
              "created_utc": "2026-01-29 18:46:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2he0z2",
              "author": "chvmnaveen",
              "text": "I agree with you same behavior for me to on $20 plan. I consumed all the weekly limit in just one night üòí",
              "score": 1,
              "created_utc": "2026-01-29 21:09:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mjlqc",
                  "author": "Grand-Management657",
                  "text": "That's where you're messing up, use synthetic as your provider and you will get more limits. Kimi was limited to 2048 requests/week last I checked. Synthetic is 135/5hrs or 1350/5hr on the pro plan.\n\n[https://synthetic.new/?referral=KBL40ujZu2S9O0G](https://synthetic.new/?referral=KBL40ujZu2S9O0G)",
                  "score": 1,
                  "created_utc": "2026-01-30 16:16:24",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2hel4l",
              "author": "I_HEART_NALGONAS",
              "text": "That's still better than Sonnet 4.5 where a couple of times I blew through Anthropic's ridiculous 5-hour quota in two (2) prompts on the Pro plan.",
              "score": 1,
              "created_utc": "2026-01-29 21:12:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2j79oq",
              "author": "GTHell",
              "text": "Same experience. Why spend $20 just to use something that replicates the OG. It barely any improvement over GLM 4.7 and GLM $40 get you 3 months and the speed is very good.",
              "score": 1,
              "created_utc": "2026-01-30 02:51:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2kiwl3",
              "author": "_Belgarath",
              "text": "It's cheap regarding the API cost. It's about 10x cheaper than Claude when using a per token billing system, not using the subscription.",
              "score": 1,
              "created_utc": "2026-01-30 08:32:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2fpd52",
          "author": "Muted_Standard175",
          "text": "Have anyone tried to use opus 4.5 or gpt 5.2 as plan and k2.5 as build? How good it was?",
          "score": 6,
          "created_utc": "2026-01-29 16:29:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2js2xn",
              "author": "degenbrain",
              "text": "In my case, I did it the other way around. K2.5 tends to provide simple solutions and plans. There are no additional features. It's straightforward. Then, I ask Opus to execute it perfectly",
              "score": 1,
              "created_utc": "2026-01-30 04:57:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2efqyi",
          "author": "HotFats",
          "text": "I think k2.5 is definitely better than sonnet might be performing as close to opus. Its not only cheaper, but its way faster. Alsovi use synthetic.new, its pretty good. I think K2.5 with thinking is the closet we've gotten to giving anthropic models a run for their money. Currently its handling browser automation and building scripts and n8n workflows just as well if not better than opus  4.5. Not canceling my claude max subscription yet, but its promising.",
          "score": 3,
          "created_utc": "2026-01-29 12:40:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2egq4o",
              "author": "Grand-Management657",
              "text": "I would wait for two more weeks to cancel that sub. I think deepseek v4 might be even better and potentially releasing before the chinese new lunar year. And that gives you enough time to really put K2.5 to the test.",
              "score": 3,
              "created_utc": "2026-01-29 12:46:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ee6jl",
          "author": "MegamillionsJackpot",
          "text": "https://preview.redd.it/wlhxth6c8agg1.jpeg?width=1080&format=pjpg&auto=webp&s=eb03955b77875aaee196565e06e862affd807354\n\nExpensive if you are not on a plan?",
          "score": 2,
          "created_utc": "2026-01-29 12:29:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eer53",
              "author": "Grand-Management657",
              "text": "Seems like you are looking at agent swarm which I do not know too much of. I do know that it spins up hundreds of K2.5's, so its going to cost significantly more. Using the model without swarm is $0.50 in/$3.00 out with API rates. With nano or synthetic as providers, your cost is significantly lower than API rates.",
              "score": 2,
              "created_utc": "2026-01-29 12:33:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2efty9",
                  "author": "MegamillionsJackpot",
                  "text": "Yeah, I know. It's just a funny bug in the pricing. And that bug was there before I wrote the agent swarm thing.\n\n\nDo you know if synthetic models work okay for multi step deep research?",
                  "score": 1,
                  "created_utc": "2026-01-29 12:40:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2eubz0",
          "author": "Salty-Standard-104",
          "text": "PR slop. why kimi would hire such terrible person for writing crap like this?",
          "score": 2,
          "created_utc": "2026-01-29 14:03:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fw2gb",
              "author": "seaal",
              "text": "kimi? this chud and all the others are just spamming their referral links are just trying to get their credits for nanogpt and synthetic.new.",
              "score": 2,
              "created_utc": "2026-01-29 16:59:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2k90zr",
          "author": "Lower_Temperature709",
          "text": "I have been working with minimax + glm + codex + code. All bare minimum plan. Coding non stop from last week. It‚Äôs crazy efficient and dirt cheap. \n\nUsing oh my open code as the agent harness with alots of agent and sub agent configured.",
          "score": 2,
          "created_utc": "2026-01-30 07:05:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dzu61",
          "author": "N2siyast",
          "text": "No way Im using this vibe coded slop site",
          "score": 3,
          "created_utc": "2026-01-29 10:39:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e0pub",
              "author": "Grand-Management657",
              "text": "Haha I agree. I was just browsing earlier and saw the home page and it is ugly",
              "score": 0,
              "created_utc": "2026-01-29 10:47:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2e0bup",
          "author": "BitterAd6419",
          "text": "Kimi is better than GLM but not as good as anthropic models.",
          "score": 2,
          "created_utc": "2026-01-29 10:43:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e5cpp",
              "author": "awfulalexey",
              "text": "GLM has approximately 350 billion parameters, Kimi has 1 trillion parameters. It's interesting why Kimi is stronger than GLM.",
              "score": 3,
              "created_utc": "2026-01-29 11:25:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ec0si",
                  "author": "Grand-Management657",
                  "text": "Not sure where I read it but K2.5 is built on K2 but with an additional training of 15 trillion mixed visual and text tokens. Not sure about GLM 4.7 but I would suspect its nowhere close to that.",
                  "score": 2,
                  "created_utc": "2026-01-29 12:15:05",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2e1086",
              "author": "Grand-Management657",
              "text": "This is a reasonable take. Is your use case mostly web? I haven't gotten a chance to test it on anything other than web development.",
              "score": 1,
              "created_utc": "2026-01-29 10:49:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2e9a5r",
                  "author": "BitterAd6419",
                  "text": "Yea only tested on few web related task nothing too complicated",
                  "score": 1,
                  "created_utc": "2026-01-29 11:55:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2e07y8",
          "author": "joakim_ogren",
          "text": "Does Synthetic.new support Kimi K2.5? (It seems supported by vLLM)",
          "score": 1,
          "created_utc": "2026-01-29 10:42:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e0u9c",
              "author": "Grand-Management657",
              "text": "They do support it but since its a new model, they haven't updated the page I'm guessing. Go to [https://synthetic.new/pricing](https://synthetic.new/pricing) and you will see it in the list.",
              "score": 4,
              "created_utc": "2026-01-29 10:48:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2e8z2y",
          "author": "seeKAYx",
          "text": "$10 discount / month with that referral or only first month?",
          "score": 1,
          "created_utc": "2026-01-29 11:53:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ea8bm",
              "author": "Grand-Management657",
              "text": "I'm pretty sure its only the first month. I wish it was recurring!\n\nhttps://preview.redd.it/l9hg8i5e3agg1.png?width=460&format=png&auto=webp&s=037b29a3023c9f2205bf51c2f67e020038f70145",
              "score": 1,
              "created_utc": "2026-01-29 12:02:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2f7hyt",
          "author": "Galendel",
          "text": "I am using deepseek v3.2 with and without thinking, I really like it for the cost, did anyones else use deepseek ?",
          "score": 1,
          "created_utc": "2026-01-29 15:09:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f8efy",
              "author": "Grand-Management657",
              "text": "I really like deepseek v3.2 for creative writing. I think it would be great for its intelligence and writing style even in agentic coding. But it just wasn't tailored towards software development like claude models, Kimi K2.5, or GLM 4.7\n\nFor the cost though, its hard to beat. Almost costs nothing to run. I have very high hopes for deepseek v4 and I think that will be on par with Opus 4.5, or at least I hope. Fingers crossed!",
              "score": 2,
              "created_utc": "2026-01-29 15:13:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2h8kmn",
                  "author": "Galendel",
                  "text": "I am spending like 3-4$ a day on it, the code he does is fine to me, it's just too slow and way more  with thinking, on aider benchmark [https://aider.chat/docs/leaderboards/](https://aider.chat/docs/leaderboards/) Kimi K2 is really low compare to deepseek. I tried GLM 4.7 free on zen ai and it was really bad for agentic coding, maybe they are overloaded. The ratio quality / cost doesn't seem to be a subject, but to me if a good LLM is 10x cheaper it can do 9x more coding with same budget. It's been a while I didn't use subscription so I can't compare yet.",
                  "score": 2,
                  "created_utc": "2026-01-29 20:43:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gtr37",
          "author": "SunflowerOS",
          "text": "Can I use my suscription on opencode like Anthropic or I need to pay the api?",
          "score": 1,
          "created_utc": "2026-01-29 19:31:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gybi9",
              "author": "Grand-Management657",
              "text": "Yes you can use any subscription with opencode but I don't recommend using claude subscription on opencode. They will ban you.\n\nThe two I recommend is\n\nNano-gpt: https://nano-gpt.com/invite/mNibVUUH\n\nor\n\nSynthetic: https://synthetic.new/?referral=KBL40ujZu2S9O0G",
              "score": 1,
              "created_utc": "2026-01-29 19:53:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2gzf7c",
                  "author": "SunflowerOS",
                  "text": "I know it, but I suscribe to kimi on december thinking that i could use it on opencode",
                  "score": 1,
                  "created_utc": "2026-01-29 19:58:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gxlll",
          "author": "VaizardX",
          "text": "How did you setup the orchestrator and agents?",
          "score": 1,
          "created_utc": "2026-01-29 19:50:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gzjxn",
              "author": "Grand-Management657",
              "text": "In OpenCode, you can set a specific model for a subagent by configuring the model property in the subagent's definition within the opencode.json or opencode.jsonc configuration file.\n\nYou can find more information here: https://opencode.ai/docs/agents",
              "score": 1,
              "created_utc": "2026-01-29 19:59:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2h8u8u",
              "author": "Galendel",
              "text": "have a look at bmad, they do have an orchestrator",
              "score": 1,
              "created_utc": "2026-01-29 20:44:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2l5v22",
          "author": "zarrasvand",
          "text": "https://preview.redd.it/c5h94oni6hgg1.png?width=1266&format=png&auto=webp&s=bd25da84a71a56eaa05ea6dfeb42dbd2196d3407\n\n\"Ok bro\"",
          "score": 1,
          "created_utc": "2026-01-30 11:52:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mpsi4",
          "author": "Grand-Management657",
          "text": "For those of you wondering about speeds\n\nI am currently getting \\~18tok/s with nano-gpt and \\~60tok/s with synthetic.\n\nI recommend synthetic for any enterprise workloads or anything you will make money from. Its super fast, privacy centered and much cheaper than Sonnet 4.5. It also gives you the stability that is required for enterprise workloads. Combine it with your favorite frontier model (Opus 4.5/GPT 5.2) for best performance.\n\nNano-gpt is much slower but much more economical. Recommending this for side projects and hobbyists. I find this to be a great option if you need to spin up many subagents at once. Currently there are some multi-turn tool call issues which the devs are working on actively to rectify. Combine with your favorite frontier model to get best results (Opus 4.5/GPT 5.2)\n\nNano: [https://nano-gpt.com/invite/mNibVUUH](https://nano-gpt.com/invite/mNibVUUH)\n\nSynthetic: [https://synthetic.new/?referral=KBL40ujZu2S9O0G](https://synthetic.new/?referral=KBL40ujZu2S9O0G)",
          "score": 1,
          "created_utc": "2026-01-30 16:43:53",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2ok1rf",
          "author": "Purple_Wear_5397",
          "text": "Thanks for the feedback, one question: which provider do you use to consume K2.5 with high parallelism ?",
          "score": 1,
          "created_utc": "2026-01-30 21:45:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2oxmrp",
              "author": "Grand-Management657",
              "text": "I've been using synthetic because when deploying parallel agents, speed matters. How much that matters is really up to you. The wait for more complex reasoning and longer chain of thoughts becomes exponentially higher if you use a more economical provider. But for any sort of production or enterprise work, you want to be running the fastest you can. Synthetic runs at ~100tok/s and nano at ~18tok/s.\n\nTry Synthetic at a discount: https://synthetic.new/?referral=KBL40ujZu2S9O0G",
              "score": 1,
              "created_utc": "2026-01-30 22:52:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ow4o5",
          "author": "pbalIII",
          "text": "Ran K2.5 for a week on a mixed TypeScript/Python codebase. Few observations from the trenches:\n\n- Frontend gen is where it shines. Visual debugging loop (screenshot, fix, verify) genuinely works and saved hours on CSS issues.\n- Backend refactors hit the SWE-bench gap. That 76.8% vs Claude's 80.9% shows up when you're touching multiple files with shared state.\n- Speed matters for iteration. 34 tok/s vs Sonnet's 91 means longer feedback loops. If you're doing tight edit-test cycles, that adds up.\n\nThe Opus-orchestrator + K2.5-workers pattern you mentioned is probably the right call. Route expensive reasoning to Claude, parallelize the grunt work with K2.5. CLI tooling is still rough though... no cost tracking and context fills fast.",
          "score": 1,
          "created_utc": "2026-01-30 22:44:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2oz7og",
              "author": "Grand-Management657",
              "text": "Great insights. Was that 80.9% SWE bench for opus 4.5 or sonnet 4.5? If that's opus, then I'm surprised K2.5 got that close to it. Also, I 100% agree on the speed part, especially when using agents in parallel. You don't want your parallelization to get sluggish due to slow inference, especially if your complexity requires more interleaved thinking. \n\nI did find synthetic as a provider to have the speed required to keep up with Opus 4.5 as an orchestrator at ~100tok/s. They are using firework ai's infrastructure, which is known for its stability and speed. I honestly have no clue how they managed those kind of speeds, but I'll take it!",
              "score": 1,
              "created_utc": "2026-01-30 23:01:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2grb7t",
          "author": "alexeiz",
          "text": "You're here just to push your referrals.  That's it.",
          "score": 1,
          "created_utc": "2026-01-29 19:20:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dzhon",
          "author": "pokemonplayer2001",
          "text": "Always be shilling.\n\nHaha, you mad.",
          "score": 1,
          "created_utc": "2026-01-29 10:36:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2gykks",
          "author": "mustafamohsen",
          "text": "Kimi, you still post slop?",
          "score": -1,
          "created_utc": "2026-01-29 19:54:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqrbry",
      "title": "Tested free Kimi K2.5 in opencode: good stuff",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qqrbry/tested_free_kimi_k25_in_opencode_good_stuff/",
      "author": "ReporterCalm6238",
      "created_utc": "2026-01-30 01:17:03",
      "score": 57,
      "num_comments": 10,
      "upvote_ratio": 0.94,
      "text": "It's fast, it's smart BUT sometimes it makes mistakes with tool calling. I would put it above glm 4.7 and minimax M2.1. \n\n  \nWe are getting close boys. Open source Opus is not too far. There are some extremely smart people in China working around the clock to crush Anthropic, that's for sure.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qqrbry/tested_free_kimi_k25_in_opencode_good_stuff/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2ixorf",
          "author": "AlternativeAir7087",
          "text": "Open source will definitely prevail!",
          "score": 7,
          "created_utc": "2026-01-30 01:58:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2iy36w",
              "author": "ReporterCalm6238",
              "text": "I surely hope so. It's in the interest of the whole humanity that open source AI resists and thrive",
              "score": 5,
              "created_utc": "2026-01-30 02:01:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2l0p37",
          "author": "Villain_99",
          "text": "Using it via Kimi api, really good\nEven as a general purpose Chatbot it‚Äôs really good answers like sonnet/opus",
          "score": 3,
          "created_utc": "2026-01-30 11:11:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lrkng",
          "author": "0xfe",
          "text": "It's really excellent -- it finished some tasks that GLM just spun forever on. I currently use codex as my daily driver and claude for smaller things, but Kimi really feels like the first open model that truly competes.",
          "score": 3,
          "created_utc": "2026-01-30 14:03:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kh6y9",
          "author": "krimpenrik",
          "text": "How/what provider are you using it with?",
          "score": 2,
          "created_utc": "2026-01-30 08:16:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2k00l8",
          "author": "kr_roach",
          "text": "I tested it. But I'm wondering if Opencode Zen Kimi is using Moonshot AI directly or serving it through their server.",
          "score": 3,
          "created_utc": "2026-01-30 05:54:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2k1lvg",
              "author": "Useful-Mistake-4221",
              "text": "It's through Fireworks",
              "score": 6,
              "created_utc": "2026-01-30 06:06:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2k0u7e",
              "author": "Recent-Success-1520",
              "text": "Most probably Moonshot directly",
              "score": -1,
              "created_utc": "2026-01-30 06:00:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2kx6b6",
                  "author": "this-is-hilarours",
                  "text": "Not true. Served through fireworks  in us. Opencode confirmed this via x",
                  "score": 4,
                  "created_utc": "2026-01-30 10:41:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2nb0ok",
          "author": "jmhunter",
          "text": "i like how agressively it calls tools.. like right away its like I'm not spending all of my context right here",
          "score": 1,
          "created_utc": "2026-01-30 18:18:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qocwc2",
      "title": "Kimi k2.5",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/hv2amvpk1wfg1.jpeg",
      "author": "ReasonableReindeer24",
      "created_utc": "2026-01-27 12:47:14",
      "score": 56,
      "num_comments": 20,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qocwc2/kimi_k25/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o233nol",
          "author": "Hoak-em",
          "text": "Submitted PR and got it merged to [models.dev](http://models.dev) for the kimi-for-coding provider -- it's a fantastic model. I was initially a bit skeptical of it being benchmaxxed given how well it performed on benchmarks, but it is genuinely an amazing orchestrator -- likely the best orchestrator I've ever used, plus it's and opus-level planner with openspec. It's really, really good at direction-following as well, and seems to be token-efficient like opus. So yeah, it is what the benchmarks said.",
          "score": 7,
          "created_utc": "2026-01-27 20:40:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25ib57",
              "author": "arshadbarves",
              "text": "Merged",
              "score": 1,
              "created_utc": "2026-01-28 03:55:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o212xzz",
          "author": "shaonline",
          "text": "I've added it manually via opencode.json and it \"works\" but I get hit with a \"Provider returned error\" after the first tool calls (basic greps...), so... not really usable for now ?",
          "score": 4,
          "created_utc": "2026-01-27 15:23:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21dgtr",
              "author": "hotairplay",
              "text": "Yeah I'm having the same error using Droid. Looks like the servers got overloaded.",
              "score": 2,
              "created_utc": "2026-01-27 16:10:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o248efh",
          "author": "Mattdeftromor",
          "text": "I got the Kimi Code Plan $40 and... It's fantastic !",
          "score": 2,
          "created_utc": "2026-01-27 23:52:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24ncl5",
              "author": "aeroumbria",
              "text": "It seems to burn out much faster though, despite being 500/5hr requests vs GLM's supposedly 600/5hr requests. It just seems GLM can't ever run out even if you try. I observed that Kimi counts every single interaction, like calling read tool as a request, whereas GLM does not seem to count most contiguous agent actions as additional requests.",
              "score": 3,
              "created_utc": "2026-01-28 01:08:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2668ty",
                  "author": "shaonline",
                  "text": "Yeah I have GLM lite coding plan and even if I let it hammer away at a task for a long while I can't ever seem to make the quota run out, even past 30% lmao. That being said it hardly ever lets you run parallel agents (at least on a single model) so there's that.",
                  "score": 2,
                  "created_utc": "2026-01-28 06:40:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27cyni",
              "author": "Phukovsky",
              "text": "How is it used? Like, run 'kimi code' in terminal and then use it like you'd use Claude Code? \n\nAny advantage to this vs using it through OpenCode?",
              "score": 1,
              "created_utc": "2026-01-28 12:39:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27y2bm",
                  "author": "Mattdeftromor",
                  "text": "I use it with OpenCode ! its kimi-cli is horrible",
                  "score": 1,
                  "created_utc": "2026-01-28 14:35:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2d1w21",
              "author": "alexeiz",
              "text": "Kimi plans are too expensive (like, would you pay $20 for Kimi or for GPT/Claude?).  And other Chinese companies like Z.ai and Minimax heavily undercut Kimi.  You really have to be a Kimi fan to pay for Kimi.",
              "score": 1,
              "created_utc": "2026-01-29 05:41:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2bh00z",
          "author": "BitterAd6419",
          "text": "It‚Äôs very slow or often errors out, guess the capacity is maxed out, even their site sometimes doesn‚Äôt work properly or returns errors",
          "score": 2,
          "created_utc": "2026-01-29 00:08:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bnbvm",
              "author": "ReasonableReindeer24",
              "text": "It's mid tier model , you cannot expect more like opus or gpt 5.2 codex",
              "score": 1,
              "created_utc": "2026-01-29 00:41:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o20strg",
          "author": "Impossible_Comment49",
          "text": "Some results are available here, but only K2 is on the leaderboard. However, I would be cautious because this leaderboard can be heavily influenced by bots.\n\n[https://lmarena.ai/leaderboard/code](https://lmarena.ai/leaderboard/code)",
          "score": 1,
          "created_utc": "2026-01-27 14:34:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o215wqe",
              "author": "ReasonableReindeer24",
              "text": "wait for update kimi k2.5 on opencode cli",
              "score": 3,
              "created_utc": "2026-01-27 15:37:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o24aaeo",
                  "author": "Mattdeftromor",
                  "text": "Already there for me too",
                  "score": 2,
                  "created_utc": "2026-01-28 00:02:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o23nafw",
                  "author": "planetearth80",
                  "text": "It‚Äôs already there for me",
                  "score": 1,
                  "created_utc": "2026-01-27 22:08:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qq8sgu",
      "title": "Be careful when using Claude Code with OpenCode",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/zqcyddtrjagg1.png",
      "author": "HzRyan",
      "created_utc": "2026-01-29 13:38:42",
      "score": 55,
      "num_comments": 35,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qq8sgu/be_careful_when_using_claude_code_with_opencode/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2eqcy2",
          "author": "HeavyDluxe",
          "text": "This is old news.  And Claude's TOS specifically speak to this use as prohibited.  (I know, I know. Who reads that stuff?)\n\nIf you want to use Opencode as your CLI environment with Claude, YOU CAN DO THAT.  You just need to buy credits for the API and create a key.  Plug it into Opencode and you're off to the races.",
          "score": 14,
          "created_utc": "2026-01-29 13:42:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2exu2z",
              "author": "OofOofOof_1867",
              "text": "My decided workaround has been to get a GitHub CoPilot Pro+ license, and then pay as I go afterwards. They pay by \"premium request\" - so it does change the way I work because I make sure every request is FAT to get most bang for my buck. But so far it's close to the same price as Claude Max5.",
              "score": 6,
              "created_utc": "2026-01-29 14:21:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2fupbn",
                  "author": "jrop2",
                  "text": "\\>¬†But so far it's close to the same price as Claude Max5.\n\nEesh. I was wondering about switching to GH CoPilot from the Claude plans. However, I was shooting for the $40/month plan, and your experience makes it sound like it still evens out to lots of money either way....",
                  "score": 1,
                  "created_utc": "2026-01-29 16:53:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2etls8",
              "author": "Sensitive_Song4219",
              "text": "API is very expensive tho. I know it's old news, but it'd be better news if Anthropic would follow OpenAI's lead and allow subscription use in OpenCode: OC still needs some polish but after two weeks or so of using it, it's really kinda incredible. (Heck, I just used *Microsoft's official OAUTH to sign into my Copilot subscription via OpenCode*!). \n\n  \nIf OpenAI and freakin' *Microsoft* (of all companies!) can see the benefit of giving their paid subscribers access (because let's face it: OpenCode isn't more token hungry than Claude Code), then why can't Anthropic follow suit?\n\nFor the first time in a year, I've gone a full week without looking at Claude Code - I'm OpenCode exclusive now, chopping-and-changing providers, models, and reasoning levels mid-conversation. It's liberating. And maybe that's the problem.",
              "score": 2,
              "created_utc": "2026-01-29 13:59:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ew0le",
                  "author": "acmethunder",
                  "text": "> If OpenAI and freakin' Microsoft (of all companies!) can see the benefit of giving their paid subscribers access (because let's face it: OpenCode isn't more token hungry than Claude Code), then why can't Anthropic follow suit?\n\nAnthropic can, they are choosing not to. It is a form of vendor lock in hoping to keep those monthly/yearly revenue streams.",
                  "score": 5,
                  "created_utc": "2026-01-29 14:12:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ko7bk",
                  "author": "Keep-Darwin-Going",
                  "text": "It is called paying for mind share. People remember Claude code more than anthropic, if you look at a lot of comparison article and video people say Claude code not opus or sonnet or anthropic. Which is why you can use cc for everything else but not use max x20 for opencode.  All this loses they have to attribute to something to answer to shareholders. OpenAI allow it because their plan is already more expensive although they are more efficient? Opus already capture majority of the enterprise and non enterprise coding users. It is like if you talk about chat AI it is always chatgpt, coding it is always opus at least for now.",
                  "score": 1,
                  "created_utc": "2026-01-30 09:20:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2er00f",
              "author": "HzRyan",
              "text": "That's on me, haha. I'm still not going to read any T&Cs though üåù",
              "score": 1,
              "created_utc": "2026-01-29 13:45:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2erzr5",
                  "author": "HeavyDluxe",
                  "text": "Can't blame you. :)",
                  "score": 1,
                  "created_utc": "2026-01-29 13:51:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ew7xg",
          "author": "aeroumbria",
          "text": "They are pretty much alone in the fight against third party UIs at this point, though. Almost all their competition are either actively supporting or acquiescing.",
          "score": 7,
          "created_utc": "2026-01-29 14:13:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gdffw",
              "author": "flexrc",
              "text": "Google antigravity just started blocking opencode usage as well.",
              "score": 5,
              "created_utc": "2026-01-29 18:17:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ez4id",
              "author": "Sensitive_Song4219",
              "text": "Yup. This can't be what developers want though: it's like saying \"You can use Visual Studio Professional but VSCode is prohibited or you get booted.\" No experienced dev would accept that.",
              "score": 3,
              "created_utc": "2026-01-29 14:28:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ezzqq",
                  "author": "Ok_Road_8710",
                  "text": "Of course not, they can try (Anthropic), their heads are too far up their own asses to make any progress I guess",
                  "score": 1,
                  "created_utc": "2026-01-29 14:32:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ezsc4",
          "author": "kiwibonga",
          "text": "Hey so... As soon as you open opencode you get banned and refunded, no questions asked?",
          "score": 1,
          "created_utc": "2026-01-29 14:31:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f0f0j",
              "author": "HzRyan",
              "text": "no I used it for 3 day straight, burn through quite a lot of tokens through Claude subs maybe that's why I got banned",
              "score": 1,
              "created_utc": "2026-01-29 14:34:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2f3db3",
                  "author": "Scriverry",
                  "text": "If you used more than one sub that's the reason you got banned, not that you used it with opencode. They are way more strict about that.",
                  "score": 3,
                  "created_utc": "2026-01-29 14:49:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lnrji",
          "author": "Peace_Seeker_1319",
          "text": "damn they refunded but banned you. probably flagged the usage pattern - too many requests too fast or something that looked automated. honestly the bigger issue is what you're shipping when you code that aggressively. if you're cranking out PRs without really understanding what's being generated, you're building tech debt bombs.  \nwe had to add automated review [codeant.ai](http://codeant.ai) in CI specifically because people were merging AI code too fast. caught race conditions, memory leaks, stuff that looked fine but broke under load. slowed us down but saved us from prod fires. maybe the ban is a feature not a bug lol.",
          "score": 1,
          "created_utc": "2026-01-30 13:44:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mf7sy",
              "author": "HzRyan",
              "text": "I have 6 years of experience DEV-ing and have worked on quite a few large projects before, so I'm not just yolo pushing any generated code and hoping it works. Every worktree branch gets manually+claude reviewed and tested before it gets locally merged. Of course I don't read every single line of code, but I do know how each section of my system works and I make sure it's specified in the prompt so CC knows where to extend or implement the feature.\n\nIt's honestly amazing how much CC has boosted my productivity. I'm holding a full time job right now while working on 3 other separate projects, and I think there is still room for improvement in my current workflow. Trying to get out of the 9-5 rat race, Locked in!! \n\nNice plug btwüòâ",
              "score": 1,
              "created_utc": "2026-01-30 15:56:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2o23t9",
          "author": "Brucesquared2",
          "text": "Your best bet, if you have a 16vram card is LM Studio and Ollama, give it a Claude wrapper. Its killer",
          "score": 1,
          "created_utc": "2026-01-30 20:20:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmwcp1",
      "title": "OpenCode Ecosystem feels overwhelmingly bloated",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qmwcp1/opencode_ecosystem_feels_overwhelmingly_bloated/",
      "author": "Codemonkeyzz",
      "created_utc": "2026-01-25 21:40:05",
      "score": 39,
      "num_comments": 30,
      "upvote_ratio": 0.93,
      "text": "I often check OpenCode ecosystem and update my setup every now and then to utilize  opencode  to the max.  I go through every plugins,  projects ...etc.   However, i noticed most of these plugins are kinda redundant. Some of them are kinda promoting certain services or products, some of them feel outdated, some of them  are for very niche use cases.\n\nIt kinda takes time to go through every single one and understand how to utilize it.  I wonder  what are you plugin and project choices from this ecosystem ?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qmwcp1/opencode_ecosystem_feels_overwhelmingly_bloated/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o1qcoa8",
          "author": "FlyingDogCatcher",
          "text": "You don't need to use any of that shit. Opencode is just fine on its own",
          "score": 16,
          "created_utc": "2026-01-26 01:02:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s7cbr",
              "author": "IIALE34II",
              "text": "Opencode with few selected skills is most certainly 80/20 rule.",
              "score": 6,
              "created_utc": "2026-01-26 08:02:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1po963",
          "author": "ekaqu1028",
          "text": "Reminds me of MCP; you go crazy adding a ton‚Ä¶ 6 months later you stop using MCP\n\nThe only plugin I use is to send notifications when the agent is waiting on me.\n\nI probably will try to find a plugin (or write one) that helps protect against unexpected deletes‚Ä¶ had a CC hook for that‚Ä¶ just not ported",
          "score": 9,
          "created_utc": "2026-01-25 23:04:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1q4uo5",
              "author": "Big_Bed_7240",
              "text": "I think all abstractions like skills, mcp, sub agents etc are anti-patterns. They will all disappear when the models get better",
              "score": 6,
              "created_utc": "2026-01-26 00:23:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1qfmxj",
                  "author": "ekaqu1028",
                  "text": "I had a sub agent to run tests and give summary for errors.  Our build is very very dense with text and 90% doesn‚Äôt matter‚Ä¶ I replaced with a script that calls the build and shows the log of the failing step‚Ä¶ simpler and more consistent‚Ä¶\n\nIm moving more and more to reusable cli and away from AI features",
                  "score": 4,
                  "created_utc": "2026-01-26 01:17:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1snfqs",
                  "author": "extreme4all",
                  "text": "I think subagents can significanly reduce the context in the main agent, but yes if context is unlimited and speed also than there is no need",
                  "score": 1,
                  "created_utc": "2026-01-26 10:28:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o24mbso",
                  "author": "zumus",
                  "text": "idk about anti-patterns. I think orchestration will continue to be useful. i agree about skills, but tool-calling is still necessary and specialists are necessary. I think it'll just come in the form of smaller specialized (fine-tuned) models, especially in the open source ecosystem",
                  "score": 1,
                  "created_utc": "2026-01-28 01:03:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1v2ch5",
              "author": "deadcoder0904",
              "text": "> I probably will try to find a plugin (or write one) that helps protect against unexpected deletes‚Ä¶ had a CC hook for that‚Ä¶ just not ported\n\nClaude Code Damage Control exists. Its by IndyDevDan & you can watch his YT video if you search his name too. Works well as it has both static `rm -rf` & dynamic (`delete my home directory`) saving.",
              "score": 2,
              "created_utc": "2026-01-26 18:14:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ttpba",
              "author": "touristtam",
              "text": "The context pruning and todo reminder are nice",
              "score": 1,
              "created_utc": "2026-01-26 14:59:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1r8zz7",
          "author": "SynapticStreamer",
          "text": "I've been running OpenCode with absolutely nothing but custom sub agents, custom commands, and a few MCP servers.\n\nJust because the ecosystem is bloated doesn't mean you have to **use** bloated software.",
          "score": 7,
          "created_utc": "2026-01-26 03:51:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22069m",
              "author": "ProfessionNo3952",
              "text": "Could you share your custom setup?",
              "score": 1,
              "created_utc": "2026-01-27 17:48:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24crug",
                  "author": "SynapticStreamer",
                  "text": "[ Removed by Reddit ]",
                  "score": 1,
                  "created_utc": "2026-01-28 00:15:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1qegyn",
          "author": "towry",
          "text": "Try PI",
          "score": 4,
          "created_utc": "2026-01-26 01:11:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uro06",
          "author": "klocus",
          "text": "Personally I find that simple solutions work best for experienced developers. In my day-to-day work, I use OpenCode with two plugins and three MCPs.\n\nMy biggest discovery, and the one I'm most pleased with, is the DCP plugin, which automatically reduces token usage by removing outdated tool results from the conversation history.\n\nBesides the AGENTS.md file, I use the Simple Memory Plugin, which saves important context (like a pattern or a new discovery) during a session as logfmt files, allowing agent to search by range, type, or query. However, I think there are some better solutions for permanent memory that would not have to be loaded at the beginning of the session.\n\nOn top of that, I use three MCPs: Angular CLI MCP, Playwright MCP, and the most important and useful one, Context Engine MCP from Augment. Code indexing is quite crucial. It helps avoid lots of \"grep\" commands and find patterns within the project.",
          "score": 5,
          "created_utc": "2026-01-26 17:28:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1p8rr7",
          "author": "alvinunreal",
          "text": "I can share mine: [https://github.com/alvinunreal/oh-my-opencode-slim](https://github.com/alvinunreal/oh-my-opencode-slim)\n\nAlso felt the same",
          "score": 10,
          "created_utc": "2026-01-25 21:55:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1pe5xy",
              "author": "Metalwell",
              "text": "Hello. I have been rawdogging 5.2 through OC for couple of days and I am pretty happy with the results. You claim it consumes less token, how much less are you claiming? Thanks! Will give this a go.\n\nEDIT: Tried this. It feels like it is eating way more token than vanilla oc.\n\nEDIT:2 Nope, seems like this config is less token hungry for some reason lol even with all those mcp servers good job",
              "score": 7,
              "created_utc": "2026-01-25 22:19:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1zjgwy",
                  "author": "alvinunreal",
                  "text": "Nice, thanks for feedback.  \nI also meant tokens compared to oh-my-opencode, which I forked...",
                  "score": 2,
                  "created_utc": "2026-01-27 09:20:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ph8rg",
              "author": "Codemonkeyzz",
              "text": "Thanks for sharing.  It looks good. I will  definitely try this.",
              "score": 3,
              "created_utc": "2026-01-25 22:33:10",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1pqdzc",
              "author": "rusl1",
              "text": "Good job on this one! This is the only one worth adding. Orchestrator does everything I need",
              "score": 3,
              "created_utc": "2026-01-25 23:14:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1sbtpm",
                  "author": "alvinunreal",
                  "text": "thank you!",
                  "score": 2,
                  "created_utc": "2026-01-26 08:42:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1qnsed",
              "author": "ahmetegesel",
              "text": "I genuinely can‚Äôt help myself from skipping these plugins only because of their extreme verbose and ‚Äúadventurous‚Äù readme files. Package says it is slim, but apparently not its readme. It is still as redundant as its parent.",
              "score": 3,
              "created_utc": "2026-01-26 01:59:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1sboyk",
                  "author": "aeroumbria",
                  "text": "It is almost mandatory at this point that if you generate a readme or even agent file with AI, you have to say something like \"no more than 300 lines\" for it to not end up being a sloppy mess...",
                  "score": 2,
                  "created_utc": "2026-01-26 08:40:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rwgev",
                  "author": "Metalwell",
                  "text": "It looks very strange not gonna lie. Feels like it might eat more tokens than default lol. I will give this a try today.",
                  "score": 1,
                  "created_utc": "2026-01-26 06:31:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rofj0",
          "author": "WeMetOnTheMountain",
          "text": "Yeah I was just thinking the same thing today and it's pretty damn buggy.¬† I made a minimal agent that works pretty well for me.¬† For local LLMS using something like oh my open code is really heavy and pretty much kills the model on startup.",
          "score": 2,
          "created_utc": "2026-01-26 05:31:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2699bn",
          "author": "AkiDenim",
          "text": "The only Plugins I use are markdown table formatter and DCP. The only reasons I use opencode tbh, let alone the customizability",
          "score": 1,
          "created_utc": "2026-01-28 07:04:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qljsw3",
      "title": "Opencode.nvim ‚Äì Opencode, fully integrated into Neovim.",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qljsw3/opencodenvim_opencode_fully_integrated_into_neovim/",
      "author": "Nexmean",
      "created_utc": "2026-01-24 10:37:41",
      "score": 38,
      "num_comments": 1,
      "upvote_ratio": 0.95,
      "text": "There is solid opencode.nvim plugin for neovim, that fully rebuilds opencode's interface to make it closer to native neovim experience: [https://github.com/sudo-tee/opencode.nvim](https://github.com/sudo-tee/opencode.nvim) \n\nhttps://preview.redd.it/br7syu7lz9fg1.png?width=2088&format=png&auto=webp&s=9a989748eeaa7d0060cb55fe70eca0683341f610\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qljsw3/opencodenvim_opencode_fully_integrated_into_neovim/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o1itpjg",
          "author": "ZeSprawl",
          "text": "Very cool, Zed has a similar integration with native ui. Dig this",
          "score": 2,
          "created_utc": "2026-01-24 23:49:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmx7h0",
      "title": "Your own dashboard for oh-my-opencode v3.0.0+",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/pr8em15bkkfg1.png",
      "author": "TheDevilKnownAsTaz",
      "created_utc": "2026-01-25 22:12:04",
      "score": 36,
      "num_comments": 10,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qmx7h0/your_own_dashboard_for_ohmyopencode_v300/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1qfiu3",
          "author": "rroj671",
          "text": "Nice, I‚Äôll try it tomorrow. Is it possible to know what the sub-agents are doing at any specific time?",
          "score": 1,
          "created_utc": "2026-01-26 01:16:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qgypo",
              "author": "TheDevilKnownAsTaz",
              "text": "Depends on your specific idea of granularity. Right now, the background agents table gets updated live. So you get 1) accurate # of tool calls 2) the most recent tool being used by the subagent. Does this satisfy your use case? If not, let me know, and I can implement what you suggest in the next version!",
              "score": 1,
              "created_utc": "2026-01-26 01:24:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1qhks4",
                  "author": "rroj671",
                  "text": "Usually I just wonder if the sub-agents are doing something (and how many of them are they) or if they got stuck. I guess if I can see the time of the last thought, message, or tool-use, I‚Äôd know if they‚Äôre working.",
                  "score": 3,
                  "created_utc": "2026-01-26 01:27:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rm38r",
                  "author": "Rand_o",
                  "text": "This is also my gripe, the built in agent for opencode you could switch over to the agent and see exactly what it is doing like what steps its taking, commands writing, etc, and with OMO you have no visibility. So long running agent tasks I have no idea if it is working through something or it froze. A lot of time they freeze I feel like because I am using the new GLM-4.7-Flash and its pretty unstable",
                  "score": 2,
                  "created_utc": "2026-01-26 05:15:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rxmvw",
          "author": "Meriu",
          "text": "This looks wonderful! I would love to see this for all agents, not only oh-my-opencode",
          "score": 1,
          "created_utc": "2026-01-26 06:41:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2it7r8",
              "author": "TheDevilKnownAsTaz",
              "text": "V0.1.0 now also supports vanilla OpenCode!",
              "score": 2,
              "created_utc": "2026-01-30 01:33:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zeuyu",
          "author": "datosweb",
          "text": "esta piola el dashboard pero me genera dudad como maneja el refresco de los datos para que no se te clave la terminal con peticiones pesadas mas si usas un monton de plugins juntos ojala la optimizacion de memoria este bien pulida xq sino se pone lento",
          "score": 1,
          "created_utc": "2026-01-27 08:36:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlp9rk",
      "title": "Crazy Potential Even for Note Taking",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/gallery/1qlp9rk",
      "author": "xdestroyer83",
      "created_utc": "2026-01-24 15:05:39",
      "score": 35,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qlp9rk/crazy_potential_even_for_note_taking/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1gt85d",
          "author": "mindgraph_dev",
          "text": "I made mindgraph-notes with opencode intengration also Zotero and ollama ich changed from Obsidian an never locked back a I need\n\nhttps://preview.redd.it/dtnvauue8cfg1.png?width=3456&format=png&auto=webp&s=3015c8d7a22eaf9eac8d10972e4bb87c532c6e6d",
          "score": 3,
          "created_utc": "2026-01-24 18:09:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1k2fdr",
              "author": "xdestroyer83",
              "text": "awesome setup!! obsidian has canvas as well so you might want to look again, and with obsidian skill models are able to write canvas really well",
              "score": 1,
              "created_utc": "2026-01-25 03:55:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1fs47q",
          "author": "bigh-aus",
          "text": "i quite like the acid brutalism one - very star trek / 90s-2000s computer game",
          "score": 1,
          "created_utc": "2026-01-24 15:22:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1gq43l",
              "author": "Heavy-Focus-1964",
              "text": "definitely got that Mechwarrior/Command and Conquer flavour",
              "score": 2,
              "created_utc": "2026-01-24 17:56:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gtrz4",
          "author": "mindgraph_dev",
          "text": "yes potential i crazy also for learning you can make /learnsetting and force opencode to learn with all the content of you notes in socratic dialog for school , university etc.",
          "score": 1,
          "created_utc": "2026-01-24 18:12:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1m7ou7",
          "author": "Fresh_Ad_1722",
          "text": "What ide?",
          "score": 1,
          "created_utc": "2026-01-25 13:48:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mpkic",
              "author": "xdestroyer83",
              "text": "not really an ide but I'm using obsidian in the screenshot, i integrated opencode using a plugin",
              "score": 1,
              "created_utc": "2026-01-25 15:21:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qp1b6e",
      "title": "Black 100 sub is too limiting as compared to Claude Max",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/18ye8k58o0gg1.png",
      "author": "Top_Shake_2649",
      "created_utc": "2026-01-28 04:24:54",
      "score": 35,
      "num_comments": 14,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qp1b6e/black_100_sub_is_too_limiting_as_compared_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o25uehb",
          "author": "jpcaparas",
          "text": "Yeah it's because of this:\n\nhttps://pub.towardsai.net/why-your-expensive-claude-subscription-is-actually-a-steal-02f10893940c?sk=65a39127cbd10532ba642181ba41fb8a\n\nYou‚Äôre paying $20 for $259 worth of api usage on a Claude Pro Plan and you're getting  $1.3k worth of api tokens on a $100 max plan and opencode black afaik, doesn't do the same level of subsidy that Anthropic does.\n\nSo if I were on OpenCode Black, I'd just stick to the low-cost Chinese models.\n\nEdit:\n\nI'll report back when I try out synthetic dot new",
          "score": 12,
          "created_utc": "2026-01-28 05:11:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o265y3y",
              "author": "aeroumbria",
              "text": "Is it really a \"steal\" if there exist providers offering similarly capable models at a flat API rate that is cheaper than the \"subsidised\" package? Seems more like self-inflicted cost bloat to me...",
              "score": 5,
              "created_utc": "2026-01-28 06:38:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o269lal",
              "author": "armindvd2018",
              "text": "In that case why black ? I can use NanoGPT just $8 !!!",
              "score": 2,
              "created_utc": "2026-01-28 07:07:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o26cr5x",
              "author": "Top_Shake_2649",
              "text": "yeah, I've read this article before, and I knew about it before jumping ship. That's why I am not really complaining as I have said. Still, slightly disappointed tho. Also, another thing, I might have accidentally used too much since MoltBot (ClawdBot) hype... üò¨",
              "score": 2,
              "created_utc": "2026-01-28 07:34:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o29ajqs",
                  "author": "kkordikk",
                  "text": "Nigha you buy 100 sub and plug into Clawdbot lmao",
                  "score": 0,
                  "created_utc": "2026-01-28 18:10:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29aga9",
              "author": "Tushar_BitYantriki",
              "text": "Well, you are comparing their price with THEIR price. That itself is a fallacy.\n\nIt's like comparing the cost of an iPhone with another iPhone. Playing right into the anchor.\n\nWhile in reality their price needs to be compared to other alternatives that are comparable. At this point, they no longer have the \"SOTA\" benefit anymore.\n\nGLM 4.7 is really killing it, and while I started planning with Opus and implementing with GLM, I have gradually moved to doing most of my planning with GLM-4.7\n\nHaven't tried Kimi, but it seems to beat them both on many benchmarks.\n\nSure, they are in their \"Give it all for cheap\" phase, but the price difference between Anthropic and everyone else is not proportionate.\n\nSo people really need to ask themselves if they really need Claude for either the sub or the APIs.\n\nAt this point, if you are writing some code that any AI company might be interested in stealing, then by all means stick to Claude or OpenAI (in case they are absolutely trustworthy). I hope no model is actually seeing any customer data to begin with.\n\nAnd if someone is really doing something that is so complicated that they feel only Claude can do it, then maybe, they either their employer or their revenue should be able to pay for it.\n\nFor everyone else, there are cheaper, and equally (in some cases, more) capable models.\n\nI have personally been weaning out my usage from Anthropic to GLM within Claude code. And at the same time, also moving my workflows to Opencode, for the day when Anthropic decides to also ban accounts for using other models in Claude code (I bet they are collecting those stats even when you are using a different model)\n\nAt this stage, I wouldn't care much, even if my account is banned. Since I moved to GLM, I haven't hit my Claude limits for 2-3 weeks. And already planning to move to $20 sub at the end of this month (already moved from 200 to 100 1.5 months ago)\n\nOr maybe, they will be happy that people who they were losing money on, are finally leaving.",
              "score": 2,
              "created_utc": "2026-01-28 18:10:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o262gmc",
          "author": "lundrog",
          "text": "Here is my set up. While its not perfect. I do think it works very well. Why? Because the claude code pro account hits a limit within minutes. Or slow performance elsewhere with overseas providers. Aka I can't afford a claude code max plan.\n\nI primarily use glm 4.7 for workflow with deep seek v3.2 for troubleshooting. But now k2.5 is out! \n\nI use claude code, with it the agent, Work flow works unattended for at least a few minutes. Opencode is good also, doesn't run as long unattended. \n\nFor agents https://github.com/VoltAgent/awesome-claude-code-subagents\n\nFor skills https://github.com/VoltAgent/awesome-claude-skills\n\nI am running it with this api gateway ( check your toc ) https://github.com/looplj/axonhub\n\nFor a main provider I use synthetic.new , great performance and privacy is much better than most. Text models but have optional image on demand available. You can back that up with Claude code or a ZAI account or anti gravity, etc. I believe official Claude code and anti gravity support are coming soon.\n\nI have a referral link  \"Invite your friends to Synthetic and both of you will receive $10.00 for standard signups. $20.00 for pro signups. in subscription credit when they subscribe!\" \n\nhttps://synthetic.new/?referral=UAWqkKQQLFkzMkY\n\nI am on my second month and on the $60 plan which gives you 1350 requests every 5 hours without a weekly limit. Should give about 5x a cluade code max plan.\n\nAnywho long story longer... It gives you a lower cost option with a higher quota to use with other plans via the api gateway. \n\n\nMaybe its helpful, ü§î\n\nGood luck on everything",
          "score": 4,
          "created_utc": "2026-01-28 06:10:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26cga8",
              "author": "Top_Shake_2649",
              "text": "[synthetic.new](http://synthetic.new) sounds promising, but I haven't heard much about it before. And the website doesn't give me confident. I rather consider go subscribe to [z.ai](http://z.ai) or moonshot directly?",
              "score": 1,
              "created_utc": "2026-01-28 07:31:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2754vj",
                  "author": "lundrog",
                  "text": "Small team, but its all hosted in usa based providers. Right now they have very generous limits; vs going direct from a cost perspective.",
                  "score": 3,
                  "created_utc": "2026-01-28 11:44:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2dcj44",
          "author": "telewebb",
          "text": "I'm sorry, I'm new to OpenCode black. Why are you not just throwing 10 bucks on OpenRouter and doing pay as you go instead of subscription? I don't understand what you get from a subscription.",
          "score": 1,
          "created_utc": "2026-01-29 07:07:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dji76",
              "author": "Top_Shake_2649",
              "text": "Yes great question. That‚Äôs exactly what I‚Äôm trying to find out with a black subscription. There is supposed to be some subsidy, so far it doesn‚Äôt seem like it.",
              "score": 1,
              "created_utc": "2026-01-29 08:08:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2flg4c",
          "author": "Ordinary-You8102",
          "text": "why not just use antigravity which have a lot models + github copilot which also have a lot of models for way cheaper? (both oauth)",
          "score": 1,
          "created_utc": "2026-01-29 16:11:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq5m8o",
      "title": "Kimi K2.5 in opencode",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qq5m8o/kimi_k25_in_opencode/",
      "author": "t4a8945",
      "created_utc": "2026-01-29 11:02:09",
      "score": 31,
      "num_comments": 20,
      "upvote_ratio": 0.84,
      "text": "Hello,\n\nI'm a big fan of Opus 4.5 especially in opencode. Fits my workflow very well and enjoy the conversational aspect of it a lot.\n\nI'm always trying new models as they come, because the space is moving so fast and also because Anthropic doesn't seem to want me as a customer. I tried GLM 4.7, MiniMax-2, Devstral 2, Mistral Large 3, and I never was satisfied by the results. Too many errors that couldn't compete with what Opus 4.5 was delivering. I also tried GPT5.2 (medium or high) but I hate it so much (good work but the interactions are hell).\n\nSo I set Kimi K2.5 up to work with a SPEC.md file that I used in a previous project  (typescript node + react, status notification app) and here is how it went:\n\n* Some tool calls error with truncated input which halted the task (solved by just saying \"continue and be careful about your tool calls\")\n* It offered to implement tests, which none of the other models did\n* It had a functional implementation quite quickly without too many back and forth\n* It lacked some logic in the UI (missing buttons) but pointing it out led to a working fix\n* Conversation with it is on par with what I get from Opus, albeit it feels like a little bit less competent coworker ; but if feels GOOD.\n* The end result is very good!\n\nI highly recommend you try it out for yourself. It is better than I expected. (edit to clarify: not as good as Opus, but better than anything else I tried - \"better\" is  very personal as I tried to laid out above, it's more about the process than the end result)\n\nWhat is your experience with it? Did I develop some patience with these models or is it quite competent?\n\nedit: I'm using the official Kimi Code sub, as I've read integration in vendors can lead to less success in tool calls especially. Since this is open weight, not all providers are equal. See [https://github.com/MoonshotAI/K2-Vendor-Verifier](https://github.com/MoonshotAI/K2-Vendor-Verifier) for instance (they updated it for K2.5 and it should equalize vendors more, but keep that in mind)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qq5m8o/kimi_k25_in_opencode/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2e546t",
          "author": "DistinctWay9169",
          "text": "I found Kimi 2.5 to be the most overrated model. I asked it to fix a problem I already knew how to fix, and it told me the problem was not what I was talking about. Then I told it, \"Then fix it with your solution\" and guess what. After a bunch of tokens spent on loop thinking, it did not solve the problem. This model is not better than opus at all. I found this model is great for a bunch of things, but for coding, it is meh.",
          "score": 11,
          "created_utc": "2026-01-29 11:23:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e96gd",
              "author": "patlux",
              "text": "Same for me. I compared it with the responses from Opus 4.5 and Opus make much more suggestions and asks better questions back than Kim 2.5.",
              "score": 3,
              "created_utc": "2026-01-29 11:54:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2e9jeo",
              "author": "mintybadgerme",
              "text": "Yep, I agree. It's vastly overrated. It's okay, but definitely nowhere near Opus.",
              "score": 2,
              "created_utc": "2026-01-29 11:57:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2enp58",
              "author": "jovialfaction",
              "text": "Unfortunately same for me.\n\nGot a month of Kimi Code subscription.\n\nFed it an issue in my codebase that was a bit tricky but definitely solvable.\n\nClaude got me the right fix. GLM 4.7 too. Kimi 2.5 started accusing database caches and offered a very complex fix that wouldn't have fixed anything.\n\nI'll still play with it, but disappointed in my first 24h",
              "score": 2,
              "created_utc": "2026-01-29 13:28:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2epxzx",
              "author": "aeroumbria",
              "text": "I am starting to assign specific models to specific tasks rather than trusting in a generalist model. I feel that Deepseek might be the best debugging / validation model. It is slow, and does not follow detailed workflow instructions very well (spent too much time debating what output document style to use), but it is very thorough, has maximum self doubt and almost zero self-confidence, and will actually debate with its former self, which is perfect for error catching. It is also markedly different in reasoning trace compared to most other models (probably due to different training data, heavier RL use and not relying much on distilling competitions), so in theory it should also be less prone to shared blind spots of other models.",
              "score": 2,
              "created_utc": "2026-01-29 13:40:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2h4oja",
              "author": "RegrettableBiscuit",
              "text": "I like it. I don't think anyone expects it to be as good as Opus, but unlike other open models that feel like a year behind Anthropic or OpenAI's current models, this feels more like six months behind.\n\n\nI could be fine with only using K2.5, which I can't say for models like GLM4.7.",
              "score": 1,
              "created_utc": "2026-01-29 20:24:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lm7b6",
              "author": "hey_ulrich",
              "text": "Interesting to hear this. I'm having a great experience with Kimi 2.5, and I have Claude Max and use Opus everyday. I mostly develop webapps with python backend and postgres. What kind of products and languages are you working with?",
              "score": 1,
              "created_utc": "2026-01-30 13:35:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2e5mfd",
              "author": "t4a8945",
              "text": "Interesting, what is your context (type of project, language, etc)?\n\nTo try it I just gave it my \"benchmark\" (start a new project from scratch, see how it works and interacts), but I'll keep throwing more cases at it to see out it fares.",
              "score": 1,
              "created_utc": "2026-01-29 11:27:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2e5sms",
                  "author": "DistinctWay9169",
                  "text": "Electron + Typescript + React.",
                  "score": 1,
                  "created_utc": "2026-01-29 11:29:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2e7sm4",
          "author": "Funny-Advertising238",
          "text": "I've been having incredible success with it! Better than any other open source model by far.¬†\n\n\nIf you've ever used Opus 4.5 and watched the way it thinks you know that kimi was definitely trained on Opus/Sonnet. The way it thinks and goes through tasks, and the way it responds, they definitely did the same scheme that deepseek did on openai.¬†\n\n\nNot saying it's Opus level but I personally love the way it goes through tasks and the interactions with it. Gpt 5.2 interactions makes my brain hurt sometimes.¬†\n\n\n\nI was having trouble with something that GPT 5.2 took an hour and still couldn't solve it, and kimi solved it in a few minutes.¬†\n\n\nNot sure how good it would be in the wild, one shotting etc. as my agents.md, skills and subagents setup is quite thorough.¬†\n\n\nBut for my use case it's absolutely killing it! Using the Kimi Coder plan too.¬†",
          "score": 6,
          "created_utc": "2026-01-29 11:44:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eaz4f",
              "author": "t4a8945",
              "text": "Haha we're the same! It feels very \"Opus\" in the interaction, your intuition about it being trained on it feels right.\n\nI'm just happy to have found again this \"coworker\" feeling when working with it. (Not like GPT5.2)",
              "score": 3,
              "created_utc": "2026-01-29 12:07:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2kzsz2",
          "author": "LongBit",
          "text": "I tried it today for the first time on an issue Opus 4.5 could not fix.  Kimi 2.5 solved it without help.",
          "score": 3,
          "created_utc": "2026-01-30 11:04:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hwj82",
          "author": "kpgalligan",
          "text": "I've been dabbling. I'm on the CC 20x plan. Always assume the \"___ is as good as Opus\" is BS, but eventually it won't be. Maybe not as good, but at least usable. In the past I've found other models to be a mess with actual work.\n\nOn Kimi, I have to agree. So far. I'm only using it for analysis tasks, but it has handled tools well, which has not be true of other models I've tried (to be fair, 6+ months ago). I haven't swapped into any major tasks, mostly because I have plenty of Claude headroom, but will over time. Kind of on an urgent project at the moment so not a lot of \"play\" time.\n\nI do want to integrate it into our tool. We're building a focused coding agent. API costs are high, so if Kimi could handle analysis that is chewing up tokens, it would probably be a great option. Sometime in the next week or two, likely.",
          "score": 2,
          "created_utc": "2026-01-29 22:38:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ocpe7",
          "author": "reduhh",
          "text": "legit I like it better than opus maybe because of speed but rn it‚Äôs my favorite model",
          "score": 2,
          "created_utc": "2026-01-30 21:10:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eckex",
          "author": "Michaeli_Starky",
          "text": "Alleged Opus killer? Yeah, expected",
          "score": -2,
          "created_utc": "2026-01-29 12:18:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ed0s4",
              "author": "t4a8945",
              "text": "I haven't stated that in the slightest. Read again",
              "score": 3,
              "created_utc": "2026-01-29 12:22:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2enb3o",
                  "author": "Michaeli_Starky",
                  "text": "I didn't say you said it. Read again",
                  "score": -2,
                  "created_utc": "2026-01-29 13:25:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qp0w55",
      "title": "Anyone using Kimi K2.5 with OpenCode?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qp0w55/anyone_using_kimi_k25_with_opencode/",
      "author": "harrsh_in",
      "created_utc": "2026-01-28 04:05:28",
      "score": 30,
      "num_comments": 41,
      "upvote_ratio": 0.91,
      "text": "Yesterday I did top up recharge for Kimi API and connected it with OpenCode via API. While I can see Kimi K2 models in the models selection, I can‚Äôt find K2.5 models. \n\nCan someone please help me with it?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qp0w55/anyone_using_kimi_k25_with_opencode/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o25nzgw",
          "author": "aeroumbria",
          "text": "`opencode models --refresh` and try again?",
          "score": 14,
          "created_utc": "2026-01-28 04:30:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2checj",
              "author": "jpcaparas",
              "text": "I'm only getting K2 thinking\n\nhttps://preview.redd.it/o4e4n2rfj7gg1.png?width=1002&format=png&auto=webp&s=2f62a5a4bceaf02f2992a5304ee1abf378244c4e",
              "score": 0,
              "created_utc": "2026-01-29 03:26:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2cie9d",
                  "author": "jpcaparas",
                  "text": "NVM, when I connected Synthetic provider I was able to see it \n\nhttps://preview.redd.it/9fwq380ik7gg1.png?width=1032&format=png&auto=webp&s=cc81eb47bc7ba7c31932b46c07ef63e93a3e1046",
                  "score": 1,
                  "created_utc": "2026-01-29 03:32:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27piz4",
          "author": "chiroro_jr",
          "text": "Yes. Used the moonshot 20$ sub. Create an API key. Updated open code. Then run auth login. Selected kimi code. Pasted my key. Done. No issues at all.",
          "score": 5,
          "created_utc": "2026-01-28 13:51:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cljdh",
              "author": "Nooddlleee",
              "text": "What is the code quality? Is it hallucinating on complex and long tasks?",
              "score": 2,
              "created_utc": "2026-01-29 03:51:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2d9a2d",
                  "author": "chiroro_jr",
                  "text": "Code quality depends on the code quality already in your codebase and your prompting skills for the most part. You can't expect top quality code in an app that has messy code and largely been vide coded and at the same time the prompts are bad. Most of these models generate decent code already. Kimi K2.5 is on the same level as an Opus or Codex, especially with a good codebase and a good prompt.\n\nEven Opus can produce shit code if it's working on shit code with a shit prompt.",
                  "score": 1,
                  "created_utc": "2026-01-29 06:39:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27sdme",
              "author": "Villain_99",
              "text": "Is the subscription better than Claude code ?\nPrice wise is same, wondering the consumption limits",
              "score": 1,
              "created_utc": "2026-01-28 14:06:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27x1t6",
                  "author": "chiroro_jr",
                  "text": "For me Kimi K2.5 is the first model that feels that close to Opus 4.5. Because it's dirt cheap, that bridges the gap. I have been using it to do tickets for the past 3 hours. It only failed to do exactly what I wanted probably once or twice. I corrected it and it immediately got back on track. So if it's not one shotting my requirements, the next prompt with do it. All for what? A fifth the price. For me this is the best value. Right now I am on their 20$ plan. 200 messages per 5 hour window. 2048 messages per week. I got a shit tonne of work done with my first 200 messages. I think Claude Code 20$ doesn't even have Opus. Only Sonnet and Hauku. Kimi 2.5 is definitely better than Sonnet.",
                  "score": 13,
                  "created_utc": "2026-01-28 14:30:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2cmfiz",
              "author": "shantz-khoji",
              "text": "For 20$ how many credits for it provides?",
              "score": 1,
              "created_utc": "2026-01-29 03:57:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2d9e42",
                  "author": "chiroro_jr",
                  "text": "2048 per week. 200 per 5 hour window. It's been enough for me  so far.",
                  "score": 1,
                  "created_utc": "2026-01-29 06:40:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2b2vc4",
              "author": "SunflowerOS",
              "text": "You can create a api ley with subscription? I subscribe on december but i didn't understand how connect the kimi with opencode I just cancellled it after the month",
              "score": 0,
              "created_utc": "2026-01-28 22:55:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hpkiv",
                  "author": "eduknives",
                  "text": "You can create here [https://www.kimi.com/code/console](https://www.kimi.com/code/console)",
                  "score": 1,
                  "created_utc": "2026-01-29 22:03:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26fmjm",
          "author": "shaonline",
          "text": "Don't forget to upgrade opencode (opencode upgrade) as it had to be added to models.dev.\n\nOverall it's pretty decent and the biggest improvement over, in my case, using GPT 5.2, is the speed, on the fastest providers (I'm using \"fireworks\" on OpenRouter, at the same API price) it reaches 100 tok/sec which is really good for execution I think. I might switch to it entirely as my \"executor/builder\".",
          "score": 4,
          "created_utc": "2026-01-28 07:59:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26pt2h",
              "author": "harrsh_in",
              "text": "How did you fix the error\n\n`invalid temperature: only 1 is allowed for this model`",
              "score": 1,
              "created_utc": "2026-01-28 09:33:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o26py0n",
                  "author": "shaonline",
                  "text": "I think you cannot set the temperature explicitely, it only has two hardcoded values (1 for thinking mode and 0.6 for non-thinking/instruct mode).",
                  "score": 1,
                  "created_utc": "2026-01-28 09:34:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27bjp1",
                  "author": "Phukovsky",
                  "text": "I'm getting this error too.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:30:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2fa1dn",
                  "author": "Ponchito147",
                  "text": "Add this to the opencode.json in the provider section:\n\n    \"moonshotai\": {\n          \"models\": {\n            \"kimi-k2.5\": {\n              \"temperature\": false,\n              \"interleaved\": {\n                \"field\": \"reasoning_content\"\n              }\n            }\n          }\n        }",
                  "score": 1,
                  "created_utc": "2026-01-29 15:21:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2cj4di",
          "author": "rokicool",
          "text": "I think I found a solution that worked for me. \n\nJust as everyone else I bought a $20 subscription from https://www.kimi.com/. Then I generated API Key at [Kimi Code Console](https://www.kimi.com/code/console?from=kfc_overview_topbar). \n\nAnd then I used `/connect` command in OpenCode and chose \"`Kimi For Coding`\" as a Provider (Not `Moonshot AI` !). Put the API Key and everything started working. \n\nHappy coding!",
          "score": 5,
          "created_utc": "2026-01-29 03:37:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28at2u",
          "author": "Juan_Ignacio",
          "text": "I tried the subscription directly from [https://www.kimi.com/](https://www.kimi.com/) and had no issues.\n\nTry running:\n\n    opencode upgrade\n    opencode models --refresh\n\nAfter that, you should be able to log in and use it in opencode without problems.  \nTo use it with oh-my-opencode, I had to define the model in `opencode.json` like this:\n\n    \"provider\": {\n      \"kimi-for-coding\": {\n        \"models\": {\n          \"kimi-k2.5\": {\n            \"id\": \"kimi-k2.5\"\n          }\n        }\n      }\n    }\n\nAlso worth mentioning: the 1-month subscription basically costs $1 if you use someone's referral link and send them jokes in the chat that opens until the price drops to $0.99. \n\nJust in case, here‚Äôs my referral link:  \n[https://www.kimi.com/kimiplus/sale?activity\\_enter\\_method=h5\\_share&invitation\\_code=AN9SGQ](https://www.kimi.com/kimiplus/sale?activity_enter_method=h5_share&invitation_code=AN9SGQ)\n\nThe jokes don‚Äôt have to be from Kimi specifically. You can generate them with any other AI and just copy paste them.",
          "score": 6,
          "created_utc": "2026-01-28 15:35:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bzr4h",
              "author": "rick_1125",
              "text": "you don't have to add provider in \\`opencode.json\\`, once you logged in provider will be generated in model list, just copy model name to \\`oh-my-opencode.json\\`, its \\`kimi-for-coding/k2p5\\` for kimi-k2.5",
              "score": 2,
              "created_utc": "2026-01-29 01:49:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2c5nwp",
                  "author": "Juan_Ignacio",
                  "text": "Thank! . I had written k2.5 and k2-5 instead of k2p5.",
                  "score": 1,
                  "created_utc": "2026-01-29 02:21:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2dp4xv",
              "author": "Appropriate_Yak_1468",
              "text": "https://preview.redd.it/d14c5y0479gg1.png?width=546&format=png&auto=webp&s=356b5cd6aef608a0ec848481e7d8877ab2acdc7c",
              "score": 2,
              "created_utc": "2026-01-29 09:01:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2g7rxb",
                  "author": "trenescese",
                  "text": "lmao it was legitimately funny to do that",
                  "score": 1,
                  "created_utc": "2026-01-29 17:52:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27fk5p",
          "author": "Simple_Split5074",
          "text": "I tried on nano-gpt, it's slow as molasses (like one rerquest per minute!) and occasionally tool calls fail or it simply gets stuck (no observable progress for 5+ min).\n\nMy suspicion: the inference providers do not have it completely figured out yet.\n\nMoonshot via openrouter was decent last night but now it crawls around at 15tps. Fireworks still claims to do 100+ tps but I have no idea if caching works with opencode and without it would get ruinous quickly.",
          "score": 2,
          "created_utc": "2026-01-28 12:56:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29ue7c",
          "author": "Ok-Connection7755",
          "text": "I upgraded opencode, did auth and then put the API key, works perfectly. So far I love the speed and responsiveness of the model. Will test more send post here.",
          "score": 2,
          "created_utc": "2026-01-28 19:36:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26nzqy",
          "author": "StrangeJedi",
          "text": "I didn‚Äôt see the 2.5 model either have they added it?",
          "score": 1,
          "created_utc": "2026-01-28 09:16:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27bsld",
          "author": "Phukovsky",
          "text": "I see two options available: Moonshot AI and Moonshot AI (China)\n\nI first tried entering API key for the (China) and I get invalid auth error. Then I added key for Moonshot AI, chose Kimi 2.5 and am now getting: \\`invalid temperature: only 1 is allowed for this model\\`",
          "score": 1,
          "created_utc": "2026-01-28 12:31:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27is6k",
          "author": "Phukovsky",
          "text": "I got an API key from [platform.moonshot.ai](http://platform.moonshot.ai) after adding some funds to my account there. I added the API key to Opencode but doesn't seem to work. Thinking I need an actual Kimi Code subscription and an API key from there instead?",
          "score": 1,
          "created_utc": "2026-01-28 13:15:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27q1g3",
          "author": "zach978",
          "text": "I see it on opencode zen, haven‚Äôt tried it though.",
          "score": 1,
          "created_utc": "2026-01-28 13:54:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a0zv2",
          "author": "Aggravating_Bad4163",
          "text": "I checked this on opencode with openrouter and it worked fine without any issues.",
          "score": 1,
          "created_utc": "2026-01-28 20:06:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fnh52",
          "author": "dxcore_35",
          "text": "This is direct answer from Kimi-K-2 official chat AI. Does it make sense?\n\nI need to be very direct with you: **You cannot use your Kimi Moderato plan with Claude Code. They are completely incompatible.**\n\nHere's why this won't work:\n\n# The Hard Technical Reality\n\n**Kimi Moderato Plan** = Subscription for Kimi's **chat interface only** (app and web)\n\n* ‚ùå Does **NOT** include API credits\n* ‚ùå Does **NOT** work with Claude Code\n* ‚ùå Does **NOT** work with any third-party tools\n\n**Claude Code** = Anthropic's CLI tool\n\n* Only connects to **Anthropic's API** (claude-3-5-sonnet, claude-3-opus)\n* Hardcoded to Anthropic's servers (`api.anthropic.com`)\n* Requires an **Anthropic API key**, not Kimi credential",
          "score": 1,
          "created_utc": "2026-01-29 16:20:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fnsvt",
              "author": "Ok_Box7357",
              "text": "Yeah, just noticed that weekly limits decreased: 2048 -> 100  \nThat sucks ...",
              "score": 1,
              "created_utc": "2026-01-29 16:22:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hp5pn",
                  "author": "Hyp3rSoniX",
                  "text": "no they switched it to % view, that's why it showed 100 for a moment. if you visit it again it should show some % number.",
                  "score": 1,
                  "created_utc": "2026-01-29 22:02:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2bcvqa",
          "author": "elllyphant",
          "text": "Yes you can use Synthetic's $20 standard sub for $12 (they're 40% off right now) to use kimi k2.5\n\nhttps://preview.redd.it/xttlpui8g6gg1.png?width=788&format=png&auto=webp&s=522494ed86621bccbde7188444f8e7b9892173ac",
          "score": 0,
          "created_utc": "2026-01-28 23:47:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dwwc2",
              "author": "kr_roach",
              "text": "What is difference between Synthetic‚Äôs and official moonshot ai",
              "score": 2,
              "created_utc": "2026-01-29 10:13:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bcwrp",
              "author": "elllyphant",
              "text": "[https://synthetic.new/?saleType=moltbot](https://synthetic.new/?saleType=moltbot)",
              "score": 0,
              "created_utc": "2026-01-28 23:47:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qmurxk",
      "title": "Why should I use my OpenAI subscription with Open Code instead of plain codex?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qmurxk/why_should_i_use_my_openai_subscription_with_open/",
      "author": "420rav",
      "created_utc": "2026-01-25 20:42:25",
      "score": 24,
      "num_comments": 31,
      "upvote_ratio": 0.93,
      "text": "I‚Äôm really interested in the project since I love open source, but I‚Äôm not sure what are the pros of using OpenCode.\n\nI love using Codex with the VSC extension and I‚Äôm not sure if i can have the same dev experience with Open Code.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qmurxk/why_should_i_use_my_openai_subscription_with_open/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o1otjwc",
          "author": "Repulsive-Western380",
          "text": "I‚Äôm using codex with open code ChatGPT plus subscription and never going back.",
          "score": 10,
          "created_utc": "2026-01-25 20:49:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1otqcp",
              "author": "420rav",
              "text": "Whats the difference between open code vs plain codex?",
              "score": 3,
              "created_utc": "2026-01-25 20:50:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1oumg1",
                  "author": "Repulsive-Western380",
                  "text": "The key difference in developer experience between OpenCode and Codex CLI is that OpenCode offers clearer, more detailed explanations for the same coding prompts, along with a faster and more reliable user interface in the terminal, making it easier and more efficient for developers to use AI-assisted coding tools, while Codex CLI provides similar core answers but often with less clarity, slower performance, and more reported bugs or usability issues.",
                  "score": 9,
                  "created_utc": "2026-01-25 20:54:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1owhtu",
          "author": "RedParaglider",
          "text": "The best thing about opencode is you can build workflows that utilize different LLM's.  If you JUST have an openai subscription then you can have an orchestrator running a small gpt model, when it needs to create an SDD and dependency map it could use the xhigh gpt5, then kick off that result to build an implementation plan to codex, and then code it up with codex.  You would get almost the exact same results as using GPT5 xhigh, but at a much reduced cost.  Enough of a reduced cost that you could then have a dialectical agent come in behind top level steps and validate results.\n\nThe downside is that there is a lot of learning.  If you use opencode I'd suggest also grabing some popular plugins to check out what is possibe such as oh my opencode.  OMO is kind of heavy, so its debatable on if it's a good daily driver BUT it absolutely is a good example of things that can be done in the system.",
          "score": 23,
          "created_utc": "2026-01-25 21:02:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1p1dk6",
              "author": "420rav",
              "text": "I was planning to get Claude Code too. Can I combine both? How?",
              "score": 0,
              "created_utc": "2026-01-25 21:23:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1p4ew0",
                  "author": "sudonem",
                  "text": "Yes, but you can no longer use a Claude code subscription with OpenAI - you‚Äôd have to use the more expensive pay as you go via API key model. \n\nAnthropic is moving to close off its subsidized subscription model in order to lock people in to their ecosystem if they aren‚Äôt going to pay b2b rates. \n\nWhich‚Ä¶ really sucks. But it‚Äôs theirs to do as they please.",
                  "score": 5,
                  "created_utc": "2026-01-25 21:36:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1phkmg",
                  "author": "RedParaglider",
                  "text": "The only way is to have opencode make an operating system level call to claude to engage a task off of the command line through a prompt.  Anthropic decided they wanted to be a closed ecosystem, which is odd because there are like 3 places I can go run opus 4.5 right now that aren't anthropic.",
                  "score": 2,
                  "created_utc": "2026-01-25 22:34:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ovi7l",
          "author": "TheOneThatIsHated",
          "text": "Stated plainly: opencode has just better ux. Terminal ui works better, has more quality of life features, etc. \n\nBut what's stopping you?\nTry codex cli, then try opencode\nYou're not paying more by trying both. \nGo nuts",
          "score": 5,
          "created_utc": "2026-01-25 20:58:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ovyvz",
              "author": "420rav",
              "text": "It has some more advanced features? Better support for mcp, skills, language server etc?",
              "score": 1,
              "created_utc": "2026-01-25 21:00:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1oxwnu",
                  "author": "TheOneThatIsHated",
                  "text": "Afaik codex cli has no lsp support at all. Opencode supports almost all languages by default without any configs. \n\nOpencode supports all the usual stuff that claude code also supports like agents.md claude.md skills mcp etc.\n\n\nIt has quality of life features like always including the files in cwd when prompting without an extra tool call. \n\nFurthermore, they have custom system prompts per model, instead of the same for all models. \n\nFull support for subagents and stuff like that. \n\nA vibrant github community, where issues are actually resolved and you get daily updates. \n\n\nAlso not to be understated is that their tui/ui engine is just for a lack of a better word 'better' \n\n\nAgain, just try it, you can judge yourself",
                  "score": 6,
                  "created_utc": "2026-01-25 21:08:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ovaee",
          "author": "420rav",
          "text": "Ok but how they can achieve that? They inject some prompts? What‚Äôs the perceived quality of the output?\n\nIm using the vsc extension for codex is way better than using terminal, does some like this exists for opencode?",
          "score": 2,
          "created_utc": "2026-01-25 20:57:08",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1oxve2",
              "author": "Repulsive-Western380",
              "text": "OpenCode gets better results by using big memory windows to remember the whole project, smart model choices for tasks, and efficient tools that run code without wasting space, while likely adding hidden instructions to make answers clearer; users see its output as high quality, reliable, and helpful for work without forgetting details. Yes, OpenCode has a VS Code extension like Codex‚Äôs, making it easier to use in the editor instead of just the terminal.",
              "score": 3,
              "created_utc": "2026-01-25 21:08:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1oyu9n",
                  "author": "Coldshalamov",
                  "text": "and opencode has a robust plugin ecosystem and is more easily hackable. I made a plugin for my opencode that loads in 5 of my subscriptions to various providers and cycles them based on task and remaining limit. I don't think that's happening on codex cli. Plus, defined subagents across providers",
                  "score": 3,
                  "created_utc": "2026-01-25 21:12:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1p1i3u",
                  "author": "420rav",
                  "text": "Can you link me the extension? Because I didnt find that",
                  "score": 0,
                  "created_utc": "2026-01-25 21:23:48",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1p19vb",
          "author": "Codemonkeyzz",
          "text": "Not sure if Codex CLI has these :  Plugins, Skills , Commands, Subagents/Primary Agents,  hooks ..etc.  \nAlso opencode allows you to have the same setup for different models.  e.g; you want to use different LLMs for different tasks, or switch between models while having the same setup.\n\nI often switch between GPT 5.2 , Opus 4.5 , Minimax 2.1 , GLM 4.7  for different tasks or when i consume my credits in one , i switch to the others.",
          "score": 2,
          "created_utc": "2026-01-25 21:22:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1q7z5r",
          "author": "meerestier",
          "text": "Opencode with chrome dev tools mcp server is very nice with the latest Gemini for web dev work.",
          "score": 2,
          "created_utc": "2026-01-26 00:39:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1pfbj5",
          "author": "ZeSprawl",
          "text": "I just like that I can use all providers with a common interface, including mcp, skills, sub agents and whatever else comes along, and if I ever need to modify the code itself I can do that too.\n\nAlso it uses a client server architecture, so the back end is just a REST endpoint, so you can run it without the TUI and make your own UI with their agent back end. They also allow you to run it with a web front end so you can use that remotely without the need to ssh from your phone.",
          "score": 1,
          "created_utc": "2026-01-25 22:24:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1shf9h",
          "author": "Rude-Needleworker-56",
          "text": "Even just for seeing the thinking tokens and tool calls, it makes a case. in codex one simply stare at screen without knowing what is happening and you go back to some other screen soon distracting yourself. Seeing the thinking summary and tool calls keeps you in loop.\n\nSecondly you can  get a second opinion from other models like opus in a couple of clicks.\n\nFor some tasks (like writing , UI etc) sonnet is  better than openai models\n\nIf you are power user opencode brings tons of customisability and front end options\n\n(But make sure that you add a apply\\_patch tool)",
          "score": 1,
          "created_utc": "2026-01-26 09:33:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1y4bc9",
          "author": "Apprehensive_Half_68",
          "text": "Opencode has LSP and Oh My Opencode, those 2 things alone are enough imo. Also with OC you can swap out underlying LLMs without relearning the workflows, shortcuts, skills, MCPs, etc",
          "score": 1,
          "created_utc": "2026-01-27 02:59:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o273495",
          "author": "PsHohe",
          "text": "I think it mainly comes down to whether you want the freedom to switch provider without having to bother setting things up for every tool again. Opencode lets you switch model, even during a conversation. Vendor specific tools may let you do that, but only their models. Having used all the tools, I can say that for 95% of the tasks, there's really not a lot of difference. If you want to only use OpenAI, there's no difference, use whichever fits you better.  \nThe real advantage comes if you want the freedom to switch and experiment.",
          "score": 1,
          "created_utc": "2026-01-28 11:29:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ovc22",
          "author": "mike3run",
          "text": "You should do whatever you want just try it out... or not it's up to you¬†",
          "score": 1,
          "created_utc": "2026-01-25 20:57:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmefpw",
      "title": "What is your experience with z.ai and MiniMax (as providers)?",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/kukswiknpgfg1.png",
      "author": "mustafamohsen",
      "created_utc": "2026-01-25 09:14:43",
      "score": 23,
      "num_comments": 34,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qmefpw/what_is_your_experience_with_zai_and_minimax_as/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1p4f2s",
          "author": "TradeViewr",
          "text": "I had subscriptions with all providers and used all models, for professional coding only.\nGLM 4.7 is very good, at the level of Sonnet 4.5 but sooooo sloooowww.¬† Cannot work with that.¬† Minimax 2.1 is not good enough for my use case.¬† It nuked my codebase 2 times in 3 days I stopped using it immediately.\nHowever, their improvement is so impressing, that I believe that I will be probably using chinese models in a couple of months, if they keep improving their open source models at the same rate.\nI don't like Scam Altman's ClosedAI and all the packages with claude have heavy rate limits and I wont be paying 200$ for an ai subscription.¬† So I am waiting eagerly the next chinese big models because I think that they will be perfect for me if they match the current opus 4.5 at a lower price.",
          "score": 9,
          "created_utc": "2026-01-25 21:36:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1mkumr",
          "author": "MrBansal",
          "text": "Z.ai cheapest plan is slower compared to claude code cheapest plan. But it works . Certainly not good as claude. No experience of minimax",
          "score": 7,
          "created_utc": "2026-01-25 14:58:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1msqt3",
              "author": "mustafamohsen",
              "text": "By slower you mean lower tps, or service instability/disruption?",
              "score": 1,
              "created_utc": "2026-01-25 15:36:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1nsvkj",
                  "author": "awfulalexey",
                  "text": "Usually, if she gives an answer, it‚Äôs pretty quick. About 60-80 tokens per second. But to start getting this answer.......sometimes you have to wait.",
                  "score": 4,
                  "created_utc": "2026-01-25 18:11:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rdmh2",
          "author": "SynapticStreamer",
          "text": "z.ai was a struggle to get acclimated too, but now that I am, it's pretty great.\n\nIf you're not willing to change the way you do things, stay away from it. Like it or not, you're used to doing things the Anthropic way, and if you try to bring it to z.ai you're going to have a lot of issues.\n\nIt's a different tool, trained a different way, by different people, using different tools. It's not weird it requires you to change the way you've done things up until now.\n\nI still struggle every now and again, but once I ironed out the workflow I'm back to working confidently. I'm on the small base plan and max it out all the time. It's 3x the usage of the $20 Claude Pro plan I was on and even though I hit limits sometimes, it generally takes 3-4 hours, so I take a coffee break and come back and work.\n\nFor $28 it's a fuckin' steal.\n\nTake your time an invest heavily into learning how to use **sub agents**. They're the key to working with z.ai.\n\nIs it slow? Kinda. I find the `4.7-Flash` model is crazy slow. I try to delegate different sub agents to use different models. Like committing things to git doesn't require an insane amount of thought, so I use `4.7-flash` instead of `4.7` which saves on tokens, especially if you're committing a lot. Gaining context? Why do you need the equivalent of deepthink enabled just to get context about your project into the context window? Use a different less expensive agent. Doing that repeatedly saves you tons of usage--but like I said, it's been kinda slow lately because it's been getting much more popular.\n\nBut generally, I find it to be about the same as Sonnet 4.5. I'm generally not glued to the AI window anyways. I run long running commands and watch a movie. Who cares if it takes 5 minutes vs 8 minutes?\n\nGLM 4.7 is ranked as the 6th best LLM for coding in the world. Sonnet 4.5 is ranked as the first. The difference between them? 4.9% yet Claude cost me $240/yr, and GLM 4.7 cost me $28/yr.\n\n*So are you willing to spend 158% more for 4.9% performance?*",
          "score": 5,
          "created_utc": "2026-01-26 04:20:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1siffr",
              "author": "mustafamohsen",
              "text": "Thank you. Actually, the efficiency argument is irrefutable, in a sense. And yes, it's a steal anyway even if you use it just once or twice, and I don't care that much for speed as long as the service is stable and consistent\n\nBut I have no idea what \"doing things their way\" means. Would you give some insights?",
              "score": 1,
              "created_utc": "2026-01-26 09:42:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1wjaho",
                  "author": "SynapticStreamer",
                  "text": "With Claude you can generally have a conversation about your code base and say \"ok, let's implement those changes\" and the model is contextual enough to generally build what you want most of the time.\n\nGLM is a bit different. It requires more structure. I always use plan mode to converse with the agent about proposed changes that I want to make. Then I'll write them to file in a multi-phase implementation plan in manageable chunks so GLM doesn't confuse itself. Then I use sub agents. The main agent will pass only enough context for the sub agent to complete its task, so context is kept to a minimum and there's less of a chance for hallucination.",
                  "score": 1,
                  "created_utc": "2026-01-26 22:04:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1msusl",
          "author": "DistinctWay9169",
          "text": "Used both. Currently I am using the [Z.ai](http://Z.ai) Max plan. Much faster than the lit plan, and a much better experience overall (though it loses connection sometimes). Had a great experience with minimax regarding speed, but it is kind of stupid sometimes. I think minimax is made for speed and not for reasoning; it's kind of dumb for reasoning.",
          "score": 3,
          "created_utc": "2026-01-25 15:37:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o202jim",
          "author": "Malek262",
          "text": "I'm surprised by people who say that GLM 4.7 is excellent for coding and similar to 4.5. I don't know, honestly, my experience with it has been very bad. Maybe for simple tasks, okay, it's passable. But there's no comparison; on the contrary, I've used it more than once, trying to give it a chance for simple tasks, and it's full of bugs. I don't know, my experience with it, honestly, was bad. Okay, it's a good model for the price, but for simple things. As for comparing it to 4.5, there's a huge difference.",
          "score": 2,
          "created_utc": "2026-01-27 12:03:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27izzu",
              "author": "xmewa",
              "text": "Yeah, it gets lost very easily and struggles even with a lot of guardrails. Maybe it's okay for some autocomplete tasks. For me the worst thing is that it can be very wrong and very confident *at the same time*. So for now I left it to do the easy stuff, like driving the browser via chrome-devtools MCP.\n\nAs far as Opus 4.5 comparisons go, I really enjoy Codex and GPT-5.2.",
              "score": 1,
              "created_utc": "2026-01-28 13:16:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1lckra",
          "author": "adamhathcock",
          "text": "The cheapest z.ai is very slow for me.  I think Minimax was giving me better code as well as faster.  I‚Äôm primarily C# and late to the z.ai subscription.  Maybe they slow it down for us cheapskates.  Worth it for the low price still.\n\nI‚Äôm thinking of trying the low minimax sub too now.",
          "score": 3,
          "created_utc": "2026-01-25 09:46:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1llh3t",
              "author": "paperbenni",
              "text": "Their pricing page used to explicitly state that the cheapest plan is slower. I don't find it very slow though. It's slightly below Anthropic and miles above openAI in terms of speed, enough for my ability to verify its output to be the bottleneck (in most cases)",
              "score": 3,
              "created_utc": "2026-01-25 11:05:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1nfqq1",
                  "author": "adamhathcock",
                  "text": "After doing more today, I find it better.  Still worthwhile but maybe kilocode isn‚Äôt as good as opencode with it",
                  "score": 1,
                  "created_utc": "2026-01-25 17:16:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1msj5j",
          "author": "Visual_Weather_7937",
          "text": "GLM 4.7 is significantly worse compared to Opus.  Btw [z.ai](http://z.ai) is slow, and spending just $3 more is not worth it. I purchased the most expensive subscription and tried to refund it on the same day, but I had no luck. I have no experience with Minimax.  \nDON'T believe the hype surrounding GLM/z.ai; there are a lot of bots with referal links :)",
          "score": 3,
          "created_utc": "2026-01-25 15:35:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mtrsp",
              "author": "mustafamohsen",
              "text": "What $3 are you referring to?",
              "score": 1,
              "created_utc": "2026-01-25 15:41:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mv7ip",
                  "author": "Visual_Weather_7937",
                  "text": "about \"cheap\" sub in z ai",
                  "score": 2,
                  "created_utc": "2026-01-25 15:47:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1p5u9o",
          "author": "Bob5k",
          "text": "been using glm coding for long long time. I'm happy so far with glm, I'm happy aswell with the coding plan with tiny caveats as speed might be fluctuating across the day due to massive load glm coding plan currently has. \n\nSaying so, I'm more frequently using [synthetic](https://synthetic.new/?referral=IDyp75aoQpW9YFt) and their subscription these days as i just want \"unlimited\" concurrency (as much as your prompts quota will allow you to do within 5h basically) and speed is there. Especially for glm and minimax models. \nAlso i noticed that [minimax coding](https://platform.minimax.io/subscribe/coding-plan?code=HO46LCwAJ5&source=link) itself promises 100 prompts but they calculate this in a way of giving much much more than 100 prompts - as single prompt means 15 model calls and it seems that it's very efficiently calculated somehow - i tried to cap 100 prompts there with my usual workload and i was not able to do so (mainly webdevelopment). \nReflinks giving you additional discounts there - feel free to use them",
          "score": 2,
          "created_utc": "2026-01-25 21:42:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1n790w",
          "author": "Impressive_Job8321",
          "text": "How much does data retention and sovereignty factor into your choices?",
          "score": 1,
          "created_utc": "2026-01-25 16:39:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nmjlg",
          "author": "PayConstantAttention",
          "text": "The z.ai API is garbage",
          "score": 1,
          "created_utc": "2026-01-25 17:45:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nqwu7",
          "author": "Snoo_57113",
          "text": "For my usecase i tried both during the free period and chose minimax api for a month.",
          "score": 1,
          "created_utc": "2026-01-25 18:03:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1o75ni",
          "author": "richardlau898",
          "text": "I use minimax it‚Äôs great, surely not as good as Claude but I am generally satisfied",
          "score": 1,
          "created_utc": "2026-01-25 19:10:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1oikli",
          "author": "Zerve",
          "text": "GLM has really bad concurrency so if you use more than 1 agent at a time it will be very difficult to get much use out of it. You'd need to do a mix of 4.7 and 4.7 flash.",
          "score": 1,
          "created_utc": "2026-01-25 20:01:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qzivc",
          "author": "54tribes",
          "text": "with [synthetic.new](http://synthetic.new) you can access to both model. [Z.ai](http://Z.ai) cheapest plan is too slow, synthetic GLM 4.7 produced better result for me",
          "score": 1,
          "created_utc": "2026-01-26 02:58:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1t8ybc",
          "author": "BitcoinGanesha",
          "text": "I had experience with glm 4.7 at z.ai on max plan. The response speed is ok but time to first token is little slow.\n\nBut one of main problem with z.ai is same same like other providers. They start give you less quant model when you start actively using.\nI run 78 tasks in queue and after ~23 tasks the problems started. I started got words with errors. Some symbols was from non English language üò¢\nEvery task was done in sub agent with own context window. It was not context rot problem.\n\nBut if you vibe coding by hand I think you‚Äôll enough quotes and will not get same result as me.\n\nP.s. you can check cerebras.ai with payment for tockens. The speed is fantastic üëå but context window size is 120k and may be they compact count of experts üò¢",
          "score": 1,
          "created_utc": "2026-01-26 13:11:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wzrsi",
          "author": "TrajansRow",
          "text": "I‚Äòm using MiniMax; I find it a good balance of speed and performance.",
          "score": 1,
          "created_utc": "2026-01-26 23:23:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1xtm3n",
          "author": "iDonBite",
          "text": "I use the copilot sub and glm for research and small tasks so it doesn't consume my subscription limit. I mainly use gpt 5.2 codex because Gemini is cooked and Opus 4.5 is insanely expensive. GLM is extremely good at research, planning and doing  small tasks. I leave the big things to gpt.",
          "score": 1,
          "created_utc": "2026-01-27 02:00:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z8q93",
          "author": "EducationalBank5328",
          "text": "[z.ai](http://z.ai) for 6 euros a month is a steal. Sometimes it‚Äôs slow due to high API demand, but it gets the job done. I use it with OpenCode",
          "score": 1,
          "created_utc": "2026-01-27 07:41:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ldthi",
          "author": "No_Success3928",
          "text": "Why not both using synthetic?",
          "score": -1,
          "created_utc": "2026-01-25 09:58:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1q9wss",
          "author": "lundrog",
          "text": "I would recommend synthetic.new for both. Fast; private servers. I have a referral [hete](https://synthetic.new/?referral=UAWqkKQQLFkzMkY) \"Invite your friends to Synthetic and both of you will receive\n$10.00 for standard signups.\n$20.00 for pro signups.\nin subscription credit when they subscribe!\" \n\nGenerous requests and prices without a weekly cap.",
          "score": 0,
          "created_utc": "2026-01-26 00:48:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tf3ad",
          "author": "Competitive-Film9107",
          "text": "I've dropped my Claude Max 20x subscription, and now have a MiniMax subscription - do all my planning and specifications via OpenRouter to take advantage of using different models to pick apart my plans and specifications. Then once I have a solid spec, I pass it over to MiniMax to do the actual implementation with OpenCode in containers. \n\nUsing Opus 4.5 for everything prior, I could be a lot more loose with specifications and generally Opus would make a reasonable choice, where as I've seen MiniMax get lost and make strange decisions (given other instructions in the spec) or just stub something and defer it. That being said, everything I've had issues with, has been fixed by just adjusting my prompts or being more specific.",
          "score": 0,
          "created_utc": "2026-01-26 13:45:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lo8w6",
          "author": "WPDumpling",
          "text": "I run multiple Claude Code and OpenCode agents at a time, often with sub-agents & background tasks involved, all on a Z.ai Pro plan and I've never once hit a limit, whereas I was hitting them multiple times a day with a Claude Code subscription.\n\nAs for reliability: I've been using Z.ai since October and the only time I've had any issues is when they were under attack yesterday. Other than that, it's been rock-solid for me, even if it's not the fastest.\n\nConsidering it's like 1/12th the cost of Claude, with greater limits, I ***HIGHLY*** recommend Z.ai. If you want to use my referral code, I'll get some credit and you'll get an extra 10% off your first invoice: https://z.ai/subscribe?ic=SBFAJLK0FI",
          "score": -7,
          "created_utc": "2026-01-25 11:29:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnbmjs",
      "title": "Is there any practical reason to use spec tools in OC?",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/34qhsa7dynfg1.png",
      "author": "mustafamohsen",
      "created_utc": "2026-01-26 09:37:25",
      "score": 23,
      "num_comments": 32,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qnbmjs/is_there_any_practical_reason_to_use_spec_tools/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1spwzm",
          "author": "Ang_Drew",
          "text": "context efficiency, reduce ai hallucinations",
          "score": 5,
          "created_utc": "2026-01-26 10:49:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sq16f",
              "author": "Ang_Drew",
              "text": "only applied to tasks that is complex and or not fit the 50% context window to be completed.\n\nall other tasks that can be done in 1-2 shot(s) are not necessary",
              "score": 3,
              "created_utc": "2026-01-26 10:50:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1wkoty",
                  "author": "Fit-Palpitation-7427",
                  "text": "I have so much stuff I‚Äôm doing in one or 2 shots. Problem I‚Äôm scared to clear context and start fresh because I‚Äôm scared of having to re-explain what we are working on, whats the solution ? Asking it to generate a md file of the current page we are working on of the webapp and reference that every time I clear?",
                  "score": 1,
                  "created_utc": "2026-01-26 22:10:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1v801w",
              "author": "franz_see",
              "text": "Wouldnt the plan agent of opencode be more efficient? And would reduce just as much ai hallucinations?",
              "score": 1,
              "created_utc": "2026-01-26 18:37:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yd2y5",
                  "author": "Ang_Drew",
                  "text": "if your task relatively small. indeed plan mode is enough.\nif your tasks is heavy (scans mono repo, fix bug, patch code, add new feat at once) then your AI will most likely hallucinate if you dont create new session each tasks.",
                  "score": 1,
                  "created_utc": "2026-01-27 03:49:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ymzs8",
          "author": "Rhiz3K",
          "text": "GET SHIT DONE!\nhttps://github.com/glittercowboy/get-shit-done",
          "score": 3,
          "created_utc": "2026-01-27 04:51:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o225aje",
          "author": "PayTheRaant",
          "text": "Afaik, the Plan agent is not writing the plan anywhere. No markdown file. It just outputs its content in the context window and eventually the todo tool. And the todo tool does not survive exiting OpenCode.\n\nTaskmaster, OpenSpec and other are persisting the plan outside the session memory. And usually they do that in a formalism that is also meant to be read and review by humans (or other agents).\n\nPersonally I usually start an exploration session where I then draft one or more OpenSpec changes. I get those reviewed/discussed by my coworkers. And for each implementation I start a fresh session, working potentially on unrelated changes in parallel with worktrees.\n\nYou cannot do that with OpenCode plan agent afaik.",
          "score": 2,
          "created_utc": "2026-01-27 18:10:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o226vhb",
              "author": "mustafamohsen",
              "text": "Good point. But doesn‚Äôt the session persist after closing OC and restored via ‚Äò/sessions‚Äô command?",
              "score": 1,
              "created_utc": "2026-01-27 18:17:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o227kxz",
                  "author": "PayTheRaant",
                  "text": "The session yes. Not the content of the todo tool.\nAnd using the plan doesn‚Äôt support starting a new session, which I use for the implementation to skim all the fat and noise of my exploration when planning the change. The implementation can start with just the finalized plan.",
                  "score": 2,
                  "created_utc": "2026-01-27 18:20:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o274mp6",
              "author": "franz_see",
              "text": "Plan then save to markdown. Simple and without the bloat",
              "score": 1,
              "created_utc": "2026-01-28 11:41:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2830vl",
                  "author": "Difficult-Stand-6414",
                  "text": "Is that its default behaviour? or do you need to instruct it to do so?",
                  "score": 1,
                  "created_utc": "2026-01-28 14:59:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1srmj5",
          "author": "jellydn",
          "text": "Conceptual like what I wrote with Speckit for Claude Code https://github.com/jellydn/keybinder/blob/main/claudedocs/spec-kit-methodology-summary.md you want AI to AI know what to do and can be followed up on the next task as their has limited context windows. Is that useful for new project? Yes, kind of. You could read more if you want https://github.com/jellydn/my-ai-tools/blob/main/docs/learning-stories.md",
          "score": 1,
          "created_utc": "2026-01-26 11:04:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ss5z2",
              "author": "franz_see",
              "text": "But how is that any better than opencode‚Äôs plan?\n\nIt‚Äôs an honest question because I dont get it either. Tbh, I just chalked it up to ‚Äúthis is probably for non tech people‚Äù",
              "score": 2,
              "created_utc": "2026-01-26 11:09:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1stjkx",
          "author": "Hot_Dig8208",
          "text": "Well, lastweek I use openspec to do a big refactor. Actually the refactor was simple, just changing some api. But the problem, there was 66 classes that I need to refactor. And another problem was each class has its own logic.\n\nWhen I use open spec, it will clarify the requirement, and create docs about the design and the tasks. The task itself is written in a markdown files and similar to to do list. It has more than 66 to do list items but because it was written in a file, I can run another open code to do the task in parallel",
          "score": 1,
          "created_utc": "2026-01-26 11:20:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1t6hsq",
              "author": "awfulalexey",
              "text": "Any modern CLI tool can do the same. This is not news.",
              "score": 2,
              "created_utc": "2026-01-26 12:55:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1v7kgc",
              "author": "franz_see",
              "text": "Curious, what do you get if you prompt the same thing with PLAN and ask for a task list? Would it not give the same thing?",
              "score": 1,
              "created_utc": "2026-01-26 18:36:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xcvop",
                  "author": "Hot_Dig8208",
                  "text": "\nI think it will give the same thing. It doesn‚Äôt make the ai smarter. \n\nOpenspec just gives some convention on how to store the plan list. Its good  when working with the team, because now you have a single folder that contains all feature changes. Imagine that your team doesn‚Äôt agree on how to store the plan. It will scattered all over your repo. Some may write in /docs folder, others may be in /plans. It just headache to track",
                  "score": 1,
                  "created_utc": "2026-01-27 00:30:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ucvbl",
          "author": "dautinjo",
          "text": "It helps create detailed plans and specs that multiple AI agents can reference, eliminating the need to repeat yourself or lose important context between sessions. I find it particularly useful for complex features and refactors that exceed a single context window, or when coordinating multiple agents on the same feature. For example: one writing tests, another implementing, and a third performing code review. Without these specs, I'd waste time repeating myself and risk losing critical context.",
          "score": 1,
          "created_utc": "2026-01-26 16:24:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v7pne",
              "author": "franz_see",
              "text": "But couldnt you do the same with plan and then write it to a doc?",
              "score": 1,
              "created_utc": "2026-01-26 18:36:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1vu4sc",
                  "author": "dautinjo",
                  "text": "Absolutely. Those tools just have a strong opinion on how your specs should be organized. If you already have a workflow that works, you likely aren't missing much. However, writing any kind of docs is different from just using plan mode, which was the OP's actual question.",
                  "score": 1,
                  "created_utc": "2026-01-26 20:12:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1us735",
          "author": "realpieflavour",
          "text": "I have recently tried GSD (get shit done) with open code and I must say im impressed by the results. It feels like the other specops tools try and mimic enterprise level planning where I just need to GSD it still creates tasks and helps plan features but in a straightforward way.\n\nI know GSD was built for claude code but the latest version now supports open code as well",
          "score": 1,
          "created_utc": "2026-01-26 17:30:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uxdvv",
              "author": "yeswearecoding",
              "text": "Thanks for these comment, I think it's was I searched for long time üôè.",
              "score": 1,
              "created_utc": "2026-01-26 17:53:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1v6rqz",
          "author": "sn0n",
          "text": "The funny thing about the spec tools is they are organic‚Ä¶ naturally forming as documents get planned and parsed, sorted and distilled. Documenting the process creates the workflows, failing upward through iteration. Some things are universal,‚Ä¶ or something",
          "score": 1,
          "created_utc": "2026-01-26 18:32:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1y1ilr",
          "author": "toadi",
          "text": "I rolled my own. I read best way to structure a spec. Played around with and made if fit for my companies tech stack and legacy codebase. Also prompts need to be tweaked for the kind of model you use. For example when you write spec with anthropic, openai or even GLM for example you will need to tweak the prompt to work well.\n\nThis is the reason when you will dive into opencode the system prompts + their plan/build prompts are different depending the provider.",
          "score": 1,
          "created_utc": "2026-01-27 02:43:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ysag5",
          "author": "UseHopeful8146",
          "text": "Each is some variant of structured planning. Even the model inference is a basic ‚Äúthought‚Äù pattern template. You ask it to plan, plans take this shape, it ‚Äúdeep plans‚Äù by some variant of deconstruction or task defining or some combination thereof, and then sets off the job.\n\n\nThe difference is that the various spec tools provide an interactive approach that provides the template directly to you. The idea being you know enough about the project and the items involved to give a certain degree of input, and then it takes all that gives the model a more structured work plan (variety of adherence may differ) to send it off to do.\n\nEither way, whatever your opencode configuration may be, you still have to be familiar enough with the project to get a good plan in the first place. So it depends on the use case.\n\nDo you prefer collaborative planning effort with your AI?\n\n-or-\n\nDo you prefer a structured planning process?\n\nEither way, you do most of the ‚Äúplanning‚Äù if you‚Äôre getting anything done in a timely fashion, else you have a very clever configuration that you‚Äôre not sharing. \n\nAs my partner is very fond of saying, ‚Äúshit in, shit out.‚Äù",
          "score": 1,
          "created_utc": "2026-01-27 05:28:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26s4et",
          "author": "yazzino",
          "text": "Try oh-my-opencode - it have builtin spec/planning tools [https://github.com/code-yeongyu/oh-my-opencode](https://github.com/code-yeongyu/oh-my-opencode)",
          "score": 1,
          "created_utc": "2026-01-28 09:54:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ssdb4",
          "author": "jhartumc",
          "text": "superpowers",
          "score": 1,
          "created_utc": "2026-01-26 11:11:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}