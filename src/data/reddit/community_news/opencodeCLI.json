{
  "metadata": {
    "last_updated": "2026-01-30 16:57:45",
    "time_filter": "week",
    "subreddit": "opencodeCLI",
    "total_items": 20,
    "total_comments": 166,
    "file_size_bytes": 222822
  },
  "items": [
    {
      "id": "1qlunsk",
      "title": "GLM 4.7 removed from the free models",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/uenlpakmbcfg1.jpeg",
      "author": "marmoure",
      "created_utc": "2026-01-24 18:28:00",
      "score": 84,
      "num_comments": 42,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qlunsk/glm_47_removed_from_the_free_models/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1han0e",
          "author": "kapiteinklapkaak",
          "text": "GLM 4.7 was free on OpenCode for a limited time so the ycan test the model. That's basicly it",
          "score": 14,
          "created_utc": "2026-01-24 19:24:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2g714g",
              "author": "Background-Leg-6840",
              "text": "I just got GLM and everything else back\n\nhttps://preview.redd.it/lbc9tqa8tbgg1.png?width=617&format=png&auto=webp&s=87b9264de0a62654109b24f1d6e7daa321886213",
              "score": 1,
              "created_utc": "2026-01-29 17:48:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1j3dhk",
          "author": "robberviet",
          "text": "Also MiniMax. Damn it was good.",
          "score": 9,
          "created_utc": "2026-01-25 00:39:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ld3gl",
              "author": "tigerbrowneye",
              "text": "And it‚Äôs super fast and very capable. Rushing through my issues‚Ä¶",
              "score": 1,
              "created_utc": "2026-01-25 09:51:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1o38xp",
              "author": "luongnv-com",
              "text": "Yeah, that‚Äôs my go to opencode too",
              "score": 1,
              "created_utc": "2026-01-25 18:54:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1h7uqq",
          "author": "disgruntledempanada",
          "text": "The ride is over.\n\nMight pick up a Zai subscription (it's super cheap) but as a hobbyist just tinkering with stuff and using it very sporadically I think my interest just took a hit.",
          "score": 19,
          "created_utc": "2026-01-24 19:12:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1l3qt3",
              "author": "ProfessorSpecialist",
              "text": "I bought the lite subscription for zai and must say i dont know what i expected. Its dirt cheap, and its impressive what it can achieve at its best. But on average its painfully slow and stupid af. It regularly fails read write toolcalls, corrupts files even at 20% context usage and is absolutely garbage at debugging. But to be fair, i use opus at work, so my point of reference is skewed.",
              "score": 3,
              "created_utc": "2026-01-25 08:28:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ral46",
                  "author": "Embarrassed_Egg2711",
                  "text": "I bought a higher end sub and been very happy so far.",
                  "score": 1,
                  "created_utc": "2026-01-26 04:01:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1m35g5",
              "author": "shooshmashta",
              "text": "You lost interest because it stopped being free?",
              "score": 1,
              "created_utc": "2026-01-25 13:21:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1mjdzw",
                  "author": "slayyou2",
                  "text": "Consumers in a nutshell",
                  "score": 1,
                  "created_utc": "2026-01-25 14:51:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1nwhv3",
                  "author": "whimsicaljess",
                  "text": "there's a certain tradeoff with these models.\n\nif you have a very capable model, it becomes almost always worth using even at relatively high cost because saving even a small percentage of human labor is worth a lot.\n\nif you have a very stupid model, it becomes almost never worth using even at a relatively low cost because it doesn't meaningfully decrease (or worse, increases) the amount of human labor required to complete a task. \n\nGLM4.7 on z.ai is, imo, in the second category. it might be worth using when free for very low priority tasks that you don't particularly care about. but for anything else, it's not worth trying- and it's certainly not worth paying for.",
                  "score": 1,
                  "created_utc": "2026-01-25 18:26:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1gyid1",
          "author": "Background-Leg-6840",
          "text": "Why is this? I am also missing Grok Code Fast. They went away after I updated the open-code desktop app.\n\nhttps://preview.redd.it/myc7k5lfccfg1.png?width=290&format=png&auto=webp&s=8bf441c2cc0e7cb6aa76838f351bb2d3c6ea3b89",
          "score": 6,
          "created_utc": "2026-01-24 18:32:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eqad6",
              "author": "Just-Veterinarian397",
              "text": "Big Pickle receives too much requests. So it is not usable at all.  \nGPT-5 Nano's quality is so so but even it became paid",
              "score": 2,
              "created_utc": "2026-01-29 13:42:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hh290",
                  "author": "Background-Leg-6840",
                  "text": "Amen, it should not take an hour for one request \n\nhttps://preview.redd.it/aaz0iphkvcgg1.png?width=274&format=png&auto=webp&s=b6035ad3394a0f9306c6765200d4891d5442f719",
                  "score": 1,
                  "created_utc": "2026-01-29 21:23:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2g6wic",
              "author": "Background-Leg-6840",
              "text": "I just got everything back \n\nhttps://preview.redd.it/nvjdkhn4tbgg1.png?width=617&format=png&auto=webp&s=720a88b8820b9cd64bd8fadee5ac5f8b591acf0a",
              "score": 1,
              "created_utc": "2026-01-29 17:48:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1kr25p",
              "author": "No_Success3928",
              "text": "Elons getting rid of that one too",
              "score": -1,
              "created_utc": "2026-01-25 06:41:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1hw8vc",
          "author": "smile132465798",
          "text": "There used to be zero free models. At least we still had Big Pickle. Now I‚Äôm just waiting for the next DeepSeek drop next month.",
          "score": 4,
          "created_utc": "2026-01-24 21:04:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2a9l8z",
              "author": "Dank-13",
              "text": "I read somewhere that it is as good as opus 4.5, with multiple tools and a highly coding-focused approach. idk if that good of a model will be open-source but if it comes then at least it's gonna be better than glm 4.7... I hope they'll provide it at a good price like Zai",
              "score": 1,
              "created_utc": "2026-01-28 20:44:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1k22gm",
          "author": "Disastrous-Mix6877",
          "text": "What about grok code fast? And minimax? They‚Äôre all gone now?",
          "score": 2,
          "created_utc": "2026-01-25 03:53:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1hwury",
          "author": "InfraScaler",
          "text": "Yup, it's gone. The good thing though is that the GLM Coding Plan goes as cheap as $2.40/mo if you pay for a year (or $3 if you pay monthly). \n\nAlso, with another subscriber's link you get 10% extra credits. Here's mine if anyone wants those extra 10% credits: [https://z.ai/subscribe?ic=WBMQNQBVIS](https://z.ai/subscribe?ic=WBMQNQBVIS)",
          "score": 5,
          "created_utc": "2026-01-24 21:07:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1i6yqc",
              "author": "DistinctWay9169",
              "text": "It is $6 monthly, $3 is only the first time.",
              "score": 4,
              "created_utc": "2026-01-24 21:55:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ia0wg",
                  "author": "InfraScaler",
                  "text": "True! But if you get the full year it becomes effectively $2.40/mo for 12 months, which is insane.",
                  "score": 5,
                  "created_utc": "2026-01-24 22:10:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ia61u",
                  "author": "WPDumpling",
                  "text": "If you buy an entire year of the cheapest plan, the first-time discount makes it work out so you're effectively paying $2.40/month",
                  "score": 2,
                  "created_utc": "2026-01-24 22:10:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1j8v07",
          "author": "lmagusbr",
          "text": "Not only that, but Big Pickle is being throttled now too.\n\nI think it's because of all the people using Clawdbot.",
          "score": 1,
          "created_utc": "2026-01-25 01:09:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1k5cup",
          "author": "wiroaj",
          "text": "Is GLM 4.7 a viable alternative to Gemini 3? I'm currently using Claude Opus 4.5 via Google Antigravity, but due to strict quota limits, I tried switching to Gemini 3. Unfortunately, I found the performance disappointing compared to Opus. Would GLM 4.7 be a better fit?",
          "score": 1,
          "created_utc": "2026-01-25 04:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1l3mec",
              "author": "shaonline",
              "text": "GLM 4.7 is an ok Sonnet 4.5 replacement, nowhere near Opus though.",
              "score": 3,
              "created_utc": "2026-01-25 08:27:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1le1f2",
              "author": "No_Success3928",
              "text": "Glm is less likely to lay waste to your codebase üòÜ",
              "score": 1,
              "created_utc": "2026-01-25 09:59:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ljb1q",
          "author": "Deep_Category_5013",
          "text": "The encoding capabilities of this model are quite good.",
          "score": 1,
          "created_utc": "2026-01-25 10:46:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29r6wk",
          "author": "Boring-Ad-5924",
          "text": "Is it worth paying for OpenCode Zen? Or should I sub somewhere else?",
          "score": 1,
          "created_utc": "2026-01-28 19:22:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2av4mw",
              "author": "marmoure",
              "text": "Don't, I paid for the 20$ offer one prompt 0.5$   \nI would go for the GLM 3$ subscription.",
              "score": 1,
              "created_utc": "2026-01-28 22:18:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2cuntc",
                  "author": "alexeiz",
                  "text": "Aren't you paying for API tokens with Opencode Zen?  It's not a subscription.  It's similar to Cline and Kilocode (or Openrouter).  Opencode Black is a subscription though.",
                  "score": 1,
                  "created_utc": "2026-01-29 04:50:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2kq33m",
                  "author": "Squale279",
                  "text": "Hi,\nCould you please explain to me how to use GLM subscription with opencode?",
                  "score": 1,
                  "created_utc": "2026-01-30 09:38:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1h1h5w",
          "author": "Suitable-Program-181",
          "text": "2 days ago if im not wrong.\n\nGroq and gpt nano where so bad not even free should be shared again. Big pickle is solid if you know what you need or have clear instructions. If they keep it free will be solid.",
          "score": 1,
          "created_utc": "2026-01-24 18:44:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h6cvv",
              "author": "sevindi",
              "text": "Big Pickle is GLM 4.6",
              "score": 6,
              "created_utc": "2026-01-24 19:05:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1iajxt",
                  "author": "RiskyBizz216",
                  "text": "I don't think it is. Big Pickle doesn't have a reasoning channel (or <think> tags) but GLM 4.6 does.\n\nI suspect its GPT 4o\n\nTested (kinda confirmed):  \nIt says its training cutoff is 2024. \n\nAnd if you straight up ask it - it'll claim to be Claude Sonnet 3.5 \n\nI told it \"Claude Sonnet 3.5 was deprecated, not hosted anywhere online\"\n\nIt had an identity crisis, and went down a list of 2024 models until it honed in on GPT-4o.",
                  "score": -1,
                  "created_utc": "2026-01-24 22:12:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1h8f9o",
                  "author": "Suitable-Program-181",
                  "text": "Im not sure, I read that yesterday and thats why I tested it and I cant complaint.\n\nI have an lenovo laptop with athlon silver cpu... tiny 10 is laggy, mint is laggy so i asked pickle just for fun if we can cook something and went straight for vulkan lol \n\nI know is not a premium model from a premium provider but gpt 5 and grok are \"premium\" providers and the difference was huge. Gpt I had to keep asking him to do something besides talking and grok feels like a free model charged as premium... classic elon scams.",
                  "score": -1,
                  "created_utc": "2026-01-24 19:14:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1isql3",
          "author": "ZeSprawl",
          "text": "It‚Äôs removed but the pay version of GLM 4.7 is like 5x the speed, and pretty cheap.",
          "score": 1,
          "created_utc": "2026-01-24 23:44:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqmdcs",
      "title": "Kimi is FREE for a limited time in OpenCode CLI!",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qqmdcs/kimi_is_free_for_a_limited_time_in_opencode_cli/",
      "author": "jpcaparas",
      "created_utc": "2026-01-29 21:55:41",
      "score": 83,
      "num_comments": 33,
      "upvote_ratio": 0.98,
      "text": "https://preview.redd.it/2t904wj91dgg1.png?width=581&format=png&auto=webp&s=ec530b8a251fccf7440a64d426c59d2e846c50d0\n\nYou heard that right boys and gals!\n\nEdit: Kimi K2.5 *specifically*.\n\nEdit 2: Check out the benchmarks and capabilities [here](https://generativeai.pub/moonshots-kimi-k2-5-can-spawn-100-ai-agents-to-do-your-work-5c7f0bd90a88?sk=f68e605853d40ad859c9d5507b9e0749).\n\nEdit 3: [Dax](https://x.com/thdxr/status/2016993820940943623?s=20) stands by Kimi K2.5, says it's at par with Opus 4.5.\n\nEdit 4: Here's my longform, non-paywalled review after trying it out for the last 24 hours (with a solid recommendation from OpenCode's co-creator, Dax):\n\n(Obviously, try it out for free first before you make the switch to a paid provider, either with Zen, Chutes, NanoGPT, or Synthetic)\n\n‚û°Ô∏è¬†[Stop using Claude‚Äôs API for Moltbot (and OpenCode)](https://jpcaparas.medium.com/stop-using-claudes-api-for-moltbot-and-opencode-52f8febd1137)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qqmdcs/kimi_is_free_for_a_limited_time_in_opencode_cli/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2hzt1j",
          "author": "jpcaparas",
          "text": "https://preview.redd.it/a4d728zubdgg1.png?width=628&format=png&auto=webp&s=3d5a1573eefed1803f4df89a7a9daf21d63e17a0\n\nThis is all the validation you need right now.",
          "score": 15,
          "created_utc": "2026-01-29 22:55:05",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2j52wk",
              "author": "Sensitive_Song4219",
              "text": "For debugging I'm finding Kimi 2.5 a definite step down from Opus / Codex-High. Feels more like Sonnet/Codex-Medium/GLM4.7 in that regard, at least in the projects I've tried it in so far. (I don't think I can do without at least a small subscription to a frontier-model - at least not quite yet!)\n\nBut  *Kimi K2.5 Free* via OpenCode-Zen is quite fast, I assume Moonshot's doing the hosting for them. Impressive. (Edit: poster below correctly mentions the X post says *fireworks* is hosting - they're doing a good job!)",
              "score": 4,
              "created_utc": "2026-01-30 02:39:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2kqio6",
                  "author": "lofuller",
                  "text": "Fireworks is providing the inference (Dax confirmed)",
                  "score": 2,
                  "created_utc": "2026-01-30 09:42:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2jl483",
                  "author": "jmhunter",
                  "text": "ya, agree...",
                  "score": 1,
                  "created_utc": "2026-01-30 04:12:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2jfpna",
              "author": "philosophical_lens",
              "text": "I‚Äôm looking forward to trying K2.5, but that post is marketing, not validation.",
              "score": 5,
              "created_utc": "2026-01-30 03:40:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ht1mo",
          "author": "Metalwell",
          "text": "I just had signed up for free tier in moonshot, imma cancel it now",
          "score": 4,
          "created_utc": "2026-01-29 22:20:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iao1l",
          "author": "baldreus",
          "text": "It‚Äôs insanely cheap on chutes too.  300 requests a day on the lowest tier for 3$ a month.  You can‚Äôt beat free but the advantage with chutes is privacy if you care (which may be a concern if pitching a provider at work)",
          "score": 3,
          "created_utc": "2026-01-29 23:52:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2idghp",
              "author": "minato_shikamaru",
              "text": "Is chutes a good provider, some people are telling me not to use third party providers because they don't run well.¬†",
              "score": 2,
              "created_utc": "2026-01-30 00:08:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2in5cv",
                  "author": "baldreus",
                  "text": "I've only been using it for a couple of days since this model came out, so take this with a grain of salt. As long as the model is hot loaded (in other words it‚Äôs actively being used by others), it seems to be pretty responsive. If the model is cold and hasn't been used for a while, it does take some time for it to spin up instances. Yes there could be occasions when you have to wait a bit but in my experience 98% of the time it's been fast. Let's wait and see what happens when everyone piles on once the hype reaches critical mass. \n\nThat said it's definitely way better than free tier openrouter models that are insanely rate limited and slow - so, so far it's been a great experience.",
                  "score": 3,
                  "created_utc": "2026-01-30 01:00:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2i8an9",
          "author": "ECrispy",
          "text": "how do I find the free model? /connect doesnt list it",
          "score": 2,
          "created_utc": "2026-01-29 23:40:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2k4tr2",
              "author": "deadcoder0904",
              "text": "https://github.com/anomalyco/opencode/issues/11261\n\ntl;dr do `opencode models --refresh` before launching `opencode`",
              "score": 6,
              "created_utc": "2026-01-30 06:31:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2k591k",
                  "author": "ECrispy",
                  "text": "thanks",
                  "score": 1,
                  "created_utc": "2026-01-30 06:35:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ik91x",
              "author": "____HakunaMatata____",
              "text": "/models",
              "score": 1,
              "created_utc": "2026-01-30 00:44:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2hsr7s",
          "author": "krimpenrik",
          "text": "Nice",
          "score": 1,
          "created_utc": "2026-01-29 22:19:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hukiu",
          "author": "SilverMethor",
          "text": "Nice!",
          "score": 1,
          "created_utc": "2026-01-29 22:28:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hwjb4",
          "author": "MeasurementPlenty514",
          "text": "I was a mega-slut for mimo 2 when free when I needed to just munch and gobble tokens.",
          "score": 1,
          "created_utc": "2026-01-29 22:38:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2idfma",
          "author": "ianxiao",
          "text": "So no thinking ?",
          "score": 1,
          "created_utc": "2026-01-30 00:08:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2idiek",
          "author": "hexa01010",
          "text": "Sweet! I agree this model is awesome first time we have real SOTA with open models finally!\n\nI think Kimi also increased their quotas a lot I went from like 10% yesterday to 1% today on the weekly",
          "score": 1,
          "created_utc": "2026-01-30 00:08:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2j3w8d",
              "author": "Hoak-em",
              "text": "Yeah they switched to token-based (instead of API-request based) and have a limited-time 3x for the higher-tier plans (I'm not sure if the lower-tier one has it too). Go ham with it now I figure -- also it's fast :3",
              "score": 1,
              "created_utc": "2026-01-30 02:33:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2jhwkf",
          "author": "LtCommanderDatum",
          "text": "Error: 401 Unauthorized",
          "score": 1,
          "created_utc": "2026-01-30 03:52:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ji0te",
              "author": "jpcaparas",
              "text": "It got reddit hugged lol",
              "score": 3,
              "created_utc": "2026-01-30 03:53:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2jm7n6",
          "author": "kr_roach",
          "text": "Provider of Kimi 2.5 free model is moonshot ai??",
          "score": 1,
          "created_utc": "2026-01-30 04:19:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l79cz",
              "author": "mizzuri",
              "text": "Fireworks AI",
              "score": 1,
              "created_utc": "2026-01-30 12:02:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2i6mpr",
          "author": "kpgalligan",
          "text": "Just bought zen credits. Have to see if I can use some extra gas here...",
          "score": 1,
          "created_utc": "2026-01-29 23:31:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2i73n0",
              "author": "jpcaparas",
              "text": "The token pricing itself will get you a lot of range for that fill.",
              "score": 1,
              "created_utc": "2026-01-29 23:33:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ij5a0",
                  "author": "kpgalligan",
                  "text": "I went to look at the dashboard after poking around for a while, and the chart struggled to show <$1 of usage :)\n\nWill try to use free, but not sure it'll matter all that much. I'm not exactly hammering it.",
                  "score": 2,
                  "created_utc": "2026-01-30 00:38:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ic1yk",
          "author": "atkr",
          "text": "why advertise it?",
          "score": 0,
          "created_utc": "2026-01-30 00:00:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmt0ib",
      "title": "Sharing my OpenCode config",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qmt0ib/sharing_my_opencode_config/",
      "author": "filipbalada",
      "created_utc": "2026-01-25 19:38:43",
      "score": 67,
      "num_comments": 19,
      "upvote_ratio": 0.96,
      "text": "I‚Äôve put together an OpenCode configuration with custom agents, skills, and commands that help with my daily workflow. Thought I‚Äôd share it in case it‚Äôs useful to anyone.üòä\n\nhttps://github.com/flpbalada/my-opencode-config\n\nI‚Äôd really appreciate any feedback on what could be improved. Also, if you have any agents or skills you‚Äôve found particularly helpful, I‚Äôd be curious to hear about them. üòä Always looking to learn from how others set things up.\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qmt0ib/sharing_my_opencode_config/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o1olvwh",
          "author": "msrdatha",
          "text": "Respect and Thanks, for taking time to prepare and share these configuration details. \n\nIt has lot of valuable contents, and I am sure, will need to spend at least a week even to fully understand some of them on how to use.\n\nReally appreciate the thought on \"Always looking to learn\" part. Just my two cents, based on my experience and the config - you seems to be using ollama. If you would like to try with llama.cpp, it might give an edge over ollama. It seems much better optimized in using system resources, and also gets support for newer models much sooner than ollama.",
          "score": 10,
          "created_utc": "2026-01-25 20:16:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sjpm4",
              "author": "filipbalada",
              "text": "Thank you! I'd love to hear more of your ideas and thoughts. Great point about llama.cpp. I'll definitely give it a try. The better resource optimization sounds promising, especially for experimenting with newer models. :))",
              "score": 5,
              "created_utc": "2026-01-26 09:54:38",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o28eq7y",
              "author": "spaceSpott",
              "text": "Isn't vllm better than Llama.cpp? Honest question",
              "score": 1,
              "created_utc": "2026-01-28 15:52:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o29hu1d",
                  "author": "msrdatha",
                  "text": "I did not try running vllm yet. My understanding is that vllm performs better, when there are multiple gpus. On single system (Mac or 1 GPU Linux) llama.cpp is more optimized. Please correct me if you happened to have more experience on this.",
                  "score": 1,
                  "created_utc": "2026-01-28 18:41:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1olqoy",
          "author": "lundrog",
          "text": "Maybe a readme?",
          "score": 5,
          "created_utc": "2026-01-25 20:15:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sk061",
              "author": "filipbalada",
              "text": "Good point. :)) Since this was just my personal config I decided to share, I hadn't thought about documentation initially. But you're right. I've now added a readme and marked the repository with an MIT license. Thanks! üôè",
              "score": 2,
              "created_utc": "2026-01-26 09:57:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1opu9e",
          "author": "jmreicha",
          "text": "How do you interact with and leverage the agent configs? Looks like some interesting personas for specific use cases.",
          "score": 2,
          "created_utc": "2026-01-25 20:33:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sks4t",
              "author": "filipbalada",
              "text": "Thank you for your question. :)) I'm still trying to find the ideal workflow to automate. What I've found useful is to start developing a feature with the requirements analyzer agent and save the content/context to a GitHub issue. After I've designed the requirements document, I use it with the \"plan\" agent. Then I loop over the implementation. I personally still handle pull requests, the implementation, and try to keep them as small as possible so that my team can easily review them. During development, I use all agents like reviewer, deep thinker, and code simplifier based on what I consider I need. I also use the git commit agent a lot, that leverages shell scripts so that committing is quite fast. :)) ",
              "score": 2,
              "created_utc": "2026-01-26 10:04:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1z9j1i",
                  "author": "silopolis",
                  "text": "I very much like this idea of keeping the issue in the loop to both control and document the process! üëç",
                  "score": 2,
                  "created_utc": "2026-01-27 07:48:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1p9fyy",
          "author": "bagvix",
          "text": "Thank you. I am a beginner to opencode will learn it by looking your config files.",
          "score": 2,
          "created_utc": "2026-01-25 21:58:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sj8vv",
              "author": "filipbalada",
              "text": "Feel free to share your thoughts or ideas :)) ",
              "score": 2,
              "created_utc": "2026-01-26 09:50:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1qmwsh",
          "author": "jellydn",
          "text": "Could you explain why and how you set up your system? Like mine here http://ai-tools.itman.fyi/",
          "score": 2,
          "created_utc": "2026-01-26 01:54:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1pcwfj",
          "author": "gia_rox",
          "text": "Are the skills there all created by the skill creator agent? Just wondering the usefulness of it",
          "score": 1,
          "created_utc": "2026-01-25 22:13:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sl1xw",
              "author": "filipbalada",
              "text": "I personally design and maintain the prompts, but the skill creator helps a lot. Before OpenCode existed, I maintained a personal 'second brain'. A collection of notes on programming patterns, product management frameworks, and psychology principles... I found it valuable to transform these notes into skills that the AI can execute (skill creator helped a lot), making my accumulated knowledge actionable during coding sessions. :)) ",
              "score": 2,
              "created_utc": "2026-01-26 10:06:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1x6kxi",
          "author": "robertmachine",
          "text": "it seems the agents/skill-creator.md is for claude",
          "score": 1,
          "created_utc": "2026-01-26 23:58:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlqj0q",
      "title": "Benchmarking with Opencode (Opus,Codex,Gemini Flash & Oh-My-Opencode)",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/7c0g6rjkjbfg1.png",
      "author": "tisDDM",
      "created_utc": "2026-01-24 15:54:37",
      "score": 62,
      "num_comments": 31,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qlqj0q/benchmarking_with_opencode_opuscodexgemini_flash/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1g9k85",
          "author": "kshnkvn",
          "text": "Subagents are not about saving context in general, but about avoiding context bloat.  \nWhen each agent and subagent strictly performs its specific task and each of them has only the information they need in their context, the quality of generation is much higher.",
          "score": 12,
          "created_utc": "2026-01-24 16:43:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h5hc8",
              "author": "tisDDM",
              "text": "I agree to a certain point. That's the theory. \n\nIn the last two week I read a hell of a lot of logs. If you see how easy the context bloats, that's crazy. Every tool call counts because it is resending the full conversation history. I've seen models cross checking sub agents results causing more context bloat than doing it themselves. And much more. Nearly impossible to switch it off by prompting.",
              "score": 0,
              "created_utc": "2026-01-24 19:02:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ipjve",
                  "author": "aeroumbria",
                  "text": "My personal theory is that every handover point (whether it is task delegation / return or context compaction) is a potential point of failure, and whichever method that can minimise handover failure risks will work better. This kind of aligns with my observation that most orchestrator workflows seem to offer no benefit over simple plan -> build, unless the orchestrator is passing continuously  curated plans instead of ad-hoc task prompts to the subagent.",
                  "score": 1,
                  "created_utc": "2026-01-24 23:28:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1idpws",
                  "author": "kshnkvn",
                  "text": ">Every tool call counts because it is resending the full conversation history.\n\nNope. Only final output. Moreover, you can write a subagent so that it does not respond at all, just performs some action. And that's it.\n\n>I've seen models cross checking sub agents results\n\nSounds like a prompt issue, nothing more. I occasionally encounter this problem, but it can be solved.",
                  "score": 1,
                  "created_utc": "2026-01-24 22:27:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1g968z",
          "author": "nicklazimbana",
          "text": "which one was the most powerfull",
          "score": 3,
          "created_utc": "2026-01-24 16:41:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h0zth",
              "author": "tisDDM",
              "text": "Well - depends on what you need. This benchmark doesn't tell which one performs best. It tells which one is the most efficient. But it might help you pick the right tool for your task.\n\nFrom that point of view I would go with Codex if it delivers a fine solution for the task. If you're running on API and need something efficient, Gemini Flash with DCP is great. If money and quota don't matter, Opus with DCP is the expensive swiss army knife.",
              "score": 2,
              "created_utc": "2026-01-24 18:42:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1hln08",
                  "author": "Michaeli_Starky",
                  "text": "Gemini Flash and Pro are unusable. They go crazy very fast on real world tasks with massive context looping some dumb ass tokens\n\nhttps://preview.redd.it/rhpnytpuucfg1.png?width=410&format=png&auto=webp&s=11109819d1796bb8e842b5fb38302b094e22f93c",
                  "score": 1,
                  "created_utc": "2026-01-24 20:14:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1gi728",
          "author": "Groundbreaking-Mud79",
          "text": "I never really see any usefulness in a tool like Oh-my-opencode. I don‚Äôt know why, but to me it just seems like it consumes a lot more tokens for very little improvement.",
          "score": 3,
          "created_utc": "2026-01-24 17:21:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1hm8iq",
              "author": "Michaeli_Starky",
              "text": "It's as good as the task given. You need to be mindful when to use it over the normal Plan or even directly build modes.",
              "score": 1,
              "created_utc": "2026-01-24 20:17:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1kg6sj",
                  "author": "Groundbreaking-Mud79",
                  "text": "I'd love to hear your use cases! I'm also learning how to use these tools. Do you have any examples?",
                  "score": 1,
                  "created_utc": "2026-01-25 05:22:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1g7vr7",
          "author": "Big-Coyote-3622",
          "text": "Thanks, quantitative analysis approach is something I was looking for evaluating some of my modification to opencode/omo as well, especially as sometimes I use deepseek/glm/minimax/qwen models‚Ä¶ I think another good metric especially for calculating efficiency is token/api cost per full test run, if I find some time I will try to do a PR",
          "score": 2,
          "created_utc": "2026-01-24 16:35:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h2st2",
              "author": "tisDDM",
              "text": "I'd really appriciate that. Especially comparisions with all the other models outthere, a result db, testing UI Code and averaging over multiple runs are missing.",
              "score": 1,
              "created_utc": "2026-01-24 18:50:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1hmi61",
              "author": "Michaeli_Starky",
              "text": "The analysis is statistically incorrect considering the huge number of factors.",
              "score": 1,
              "created_utc": "2026-01-24 20:18:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gdojq",
          "author": "MissingHand",
          "text": "So based on this, what‚Äôs in #1,2, and 3 spot",
          "score": 2,
          "created_utc": "2026-01-24 17:01:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h1vxw",
              "author": "tisDDM",
              "text": "As said before: If it does the trick - codex is great and most efficient and Gemini Flash is cheap and got lots of quota over antigravity.",
              "score": 1,
              "created_utc": "2026-01-24 18:46:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1hn6du",
          "author": "Michaeli_Starky",
          "text": "Solving the task once is statistically incorrect even for deterministic computational environments. LLMs are non-deterministic by their nature. You would need 100+ runs per each category at VERY least for a proper study.",
          "score": 2,
          "created_utc": "2026-01-24 20:21:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1hjff7",
          "author": "MakesNotSense",
          "text": "I think we need more efforts to create systems for data collection and quantitative testing by users. Too many projects are vibing their way to a good idea, and falling short because testing and data collection remain a complex challenge. \n\n  \nAn automated agentic system that collects, analyzes, and reports on agent and model performance seems like it should be a top priority for OpenCode. \n\nWe already have most of the tools; session data, session read/search/info tools in OMO. With direct integration into OpenCode, and a workflow, you could tasks something like Grok fast or Gemini Flash to churn through a dataset to extract and consolidate information on actual workloads. \n\nImagine, you have a release, users produce data, return data to main dev, gets processed by a main dev agentic workflow, report produced, report used to generate spec, spec used to implement PR. The optimization of a project gets automated by real-world performance. Even the data collection system itself could be built for automated optimization.",
          "score": 1,
          "created_utc": "2026-01-24 20:04:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1iazsi",
          "author": "Codemonkeyzz",
          "text": "I used  Oh-my-opencode plugin before then later i dropped it.  Opencode's default  Plan and Build agents seems a lot more efficient  in terms of token and cost. I wonder  what exactly  oh-my-opencode plugin does well ? It's obviouslly not efficient with time and token cost, so  is  it about accuracy ? Does it have some prompts that produce more accurate output ?",
          "score": 1,
          "created_utc": "2026-01-24 22:14:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1n8hlf",
          "author": "rothnic",
          "text": "I at first thought oh my opencode was useful, but just keep running into what you are seeing. The most useful part of it is more in regard to some of the tooling that is added to help keep an agent running. Ralph loop, todo continuation, etc. I also noticed that it doesn't seem to spawn subagents very often and when it does it seems to not handle it that well.\n\nPersonally, I think there isn't a ton to get out of an orchestrating agent being the orchestrator because the orchestrator is inherently error prone and likely to suffer the same issues as using an agent regularly. Eventually as the context grows, the agent is less likely to do the things you tell are important. It just isn't ever going to happen consistently when the agent is managing everything. It starts great, but eventually will break down.\n\nI've been working with various orchestrators that leverage beads and deterministic state to execute workflows around centrally planned and broken down work and get much better results from this approach. I've also been working on my own state-machine based orchestration layer as well that is getting close to being usable. Basically, the idea is to have a state machine based orchestration layer that orchestrates the agents and when needed can leverage an agent to recover from bad states, etc.",
          "score": 1,
          "created_utc": "2026-01-25 16:45:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wwx89",
          "author": "Express-Peace-4002",
          "text": "That's my guy!",
          "score": 1,
          "created_utc": "2026-01-26 23:09:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zcaub",
          "author": "datosweb",
          "text": "Buen√≠simo el benchmark, se agradece el laburo de postear los datos crudos. Me qued√© pensando en la latencia de **Gemini Flash**. Es un ca√±o para tareas de bajo costo, pero a veces siento que en el razonamiento l√≥gico de *multistep* se queda un poco atr√°s comparado con Opus, sobre todo cuando el context window se empieza a llenar de basura.\n\nMe llama la atenci√≥n que los resultados de *tokens por segundo* var√≠en tanto en el test de c√≥digo complejo. ¬øUsaste algun tipo de **quantizaci√≥n** espec√≠fica para correr local o es todo via API pura? Pregunto porque a veces el cuello de botella no es el modelo en s√≠, sino c√≥mo el CLI maneja el streaming de los buffers de memoria cuando la rsta es muy larga.\n\nCapaz me equivoco, pero me da la sensasi√≥n de que en ciertos lenguajes menos comunes el Flash alucina un toque m√°s que el restoo. ¬øNotaste algun patron de errores sint√°cticos en Python vs Rust, o maso menos performan igual en tdos los casos?",
          "score": 1,
          "created_utc": "2026-01-27 08:13:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zfhzg",
              "author": "tisDDM",
              "text": "I did not perform so many tests - this benchmark I have written to test my subagent plugin. The comparison of the models I did out of pure curiosity. \n\nAll models are used from cloud offerings. \n\n\\- Flash from Gemini API\n\n\\- Codex from Azure\n\n\\- Opus from  Google Antigravity (this might have caused some delay)\n\nIt is fairly easy to use smaller models - even Devstral2Small - if the tasks are cut in smaller pieces. (\"If it fails, the task was to big\") This is what I deterministically did with my subagent plugin. \n\nFlash is great ( and so are Codex-Mini or Devstral2) if the task is small enough. Never expect it to get it like Opus. It is not made for reasoning about bigger complex tasks",
              "score": 1,
              "created_utc": "2026-01-27 08:42:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27mpaq",
          "author": "ArgenEgo",
          "text": "TOS",
          "score": 1,
          "created_utc": "2026-01-28 13:37:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq4vxu",
      "title": "Kimi K2.5, a Sonnet 4.5 alternative for a fraction of the cost",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qq4vxu/kimi_k25_a_sonnet_45_alternative_for_a_fraction/",
      "author": "Grand-Management657",
      "created_utc": "2026-01-29 10:19:28",
      "score": 60,
      "num_comments": 78,
      "upvote_ratio": 0.77,
      "text": "\n\nYes you read the title correctly. Kimi K2.5 is THAT good. \n\nI would place it around Sonnet 4.5 level quality. It‚Äôs great for agentic coding and uses structured to-do lists similar to other frontier models, so it‚Äôs able to work autonomously like Sonnet or Opus.\n\nIt's thinking is very methodical and highly logical, so its not the best at creative writing but the tradeoff is that it is very good for agentic use.\n\nThe move from K2 -> K2.5 brought multimodality, which means that you can drive it to self-verify changes. Prior to this, I used antigravity almost exclusively because of its ability to drive the browser agent to verify its changes. This is now a core agentic feature of K2.5. It can build the app, open it in a browser, take a screenshot to see if it rendered correctly, and then loop back to fix the UI based on what it \"saw\". Hookup playwright or vercel's browser-agent and you're good to go.\n\nNow like I said before, I would still classify Opus 4.5 as superior outside of JS or TS environments. If you are able to afford it you should continue using Opus, especially for complex applications.¬†\n\nBut for many workloads the best economical and capable pairing would be Opus as an orchestrator/planner + Kimi K2.5 as workers/subagents. This way you save a ton of money while getting 99% of the performance (depending on your workflow).  \n\n\n\\+ You don't have to be locked into a single provider for it to work.\n\n\\+ Screw closed source models.\n\n\\+ Spawn hundreds of parallel agents like you've always wanted WITHOUT despawning your bank account.\n\n\n\n*Btw this is coming from someone who very much disliked GLM 4.7 and thought it was benchmaxxed to the moon*\n\n\n\n# Get Started\n\n  \nThere are plenty of providers for open source models and only one for claude (duh)\n\n[Nano-GPT](https://nano-gpt.com/invite/mNibVUUH)\n\nA provider aggregator. Essentially routing all of your requests to a provider in their network. This is by far the most cost effective way to drive opencode, claude code, vscode (insiders), or any other harness. For the cost of a one extremely large cup of coffee, $8/month, you get 60,000 requests/month. That is $0.00013 per request regardless of input or output size. To put that into perspective, Sonnet 4.5 would cost you $0.45 for a request of 100k in/1k out (small-medium codebase) and not taking caching into account. Sonnet is 3,461x more expensive.\n\nAlso you can use Opus 4.5 through nano-gpt at API rates like I do to drive the orchestrator and then my subscription covers K2.5 subagents.\n\nCheap AF, solid community, founders are very active and helpful  \n  \nMy referral for 5% off web: [https://nano-gpt.com/invite/mNibVUUH](https://nano-gpt.com/invite/mNibVUUH)\n\n  \n[**Synethetic.new**](https://synthetic.new/?referral=KBL40ujZu2S9O0G)\n\nThis is what I would recommend for anyone needing maximum security and lightning fast inference. It costs a premium of $20/month ($10 with my referral), but compared to claude pro plan's usage limit, its a bargain. 135 requests/5hrs with tool calls only counting as 0.1 requests. This is the best plan for professionals and you can hook it up with practically any tool like claude code and opencode. Within a 10 hour period, you can use up to 270 requests which comes out to $0.002. Sonnet 4.5 is 225x more expensive.\n\nCheap, fast speed, $60/month plan gets you 1,350 requests/5hr, data not trained on \n\nMy referral for $10 or $20 off: [https://synthetic.new/?referral=KBL40ujZu2S9O0G](https://synthetic.new/?referral=KBL40ujZu2S9O0G)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qq4vxu/kimi_k25_a_sonnet_45_alternative_for_a_fraction/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2dz8i6",
          "author": "Hozukr",
          "text": "Marketing hype is really strong with this one. Running away as fast as possible.",
          "score": 24,
          "created_utc": "2026-01-29 10:34:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dzeg0",
              "author": "Repulsive_Educator61",
              "text": "I tried it and it's somewhere between sonnet 4.5 and opus 4.5\n\ndefinitely not benchmaxxed",
              "score": 18,
              "created_utc": "2026-01-29 10:35:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2f3p92",
                  "author": "kr_roach",
                  "text": "Is it fast? Im using GLM 4.7 but its so slow",
                  "score": 2,
                  "created_utc": "2026-01-29 14:51:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2dzqfu",
                  "author": "Grand-Management657",
                  "text": "That is similar to my findings but I'm not sure if its really better or on par with Sonnet 4.5  \nI think it may just come down to preference or its performance in the harness you use. I was very surprised it wasn't benchmaxxed unlike GLM 4.7. I tried to love that model but nope.",
                  "score": 1,
                  "created_utc": "2026-01-29 10:38:42",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ef7mp",
              "author": "annakhouri2150",
              "text": "Nah, K2.5 is actually this good imo",
              "score": 3,
              "created_utc": "2026-01-29 12:36:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2fb5se",
              "author": "randvoo12",
              "text": "No hype tbh, quality of work produced is comparable to Opus not even Sonnet.",
              "score": 3,
              "created_utc": "2026-01-29 15:26:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2g8bdh",
              "author": "RegrettableBiscuit",
              "text": "I've started running it in opencode today, and it's a strong model. I would put it above GLM4.7 and at least in the same general ballpark as Sonnet 4.5.\n\n\nThere's lots of hype, and I find it increasingly difficult to tell real grassroots support from manufactured hype (like why TF is synthetic mentioned in every post, is that real or just BS), but it is a genuinely good model.¬†",
              "score": 3,
              "created_utc": "2026-01-29 17:54:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mlw54",
                  "author": "Grand-Management657",
                  "text": "I'm not familiar with the hype around synthetic. Its just one of the only providers that I found that is subscription based, has privacy, gets around \\~60tok/s on K2.5, and pretty cheap compared to Sonnet 4.5. There's not really any other options and if you know any, feel free to link it for everyone.",
                  "score": 1,
                  "created_utc": "2026-01-30 16:26:39",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2l5xdy",
              "author": "zarrasvand",
              "text": "Yeah, and it is distilling Claude under the hood anyway so not a lot of new things to see here...",
              "score": 1,
              "created_utc": "2026-01-30 11:52:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2dze6l",
              "author": "Grand-Management657",
              "text": "Which part is hype? Please elaborate.",
              "score": 1,
              "created_utc": "2026-01-29 10:35:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2dztzr",
                  "author": "Repulsive_Educator61",
                  "text": "I would say only [synthetic.new](http://synthetic.new) part is hype, not kimi k2.5\n\nseeing lots of posts here about [synthetic.new](http://synthetic.new), could be their marketing team, i can't confirm",
                  "score": 5,
                  "created_utc": "2026-01-29 10:39:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ewxfg",
          "author": "rokicool",
          "text": "Yesterday I tried their 'native' subscription (via kimi.com) - Moderato ($20 per month).\n\nI spent 5 hours allowance within 30 min. This tier of subscription seems useless.\n\nThe next tier is $40... I will be working for 1 hour and 4 hours cooldown. Useless as well.\n\nSo, the only tier that gives access (for one thread of work!) is $200. And... Why spending the same amount for something that barely imitates the original (Anthropic) when the original costs the same?\n\nI don't understand why people call it 'cheap'. It is on par with Anthropic's subscriptions.\n\nhttps://preview.redd.it/vri1l76sqagg1.png?width=2582&format=png&auto=webp&s=09f5ad25b14b14f9d6b96c11b1ccc81957ea66a7\n\n  \nUPD: There were some changes to the Console interface and I looks different and shows different metrics. And IF they are relevant, I have a lot of allowance with my $20 subscription. \n\nSorry for jumping to conclusions.",
          "score": 9,
          "created_utc": "2026-01-29 14:17:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2exv9x",
              "author": "Grand-Management657",
              "text": "Its more expensive through the moonshot subscription compared to the ones I linked in the post. From what I remember, \"Moderato\" allows 2048 requests per week. Nano-gpt allows 15,000 requests per week. Also nano is $8 instead of $20. If you get two nano subs for $16, you will get almost \\~15x the usage of \"moderato\" for less.\n\nMy referral to nano if you want to give it a try: [https://nano-gpt.com/invite/mNibVUUH](https://nano-gpt.com/invite/mNibVUUH)",
              "score": 4,
              "created_utc": "2026-01-29 14:21:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2fx86f",
                  "author": "rokicool",
                  "text": "Thank your for your research.\n\nUnfortunately, I remember complains about sluggishness of nano-gpt and wanted to test 'original' provider. And despite the really impressive outcome of the Kimi2.5 model I find the Kimi Subscriptions useless.\n\nUPD: Since there are some changes to the Console interface and it looks much more logical and promising now... I should admit that my previous assumption 'everything is useless' might be wrong. Time will show!",
                  "score": 1,
                  "created_utc": "2026-01-29 17:04:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2hoe1q",
                  "author": "Western_Objective209",
                  "text": "is nano-gpt legit? seems like it automatically creates an anonymous account, even takes XMR for payments",
                  "score": 1,
                  "created_utc": "2026-01-29 21:58:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2fw5h2",
              "author": "rokicool",
              "text": "It is getting ridiculous. I managed to spend week allowance of $20 subscription within 1-1.5 hour(s) of OpenCode development.\n\nhttps://preview.redd.it/h1fip0d8kbgg1.png?width=2290&format=png&auto=webp&s=9936622b91898f936ee0670068a9c67a40a9e6c1\n\nAre you sure you would call something like $20 an hour as 'cheap'?\n\nUPD: \n\nIt seems to me that they were changing the interface while I was bitching. Now, after several hours it look  1% and 11%. \n\nSo, I might got it wrong. And it might be cheap.",
              "score": 2,
              "created_utc": "2026-01-29 16:59:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mjejg",
                  "author": "Grand-Management657",
                  "text": "That's where you're messing up, use synthetic as your provider and you will get more limits. Kimi was limited to 2048 requests/week last I checked. Synthetic is 135/5hrs or 1350/5hr on the pro plan.\n\n[https://synthetic.new/?referral=KBL40ujZu2S9O0G](https://synthetic.new/?referral=KBL40ujZu2S9O0G)",
                  "score": 2,
                  "created_utc": "2026-01-30 16:15:31",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2gjutd",
              "author": "chiroro_jr",
              "text": "What do y'all be doing really?",
              "score": 1,
              "created_utc": "2026-01-29 18:46:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2he0z2",
              "author": "chvmnaveen",
              "text": "I agree with you same behavior for me to on $20 plan. I consumed all the weekly limit in just one night üòí",
              "score": 1,
              "created_utc": "2026-01-29 21:09:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mjlqc",
                  "author": "Grand-Management657",
                  "text": "That's where you're messing up, use synthetic as your provider and you will get more limits. Kimi was limited to 2048 requests/week last I checked. Synthetic is 135/5hrs or 1350/5hr on the pro plan.\n\n[https://synthetic.new/?referral=KBL40ujZu2S9O0G](https://synthetic.new/?referral=KBL40ujZu2S9O0G)",
                  "score": 1,
                  "created_utc": "2026-01-30 16:16:24",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2hel4l",
              "author": "I_HEART_NALGONAS",
              "text": "That's still better than Sonnet 4.5 where a couple of times I blew through Anthropic's ridiculous 5-hour quota in two (2) prompts on the Pro plan.",
              "score": 1,
              "created_utc": "2026-01-29 21:12:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2j79oq",
              "author": "GTHell",
              "text": "Same experience. Why spend $20 just to use something that replicates the OG. It barely any improvement over GLM 4.7 and GLM $40 get you 3 months and the speed is very good.",
              "score": 1,
              "created_utc": "2026-01-30 02:51:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2kiwl3",
              "author": "_Belgarath",
              "text": "It's cheap regarding the API cost. It's about 10x cheaper than Claude when using a per token billing system, not using the subscription.",
              "score": 1,
              "created_utc": "2026-01-30 08:32:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2fpd52",
          "author": "Muted_Standard175",
          "text": "Have anyone tried to use opus 4.5 or gpt 5.2 as plan and k2.5 as build? How good it was?",
          "score": 5,
          "created_utc": "2026-01-29 16:29:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2js2xn",
              "author": "degenbrain",
              "text": "In my case, I did it the other way around. K2.5 tends to provide simple solutions and plans. There are no additional features. It's straightforward. Then, I ask Opus to execute it perfectly",
              "score": 1,
              "created_utc": "2026-01-30 04:57:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2efqyi",
          "author": "HotFats",
          "text": "I think k2.5 is definitely better than sonnet might be performing as close to opus. Its not only cheaper, but its way faster. Alsovi use synthetic.new, its pretty good. I think K2.5 with thinking is the closet we've gotten to giving anthropic models a run for their money. Currently its handling browser automation and building scripts and n8n workflows just as well if not better than opus  4.5. Not canceling my claude max subscription yet, but its promising.",
          "score": 3,
          "created_utc": "2026-01-29 12:40:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2egq4o",
              "author": "Grand-Management657",
              "text": "I would wait for two more weeks to cancel that sub. I think deepseek v4 might be even better and potentially releasing before the chinese new lunar year. And that gives you enough time to really put K2.5 to the test.",
              "score": 3,
              "created_utc": "2026-01-29 12:46:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ee6jl",
          "author": "MegamillionsJackpot",
          "text": "https://preview.redd.it/wlhxth6c8agg1.jpeg?width=1080&format=pjpg&auto=webp&s=eb03955b77875aaee196565e06e862affd807354\n\nExpensive if you are not on a plan?",
          "score": 2,
          "created_utc": "2026-01-29 12:29:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eer53",
              "author": "Grand-Management657",
              "text": "Seems like you are looking at agent swarm which I do not know too much of. I do know that it spins up hundreds of K2.5's, so its going to cost significantly more. Using the model without swarm is $0.50 in/$3.00 out with API rates. With nano or synthetic as providers, your cost is significantly lower than API rates.",
              "score": 2,
              "created_utc": "2026-01-29 12:33:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2efty9",
                  "author": "MegamillionsJackpot",
                  "text": "Yeah, I know. It's just a funny bug in the pricing. And that bug was there before I wrote the agent swarm thing.\n\n\nDo you know if synthetic models work okay for multi step deep research?",
                  "score": 1,
                  "created_utc": "2026-01-29 12:40:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2eubz0",
          "author": "Salty-Standard-104",
          "text": "PR slop. why kimi would hire such terrible person for writing crap like this?",
          "score": 2,
          "created_utc": "2026-01-29 14:03:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fw2gb",
              "author": "seaal",
              "text": "kimi? this chud and all the others are just spamming their referral links are just trying to get their credits for nanogpt and synthetic.new.",
              "score": 2,
              "created_utc": "2026-01-29 16:59:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2k90zr",
          "author": "Lower_Temperature709",
          "text": "I have been working with minimax + glm + codex + code. All bare minimum plan. Coding non stop from last week. It‚Äôs crazy efficient and dirt cheap. \n\nUsing oh my open code as the agent harness with alots of agent and sub agent configured.",
          "score": 2,
          "created_utc": "2026-01-30 07:05:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dzu61",
          "author": "N2siyast",
          "text": "No way Im using this vibe coded slop site",
          "score": 2,
          "created_utc": "2026-01-29 10:39:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e0pub",
              "author": "Grand-Management657",
              "text": "Haha I agree. I was just browsing earlier and saw the home page and it is ugly",
              "score": 0,
              "created_utc": "2026-01-29 10:47:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2e0bup",
          "author": "BitterAd6419",
          "text": "Kimi is better than GLM but not as good as anthropic models.",
          "score": 2,
          "created_utc": "2026-01-29 10:43:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e5cpp",
              "author": "awfulalexey",
              "text": "GLM has approximately 350 billion parameters, Kimi has 1 trillion parameters. It's interesting why Kimi is stronger than GLM.",
              "score": 2,
              "created_utc": "2026-01-29 11:25:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ec0si",
                  "author": "Grand-Management657",
                  "text": "Not sure where I read it but K2.5 is built on K2 but with an additional training of 15 trillion mixed visual and text tokens. Not sure about GLM 4.7 but I would suspect its nowhere close to that.",
                  "score": 2,
                  "created_utc": "2026-01-29 12:15:05",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2e1086",
              "author": "Grand-Management657",
              "text": "This is a reasonable take. Is your use case mostly web? I haven't gotten a chance to test it on anything other than web development.",
              "score": 1,
              "created_utc": "2026-01-29 10:49:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2e9a5r",
                  "author": "BitterAd6419",
                  "text": "Yea only tested on few web related task nothing too complicated",
                  "score": 1,
                  "created_utc": "2026-01-29 11:55:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2e07y8",
          "author": "joakim_ogren",
          "text": "Does Synthetic.new support Kimi K2.5? (It seems supported by vLLM)",
          "score": 1,
          "created_utc": "2026-01-29 10:42:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e0u9c",
              "author": "Grand-Management657",
              "text": "They do support it but since its a new model, they haven't updated the page I'm guessing. Go to [https://synthetic.new/pricing](https://synthetic.new/pricing) and you will see it in the list.",
              "score": 4,
              "created_utc": "2026-01-29 10:48:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2e8z2y",
          "author": "seeKAYx",
          "text": "$10 discount / month with that referral or only first month?",
          "score": 1,
          "created_utc": "2026-01-29 11:53:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ea8bm",
              "author": "Grand-Management657",
              "text": "I'm pretty sure its only the first month. I wish it was recurring!\n\nhttps://preview.redd.it/l9hg8i5e3agg1.png?width=460&format=png&auto=webp&s=037b29a3023c9f2205bf51c2f67e020038f70145",
              "score": 1,
              "created_utc": "2026-01-29 12:02:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2f7hyt",
          "author": "Galendel",
          "text": "I am using deepseek v3.2 with and without thinking, I really like it for the cost, did anyones else use deepseek ?",
          "score": 1,
          "created_utc": "2026-01-29 15:09:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f8efy",
              "author": "Grand-Management657",
              "text": "I really like deepseek v3.2 for creative writing. I think it would be great for its intelligence and writing style even in agentic coding. But it just wasn't tailored towards software development like claude models, Kimi K2.5, or GLM 4.7\n\nFor the cost though, its hard to beat. Almost costs nothing to run. I have very high hopes for deepseek v4 and I think that will be on par with Opus 4.5, or at least I hope. Fingers crossed!",
              "score": 2,
              "created_utc": "2026-01-29 15:13:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2h8kmn",
                  "author": "Galendel",
                  "text": "I am spending like 3-4$ a day on it, the code he does is fine to me, it's just too slow and way more  with thinking, on aider benchmark [https://aider.chat/docs/leaderboards/](https://aider.chat/docs/leaderboards/) Kimi K2 is really low compare to deepseek. I tried GLM 4.7 free on zen ai and it was really bad for agentic coding, maybe they are overloaded. The ratio quality / cost doesn't seem to be a subject, but to me if a good LLM is 10x cheaper it can do 9x more coding with same budget. It's been a while I didn't use subscription so I can't compare yet.",
                  "score": 2,
                  "created_utc": "2026-01-29 20:43:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gtr37",
          "author": "SunflowerOS",
          "text": "Can I use my suscription on opencode like Anthropic or I need to pay the api?",
          "score": 1,
          "created_utc": "2026-01-29 19:31:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gybi9",
              "author": "Grand-Management657",
              "text": "Yes you can use any subscription with opencode but I don't recommend using claude subscription on opencode. They will ban you.\n\nThe two I recommend is\n\nNano-gpt: https://nano-gpt.com/invite/mNibVUUH\n\nor\n\nSynthetic: https://synthetic.new/?referral=KBL40ujZu2S9O0G",
              "score": 1,
              "created_utc": "2026-01-29 19:53:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2gzf7c",
                  "author": "SunflowerOS",
                  "text": "I know it, but I suscribe to kimi on december thinking that i could use it on opencode",
                  "score": 1,
                  "created_utc": "2026-01-29 19:58:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gxlll",
          "author": "VaizardX",
          "text": "How did you setup the orchestrator and agents?",
          "score": 1,
          "created_utc": "2026-01-29 19:50:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gzjxn",
              "author": "Grand-Management657",
              "text": "In OpenCode, you can set a specific model for a subagent by configuring the model property in the subagent's definition within the opencode.json or opencode.jsonc configuration file.\n\nYou can find more information here: https://opencode.ai/docs/agents",
              "score": 1,
              "created_utc": "2026-01-29 19:59:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2h8u8u",
              "author": "Galendel",
              "text": "have a look at bmad, they do have an orchestrator",
              "score": 1,
              "created_utc": "2026-01-29 20:44:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2l5v22",
          "author": "zarrasvand",
          "text": "https://preview.redd.it/c5h94oni6hgg1.png?width=1266&format=png&auto=webp&s=bd25da84a71a56eaa05ea6dfeb42dbd2196d3407\n\n\"Ok bro\"",
          "score": 1,
          "created_utc": "2026-01-30 11:52:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mpsi4",
          "author": "Grand-Management657",
          "text": "For those of you wondering about speeds\n\nI am currently getting \\~18tok/s with nano-gpt and \\~60tok/s with synthetic.\n\nI recommend synthetic for any enterprise workloads or anything you will make money from. Its super fast, privacy centered and much cheaper than Sonnet 4.5. It also gives you the stability that is required for enterprise workloads. Combine it with your favorite frontier model (Opus 4.5/GPT 5.2) for best performance.\n\nNano-gpt is much slower but much more economical. Recommending this for side projects and hobbyists. I find this to be a great option if you need to spin up many subagents at once. Currently there are some multi-turn tool call issues which the devs are working on actively to rectify. Combine with your favorite frontier model to get best results (Opus 4.5/GPT 5.2)\n\nNano: [https://nano-gpt.com/invite/mNibVUUH](https://nano-gpt.com/invite/mNibVUUH)\n\nSynthetic: [https://synthetic.new/?referral=KBL40ujZu2S9O0G](https://synthetic.new/?referral=KBL40ujZu2S9O0G)",
          "score": 1,
          "created_utc": "2026-01-30 16:43:53",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2grb7t",
          "author": "alexeiz",
          "text": "You're here just to push your referrals.  That's it.",
          "score": 1,
          "created_utc": "2026-01-29 19:20:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dzhon",
          "author": "pokemonplayer2001",
          "text": "Always be shilling.\n\nHaha, you mad.",
          "score": 1,
          "created_utc": "2026-01-29 10:36:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2gykks",
          "author": "mustafamohsen",
          "text": "Kimi, you still post slop?",
          "score": -1,
          "created_utc": "2026-01-29 19:54:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qocwc2",
      "title": "Kimi k2.5",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/hv2amvpk1wfg1.jpeg",
      "author": "ReasonableReindeer24",
      "created_utc": "2026-01-27 12:47:14",
      "score": 55,
      "num_comments": 20,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qocwc2/kimi_k25/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o233nol",
          "author": "Hoak-em",
          "text": "Submitted PR and got it merged to [models.dev](http://models.dev) for the kimi-for-coding provider -- it's a fantastic model. I was initially a bit skeptical of it being benchmaxxed given how well it performed on benchmarks, but it is genuinely an amazing orchestrator -- likely the best orchestrator I've ever used, plus it's and opus-level planner with openspec. It's really, really good at direction-following as well, and seems to be token-efficient like opus. So yeah, it is what the benchmarks said.",
          "score": 6,
          "created_utc": "2026-01-27 20:40:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25ib57",
              "author": "arshadbarves",
              "text": "Merged",
              "score": 1,
              "created_utc": "2026-01-28 03:55:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o212xzz",
          "author": "shaonline",
          "text": "I've added it manually via opencode.json and it \"works\" but I get hit with a \"Provider returned error\" after the first tool calls (basic greps...), so... not really usable for now ?",
          "score": 5,
          "created_utc": "2026-01-27 15:23:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21dgtr",
              "author": "hotairplay",
              "text": "Yeah I'm having the same error using Droid. Looks like the servers got overloaded.",
              "score": 2,
              "created_utc": "2026-01-27 16:10:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o248efh",
          "author": "Mattdeftromor",
          "text": "I got the Kimi Code Plan $40 and... It's fantastic !",
          "score": 2,
          "created_utc": "2026-01-27 23:52:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24ncl5",
              "author": "aeroumbria",
              "text": "It seems to burn out much faster though, despite being 500/5hr requests vs GLM's supposedly 600/5hr requests. It just seems GLM can't ever run out even if you try. I observed that Kimi counts every single interaction, like calling read tool as a request, whereas GLM does not seem to count most contiguous agent actions as additional requests.",
              "score": 3,
              "created_utc": "2026-01-28 01:08:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2668ty",
                  "author": "shaonline",
                  "text": "Yeah I have GLM lite coding plan and even if I let it hammer away at a task for a long while I can't ever seem to make the quota run out, even past 30% lmao. That being said it hardly ever lets you run parallel agents (at least on a single model) so there's that.",
                  "score": 2,
                  "created_utc": "2026-01-28 06:40:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27cyni",
              "author": "Phukovsky",
              "text": "How is it used? Like, run 'kimi code' in terminal and then use it like you'd use Claude Code? \n\nAny advantage to this vs using it through OpenCode?",
              "score": 1,
              "created_utc": "2026-01-28 12:39:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27y2bm",
                  "author": "Mattdeftromor",
                  "text": "I use it with OpenCode ! its kimi-cli is horrible",
                  "score": 1,
                  "created_utc": "2026-01-28 14:35:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2d1w21",
              "author": "alexeiz",
              "text": "Kimi plans are too expensive (like, would you pay $20 for Kimi or for GPT/Claude?).  And other Chinese companies like Z.ai and Minimax heavily undercut Kimi.  You really have to be a Kimi fan to pay for Kimi.",
              "score": 1,
              "created_utc": "2026-01-29 05:41:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2bh00z",
          "author": "BitterAd6419",
          "text": "It‚Äôs very slow or often errors out, guess the capacity is maxed out, even their site sometimes doesn‚Äôt work properly or returns errors",
          "score": 2,
          "created_utc": "2026-01-29 00:08:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bnbvm",
              "author": "ReasonableReindeer24",
              "text": "It's mid tier model , you cannot expect more like opus or gpt 5.2 codex",
              "score": 1,
              "created_utc": "2026-01-29 00:41:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o20strg",
          "author": "Impossible_Comment49",
          "text": "Some results are available here, but only K2 is on the leaderboard. However, I would be cautious because this leaderboard can be heavily influenced by bots.\n\n[https://lmarena.ai/leaderboard/code](https://lmarena.ai/leaderboard/code)",
          "score": 1,
          "created_utc": "2026-01-27 14:34:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o215wqe",
              "author": "ReasonableReindeer24",
              "text": "wait for update kimi k2.5 on opencode cli",
              "score": 3,
              "created_utc": "2026-01-27 15:37:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o24aaeo",
                  "author": "Mattdeftromor",
                  "text": "Already there for me too",
                  "score": 2,
                  "created_utc": "2026-01-28 00:02:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o23nafw",
                  "author": "planetearth80",
                  "text": "It‚Äôs already there for me",
                  "score": 1,
                  "created_utc": "2026-01-27 22:08:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qq8sgu",
      "title": "Be careful when using Claude Code with OpenCode",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/zqcyddtrjagg1.png",
      "author": "HzRyan",
      "created_utc": "2026-01-29 13:38:42",
      "score": 49,
      "num_comments": 34,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qq8sgu/be_careful_when_using_claude_code_with_opencode/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2ew7xg",
          "author": "aeroumbria",
          "text": "They are pretty much alone in the fight against third party UIs at this point, though. Almost all their competition are either actively supporting or acquiescing.",
          "score": 9,
          "created_utc": "2026-01-29 14:13:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gdffw",
              "author": "flexrc",
              "text": "Google antigravity just started blocking opencode usage as well.",
              "score": 5,
              "created_utc": "2026-01-29 18:17:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ez4id",
              "author": "Sensitive_Song4219",
              "text": "Yup. This can't be what developers want though: it's like saying \"You can use Visual Studio Professional but VSCode is prohibited or you get booted.\" No experienced dev would accept that.",
              "score": 3,
              "created_utc": "2026-01-29 14:28:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ezzqq",
                  "author": "Ok_Road_8710",
                  "text": "Of course not, they can try (Anthropic), their heads are too far up their own asses to make any progress I guess",
                  "score": 1,
                  "created_utc": "2026-01-29 14:32:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2eqcy2",
          "author": "HeavyDluxe",
          "text": "This is old news.  And Claude's TOS specifically speak to this use as prohibited.  (I know, I know. Who reads that stuff?)\n\nIf you want to use Opencode as your CLI environment with Claude, YOU CAN DO THAT.  You just need to buy credits for the API and create a key.  Plug it into Opencode and you're off to the races.",
          "score": 14,
          "created_utc": "2026-01-29 13:42:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2exu2z",
              "author": "OofOofOof_1867",
              "text": "My decided workaround has been to get a GitHub CoPilot Pro+ license, and then pay as I go afterwards. They pay by \"premium request\" - so it does change the way I work because I make sure every request is FAT to get most bang for my buck. But so far it's close to the same price as Claude Max5.",
              "score": 4,
              "created_utc": "2026-01-29 14:21:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2fupbn",
                  "author": "jrop2",
                  "text": "\\>¬†But so far it's close to the same price as Claude Max5.\n\nEesh. I was wondering about switching to GH CoPilot from the Claude plans. However, I was shooting for the $40/month plan, and your experience makes it sound like it still evens out to lots of money either way....",
                  "score": 1,
                  "created_utc": "2026-01-29 16:53:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2etls8",
              "author": "Sensitive_Song4219",
              "text": "API is very expensive tho. I know it's old news, but it'd be better news if Anthropic would follow OpenAI's lead and allow subscription use in OpenCode: OC still needs some polish but after two weeks or so of using it, it's really kinda incredible. (Heck, I just used *Microsoft's official OAUTH to sign into my Copilot subscription via OpenCode*!). \n\n  \nIf OpenAI and freakin' *Microsoft* (of all companies!) can see the benefit of giving their paid subscribers access (because let's face it: OpenCode isn't more token hungry than Claude Code), then why can't Anthropic follow suit?\n\nFor the first time in a year, I've gone a full week without looking at Claude Code - I'm OpenCode exclusive now, chopping-and-changing providers, models, and reasoning levels mid-conversation. It's liberating. And maybe that's the problem.",
              "score": 2,
              "created_utc": "2026-01-29 13:59:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ew0le",
                  "author": "acmethunder",
                  "text": "> If OpenAI and freakin' Microsoft (of all companies!) can see the benefit of giving their paid subscribers access (because let's face it: OpenCode isn't more token hungry than Claude Code), then why can't Anthropic follow suit?\n\nAnthropic can, they are choosing not to. It is a form of vendor lock in hoping to keep those monthly/yearly revenue streams.",
                  "score": 5,
                  "created_utc": "2026-01-29 14:12:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ko7bk",
                  "author": "Keep-Darwin-Going",
                  "text": "It is called paying for mind share. People remember Claude code more than anthropic, if you look at a lot of comparison article and video people say Claude code not opus or sonnet or anthropic. Which is why you can use cc for everything else but not use max x20 for opencode.  All this loses they have to attribute to something to answer to shareholders. OpenAI allow it because their plan is already more expensive although they are more efficient? Opus already capture majority of the enterprise and non enterprise coding users. It is like if you talk about chat AI it is always chatgpt, coding it is always opus at least for now.",
                  "score": 1,
                  "created_utc": "2026-01-30 09:20:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2er00f",
              "author": "HzRyan",
              "text": "That's on me, haha. I'm still not going to read any T&Cs though üåù",
              "score": 1,
              "created_utc": "2026-01-29 13:45:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2erzr5",
                  "author": "HeavyDluxe",
                  "text": "Can't blame you. :)",
                  "score": 1,
                  "created_utc": "2026-01-29 13:51:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ezsc4",
          "author": "kiwibonga",
          "text": "Hey so... As soon as you open opencode you get banned and refunded, no questions asked?",
          "score": 1,
          "created_utc": "2026-01-29 14:31:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f0f0j",
              "author": "HzRyan",
              "text": "no I used it for 3 day straight, burn through quite a lot of tokens through Claude subs maybe that's why I got banned",
              "score": 1,
              "created_utc": "2026-01-29 14:34:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2f3db3",
                  "author": "Scriverry",
                  "text": "If you used more than one sub that's the reason you got banned, not that you used it with opencode. They are way more strict about that.",
                  "score": 2,
                  "created_utc": "2026-01-29 14:49:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lnrji",
          "author": "Peace_Seeker_1319",
          "text": "damn they refunded but banned you. probably flagged the usage pattern - too many requests too fast or something that looked automated. honestly the bigger issue is what you're shipping when you code that aggressively. if you're cranking out PRs without really understanding what's being generated, you're building tech debt bombs.  \nwe had to add automated review [codeant.ai](http://codeant.ai) in CI specifically because people were merging AI code too fast. caught race conditions, memory leaks, stuff that looked fine but broke under load. slowed us down but saved us from prod fires. maybe the ban is a feature not a bug lol.",
          "score": 1,
          "created_utc": "2026-01-30 13:44:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mf7sy",
              "author": "HzRyan",
              "text": "I have 6 years of experience DEV-ing and have worked on quite a few large projects before, so I'm not just yolo pushing any generated code and hoping it works. Every worktree branch gets manually+claude reviewed and tested before it gets locally merged. Of course I don't read every single line of code, but I do know how each section of my system works and I make sure it's specified in the prompt so CC knows where to extend or implement the feature.\n\nIt's honestly amazing how much CC has boosted my productivity. I'm holding a full time job right now while working on 3 other separate projects, and I think there is still room for improvement in my current workflow. Trying to get out of the 9-5 rat race, Locked in!! \n\nNice plug btwüòâ",
              "score": 1,
              "created_utc": "2026-01-30 15:56:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qqrbry",
      "title": "Tested free Kimi K2.5 in opencode: good stuff",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qqrbry/tested_free_kimi_k25_in_opencode_good_stuff/",
      "author": "ReporterCalm6238",
      "created_utc": "2026-01-30 01:17:03",
      "score": 45,
      "num_comments": 9,
      "upvote_ratio": 0.96,
      "text": "It's fast, it's smart BUT sometimes it makes mistakes with tool calling. I would put it above glm 4.7 and minimax M2.1. \n\n  \nWe are getting close boys. Open source Opus is not too far. There are some extremely smart people in China working around the clock to crush Anthropic, that's for sure.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qqrbry/tested_free_kimi_k25_in_opencode_good_stuff/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2ixorf",
          "author": "AlternativeAir7087",
          "text": "Open source will definitely prevail!",
          "score": 5,
          "created_utc": "2026-01-30 01:58:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2iy36w",
              "author": "ReporterCalm6238",
              "text": "I surely hope so. It's in the interest of the whole humanity that open source AI resists and thrive",
              "score": 5,
              "created_utc": "2026-01-30 02:01:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2k00l8",
          "author": "kr_roach",
          "text": "I tested it. But I'm wondering if Opencode Zen Kimi is using Moonshot AI directly or serving it through their server.",
          "score": 3,
          "created_utc": "2026-01-30 05:54:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2k1lvg",
              "author": "Useful-Mistake-4221",
              "text": "It's through Fireworks",
              "score": 4,
              "created_utc": "2026-01-30 06:06:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2k0u7e",
              "author": "Recent-Success-1520",
              "text": "Most probably Moonshot directly",
              "score": 0,
              "created_utc": "2026-01-30 06:00:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2kx6b6",
                  "author": "this-is-hilarours",
                  "text": "Not true. Served through fireworks  in us. Opencode confirmed this via x",
                  "score": 2,
                  "created_utc": "2026-01-30 10:41:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2kh6y9",
          "author": "krimpenrik",
          "text": "How/what provider are you using it with?",
          "score": 2,
          "created_utc": "2026-01-30 08:16:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2l0p37",
          "author": "Villain_99",
          "text": "Using it via Kimi api, really good\nEven as a general purpose Chatbot it‚Äôs really good answers like sonnet/opus",
          "score": 2,
          "created_utc": "2026-01-30 11:11:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lrkng",
          "author": "0xfe",
          "text": "It's really excellent -- it finished some tasks that GLM just spun forever on. I currently use codex as my daily driver and claude for smaller things, but Kimi really feels like the first open model that truly competes.",
          "score": 1,
          "created_utc": "2026-01-30 14:03:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qljsw3",
      "title": "Opencode.nvim ‚Äì Opencode, fully integrated into Neovim.",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qljsw3/opencodenvim_opencode_fully_integrated_into_neovim/",
      "author": "Nexmean",
      "created_utc": "2026-01-24 10:37:41",
      "score": 36,
      "num_comments": 1,
      "upvote_ratio": 0.95,
      "text": "There is solid opencode.nvim plugin for neovim, that fully rebuilds opencode's interface to make it closer to native neovim experience: [https://github.com/sudo-tee/opencode.nvim](https://github.com/sudo-tee/opencode.nvim) \n\nhttps://preview.redd.it/br7syu7lz9fg1.png?width=2088&format=png&auto=webp&s=9a989748eeaa7d0060cb55fe70eca0683341f610\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qljsw3/opencodenvim_opencode_fully_integrated_into_neovim/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o1itpjg",
          "author": "ZeSprawl",
          "text": "Very cool, Zed has a similar integration with native ui. Dig this",
          "score": 2,
          "created_utc": "2026-01-24 23:49:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmwcp1",
      "title": "OpenCode Ecosystem feels overwhelmingly bloated",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qmwcp1/opencode_ecosystem_feels_overwhelmingly_bloated/",
      "author": "Codemonkeyzz",
      "created_utc": "2026-01-25 21:40:05",
      "score": 36,
      "num_comments": 30,
      "upvote_ratio": 0.92,
      "text": "I often check OpenCode ecosystem and update my setup every now and then to utilize  opencode  to the max.  I go through every plugins,  projects ...etc.   However, i noticed most of these plugins are kinda redundant. Some of them are kinda promoting certain services or products, some of them feel outdated, some of them  are for very niche use cases.\n\nIt kinda takes time to go through every single one and understand how to utilize it.  I wonder  what are you plugin and project choices from this ecosystem ?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qmwcp1/opencode_ecosystem_feels_overwhelmingly_bloated/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o1qcoa8",
          "author": "FlyingDogCatcher",
          "text": "You don't need to use any of that shit. Opencode is just fine on its own",
          "score": 17,
          "created_utc": "2026-01-26 01:02:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s7cbr",
              "author": "IIALE34II",
              "text": "Opencode with few selected skills is most certainly 80/20 rule.",
              "score": 6,
              "created_utc": "2026-01-26 08:02:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1po963",
          "author": "ekaqu1028",
          "text": "Reminds me of MCP; you go crazy adding a ton‚Ä¶ 6 months later you stop using MCP\n\nThe only plugin I use is to send notifications when the agent is waiting on me.\n\nI probably will try to find a plugin (or write one) that helps protect against unexpected deletes‚Ä¶ had a CC hook for that‚Ä¶ just not ported",
          "score": 9,
          "created_utc": "2026-01-25 23:04:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1q4uo5",
              "author": "Big_Bed_7240",
              "text": "I think all abstractions like skills, mcp, sub agents etc are anti-patterns. They will all disappear when the models get better",
              "score": 7,
              "created_utc": "2026-01-26 00:23:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1qfmxj",
                  "author": "ekaqu1028",
                  "text": "I had a sub agent to run tests and give summary for errors.  Our build is very very dense with text and 90% doesn‚Äôt matter‚Ä¶ I replaced with a script that calls the build and shows the log of the failing step‚Ä¶ simpler and more consistent‚Ä¶\n\nIm moving more and more to reusable cli and away from AI features",
                  "score": 4,
                  "created_utc": "2026-01-26 01:17:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1snfqs",
                  "author": "extreme4all",
                  "text": "I think subagents can significanly reduce the context in the main agent, but yes if context is unlimited and speed also than there is no need",
                  "score": 1,
                  "created_utc": "2026-01-26 10:28:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o24mbso",
                  "author": "zumus",
                  "text": "idk about anti-patterns. I think orchestration will continue to be useful. i agree about skills, but tool-calling is still necessary and specialists are necessary. I think it'll just come in the form of smaller specialized (fine-tuned) models, especially in the open source ecosystem",
                  "score": 1,
                  "created_utc": "2026-01-28 01:03:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1v2ch5",
              "author": "deadcoder0904",
              "text": "> I probably will try to find a plugin (or write one) that helps protect against unexpected deletes‚Ä¶ had a CC hook for that‚Ä¶ just not ported\n\nClaude Code Damage Control exists. Its by IndyDevDan & you can watch his YT video if you search his name too. Works well as it has both static `rm -rf` & dynamic (`delete my home directory`) saving.",
              "score": 2,
              "created_utc": "2026-01-26 18:14:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ttpba",
              "author": "touristtam",
              "text": "The context pruning and todo reminder are nice",
              "score": 1,
              "created_utc": "2026-01-26 14:59:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1r8zz7",
          "author": "SynapticStreamer",
          "text": "I've been running OpenCode with absolutely nothing but custom sub agents, custom commands, and a few MCP servers.\n\nJust because the ecosystem is bloated doesn't mean you have to **use** bloated software.",
          "score": 8,
          "created_utc": "2026-01-26 03:51:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22069m",
              "author": "ProfessionNo3952",
              "text": "Could you share your custom setup?",
              "score": 1,
              "created_utc": "2026-01-27 17:48:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24crug",
                  "author": "SynapticStreamer",
                  "text": "[ Removed by Reddit ]",
                  "score": 1,
                  "created_utc": "2026-01-28 00:15:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1qegyn",
          "author": "towry",
          "text": "Try PI",
          "score": 5,
          "created_utc": "2026-01-26 01:11:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uro06",
          "author": "klocus",
          "text": "Personally I find that simple solutions work best for experienced developers. In my day-to-day work, I use OpenCode with two plugins and three MCPs.\n\nMy biggest discovery, and the one I'm most pleased with, is the DCP plugin, which automatically reduces token usage by removing outdated tool results from the conversation history.\n\nBesides the AGENTS.md file, I use the Simple Memory Plugin, which saves important context (like a pattern or a new discovery) during a session as logfmt files, allowing agent to search by range, type, or query. However, I think there are some better solutions for permanent memory that would not have to be loaded at the beginning of the session.\n\nOn top of that, I use three MCPs: Angular CLI MCP, Playwright MCP, and the most important and useful one, Context Engine MCP from Augment. Code indexing is quite crucial. It helps avoid lots of \"grep\" commands and find patterns within the project.",
          "score": 4,
          "created_utc": "2026-01-26 17:28:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1p8rr7",
          "author": "alvinunreal",
          "text": "I can share mine: [https://github.com/alvinunreal/oh-my-opencode-slim](https://github.com/alvinunreal/oh-my-opencode-slim)\n\nAlso felt the same",
          "score": 11,
          "created_utc": "2026-01-25 21:55:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1pe5xy",
              "author": "Metalwell",
              "text": "Hello. I have been rawdogging 5.2 through OC for couple of days and I am pretty happy with the results. You claim it consumes less token, how much less are you claiming? Thanks! Will give this a go.\n\nEDIT: Tried this. It feels like it is eating way more token than vanilla oc.\n\nEDIT:2 Nope, seems like this config is less token hungry for some reason lol even with all those mcp servers good job",
              "score": 7,
              "created_utc": "2026-01-25 22:19:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1zjgwy",
                  "author": "alvinunreal",
                  "text": "Nice, thanks for feedback.  \nI also meant tokens compared to oh-my-opencode, which I forked...",
                  "score": 2,
                  "created_utc": "2026-01-27 09:20:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ph8rg",
              "author": "Codemonkeyzz",
              "text": "Thanks for sharing.  It looks good. I will  definitely try this.",
              "score": 3,
              "created_utc": "2026-01-25 22:33:10",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1pqdzc",
              "author": "rusl1",
              "text": "Good job on this one! This is the only one worth adding. Orchestrator does everything I need",
              "score": 3,
              "created_utc": "2026-01-25 23:14:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1sbtpm",
                  "author": "alvinunreal",
                  "text": "thank you!",
                  "score": 2,
                  "created_utc": "2026-01-26 08:42:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1qnsed",
              "author": "ahmetegesel",
              "text": "I genuinely can‚Äôt help myself from skipping these plugins only because of their extreme verbose and ‚Äúadventurous‚Äù readme files. Package says it is slim, but apparently not its readme. It is still as redundant as its parent.",
              "score": 3,
              "created_utc": "2026-01-26 01:59:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1sboyk",
                  "author": "aeroumbria",
                  "text": "It is almost mandatory at this point that if you generate a readme or even agent file with AI, you have to say something like \"no more than 300 lines\" for it to not end up being a sloppy mess...",
                  "score": 2,
                  "created_utc": "2026-01-26 08:40:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rwgev",
                  "author": "Metalwell",
                  "text": "It looks very strange not gonna lie. Feels like it might eat more tokens than default lol. I will give this a try today.",
                  "score": 1,
                  "created_utc": "2026-01-26 06:31:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rofj0",
          "author": "WeMetOnTheMountain",
          "text": "Yeah I was just thinking the same thing today and it's pretty damn buggy.¬† I made a minimal agent that works pretty well for me.¬† For local LLMS using something like oh my open code is really heavy and pretty much kills the model on startup.",
          "score": 2,
          "created_utc": "2026-01-26 05:31:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2699bn",
          "author": "AkiDenim",
          "text": "The only Plugins I use are markdown table formatter and DCP. The only reasons I use opencode tbh, let alone the customizability",
          "score": 1,
          "created_utc": "2026-01-28 07:04:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlp9rk",
      "title": "Crazy Potential Even for Note Taking",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/gallery/1qlp9rk",
      "author": "xdestroyer83",
      "created_utc": "2026-01-24 15:05:39",
      "score": 36,
      "num_comments": 7,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qlp9rk/crazy_potential_even_for_note_taking/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1gt85d",
          "author": "mindgraph_dev",
          "text": "I made mindgraph-notes with opencode intengration also Zotero and ollama ich changed from Obsidian an never locked back a I need\n\nhttps://preview.redd.it/dtnvauue8cfg1.png?width=3456&format=png&auto=webp&s=3015c8d7a22eaf9eac8d10972e4bb87c532c6e6d",
          "score": 3,
          "created_utc": "2026-01-24 18:09:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1k2fdr",
              "author": "xdestroyer83",
              "text": "awesome setup!! obsidian has canvas as well so you might want to look again, and with obsidian skill models are able to write canvas really well",
              "score": 1,
              "created_utc": "2026-01-25 03:55:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1fs47q",
          "author": "bigh-aus",
          "text": "i quite like the acid brutalism one - very star trek / 90s-2000s computer game",
          "score": 1,
          "created_utc": "2026-01-24 15:22:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1gq43l",
              "author": "Heavy-Focus-1964",
              "text": "definitely got that Mechwarrior/Command and Conquer flavour",
              "score": 2,
              "created_utc": "2026-01-24 17:56:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gtrz4",
          "author": "mindgraph_dev",
          "text": "yes potential i crazy also for learning you can make /learnsetting and force opencode to learn with all the content of you notes in socratic dialog for school , university etc.",
          "score": 1,
          "created_utc": "2026-01-24 18:12:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1m7ou7",
          "author": "Fresh_Ad_1722",
          "text": "What ide?",
          "score": 1,
          "created_utc": "2026-01-25 13:48:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mpkic",
              "author": "xdestroyer83",
              "text": "not really an ide but I'm using obsidian in the screenshot, i integrated opencode using a plugin",
              "score": 1,
              "created_utc": "2026-01-25 15:21:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qky971",
      "title": "How OpenCode went from zero to titan in eight months",
      "subreddit": "opencodeCLI",
      "url": "https://jpcaparas.medium.com/dcdcd8ff5572?sk=e4d734988b81d43db236a2c910e584e4",
      "author": "jpcaparas",
      "created_utc": "2026-01-23 18:11:11",
      "score": 34,
      "num_comments": 5,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qky971/how_opencode_went_from_zero_to_titan_in_eight/",
      "domain": "jpcaparas.medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1bjz4h",
          "author": "FreakyT",
          "text": "I feel like this post doesn't address what, to me, is the most important part of the story: why are people running Opencode with Claude anyway?\n\nWasn't the main purpose of Opencode to be Claude code but for people who want to use other models?",
          "score": 7,
          "created_utc": "2026-01-23 22:13:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dm7ud",
              "author": "james__jam",
              "text": "Here‚Äôs my interpretation of what happened\n\nTDLR: opencode was for terminal lovers, but what resonated the most to people is its model agnostic nature\n\nOpencode was made for terminal lovers. It‚Äôs when you want to hack and customize your agentic system just like you would your nvim. I remember their docs saying something like that. So the tui experience was their #1 priority. And that‚Äôs the reason why they have keybindings and extensibility. extensibility in the form of you can have your own commands, agents, your own models, and a plugin system for further expansion (_those are the first few that they have out of the box_).\n\nAlso, they‚Äôve always recommend claude models (_im not sure if they do still, but they did for the longest time_). \n\nBut i think what got people reaching for opencode is to avoid the vendor lockin. It‚Äôs not just anthropic. Every model provider has been accused of nerfing their model post launch. And having your own setup from one agentic tool only to be rug pulled is annoying to the say the least. \n\nSo yeah, i think people are switching to opencode because it‚Äôs model agnostic. \n\nAnd although not everybody would hack and customize their setup, we have a few that would even turn their setups into plugins and make it available for others.",
              "score": 5,
              "created_utc": "2026-01-24 05:25:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1emb8x",
              "author": "hey_ulrich",
              "text": "I use both: Claude Code for my Claude subscription and OpenCode for everything else.\n\n\nThere are a few things that CC still does better, but for the most part OpenCode is definitely the better tool.",
              "score": 3,
              "created_utc": "2026-01-24 10:41:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ezvas",
                  "author": "wokkieman",
                  "text": "Any article / video you could point to why it's better? I read so many mixed stories?\n\nCurrently using roo code, but got curious to opencode. So far too many mixed comments about opencode to really start exploring",
                  "score": 1,
                  "created_utc": "2026-01-24 12:36:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1d8hte",
          "author": "cuba_guy",
          "text": "Definitely not from zero, some very respected top devs behind it. We've been using sst (serverless framework from the same people) for years at work as default",
          "score": 2,
          "created_utc": "2026-01-24 03:52:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmx7h0",
      "title": "Your own dashboard for oh-my-opencode v3.0.0+",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/pr8em15bkkfg1.png",
      "author": "TheDevilKnownAsTaz",
      "created_utc": "2026-01-25 22:12:04",
      "score": 34,
      "num_comments": 10,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qmx7h0/your_own_dashboard_for_ohmyopencode_v300/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1qfiu3",
          "author": "rroj671",
          "text": "Nice, I‚Äôll try it tomorrow. Is it possible to know what the sub-agents are doing at any specific time?",
          "score": 1,
          "created_utc": "2026-01-26 01:16:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qgypo",
              "author": "TheDevilKnownAsTaz",
              "text": "Depends on your specific idea of granularity. Right now, the background agents table gets updated live. So you get 1) accurate # of tool calls 2) the most recent tool being used by the subagent. Does this satisfy your use case? If not, let me know, and I can implement what you suggest in the next version!",
              "score": 1,
              "created_utc": "2026-01-26 01:24:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1qhks4",
                  "author": "rroj671",
                  "text": "Usually I just wonder if the sub-agents are doing something (and how many of them are they) or if they got stuck. I guess if I can see the time of the last thought, message, or tool-use, I‚Äôd know if they‚Äôre working.",
                  "score": 3,
                  "created_utc": "2026-01-26 01:27:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rm38r",
                  "author": "Rand_o",
                  "text": "This is also my gripe, the built in agent for opencode you could switch over to the agent and see exactly what it is doing like what steps its taking, commands writing, etc, and with OMO you have no visibility. So long running agent tasks I have no idea if it is working through something or it froze. A lot of time they freeze I feel like because I am using the new GLM-4.7-Flash and its pretty unstable",
                  "score": 2,
                  "created_utc": "2026-01-26 05:15:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rxmvw",
          "author": "Meriu",
          "text": "This looks wonderful! I would love to see this for all agents, not only oh-my-opencode",
          "score": 1,
          "created_utc": "2026-01-26 06:41:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2it7r8",
              "author": "TheDevilKnownAsTaz",
              "text": "V0.1.0 now also supports vanilla OpenCode!",
              "score": 2,
              "created_utc": "2026-01-30 01:33:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zeuyu",
          "author": "datosweb",
          "text": "esta piola el dashboard pero me genera dudad como maneja el refresco de los datos para que no se te clave la terminal con peticiones pesadas mas si usas un monton de plugins juntos ojala la optimizacion de memoria este bien pulida xq sino se pone lento",
          "score": 1,
          "created_utc": "2026-01-27 08:36:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ql85ej",
      "title": "Opencode Beginner crash course",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1ql85ej/opencode_beginner_crash_course/",
      "author": "StressPsychological4",
      "created_utc": "2026-01-24 00:34:41",
      "score": 32,
      "num_comments": 12,
      "upvote_ratio": 0.94,
      "text": "Hey All\n\nI've had a bunch of people reaching out asking for a straightforward way to get up to speed with Opencode. Instead of writing out long explanations, I figured I'd just make a video covering the essentials.\n\nA lot of people mentioned feeling overwhelmed by the options or not knowing where to start, so I tried to keep this focused and practical no fluff, just the core stuff you actually need to know to start using it.\n\nHere's what we go through:\n\n* Installation & setup\n* Default agents and how they work\n* Configuring models (including free options)\n* Creating your own custom agents\n* Sub-agents and how to use them effectively\n* Skills and how to build them\n* Permissions and security basics\n* Using the Open Agents Control repo for a faster setup\n\nThe whole thing is under 20 minutes. Figured this might help some of you who've been wanting to jump in but didn't know how.\n\nIf you're looking to get started with Opencode quickly, hopefully this saves you some time. And if you're already using it, let me know what else you'd want to see.\n\nCheers, and thanks for the requests.\n\n[![Opencode-setup-guide](https://img.youtube.com/vi/8toBNmRDO90/hqdefault.jpg)](https://youtu.be/8toBNmRDO90?si=v1bIb3SjUOP5_3Ll)\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1ql85ej/opencode_beginner_crash_course/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o1d266n",
          "author": "srcnix",
          "text": "Nice one.",
          "score": 3,
          "created_utc": "2026-01-24 03:13:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ew5gh",
              "author": "StressPsychological4",
              "text": "Thanks. Hope it helps clear about getting Opencode setup.",
              "score": 1,
              "created_utc": "2026-01-24 12:07:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1dfwvp",
          "author": "FuzzeWuzze",
          "text": "For me personally setting up MCP's was the most annoying thing.  Things like Roo etc already have like a library/store you just click a button and they download and configure themselves, not so much for Opencode.",
          "score": 2,
          "created_utc": "2026-01-24 04:41:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1erpa7",
              "author": "StressPsychological4",
              "text": "This might be a feature they add to the desktop version of Opencode. \n\nThere is a lot that happens under the hood when dealing with MCPs so didn‚Äôt want to include that in the video as felt there is a lot to explain with MCPs.\n\nAdding MCPs can negatively affect your coding experience due to how much context it can take up and something I would rather beginners not need to worry about right now.",
              "score": 1,
              "created_utc": "2026-01-24 11:29:50",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1dv9sj",
              "author": "e979d9",
              "text": "The docker MCP catalog should make things easier https://docs.docker.com/ai/mcp-catalog-and-toolkit/catalog/\nHaven't tried yet, but very curious¬†",
              "score": 1,
              "created_utc": "2026-01-24 06:37:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1cbyg6",
          "author": "StressPsychological4",
          "text": "Any feedback or requests are welcome.",
          "score": 4,
          "created_utc": "2026-01-24 00:41:07",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1djfmu",
          "author": "TraditionalDesk7039",
          "text": "how to optimize the token usage might be usefull",
          "score": 1,
          "created_utc": "2026-01-24 05:05:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1etxyd",
              "author": "StressPsychological4",
              "text": "Got a few videos on this but need updating. This is extremely important for Local models. \n\nThanks for the suggestion!",
              "score": 1,
              "created_utc": "2026-01-24 11:49:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1drhtv",
          "author": "Mindless_Art4177",
          "text": "Thank you for that.\nYou have perefect English that‚Äôs made it‚Äôs very easy to understand.\n\nThe part where you created subagents and its don‚Äôt called because naming issue wasn‚Äôt clear enough because you changed to a name not shown anywhere.\n\nBtw \nI have your repo installed (amazing work) - probably found it through awesome opencode repo.\n\nIn your agents you‚Äôre using xml tags to control sections of the instructions to agent, it‚Äôs too bad you didn‚Äôt explain about that either.\n\nThank you üôè",
          "score": 1,
          "created_utc": "2026-01-24 06:05:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eqxj0",
              "author": "StressPsychological4",
              "text": "I have a video on the xml part: \nhttps://youtu.be/BUIpHp2qLSI?si=g7RxcLFwK75WY_ab\n\nWill look to do another updated video one more focused on my repo and explaining the decisions. \nAppreciate the feedback.",
              "score": 2,
              "created_utc": "2026-01-24 11:23:04",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1epysj",
              "author": "StressPsychological4",
              "text": "The Subagent failed as the original main agent‚Äôs prompt wasn‚Äôt explicit enough to call my exact subagent and that‚Äôs why wasn‚Äôt called. \n\nEssentially under the hood opencode uses a Task tool to call agents and agent name is important for that call. \nSo in first prompt the Main agent saw call subagent but didn‚Äôt quite understand that there was a dedicated subagent for planning. So it called a general task agent which is not what we wanted and we wanted it to call our specialised agent. \n\nSo we had to explicitly highlight the name so the main agent would call it directly and not get confused.\n\nHope that helps clear some things up.",
              "score": 1,
              "created_utc": "2026-01-24 11:14:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2kh7ok",
          "author": "MattU2000",
          "text": "Thanks mate! Planning to use it alternative to Antigravity.",
          "score": 1,
          "created_utc": "2026-01-30 08:17:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp1b6e",
      "title": "Black 100 sub is too limiting as compared to Claude Max",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/18ye8k58o0gg1.png",
      "author": "Top_Shake_2649",
      "created_utc": "2026-01-28 04:24:54",
      "score": 32,
      "num_comments": 14,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qp1b6e/black_100_sub_is_too_limiting_as_compared_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o25uehb",
          "author": "jpcaparas",
          "text": "Yeah it's because of this:\n\nhttps://pub.towardsai.net/why-your-expensive-claude-subscription-is-actually-a-steal-02f10893940c?sk=65a39127cbd10532ba642181ba41fb8a\n\nYou‚Äôre paying $20 for $259 worth of api usage on a Claude Pro Plan and you're getting  $1.3k worth of api tokens on a $100 max plan and opencode black afaik, doesn't do the same level of subsidy that Anthropic does.\n\nSo if I were on OpenCode Black, I'd just stick to the low-cost Chinese models.\n\nEdit:\n\nI'll report back when I try out synthetic dot new",
          "score": 13,
          "created_utc": "2026-01-28 05:11:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o265y3y",
              "author": "aeroumbria",
              "text": "Is it really a \"steal\" if there exist providers offering similarly capable models at a flat API rate that is cheaper than the \"subsidised\" package? Seems more like self-inflicted cost bloat to me...",
              "score": 5,
              "created_utc": "2026-01-28 06:38:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o269lal",
              "author": "armindvd2018",
              "text": "In that case why black ? I can use NanoGPT just $8 !!!",
              "score": 2,
              "created_utc": "2026-01-28 07:07:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o26cr5x",
              "author": "Top_Shake_2649",
              "text": "yeah, I've read this article before, and I knew about it before jumping ship. That's why I am not really complaining as I have said. Still, slightly disappointed tho. Also, another thing, I might have accidentally used too much since MoltBot (ClawdBot) hype... üò¨",
              "score": 2,
              "created_utc": "2026-01-28 07:34:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o29ajqs",
                  "author": "kkordikk",
                  "text": "Nigha you buy 100 sub and plug into Clawdbot lmao",
                  "score": 0,
                  "created_utc": "2026-01-28 18:10:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29aga9",
              "author": "Tushar_BitYantriki",
              "text": "Well, you are comparing their price with THEIR price. That itself is a fallacy.\n\nIt's like comparing the cost of an iPhone with another iPhone. Playing right into the anchor.\n\nWhile in reality their price needs to be compared to other alternatives that are comparable. At this point, they no longer have the \"SOTA\" benefit anymore.\n\nGLM 4.7 is really killing it, and while I started planning with Opus and implementing with GLM, I have gradually moved to doing most of my planning with GLM-4.7\n\nHaven't tried Kimi, but it seems to beat them both on many benchmarks.\n\nSure, they are in their \"Give it all for cheap\" phase, but the price difference between Anthropic and everyone else is not proportionate.\n\nSo people really need to ask themselves if they really need Claude for either the sub or the APIs.\n\nAt this point, if you are writing some code that any AI company might be interested in stealing, then by all means stick to Claude or OpenAI (in case they are absolutely trustworthy). I hope no model is actually seeing any customer data to begin with.\n\nAnd if someone is really doing something that is so complicated that they feel only Claude can do it, then maybe, they either their employer or their revenue should be able to pay for it.\n\nFor everyone else, there are cheaper, and equally (in some cases, more) capable models.\n\nI have personally been weaning out my usage from Anthropic to GLM within Claude code. And at the same time, also moving my workflows to Opencode, for the day when Anthropic decides to also ban accounts for using other models in Claude code (I bet they are collecting those stats even when you are using a different model)\n\nAt this stage, I wouldn't care much, even if my account is banned. Since I moved to GLM, I haven't hit my Claude limits for 2-3 weeks. And already planning to move to $20 sub at the end of this month (already moved from 200 to 100 1.5 months ago)\n\nOr maybe, they will be happy that people who they were losing money on, are finally leaving.",
              "score": 2,
              "created_utc": "2026-01-28 18:10:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o262gmc",
          "author": "lundrog",
          "text": "Here is my set up. While its not perfect. I do think it works very well. Why? Because the claude code pro account hits a limit within minutes. Or slow performance elsewhere with overseas providers. Aka I can't afford a claude code max plan.\n\nI primarily use glm 4.7 for workflow with deep seek v3.2 for troubleshooting. But now k2.5 is out! \n\nI use claude code, with it the agent, Work flow works unattended for at least a few minutes. Opencode is good also, doesn't run as long unattended. \n\nFor agents https://github.com/VoltAgent/awesome-claude-code-subagents\n\nFor skills https://github.com/VoltAgent/awesome-claude-skills\n\nI am running it with this api gateway ( check your toc ) https://github.com/looplj/axonhub\n\nFor a main provider I use synthetic.new , great performance and privacy is much better than most. Text models but have optional image on demand available. You can back that up with Claude code or a ZAI account or anti gravity, etc. I believe official Claude code and anti gravity support are coming soon.\n\nI have a referral link  \"Invite your friends to Synthetic and both of you will receive $10.00 for standard signups. $20.00 for pro signups. in subscription credit when they subscribe!\" \n\nhttps://synthetic.new/?referral=UAWqkKQQLFkzMkY\n\nI am on my second month and on the $60 plan which gives you 1350 requests every 5 hours without a weekly limit. Should give about 5x a cluade code max plan.\n\nAnywho long story longer... It gives you a lower cost option with a higher quota to use with other plans via the api gateway. \n\n\nMaybe its helpful, ü§î\n\nGood luck on everything",
          "score": 2,
          "created_utc": "2026-01-28 06:10:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26cga8",
              "author": "Top_Shake_2649",
              "text": "[synthetic.new](http://synthetic.new) sounds promising, but I haven't heard much about it before. And the website doesn't give me confident. I rather consider go subscribe to [z.ai](http://z.ai) or moonshot directly?",
              "score": 1,
              "created_utc": "2026-01-28 07:31:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2754vj",
                  "author": "lundrog",
                  "text": "Small team, but its all hosted in usa based providers. Right now they have very generous limits; vs going direct from a cost perspective.",
                  "score": 3,
                  "created_utc": "2026-01-28 11:44:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2dcj44",
          "author": "telewebb",
          "text": "I'm sorry, I'm new to OpenCode black. Why are you not just throwing 10 bucks on OpenRouter and doing pay as you go instead of subscription? I don't understand what you get from a subscription.",
          "score": 1,
          "created_utc": "2026-01-29 07:07:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dji76",
              "author": "Top_Shake_2649",
              "text": "Yes great question. That‚Äôs exactly what I‚Äôm trying to find out with a black subscription. There is supposed to be some subsidy, so far it doesn‚Äôt seem like it.",
              "score": 1,
              "created_utc": "2026-01-29 08:08:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2flg4c",
          "author": "Ordinary-You8102",
          "text": "why not just use antigravity which have a lot models + github copilot which also have a lot of models for way cheaper? (both oauth)",
          "score": 1,
          "created_utc": "2026-01-29 16:11:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp0w55",
      "title": "Anyone using Kimi K2.5 with OpenCode?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qp0w55/anyone_using_kimi_k25_with_opencode/",
      "author": "harrsh_in",
      "created_utc": "2026-01-28 04:05:28",
      "score": 31,
      "num_comments": 41,
      "upvote_ratio": 0.94,
      "text": "Yesterday I did top up recharge for Kimi API and connected it with OpenCode via API. While I can see Kimi K2 models in the models selection, I can‚Äôt find K2.5 models. \n\nCan someone please help me with it?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qp0w55/anyone_using_kimi_k25_with_opencode/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o25nzgw",
          "author": "aeroumbria",
          "text": "`opencode models --refresh` and try again?",
          "score": 14,
          "created_utc": "2026-01-28 04:30:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2checj",
              "author": "jpcaparas",
              "text": "I'm only getting K2 thinking\n\nhttps://preview.redd.it/o4e4n2rfj7gg1.png?width=1002&format=png&auto=webp&s=2f62a5a4bceaf02f2992a5304ee1abf378244c4e",
              "score": 0,
              "created_utc": "2026-01-29 03:26:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2cie9d",
                  "author": "jpcaparas",
                  "text": "NVM, when I connected Synthetic provider I was able to see it \n\nhttps://preview.redd.it/9fwq380ik7gg1.png?width=1032&format=png&auto=webp&s=cc81eb47bc7ba7c31932b46c07ef63e93a3e1046",
                  "score": 1,
                  "created_utc": "2026-01-29 03:32:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27piz4",
          "author": "chiroro_jr",
          "text": "Yes. Used the moonshot 20$ sub. Create an API key. Updated open code. Then run auth login. Selected kimi code. Pasted my key. Done. No issues at all.",
          "score": 7,
          "created_utc": "2026-01-28 13:51:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cljdh",
              "author": "Nooddlleee",
              "text": "What is the code quality? Is it hallucinating on complex and long tasks?",
              "score": 2,
              "created_utc": "2026-01-29 03:51:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2d9a2d",
                  "author": "chiroro_jr",
                  "text": "Code quality depends on the code quality already in your codebase and your prompting skills for the most part. You can't expect top quality code in an app that has messy code and largely been vide coded and at the same time the prompts are bad. Most of these models generate decent code already. Kimi K2.5 is on the same level as an Opus or Codex, especially with a good codebase and a good prompt.\n\nEven Opus can produce shit code if it's working on shit code with a shit prompt.",
                  "score": 1,
                  "created_utc": "2026-01-29 06:39:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27sdme",
              "author": "Villain_99",
              "text": "Is the subscription better than Claude code ?\nPrice wise is same, wondering the consumption limits",
              "score": 1,
              "created_utc": "2026-01-28 14:06:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27x1t6",
                  "author": "chiroro_jr",
                  "text": "For me Kimi K2.5 is the first model that feels that close to Opus 4.5. Because it's dirt cheap, that bridges the gap. I have been using it to do tickets for the past 3 hours. It only failed to do exactly what I wanted probably once or twice. I corrected it and it immediately got back on track. So if it's not one shotting my requirements, the next prompt with do it. All for what? A fifth the price. For me this is the best value. Right now I am on their 20$ plan. 200 messages per 5 hour window. 2048 messages per week. I got a shit tonne of work done with my first 200 messages. I think Claude Code 20$ doesn't even have Opus. Only Sonnet and Hauku. Kimi 2.5 is definitely better than Sonnet.",
                  "score": 13,
                  "created_utc": "2026-01-28 14:30:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2cmfiz",
              "author": "shantz-khoji",
              "text": "For 20$ how many credits for it provides?",
              "score": 1,
              "created_utc": "2026-01-29 03:57:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2d9e42",
                  "author": "chiroro_jr",
                  "text": "2048 per week. 200 per 5 hour window. It's been enough for me  so far.",
                  "score": 1,
                  "created_utc": "2026-01-29 06:40:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2b2vc4",
              "author": "SunflowerOS",
              "text": "You can create a api ley with subscription? I subscribe on december but i didn't understand how connect the kimi with opencode I just cancellled it after the month",
              "score": 0,
              "created_utc": "2026-01-28 22:55:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hpkiv",
                  "author": "eduknives",
                  "text": "You can create here [https://www.kimi.com/code/console](https://www.kimi.com/code/console)",
                  "score": 1,
                  "created_utc": "2026-01-29 22:03:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26fmjm",
          "author": "shaonline",
          "text": "Don't forget to upgrade opencode (opencode upgrade) as it had to be added to models.dev.\n\nOverall it's pretty decent and the biggest improvement over, in my case, using GPT 5.2, is the speed, on the fastest providers (I'm using \"fireworks\" on OpenRouter, at the same API price) it reaches 100 tok/sec which is really good for execution I think. I might switch to it entirely as my \"executor/builder\".",
          "score": 4,
          "created_utc": "2026-01-28 07:59:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26pt2h",
              "author": "harrsh_in",
              "text": "How did you fix the error\n\n`invalid temperature: only 1 is allowed for this model`",
              "score": 1,
              "created_utc": "2026-01-28 09:33:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o26py0n",
                  "author": "shaonline",
                  "text": "I think you cannot set the temperature explicitely, it only has two hardcoded values (1 for thinking mode and 0.6 for non-thinking/instruct mode).",
                  "score": 1,
                  "created_utc": "2026-01-28 09:34:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27bjp1",
                  "author": "Phukovsky",
                  "text": "I'm getting this error too.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:30:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2fa1dn",
                  "author": "Ponchito147",
                  "text": "Add this to the opencode.json in the provider section:\n\n    \"moonshotai\": {\n          \"models\": {\n            \"kimi-k2.5\": {\n              \"temperature\": false,\n              \"interleaved\": {\n                \"field\": \"reasoning_content\"\n              }\n            }\n          }\n        }",
                  "score": 1,
                  "created_utc": "2026-01-29 15:21:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2cj4di",
          "author": "rokicool",
          "text": "I think I found a solution that worked for me. \n\nJust as everyone else I bought a $20 subscription from https://www.kimi.com/. Then I generated API Key at [Kimi Code Console](https://www.kimi.com/code/console?from=kfc_overview_topbar). \n\nAnd then I used `/connect` command in OpenCode and chose \"`Kimi For Coding`\" as a Provider (Not `Moonshot AI` !). Put the API Key and everything started working. \n\nHappy coding!",
          "score": 4,
          "created_utc": "2026-01-29 03:37:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28at2u",
          "author": "Juan_Ignacio",
          "text": "I tried the subscription directly from [https://www.kimi.com/](https://www.kimi.com/) and had no issues.\n\nTry running:\n\n    opencode upgrade\n    opencode models --refresh\n\nAfter that, you should be able to log in and use it in opencode without problems.  \nTo use it with oh-my-opencode, I had to define the model in `opencode.json` like this:\n\n    \"provider\": {\n      \"kimi-for-coding\": {\n        \"models\": {\n          \"kimi-k2.5\": {\n            \"id\": \"kimi-k2.5\"\n          }\n        }\n      }\n    }\n\nAlso worth mentioning: the 1-month subscription basically costs $1 if you use someone's referral link and send them jokes in the chat that opens until the price drops to $0.99. \n\nJust in case, here‚Äôs my referral link:  \n[https://www.kimi.com/kimiplus/sale?activity\\_enter\\_method=h5\\_share&invitation\\_code=AN9SGQ](https://www.kimi.com/kimiplus/sale?activity_enter_method=h5_share&invitation_code=AN9SGQ)\n\nThe jokes don‚Äôt have to be from Kimi specifically. You can generate them with any other AI and just copy paste them.",
          "score": 6,
          "created_utc": "2026-01-28 15:35:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bzr4h",
              "author": "rick_1125",
              "text": "you don't have to add provider in \\`opencode.json\\`, once you logged in provider will be generated in model list, just copy model name to \\`oh-my-opencode.json\\`, its \\`kimi-for-coding/k2p5\\` for kimi-k2.5",
              "score": 2,
              "created_utc": "2026-01-29 01:49:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2c5nwp",
                  "author": "Juan_Ignacio",
                  "text": "Thank! . I had written k2.5 and k2-5 instead of k2p5.",
                  "score": 1,
                  "created_utc": "2026-01-29 02:21:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2dp4xv",
              "author": "Appropriate_Yak_1468",
              "text": "https://preview.redd.it/d14c5y0479gg1.png?width=546&format=png&auto=webp&s=356b5cd6aef608a0ec848481e7d8877ab2acdc7c",
              "score": 2,
              "created_utc": "2026-01-29 09:01:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2g7rxb",
                  "author": "trenescese",
                  "text": "lmao it was legitimately funny to do that",
                  "score": 1,
                  "created_utc": "2026-01-29 17:52:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27fk5p",
          "author": "Simple_Split5074",
          "text": "I tried on nano-gpt, it's slow as molasses (like one rerquest per minute!) and occasionally tool calls fail or it simply gets stuck (no observable progress for 5+ min).\n\nMy suspicion: the inference providers do not have it completely figured out yet.\n\nMoonshot via openrouter was decent last night but now it crawls around at 15tps. Fireworks still claims to do 100+ tps but I have no idea if caching works with opencode and without it would get ruinous quickly.",
          "score": 2,
          "created_utc": "2026-01-28 12:56:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29ue7c",
          "author": "Ok-Connection7755",
          "text": "I upgraded opencode, did auth and then put the API key, works perfectly. So far I love the speed and responsiveness of the model. Will test more send post here.",
          "score": 2,
          "created_utc": "2026-01-28 19:36:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26nzqy",
          "author": "StrangeJedi",
          "text": "I didn‚Äôt see the 2.5 model either have they added it?",
          "score": 1,
          "created_utc": "2026-01-28 09:16:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27bsld",
          "author": "Phukovsky",
          "text": "I see two options available: Moonshot AI and Moonshot AI (China)\n\nI first tried entering API key for the (China) and I get invalid auth error. Then I added key for Moonshot AI, chose Kimi 2.5 and am now getting: \\`invalid temperature: only 1 is allowed for this model\\`",
          "score": 1,
          "created_utc": "2026-01-28 12:31:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27is6k",
          "author": "Phukovsky",
          "text": "I got an API key from [platform.moonshot.ai](http://platform.moonshot.ai) after adding some funds to my account there. I added the API key to Opencode but doesn't seem to work. Thinking I need an actual Kimi Code subscription and an API key from there instead?",
          "score": 1,
          "created_utc": "2026-01-28 13:15:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27q1g3",
          "author": "zach978",
          "text": "I see it on opencode zen, haven‚Äôt tried it though.",
          "score": 1,
          "created_utc": "2026-01-28 13:54:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a0zv2",
          "author": "Aggravating_Bad4163",
          "text": "I checked this on opencode with openrouter and it worked fine without any issues.",
          "score": 1,
          "created_utc": "2026-01-28 20:06:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fnh52",
          "author": "dxcore_35",
          "text": "This is direct answer from Kimi-K-2 official chat AI. Does it make sense?\n\nI need to be very direct with you: **You cannot use your Kimi Moderato plan with Claude Code. They are completely incompatible.**\n\nHere's why this won't work:\n\n# The Hard Technical Reality\n\n**Kimi Moderato Plan** = Subscription for Kimi's **chat interface only** (app and web)\n\n* ‚ùå Does **NOT** include API credits\n* ‚ùå Does **NOT** work with Claude Code\n* ‚ùå Does **NOT** work with any third-party tools\n\n**Claude Code** = Anthropic's CLI tool\n\n* Only connects to **Anthropic's API** (claude-3-5-sonnet, claude-3-opus)\n* Hardcoded to Anthropic's servers (`api.anthropic.com`)\n* Requires an **Anthropic API key**, not Kimi credential",
          "score": 1,
          "created_utc": "2026-01-29 16:20:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fnsvt",
              "author": "Ok_Box7357",
              "text": "Yeah, just noticed that weekly limits decreased: 2048 -> 100  \nThat sucks ...",
              "score": 1,
              "created_utc": "2026-01-29 16:22:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hp5pn",
                  "author": "Hyp3rSoniX",
                  "text": "no they switched it to % view, that's why it showed 100 for a moment. if you visit it again it should show some % number.",
                  "score": 1,
                  "created_utc": "2026-01-29 22:02:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2bcvqa",
          "author": "elllyphant",
          "text": "Yes you can use Synthetic's $20 standard sub for $12 (they're 40% off right now) to use kimi k2.5\n\nhttps://preview.redd.it/xttlpui8g6gg1.png?width=788&format=png&auto=webp&s=522494ed86621bccbde7188444f8e7b9892173ac",
          "score": 0,
          "created_utc": "2026-01-28 23:47:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dwwc2",
              "author": "kr_roach",
              "text": "What is difference between Synthetic‚Äôs and official moonshot ai",
              "score": 2,
              "created_utc": "2026-01-29 10:13:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bcwrp",
              "author": "elllyphant",
              "text": "[https://synthetic.new/?saleType=moltbot](https://synthetic.new/?saleType=moltbot)",
              "score": 0,
              "created_utc": "2026-01-28 23:47:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qq5m8o",
      "title": "Kimi K2.5 in opencode",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qq5m8o/kimi_k25_in_opencode/",
      "author": "t4a8945",
      "created_utc": "2026-01-29 11:02:09",
      "score": 29,
      "num_comments": 19,
      "upvote_ratio": 0.83,
      "text": "Hello,\n\nI'm a big fan of Opus 4.5 especially in opencode. Fits my workflow very well and enjoy the conversational aspect of it a lot.\n\nI'm always trying new models as they come, because the space is moving so fast and also because Anthropic doesn't seem to want me as a customer. I tried GLM 4.7, MiniMax-2, Devstral 2, Mistral Large 3, and I never was satisfied by the results. Too many errors that couldn't compete with what Opus 4.5 was delivering. I also tried GPT5.2 (medium or high) but I hate it so much (good work but the interactions are hell).\n\nSo I set Kimi K2.5 up to work with a SPEC.md file that I used in a previous project  (typescript node + react, status notification app) and here is how it went:\n\n* Some tool calls error with truncated input which halted the task (solved by just saying \"continue and be careful about your tool calls\")\n* It offered to implement tests, which none of the other models did\n* It had a functional implementation quite quickly without too many back and forth\n* It lacked some logic in the UI (missing buttons) but pointing it out led to a working fix\n* Conversation with it is on par with what I get from Opus, albeit it feels like a little bit less competent coworker ; but if feels GOOD.\n* The end result is very good!\n\nI highly recommend you try it out for yourself. It is better than I expected. (edit to clarify: not as good as Opus, but better than anything else I tried - \"better\" is  very personal as I tried to laid out above, it's more about the process than the end result)\n\nWhat is your experience with it? Did I develop some patience with these models or is it quite competent?\n\nedit: I'm using the official Kimi Code sub, as I've read integration in vendors can lead to less success in tool calls especially. Since this is open weight, not all providers are equal. See [https://github.com/MoonshotAI/K2-Vendor-Verifier](https://github.com/MoonshotAI/K2-Vendor-Verifier) for instance (they updated it for K2.5 and it should equalize vendors more, but keep that in mind)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qq5m8o/kimi_k25_in_opencode/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o2e546t",
          "author": "DistinctWay9169",
          "text": "I found Kimi 2.5 to be the most overrated model. I asked it to fix a problem I already knew how to fix, and it told me the problem was not what I was talking about. Then I told it, \"Then fix it with your solution\" and guess what. After a bunch of tokens spent on loop thinking, it did not solve the problem. This model is not better than opus at all. I found this model is great for a bunch of things, but for coding, it is meh.",
          "score": 9,
          "created_utc": "2026-01-29 11:23:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e96gd",
              "author": "patlux",
              "text": "Same for me. I compared it with the responses from Opus 4.5 and Opus make much more suggestions and asks better questions back than Kim 2.5.",
              "score": 3,
              "created_utc": "2026-01-29 11:54:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2e9jeo",
              "author": "mintybadgerme",
              "text": "Yep, I agree. It's vastly overrated. It's okay, but definitely nowhere near Opus.",
              "score": 2,
              "created_utc": "2026-01-29 11:57:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2enp58",
              "author": "jovialfaction",
              "text": "Unfortunately same for me.\n\nGot a month of Kimi Code subscription.\n\nFed it an issue in my codebase that was a bit tricky but definitely solvable.\n\nClaude got me the right fix. GLM 4.7 too. Kimi 2.5 started accusing database caches and offered a very complex fix that wouldn't have fixed anything.\n\nI'll still play with it, but disappointed in my first 24h",
              "score": 2,
              "created_utc": "2026-01-29 13:28:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2epxzx",
              "author": "aeroumbria",
              "text": "I am starting to assign specific models to specific tasks rather than trusting in a generalist model. I feel that Deepseek might be the best debugging / validation model. It is slow, and does not follow detailed workflow instructions very well (spent too much time debating what output document style to use), but it is very thorough, has maximum self doubt and almost zero self-confidence, and will actually debate with its former self, which is perfect for error catching. It is also markedly different in reasoning trace compared to most other models (probably due to different training data, heavier RL use and not relying much on distilling competitions), so in theory it should also be less prone to shared blind spots of other models.",
              "score": 2,
              "created_utc": "2026-01-29 13:40:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2h4oja",
              "author": "RegrettableBiscuit",
              "text": "I like it. I don't think anyone expects it to be as good as Opus, but unlike other open models that feel like a year behind Anthropic or OpenAI's current models, this feels more like six months behind.\n\n\nI could be fine with only using K2.5, which I can't say for models like GLM4.7.",
              "score": 1,
              "created_utc": "2026-01-29 20:24:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lm7b6",
              "author": "hey_ulrich",
              "text": "Interesting to hear this. I'm having a great experience with Kimi 2.5, and I have Claude Max and use Opus everyday. I mostly develop webapps with python backend and postgres. What kind of products and languages are you working with?",
              "score": 1,
              "created_utc": "2026-01-30 13:35:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2e5mfd",
              "author": "t4a8945",
              "text": "Interesting, what is your context (type of project, language, etc)?\n\nTo try it I just gave it my \"benchmark\" (start a new project from scratch, see how it works and interacts), but I'll keep throwing more cases at it to see out it fares.",
              "score": 1,
              "created_utc": "2026-01-29 11:27:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2e5sms",
                  "author": "DistinctWay9169",
                  "text": "Electron + Typescript + React.",
                  "score": 1,
                  "created_utc": "2026-01-29 11:29:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2e7sm4",
          "author": "Funny-Advertising238",
          "text": "I've been having incredible success with it! Better than any other open source model by far.¬†\n\n\nIf you've ever used Opus 4.5 and watched the way it thinks you know that kimi was definitely trained on Opus/Sonnet. The way it thinks and goes through tasks, and the way it responds, they definitely did the same scheme that deepseek did on openai.¬†\n\n\nNot saying it's Opus level but I personally love the way it goes through tasks and the interactions with it. Gpt 5.2 interactions makes my brain hurt sometimes.¬†\n\n\n\nI was having trouble with something that GPT 5.2 took an hour and still couldn't solve it, and kimi solved it in a few minutes.¬†\n\n\nNot sure how good it would be in the wild, one shotting etc. as my agents.md, skills and subagents setup is quite thorough.¬†\n\n\nBut for my use case it's absolutely killing it! Using the Kimi Coder plan too.¬†",
          "score": 7,
          "created_utc": "2026-01-29 11:44:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eaz4f",
              "author": "t4a8945",
              "text": "Haha we're the same! It feels very \"Opus\" in the interaction, your intuition about it being trained on it feels right.\n\nI'm just happy to have found again this \"coworker\" feeling when working with it. (Not like GPT5.2)",
              "score": 3,
              "created_utc": "2026-01-29 12:07:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2hwj82",
          "author": "kpgalligan",
          "text": "I've been dabbling. I'm on the CC 20x plan. Always assume the \"___ is as good as Opus\" is BS, but eventually it won't be. Maybe not as good, but at least usable. In the past I've found other models to be a mess with actual work.\n\nOn Kimi, I have to agree. So far. I'm only using it for analysis tasks, but it has handled tools well, which has not be true of other models I've tried (to be fair, 6+ months ago). I haven't swapped into any major tasks, mostly because I have plenty of Claude headroom, but will over time. Kind of on an urgent project at the moment so not a lot of \"play\" time.\n\nI do want to integrate it into our tool. We're building a focused coding agent. API costs are high, so if Kimi could handle analysis that is chewing up tokens, it would probably be a great option. Sometime in the next week or two, likely.",
          "score": 2,
          "created_utc": "2026-01-29 22:38:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kzsz2",
          "author": "LongBit",
          "text": "I tried it today for the first time on an issue Opus 4.5 could not fix.  Kimi 2.5 solved it without help.",
          "score": 2,
          "created_utc": "2026-01-30 11:04:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eckex",
          "author": "Michaeli_Starky",
          "text": "Alleged Opus killer? Yeah, expected",
          "score": -3,
          "created_utc": "2026-01-29 12:18:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ed0s4",
              "author": "t4a8945",
              "text": "I haven't stated that in the slightest. Read again",
              "score": 3,
              "created_utc": "2026-01-29 12:22:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2enb3o",
                  "author": "Michaeli_Starky",
                  "text": "I didn't say you said it. Read again",
                  "score": -2,
                  "created_utc": "2026-01-29 13:25:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qmefpw",
      "title": "What is your experience with z.ai and MiniMax (as providers)?",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/kukswiknpgfg1.png",
      "author": "mustafamohsen",
      "created_utc": "2026-01-25 09:14:43",
      "score": 25,
      "num_comments": 34,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qmefpw/what_is_your_experience_with_zai_and_minimax_as/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1p4f2s",
          "author": "TradeViewr",
          "text": "I had subscriptions with all providers and used all models, for professional coding only.\nGLM 4.7 is very good, at the level of Sonnet 4.5 but sooooo sloooowww.¬† Cannot work with that.¬† Minimax 2.1 is not good enough for my use case.¬† It nuked my codebase 2 times in 3 days I stopped using it immediately.\nHowever, their improvement is so impressing, that I believe that I will be probably using chinese models in a couple of months, if they keep improving their open source models at the same rate.\nI don't like Scam Altman's ClosedAI and all the packages with claude have heavy rate limits and I wont be paying 200$ for an ai subscription.¬† So I am waiting eagerly the next chinese big models because I think that they will be perfect for me if they match the current opus 4.5 at a lower price.",
          "score": 10,
          "created_utc": "2026-01-25 21:36:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1mkumr",
          "author": "MrBansal",
          "text": "Z.ai cheapest plan is slower compared to claude code cheapest plan. But it works . Certainly not good as claude. No experience of minimax",
          "score": 7,
          "created_utc": "2026-01-25 14:58:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1msqt3",
              "author": "mustafamohsen",
              "text": "By slower you mean lower tps, or service instability/disruption?",
              "score": 1,
              "created_utc": "2026-01-25 15:36:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1nsvkj",
                  "author": "awfulalexey",
                  "text": "Usually, if she gives an answer, it‚Äôs pretty quick. About 60-80 tokens per second. But to start getting this answer.......sometimes you have to wait.",
                  "score": 4,
                  "created_utc": "2026-01-25 18:11:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rdmh2",
          "author": "SynapticStreamer",
          "text": "z.ai was a struggle to get acclimated too, but now that I am, it's pretty great.\n\nIf you're not willing to change the way you do things, stay away from it. Like it or not, you're used to doing things the Anthropic way, and if you try to bring it to z.ai you're going to have a lot of issues.\n\nIt's a different tool, trained a different way, by different people, using different tools. It's not weird it requires you to change the way you've done things up until now.\n\nI still struggle every now and again, but once I ironed out the workflow I'm back to working confidently. I'm on the small base plan and max it out all the time. It's 3x the usage of the $20 Claude Pro plan I was on and even though I hit limits sometimes, it generally takes 3-4 hours, so I take a coffee break and come back and work.\n\nFor $28 it's a fuckin' steal.\n\nTake your time an invest heavily into learning how to use **sub agents**. They're the key to working with z.ai.\n\nIs it slow? Kinda. I find the `4.7-Flash` model is crazy slow. I try to delegate different sub agents to use different models. Like committing things to git doesn't require an insane amount of thought, so I use `4.7-flash` instead of `4.7` which saves on tokens, especially if you're committing a lot. Gaining context? Why do you need the equivalent of deepthink enabled just to get context about your project into the context window? Use a different less expensive agent. Doing that repeatedly saves you tons of usage--but like I said, it's been kinda slow lately because it's been getting much more popular.\n\nBut generally, I find it to be about the same as Sonnet 4.5. I'm generally not glued to the AI window anyways. I run long running commands and watch a movie. Who cares if it takes 5 minutes vs 8 minutes?\n\nGLM 4.7 is ranked as the 6th best LLM for coding in the world. Sonnet 4.5 is ranked as the first. The difference between them? 4.9% yet Claude cost me $240/yr, and GLM 4.7 cost me $28/yr.\n\n*So are you willing to spend 158% more for 4.9% performance?*",
          "score": 4,
          "created_utc": "2026-01-26 04:20:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1siffr",
              "author": "mustafamohsen",
              "text": "Thank you. Actually, the efficiency argument is irrefutable, in a sense. And yes, it's a steal anyway even if you use it just once or twice, and I don't care that much for speed as long as the service is stable and consistent\n\nBut I have no idea what \"doing things their way\" means. Would you give some insights?",
              "score": 1,
              "created_utc": "2026-01-26 09:42:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1wjaho",
                  "author": "SynapticStreamer",
                  "text": "With Claude you can generally have a conversation about your code base and say \"ok, let's implement those changes\" and the model is contextual enough to generally build what you want most of the time.\n\nGLM is a bit different. It requires more structure. I always use plan mode to converse with the agent about proposed changes that I want to make. Then I'll write them to file in a multi-phase implementation plan in manageable chunks so GLM doesn't confuse itself. Then I use sub agents. The main agent will pass only enough context for the sub agent to complete its task, so context is kept to a minimum and there's less of a chance for hallucination.",
                  "score": 1,
                  "created_utc": "2026-01-26 22:04:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1msusl",
          "author": "DistinctWay9169",
          "text": "Used both. Currently I am using the [Z.ai](http://Z.ai) Max plan. Much faster than the lit plan, and a much better experience overall (though it loses connection sometimes). Had a great experience with minimax regarding speed, but it is kind of stupid sometimes. I think minimax is made for speed and not for reasoning; it's kind of dumb for reasoning.",
          "score": 3,
          "created_utc": "2026-01-25 15:37:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o202jim",
          "author": "Malek262",
          "text": "I'm surprised by people who say that GLM 4.7 is excellent for coding and similar to 4.5. I don't know, honestly, my experience with it has been very bad. Maybe for simple tasks, okay, it's passable. But there's no comparison; on the contrary, I've used it more than once, trying to give it a chance for simple tasks, and it's full of bugs. I don't know, my experience with it, honestly, was bad. Okay, it's a good model for the price, but for simple things. As for comparing it to 4.5, there's a huge difference.",
          "score": 2,
          "created_utc": "2026-01-27 12:03:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27izzu",
              "author": "xmewa",
              "text": "Yeah, it gets lost very easily and struggles even with a lot of guardrails. Maybe it's okay for some autocomplete tasks. For me the worst thing is that it can be very wrong and very confident *at the same time*. So for now I left it to do the easy stuff, like driving the browser via chrome-devtools MCP.\n\nAs far as Opus 4.5 comparisons go, I really enjoy Codex and GPT-5.2.",
              "score": 1,
              "created_utc": "2026-01-28 13:16:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1lckra",
          "author": "adamhathcock",
          "text": "The cheapest z.ai is very slow for me.  I think Minimax was giving me better code as well as faster.  I‚Äôm primarily C# and late to the z.ai subscription.  Maybe they slow it down for us cheapskates.  Worth it for the low price still.\n\nI‚Äôm thinking of trying the low minimax sub too now.",
          "score": 2,
          "created_utc": "2026-01-25 09:46:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1llh3t",
              "author": "paperbenni",
              "text": "Their pricing page used to explicitly state that the cheapest plan is slower. I don't find it very slow though. It's slightly below Anthropic and miles above openAI in terms of speed, enough for my ability to verify its output to be the bottleneck (in most cases)",
              "score": 2,
              "created_utc": "2026-01-25 11:05:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1nfqq1",
                  "author": "adamhathcock",
                  "text": "After doing more today, I find it better.  Still worthwhile but maybe kilocode isn‚Äôt as good as opencode with it",
                  "score": 1,
                  "created_utc": "2026-01-25 17:16:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1msj5j",
          "author": "Visual_Weather_7937",
          "text": "GLM 4.7 is significantly worse compared to Opus.  Btw [z.ai](http://z.ai) is slow, and spending just $3 more is not worth it. I purchased the most expensive subscription and tried to refund it on the same day, but I had no luck. I have no experience with Minimax.  \nDON'T believe the hype surrounding GLM/z.ai; there are a lot of bots with referal links :)",
          "score": 3,
          "created_utc": "2026-01-25 15:35:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mtrsp",
              "author": "mustafamohsen",
              "text": "What $3 are you referring to?",
              "score": 1,
              "created_utc": "2026-01-25 15:41:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mv7ip",
                  "author": "Visual_Weather_7937",
                  "text": "about \"cheap\" sub in z ai",
                  "score": 2,
                  "created_utc": "2026-01-25 15:47:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1p5u9o",
          "author": "Bob5k",
          "text": "been using glm coding for long long time. I'm happy so far with glm, I'm happy aswell with the coding plan with tiny caveats as speed might be fluctuating across the day due to massive load glm coding plan currently has. \n\nSaying so, I'm more frequently using [synthetic](https://synthetic.new/?referral=IDyp75aoQpW9YFt) and their subscription these days as i just want \"unlimited\" concurrency (as much as your prompts quota will allow you to do within 5h basically) and speed is there. Especially for glm and minimax models. \nAlso i noticed that [minimax coding](https://platform.minimax.io/subscribe/coding-plan?code=HO46LCwAJ5&source=link) itself promises 100 prompts but they calculate this in a way of giving much much more than 100 prompts - as single prompt means 15 model calls and it seems that it's very efficiently calculated somehow - i tried to cap 100 prompts there with my usual workload and i was not able to do so (mainly webdevelopment). \nReflinks giving you additional discounts there - feel free to use them",
          "score": 2,
          "created_utc": "2026-01-25 21:42:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1n790w",
          "author": "Impressive_Job8321",
          "text": "How much does data retention and sovereignty factor into your choices?",
          "score": 1,
          "created_utc": "2026-01-25 16:39:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nmjlg",
          "author": "PayConstantAttention",
          "text": "The z.ai API is garbage",
          "score": 1,
          "created_utc": "2026-01-25 17:45:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nqwu7",
          "author": "Snoo_57113",
          "text": "For my usecase i tried both during the free period and chose minimax api for a month.",
          "score": 1,
          "created_utc": "2026-01-25 18:03:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1o75ni",
          "author": "richardlau898",
          "text": "I use minimax it‚Äôs great, surely not as good as Claude but I am generally satisfied",
          "score": 1,
          "created_utc": "2026-01-25 19:10:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1oikli",
          "author": "Zerve",
          "text": "GLM has really bad concurrency so if you use more than 1 agent at a time it will be very difficult to get much use out of it. You'd need to do a mix of 4.7 and 4.7 flash.",
          "score": 1,
          "created_utc": "2026-01-25 20:01:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qzivc",
          "author": "54tribes",
          "text": "with [synthetic.new](http://synthetic.new) you can access to both model. [Z.ai](http://Z.ai) cheapest plan is too slow, synthetic GLM 4.7 produced better result for me",
          "score": 1,
          "created_utc": "2026-01-26 02:58:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1t8ybc",
          "author": "BitcoinGanesha",
          "text": "I had experience with glm 4.7 at z.ai on max plan. The response speed is ok but time to first token is little slow.\n\nBut one of main problem with z.ai is same same like other providers. They start give you less quant model when you start actively using.\nI run 78 tasks in queue and after ~23 tasks the problems started. I started got words with errors. Some symbols was from non English language üò¢\nEvery task was done in sub agent with own context window. It was not context rot problem.\n\nBut if you vibe coding by hand I think you‚Äôll enough quotes and will not get same result as me.\n\nP.s. you can check cerebras.ai with payment for tockens. The speed is fantastic üëå but context window size is 120k and may be they compact count of experts üò¢",
          "score": 1,
          "created_utc": "2026-01-26 13:11:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wzrsi",
          "author": "TrajansRow",
          "text": "I‚Äòm using MiniMax; I find it a good balance of speed and performance.",
          "score": 1,
          "created_utc": "2026-01-26 23:23:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1xtm3n",
          "author": "iDonBite",
          "text": "I use the copilot sub and glm for research and small tasks so it doesn't consume my subscription limit. I mainly use gpt 5.2 codex because Gemini is cooked and Opus 4.5 is insanely expensive. GLM is extremely good at research, planning and doing  small tasks. I leave the big things to gpt.",
          "score": 1,
          "created_utc": "2026-01-27 02:00:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z8q93",
          "author": "EducationalBank5328",
          "text": "[z.ai](http://z.ai) for 6 euros a month is a steal. Sometimes it‚Äôs slow due to high API demand, but it gets the job done. I use it with OpenCode",
          "score": 1,
          "created_utc": "2026-01-27 07:41:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ldthi",
          "author": "No_Success3928",
          "text": "Why not both using synthetic?",
          "score": -2,
          "created_utc": "2026-01-25 09:58:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1q9wss",
          "author": "lundrog",
          "text": "I would recommend synthetic.new for both. Fast; private servers. I have a referral [hete](https://synthetic.new/?referral=UAWqkKQQLFkzMkY) \"Invite your friends to Synthetic and both of you will receive\n$10.00 for standard signups.\n$20.00 for pro signups.\nin subscription credit when they subscribe!\" \n\nGenerous requests and prices without a weekly cap.",
          "score": 0,
          "created_utc": "2026-01-26 00:48:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tf3ad",
          "author": "Competitive-Film9107",
          "text": "I've dropped my Claude Max 20x subscription, and now have a MiniMax subscription - do all my planning and specifications via OpenRouter to take advantage of using different models to pick apart my plans and specifications. Then once I have a solid spec, I pass it over to MiniMax to do the actual implementation with OpenCode in containers. \n\nUsing Opus 4.5 for everything prior, I could be a lot more loose with specifications and generally Opus would make a reasonable choice, where as I've seen MiniMax get lost and make strange decisions (given other instructions in the spec) or just stub something and defer it. That being said, everything I've had issues with, has been fixed by just adjusting my prompts or being more specific.",
          "score": 0,
          "created_utc": "2026-01-26 13:45:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lo8w6",
          "author": "WPDumpling",
          "text": "I run multiple Claude Code and OpenCode agents at a time, often with sub-agents & background tasks involved, all on a Z.ai Pro plan and I've never once hit a limit, whereas I was hitting them multiple times a day with a Claude Code subscription.\n\nAs for reliability: I've been using Z.ai since October and the only time I've had any issues is when they were under attack yesterday. Other than that, it's been rock-solid for me, even if it's not the fastest.\n\nConsidering it's like 1/12th the cost of Claude, with greater limits, I ***HIGHLY*** recommend Z.ai. If you want to use my referral code, I'll get some credit and you'll get an extra 10% off your first invoice: https://z.ai/subscribe?ic=SBFAJLK0FI",
          "score": -5,
          "created_utc": "2026-01-25 11:29:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmurxk",
      "title": "Why should I use my OpenAI subscription with Open Code instead of plain codex?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qmurxk/why_should_i_use_my_openai_subscription_with_open/",
      "author": "420rav",
      "created_utc": "2026-01-25 20:42:25",
      "score": 23,
      "num_comments": 31,
      "upvote_ratio": 0.93,
      "text": "I‚Äôm really interested in the project since I love open source, but I‚Äôm not sure what are the pros of using OpenCode.\n\nI love using Codex with the VSC extension and I‚Äôm not sure if i can have the same dev experience with Open Code.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qmurxk/why_should_i_use_my_openai_subscription_with_open/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o1otjwc",
          "author": "Repulsive-Western380",
          "text": "I‚Äôm using codex with open code ChatGPT plus subscription and never going back.",
          "score": 10,
          "created_utc": "2026-01-25 20:49:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1otqcp",
              "author": "420rav",
              "text": "Whats the difference between open code vs plain codex?",
              "score": 3,
              "created_utc": "2026-01-25 20:50:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1oumg1",
                  "author": "Repulsive-Western380",
                  "text": "The key difference in developer experience between OpenCode and Codex CLI is that OpenCode offers clearer, more detailed explanations for the same coding prompts, along with a faster and more reliable user interface in the terminal, making it easier and more efficient for developers to use AI-assisted coding tools, while Codex CLI provides similar core answers but often with less clarity, slower performance, and more reported bugs or usability issues.",
                  "score": 9,
                  "created_utc": "2026-01-25 20:54:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1owhtu",
          "author": "RedParaglider",
          "text": "The best thing about opencode is you can build workflows that utilize different LLM's.  If you JUST have an openai subscription then you can have an orchestrator running a small gpt model, when it needs to create an SDD and dependency map it could use the xhigh gpt5, then kick off that result to build an implementation plan to codex, and then code it up with codex.  You would get almost the exact same results as using GPT5 xhigh, but at a much reduced cost.  Enough of a reduced cost that you could then have a dialectical agent come in behind top level steps and validate results.\n\nThe downside is that there is a lot of learning.  If you use opencode I'd suggest also grabing some popular plugins to check out what is possibe such as oh my opencode.  OMO is kind of heavy, so its debatable on if it's a good daily driver BUT it absolutely is a good example of things that can be done in the system.",
          "score": 23,
          "created_utc": "2026-01-25 21:02:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1p1dk6",
              "author": "420rav",
              "text": "I was planning to get Claude Code too. Can I combine both? How?",
              "score": 0,
              "created_utc": "2026-01-25 21:23:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1p4ew0",
                  "author": "sudonem",
                  "text": "Yes, but you can no longer use a Claude code subscription with OpenAI - you‚Äôd have to use the more expensive pay as you go via API key model. \n\nAnthropic is moving to close off its subsidized subscription model in order to lock people in to their ecosystem if they aren‚Äôt going to pay b2b rates. \n\nWhich‚Ä¶ really sucks. But it‚Äôs theirs to do as they please.",
                  "score": 4,
                  "created_utc": "2026-01-25 21:36:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1phkmg",
                  "author": "RedParaglider",
                  "text": "The only way is to have opencode make an operating system level call to claude to engage a task off of the command line through a prompt.  Anthropic decided they wanted to be a closed ecosystem, which is odd because there are like 3 places I can go run opus 4.5 right now that aren't anthropic.",
                  "score": 2,
                  "created_utc": "2026-01-25 22:34:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ovi7l",
          "author": "TheOneThatIsHated",
          "text": "Stated plainly: opencode has just better ux. Terminal ui works better, has more quality of life features, etc. \n\nBut what's stopping you?\nTry codex cli, then try opencode\nYou're not paying more by trying both. \nGo nuts",
          "score": 6,
          "created_utc": "2026-01-25 20:58:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ovyvz",
              "author": "420rav",
              "text": "It has some more advanced features? Better support for mcp, skills, language server etc?",
              "score": 1,
              "created_utc": "2026-01-25 21:00:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1oxwnu",
                  "author": "TheOneThatIsHated",
                  "text": "Afaik codex cli has no lsp support at all. Opencode supports almost all languages by default without any configs. \n\nOpencode supports all the usual stuff that claude code also supports like agents.md claude.md skills mcp etc.\n\n\nIt has quality of life features like always including the files in cwd when prompting without an extra tool call. \n\nFurthermore, they have custom system prompts per model, instead of the same for all models. \n\nFull support for subagents and stuff like that. \n\nA vibrant github community, where issues are actually resolved and you get daily updates. \n\n\nAlso not to be understated is that their tui/ui engine is just for a lack of a better word 'better' \n\n\nAgain, just try it, you can judge yourself",
                  "score": 6,
                  "created_utc": "2026-01-25 21:08:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ovaee",
          "author": "420rav",
          "text": "Ok but how they can achieve that? They inject some prompts? What‚Äôs the perceived quality of the output?\n\nIm using the vsc extension for codex is way better than using terminal, does some like this exists for opencode?",
          "score": 2,
          "created_utc": "2026-01-25 20:57:08",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1oxve2",
              "author": "Repulsive-Western380",
              "text": "OpenCode gets better results by using big memory windows to remember the whole project, smart model choices for tasks, and efficient tools that run code without wasting space, while likely adding hidden instructions to make answers clearer; users see its output as high quality, reliable, and helpful for work without forgetting details. Yes, OpenCode has a VS Code extension like Codex‚Äôs, making it easier to use in the editor instead of just the terminal.",
              "score": 4,
              "created_utc": "2026-01-25 21:08:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1oyu9n",
                  "author": "Coldshalamov",
                  "text": "and opencode has a robust plugin ecosystem and is more easily hackable. I made a plugin for my opencode that loads in 5 of my subscriptions to various providers and cycles them based on task and remaining limit. I don't think that's happening on codex cli. Plus, defined subagents across providers",
                  "score": 3,
                  "created_utc": "2026-01-25 21:12:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1p1i3u",
                  "author": "420rav",
                  "text": "Can you link me the extension? Because I didnt find that",
                  "score": 0,
                  "created_utc": "2026-01-25 21:23:48",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1p19vb",
          "author": "Codemonkeyzz",
          "text": "Not sure if Codex CLI has these :  Plugins, Skills , Commands, Subagents/Primary Agents,  hooks ..etc.  \nAlso opencode allows you to have the same setup for different models.  e.g; you want to use different LLMs for different tasks, or switch between models while having the same setup.\n\nI often switch between GPT 5.2 , Opus 4.5 , Minimax 2.1 , GLM 4.7  for different tasks or when i consume my credits in one , i switch to the others.",
          "score": 2,
          "created_utc": "2026-01-25 21:22:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1q7z5r",
          "author": "meerestier",
          "text": "Opencode with chrome dev tools mcp server is very nice with the latest Gemini for web dev work.",
          "score": 2,
          "created_utc": "2026-01-26 00:39:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1pfbj5",
          "author": "ZeSprawl",
          "text": "I just like that I can use all providers with a common interface, including mcp, skills, sub agents and whatever else comes along, and if I ever need to modify the code itself I can do that too.\n\nAlso it uses a client server architecture, so the back end is just a REST endpoint, so you can run it without the TUI and make your own UI with their agent back end. They also allow you to run it with a web front end so you can use that remotely without the need to ssh from your phone.",
          "score": 1,
          "created_utc": "2026-01-25 22:24:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1shf9h",
          "author": "Rude-Needleworker-56",
          "text": "Even just for seeing the thinking tokens and tool calls, it makes a case. in codex one simply stare at screen without knowing what is happening and you go back to some other screen soon distracting yourself. Seeing the thinking summary and tool calls keeps you in loop.\n\nSecondly you can  get a second opinion from other models like opus in a couple of clicks.\n\nFor some tasks (like writing , UI etc) sonnet is  better than openai models\n\nIf you are power user opencode brings tons of customisability and front end options\n\n(But make sure that you add a apply\\_patch tool)",
          "score": 1,
          "created_utc": "2026-01-26 09:33:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1y4bc9",
          "author": "Apprehensive_Half_68",
          "text": "Opencode has LSP and Oh My Opencode, those 2 things alone are enough imo. Also with OC you can swap out underlying LLMs without relearning the workflows, shortcuts, skills, MCPs, etc",
          "score": 1,
          "created_utc": "2026-01-27 02:59:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o273495",
          "author": "PsHohe",
          "text": "I think it mainly comes down to whether you want the freedom to switch provider without having to bother setting things up for every tool again. Opencode lets you switch model, even during a conversation. Vendor specific tools may let you do that, but only their models. Having used all the tools, I can say that for 95% of the tasks, there's really not a lot of difference. If you want to only use OpenAI, there's no difference, use whichever fits you better.  \nThe real advantage comes if you want the freedom to switch and experiment.",
          "score": 1,
          "created_utc": "2026-01-28 11:29:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ovc22",
          "author": "mike3run",
          "text": "You should do whatever you want just try it out... or not it's up to you¬†",
          "score": 1,
          "created_utc": "2026-01-25 20:57:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlfqcb",
      "title": "OpenCode Black feedback?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1qlfqcb/opencode_black_feedback/",
      "author": "jpcaparas",
      "created_utc": "2026-01-24 06:37:12",
      "score": 23,
      "num_comments": 34,
      "upvote_ratio": 0.96,
      "text": "Hi all,\n\nI've been meaning to jump on the fence and start off with Black $100.\n\nAre there any of you guys under the Black $100 or $200 plan? Can you share your thoughts about how it fares relative to other providers, ie if you were on Claude Max $200 before, how does OpenCode Black $200 compare.\n\nThanks. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1qlfqcb/opencode_black_feedback/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o1ehkot",
          "author": "osvaldolovemachine",
          "text": "Yeah, I'm on Black 100 and I burned through my usage really quickly, 30% weekly allowance in 2 hours with Oh My Opencode. Claude Max I could use all day, rarely hitting limits. I'll probably look for something else now.",
          "score": 6,
          "created_utc": "2026-01-24 09:58:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1gdu6p",
              "author": "PhilosophyFluid8742",
              "text": "I‚Äôm on $200 black. I‚Äôve deemed that oMo doesn‚Äôt bring enough value. It burns too many tokens to get through one spec implementation. I think there‚Äôs is too much system prompt and context bloat with the plug-in - 70%+ context is used by the time a task is done (on a normal run). Think i may go back to standard plan+build loops, sprinkling 5.2 codex xhigh ok the side when needed. Use markdown files for everything.",
              "score": 5,
              "created_utc": "2026-01-24 17:02:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1o2rgi",
              "author": "lundrog",
              "text": " [here](https://synthetic.new/?referral=UAWqkKQQLFkzMkY) referral for synthetic.new,  \n\n\"Invite your friends to Synthetic and both of you will receive\n$10.00 for standard signups.\n$20.00 for pro signups.\nin subscription credit when they subscribe!\"\n\nThey're open source models but they have a $60 plan with 5x of a max plan. I did a month of the $20 plan and now am on the $60 which gives 1350 requests every 5 hours. No weekly caps.",
              "score": 1,
              "created_utc": "2026-01-25 18:52:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1sdifl",
                  "author": "Vaviloff",
                  "text": "So what models do you use?",
                  "score": 1,
                  "created_utc": "2026-01-26 08:57:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1dxxfd",
          "author": "PottsPointPilgrim",
          "text": "I got black 20 and couldn‚Äôt do anything out of it. Hit my weekly limit in 2 sessions of 10 minutes",
          "score": 9,
          "created_utc": "2026-01-24 06:59:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e9bwn",
              "author": "Old-Sherbert-4495",
              "text": "did u try copilot 10$ plan? looks promising I haven't tried my self",
              "score": 2,
              "created_utc": "2026-01-24 08:41:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1eaxif",
              "author": "fldc",
              "text": "Same experience, tried sonnet for 15 minutes and hit the limit, will not upgrade.",
              "score": 1,
              "created_utc": "2026-01-24 08:56:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1iqfqx",
                  "author": "ZeSprawl",
                  "text": "You expected them to subsidize your api tokens with investor money?",
                  "score": -1,
                  "created_utc": "2026-01-24 23:32:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1dy4qw",
              "author": "jpcaparas",
              "text": "jfc that's sad",
              "score": 1,
              "created_utc": "2026-01-24 07:01:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1dxylf",
          "author": "lukaboulpaep",
          "text": "I had a Claude Max (100 dollar) before and now using OpenCode Black (100 dollar) since yesterday but you definitely feel the difference in usage, used my whole 5 hour limit which is 32% of my weekly usage.\n\nComparing to Claude Max that would have been around 5-10% I think. It makes sense though, it‚Äôs still API billing",
          "score": 12,
          "created_utc": "2026-01-24 07:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e5yom",
              "author": "Codemonkeyzz",
              "text": "What about consumption rate of the other  models ? GLM , Minimax ...etc ?",
              "score": 2,
              "created_utc": "2026-01-24 08:10:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ehh40",
                  "author": "lukaboulpaep",
                  "text": "Have tried GLM, and Gemini Pro 3 as well. It goes slower but I think not at the level of usage you get from a Max subscription",
                  "score": 1,
                  "created_utc": "2026-01-24 09:57:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1dy71y",
              "author": "jpcaparas",
              "text": "thanks for the feedback. might just actually stick with a chatgpt plus sub for now.\n\nI wonder if they've fixed the premium requests bug for GH Copilot oAuth.",
              "score": 0,
              "created_utc": "2026-01-24 07:02:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1dycpq",
                  "author": "lukaboulpaep",
                  "text": "Yeah, it‚Äôs a pretty solid plan and my fallback as well. I have the plus plan and you def get a lot of usage and the codex models are solid. I will use the black plan if I need a different model, mostly using my codex plan for the heavy lifting",
                  "score": 3,
                  "created_utc": "2026-01-24 07:03:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1e1jhz",
                  "author": "vienna_city_skater",
                  "text": "Yes Copilot works well now. 2 weeks of professional work snd I haven‚Äôt burned through my premium requests yet. But then I also use Devstral or Gemini Flash as my subagent model and for trivial task such as commits, not Opus.",
                  "score": 2,
                  "created_utc": "2026-01-24 07:31:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ex7va",
                  "author": "BuildAISkills",
                  "text": "I think the plus plan is pretty reasonable, at least compared to Claude Pro.",
                  "score": 2,
                  "created_utc": "2026-01-24 12:15:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1h15c0",
              "author": "verkavo",
              "text": "Did you use it for complex tasks (like complex bug fixing), or more simple code generation.",
              "score": 0,
              "created_utc": "2026-01-24 18:43:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1i7ajo",
                  "author": "lukaboulpaep",
                  "text": "Just simple tasks for code generation, today I tested with only GLM 4.7 and it‚Äôs way more generous. I avoid using claude models unless I need to",
                  "score": 1,
                  "created_utc": "2026-01-24 21:57:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1dycz1",
          "author": "LostLakkris",
          "text": "On Claude max200, the cutoff totally ruined my workflow.\n\nStill on the black200 wait-list... I signed up within 30 minutes of the announcement tweet and still haven't been activated... So... I don't think they have the capacity?",
          "score": 5,
          "created_utc": "2026-01-24 07:03:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ezqcn",
          "author": "Philipp_Nut",
          "text": "So the Claude Max is still the better value ?",
          "score": 2,
          "created_utc": "2026-01-24 12:35:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1egnbw",
          "author": "WPDumpling",
          "text": "I've had excellent results using Z.ai as a backend. I literally haven't hit any kind of daily/weekly limits since switching.",
          "score": 1,
          "created_utc": "2026-01-24 09:49:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1fgki8",
              "author": "wilkie1990",
              "text": "Is the output as good as you would get with frontier models?",
              "score": 1,
              "created_utc": "2026-01-24 14:21:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1h1ihh",
                  "author": "WPDumpling",
                  "text": "I've never done a side-by-side comparison of the same project to really contrast the two; I think I'd rate it a little higher than /u/JuiceAffectionate477 and say it's maybe 70-90% as good as Opus/Sonnet\n\nWhat I CAN say is that, even if it's not quite as good as Opus/Sonnet and it sometimes takes a couple extra iterations to get where I want, I'm still WAY more productive because I went from hitting the limits every 5-hour period with Anthropic to not hitting them a single time since changing. And I got an entire YEAR of Z.ai for less than 1 month of Claude, with those higher limits.\n\nSo it might be 75% as good, but it's 1/12th of the price with higher limits.",
                  "score": 3,
                  "created_utc": "2026-01-24 18:45:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1g6axe",
                  "author": "JuiceAffectionate477",
                  "text": "In a score of 0-10, where 10 is opus and codex high. GLM would be like 5-6 for me. It's good, but not veryyy smart. For me it performs a bit better than gemini 3 flash, but, its not fast as the 3flash. it's quite good for some tasks. I use as hands for my big models.",
                  "score": 1,
                  "created_utc": "2026-01-24 16:28:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1gne2h",
          "author": "RedParaglider",
          "text": "What is on the opencode black platform?  Is it just GLM 4.7?  If that's the case is that even worth it?",
          "score": 1,
          "created_utc": "2026-01-24 17:44:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1iqqb9",
              "author": "ZeSprawl",
              "text": "No it has a full set of models including Claude and gpt models",
              "score": 1,
              "created_utc": "2026-01-24 23:34:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1dvrq6",
          "author": "jpcaparas",
          "text": "https://x.com/syedfahimanwer1/status/2012320103388553706?s=46\n\nhttps://x.com/mwikala_/status/2012620222016950305?s=46\n\nhttps://x.com/benkraus/status/2013291948006584812?s=46\n\nAsking because I'm getting mixed feedback from the twitterverse, but mostly skewing towards less value for money.",
          "score": 1,
          "created_utc": "2026-01-24 06:41:17",
          "is_submitter": true,
          "replies": []
        }
      ]
    }
  ]
}