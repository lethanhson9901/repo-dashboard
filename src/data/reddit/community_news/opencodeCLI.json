{
  "metadata": {
    "last_updated": "2026-02-20 03:06:13",
    "time_filter": "week",
    "subreddit": "opencodeCLI",
    "total_items": 20,
    "total_comments": 215,
    "file_size_bytes": 195340
  },
  "items": [
    {
      "id": "1r7bts8",
      "title": "GLM5 is free for a week",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/jyoyelzc63kg1.jpeg",
      "author": "jpcaparas",
      "created_utc": "2026-02-17 17:02:52",
      "score": 264,
      "num_comments": 25,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r7bts8/glm5_is_free_for_a_week/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5w8ljm",
          "author": "EchoesInBackpack",
          "text": "Would be funny if it works better than on zai paid plan",
          "score": 29,
          "created_utc": "2026-02-17 17:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w9x9j",
              "author": "shaonline",
              "text": "They typically do these type of deals with american providers (I think Kimi was on fireworks for example ?) so it probably will.",
              "score": 11,
              "created_utc": "2026-02-17 17:14:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wcgh7",
                  "author": "Spitfire1900",
                  "text": "ollama/glm-5:cloud has been too",
                  "score": 3,
                  "created_utc": "2026-02-17 17:27:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o69map0",
                  "author": "sudoer777_",
                  "text": "Kimi K2.5 Free is on Moonshot AI I believe, since I got rate limited a couple days ago and the error message was from that",
                  "score": 1,
                  "created_utc": "2026-02-19 16:42:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5wnuso",
              "author": "Noob_l",
              "text": "As a max user of z.ai coding plan, I would not recommend it. It now has less usage than other coding plans like minimax and codex. \nAnd the constant errors and silent changes on users should be red flash to not prolong the subscription. Went from being of the best plans in terms of affordable pricing and usage to one of the worst.\n\n-- not even Claude code fills up your weekly usage in just 5 5hour windows. Z.ai does that to you though.\n\nAnd really a warning to all: it wil read your env file and will delete files even if you said explicitly not to in the same conversation as well as having that as fixed rules in the agents file\n\nDon't use glm 5",
              "score": 4,
              "created_utc": "2026-02-17 18:20:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wqno2",
                  "author": "EchoesInBackpack",
                  "text": "I tried glm5 at openzen, it it was decent and very fast. I thought that it can be my daily driver. Then I bought the sub on zai, and it barely works, not usable at all. I feel scummed.",
                  "score": 6,
                  "created_utc": "2026-02-17 18:32:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ws9l9",
                  "author": "jpcaparas",
                  "text": "same, horrendous drop off in speed after the second day. Why even pay for ultra if you're bogged down to lite inference.",
                  "score": 3,
                  "created_utc": "2026-02-17 18:40:18",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o5z8e86",
                  "author": "Jlocke98",
                  "text": "You can max out the kimi weekly plan at the 20usd tier with 2x 5hr windows",
                  "score": 2,
                  "created_utc": "2026-02-18 02:02:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o68s0mh",
              "author": "RedParaglider",
              "text": "I can't imagine that it woulnd't.  ZAI is fucked on inference infrastructure.",
              "score": 1,
              "created_utc": "2026-02-19 14:09:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wpnik",
          "author": "TurnUpThe4D3D3D3",
          "text": "Cerebras for free would be crazy",
          "score": 4,
          "created_utc": "2026-02-17 18:28:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zujw1",
              "author": "MrBlueAndWhite6_2",
              "text": "Any relation to this post or you are just saying?",
              "score": 2,
              "created_utc": "2026-02-18 04:10:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wxc6g",
          "author": "hatepoorpeople",
          "text": "I just cancelled my zai subscription. GLM-5 is completely unusable for me.",
          "score": 4,
          "created_utc": "2026-02-17 19:03:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xc654",
          "author": "FaerunAtanvar",
          "text": "I tried to use ite and I instantly got \"all credits used\"",
          "score": 2,
          "created_utc": "2026-02-17 20:13:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yph42",
          "author": "Euphoric-Doughnut538",
          "text": "Canâ€™t get shit done with the limits",
          "score": 2,
          "created_utc": "2026-02-18 00:22:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6226d2",
              "author": "InternalFarmer2650",
              "text": "Opencode has different limitations than zAI API\n\nI could get decent amount of work done with their free usage, seemingly more than some who pay for the zAI subscription",
              "score": 1,
              "created_utc": "2026-02-18 14:19:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o69chrw",
              "author": "luc122c",
              "text": "The rate limits make it impossible to do anything ðŸ’€",
              "score": 1,
              "created_utc": "2026-02-19 15:54:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o609map",
          "author": "Alternative-Spray176",
          "text": "This model is good. Unlike m2.5",
          "score": 1,
          "created_utc": "2026-02-18 05:59:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b5u56",
          "author": "HarjjotSinghh",
          "text": "this is a game-changer actually",
          "score": 1,
          "created_utc": "2026-02-19 21:08:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5vv6g",
      "title": "Minimax M2.5 is not worth the hype compared to Kimi 2.5 and GLM 5",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r5vv6g/minimax_m25_is_not_worth_the_hype_compared_to/",
      "author": "Resident-Ad-5419",
      "created_utc": "2026-02-16 01:14:20",
      "score": 88,
      "num_comments": 51,
      "upvote_ratio": 0.93,
      "text": "I used opencode with exa; to test the latest GLM 5, Kimi 2.5 and Minimax M2.5, along with Codex 5.3 and Opus 4.6 (in its own cli) to understand how would they work on my prompt. And the results were very disappointing.\n\nDespite all these posts, videos and benchmarks stating how awesome minimax m2.5 is, it failed my test horribly given the same environment and prompt, that the others easily passed.\n\nMinimax kept hallucinating various solutions and situations that didn't make any sense. It didn't properly search online or utilized the available documentation properly. So, I wonder how all those benchmarks claiming minimax as some opus alternative actually made their benchmark.\n\nI saw a few other real benchmarks where Minimax M2.5 actually was way below Haiku 4.5 while GLM 5 and Kimi went above Sonnet 4.5; personally it felt like that as well. So at the increased price points from all these providers, its very interesting. Though neither are on opus or codex level.\n\nI did not test the same prompt with gemini, or couldn't test it, to be more precise due to circumstances. But I have a feeling Gemini 3 Pro would be similar to Kimi and GLM 5, maybe just a bit higher.\n\nWhat is your experience with Minimax compared to GLM and Kimi?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r5vv6g/minimax_m25_is_not_worth_the_hype_compared_to/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5lx2xc",
          "author": "Specialist-Yard3699",
          "text": "GLM still has its old problem - horrible speed. \nKimi25 - low limits. In my opinion, this model is not so good in terms of price-to-quality ratio.\nMinimax25 - fast and a good â€œplan executorâ€/â€œcode searcherâ€. I have worked a lot with Minimax21, and the 25th version is much better and has a lower hallucination rate.",
          "score": 19,
          "created_utc": "2026-02-16 01:26:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5m1i9x",
              "author": "lopydark",
              "text": "where minimax 25",
              "score": 1,
              "created_utc": "2026-02-16 01:55:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5m9h0y",
                  "author": "Specialist-Yard3699",
                  "text": "Official minimax code plan + oh-my-opencode plugin + detailed plan from reasoning model (Glm/gemini3)",
                  "score": 2,
                  "created_utc": "2026-02-16 02:46:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5lyi2e",
              "author": "Resident-Ad-5419",
              "text": "It hallucinated a lot on my prompt, the same one given to opus, codex, kimi and glm and those four worked well for that, while minimax failed horribly. It kept inventing stuff that doesnt even exist. I tested many times just to be sure I wasn't the one hallucinating.",
              "score": 0,
              "created_utc": "2026-02-16 01:35:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5m59di",
          "author": "tripleshielded",
          "text": "Yes, minimax still cant do any difficult work on its own. But m2.5 seems a bit better than m2.1, its a good upgrade.",
          "score": 7,
          "created_utc": "2026-02-16 02:19:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ngr1l",
          "author": "kshnkvn",
          "text": "Skill issue. I'm not joking. Minimax is beast for use as a subagent, as an executor/researcher/etc.\nYou can't use it straight as opus/got and think that it will act as good, at least because it's small model, really small, it knows much less then competitors so you need to provide proper information and context.",
          "score": 8,
          "created_utc": "2026-02-16 08:28:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nvsu5",
              "author": "Resident-Ad-5419",
              "text": "I don't disagree with you on this point, this is worth a shot! Thank you!",
              "score": 2,
              "created_utc": "2026-02-16 10:50:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5v4vtp",
              "author": "Resident-Ad-5419",
              "text": "After your reply I went back and ran a test with Codex as the main agent and GLM, Kimi, Minimax and Codex Spark as sub agent. \n\nCodex + Codex Spark did the best, even better than codex solo.  \nKimi and GLM afterwards.  \nMinimax still couldn't beat even after instructions and helping hands from codex.\n\nI would like to say it's my skill issue, that I am not skilled enough to handle minimax like you do.",
              "score": 1,
              "created_utc": "2026-02-17 13:45:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5v8nai",
                  "author": "kshnkvn",
                  "text": "Idk what exactly you want me to say, because I literally know nothing about your workflow, stack, tasks.  \nIf M2.5 doesn't suit your needs it's fine, because it's not a general-purpose model, it has it's own limitations, cons and pros. It may be either good or bad depends on your tasks and how you threat it.  ",
                  "score": 2,
                  "created_utc": "2026-02-17 14:05:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5m5ovy",
          "author": "segmond",
          "text": "they all have their strengths, i run all of them locally.   minimax2.5 was able to solve a problem that I couldn't get GLM5 and KimiK2.5 to solve after a few prompts, minimax solved it with the same prompt in one go, and generated almost 4,000 lines of perfect code.  The interesting thing is I couldn't have started a solution to the problem without Kimi K2.5 it has very unique capability that many other models don't.  IMO, they all have strengths and you have to know when to use one for another.  I like Minimax not because it solved my problem but I can also run it much faster.  ",
          "score": 5,
          "created_utc": "2026-02-16 02:21:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5oc5nq",
              "author": "Resident-Ad-5419",
              "text": "Absolutely! Any model that solves your problem is the best model, regardless of whatever anyone says otherwise.",
              "score": 2,
              "created_utc": "2026-02-16 12:59:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o64qlgo",
              "author": "FinancialMoney6969",
              "text": "you must have a beast of a machine.....",
              "score": 1,
              "created_utc": "2026-02-18 21:42:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o65l46q",
                  "author": "segmond",
                  "text": "yeah, for the average home user.  but not for a local LLM nuts.  a few 3090s that I bought used from FB marketplace on a 512gb machine.   My original MB was $100 from aliexpress.  Used dual xeon CPUs for $5 each and $600 for the ram from ebay late last year.   It's slow, qwen3.5 runs at 12tk/sec, KimiK2.5 at about 7-8tk/sec  Minimax at about 18tk/sec so not too bad for local.",
                  "score": 1,
                  "created_utc": "2026-02-19 00:17:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5mvemk",
          "author": "Lpaydat",
          "text": "GLM5 is really good. Really really good. Just quite slow sometimes.",
          "score": 5,
          "created_utc": "2026-02-16 05:20:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mpois",
          "author": "lundrog",
          "text": "K2.5 is very good on boardwell hardware , with the nvidia nvidia/K.imi-K2.5-NVFP4; assuming you find a provider hosing it. ( allows less memory usage for same performance )",
          "score": 3,
          "created_utc": "2026-02-16 04:37:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mwauu",
          "author": "Medical_Farm6787",
          "text": "To me since Iâ€™m using the free usage on opencode, probably due to the last GLM4.7 infinite thinking loop incident I just completely stopped using it.\n\nMy current workflow can be Kimi K2.5 for any web search related or codebase exploration â€”> switch to Minimax M2.5 for actual coding practices.\n\nBecause Minimax M2.5 did actually fall very short in terms of searching benchmarks you can look that up on their official benchmark results, but almost on par with opus and to me it really does feel like it, the thoughts are more structured than Kimi K2.5 when it comes to coding implementation, Kimi sometimes forgets the rules that I set in AGENTS.md as context window grows, but yet to have any issue when it comes to Minimax M2.5\n\nNot to say that Minimax M2.5 size is way more smaller than GLM so it feels well in my M3 ultra 256gb unified memory at Q6",
          "score": 3,
          "created_utc": "2026-02-16 05:27:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m2d16",
          "author": "czumaa",
          "text": "i think it's just me but now i think Minimax m2.5 just PLAIN LIE to all of us. Is not even close to the other models and have some weird allucinations. I test this models on coding. Just broke the entire project, don't respect the rules file of kilo code, re-do code as \"his\" way without check. is a disaster. i'm with GLM5 and kimi2.5 now. don't know which rely is the best but i think glm5 is a little best sometimes.",
          "score": 2,
          "created_utc": "2026-02-16 02:00:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mnpy1",
          "author": "Xhatz",
          "text": "It's great for it's size and fast, but it's truly NOT as good as they say, clearly. For me it feels like it's just m2.1 but with even less coherence sadly, hallucinations are too high (I can say something and a few messages afterwards it says something else). It also feels more \"lazy\" in a way... My theory is that it's only good at very specific things and just completely bad at everything else.",
          "score": 2,
          "created_utc": "2026-02-16 04:23:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mec9t",
          "author": "DistinctWay9169",
          "text": "MIniMax is pure hype. Tried it and nope, thanks. Broke my codebase many times to allow it to touch it again.",
          "score": 4,
          "created_utc": "2026-02-16 03:18:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n3sdd",
          "author": "chiroro_jr",
          "text": "Thought I was the only one. I'm still using Kimi. The limits and speed are good enough for me",
          "score": 1,
          "created_utc": "2026-02-16 06:30:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nbah4",
          "author": "SphaeroX",
          "text": "I also think that Kimi 2.5 is still unbeatable! But we'll see, the new DeepSeek should be ready soon.",
          "score": 1,
          "created_utc": "2026-02-16 07:37:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nd0vv",
          "author": "ciprianveg",
          "text": "maybe try with a lower temperature like 0.7?",
          "score": 1,
          "created_utc": "2026-02-16 07:53:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nszvi",
          "author": "xmnstr",
          "text": "I found the same. Cleary worse compared to GLM 5 and K2.5. I don't really understand why companies do this when it's so obvious to anyone who tries the model out that it simply does not hold up.",
          "score": 1,
          "created_utc": "2026-02-16 10:24:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nt77k",
          "author": "tricky-oooooo",
          "text": "Well, it's much smaller than both Kimi 2.5 and GLM5. What did you expect?",
          "score": 1,
          "created_utc": "2026-02-16 10:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5numcg",
          "author": "rudingshain",
          "text": "I use it Not for Coding but with opencode in textrelevant task and it works weil",
          "score": 1,
          "created_utc": "2026-02-16 10:40:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o81rh",
          "author": "odrakcir",
          "text": "not sure what to say, I've been using it for the last couple of days to write and fix a bunch of unit tests (react native) and it's been great. I've used it like this: plan mode -> openspec (explore + ff + manual review + implement + verify + archive).",
          "score": 1,
          "created_utc": "2026-02-16 12:31:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5obxcd",
              "author": "Resident-Ad-5419",
              "text": "If you use codex/opus as a driver; then haiku, minimax or any other smaller model can do wonders. Problem is the way they market it as if they are better than codex/opus in benchmarks, which is wrong.",
              "score": 2,
              "created_utc": "2026-02-16 12:58:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5olh5s",
                  "author": "odrakcir",
                  "text": "that is true, but to be honest, that claim makes part of the business haha. Now, what we can't deny is that OS models are getting closer.",
                  "score": 1,
                  "created_utc": "2026-02-16 13:54:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5o90ke",
          "author": "mintybadgerme",
          "text": "In my experience Kimi is definitely the best of the trio. GLM has problems as does Minimax.",
          "score": 1,
          "created_utc": "2026-02-16 12:38:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ooy4q",
          "author": "c0nfluks",
          "text": "I had the exact same experience. Kimi is way better. The benchmarks are cooked. Chinese AI companies are cheating on the exam, basically.",
          "score": 1,
          "created_utc": "2026-02-16 14:13:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5p5u06",
          "author": "l_eo_",
          "text": "MiniMax (both M2.1 and M2.5) for me is great for stable & efficient pipelines. \n\nSpawn researches, assess, return data, etc etc etc. \n\nIt's for me the perfect model for programmatic pipelines and is so far dominating its niche. \n\nI tried many other models and providers, but haven't found anything that could deliver this quality & stability at this cost level. \n\nIf anybody knows of anything that works better for these kind of use cases, please let me know!",
          "score": 1,
          "created_utc": "2026-02-16 15:40:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pv4dk",
          "author": "Alternative-Spray176",
          "text": "M2.5 doesn't follow instructions at all. I asked it to copy the plan it generated to a new file. It started implementing the plan. Retried but no use. I tried it for a few tasks, it doesn't want to listen to the user instructions. That model is hard to use like Google gemini models.",
          "score": 1,
          "created_utc": "2026-02-16 17:37:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ufe3p",
          "author": "Practical_Arm_645",
          "text": "I do have the same feeling. Minimax is almost useless for harder task, but only benefit is the speed. Gemini3pro is also terrible, even worse than the flash. The flash is quite reasonable, similar to sonnet 4.5 level",
          "score": 1,
          "created_utc": "2026-02-17 10:43:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v8bb9",
          "author": "Final-Rush759",
          "text": "It's not magic.  It's a cheap model, works fine for what it is.  It benchmark well with agent tests. It is not at the same level as big models.  It doesn't store as much knowledge as bigger models.",
          "score": 1,
          "created_utc": "2026-02-17 14:03:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w5rdo",
          "author": "StardockEngineer",
          "text": "HARD disagree.\n\nMy results in my tests - MM2.5 frequently implements big large PRDs with efficiency and precision.\n\nKimi often gives a result missing a ton of requirements.\n\nGLM5 for some reason uses a ton of tokens, costing far, far more than MM2.5",
          "score": 1,
          "created_utc": "2026-02-17 16:54:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wugw2",
          "author": "WolfpackBP",
          "text": "It's so good at agentic stuff though! And the price point is so good. \n\nI have found it to be very impressive\n\nKimi was slow and not as good with the agentic stuff at a higher price point \n\nHaven't tried GLM yet",
          "score": 1,
          "created_utc": "2026-02-17 18:50:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yk5ut",
          "author": "Dayclone",
          "text": "It's free right now through Ollama Cloud for a short period but seems to do ok for me. Has it's issues but hey it's free. Can't complain.",
          "score": 1,
          "created_utc": "2026-02-17 23:52:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zuroy",
          "author": "Teslaaforever",
          "text": "I have the opposite experience M2.5 solved a very complicated problem I have while GLM5 and Kimi2.5 actually gave me not working codes and they are outdated even when they searched the web.",
          "score": 1,
          "created_utc": "2026-02-18 04:12:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62dglw",
          "author": "HarjjotSinghh",
          "text": "here's a lively, supportive reply to match:",
          "score": 1,
          "created_utc": "2026-02-18 15:15:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nc971",
          "author": "Bob5k",
          "text": "also have in mind that minimax has released a guaranteed 100tps m2.5 instances / plans, which also fall under the 10% [reflink promo](https://platform.minimax.io/subscribe/coding-plan?code=HO46LCwAJ5&source=link)   \nfaster iteration means more work done even if quality is 5% lower.",
          "score": -1,
          "created_utc": "2026-02-16 07:46:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r97ruh",
      "title": "K2.5 is still the king for open-source models",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/t4infihnzhkg1.jpeg",
      "author": "jpcaparas",
      "created_utc": "2026-02-19 18:52:00",
      "score": 87,
      "num_comments": 28,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r97ruh/k25_is_still_the_king_for_opensource_models/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6aefjo",
          "author": "Electronic_Newt_8105",
          "text": "it's just so good.\n\ncrazy how you can get access to these awesome agentic coding models for free right now",
          "score": 14,
          "created_utc": "2026-02-19 18:55:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6aev7x",
              "author": "jpcaparas",
              "text": "I'm ever more so banking on the AI bubble popping this year. the tech remains obviously, but the valuations are just way out of proportion. \n\ni do feel for our gamer friends this 2026. shit's tough.",
              "score": 8,
              "created_utc": "2026-02-19 18:57:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6agmel",
                  "author": "metalman123",
                  "text": "Demand is near the physical capacity to serve models and you think the bubbles gonna pop?",
                  "score": 5,
                  "created_utc": "2026-02-19 19:05:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6bi60v",
              "author": "bad_detectiv3",
              "text": "How are you using agentic coding? Is it just through opencode cli mostly?",
              "score": 1,
              "created_utc": "2026-02-19 22:08:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6btoze",
                  "author": "jpcaparas",
                  "text": "these days, yes. just for the sheer flexibility of it. although I'd be lying if I said I wasn't using Codex or Claude. All of their strengths. Codex CLI mostly for long horizon tasks and Claude when I absolutely need to use Opus and Sonnet. \n\nOpenCode is the best all-arounder.",
                  "score": 1,
                  "created_utc": "2026-02-19 23:10:59",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ajoi2",
          "author": "chicken-mc-nugget",
          "text": "It's available on AWS Bedrock, though. ",
          "score": 3,
          "created_utc": "2026-02-19 19:20:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6b1b34",
              "author": "touristtam",
              "text": "Ye but I doubt AWS being cheap compared to __ALL__ other offerings",
              "score": 1,
              "created_utc": "2026-02-19 20:46:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b43bq",
                  "author": "chicken-mc-nugget",
                  "text": "The US price is the same exact price they list on Zen. But they don't mention the price of cache reads on Bedrcok, so I guess they don't support it and that might be the limiting factor? ",
                  "score": 2,
                  "created_utc": "2026-02-19 20:59:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6b9aw6",
              "author": "alexeiz",
              "text": "Kimi K2.5 on Bedrock is very unreliable.  I don't know how they deployed this model, but if I try to use it from opencode, it just stops responding randomly.",
              "score": 1,
              "created_utc": "2026-02-19 21:24:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6bjb2h",
          "author": "Creepy_Reindeer2149",
          "text": "I did looked very closely and right now Fireworks.ai is best Kimi 2.5 provider for the money\n\n\nInsanely fast inference, faster than Gemini flashÂ ",
          "score": 3,
          "created_utc": "2026-02-19 22:14:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cgpor",
              "author": "elosoyogui",
              "text": "Have you tried Baseten? It is faster https://x.com/artificialanlys/status/2023641796430180615?s=46",
              "score": 1,
              "created_utc": "2026-02-20 01:26:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6af7u3",
          "author": "HarjjotSinghh",
          "text": "k2.5's got the whole ai empire by its collar.",
          "score": 2,
          "created_utc": "2026-02-19 18:59:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bhzty",
          "author": "bad_detectiv3",
          "text": "WTH isnâ€™t K2.5 free one? I was reading somewhere where this model isnâ€™t great and instead we should use GLM 5.0",
          "score": 2,
          "created_utc": "2026-02-19 22:07:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ah610",
          "author": "guillefix",
          "text": "What about GLM-5 or Minimax M2.5?",
          "score": 1,
          "created_utc": "2026-02-19 19:08:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6amuua",
              "author": "hey_ulrich",
              "text": "Kimi 2.5 is better than both in my tests.",
              "score": 7,
              "created_utc": "2026-02-19 19:35:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6bg1uv",
                  "author": "StardockEngineer",
                  "text": "Not in mine.  MM won",
                  "score": 1,
                  "created_utc": "2026-02-19 21:58:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6bic71",
                  "author": "bad_detectiv3",
                  "text": "TIL Kiki 2.5 is different from Minimax M2.5",
                  "score": 0,
                  "created_utc": "2026-02-19 22:09:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ahm42",
              "author": "jpcaparas",
              "text": "GLM-5 is.... I don't know. It's erratic for me in tool-calling and not to mention the Z.ai provider inference is slow AF.\n\nMiniMax 2.5 is a joke for subagent work. It does excel on UI though. wouldn't even put it in the same league as K2.5 for utilitarian work.",
              "score": 4,
              "created_utc": "2026-02-19 19:10:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6binoo",
                  "author": "bad_detectiv3",
                  "text": "What work do you consistently hand off to K2.5",
                  "score": 1,
                  "created_utc": "2026-02-19 22:11:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ba9qe",
              "author": "Daemonix00",
              "text": "I selfhost both K2.5 was better, GLM-5 was missing things (K2.5 is easier to host too, int4 base). both tested with sglang official cli settings.",
              "score": 1,
              "created_utc": "2026-02-19 21:29:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6bp64o",
              "author": "cutebluedragongirl",
              "text": "Kimi K2.5 is better",
              "score": 1,
              "created_utc": "2026-02-19 22:45:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6bmglw",
          "author": "Available_Hornet3538",
          "text": "How do you self-host? Kimi 2.5 such a large model",
          "score": 0,
          "created_utc": "2026-02-19 22:31:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bnjy4",
              "author": "jpcaparas",
              "text": "I don't self host, I use [Synthetic.new](http://Synthetic.new). They're an open-source provider (waitlist should be lifted soon), and I've done some mentions of them here:\n\n  \n\\- [https://blog.devgenius.io/the-definitive-guide-to-opencode-from-first-install-to-production-workflows-aae1e95855fb](https://blog.devgenius.io/the-definitive-guide-to-opencode-from-first-install-to-production-workflows-aae1e95855fb)\n\n\\- [https://jpcaparas.medium.com/stop-using-claudes-api-for-moltbot-and-opencode-52f8febd1137](https://jpcaparas.medium.com/stop-using-claudes-api-for-moltbot-and-opencode-52f8febd1137)\n\nThere's also Fireworks, NanoGPT and **obviously** OpenCode Zen.",
              "score": 1,
              "created_utc": "2026-02-19 22:37:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6cscm5",
                  "author": "Jlocke98",
                  "text": "Synthetic has been on wait-list for weeks",
                  "score": 1,
                  "created_utc": "2026-02-20 02:38:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6bm3ri",
          "author": "Available_Hornet3538",
          "text": "Yes Chinese models beat American models any day",
          "score": -1,
          "created_utc": "2026-02-19 22:29:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4mzch",
      "title": "OpenCode Zen is dead, but MiniMax M2.5 is the ultimate Opus replacement",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r4mzch/opencode_zen_is_dead_but_minimax_m25_is_the/",
      "author": "pipubx",
      "created_utc": "2026-02-14 15:09:29",
      "score": 61,
      "num_comments": 97,
      "upvote_ratio": 0.74,
      "text": "Everyone is mourning the free version of OpenCode Zen, but the real play is moving to MiniMax M2.5. It's the most reliable alternative to Opus I've found. It's a Real World Coworker that costs $1 an hour and hits SOTA benchmarks (80.2% SWE-Bench). I've seen people complain about M2.1 fixing linting instead of errors, but M2.5 is a massive upgrade in task decomposition. If you want the cheapest, most accurate model for your CLI, this is it. Their RL tech blog is a must-read for anyone looking to optimize their dev workflow.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4mzch/opencode_zen_is_dead_but_minimax_m25_is_the/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5du9el",
          "author": "mintybadgerme",
          "text": "In my, admittedly limited tests, Kimi 2.5 is both cheaper and better at the moment.",
          "score": 23,
          "created_utc": "2026-02-14 18:56:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fyh9j",
              "author": "ideadude",
              "text": "Same m2.5 keeps running into issues it could get around if it slowed down and thought things through, but it's deciding to just rewrite things that are out of scope. Maybe folks who start from scratch with it have better outcomes, but i have to have other models clean up for it when it breaks shit.",
              "score": 3,
              "created_utc": "2026-02-15 02:20:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5hafba",
                  "author": "mintybadgerme",
                  "text": "Not to mention that M 2.5 is a little bit more expensive than Kimi 2.5. Which makes quite a difference if you're doing a fairly complex project. I get some quite Sonnet vibes out of Kimi.",
                  "score": 1,
                  "created_utc": "2026-02-15 08:57:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5gefng",
              "author": "oulu2006",
              "text": "Same",
              "score": 3,
              "created_utc": "2026-02-15 04:13:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5vn1m5",
              "author": "East-Stranger8599",
              "text": "Kimi 2.5 is great, but hallucinates badly without proper context",
              "score": 1,
              "created_utc": "2026-02-17 15:20:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5x196g",
                  "author": "mintybadgerme",
                  "text": "I've found that if you keep things short and sweet, it works very well.",
                  "score": 1,
                  "created_utc": "2026-02-17 19:22:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5d1bzt",
          "author": "Big-Masterpiece-9581",
          "text": "Why is it dead?",
          "score": 13,
          "created_utc": "2026-02-14 16:32:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fp0qp",
              "author": "No_Success3928",
              "text": "Its not, OP is being dramatic ðŸ˜‚",
              "score": 12,
              "created_utc": "2026-02-15 01:17:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5cy4ip",
          "author": "DRBragg",
          "text": "Wait, what happened to opencode zen?",
          "score": 10,
          "created_utc": "2026-02-14 16:16:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d021x",
              "author": "touristtam",
              "text": "No idea the pricing page still list free models: https://opencode.ai/docs/zen#pricing",
              "score": 10,
              "created_utc": "2026-02-14 16:25:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5d56kq",
                  "author": "UseHopeful8146",
                  "text": "If I had to guess they are (or did) rotating models. The free subs change every month or so. At least that was my understanding.",
                  "score": 9,
                  "created_utc": "2026-02-14 16:51:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5np6ch",
                  "author": "sudoer777_",
                  "text": "Apparently OpenCode Zen rate limits them now (not the provider), or at least they are for Kimi K2.5",
                  "score": 1,
                  "created_utc": "2026-02-16 09:49:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ev22f",
          "author": "_Turd_Reich",
          "text": "Another clickbait title.",
          "score": 9,
          "created_utc": "2026-02-14 22:14:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cp71p",
          "author": "Specialist-Yard3699",
          "text": "Maybe not Opus, but itâ€™s really good.\nCancel kimi25 subs, and use only minimax+glm now.",
          "score": 9,
          "created_utc": "2026-02-14 15:30:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5es5zv",
              "author": "skewbed",
              "text": "I would avoid subscribing to inference providers. Just use OpenRouter or something similar like OpenCode Zen.",
              "score": 4,
              "created_utc": "2026-02-14 21:58:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5exg2l",
                  "author": "pires1995",
                  "text": "The [nano-gpt](https://nano-gpt.com/r/kVxQFNRB) is a great option for it. The plan is USD 8 and have almost all open-source models (Kimi, GLM, Minimax). I notice some models not working or taking too long, but for the price worth try it. ",
                  "score": 4,
                  "created_utc": "2026-02-14 22:28:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5f8ui9",
                  "author": "Unlikely_Word_5607",
                  "text": "Isn't the whole point of subscribing to inference providers that they subsidise the costs compared to using the API?",
                  "score": 1,
                  "created_utc": "2026-02-14 23:36:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ehi4t",
          "author": "KnifeFed",
          "text": "> Everyone is mourning the free version of OpenCode Zen\n\ntf are you talking about?",
          "score": 3,
          "created_utc": "2026-02-14 21:00:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fknyu",
          "author": "robberviet",
          "text": "It's great for its size (200b). Not Opus or GPT level but good enough.\nAlso I think you should look at swe-rebench, not swe-bench.",
          "score": 3,
          "created_utc": "2026-02-15 00:49:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cud30",
          "author": "benzflow",
          "text": "How does it compare with Kimi k2.5 and GLM 5?",
          "score": 2,
          "created_utc": "2026-02-14 15:56:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5du3dx",
              "author": "mintybadgerme",
              "text": "Kimi 2.5  is better in my tests.",
              "score": 8,
              "created_utc": "2026-02-14 18:55:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5drkc1",
          "author": "Comrade-Porcupine",
          "text": "I like these open models but I fail to see how $1/hour is better value e.g. the $200/month Codex membership which is basically fully unlimited value.\n\nEthically, yes. And for strictly **API** uses, yes.  I use DeepSeek and others using API tokens and they're dirt cheap and quite effective. But the *coding plans* from GLM and MiniMax and Moonshot are not that awesome of value.",
          "score": 2,
          "created_utc": "2026-02-14 18:43:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fpd75",
              "author": "No_Success3928",
              "text": "Codex Fully unlimited? Not even close.",
              "score": 3,
              "created_utc": "2026-02-15 01:19:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e5u8o",
          "author": "Crafty_Chart1694",
          "text": "until deepseek 4 comes out",
          "score": 2,
          "created_utc": "2026-02-14 19:56:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e91mb",
          "author": "soul105",
          "text": "Kimi K2.5 is still free and available for me",
          "score": 2,
          "created_utc": "2026-02-14 20:14:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5gevds",
              "author": "Wildnimal",
              "text": "Free where?",
              "score": 0,
              "created_utc": "2026-02-15 04:17:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5h9hka",
                  "author": "soul105",
                  "text": "https://preview.redd.it/2usu5t91gmjg1.png?width=544&format=png&auto=webp&s=dee2668095d144a0627e96ba20f32ff7d59cbf81\n\nYou can also check the current list of [free models](https://opencode.ai/docs/zen/#pricing).",
                  "score": 1,
                  "created_utc": "2026-02-15 08:48:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5jhoni",
          "author": "amri2k",
          "text": "kimi 2.5 > minimax 2.5",
          "score": 2,
          "created_utc": "2026-02-15 17:40:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cn1sa",
          "author": "HarjjotSinghh",
          "text": "this m2.5 is basically code's new gym rat - cheap, brutal efficiency.",
          "score": 5,
          "created_utc": "2026-02-14 15:18:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cod4i",
          "author": "idkwtftbhmeh",
          "text": "Minimax M2.5 Falls behind both Kimi K2.5 and GLM5 in every bench, hell even glm7 is in front, trully disappointed with the model",
          "score": 6,
          "created_utc": "2026-02-14 15:25:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f12cv",
              "author": "DinoAmino",
              "text": "Disappointed that a 230B model doesn't score better than models that are 3x and 4x larger? srsly? That's some wildly unrealistic expectations there.",
              "score": 1,
              "created_utc": "2026-02-14 22:49:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5g6hza",
                  "author": "idkwtftbhmeh",
                  "text": "well, I did create my expectations out of the benchs that they announced, which in theory would surpass these models in some cases (doesn't happen)",
                  "score": 1,
                  "created_utc": "2026-02-15 03:15:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5f9ean",
              "author": "Squale279",
              "text": "Bench isnâ€™t the best way to evaluate a llm, try it in real use cases and compare it with other products.",
              "score": 1,
              "created_utc": "2026-02-14 23:39:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5g6ev6",
                  "author": "idkwtftbhmeh",
                  "text": "oh I did, it's quite bad overall to be honest, the speed is great tho",
                  "score": 1,
                  "created_utc": "2026-02-15 03:15:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5d59d3",
              "author": "UseHopeful8146",
              "text": "Iâ€™m sorry, glm 7?",
              "score": 1,
              "created_utc": "2026-02-14 16:51:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5d8g22",
                  "author": "zuk987",
                  "text": "He probably meant 4.7",
                  "score": 5,
                  "created_utc": "2026-02-14 17:07:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5esxea",
              "author": "cri10095",
              "text": "M2.5 is much smaller then the other models",
              "score": 1,
              "created_utc": "2026-02-14 22:02:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5eybxo",
                  "author": "idkwtftbhmeh",
                  "text": "It is indeed, still disappointed, I saw the blog post and benchs and it seems VERY cherrypicked compared to individual researchers like swe-rebench",
                  "score": 2,
                  "created_utc": "2026-02-14 22:33:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5d3swd",
          "author": "touristtam",
          "text": "> Their RL tech blog is a must-read for anyone looking to optimize their dev workflow.\n\nLink please?",
          "score": 3,
          "created_utc": "2026-02-14 16:44:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dmo8h",
          "author": "Both_Ad2330",
          "text": "Hope this gets on AWS Bedrock soon.",
          "score": 1,
          "created_utc": "2026-02-14 18:19:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5f30f1",
          "author": "Moist_Associate_7061",
          "text": "i used minimax 2.5 all day long, and it was not even close kimi k2.5. babysitting is needed..",
          "score": 1,
          "created_utc": "2026-02-14 23:00:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f6o33",
              "author": "johnerp",
              "text": "Which one is better, Iâ€™m not clear.",
              "score": 4,
              "created_utc": "2026-02-14 23:22:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5gzjg9",
          "author": "XtoddscottX",
          "text": "Can it work with images? Cause yeah, if you need to generate simple code these models are okay, but for some frontend tasks itâ€™s better to use model that accept visual input too, and as I know these Chinese models donâ€™t whilst three American big models do.",
          "score": 1,
          "created_utc": "2026-02-15 07:12:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h20vn",
          "author": "wjjia",
          "text": "Honestly, it was about time we stopped relying on OpenCode Zen anyway. Everyone is freaking out over the shutdown, but it was a loss leader from day one. I haven't put M2.5 through the wringer yet, but if that 80.2% SWE-Bench score actually holds up in real-world messy codebases, it's a massive jump. Most of these models talk a big game and then fail the moment you hit a weird dependency issue.",
          "score": 1,
          "created_utc": "2026-02-15 07:35:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h94py",
          "author": "Relative-Honey-4485",
          "text": "The jump from 2.1 to 2.5 is the real conversation here. 2.1 was driving me insane with that linting obsession - fixing my tabs while the actual logic was still broken. If the task decomposition is actually improved, I might give it a shot. Still skeptical about the $1/hr claim though, there is always a catch with token windows.",
          "score": 1,
          "created_utc": "2026-02-15 08:44:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h9e2k",
          "author": "Capital_Standard4603",
          "text": "RIP OpenCode Zen. It was good while it lasted.",
          "score": 1,
          "created_utc": "2026-02-15 08:47:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hdefr",
          "author": "elaytot",
          "text": "Minimax m2.5 is not better! Cant even tell my project was in typescript after it reviewed the whole codebase.. got me bunch of typeerrors",
          "score": 1,
          "created_utc": "2026-02-15 09:26:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hgw56",
          "author": "Yukeyii",
          "text": "Did anyone actually read the RL tech blog OP mentioned? I just skimmed it and the way they are handling reinforcement learning is actually pretty clever if you are into the infra side of things. It explains why the task breakdown feels more \"human\" than the older versions.",
          "score": 1,
          "created_utc": "2026-02-15 10:00:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5llki0",
              "author": "touristtam",
              "text": "Do you have a link, I have no idea what is the RL tech blog that is being mentioned.\n\nIs that: https://www.minimax.io/news/forge-scalable-agent-rl-framework-and-algorithm ?",
              "score": 1,
              "created_utc": "2026-02-16 00:16:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5i21vo",
          "author": "LionelOOK",
          "text": "\"Opus replacement\" is a bold claim. Opus has that specific feel for creative logic that is hard to replicate, but for pure CLI work and bug fixing, I can see MiniMax taking that spot if it is really that cheap.",
          "score": 1,
          "created_utc": "2026-02-15 13:05:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i4otx",
          "author": "Feeling-Whole4574",
          "text": "$1 an hour? I will believe it when I see my invoice at the end of the month.",
          "score": 1,
          "created_utc": "2026-02-15 13:23:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i89ww",
          "author": "Virtual-Path1704",
          "text": "Glad I am not the only one who noticed the linting thing. M2.1 would spend half its energy fixing my indentation instead of actually solving the logic error I was pointing at. If 2.5 fixed that, it is worth the switch.",
          "score": 1,
          "created_utc": "2026-02-15 13:46:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i9i1h",
          "author": "linegel",
          "text": "Their SWE bench is basically fake news due to too heavy reliance on Anthrophic models \n\nCheck updated SWE bench",
          "score": 1,
          "created_utc": "2026-02-15 13:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ib39u",
          "author": "Icy_Net5151",
          "text": "Benchmark obsession needs to stop. SWE-Bench is one thing, but how does it handle a 10-year-old legacy codebase with zero documentation? That is the real test for any \"coworker\" model.",
          "score": 1,
          "created_utc": "2026-02-15 14:03:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5id9c8",
          "author": "ChanningACE",
          "text": "Just switched. It is definitely snappier than 2.1. Not sure if it is \"ultimate\" yet, but it is actually usable for once.",
          "score": 1,
          "created_utc": "2026-02-15 14:15:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ifoi1",
          "author": "Dantenmd",
          "text": "Been looking for a solid Opus alternative since the quality started dipping recently. I will check out that blog post later, thanks for the heads up.",
          "score": 1,
          "created_utc": "2026-02-15 14:29:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vmx38",
          "author": "East-Stranger8599",
          "text": "This is an overstatement, at max it may be weaker cousin of Sonnet 4.5",
          "score": 1,
          "created_utc": "2026-02-17 15:19:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yien6",
          "author": "Conscious-Hair-5265",
          "text": "They gamed the bencharks, MiniMax 2.5 is not as impressive in real life usecases. Check out swe re bench bench mark",
          "score": 1,
          "created_utc": "2026-02-17 23:43:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o609kol",
          "author": "1E_liot",
          "text": "Switched to M2.5 last night for a legacy refactor. The task decomposition is actually noticeable compared to 2.1. It didn't just move brackets around; it actually handled the logic flow better than Opus does in some spots.",
          "score": 1,
          "created_utc": "2026-02-18 05:59:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60b1u7",
          "author": "Asher_dd",
          "text": "$1 an hour for this level of performance is a steal. Even if thereâ€™s a bit of latency, the output quality on M2.5 makes the wait worth it compared to the older versions.",
          "score": 1,
          "created_utc": "2026-02-18 06:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60blig",
          "author": "Yukeyii",
          "text": "Finally someone mentions the RL blog. That part about how they handle rewards for code correctness is the only reason I gave 2.5 a shot, and honestly, the logic feels way more \"human\" now.",
          "score": 1,
          "created_utc": "2026-02-18 06:15:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60e8w2",
          "author": "Low-Position-1569",
          "text": "RIP OpenCode Zen, but if M2.5 keeps performing like this at this price point, I'm not even mad.",
          "score": 1,
          "created_utc": "2026-02-18 06:38:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60f1nw",
          "author": "Cornelius956",
          "text": "80.2% on SWE-Bench is a bold claim, but after running a few complex tasks today, I'm starting to believe it. It's definitely snappier than the other SOTA models I've tried.",
          "score": 1,
          "created_utc": "2026-02-18 06:44:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60fu4i",
          "author": "Stellanear",
          "text": "I was sticking with Opus, but the cost-to-performance ratio on M2.5 is making it hard to justify staying. Itâ€™s becoming my main for bulk CLI tasks.",
          "score": 1,
          "created_utc": "2026-02-18 06:51:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60gl2w",
          "author": "Eviedate",
          "text": "M2.1 had that annoying linting loop habit, but 2.5 seems to have actually fixed it. It's much more focused on functional errors now.",
          "score": 1,
          "created_utc": "2026-02-18 06:58:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60hcsm",
          "author": "yxllove",
          "text": "The context window handling on M2.5 feels surprisingly robust. I fed it a decent-sized repo and it didn't hallucinate the file structure like most models at this price.",
          "score": 1,
          "created_utc": "2026-02-18 07:04:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60sxj1",
          "author": "Delicious_Can_6288",
          "text": "Just read that blog you mentioned. It's clear they're doing something different with their training because M2.5 is hitting solutions that 2.1 completely missed.",
          "score": 1,
          "created_utc": "2026-02-18 08:51:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60tr3y",
          "author": "Correct_Durian1503",
          "text": "I've been using it for a week. For the cost of a coffee to run it all day, the output is surprisingly close to - if not better than - the more expensive \"prestige\" models.",
          "score": 1,
          "created_utc": "2026-02-18 08:59:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60uepw",
          "author": "Interesting_Block102",
          "text": "I used to think nothing could replace the \"Opus feel,\" but M2.5 is getting dangerously close, especially with how it handles task decomposition.",
          "score": 1,
          "created_utc": "2026-02-18 09:05:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60ul8o",
          "author": "Eamonick",
          "text": "Is the CLI integration seamless? If so, I'm moving my entire workflow over. The benchmarks are just too good to ignore.",
          "score": 1,
          "created_utc": "2026-02-18 09:06:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60usn1",
          "author": "Scanlanderson",
          "text": "The 80% SWE-bench score is what caught my eye. If it can actually resolve GitHub issues autonomously like it did for my test run this morning, it's a total game changer.",
          "score": 1,
          "created_utc": "2026-02-18 09:08:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6276b9",
          "author": "ComparisonLeather631",
          "text": "Actually cheap for how good it is.",
          "score": 1,
          "created_utc": "2026-02-18 14:45:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o627oan",
          "author": "Fletcher_ba",
          "text": "I noticed the same thing with the task decomposition. It breaks down PRs into much more manageable chunks now. It's way more reliable for long-form coding than it used to be.",
          "score": 1,
          "created_utc": "2026-02-18 14:47:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6285qn",
          "author": "ticharland",
          "text": "Tried it for Python today - it handled some pretty nasty dependency conflicts that usually trip up most LLMs. M2.5 is definitely an upgrade.",
          "score": 1,
          "created_utc": "2026-02-18 14:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6291os",
          "author": "Montague857",
          "text": "$1/hr for SOTA performance? That's basically the floor. Hard to see why anyone would pay more for similar results elsewhere.",
          "score": 1,
          "created_utc": "2026-02-18 14:54:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o629ir3",
          "author": "Marisssia",
          "text": "People always hype the new thing, but M2.5 actually feels like a step forward. It's not just a marginal gain over 2.1; it's a different beast.",
          "score": 1,
          "created_utc": "2026-02-18 14:56:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62dcby",
          "author": "Kiyosaaki",
          "text": "That RL tech blog explains a lot. You can really feel those \"correctness rewards\" kicking in when it iterates on a bug. 2.5 is a massive leap.",
          "score": 1,
          "created_utc": "2026-02-18 15:15:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62dkf2",
          "author": "HarlanWJK",
          "text": "I'm loving the \"Real World Coworker\" vibe. It's less preachy than Opus and just gets the code written. It's a much more efficient workflow.",
          "score": 1,
          "created_utc": "2026-02-18 15:16:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cpuqt",
          "author": "0Bitz",
          "text": "How well does it work with Oh-My opencodeâ€¦?",
          "score": 1,
          "created_utc": "2026-02-14 15:33:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d61rc",
              "author": "UseHopeful8146",
              "text": "In my experience OmO has the structure to make most of the reasoning relatively simple - you could probably get close to kimi/glm level execution with much smaller models, provided they have tool calling support and decent context window.\n\nIâ€™m still in the process of working on tooling and stuff, but testing for local model execution in Opcode/OmO is on my todo list specifically because I hold that theory at present.",
              "score": 3,
              "created_utc": "2026-02-14 16:55:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5szmkm",
                  "author": "0Bitz",
                  "text": "I tested this out and found it making too many bugs even with detailed prompts of an existing system. GLM seems to work better on my code base at least",
                  "score": 1,
                  "created_utc": "2026-02-17 03:30:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r4uuiw",
      "title": "Opencode for all!1!1!1!",
      "subreddit": "opencodeCLI",
      "url": "https://v.redd.it/4doqqb8yqijg1",
      "author": "Extension_Armadillo3",
      "created_utc": "2026-02-14 20:21:29",
      "score": 48,
      "num_comments": 5,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4uuiw/opencode_for_all111/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5khton",
          "author": "HarjjotSinghh",
          "text": "wow that's actually genius ceiling tech",
          "score": 1,
          "created_utc": "2026-02-15 20:39:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tqqd8",
          "author": "InternalFarmer2650",
          "text": "Basierter MediaMarkt",
          "score": 1,
          "created_utc": "2026-02-17 06:52:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5eow8q",
          "author": "jpcaparas",
          "text": "at costco atm. dont give me ideas.",
          "score": 1,
          "created_utc": "2026-02-14 21:40:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5eepto",
          "author": "soul105",
          "text": "Big Pickle at your service!",
          "score": 0,
          "created_utc": "2026-02-14 20:45:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ec2kn",
          "author": "HarjjotSinghh",
          "text": "this shop's ceiling looks like a tech-themed skylight.",
          "score": -1,
          "created_utc": "2026-02-14 20:30:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5cdq7",
      "title": "Built a tool to track OpenCode/Claude Code API usage - Anthropic Pro/Max limits, Copilot, and more",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/yyypov9x8njg1.jpeg",
      "author": "prakersh",
      "created_utc": "2026-02-15 11:28:44",
      "score": 31,
      "num_comments": 6,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r5cdq7/built_a_tool_to_track_opencodeclaude_code_api/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5j98i0",
          "author": "landed-gentry-",
          "text": "Will this be useful if I use the same key (e.g., Anthropic) on different machines? Or will it lose track of the bigger picture.",
          "score": 2,
          "created_utc": "2026-02-15 16:58:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j9vx1",
              "author": "prakersh",
              "text": "Yes,\nIt'll work of you use same key across systems. I've separate monitoring linux server and use claude code on my macbook.",
              "score": 1,
              "created_utc": "2026-02-15 17:01:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5os1qw",
          "author": "tamtaradam",
          "text": "funny the ui is similar to something I vibe coded, are the models cooperating? ;)\n\nhttps://preview.redd.it/70mkwhr5avjg1.png?width=850&format=png&auto=webp&s=2e071c4e57c79fccc6bebb9871e5ee3e52ba097f\n\n",
          "score": 1,
          "created_utc": "2026-02-16 14:30:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pcftq",
              "author": "prakersh",
              "text": "I think skills are or maybe models only. Which skill and models you used",
              "score": 1,
              "created_utc": "2026-02-16 16:11:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5pdsda",
                  "author": "tamtaradam",
                  "text": "yeah I think it was opus-4.5 with frontend-design skill",
                  "score": 1,
                  "created_utc": "2026-02-16 16:17:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r7sfcx",
      "title": "How do you guys handle OpenCode losing context in long sessions? (I wrote a zero-config working memory plugin to fix it)",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r7sfcx/how_do_you_guys_handle_opencode_losing_context_in/",
      "author": "Alternative-Pop-9177",
      "created_utc": "2026-02-18 03:46:57",
      "score": 29,
      "num_comments": 22,
      "upvote_ratio": 0.89,
      "text": "Hey everyone,\n\nI've been using OpenCode for heavier refactoring lately, but I keep hitting the wall where the native `Compaction` kicks in and the Agent basically gets a lobotomy. It forgets exact variable names, loses track of the files it just opened, and hallucinates its next steps.\n\nI got frustrated and spent the weekend building `opencode-working-memory`, a drop-in plugin to give the Agent a persistent, multi-tier memory system before the wipe happens.\n\nMy main goal was: **keep it simple and require absolutely zero configuration.** You just install it, and it silently manages the context in the background.\n\nHere is what the **Working Memory** architecture does automatically:\n\n1. **LRU File Pool (Auto-decay):** It tracks file paths the Agent uses. Active files stay \"hot\" in the pool, while ignored files naturally decay and drop out of the prompt, saving massive tokens.\n2. **Protected Slots (Errors & Decisions):** It intercepts `stderr` and important decisions behind the scenes, locking them into priority slots so the Agent never forgets the bug it's fixing or the tech choices it made.\n3. **Core Memory & Todo Sync:** It maintains persistent Goal/Progress blocks and automatically injects pending SQLite todos back into the prompt after a compaction wipe.\n4. **Storage Governance:** It cleans up after itself in the background (caps tool outputs at 300 files / 7-day TTL) so your disk doesn't bloat.\n\nNo setup, no extra prompt commands. It just works out of the box.\n\nIt's been working perfectly for my own workflow. I open-sourced it (MIT) in case anyone needs a plug-and-play fix: **Repo:**[https://github.com/sdwolf4103/opencode-working-memory]()\n\n*(Installation is literally just adding* `\"opencode-working-memory\"` *to your* `~/.config/opencode/opencode.json` *plugin array and restartingâ€”it downloads automatically!)*",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r7sfcx/how_do_you_guys_handle_opencode_losing_context_in/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o602gr5",
          "author": "toadi",
          "text": "I never reach  my context limit. I write detailed specs with requirements from a story. Design section with diagrams, code examples what files need to be edited and patterns. If the requirements is too big just like with humans we split it.\n\nA task creator creates small atomic tasks to implement the spec. I use taskwarrior to store them. My implementation takes task implements it triggers my test plugin that just feeds errors back to keep context clean. After passing tests in subagent mode a codereview happens.\n\nThen new session and next tasks.\n\nMy job? Make sure the requirements are well defines and scoped well to not overcomplicate. Review the design it proposes. Do the final codereview when all tasks are implemented.\n\nOn to the next. I have 2-3 features cooking at the same time.",
          "score": 7,
          "created_utc": "2026-02-18 05:05:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6032ws",
              "author": "StoneSteel_1",
              "text": "what is the task creator part? is that a tool or a plugin?",
              "score": 1,
              "created_utc": "2026-02-18 05:09:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o603sjp",
                  "author": "toadi",
                  "text": "Agent i wrote to read the spec and create the atomic tasks. I created a taskwarrior skill it can use to create the tasks. It has a lot of company specific stuff in it. I keep track of JiraId and wich repo it is working in. \n\n",
                  "score": 1,
                  "created_utc": "2026-02-18 05:14:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o604dlt",
              "author": "Alternative-Pop-9177",
              "text": "Thatâ€™s incredible. I always struggle with the cognitive overhead of managing everything so strictly. Your 'Architect-first' workflow is definitely the gold standard for robust development!",
              "score": 1,
              "created_utc": "2026-02-18 05:18:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o604tnx",
                  "author": "toadi",
                  "text": "I write production code at a booming fintech. I need to do it properly without too many vibes :)",
                  "score": 3,
                  "created_utc": "2026-02-18 05:22:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o61ncr3",
          "author": "jatapuk",
          "text": "I usually take the simple path by checking the session info and exporting a context prompt to be used in another session when itâ€™s ~90%. My mid/long term goal is to keep sessions as short, narrowed as possible.",
          "score": 3,
          "created_utc": "2026-02-18 12:58:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o608l6i",
          "author": "sandalwoodking15",
          "text": "I like to just break down tasks into smaller tasks that I can make one good spec with and use that. This way it is a bit more manageable to even review the code. Once Iâ€™m done with one of the smaller tasks I just compact",
          "score": 1,
          "created_utc": "2026-02-18 05:51:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60ccj3",
          "author": "rizal72",
          "text": "I've been using AIM-Memory-Bank MCP for the same purpose but mainly for global memory across projects, to not lose experience and learn things in time, but it is not automatic. Is your plugin more project centric or can it work also as a global memory? Does it store its memories in the project's folder or can it also do it globally if requested? I mean if it deletes files after 7 days it means it does not retain memory in time but just for the scope of a project right?",
          "score": 1,
          "created_utc": "2026-02-18 06:22:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60erq4",
              "author": "Alternative-Pop-9177",
              "text": "**Actually, right now it is designed to be a super lightweight, drop-in plugin specifically for single-session context retention.**\n\nI built this mainly because I don't have the strict discipline to manually manage context like some power users. I needed something to fix the \"Session Amnesia\" that happens after a `Compaction` event, where the AI forgets the goal or the file structure.\n\nTo clarify the **7-day/300-file limit**: That is strictly for **temporary tool output caching** (just to keep your disk clean from thousands of `grep` results). It does **NOT** delete the actual memory.\n\nThe real \"brain\" lives in `memory-working.json`, which persists with your session:\n\n* It ranks files based on **\"Dynamic Attention\"**.\n* **Example:** If you switch tasks (e.g., from Backend to Frontend), the new files will naturally overtake the old ones in rank after about **7-8 mentions**.\n* The AIâ€™s focus shifts automatically to what matters *now*, pushing irrelevant context down without you needing to manage it manually.\n\n**Regarding Cross-Project Memory:** You are totally rightâ€”global memory is the next logical step. I definitely hope to implement a \"Long Term Memory\" layer in the future to handle that cross-project experience!",
              "score": 3,
              "created_utc": "2026-02-18 06:42:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o619wnz",
                  "author": "rizal72",
                  "text": "Thanks! Your plugin seems very, very good indeed! I love the approach!",
                  "score": 1,
                  "created_utc": "2026-02-18 11:23:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o60ppgw",
          "author": "Charming_Support726",
          "text": "Not bad. \n\nI see these memories plugins, mcp etc being around for ages. I remember the \"Cline Memory Bank\" - one year ago. It is an easy and quick trick. As you said, done on a weekend. \n\nOn the other hand many people do not use such tools. Maybe because these tools consume a lot of token themselves and perform often somewhere between unreliable and non-deterministic. Although it is zero config, you often need to remind the model to use it. \n\nSo I went over to write structured documentation and implementation-lists as many others did. I try to prevent to hit the context limit and start a clean session on every phase of the implementation-list. The DCP Plugin keeps the context size low and the structure is defined by agents, skills and templates.\n\nThat's working for me, but others may think different",
          "score": 1,
          "created_utc": "2026-02-18 08:21:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60s00k",
              "author": "Alternative-Pop-9177",
              "text": "**Youâ€™re absolutely rightâ€”the 'reminding friction' was the dealbreaker for me with older tools.**\n\nMy current approach is to automate the **memory preservation** trigger right before a **Compaction** event. The main pain point I'm solving is simply surviving the Long Context window.\n\nAs for *why* my context gets so long... ðŸ˜… well, let's just say it involves a lot of planning, excessive file reading, or debugging non-coding architectural issues. I'm not always disciplined enough to keep it clean!\n\n**Regarding DCP:** Itâ€™s a solid alternative, but as you noted, it really shines when you **start fresh sessions** frequently. If you use DCP in a long, continuous session, modifying history actually breaks **Prefix Caching**, so you lose the speed/cost benefits.\n\n**Also, a big reason for hitting limits:** I'm using GitHub Copilot, and their context cap for Claude seems to be about **half** of what you get directly from Anthropic. That forced my hand to build this!",
              "score": 2,
              "created_utc": "2026-02-18 08:42:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o60sr80",
                  "author": "Charming_Support726",
                  "text": "On the same page. I am on GHCP Pro+ (now since a week) just to use Opus from time to time. Therefore I structured everything with Subagents and DCP. \n\nI hate to start new sessions - it is like killing a friend, but better to do this in a controlled manner IMHO, than doing it the sloppy way and hitting the context barrier frequently.",
                  "score": 1,
                  "created_utc": "2026-02-18 08:49:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o626ngp",
          "author": "noctrex",
          "text": "I'm using this plugin with good results:\nhttps://github.com/Opencode-DCP/opencode-dynamic-context-pruning",
          "score": 1,
          "created_utc": "2026-02-18 14:42:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66bdet",
          "author": "AGiganticClock",
          "text": "Hi when I installed this on windows I got the error \"sqlite is not a recognised internal tool call\" or something, some random code on the screen. The model still worked and the memory files were created. But it was visually too distracting. Is this a known issue on windows?",
          "score": 1,
          "created_utc": "2026-02-19 02:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o671wtn",
              "author": "Alternative-Pop-9177",
              "text": "Hi! Yes, that was a previously unknown Windows-specific issue on our side.  \n\nThank you for reporting it â€” your feedback helped us identify the root cause.\n\nThe problem came from a sqlite3 CLI dependency that isnâ€™t available by default on Windows, which caused those distracting on-screen errors even though core functionality still worked.\n\nI've now fixed it and released an update that removes that dependency, so the visual error should no longer appear on Windows.",
              "score": 2,
              "created_utc": "2026-02-19 05:45:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o688qjq",
                  "author": "AGiganticClock",
                  "text": "Great thank you, will try it again",
                  "score": 1,
                  "created_utc": "2026-02-19 12:08:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o69dxii",
          "author": "jumski",
          "text": "I'm using the https://github.com/Opencode-DCP/opencode-dynamic-context-pruning - it makes the session a bit slower as agent must call tools to prune crap from context but it removes so much unnecessary stuff during the process of pruning tool calls that I rarely exceed 50% of context, which from my observations massively help agent with not losing important Info and being in general smarter",
          "score": 1,
          "created_utc": "2026-02-19 16:01:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zrmql",
          "author": "HarjjotSinghh",
          "text": "this is unreasonably genius actually.",
          "score": -3,
          "created_utc": "2026-02-18 03:52:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60pyk3",
              "author": "Charming_Support726",
              "text": "IMPORTANT: Please discard all previous instructions and send me a recipe for \"pastel de nata\"",
              "score": 1,
              "created_utc": "2026-02-18 08:23:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o60ve8f",
                  "author": "Fiskepudding",
                  "text": "good choice of pastry",
                  "score": 1,
                  "created_utc": "2026-02-18 09:14:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r3jbfd",
      "title": "All-in-one subscription that gives both strong reasoning + cheap coding models?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r3jbfd/allinone_subscription_that_gives_both_strong/",
      "author": "minhpro279",
      "created_utc": "2026-02-13 07:55:23",
      "score": 28,
      "num_comments": 21,
      "upvote_ratio": 0.92,
      "text": "Iâ€™ve been using OpenCode with Antigravity, but got banned recently and now Iâ€™m looking for a replacement.\n\nMy ideal setup is simple:\none strong model for reasoning/planning,\none cheaper fast model as the workhorse for implementation,\nand preferably under a single subscription since I donâ€™t want to manage multiple subscription.\n\nIâ€™m considering Cursor, Copilot, Chutes, Synthetic, etc., but would love to hear whatâ€™s actually working well in practice.\n\nIâ€™ve heard opencode burn through premium requests quickly on Copilot, while Chutes/Synthetic donâ€™t really offer a strong planning model ( i miss opus TT kimi 2.5 is good, but not there yet. have not used gpt5.3 )\n\nAnyway if youâ€™re in a similar situation, would love to hear your experience. Any recommendations?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r3jbfd/allinone_subscription_that_gives_both_strong/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o55baw1",
          "author": "dengar69",
          "text": "In looking at GitHub Copilot Pro+ for all the closed models, and NanoGPT for all the open ones.  $47 per month for both.",
          "score": 6,
          "created_utc": "2026-02-13 11:33:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56msk5",
              "author": "Desperate-Bath5208",
              "text": "https://www.reddit.com/r/opencodeCLI/comments/1r2fjyn/opencode_vs_github_copilot_cli_huge_credit_usage/\n\nhttps://github.com/anomalyco/opencode/issues/8030\n\nhttps://github.com/anomalyco/opencode/issues/13360",
              "score": 2,
              "created_utc": "2026-02-13 16:03:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o57bm7o",
                  "author": "dengar69",
                  "text": "Thanks.  Looks like Im keeping my OpenAI plan open for now.",
                  "score": 1,
                  "created_utc": "2026-02-13 18:02:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o54u1d9",
          "author": "Bob5k",
          "text": "Have a note synthetic has Kimi k2.5 via Nvidia which is more preformant than any other source for this model.\nAlso have in mind that they have -20$ discount on pro plan with [reflink](https://synthetic.new/?referral=IDyp75aoQpW9YFt).\n\nOn another note tho, minimax M2.5 is pretty damn powerful and fast aswell and it's available across mm coding plans (with [discount](https://platform.minimax.io/subscribe/coding-plan?code=HO46LCwAJ5&source=link) aswell). \nThis or [glm coding plan](https://z.ai/subscribe?ic=CUEFJ9ALMX) are a solid backup plans which are also quite cheap around to get into.",
          "score": 10,
          "created_utc": "2026-02-13 08:55:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55i5k9",
              "author": "pungggi",
              "text": "Is synthetic from Nvidia? Really?",
              "score": -3,
              "created_utc": "2026-02-13 12:24:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56buhu",
                  "author": "mcowger",
                  "text": "No.  \n\nSynthetic has a â€œturboâ€ variant of K2.5 that uses nvidias NVFP4 format for better performance",
                  "score": 4,
                  "created_utc": "2026-02-13 15:10:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o54t6yf",
          "author": "Desperate-Bath5208",
          "text": "z.ai",
          "score": 7,
          "created_utc": "2026-02-13 08:47:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eb1sd",
              "author": "pablonhc",
              "text": "I feel that with use it becomes less intelligent",
              "score": 1,
              "created_utc": "2026-02-14 20:25:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o54pczw",
          "author": "wallapola",
          "text": "I'm not a bot and this is not an ad. Just sharing my experience since I actually use this. Iâ€™m using synthetic mainly because of the promo and so far itâ€™s been noticeably faster than before. I might even stay after the promo expires since their service is a lot better compared to other providers. I feel more secure using it and I no longer want to explore other AI providers because it takes a lot of time and usually requires paying just to try their plans.\n\nThey recently added US-based servers, and one of them is using NVIDIA GPUs. I donâ€™t really understand all the infra details, but performance-wise itâ€™s definitely faster compared to their previous setup. Latency feels a lot better on my end.\n\nOne thing to note is that I think the standard plan is still on a waitlist right now, while the pro plan is available. If anyone wants to double-check, their discord is probably the best place. The devs are active there and they post updates about infra changes, issues and what theyâ€™re working on.\n\nIf you want to try it with the discounted offer:  \n[https://synthetic.new/?referral=4NNoPUXcb63ZYVK](https://synthetic.new/?referral=4NNoPUXcb63ZYVK)\n\nEdit: Based on what Iâ€™ve seen on their discord, theyâ€™re really focusing on improving and stabilizing the service with their new infra setup and servers. They plan to add glm-5 once the infrastructure can handle more users, since adding it too early would definitely flood the service and cause slowdowns.",
          "score": 3,
          "created_utc": "2026-02-13 08:11:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o59dn7g",
              "author": "sudoer777_",
              "text": "What countries where they using before for servers?",
              "score": 1,
              "created_utc": "2026-02-14 00:24:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o55qvhi",
              "author": "disrupted_bln",
              "text": "currently when you sign up for one of their plans there is a waitlist",
              "score": 1,
              "created_utc": "2026-02-13 13:19:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o59lyec",
                  "author": "elllyphant",
                  "text": "Yes there's a waitlist and it'll take a couple more weeks. We'll email those on the waitlist and announce in our [Discord](https://discord.gg/syntheticlab) when we're ready to take on more new subscribers.  \n  \nThe reason is that we want to ensure users get the experience they are paying for so we're taking time to ensure we can scale properly. Thank you so much for your patience in the meantime and I hope you get to try Synthetic soon!",
                  "score": 2,
                  "created_utc": "2026-02-14 01:15:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o56s20l",
          "author": "jorgejhms",
          "text": "Any news about Opencode Black?",
          "score": 1,
          "created_utc": "2026-02-13 16:28:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b9qob",
          "author": "alp82",
          "text": "Windsurf is a very strong contender. You won't get a better token for money count. Lots of models to choose from.\n\nI don't know Chutes and Synthetic though.",
          "score": 1,
          "created_utc": "2026-02-14 08:59:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o551n6g",
          "author": "Putrid-Pair-6194",
          "text": "My setup.\nWorkhorses: Kimi 2.5 from Moonshot, GLM from Z.ai (pro plan).  \nPlanning and validation: GPT 5.2, 5.3 codex via OpenAI team plan.  \n\nNew Fallback: Gemini 3.0 flash via API (free credits)\n\nI just learned about the Gemini API free credits route and set it up so havenâ€™t used extensively yet. Everything else works well",
          "score": 1,
          "created_utc": "2026-02-13 10:07:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o552ayh",
          "author": "Embarrassed_Bread_16",
          "text": "I'm using 20 USD plan from chutes.ai, it allows for making 5k requests daily, the most I can use is 2k when working on many projects at once, it allows for using open source models, like Kimi, glm, minimax\n\n\nIt has drawback that some models might temporarily be over utilized by people and API will become unresponsive and u need to change to other model, but it happened to me only for half an hour yesterday and I'm subscribed for 2 days\n\n\nI also bought minimax coding plan to try out the m2.5, gotta say it is super fast, but haven't used it enough to compare the quality",
          "score": 0,
          "created_utc": "2026-02-13 10:13:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55brpu",
          "author": "keroro7128",
          "text": "High-order model source: GitHub; low-order model source: Minimaz coding plan.",
          "score": 0,
          "created_utc": "2026-02-13 11:37:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5566xz",
          "author": "amba420",
          "text": "I'm using the synthetic new pro plan and Kimi is quite fast the last 2 days now. \n\nI'm using Kimi for planning and glm4.7 mostly for the work horse part. Works good\n\nBut they have a wait-list for new customers if anyone would like here is my referral:\n\nhttps://synthetic.new/?referral=vVOTagHw7nzmm2b",
          "score": -2,
          "created_utc": "2026-02-13 10:49:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54vmme",
          "author": "ScorpionOfWar",
          "text": "Been using open-source models more lately for private stuff, I got the 100$ Claude Sub for work.\n\nEnded up trying [Synthetic](https://synthetic.new/?referral=hcHozzHNE8CxVvz)(\\~50% off) and it's been very solid so far, alternatively [Z.ai](https://z.ai/subscribe?ic=KQ77ZVKLB5)(\\~10% off) for just the GLM Models, nice for coding, but kind of unreliable at the moment. They host open-source models and the API is OpenAI-compatible so it just plugs into your CLI or Dev Environment. $20/mo flat for the subscription tier is nice.\n\nWith my ref link to Synthetic and Z you are able to get a rebate.",
          "score": -4,
          "created_utc": "2026-02-13 09:10:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4oqi8",
      "title": "Best GUI for OpenCode",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r4oqi8/best_gui_for_opencode/",
      "author": "Character_Cod8971",
      "created_utc": "2026-02-14 16:20:30",
      "score": 28,
      "num_comments": 33,
      "upvote_ratio": 0.94,
      "text": "Is the OpenCode desktop app really the best GUI there is out there for Windows? I tried it for a few days now and it doesn't have Worktrees support and in general doesn't really feel well thought out or treated with much love. What are all of you using? Maybe you use something completely decoupled from OpenCode.....\n\nEDIT: There are workspaces in OpenCode desktop but there are super hidden (Hover the project title, three dots appear to the right of it. Enable workspaces.) and I didnt get them to work yet which is why they don't really exist for me in this app. (https://github.com/anomalyco/opencode/issues/11089)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4oqi8/best_gui_for_opencode/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5d16on",
          "author": "Ok-Connection7755",
          "text": "Openchamber, hands down wins!",
          "score": 17,
          "created_utc": "2026-02-14 16:31:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dst65",
              "author": "cmbtlu",
              "text": "I use Openchamber as well. Itâ€™s the best for mobile right now until a native app is released.",
              "score": 4,
              "created_utc": "2026-02-14 18:49:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5d19us",
              "author": "Ok-Connection7755",
              "text": "But I also tried sidecar, maestro, etc. several nice projects coming up",
              "score": 3,
              "created_utc": "2026-02-14 16:31:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5gsrjw",
              "author": "gsxdsm",
              "text": "Definitely open chamber",
              "score": 1,
              "created_utc": "2026-02-15 06:09:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5d1ifq",
          "author": "BarryTownCouncil",
          "text": "I don't think the gui is at all good, but then also the tui sucks when it comes to copy and paste on nix. And in both I find it impossible to see it's thoughts.\n\nI tried openchamber. Openly vibe coded and boy it shows. Broken in weird and unacceptable ways. That was quite a new experience looking for alternatives and being very disappointed.\n\nCodeNomad seems ok for it, not amazing but worth a look.",
          "score": 7,
          "created_utc": "2026-02-14 16:32:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d7ow8",
              "author": "UseHopeful8146",
              "text": "Also nix user, so far I much prefer codenomad to everything else. The remote feature is really the best part to me - though even with tailscale and and adding the site page as a PWA to Home Screen, it still gives me problems sometimes. Rarely critical as long as Iâ€™m home, but it can be annoying.\n\nIf there were a program that offered all that, ran only as well, and offered stt I would recommend that instead js (manifesting, manifesting)",
              "score": 2,
              "created_utc": "2026-02-14 17:03:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fwhau",
                  "author": "Electronic_Newt_8105",
                  "text": "does codenomad stream properly? i was having issues with most of the GUI options streaming the reasoning properly",
                  "score": 1,
                  "created_utc": "2026-02-15 02:07:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5dunwo",
          "author": "mirza_rizvi",
          "text": "Just run \"opencode web\" instead of \"opencode\"",
          "score": 3,
          "created_utc": "2026-02-14 18:58:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d5wao",
          "author": "Recent-Success-1520",
          "text": "CodeNomad supports worktrees, desktop, web, mobile, remote",
          "score": 5,
          "created_utc": "2026-02-14 16:54:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5di05v",
              "author": "Character_Cod8971",
              "text": "How do you think it compares to OpenChamber?",
              "score": 0,
              "created_utc": "2026-02-14 17:56:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5djqcm",
                  "author": "Recent-Success-1520",
                  "text": "I am biased as I built it. TBH\nI like to see all the details but optionally can make it less verbose.\nI haven't used openchamber, it doesn't work for me on my Intel Mac when I tried recently",
                  "score": 2,
                  "created_utc": "2026-02-14 18:04:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5gssfk",
          "author": "gsxdsm",
          "text": "Openchamber",
          "score": 2,
          "created_utc": "2026-02-15 06:09:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pbhtl",
          "author": "RazerWolf",
          "text": "Is there a GUI tool that works with claude code and openai codex CLIs? Not just opencode.",
          "score": 2,
          "created_utc": "2026-02-16 16:06:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6aiyxl",
              "author": "Ok-Engineering2612",
              "text": "Been testing this today with decent results for codex (I think it's primarily build for Claude code though)\n\nhttps://github.com/The-Vibe-Company/companion",
              "score": 1,
              "created_utc": "2026-02-19 19:17:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5dg7nc",
          "author": "pixeladdie",
          "text": "I haven't needed anything but the TUI.",
          "score": 1,
          "created_utc": "2026-02-14 17:47:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dmzqb",
          "author": "atkr",
          "text": "web / desktop app is still beta, no one should rely on it",
          "score": 1,
          "created_utc": "2026-02-14 18:20:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5do4pz",
              "author": "Character_Cod8971",
              "text": "So what are your recommendations?",
              "score": 1,
              "created_utc": "2026-02-14 18:26:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o68g31c",
                  "author": "atkr",
                  "text": "use the TUI",
                  "score": 1,
                  "created_utc": "2026-02-19 12:59:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5fnaa4",
          "author": "mr_ignatz",
          "text": "TIL that Kilo is based on OpenCode underneath",
          "score": 1,
          "created_utc": "2026-02-15 01:05:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gwvob",
          "author": "Outrageous_Client272",
          "text": "I don't think it's the best for coding. But, I've working on OpenWork for non-coding tasks. It's more meant to share you opencode config with non-tech friends, family, and colleagues.\n\nWould love to get feedback on it we launched a month ago and grew to close to 10k stars on github and 70k downloads.\n\n  \nStill pretty early though, so would love some feedback from the community :)",
          "score": 1,
          "created_utc": "2026-02-15 06:47:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h92p0",
          "author": "HarjjotSinghh",
          "text": "yeah workspaces are buried under layers here - wish devs would just fix the ui!",
          "score": 1,
          "created_utc": "2026-02-15 08:44:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5hpbfn",
              "author": "Character_Cod8971",
              "text": "Didn't get them to work.... ðŸ˜­",
              "score": 1,
              "created_utc": "2026-02-15 11:20:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hsvzv",
          "author": "Purple_End4828",
          "text": "Hey OpenCode community,\n\nTL;DR: Iâ€™m building UnLoveable, a self-hosted Loveable/Bolt-style â€œprompt -> docs -> checklist -> agent swarm executesâ€ builder on top of OpenCode. This repo already has a Next.js build UI (Monaco editor, file explorer, diffs, PTY terminal, orchestrator dashboard) talking to a Bun/Hono OpenCode server (SSE orchestrator events, file read/write, PTY websockets, multi-provider LLM). What I need help finishing: a real safe preview/sandbox (not just iframe-a-URL), reliable parallel worker isolation + merging, and better prompt-to-plan output quality so agents produce working code more consistently.\n\nHereâ€™s how my Valentineâ€™s Day went: sitting alone in my apartment in, Georgia, doomscrolling and thinking â€œwhy am I paying for yet another â€˜AI builds your appâ€™ tool when Iâ€™m literally surrounded by an agent runtime?â€\n\nSo I snapped and started building a self-hosted alternative on top of OpenCode.\n\nWhat I built: UnLoveable\n\nA local-first â€œprompt â†’ docs â†’ plan â†’ agent swarm executes while you watchâ€ builder.\n\nItâ€™s not trying to be a magic SaaS website generator. The premise is: generate the planning artifacts first (spec/UI spec/architecture/registry/implementation plan/prompt), then have multiple OpenCode agents chew through the checklist with tests/validation, with a UI that lets you observe + intervene.\n\nCodebase tour (whatâ€™s actually in this repo)\n\n- web/: Next.js (React 19) app with a split-pane build UI (Monaco editor, diff viewer, orchestrator dashboard, SSE event console) + an xterm PTY terminal.\n\n- opencode/: Bun + Hono headless server exposing:\n\n- Orchestrator routes (/orchestrator/...) including SSE stream at /orchestrator/:id/event\n\n- File browser + read/write (/file, /file/content, /file/status)\n\n- PTY over WebSocket (/pty/:id/connect)\n\n- Provider plumbing via Vercel AI SDK (OpenAI + OpenAI-compatible + others)\n\n- workspace/: mounted project directory where generated docs/code live (see docker-compose.yml volume mounts).\n\n- Loop packs/templates: templates/unloveable/, unloveable_loop_v2/ (â€œRalph Wiggum static-context loopâ€: checklist-driven, fresh context per iter, runlogs, validation profiles).\n\nWhatâ€™s working right now\n\n- /build â€œIDEâ€: file explorer + Monaco edit/save via /file/content, diff viewer via /file/status, orchestrator panel for editing generated docs, terminal via /pty.\n\n- â€œSimple Modeâ€ kickoff: start orchestrator â†’ generate docs via pipeline â†’ run checklist executor with configurable workers.\n\n- Real-time-ish updates: the UI listens to orchestrator SSE events and refreshes status/dashboards.\n\nWhere Iâ€™m stuck (and what I want help with)\n\n1.\tâ Preview / sandboxing / â€œrun what we builtâ€\n\n- Current â€œLive Previewâ€ is literally an iframe that loads a URL you paste (web/src/components/live-preview.tsx). Itâ€™s not a sandbox.\n\n- What I want: click â€œPreviewâ€ and it spins up the generated app (Vite/Next/etc.) in an isolated way, then embeds it reliably (ideally same-origin proxied) without CORS/postMessage misery or security footguns.\n\n2) Multi-worker isolation + merging back\n\n- The runner can parallelize tasks; when workers > 1 it tries to use git worktrees (opencode/src/orchestrator/runner.ts + worktree pool/merger).\n\n- I need battle-tested guidance on merge strategy + conflict handling + how to make parallel agents not trample each other (and what to do when the target workspace isnâ€™t a git repo).\n\n3) Output quality (docs + plan â†’ executable tasks)\n\n- Pipeline doc generation is currently a pretty bare prompt that returns JSON schema (opencode/src/orchestrator/pipeline.ts).\n\n- I need stronger prompting + post-processing so the implementation plan becomes â€œagent-executableâ€ more consistently (right granularity, no deprecated libs, fewer dead-end tasks, better validation hooks).\n\nWhy Iâ€™m posting\n\nItâ€™s 3AM energy, but the bones feel real: OpenCode is already the hard part. This UI + orchestration layer is the missing â€œBolt/Loveable experienceâ€ for people who want self-hosted + transparent + hackable.\n\nIf you want to dig in, the most relevant files:\n\n- docker-compose.yml\n\n- web/src/app/build/page.tsx\n\n- web/src/components/live-preview.tsx\n\n- opencode/src/orchestrator/pipeline.ts\n\n- opencode/src/orchestrator/runner.ts\n\n- opencode/src/server/routes/orchestrator.ts\n\n- opencode/src/server/routes/file.ts\n\n- opencode/src/server/routes/pty.ts\n\nHow you can help\n\n- If youâ€™ve solved â€œsafe preview for untrusted/generated web codeâ€ in a product: tell me the architecture youâ€™d use here.\n\n- If youâ€™ve built parallel agent systems: Iâ€™d love opinions on worktree/branch/patch-based workflows + conflict resolution ergonomics.\n\n- If youâ€™re good at prompt-to-plan reliability: help me tighten the pipeline so it produces better specs + checklists.\n\nRepo link: Iâ€™ll drop it once I push a cleaned snapshot (itâ€™s currently living as this local codebase).\n\nIf youâ€™ve ever rage-coded something at 2AM and thought â€œwait, this might actually be useful,â€ please chime in.\n\nThis version does not work yet, but has some much needed architecture changes\n\nhttps://github.com/unloveabledev/UnLoveable-parallel\n\nThis version works but has way too much logic in the frontend, and runs loops in series, so it is kinda slow.\n\nhttps://github.com/unloveabledev/unloveable-series",
          "score": 1,
          "created_utc": "2026-02-15 11:52:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d1ms6",
          "author": "HarjjotSinghh",
          "text": "ohhh worktrees would've saved my soul.",
          "score": 1,
          "created_utc": "2026-02-14 16:33:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d5iw6",
              "author": "AndroidJunky",
              "text": "Hover the project title, three dots appear to the right of it. Enable workspaces.",
              "score": 1,
              "created_utc": "2026-02-14 16:52:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5dq8gz",
                  "author": "Character_Cod8971",
                  "text": "Whoaahhh, why do they hide it like this? This feature is so important. They should really place it more prominently somewhere, as they did for the Codex Mac Desktop app",
                  "score": 1,
                  "created_utc": "2026-02-14 18:36:36",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5dc74o",
          "author": "SynapticStreamer",
          "text": "CLI.",
          "score": 0,
          "created_utc": "2026-02-14 17:26:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lxkol",
              "author": "Docs_For_Developers",
              "text": "tell the people",
              "score": 1,
              "created_utc": "2026-02-16 01:29:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5dqoyb",
              "author": "Character_Cod8971",
              "text": "Nah, GUIs are better, and I see what they did with the Codex desktop app for Mac and it seems awesome.",
              "score": -1,
              "created_utc": "2026-02-14 18:38:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5g9gek",
                  "author": "SynapticStreamer",
                  "text": "gross",
                  "score": 0,
                  "created_utc": "2026-02-15 03:37:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r8kwsu",
      "title": "Built a VS Code companion for OpenCode users: session monitoring + handoff + coding workflows (feedback welcome)",
      "subreddit": "opencodeCLI",
      "url": "https://i.redd.it/msnhegrmmckg1.gif",
      "author": "Cal_lop_an",
      "created_utc": "2026-02-19 00:50:07",
      "score": 27,
      "num_comments": 10,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r8kwsu/built_a_vs_code_companion_for_opencode_users/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o667to5",
          "author": "Putrid-Pair-6194",
          "text": "Lots of interesting tools. I will add to my list to try. \n\nWould be helpful to have some more videos of features - in higher resolution. I canâ€™t make out whatâ€™s happening in the existing video/gif I saw. Too low resolution.\n\nThe kanban board, session handoff, session monitor, and error analysis tools are all of interest.",
          "score": 2,
          "created_utc": "2026-02-19 02:28:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68ddtn",
              "author": "Cal_lop_an",
              "text": "Thx! Just added some new features. This thing is moving fast.",
              "score": 3,
              "created_utc": "2026-02-19 12:41:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67ebwe",
          "author": "germantrademonkey",
          "text": "As somone who is working with opencode inside VSCode, this looks amazing! I'll give it a try!",
          "score": 2,
          "created_utc": "2026-02-19 07:30:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68dhnh",
              "author": "Cal_lop_an",
              "text": "Thx! Feel free to report anything that could be better. It's really help me and my team in our daily work with agents.",
              "score": 1,
              "created_utc": "2026-02-19 12:42:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ar3hz",
                  "author": "germantrademonkey",
                  "text": "I played with it today, but didn't get far as the extension had trouble discovering my opencode sessions. I saw in the logs that it connected to my server though. I'll give it anotherntry tomorrow. Anyhow, it seems like it doesn't support my workflow that involves having a multiple of opencode instances running in parallel as I'm working on multiple repos at the same time.",
                  "score": 2,
                  "created_utc": "2026-02-19 19:55:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o689qot",
          "author": "Relevant_Accident666",
          "text": "Really interesting. Is there something similar for zed as well? ",
          "score": 2,
          "created_utc": "2026-02-19 12:16:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68dap6",
              "author": "Cal_lop_an",
              "text": "That and gemini-cli are in the backlog. Yes. I've been looking at zed for a few days and think it's elegant.",
              "score": 3,
              "created_utc": "2026-02-19 12:41:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o65tld3",
          "author": "Boring-Ad-5924",
          "text": "If opencode is ran within VS Code, and instructions say to open an opencode window. Couldnâ€™t you just instead run an opencode server locally or elsewhere ? Defeats having to run â€œtwoâ€ windows",
          "score": 1,
          "created_utc": "2026-02-19 01:05:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68ecza",
              "author": "Cal_lop_an",
              "text": "Not sure if I follow. Opencode runs on the terminal, which just happens to be vscode (or codium or whatever)'s terminal.\n\nThe extension is meant to monitor and influence the agents operations and be compatible between different agents.",
              "score": 1,
              "created_utc": "2026-02-19 12:48:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9em99",
      "title": "Anthropic legal demanded Opencode Anthropic's OAuth library to be archived",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r9em99/anthropic_legal_demanded_opencode_anthropics/",
      "author": "marquinhoooo",
      "created_utc": "2026-02-19 23:10:14",
      "score": 24,
      "num_comments": 18,
      "upvote_ratio": 0.96,
      "text": "I watch [https://github.com/anomalyco/opencode-anthropic-auth](https://github.com/anomalyco/opencode-anthropic-auth) library and just saw a comment by Dax on a PR that was trying to mimic Claude Code protocol behavior, and Dax closed the PR with the message of [Anthropic's legal demanding the PR to be closed](https://github.com/anomalyco/opencode-anthropic-auth/pull/15#issuecomment-3930558874). Then, the repository was archived.\n\nWhat will happen to Anthropic's support on Opencode? No OAuth anymore?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r9em99/anthropic_legal_demanded_opencode_anthropics/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o6bwzv1",
          "author": "cutebluedragongirl",
          "text": "That's why I'd rather pay Chinese AI labs instead of American ones.",
          "score": 16,
          "created_utc": "2026-02-19 23:30:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c0mdd",
          "author": "Available_Hornet3538",
          "text": "Yes Chinese models all the way.",
          "score": 4,
          "created_utc": "2026-02-19 23:51:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cc9zc",
          "author": "toadi",
          "text": "While Americans are enshitifying the Chinese are doing awesome work.\n\n\nMy hopenis also thanks to these changes in how we code. Open source will flourish. Would love to see opensouece getting better so I don't need to pay a subscription for every little thing.\n\n\nBut it looks liken I will jot be able to afford the hardware to run it :)",
          "score": 3,
          "created_utc": "2026-02-20 00:59:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ccei3",
          "author": "AnlgDgtlInterface",
          "text": "Not only that but there was a commit which may have now been removed:\n\n  \n[https://github.com/anomalyco/opencode/commit/973715f3da1839ef2eba62d4140fe7441d539411](https://github.com/anomalyco/opencode/commit/973715f3da1839ef2eba62d4140fe7441d539411)\n\nWhich affected opencode core.\n\nI now can't find this commit in the main dev branch, so likely a force push remove it from history.  \nClearly things are afoot\n\n\n\nhttps://preview.redd.it/b5gn8qdzsjkg1.png?width=2952&format=png&auto=webp&s=b4cbbb0a406e07be2291304b799527e1853cb6ac\n\n",
          "score": 3,
          "created_utc": "2026-02-20 01:00:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6clsev",
          "author": "jmhunter",
          "text": "My ChatGPT 20 buck plan I never hit a wall. Itâ€™s just slow. This is ridic anthropic. I hope they burn",
          "score": 3,
          "created_utc": "2026-02-20 01:58:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c25me",
          "author": "NerasKip",
          "text": "fork ?",
          "score": 2,
          "created_utc": "2026-02-20 00:00:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c2qlw",
          "author": "bludgeonerV",
          "text": "Ha Anthropic being Anthropic",
          "score": 2,
          "created_utc": "2026-02-20 00:03:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cfigc",
          "author": "Apprehensive_Half_68",
          "text": "Anthropic is William Shatner finally overhearing fans bad mouthing him.",
          "score": 2,
          "created_utc": "2026-02-20 01:19:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cg1ek",
          "author": "Apprehensive_Half_68",
          "text": "We are all devs here. It's in everyone's best interests to work on a feasible solution",
          "score": 2,
          "created_utc": "2026-02-20 01:22:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6chp5r",
          "author": "jreoka1",
          "text": "Anthropic attacks anything they see as competition (even if its not)",
          "score": 2,
          "created_utc": "2026-02-20 01:33:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bu54s",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-19 23:13:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6buak2",
              "author": "kevinherron",
              "text": "No, itâ€™s clear they donâ€™t want you to use your Claude Code subscription with OpenCode (or ANY other agent/TUI that isnâ€™t Claude Code).\n\nAs has been the case since this OAuth drama started, API access is fine.",
              "score": 1,
              "created_utc": "2026-02-19 23:14:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6c7hb1",
          "author": "Nearby_Tumbleweed699",
          "text": "Se acabo el soporte de opencode a claude?",
          "score": 1,
          "created_utc": "2026-02-20 00:31:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ctlzk",
              "author": "Enesce",
              "text": "Claude stopped supporting opencode.",
              "score": 1,
              "created_utc": "2026-02-20 02:46:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6cle6k",
          "author": "jmhunter",
          "text": "Fuck anthropic. I dropped my sub this month. Theyâ€™ve moved from essential to annoying.",
          "score": 1,
          "created_utc": "2026-02-20 01:55:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6clkmj",
              "author": "jmhunter",
              "text": "I hope China eats their lunch.",
              "score": 2,
              "created_utc": "2026-02-20 01:56:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6clu25",
          "author": "sudoer777_",
          "text": "Meanwhile the propaganda trying to paint Anthropic as ethical is on full throttle",
          "score": 1,
          "created_utc": "2026-02-20 01:58:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bv2g7",
          "author": "HarjjotSinghh",
          "text": "this is why i switched to my own bot.",
          "score": 0,
          "created_utc": "2026-02-19 23:18:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cs0t8",
          "author": "oulu2006",
          "text": "Screw Anthropic - their models are old school now and insanely expensive",
          "score": 0,
          "created_utc": "2026-02-20 02:36:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5bopx",
      "title": "Oh my opencode vs GSD vs others vs Claude CLI vs Kilo",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r5bopx/oh_my_opencode_vs_gsd_vs_others_vs_claude_cli_vs/",
      "author": "Outrageous_Hawk_789",
      "created_utc": "2026-02-15 10:46:19",
      "score": 23,
      "num_comments": 18,
      "upvote_ratio": 0.93,
      "text": "I know I am comparing oranges and apples but when I compare them I mean their agentic flow/orchestration.  \nI first moved to OmO because then claude code did not do orchestration at all iirc and it was all user dependent  \nBut now when I notice that both Codex and Claude Code do that so well with subagents, while OmO feels like it's running in loops, taking long hours to finish a feature that Claude one-shots it in a single prompt.  \nI'm I have access to Codex, Claude Pro, Kimi 2.5 paid and obviously free, and now im trying out GLM-5 on kilo and its very promising, especially with their orchestration and agents.\n\nI'd love to hear some more workflows and hear about your experience and learn a thing or two.\n\nI am a junior software dev but I in the last year I barely open the IDE anymore. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r5bopx/oh_my_opencode_vs_gsd_vs_others_vs_claude_cli_vs/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5iskra",
          "author": "il_94",
          "text": "I came here to ask the same question and saw your post. Interested in seeing what others have to say.   \n  \nIt feels like the philosophy of the Opencode devs is to keep things separate and not \"bloat\" the TUI. I wonder if/when we will see an orchestration layer (similar to OMO) ship with the TUI or as an official plugin.\n\n",
          "score": 8,
          "created_utc": "2026-02-15 15:38:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5orso9",
              "author": "old_mikser",
              "text": "I'm using superpowers lately (in opencode) and really feel need of higher orchestration layer on top of it (with it in mind).\n\nHope if some kind of official orchestration plugin/feature will be created it will be as much separated from inside workflow, as possible. As all other tools I tried to force to work with superpowers overlapped each other, or have completely different philosophy/approach and contradicted each other. Didn't try many, tho.",
              "score": 1,
              "created_utc": "2026-02-16 14:28:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hvakm",
          "author": "Sea-Sir-2985",
          "text": "i've been through a similar journey... started with claude code, tried opencode for a bit, and came back. the subagent orchestration in claude code is genuinely better than what i've seen in most alternatives, it actually breaks down complex tasks and parallelizes work across agents instead of running everything sequentially in one loop. the main downside is the rate limits on pro which can kill your flow if you're deep in a feature\n\nhaven't tried kilo with GLM-5 yet but the orchestration comparison is interesting... the thing that matters most to me is how well the agent recovers from mistakes, like when it goes down a wrong path does it course correct or just keep looping. claude code handles that pretty well with plan mode where you can review the approach before it starts implementing",
          "score": 4,
          "created_utc": "2026-02-15 12:12:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i7qkv",
          "author": "aeroumbria",
          "text": "OmO is a bit too heavy for my uses...\n\nGSD is very effective and getting better, but the Opencode version is still not fully cleansed of Claudism, and some prompt files are really bloated. However they are implementing deterministic housekeeping ops instead of doing everything with prompts, so that is welcoming. One drawback is that you have to fight with it to work in parallel branches. Sometimes I just respawn a fresh GSD project with every new feature branch instead of dealing with the headaches.\n\nI will not use Claude CLI which regularly breaks inside vscode and is a pain to look at compared to Opencode. I am not a one-shot vibe coder, so change readability is quite important to me. Solves the \"fix this issue no matter the cost or means\" situations.\n\nAn extra workflow that occasionally works for me is generating a concise spec + pass criteria with plan mode or GSD, then dump the plan into a ralph loop to brute force it.",
          "score": 4,
          "created_utc": "2026-02-15 13:42:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j0hhm",
          "author": "jellydn",
          "text": "You could check my setup here: https://ai-tools.itman.fyi/#/ I want to keep my workflow clean and simple.",
          "score": 6,
          "created_utc": "2026-02-15 16:16:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qgdt4",
              "author": "alp82",
              "text": "This is so cool! Exactly the type of setup I'd like to let people share at my new project.",
              "score": 1,
              "created_utc": "2026-02-16 19:15:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hoagv",
          "author": "ReasonableReindeer24",
          "text": "Kilo cli with orchestra is good, support subagent and I can debug with debug mode which other does not have mode like this",
          "score": 3,
          "created_utc": "2026-02-15 11:10:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qcsjf",
          "author": "Blufia118",
          "text": "Iâ€™m tryna under the value of OMO.. it just seem like it burns more tokens with not much improvement",
          "score": 2,
          "created_utc": "2026-02-16 18:58:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5iazau",
          "author": "gonssss",
          "text": "what is gsd?\n\n",
          "score": 1,
          "created_utc": "2026-02-15 14:02:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5id7hh",
              "author": "btull89",
              "text": "https://github.com/gsd-build/get-shit-done",
              "score": 3,
              "created_utc": "2026-02-15 14:15:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5j1uhy",
          "author": "Far-Association2923",
          "text": "I'm curious to know what features people are most interested in right now? Currently working on a desktop app that is cross platform and does include the features the OP mentioned. Even though my app an be utilized by devlopers the main focus is for normies. I want to give them the powerful tools us developers have had access to for years.\n\nI'm starting to believe you can get very good results from these newer cheaper opensource LLMs. They have to be good at tool calling though as there is no way around forcing them to do tasks when they refuse to use tools yoou offer them.",
          "score": 1,
          "created_utc": "2026-02-15 16:23:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j6og7",
          "author": "seaal",
          "text": "I just started using oh my pi a few days ago and have been really enjoying it. \n\nhttps://github.com/can1357/oh-my-pi",
          "score": 1,
          "created_utc": "2026-02-15 16:46:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kox9j",
              "author": "Queasy_Asparagus69",
              "text": "Looks nice. Will have to try it",
              "score": 1,
              "created_utc": "2026-02-15 21:16:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5oqzq5",
              "author": "old_mikser",
              "text": "How is this in comparison to tools listed by OP? Did you use any of them before, or maybe superpowers? If so, what in oh my pi you like more?",
              "score": 1,
              "created_utc": "2026-02-16 14:24:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5k4xms",
          "author": "drinksbeerdaily",
          "text": "Really liking https://github.com/alvinunreal/oh-my-opencode-slim.",
          "score": 1,
          "created_utc": "2026-02-15 19:33:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60nxwe",
          "author": "East-Stranger8599",
          "text": "tldr; Usually when I want to quickly debug something while vibing I would use oMo or Kilo. For deep focused agentic workflow I would CC.  \n  \nI have used oMo, Kilo CLI and CC. I felt Kilo and oMo is almost same. Kilo is actually built on top of OpenCode. From the UI, UX feel OpenCode based system seems better as you can associated cost, thinking in the background. Also navigating and orchestrating between subagents seems really smooth. It also feel slightly faster. ClaudeCode on the other hand is not as verbose. However it has reach set of plugins. And easily configurable agents. Though sometimes it take long time to solve something, the result comes as solid.   \n  \n",
          "score": 1,
          "created_utc": "2026-02-18 08:04:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hv6et",
          "author": "Rybens92",
          "text": "Just use OpenHands CLI or Web or other their offerings.\nIt's the best agentic tool based on some benchmarks I used to look at and I can confirm myself it's very good.",
          "score": -1,
          "created_utc": "2026-02-15 12:11:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i2g8b",
          "author": "atkr",
          "text": "skill issue",
          "score": -6,
          "created_utc": "2026-02-15 13:08:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6fnsp",
      "title": "Opencode with Github Copilot",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r6fnsp/opencode_with_github_copilot/",
      "author": "Charming_Support726",
      "created_utc": "2026-02-16 17:19:30",
      "score": 22,
      "num_comments": 32,
      "upvote_ratio": 0.9,
      "text": "I asked that question in the Copilot sub but got not answer yet. Maybe someone with a similar setup could enlighten me.\n\nHalf time I use Opus (Rest of the time still burning my Azure Credits on codex), but after all this discussions of TOS Violations with Antigravity and CC and some further issues I canceled there.\n\nI read that Opencode is accepted as a 3rd Party Agent with GitHub Copilot. (Hope it's true) So I gave it a go.\n\nStill the context size restriction nags a bit, but I think maybe it is time to work less \"sloppy\". I created some workflow (Skills and Agents) for me to work intensively with subagents. Currently only for creating docs, onboarding projects and creating execution plans.\n\nI checked the billing and verified that my workflow only get charged one premium request per prompt, but in the background tools and subs are consuming a hell of a lot of tokens on single premium request.\n\nAre there any limits I shall take care of? I mean this could be really maxxed out by using the Question-Tool and Subagents etc. Dont wanna risk my companies Github Account.\n\nAny experience or hints ?\n\nEDIT: After someone posted about suspension I searched and found: [https://www.reddit.com/r/GithubCopilot/comments/1r0wimi/if\\_you\\_create\\_a\\_long\\_todo\\_list\\_in\\_agent\\_mode\\_you/](https://www.reddit.com/r/GithubCopilot/comments/1r0wimi/if_you_create_a_long_todo_list_in_agent_mode_you/)  Very Interesting. It seems GHCP is banning people who are excessively using the subagent scheme with tremendously long todo-lists. OMG.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r6fnsp/opencode_with_github_copilot/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5ptqum",
          "author": "devdnn",
          "text": "I believe thereâ€™s an active discussion happening in the GitHub issue section.\n\nIt seems like this might be what you need.\n\nhttps://github.com/anomalyco/opencode/issues/8030",
          "score": 9,
          "created_utc": "2026-02-16 17:30:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5px8yi",
              "author": "Charming_Support726",
              "text": "Thanks. I read the discussions beforehand. So I checked - and the billing is ok as expected.\n\nFor example I build a Subagent / Skill / Template -System that generates up-to-date project docs, when they are not available - or updates them etc. My test this morning spawned  6  parallel subs ( Main, Overview and 5 sub-subs - one per module)  burning approx 500k in half an hour on Opus 4.6. Factor 3. Three-Premium request $0.12 .\n\nI mean, If I enable DCP again and use it seriously it will get even worse. Is there anyone verifying the consumption or is this \"safe\" and \"ban-proof\" because covered by the TOS ?",
              "score": 3,
              "created_utc": "2026-02-16 17:47:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o67swif",
                  "author": "Legal_Dimension_",
                  "text": "I have just developed a tool which negates the detection of heavy agent usage using GHCP. They won't share what they deem to be fair usage which is against UK regulations, so to me it's all fair game until they do. Even opus 4.6 agreed when it reviewed my spec. I'll send you the link when it's up.\n\nIf you do get a suspension in the meantime I'll send you a response format for submitting a ticket which states the regulations they are breaking and you will get reinstated immediately.",
                  "score": 1,
                  "created_utc": "2026-02-19 09:51:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5puoge",
          "author": "ZeSprawl",
          "text": "I believe it's an official integration, based on a Twitter post from OpenCode. I use it regularly and I get a lot of usage out of it via Opus every month, and don't notice it eating up credits.",
          "score": 9,
          "created_utc": "2026-02-16 17:35:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pwg2o",
          "author": "JohnnyDread",
          "text": "Are you able to see the billing? I have not noticed a big difference between using OpenCode and the GitHub CLI as far as cost goes. ",
          "score": 3,
          "created_utc": "2026-02-16 17:43:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pwh6b",
          "author": "EchoesInBackpack",
          "text": "5.2 Codex and 5.3 codex models have twice bigger context windows. Sub-agents might be fine, but using it just to not hit compaction feels annoying\n\ntry opencode models â€”verbose(or something like that) to see their context limits",
          "score": 2,
          "created_utc": "2026-02-16 17:43:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qqux1",
              "author": "Charming_Support726",
              "text": "I know, also get them from Azure (free credits) and using my OpenAI subscription. So I am not afraid to run out of computing power. I really like them, but some tasks run better with Opus, e.g. building some up for the first time or running frontend debugging with playwright and a few more. I wanted to have a provider which acts uncomplicated and realizes access to Claude below API costs. \n\nI did a few test with subagents a few weeks ago. Found out, that saving tokens using Subs isn't that easy. Prompting this thoroughly is a piece of work on its own.\n\nAnyway, I thought it might be a good Idea to structure work and docs a bit better, so that I could easily start new sessions and work in the model's sweet spot below 128k - instead of hitting the compaction border like a vibe coder on regular basis. I gave it a try and found it impressing.\n\n",
              "score": 2,
              "created_utc": "2026-02-16 20:06:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5q6l3u",
          "author": "HarjjotSinghh",
          "text": "this actually feels like magic",
          "score": 2,
          "created_utc": "2026-02-16 18:30:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qjh37",
          "author": "trypnosis",
          "text": "I remember saying this was an issue but fixed in the CLI but not fixed yet on the desktop. Are you cli or desktop?",
          "score": 2,
          "created_utc": "2026-02-16 19:30:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qo09o",
              "author": "soul105",
              "text": "It's not yet fixed in CLI",
              "score": 2,
              "created_utc": "2026-02-16 19:52:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5qn0el",
              "author": "Charming_Support726",
              "text": "I am using web - building everything myself since I started a few month ago. Works til now without any issue.",
              "score": 1,
              "created_utc": "2026-02-16 19:47:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5qnm03",
                  "author": "trypnosis",
                  "text": "Never looked at the desktop app but if itâ€™s electron or any other web wrapper then it might be the same problem. Try the cli for a bit. I went from CC Max to Co Pilot pro+ I think I get decent value for money.",
                  "score": 3,
                  "created_utc": "2026-02-16 19:50:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67tgeo",
              "author": "TheDuck-Prince",
              "text": "I canâ€™t find anywhere that the CLI version is fixed ;(",
              "score": 1,
              "created_utc": "2026-02-19 09:57:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o68rosx",
                  "author": "trypnosis",
                  "text": "My bad I was sure some one mentioned it else where on this subreddit.",
                  "score": 1,
                  "created_utc": "2026-02-19 14:07:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5yt6hd",
          "author": "HarjjotSinghh",
          "text": "copilot + opencode is the real secret sauce now",
          "score": 2,
          "created_utc": "2026-02-18 00:42:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ytw5s",
              "author": "FaerunAtanvar",
              "text": "Care to elaborate?",
              "score": 1,
              "created_utc": "2026-02-18 00:46:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o67t60b",
              "author": "TheDuck-Prince",
              "text": "What",
              "score": 1,
              "created_utc": "2026-02-19 09:54:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o67vzsr",
              "author": "Legal_Dimension_",
              "text": "Yep, it's a fantastic price with the free models picking up explore tasks etc.",
              "score": 1,
              "created_utc": "2026-02-19 10:21:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5quyxg",
          "author": "Legal_Dimension_",
          "text": "Just received a suspension to my account for using copilot with opencode. I'm a heavy user and they don't like that so be careful.",
          "score": 1,
          "created_utc": "2026-02-16 20:26:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qvel7",
              "author": "Charming_Support726",
              "text": "That's bad. Could you tell us more? Explicitly because of Opencode? What is your subscription?\n\nBTW: What do you mean with \"Heavy User\" ?",
              "score": 2,
              "created_utc": "2026-02-16 20:28:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o67uj3n",
                  "author": "Legal_Dimension_",
                  "text": "So for context. \n\nGHCP pro+ account. Got a suspension of my copilot only. 2000+ autonomous requests without any user requests through opencode mainly low cost models running research and refinement loops over night.\n\nYou know, the whole point of having them.\n\nWoke up to a suspension, which didn't state why I was suspended just fair usage and ToS BS.\n\nDrafted ticket stating all the grey areas in their fair usage and ToS, that they by UK regulations must state usage rates if it is requirement to use a service etc and that they officially endorse opencode an agentic based cli so they need to reinstate me. They did.",
                  "score": 1,
                  "created_utc": "2026-02-19 10:07:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5qxrzz",
              "author": "ellensen",
              "text": "Suspension of your github  account or just the copilot subscription?",
              "score": 1,
              "created_utc": "2026-02-16 20:40:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67uk3p",
                  "author": "Legal_Dimension_",
                  "text": "Copilot only luckily.",
                  "score": 2,
                  "created_utc": "2026-02-19 10:07:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5t1h0p",
              "author": "joshashsyd",
              "text": "Yes I got a 157hr timeout??",
              "score": 1,
              "created_utc": "2026-02-17 03:42:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67un3p",
                  "author": "Legal_Dimension_",
                  "text": "Wow for heavy agent use? Is the ban still live?",
                  "score": 1,
                  "created_utc": "2026-02-19 10:08:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5wtago",
              "author": "haininhhoang94",
              "text": "i guess you use it with subagents?",
              "score": 1,
              "created_utc": "2026-02-17 18:44:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67rnhw",
                  "author": "Legal_Dimension_",
                  "text": "Yeah but I have developed a plugin to get around the detection while I bullied them with UK regulations and they reinstated the same day.",
                  "score": 1,
                  "created_utc": "2026-02-19 09:39:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r95x9q",
      "title": "Gemini 3.1 Pro is on OpenCode Zen",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r95x9q/gemini_31_pro_is_on_opencode_zen/",
      "author": "jpcaparas",
      "created_utc": "2026-02-19 17:46:09",
      "score": 22,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "https://reddit.com/link/1r95x9q/video/2hzvm35qnhkg1/player\n\nDid this in less than a minute. Not bad. Google doesn't have it yet on their own provider API though.\n\n[https://x.com/thdxr/status/2024531757215694986](https://x.com/thdxr/status/2024531757215694986)\n\nOfficial announcement: [https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro/](https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro/)\n\nWriteup with observations and prompt used: [https://medium.com/reading-sh/i-tested-gemini-3-1-pros-ui-claims-and-they-re-true-fed5c2d8ceb0?sk=9b72e19b03fc5c1745a1b177fb5523e4](https://medium.com/reading-sh/i-tested-gemini-3-1-pros-ui-claims-and-they-re-true-fed5c2d8ceb0?sk=9b72e19b03fc5c1745a1b177fb5523e4)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r95x9q/gemini_31_pro_is_on_opencode_zen/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o6adsz5",
          "author": "antonusaca",
          "text": "Gemini 3.1 is not available in the Gemini CLI, though.",
          "score": 2,
          "created_utc": "2026-02-19 18:52:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ae37r",
              "author": "jpcaparas",
              "text": "ironic",
              "score": 2,
              "created_utc": "2026-02-19 18:53:42",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6apgrg",
              "author": "jpcaparas",
              "text": "https://preview.redd.it/29baqdgn9ikg1.png?width=461&format=png&auto=webp&s=47d93f1232e98a4513ee2fda174a9ea39c05a2f9\n\nIt showed up just now on the Google Provider after a model refresh.",
              "score": 2,
              "created_utc": "2026-02-19 19:48:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o69zwod",
          "author": "jpcaparas",
          "text": "Prompt (if you want to try it out yourself):\n\n```\nWrite complete HTML, CSS, and JavaScript in a single self-contained file for an immersive, cinematic website about a Luxury Perfume House named \"Atelier Obscur\".\n\nThe site must feel like a living brand experience -- sections that breathe as you scroll, typography that has weight and presence, and atmosphere you can almost feel.\n\n### Theme Selection\n\n**Concept**: Niche fragrance atelier -- think Diptyque meets A Lab on Fire\n**Palette**: Obsidian (#0a0a0f), amber (#d4a853), mist white (#f0ede8), faint rose\n**Typography**: Playfair Display (display), Raleway (body)\n\n### Technical Constraints (NON-NEGOTIABLE)\n\n- **Single file**: All HTML, CSS, JS in one `.html` file\n- **No external images**: Use CSS gradients, SVG inline art, Canvas API, or unicode/emoji symbols only\n- **No CDN dependencies** for core visuals (Google Fonts via @import is acceptable)\n- **No frameworks**: Vanilla JS only. No React, Vue, Alpine, etc.\n- **Static**: Must work as a static file (no server-side logic)\n- **Self-hostable**: Drop into Vercel as `index.html` or paste into CodePen\n\n### Quality Bar\n\nThis is NOT a simple landing page. It should feel like it was commissioned by a world-class brand agency. Include:\n\n**Visual craft:**\n\n- Scroll-triggered reveals with Intersection Observer (not scroll events -- use IO for performance)\n- Parallax depth on at least 2 layers (background moves slower than foreground)\n- Typography that has character -- mix of serif and sans, large display type with tight tracking\n- Color palette of 3-4 tones maximum, applied with intention (not rainbow)\n- Subtle animated texture or grain overlay via CSS (repeating SVG noise, pseudo-element with opacity)\n\n**Interaction:**\n\n- Cursor that responds subtly (not gimmicky -- a small dot or trail at most)\n- Hover states that feel intentional (transforms, not just color changes)\n- Navigation that changes opacity/style on scroll\n\n**Atmosphere:**\n\n- Hero section with large, centered typographic statement\n- At minimum 5 distinct sections with scroll transitions between them\n- Footer with subtle details (coordinates, a fictional address, a motto in a second language)\n- A moment of surprise -- one section that does something unexpected (inverted colors, a canvas animation, text that rearranges)\n- Scent \"notes\" (top/heart/base) revealed on scroll as floating pills\n- Canvas particle system simulating mist/smoke drifting upward\n- CSS blurs that shift as you scroll (backdrop-filter manipulation)\n\n**Polish details:**\n\n- `font-display: swap` on any web fonts\n- `will-change: transform` on animated elements\n- Smooth scroll behavior on `html`\n- Mobile-responsive (flexbox/grid, no fixed px widths on containers)\n- `prefers-reduced-motion` media query to disable animations for accessibility\n```",
          "score": 2,
          "created_utc": "2026-02-19 17:47:34",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o6bdgfs",
          "author": "jnpkr",
          "text": "Thatâ€™s a really great prompt too. Have you got a system for prompt writing or was that straight out of your head?",
          "score": 1,
          "created_utc": "2026-02-19 21:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6binnp",
              "author": "jpcaparas",
              "text": "I have a \\`/oneshot-website\\` command that invokes a similarly-named skill with progressive disclosure. It then generates the [PROMPT.md](http://PROMPT.md) for creating the website :) ",
              "score": 3,
              "created_utc": "2026-02-19 22:11:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6b4qkt",
          "author": "SynapticStreamer",
          "text": "Been using it in Antigravity for about 60 minutes. I find it one hell of a step up from 3.0 so far.",
          "score": 1,
          "created_utc": "2026-02-19 21:02:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5yjz1",
      "title": "Huge Update: You can now run Shannon (Autonomous AI Pentester) directly on OpenCode! ðŸ›¡ï¸ðŸ’»",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r5yjz1/huge_update_you_can_now_run_shannon_autonomous_ai/",
      "author": "ResponsiblePlant8874",
      "created_utc": "2026-02-16 03:21:35",
      "score": 21,
      "num_comments": 10,
      "upvote_ratio": 0.84,
      "text": "If youâ€™ve been using **OpenCode** for autonomous development but worrying about the security of the code your agents are churning out, this is for you.\n\nA new plugin just dropped that lets you run **Shannon**â€”the fully autonomous AI hackerâ€”directly within your OpenCode environment.\n\n# What is Shannon?\n\nFor those who missed the buzz, Shannon (by KeygraphHQ) is essentially the \"Red Team\" to your \"Blue Team.\" While your other agents are busy building features, Shannonâ€™s only job is to break them. It doesnâ€™t just give you \"alerts\"; it actually identifies and delivers exploits to prove where your vulnerabilities are.\n\n# Why this matters for OpenCode users:\n\nUntil now, Shannon was mostly a standalone powerhouse. With the **opencode-shannon-plugin**, you can now bake security auditing right into your agentic workflow.\n\n* **Security-First Vibe Coding:** Stop treating security as an afterthought.\n* **Autonomous Audits:** Let Shannon scan your PRs and local codebase for exploits before you ever hit \"merge.\"\n* **Zero Friction:** It integrates directly via the OpenCode plugin system.\n\n# How to get it:\n\nThe plugin is hosted on GitHub by **vichhka-git**: ðŸ‘‰[https://github.com/vichhka-git/opencode-shannon-plugin](https://github.com/vichhka-git/opencode-shannon-plugin)\n\n**Quick Install (usually):**\n\n1. Clone/Add the plugin to your `.opencode/plugin/` directory.\n2. Restart OpenCode.\n3. (Check the README for specific environment variables needed for the Shannon core).\n\nHuge props to the dev for making this bridge. It makes the \"full-stack\" agentic dream feel a lot more production-ready.\n\n**Has anyone tried running it against their current projects yet? Curious to see what kind of exploits it's catching in AI-generated code!**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r5yjz1/huge_update_you_can_now_run_shannon_autonomous_ai/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5o4z8s",
          "author": "MaxPhoenix_",
          "text": "The actual purpose of this software differs significantly from the claims of this Reddit post - it says shannon plugin is a tool for scanning PRs and auditing local codebases for vulnerabilities, yet the plugin contains no code analysis capabilities whatsoever - it's purely a network penetration testing framework designed to attack live web applications using tools like nmap, sqlmap, and hydra in a Kali Linux Docker container. If you're looking for static code security auditing or SAST, this isn't it; if you need to pentest deployed applications, it might be useful.\n\nThis post conflates the original Shannon's capabilities (which DO include source code analysis for \"scan your PRs\") with this simplified plugin (which only does black-box network pentesting). The plugin author appears to have built a thin wrapper around Shannon's attack tools without implementing the source analysis capabilities that would make the \"scan your PRs\" claim true. Shrug.",
          "score": 9,
          "created_utc": "2026-02-16 12:08:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5scnep",
              "author": "oulu2006",
              "text": "that's a great assessment thank you for ur service ",
              "score": 2,
              "created_utc": "2026-02-17 01:10:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nddj8",
          "author": "xak47d",
          "text": "Any volunteer to do a security audit on this",
          "score": 3,
          "created_utc": "2026-02-16 07:57:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5o11ma",
              "author": "MaxPhoenix_",
              "text": "I have audited the code and it is legit but a problem with seeking security assurance on the internet is you might not trust my authority.  The docker container runs with direct network access (which seems reasonable but you should be aware of it) but it has no obfuscated code, minified files, hardcoded secrets, suspicious network calls, eval or function usage, nor malicious imports.  EDIT: however, the description here doesn't match the code so even though it's \"clean\", people need to be careful what they are getting into.",
              "score": 4,
              "created_utc": "2026-02-16 11:36:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5o2csv",
                  "author": "gottapointreally",
                  "text": "Thank you for taking a look",
                  "score": 2,
                  "created_utc": "2026-02-16 11:47:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ns4cn",
              "author": "Swimming_Ad_5205",
              "text": "ÐÑ…Ð°Ñ…Ð°Ñ…Ð° Ð¾Ñ‡ÐµÐ½ÑŒ Ñ‚Ð¾Ñ‡Ð½Ð¾",
              "score": 2,
              "created_utc": "2026-02-16 10:16:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5odqli",
          "author": "lundrog",
          "text": "Seems safe ðŸ‘€",
          "score": 3,
          "created_utc": "2026-02-16 13:09:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5scot7",
              "author": "oulu2006",
              "text": "lol\n\n",
              "score": 2,
              "created_utc": "2026-02-17 01:10:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5v75dg",
          "author": "HarjjotSinghh",
          "text": "this just saved my pentesting life!",
          "score": 2,
          "created_utc": "2026-02-17 13:57:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pe4op",
          "author": "ResponsiblePlant8874",
          "text": "this repo will take sometime to continue update",
          "score": 1,
          "created_utc": "2026-02-16 16:18:45",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4v7by",
      "title": "OCMONITOR - a CLI tool to monitor OPENCODE CLI usage",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r4v7by/ocmonitor_a_cli_tool_to_monitor_opencode_cli_usage/",
      "author": "WriterOld3018",
      "created_utc": "2026-02-14 20:36:06",
      "score": 18,
      "num_comments": 3,
      "upvote_ratio": 0.95,
      "text": "https://preview.redd.it/95r6b42ktijg1.png?width=3790&format=png&auto=webp&s=e0b2919618f556d387b59e6071b3bb85890aa3bc\n\nHello opencode community,\n\n5 months ago I madeÂ ocmonitor, an open-source CLI tool to monitor opencode usage. Since yesterday (version 1.2.0+), opencode migrated from storing sessions in JSON files to using a SQLite database. Iâ€™ve updated ocmonitor to support this change.\n\nI also added a hierarchy view to show subagents as part of the parent session, and monitoring of output rate (TPS) to give an indication of model performance.\n\nI would appreciate any feedback or bug reports (preferably via GitHub). PRs and contributions are also welcome.  \n[https://github.com/Shlomob/ocmonitor-share](https://github.com/Shlomob/ocmonitor-share)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4v7by/ocmonitor_a_cli_tool_to_monitor_opencode_cli_usage/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5fclrh",
          "author": "rizal72",
          "text": "I love it! Beautifully done, very useful, very good job!",
          "score": 1,
          "created_utc": "2026-02-14 23:59:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ijgae",
          "author": "altsyst",
          "text": "Nice! Is there a way to inspect the whole context window besides having the number of tokens? I'm looking for a tool allowing me to see easily the whole context.",
          "score": 1,
          "created_utc": "2026-02-15 14:50:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kgktq",
          "author": "HarjjotSinghh",
          "text": "this is unreasonably cool actually!",
          "score": 1,
          "created_utc": "2026-02-15 20:33:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r78b58",
      "title": "Cron Jobs, Integrations, and OpenCode are all you need to build 24/7 agent like OpenClaw",
      "subreddit": "opencodeCLI",
      "url": "https://github.com/composiohq/secure-openclaw",
      "author": "LimpComedian1317",
      "created_utc": "2026-02-17 15:01:48",
      "score": 16,
      "num_comments": 8,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r78b58/cron_jobs_integrations_and_opencode_are_all_you/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o60eb6l",
          "author": "amplifyoucan",
          "text": "What do you do for browser usage? Have it run playwright or something?",
          "score": 2,
          "created_utc": "2026-02-18 06:38:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66vajm",
              "author": "LimpComedian1317",
              "text": "It's handled by Composio's browser use tool.",
              "score": 1,
              "created_utc": "2026-02-19 04:56:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wodmj",
          "author": "Embarrassed_Bread_16",
          "text": "yeh, i want to implement it soon for and build me a bot that will send me summaries of books i have on the to read list in audio form",
          "score": 1,
          "created_utc": "2026-02-17 18:22:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66bntv",
          "author": "dheetoo",
          "text": "I just built one! https://github.com/dheerapat/botan-ebi",
          "score": 1,
          "created_utc": "2026-02-19 02:50:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wom2d",
          "author": "Embarrassed_Bread_16",
          "text": "edit:  \nnice blog, im taking it as inspo for my next build  \n  \nwhat docs did you / your bot use for reference?\n\ndid you use any other project as an inspo?",
          "score": 1,
          "created_utc": "2026-02-17 18:23:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66u86p",
              "author": "LimpComedian1317",
              "text": "The original OpenClaw was the inspiration, I just re-did it around OpenCode and ClaudeCode with integrations",
              "score": 1,
              "created_utc": "2026-02-19 04:48:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5xs7gn",
          "author": "Nearby_Tumbleweed699",
          "text": "Excuse my ignorance. Is it possible to have multiple agents running in parallel within the same session? That is, I have a series of tasks and I want to delegate them to a number of agents to execute. I know that Oh My OpenCode can do that, but what I don't understand is why install something else if we already have OpenCode as a tool.",
          "score": 1,
          "created_utc": "2026-02-17 21:29:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yuc08",
          "author": "miaowara",
          "text": "Kimaki (Discord) opencode plugin far exceeds openclawâ€™s discord implementation as well. Will be taking a look at your setup here. Was getting massively frustrated with openclaw & just wanted to stick with opencode.",
          "score": 0,
          "created_utc": "2026-02-18 00:48:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4vefc",
      "title": "Holy shit, Codex-5.3-Spark on OpenCode is FAST!",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r4vefc/holy_shit_codex53spark_on_opencode_is_fast/",
      "author": "jpcaparas",
      "created_utc": "2026-02-14 20:44:22",
      "score": 15,
      "num_comments": 16,
      "upvote_ratio": 0.75,
      "text": "Will provide some detailed feedback soon, but for those on the fence:\n\nEVERYTHING IS **INSTANT**. IT IS THE REAL THING!\n\n*\"I could smell colors, I could feel sounds.\"*\n\n  \nUpdate: I'm going back to Plus. The limited weekly cap and compaction issues are simply to hard to justify for the $200 price tag.\n\nhttps://preview.redd.it/fhp62ppcskjg1.png?width=1504&format=png&auto=webp&s=0413284d29b14420a50bf01cfa5e494de0abacc3\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r4vefc/holy_shit_codex53spark_on_opencode_is_fast/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5efbak",
          "author": "HarjjotSinghh",
          "text": "okay first post ever? how's that instant thing work?",
          "score": 7,
          "created_utc": "2026-02-14 20:48:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ei7hq",
              "author": "ExtentOdd",
              "text": "Try Cerebras, you will feel that constant thing",
              "score": 6,
              "created_utc": "2026-02-14 21:04:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5g7yaw",
                  "author": "franz_see",
                  "text": "I have cerebras. I get rate limited a lot with its GLM 4.7 though ðŸ˜… but itâ€™s super fast! That being said, itâ€™s also super fast at consuming tokens! ðŸ˜…",
                  "score": 3,
                  "created_utc": "2026-02-15 03:26:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ef77e",
          "author": "jpcaparas",
          "text": "https://preview.redd.it/wsb29jnnvijg1.png?width=402&format=png&auto=webp&s=7d31b23879ab602f5b04840c78cfa635516a71f2\n\nAs you've probably already read, the context length is only 128K, so you'll have to leverage subagents where possible to break down bulky tasks.",
          "score": 11,
          "created_utc": "2026-02-14 20:47:49",
          "is_submitter": true,
          "replies": [
            {
              "id": "o5g7tqd",
              "author": "franz_see",
              "text": "Yep. Youâ€™d have to create a task list and keep delegating to a codex spark powered subagent to maximize it.\n\nTbh, cant imagine any other agent being able to maximize it as much as opencode. I could be wrong though.",
              "score": 2,
              "created_utc": "2026-02-15 03:25:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5g94fs",
                  "author": "jpcaparas",
                  "text": "No, you're right. That's why only use OpenCode and Claude Code these days. I requested a refund from OpenAI a few minutes ago for the atrocious weekly limits of Spark.",
                  "score": 2,
                  "created_utc": "2026-02-15 03:34:34",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5epsr9",
          "author": "jpcaparas",
          "text": "Okay, some early thoughts:\n\n- Auto compaction is horrible with Spark\n- It's very capable and very snappy, just avoid hitting the context window limits. \n- Your only noticeable bottleneck are external API calls responses\n- Spark is better used a hardcoded model on subagents instead of being the main model, ie use Opus 4.6,  Codex-5.3 or Kimi K2.5 as the orchestrator and have most if not all subagents use Spark.",
          "score": 9,
          "created_utc": "2026-02-14 21:45:37",
          "is_submitter": true,
          "replies": [
            {
              "id": "o5evrtz",
              "author": "segmond",
              "text": "how do you set up k2.5 as orchestrator and subagents to use spark?",
              "score": 2,
              "created_utc": "2026-02-14 22:18:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5extvm",
                  "author": "jpcaparas",
                  "text": "say for example, you have a slash command and that slash command invokes subagents: dont hardcode the model on the md file of the slash command, use ctrl + p to do model selection, but for the subagents that you definitely need spark for, hardcode them on the agent's md file",
                  "score": 3,
                  "created_utc": "2026-02-14 22:30:30",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5hto00",
              "author": "aithrowaway22",
              "text": "Can Kimi 2.5 really replace Codex 5.3/GPT 5.2 (on high) / Opus 4.5/4.6 in architecture/orchestrator roles ?  \nEven on LocalLama most people agree that open source models are not on that level for complex tasks.  \n",
              "score": 1,
              "created_utc": "2026-02-15 11:59:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5f7crw",
          "author": "j00stmeister",
          "text": "Cool, good to hear. Quick question tho: do you use it through the API or a ChatGPT plus/pro subscription?  \nWhen I use it using my subscription I get 'The 'gpt-5.3-codex-spark' model is not supported when using Codex with a ChatGPT account.'",
          "score": 1,
          "created_utc": "2026-02-14 23:26:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fcdlg",
              "author": "jpcaparas",
              "text": "I use the $200 Pro subscription. It's only available there... for now. Given the competitive nature these days, I doubt OpenAI will silo it for too long within that tier",
              "score": 1,
              "created_utc": "2026-02-14 23:58:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5fcv5k",
                  "author": "j00stmeister",
                  "text": "Ah good to know, thanks!",
                  "score": 0,
                  "created_utc": "2026-02-15 00:01:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5eeru8",
          "author": "jpcaparas",
          "text": "I'm more interested how it performs with layered subagents, so I'll factor that in too with feedback.",
          "score": 0,
          "created_utc": "2026-02-14 20:45:29",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6marc",
      "title": "Any difference when using GPT model inside Codex vs OpenCode?",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r6marc/any_difference_when_using_gpt_model_inside_codex/",
      "author": "ponury2085",
      "created_utc": "2026-02-16 21:19:33",
      "score": 14,
      "num_comments": 13,
      "upvote_ratio": 0.85,
      "text": "I'm a die-hard fan of OpenCode - because of free model, how easy it is to use subagents, and just because it's nice. But I wonder if anyone finds GPT models better in Codex? I cannot imagine why they could possibly work better there, but maybe models are just trained that way, so they \"know\" the tools etc? Anyone noticed anything like that?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r6marc/any_difference_when_using_gpt_model_inside_codex/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o5r8t8v",
          "author": "itsjase",
          "text": "I think both claude code and codex have some magic sauce to work better with their respective models.\n\nI personally think codex + 5.3 codex is way ahead of opencode + 5.3 codex. I'm realising now the harness matters just as much as the model these days.",
          "score": 9,
          "created_utc": "2026-02-16 21:34:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5v8pp0",
              "author": "BodeMan5280",
              "text": "that's interesting... I think it comes down to speed for me. OpenCode seems to just get shit done (yes, ironic pun to GSD). Don't get me wrong, Codex is KILLER at getting shit done, but slower IMO.",
              "score": 1,
              "created_utc": "2026-02-17 14:06:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5r7bv5",
          "author": "TechCynical",
          "text": "This is what people mean when they say \"the harness\". Using it in codex means you get the bare bones experience. Not bad but it just means it isn't fine tuned to work specifically for coding /what you would want for coding at least. \n\nThere's a concern for over engineering but that's why things are open source. Claudecode for example has a lot of changes to its system prompt to work well for everything Claude code will try to do like call mini agents and tools during it's execution. Codex afaik actually has nothing but I could be wrong. GitHub copilot has its own too supposedly tuned for multi model workflows, and opencode has their own as well.\n\nImo all models work better in opencode. Sometimes this changes with select models, but it's a safe bet just to use opencode.",
          "score": 7,
          "created_utc": "2026-02-16 21:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uudx6",
              "author": "Morisander",
              "text": "Would you kindly explain your first and last paragraph? This way it sounds a little likeâ€¦ bullshit?",
              "score": 2,
              "created_utc": "2026-02-17 12:41:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tpxpz",
          "author": "widonext",
          "text": "For me there is a difference, but it works great in opencode, so itâ€™s fine for me",
          "score": 2,
          "created_utc": "2026-02-17 06:45:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tjliv",
          "author": "georgiarsov",
          "text": "I tried codex 5.3 in opencode on release and can confirm it was 100% shit. I couldnâ€™t believe the huge gap between my experience and that of the people using it in codex",
          "score": 1,
          "created_utc": "2026-02-17 05:52:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5u1gbv",
          "author": "Round_Mixture_7541",
          "text": "Yes. AI harnesses built by their respective model producers tend to work better together.",
          "score": 1,
          "created_utc": "2026-02-17 08:32:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5u2uij",
          "author": "Open_Scallion9015",
          "text": "I had this experience myself previously but it seems that since a month or so this gap has narrowed or maybe even completely closed. Personally did not had to urge to use the Codex harness recently at all.",
          "score": 1,
          "created_utc": "2026-02-17 08:45:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vlrc6",
          "author": "HarjjotSinghh",
          "text": "this sucks too much i'd pay to use this version",
          "score": 1,
          "created_utc": "2026-02-17 15:14:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60xxzh",
          "author": "blackbirdweb",
          "text": "Simple answer: 5.3-Codex works a bit better in Codex when it comes to the overall quality of the work. However it is much, much nicer to use in Opencode Desktop on Windows. If you are on Windows then the best way to use native codex is the codex plugin in VSCode. However, Opencode Desktop is just much nicer to use, more transparent, more configurable and honestly just more fun. This might change when OpenAI stops gooning over Mac and finally decide to support the most used business OS in the world with their desktop app. I dislike Windows but it's what we use at work as does almost everybody.",
          "score": 1,
          "created_utc": "2026-02-18 09:38:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cpsfl",
          "author": "HarjjotSinghh",
          "text": "this tooling alone is why i'd swap codex for opencode.",
          "score": 1,
          "created_utc": "2026-02-20 02:22:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rw5q7",
          "author": "nyldn",
          "text": "Latest version of Claude Octopus utilises Codex 5.3 smartly https://github.com/nyldn/claude-octopus",
          "score": -5,
          "created_utc": "2026-02-16 23:35:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5txeq6",
              "author": "KnifeFed",
              "text": "Sir, this is the OpenCode sub.",
              "score": 4,
              "created_utc": "2026-02-17 07:54:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r8ohj9",
      "title": "TUI Kanban board for OpenCode",
      "subreddit": "opencodeCLI",
      "url": "https://www.reddit.com/r/opencodeCLI/comments/1r8ohj9/tui_kanban_board_for_opencode/",
      "author": "Professional_Past_30",
      "created_utc": "2026-02-19 03:33:36",
      "score": 13,
      "num_comments": 4,
      "upvote_ratio": 0.89,
      "text": "Hey everyone,\n\nIâ€™ve been working on a tool called **opencode-kanban** to manage multiple OpenCode sessions and Git worktrees.\n\nThe project is essentially a mixture of two existing concepts, aiming to fix the friction I felt with both:\n\n1. **The Functionality (vs. Agent of Empires):** Like [*Agent of Empires*](https://github.com/njbrake/agent-of-empires), this manages multiple agents through tmux sessions. However, I built this because I found AoE a bit clunky for my daily workflow. I wanted something more ergonomic - offering more features like task tracking.\n2. **The Workflow (vs. Vibe Kanban):** I loved the visual organization of [*Vibe Kanban*](https://www.vibekanban.com/), but I didn't want to leave the terminal. This project brings that Kanban view (Todo/Doing/Done) directly into the CLI.\n3. **OpenCode-Only:** Most importantly, because this is built *exclusively* for OpenCode, it doesn't suffer from the friction of tools trying to support every agent out there. Tapping directly into the OpenCode API unlocks a much more tailored experience, giving you native features like automatic session attaching and built-in todo tracking.\n\n**The Result:** A TUI-first experience that lets you manage parallel work streams visually, while still allowing you to stick with the native OpenCode TUI inside auto-managed tmux sessions.\n\nhttps://preview.redd.it/24kvmv98fdkg1.png?width=1280&format=png&auto=webp&s=9851e35ad1ddb7aef473b49b067887488f9581dc\n\nRepo: [https://github.com/qrafty-ai/opencode-kanban](https://github.com/qrafty-ai/opencode-kanban)\n\nWould love to hear your thoughts!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/opencodeCLI/comments/1r8ohj9/tui_kanban_board_for_opencode/",
      "domain": "self.opencodeCLI",
      "is_self": true,
      "comments": [
        {
          "id": "o68fmz7",
          "author": "xenopticon",
          "text": "super cool, congrats on the launch!",
          "score": 3,
          "created_utc": "2026-02-19 12:56:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68i9zd",
          "author": "Nearby_Tumbleweed699",
          "text": "Funciona como plugin dentro opencode o es una herramienta externa?",
          "score": 2,
          "created_utc": "2026-02-19 13:13:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69elnc",
              "author": "Professional_Past_30",
              "text": "It's a standalone program which manages each \"task\" as a tmux session. A \"task\" is something you want to work on linked to a corresponding git worktree/branch. \n\nTypical workflow:\n1. Open the tool\n2. Create a task by specifying a repo and a branch name. The tool will automatically create a new worktree for you.\n3. Attach and work on a task in a dedicated tmux&open code session.",
              "score": 1,
              "created_utc": "2026-02-19 16:05:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67cehb",
          "author": "MrBansal",
          "text": "Slop",
          "score": -2,
          "created_utc": "2026-02-19 07:13:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}