{
  "metadata": {
    "last_updated": "2026-02-04 08:57:42",
    "time_filter": "week",
    "subreddit": "FluxAI",
    "total_items": 4,
    "total_comments": 1,
    "file_size_bytes": 3909
  },
  "items": [
    {
      "id": "1qu6bph",
      "title": "A sketchpad i am working for comfyui.",
      "subreddit": "FluxAI",
      "url": "https://v.redd.it/u4ttrlcl25hg1",
      "author": "Vivid-Loss9868",
      "created_utc": "2026-02-02 20:17:17",
      "score": 13,
      "num_comments": 0,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Not Included",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qu6bph/a_sketchpad_i_am_working_for_comfyui/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qsib7a",
      "title": "Which base model should I use the quants of, Klein or Dev?",
      "subreddit": "FluxAI",
      "url": "https://www.reddit.com/r/FluxAI/comments/1qsib7a/which_base_model_should_i_use_the_quants_of_klein/",
      "author": "ts4m8r",
      "created_utc": "2026-01-31 23:48:55",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I'm on a 3060 12GB, 32GB RAM. unsloth's Flux.2 Klein 9B Q8\\_0 is 9.98GB. Flux.2 Dev has a Q4\\_K\\_M for 20.1 GB. Considering that Klein is already a distilled model, does \"distilling it twice\" by making a quant of it cause enough degradation that I'd be better off just using a different base model? Would the Dev Q4 be too much for my system to handle practically? Am I better off just going with a 4B model for speed generation and then i2i with a higher model for quality later?",
      "is_original_content": false,
      "link_flair_text": "Question / Help",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qsib7a/which_base_model_should_i_use_the_quants_of_klein/",
      "domain": "self.FluxAI",
      "is_self": true,
      "comments": [
        {
          "id": "o2x4pqp",
          "author": "benkei_sudo",
          "text": "Use Flux.2 Klein 9B if you want speed and quality.  \nUse Flux.2 Dev if you want to use LoRA.\n\nQuantization and distillation are two different things:\n\n* Quantization is converting a model to a lower dtype, compressing its size.\n* In distillation, a model is trained in fewer steps. In Klein's case, this makes it capable of inferring in 4 steps (non-distilled needs 50 steps), but the size remains the same.",
          "score": 1,
          "created_utc": "2026-02-01 04:59:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsnd87",
      "title": "Classic Snowman in Winter Landscape",
      "subreddit": "FluxAI",
      "url": "https://i.redd.it/15djf0myxsgg1.png",
      "author": "ai_scribbles",
      "created_utc": "2026-02-01 03:33:09",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.83,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qsnd87/classic_snowman_in_winter_landscape/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qq4hfn",
      "title": "Historical Storyboards of Key Events Created with FLUX 2",
      "subreddit": "FluxAI",
      "url": "https://www.reddit.com/gallery/1qq4gue",
      "author": "Substantial-Fee-3910",
      "created_utc": "2026-01-29 09:56:02",
      "score": 2,
      "num_comments": 0,
      "upvote_ratio": 0.67,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "FLUX 2",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qq4hfn/historical_storyboards_of_key_events_created_with/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": []
    }
  ]
}