{
  "metadata": {
    "last_updated": "2026-02-09 09:19:59",
    "time_filter": "week",
    "subreddit": "FluxAI",
    "total_items": 6,
    "total_comments": 13,
    "file_size_bytes": 28893
  },
  "items": [
    {
      "id": "1qzbz0f",
      "title": "Meet the game dev's new best friend: a Flux model that generates sprite sheets in one go! This AI creates 2x2 multi-view character grids perfect for top-down or isometric games. No more painstakingly drawing each angle separately.",
      "subreddit": "FluxAI",
      "url": "https://i.redd.it/zggq1m4fiaig1.png",
      "author": "Unreal_777",
      "created_utc": "2026-02-08 15:35:19",
      "score": 54,
      "num_comments": 12,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/FluxAI/comments/1qzbz0f/meet_the_game_devs_new_best_friend_a_flux_model/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o49l7k1",
          "author": "Unreal_777",
          "text": "[https://huggingface.co/fal/flux-2-klein-4b-spritesheet-lora](https://huggingface.co/fal/flux-2-klein-4b-spritesheet-lora)",
          "score": 8,
          "created_utc": "2026-02-08 15:35:34",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o49qnqp",
          "author": "siete82",
          "text": "Remember to never mention to have used AI in your game art, for your own sake",
          "score": 11,
          "created_utc": "2026-02-08 16:02:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4adcur",
              "author": "JohnToFire",
              "text": "Is that due to lack of copyright ?",
              "score": 1,
              "created_utc": "2026-02-08 17:53:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4agm0v",
                  "author": "siete82",
                  "text": "No, people simply hate the use of generative AI in making videogames, unless it is used for programming, in which case nobody cares lol.",
                  "score": 15,
                  "created_utc": "2026-02-08 18:08:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4dp808",
              "author": "ICantWatchYouDoThis",
              "text": "if you lie Steam will kill your game anyway",
              "score": 1,
              "created_utc": "2026-02-09 04:28:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4acqoa",
          "author": "Sir_McDouche",
          "text": "Yeah, no game dev will ever go near this. AI is a death sentence in that industry.",
          "score": -10,
          "created_utc": "2026-02-08 17:50:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4adblu",
              "author": "JohnToFire",
              "text": "Is that due to lack of copyright ?",
              "score": 2,
              "created_utc": "2026-02-08 17:52:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4adnt9",
                  "author": "Sir_McDouche",
                  "text": "Because anti-AI people get triggered by it. Have you not heard about what happened to Claire Obscur?",
                  "score": 7,
                  "created_utc": "2026-02-08 17:54:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4adbv9",
              "author": "EuroTrash1999",
              "text": "https://www.youtube.com/watch?v=2ovgdMZzM3Q",
              "score": 1,
              "created_utc": "2026-02-08 17:52:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxoex2",
      "title": "World of Vulcan. A film made with Flux LoRAs trained on my own analog photography",
      "subreddit": "FluxAI",
      "url": "https://v.redd.it/w5zhdmktuwhg1",
      "author": "Significant-Scar2591",
      "created_utc": "2026-02-06 17:44:16",
      "score": 40,
      "num_comments": 6,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "VIDEO",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qxoex2/world_of_vulcan_a_film_made_with_flux_loras/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3ynw3d",
          "author": "pmp22",
          "text": "Amazing. As a lover of antiquity this tickles me. I have all these wonderful images in my head after reading old greek and roman writers and seeing the art and architecture but nothing on screen has been able to compare to that mental imagry. With these tools we'll be able to make it all come to life in ways never seen before.\nSome if these shots are fantastic, great job.",
          "score": 3,
          "created_utc": "2026-02-06 20:17:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yquc1",
              "author": "Significant-Scar2591",
              "text": "Thanks so much for the nice words. It’s amazing what these tools can visualize :)",
              "score": 1,
              "created_utc": "2026-02-06 20:32:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o41fmia",
          "author": "Helpful-Birthday-388",
          "text": "Very good!!",
          "score": 2,
          "created_utc": "2026-02-07 06:21:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o435pry",
              "author": "Significant-Scar2591",
              "text": "Thanks :)",
              "score": 1,
              "created_utc": "2026-02-07 14:50:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43xg3p",
          "author": "realadultactionman",
          "text": "very cool. thanks for sharing.",
          "score": 2,
          "created_utc": "2026-02-07 17:09:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44sja6",
              "author": "Significant-Scar2591",
              "text": "Thanks for the support :)\n\n",
              "score": 2,
              "created_utc": "2026-02-07 19:44:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qwasra",
      "title": "50+ Flux 2 Klein LoRA training runs (Dev and Klein) to see what config parameters actually matter [Research + Video]",
      "subreddit": "FluxAI",
      "url": "https://www.reddit.com/r/FluxAI/comments/1qwasra/50_flux_2_klein_lora_training_runs_dev_and_klein/",
      "author": "Significant-Scar2591",
      "created_utc": "2026-02-05 03:54:57",
      "score": 36,
      "num_comments": 7,
      "upvote_ratio": 0.95,
      "text": "https://preview.redd.it/lpreh1bhdlhg1.png?width=1700&format=png&auto=webp&s=166bc9249cdb1172c01147b1a3a88d813d6ba5db\n\n**Full video here**: [https://youtu.be/Nt2yXplkrVc](https://youtu.be/Nt2yXplkrVc)\n\nI just finished a systematic training study for Flux 2 Klein and wanted to share what I learned. The goal was to train an analog film aesthetic LoRA (grain, halation, optical artifacts, low-latitude contrast)\n\nI came out with two versions of the Klein models I was training Flux 2 Klein, a 3K step version with more artifacts/flares and a 7K step version with better subject fidelity. As well as a version for the dev model. Free on Civitai. But the interesting part is the research.\n\n[https://civitai.com/models/691668/herbst-photo-analog-film](https://civitai.com/models/691668/herbst-photo-analog-film)\n\n# Methodology\n\n50+  training runs using AI Toolkit, changing **one parameter per run** to get clean A/B comparisons. All tests used the same dataset (my own analog photography) with simple captions. Most of the tests were conducted with the Dev model, though when I mirrored the configs for Klein-9b ,I observed the same patterns. I tested on thousands of image generations not covered in this reasearch as I will only touch on what I found was the most noteworthy. \\*I'd also like to mention that the training configs are only 1 of three parts of this process. The training data is the most important; I won't cover that here, as well as the sampling settings when using the model\n\nFor each test, I generated two images:\n\n1. A prompt pulled directly from training data (can the model recreate what it learned?)\n2. \"Dog on a log\" ,tokens that don't exist anywhere in the dataset (can the model transfer style to new prompts?)\n\nThe second test is more important. If your LoRA only works on prompts similar to training data, it's not actually learning style, it's memorizing.\n\n[Example of the two prompts A\\/B testing format. Top row is the default AI toolkit config, bottom row is A\\/B parameter changes \\(in this case, network dimention ratio variation\\)](https://preview.redd.it/0n698a73elhg1.png?width=1986&format=png&auto=webp&s=6ef77ca4b3a9c50a33dbd3ca65eac1ac085a41cc)\n\n# Scheduler/Sampler Testing\n\nBefore touching any training parameters, I tested **every combination of scheduler and sampler** in the K sampler. \\~300 combinations.\n\n**Winner for filmic/grain aesthetic:** `dpmpp_2s_ancestral` \\+ `sgm_uniform`\n\nThis isn't universal, if you want clean digital output or animation, your optimal combo will be different. But for analog texture, this was clearly the best.\n\n[my top picks from testing every scheduler and sampler combo](https://preview.redd.it/4szgr3upelhg1.png?width=2948&format=png&auto=webp&s=068c6308fb791c54a0bf79eb56213424a57d9784)\n\n# Key Parameter Findings\n\n**Network Dimensions**\n\n* Winner: `128, 64, 64, 32` (linear, linear\\_alpha, conv, conv\\_alpha) \\*\\*if you want some secret sauce: something I found across every base model I have trained on is that this combo is universally strong for training style LoRAs of any intent. Many other parameters have effects that are subject to the goal of the user and their taste.\n\nhttps://preview.redd.it/kuigiqhjilhg1.png?width=1988&format=png&auto=webp&s=34d667ceea37b5dc25546005077388222782d095\n\n* Past this = diminishing returns\n* Cranking all to 256 = images totally destroyed (honestly, it looks coo,l and it made me want to make some experimental models that are designed for extreme degradation and I'd like to test further, but for this use case: unusable)\n\n[256 universal rank degredationon the lower right images](https://preview.redd.it/hk96h5fyhlhg1.png?width=2171&format=png&auto=webp&s=0b68b45176697bab38e1f6fb83625fd6eb4bd7a7)\n\n**Decay**\n\n* Lowering decay by 10x from the default improved grain pickup and shadow texture. This is a parameter that had a huge enhancement in the low noise learning of grain patterns, but for illustrative and animation models, I would recommend the opposite, to increase this setting.\n* Highlights bloomed more naturally with visible halation\n* This was one of the biggest improvements\n\n[Decay lowered 5x \\(bottom\\) for the Dev model ](https://preview.redd.it/k2cmrzbfjlhg1.png?width=1179&format=png&auto=webp&s=896a880328cd37138a02c4f8b872500b1991f1d4)\n\n**Lower decay (left):**\n\n* Lifted black point\n* RGB channels bleed into each other\n* Less saturated, more washed-out look\n\n**Higher decay (right):**\n\n* Deeper blacks\n* More channel separation\n* Punchier saturation, more contrast\n\nNeither end is \"correct\". It's about understanding that these parameter changes, though mysterious computer math under the hood, produce measurable differences in the output. The waveform shows it's not placebo; decay has a real, visible effect on black point, channel separation, and saturation.\n\n[Far left - low decay, far right, high decay. ](https://preview.redd.it/1aoxdps5llhg1.png?width=2474&format=png&auto=webp&s=f28221b1f63645e1dae5b1438194fb4250c93987)\n\n**Timestep Type**\n\n* Tested sigmoid, linear, shift\n* Shift gave interesting outputs but defaults (balanced) were better overall for this look. I've noticed when training anime / illustrative LoRAs that training with Shift increased the prevalence of the brush strokes and medium-level noise learning.\n\nhttps://preview.redd.it/hv6a7yu1mlhg1.png?width=1959&format=png&auto=webp&s=c09065ac88ffbfe91eed0d09933c4d7e1116db68\n\n**FP32 vs FP8 Training**\n\n* For Flux 2 Klein specifically, FP8 training produced better film grain texture\n* Non-FP8 had better subject fidelity but the texture looked neural-network-generated rather than film-like\n* This might be model-specific, on others I found training with the dtype of fp32 gave a noticeably higher fidelity. (training time increases nearly 10x, though, it's often not worth the squeeze to test until the final iterations of the fine-tune)\n\n# Step Count\n\nAll parameter tests run at 3K steps (good enough to see if the config is working without burning compute).\n\nOnce I found a winning config (v47), I tested epochs from 1K → 10K+ steps:\n\n* **3K steps:** More optical artifacts, lens flares, aggressive degradation\n* **7K steps (dev winner):** Better subject retention while keeping grain, bloom, tinted shadows\n* Past 7k steps was a noticeable spike in degradation to the point of anatomical distortion that was not desirable.\n\nI'm releasing both\n\n[testing v47 of the dev model 1-10k steps at epochs every 250 steps. \\(1-8k depicted here\\)](https://preview.redd.it/2qqr23fxglhg1.png?width=2235&format=png&auto=webp&s=be178f4573c92d53cfb536b69b418e4585b0130e)\n\nIf you care to try any of the modes:\n\n**Recommended settings:**\n\n* Trigger word: `HerbstPhoto`\n* LoRA strength: 0.73 sweet spot (0.4-0.75 balanced, 0.8-1.0 max texture)\n* Sampler: `dpmpp_2s_ancestral` \\+ `sgm_uniform`\n* Resolution: up to 2K\n\nHappy to answer questions about methodology or specific parameter choices.",
      "is_original_content": false,
      "link_flair_text": "FLUX 2",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qwasra/50_flux_2_klein_lora_training_runs_dev_and_klein/",
      "domain": "self.FluxAI",
      "is_self": true,
      "comments": [
        {
          "id": "o3ox7ut",
          "author": "addandsubtract",
          "text": "Hey, I happened to skim through the video when you released it last week, and was hoping for a written version, so thanks for this post! Also thanks for putting the time into test the various scheduler's / sampler's. Do you plan on making more of these studies, maybe with a focus on the dataset or the other training parameters?",
          "score": 3,
          "created_utc": "2026-02-05 09:58:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pbyos",
              "author": "Significant-Scar2591",
              "text": "Ah cool :) oh absolutely, I have more in the process now. This was a test about emulation low noise analog details, moving forward I will be doing less emulation and more distillation + deliberate overfitting to create unique aesthetics. This I am much more excited about as the parameters become more of a playground when there isn't such a narrow qualifier of what \"passes and fails\". Training data and theory behind it is more important than this stuff so I wanted to release this first.",
              "score": 3,
              "created_utc": "2026-02-05 12:05:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vcrzm",
          "author": "Agreeable_Effect938",
          "text": "Hello. Nice test.\n\nHere's what you may find interesting: i have tried different approaches to training flux.2 klein, and found one particularly powerful trick. Training on downscaled 768x resolution for like 70% of the first steps, then go up to 1024-1256x resolution for the remaining 30%. it gave amazing style lora, and the only way I'm able to reach simillar quality is with this approach.  \nThe other trick is merging. It applies to all models, and is super powerful, yet no one seem to know about this. Training a bit different style loras and them merging them together gives much better result then the sum of it's parts. It's like SD1.5 merges, they have this amazing almost Flux-like quality, that none of the base models from the merges had by itself. I've made some very known Flux Loras like AntiBlur, and the secret was always to use merging of different variants to get a really polished Lora",
          "score": 3,
          "created_utc": "2026-02-06 08:44:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vtelv",
              "author": "Significant-Scar2591",
              "text": "Hey, thanks for sharing this, very interesting approach. I've used AntiBlur as well as SameFaceFix and am familiar with abit of your work :) I have a few questions if you can help me better understand: \n\nOn the resolution progression:\n\n* Are you preprocessing two separate datasets (one at 768x, one at 1024-1256x) and switching between them during training? Or are you changing the resolution settings in your training config at \\~70% completion while keeping the same source images?\n* If it's config-based: are you using resolution buckets that cap at 768x for the first phase, then expanding the bucket range?\n\nOn the merging:\n\n* When you say \"different style loras\" - are these trained on different subsets of your dataset, or the same images with different parameters? \n* \"a bit different\" - how different? \n* What's your merge method; how and when is this done? ",
              "score": 2,
              "created_utc": "2026-02-06 11:18:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o420ho4",
                  "author": "Agreeable_Effect938",
                  "text": "For Klein, I've mostly used OneTrainer so far. I only changed the resolution settings in it, without changing the dataset. The resolution there acts as a \"budget\" for the total number of pixels, while also doing image bucketing. So yeah, it works as a cap, but with a bit of variations in width and height around 768x budget. The training on 768x is also much faster in my case, which is a nice bonus (I use rtx4090)\n\nWith merging though, it's the opposite. The more different the merged LORAs, the better. So ideally, you want completely different datasets.\n\nFor example, when I was doing AntiBlur, I first collected [Multi-focus image fusion](https://en.wikipedia.org/wiki/Multi-focus_image_fusion) / Focus stacking / deep DoF photos for the dataset.\n\nBut the model picked up too many features from the dataset. For example, it de-blurred the mountains, but also made the mountains look a bit like those in the dataset, and so on.\n\n",
                  "score": 1,
                  "created_utc": "2026-02-07 09:38:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3vzzl3",
              "author": "alb5357",
              "text": "I think it'd be great if AI toolkit had a setting for the first epoch to train at 256, then 512, 768, 1024 etc in order.",
              "score": 1,
              "created_utc": "2026-02-06 12:09:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3w6p67",
                  "author": "Significant-Scar2591",
                  "text": "It's a great idea. I'm sure there is a sweet spot on which resolutions to train through which steps. I'll add this to the list of tests to run and share what I find. ",
                  "score": 2,
                  "created_utc": "2026-02-06 12:54:57",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qvraev",
      "title": "Fine tuning flux 2 Klein 9b for unwrapped textures, UV maps",
      "subreddit": "FluxAI",
      "url": "https://www.reddit.com/gallery/1qvraev",
      "author": "Zealousideal-Check77",
      "created_utc": "2026-02-04 15:18:53",
      "score": 36,
      "num_comments": 6,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "LORAS, MODELS, etc  [Fine Tuned]",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qvraev/fine_tuning_flux_2_klein_9b_for_unwrapped/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3mpnay",
          "author": "orph_reup",
          "text": "You can try training an edit lora of paired images 1a The face to unwrap and 1b The unwrapped face.\n\nUse a consistent UV map across all your unwrapped images. \n\nThere are many lora training tools out there, local and cloud. \n\nI think you will get reasonable results.\n\nYou might even do a 2 stage process.\n\n1 unwrap the face.\n2 correct the face to the uv map.\n\nThe second stage of the process would be using the universal uv map as a reference and getting klein to correct your unwrapped uv to the landmarks on the universal map. You might train a lora on this part.\n\nImo Klein is accurate enough to make these experiments worth while.",
          "score": 5,
          "created_utc": "2026-02-05 00:26:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kr3bt",
          "author": "alb5357",
          "text": "I imagine people in the future seeing our unwrapped face textures and thinking it's our weird stylistic art.",
          "score": 2,
          "created_utc": "2026-02-04 18:37:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3laq1h",
              "author": "Zealousideal-Check77",
              "text": "Bro u everywhere xDDD",
              "score": 3,
              "created_utc": "2026-02-04 20:08:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lr6rk",
          "author": "Unis_Torvalds",
          "text": "You can't just give it the UV maps as a dataset.  You need to teach the model how to go from a face photograph to a UV unwrap.\n\nAre there not existing (non-AI) tools or apps which are already set up to do this?",
          "score": 1,
          "created_utc": "2026-02-04 21:26:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n47t7",
              "author": "Zealousideal-Check77",
              "text": "There are tools but we are looking for an AI model to speed up our pipeline",
              "score": 1,
              "created_utc": "2026-02-05 01:49:05",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3nbdwh",
              "author": "Unis_Torvalds",
              "text": "Stitched images (like before/after composites) might work for training. Would require testing of course.",
              "score": 1,
              "created_utc": "2026-02-05 02:30:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qu6bph",
      "title": "A sketchpad i am working for comfyui.",
      "subreddit": "FluxAI",
      "url": "https://v.redd.it/u4ttrlcl25hg1",
      "author": "Vivid-Loss9868",
      "created_utc": "2026-02-02 20:17:17",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Not Included",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qu6bph/a_sketchpad_i_am_working_for_comfyui/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qyqedu",
      "title": "Flux Klein performance difference between 5090 vs 3090?",
      "subreddit": "FluxAI",
      "url": "https://www.reddit.com/r/FluxAI/comments/1qyqedu/flux_klein_performance_difference_between_5090_vs/",
      "author": "TheTwelveYearOld",
      "created_utc": "2026-02-07 21:44:47",
      "score": 4,
      "num_comments": 9,
      "upvote_ratio": 0.75,
      "text": "**Edit:** Here's the workflow: [https://pastebin.com/AWst9jX1](https://pastebin.com/AWst9jX1). On Runpod I replaced the distilled int8 model with the [distilled nvfp4 model](https://huggingface.co/black-forest-labs/FLUX.2-klein-9b-nvfp4), replaced the load int8 node with a regular load diffusion model node, and removed torchcompilemodel. Int8 models: [https://huggingface.co/aydin99/FLUX.2-klein-4B-int8](https://huggingface.co/aydin99/FLUX.2-klein-4B-int8).\n\nI've been wondering if I should upgrade from a 3090 to a 50 card. On the 3090 I use Klein 9B int8, and on a 5090 Runpod instance: Klein 9B nvfp4. Same comfyui workflow, using the in-paint crop and stitch node on 1536 x 3424 images for in-painting. Overall it was on average 2x faster, \\~20 secs on the 5090 and 30-40 on the 3090, little quality difference.\n\nI don't feel like its worth upgrading. These were quick and dirty tests, but tell me your thoughts.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qyqedu/flux_klein_performance_difference_between_5090_vs/",
      "domain": "self.FluxAI",
      "is_self": true,
      "comments": [
        {
          "id": "o45g1dn",
          "author": "ArsInvictus",
          "text": "Not using distilled for 9B?  Then it’s like 3-4 seconds on a 5090.  Gens faster than I can decide it’s a good gen or not.  For the 2x faster, that’s a personal decision on value.  I am sitting here with a 5090 debating a Blackwell 6000 upgrade :)",
          "score": 3,
          "created_utc": "2026-02-07 21:50:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45hh2d",
              "author": "TheTwelveYearOld",
              "text": "Isn't this the distilled one? https://huggingface.co/black-forest-labs/FLUX.2-klein-9b-nvfp4 this is the one I tried.",
              "score": 1,
              "created_utc": "2026-02-07 21:58:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o45pwlc",
                  "author": "ArsInvictus",
                  "text": "Yeah that should be the distilled one, the base model has base in the name.  If you are gettin 20 second generation times on distilled with nvfp4 on a 5090 then something is definitely wrong with the setup or workflow.",
                  "score": 1,
                  "created_utc": "2026-02-07 22:45:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47vsqo",
          "author": "jib_reddit",
          "text": "The 5090 is 2-3x faster than 3090 with the same models, I am going to wait for the 6090 now as it will likely be out in 12 months and is the new 3nm Rubin architecture that will be 4-6x faster than a 3090. In the mean time I just rent a 5090 for a $0.79 an hour on Runpod.",
          "score": 1,
          "created_utc": "2026-02-08 07:45:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47y6b6",
              "author": "TheTwelveYearOld",
              "text": "What GPUs do u have right now?",
              "score": 1,
              "created_utc": "2026-02-08 08:07:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48u7id",
                  "author": "jib_reddit",
                  "text": "A 3090.\n\nI tried to get a 5090 at launch but they sold out in 1 second and were unavailable for ages, now the prices have gone really high again and it doesn't feel worth it.\n\n But I am guessing the same thing will just happen when the 6090 comes out.",
                  "score": 1,
                  "created_utc": "2026-02-08 12:56:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}