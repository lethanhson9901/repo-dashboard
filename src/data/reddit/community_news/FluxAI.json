{
  "metadata": {
    "last_updated": "2026-02-05 09:15:02",
    "time_filter": "week",
    "subreddit": "FluxAI",
    "total_items": 7,
    "total_comments": 5,
    "file_size_bytes": 16214
  },
  "items": [
    {
      "id": "1qvraev",
      "title": "Fine tuning flux 2 Klein 9b for unwrapped textures, UV maps",
      "subreddit": "FluxAI",
      "url": "https://www.reddit.com/gallery/1qvraev",
      "author": "Zealousideal-Check77",
      "created_utc": "2026-02-04 15:18:53",
      "score": 26,
      "num_comments": 6,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "LORAS, MODELS, etc  [Fine Tuned]",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qvraev/fine_tuning_flux_2_klein_9b_for_unwrapped/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3mpnay",
          "author": "orph_reup",
          "text": "You can try training an edit lora of paired images 1a The face to unwrap and 1b The unwrapped face.\n\nUse a consistent UV map across all your unwrapped images. \n\nThere are many lora training tools out there, local and cloud. \n\nI think you will get reasonable results.\n\nYou might even do a 2 stage process.\n\n1 unwrap the face.\n2 correct the face to the uv map.\n\nThe second stage of the process would be using the universal uv map as a reference and getting klein to correct your unwrapped uv to the landmarks on the universal map. You might train a lora on this part.\n\nImo Klein is accurate enough to make these experiments worth while.",
          "score": 5,
          "created_utc": "2026-02-05 00:26:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kr3bt",
          "author": "alb5357",
          "text": "I imagine people in the future seeing our unwrapped face textures and thinking it's our weird stylistic art.",
          "score": 3,
          "created_utc": "2026-02-04 18:37:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3laq1h",
              "author": "Zealousideal-Check77",
              "text": "Bro u everywhere xDDD",
              "score": 1,
              "created_utc": "2026-02-04 20:08:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lr6rk",
          "author": "Unis_Torvalds",
          "text": "You can't just give it the UV maps as a dataset.  You need to teach the model how to go from a face photograph to a UV unwrap.\n\nAre there not existing (non-AI) tools or apps which are already set up to do this?",
          "score": 1,
          "created_utc": "2026-02-04 21:26:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n47t7",
              "author": "Zealousideal-Check77",
              "text": "There are tools but we are looking for an AI model to speed up our pipeline",
              "score": 1,
              "created_utc": "2026-02-05 01:49:05",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3nbdwh",
              "author": "Unis_Torvalds",
              "text": "Stitched images (like before/after composites) might work for training. Would require testing of course.",
              "score": 1,
              "created_utc": "2026-02-05 02:30:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qu6bph",
      "title": "A sketchpad i am working for comfyui.",
      "subreddit": "FluxAI",
      "url": "https://v.redd.it/u4ttrlcl25hg1",
      "author": "Vivid-Loss9868",
      "created_utc": "2026-02-02 20:17:17",
      "score": 13,
      "num_comments": 0,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Not Included",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qu6bph/a_sketchpad_i_am_working_for_comfyui/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qwasra",
      "title": "50+ Flux 2 Klein LoRA training runs (Dev and Klein) to see what config parameters actually matter [Research + Video]",
      "subreddit": "FluxAI",
      "url": "https://www.reddit.com/r/FluxAI/comments/1qwasra/50_flux_2_klein_lora_training_runs_dev_and_klein/",
      "author": "Significant-Scar2591",
      "created_utc": "2026-02-05 03:54:57",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.87,
      "text": "https://preview.redd.it/lpreh1bhdlhg1.png?width=1700&format=png&auto=webp&s=166bc9249cdb1172c01147b1a3a88d813d6ba5db\n\n**Full video here**: [https://youtu.be/Nt2yXplkrVc](https://youtu.be/Nt2yXplkrVc)\n\nI just finished a systematic training study for Flux 2 Klein and wanted to share what I learned. The goal was to train an analog film aesthetic LoRA (grain, halation, optical artifacts, low-latitude contrast)\n\nI came out with two versions of the Klein models I was training Flux 2 Klein, a 3K step version with more artifacts/flares and a 7K step version with better subject fidelity. As well as a version for the dev model. Free on Civitai. But the interesting part is the research.\n\n[https://civitai.com/models/691668/herbst-photo-analog-film](https://civitai.com/models/691668/herbst-photo-analog-film)\n\n# Methodology\n\n50+  training runs using AI Toolkit, changing **one parameter per run** to get clean A/B comparisons. All tests used the same dataset (my own analog photography) with simple captions. Most of the tests were conducted with the Dev model, though when I mirrored the configs for Klein-9b ,I observed the same patterns. I tested on thousands of image generations not covered in this reasearch as I will only touch on what I found was the most noteworthy. \\*I'd also like to mention that the training configs are only 1 of three parts of this process. The training data is the most important; I won't cover that here, as well as the sampling settings when using the model\n\nFor each test, I generated two images:\n\n1. A prompt pulled directly from training data (can the model recreate what it learned?)\n2. \"Dog on a log\" ,tokens that don't exist anywhere in the dataset (can the model transfer style to new prompts?)\n\nThe second test is more important. If your LoRA only works on prompts similar to training data, it's not actually learning style, it's memorizing.\n\n[Example of the two prompts A\\/B testing format. Top row is the default AI toolkit config, bottom row is A\\/B parameter changes \\(in this case, network dimention ratio variation\\)](https://preview.redd.it/0n698a73elhg1.png?width=1986&format=png&auto=webp&s=6ef77ca4b3a9c50a33dbd3ca65eac1ac085a41cc)\n\n# Scheduler/Sampler Testing\n\nBefore touching any training parameters, I tested **every combination of scheduler and sampler** in the K sampler. \\~300 combinations.\n\n**Winner for filmic/grain aesthetic:** `dpmpp_2s_ancestral` \\+ `sgm_uniform`\n\nThis isn't universal, if you want clean digital output or animation, your optimal combo will be different. But for analog texture, this was clearly the best.\n\n[my top picks from testing every scheduler and sampler combo](https://preview.redd.it/4szgr3upelhg1.png?width=2948&format=png&auto=webp&s=068c6308fb791c54a0bf79eb56213424a57d9784)\n\n# Key Parameter Findings\n\n**Network Dimensions**\n\n* Winner: `128, 64, 64, 32` (linear, linear\\_alpha, conv, conv\\_alpha) \\*\\*if you want some secret sauce: something I found across every base model I have trained on is that this combo is universally strong for training style LoRAs of any intent. Many other parameters have effects that are subject to the goal of the user and their taste.\n\nhttps://preview.redd.it/kuigiqhjilhg1.png?width=1988&format=png&auto=webp&s=34d667ceea37b5dc25546005077388222782d095\n\n* Past this = diminishing returns\n* Cranking all to 256 = images totally destroyed (honestly, it looks coo,l and it made me want to make some experimental models that are designed for extreme degradation and I'd like to test further, but for this use case: unusable)\n\n[256 universal rank degredationon the lower right images](https://preview.redd.it/hk96h5fyhlhg1.png?width=2171&format=png&auto=webp&s=0b68b45176697bab38e1f6fb83625fd6eb4bd7a7)\n\n**Decay**\n\n* Lowering decay by 10x from the default improved grain pickup and shadow texture. This is a parameter that had a huge enhancement in the low noise learning of grain patterns, but for illustrative and animation models, I would recommend the opposite, to increase this setting.\n* Highlights bloomed more naturally with visible halation\n* This was one of the biggest improvements\n\n[Decay lowered 5x \\(bottom\\) for the Dev model ](https://preview.redd.it/k2cmrzbfjlhg1.png?width=1179&format=png&auto=webp&s=896a880328cd37138a02c4f8b872500b1991f1d4)\n\n**Lower decay (left):**\n\n* Lifted black point\n* RGB channels bleed into each other\n* Less saturated, more washed-out look\n\n**Higher decay (right):**\n\n* Deeper blacks\n* More channel separation\n* Punchier saturation, more contrast\n\nNeither end is \"correct\". It's about understanding that these parameter changes, though mysterious computer math under the hood, produce measurable differences in the output. The waveform shows it's not placebo; decay has a real, visible effect on black point, channel separation, and saturation.\n\n[Far left - low decay, far right, high decay. ](https://preview.redd.it/1aoxdps5llhg1.png?width=2474&format=png&auto=webp&s=f28221b1f63645e1dae5b1438194fb4250c93987)\n\n**Timestep Type**\n\n* Tested sigmoid, linear, shift\n* Shift gave interesting outputs but defaults (balanced) were better overall for this look. I've noticed when training anime / illustrative LoRAs that training with Shift increased the prevalence of the brush strokes and medium-level noise learning.\n\nhttps://preview.redd.it/hv6a7yu1mlhg1.png?width=1959&format=png&auto=webp&s=c09065ac88ffbfe91eed0d09933c4d7e1116db68\n\n**FP32 vs FP8 Training**\n\n* For Flux 2 Klein specifically, FP8 training produced better film grain texture\n* Non-FP8 had better subject fidelity but the texture looked neural-network-generated rather than film-like\n* This might be model-specific, on others I found training with the dtype of fp32 gave a noticeably higher fidelity. (training time increases nearly 10x, though, it's often not worth the squeeze to test until the final iterations of the fine-tune)\n\n# Step Count\n\nAll parameter tests run at 3K steps (good enough to see if the config is working without burning compute).\n\nOnce I found a winning config (v47), I tested epochs from 1K → 10K+ steps:\n\n* **3K steps:** More optical artifacts, lens flares, aggressive degradation\n* **7K steps (dev winner):** Better subject retention while keeping grain, bloom, tinted shadows\n* Past 7k steps was a noticeable spike in degradation to the point of anatomical distortion that was not desirable.\n\nI'm releasing both\n\n[testing v47 of the dev model 1-10k steps at epochs every 250 steps. \\(1-8k depicted here\\)](https://preview.redd.it/2qqr23fxglhg1.png?width=2235&format=png&auto=webp&s=be178f4573c92d53cfb536b69b418e4585b0130e)\n\nIf you care to try any of the modes:\n\n**Recommended settings:**\n\n* Trigger word: `HerbstPhoto`\n* LoRA strength: 0.73 sweet spot (0.4-0.75 balanced, 0.8-1.0 max texture)\n* Sampler: `dpmpp_2s_ancestral` \\+ `sgm_uniform`\n* Resolution: up to 2K\n\nHappy to answer questions about methodology or specific parameter choices.",
      "is_original_content": false,
      "link_flair_text": "FLUX 2",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qwasra/50_flux_2_klein_lora_training_runs_dev_and_klein/",
      "domain": "self.FluxAI",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qsib7a",
      "title": "Which base model should I use the quants of, Klein or Dev?",
      "subreddit": "FluxAI",
      "url": "https://www.reddit.com/r/FluxAI/comments/1qsib7a/which_base_model_should_i_use_the_quants_of_klein/",
      "author": "ts4m8r",
      "created_utc": "2026-01-31 23:48:55",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I'm on a 3060 12GB, 32GB RAM. unsloth's Flux.2 Klein 9B Q8\\_0 is 9.98GB. Flux.2 Dev has a Q4\\_K\\_M for 20.1 GB. Considering that Klein is already a distilled model, does \"distilling it twice\" by making a quant of it cause enough degradation that I'd be better off just using a different base model? Would the Dev Q4 be too much for my system to handle practically? Am I better off just going with a 4B model for speed generation and then i2i with a higher model for quality later?",
      "is_original_content": false,
      "link_flair_text": "Question / Help",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qsib7a/which_base_model_should_i_use_the_quants_of_klein/",
      "domain": "self.FluxAI",
      "is_self": true,
      "comments": [
        {
          "id": "o2x4pqp",
          "author": "benkei_sudo",
          "text": "Use Flux.2 Klein 9B if you want speed and quality.  \nUse Flux.2 Dev if you want to use LoRA.\n\nQuantization and distillation are two different things:\n\n* Quantization is converting a model to a lower dtype, compressing its size.\n* In distillation, a model is trained in fewer steps. In Klein's case, this makes it capable of inferring in 4 steps (non-distilled needs 50 steps), but the size remains the same.",
          "score": 1,
          "created_utc": "2026-02-01 04:59:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qt6tcp",
      "title": "Create a consistent character animation sprite | No Workflow",
      "subreddit": "FluxAI",
      "url": "https://www.reddit.com/gallery/1qt4ku4",
      "author": "TawusGame",
      "created_utc": "2026-02-01 18:44:00",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qt6tcp/create_a_consistent_character_animation_sprite_no/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o35ia7c",
          "author": "TawusGame",
          "text": "I published a post explaining how the workflow works. Since I don’t know how to use sites like Civit AI, I uploaded it to the [Itch.io](http://itch.io/) site.\n\n[https://tawusgames.itch.io/ai-gen-sprite-tutorial](https://tawusgames.itch.io/ai-gen-sprite-tutorial)",
          "score": 1,
          "created_utc": "2026-02-02 13:36:29",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsnd87",
      "title": "Classic Snowman in Winter Landscape",
      "subreddit": "FluxAI",
      "url": "https://i.redd.it/15djf0myxsgg1.png",
      "author": "ai_scribbles",
      "created_utc": "2026-02-01 03:33:09",
      "score": 2,
      "num_comments": 0,
      "upvote_ratio": 0.67,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qsnd87/classic_snowman_in_winter_landscape/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qq4hfn",
      "title": "Historical Storyboards of Key Events Created with FLUX 2",
      "subreddit": "FluxAI",
      "url": "https://www.reddit.com/gallery/1qq4gue",
      "author": "Substantial-Fee-3910",
      "created_utc": "2026-01-29 09:56:02",
      "score": 2,
      "num_comments": 0,
      "upvote_ratio": 0.67,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "FLUX 2",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qq4hfn/historical_storyboards_of_key_events_created_with/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": []
    }
  ]
}