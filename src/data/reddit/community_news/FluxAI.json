{
  "metadata": {
    "last_updated": "2026-02-07 02:58:05",
    "time_filter": "week",
    "subreddit": "FluxAI",
    "total_items": 7,
    "total_comments": 8,
    "file_size_bytes": 21745
  },
  "items": [
    {
      "id": "1qwasra",
      "title": "50+ Flux 2 Klein LoRA training runs (Dev and Klein) to see what config parameters actually matter [Research + Video]",
      "subreddit": "FluxAI",
      "url": "https://www.reddit.com/r/FluxAI/comments/1qwasra/50_flux_2_klein_lora_training_runs_dev_and_klein/",
      "author": "Significant-Scar2591",
      "created_utc": "2026-02-05 03:54:57",
      "score": 34,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "https://preview.redd.it/lpreh1bhdlhg1.png?width=1700&format=png&auto=webp&s=166bc9249cdb1172c01147b1a3a88d813d6ba5db\n\n**Full video here**: [https://youtu.be/Nt2yXplkrVc](https://youtu.be/Nt2yXplkrVc)\n\nI just finished a systematic training study for Flux 2 Klein and wanted to share what I learned. The goal was to train an analog film aesthetic LoRA (grain, halation, optical artifacts, low-latitude contrast)\n\nI came out with two versions of the Klein models I was training Flux 2 Klein, a 3K step version with more artifacts/flares and a 7K step version with better subject fidelity. As well as a version for the dev model. Free on Civitai. But the interesting part is the research.\n\n[https://civitai.com/models/691668/herbst-photo-analog-film](https://civitai.com/models/691668/herbst-photo-analog-film)\n\n# Methodology\n\n50+  training runs using AI Toolkit, changing **one parameter per run** to get clean A/B comparisons. All tests used the same dataset (my own analog photography) with simple captions. Most of the tests were conducted with the Dev model, though when I mirrored the configs for Klein-9b ,I observed the same patterns. I tested on thousands of image generations not covered in this reasearch as I will only touch on what I found was the most noteworthy. \\*I'd also like to mention that the training configs are only 1 of three parts of this process. The training data is the most important; I won't cover that here, as well as the sampling settings when using the model\n\nFor each test, I generated two images:\n\n1. A prompt pulled directly from training data (can the model recreate what it learned?)\n2. \"Dog on a log\" ,tokens that don't exist anywhere in the dataset (can the model transfer style to new prompts?)\n\nThe second test is more important. If your LoRA only works on prompts similar to training data, it's not actually learning style, it's memorizing.\n\n[Example of the two prompts A\\/B testing format. Top row is the default AI toolkit config, bottom row is A\\/B parameter changes \\(in this case, network dimention ratio variation\\)](https://preview.redd.it/0n698a73elhg1.png?width=1986&format=png&auto=webp&s=6ef77ca4b3a9c50a33dbd3ca65eac1ac085a41cc)\n\n# Scheduler/Sampler Testing\n\nBefore touching any training parameters, I tested **every combination of scheduler and sampler** in the K sampler. \\~300 combinations.\n\n**Winner for filmic/grain aesthetic:** `dpmpp_2s_ancestral` \\+ `sgm_uniform`\n\nThis isn't universal, if you want clean digital output or animation, your optimal combo will be different. But for analog texture, this was clearly the best.\n\n[my top picks from testing every scheduler and sampler combo](https://preview.redd.it/4szgr3upelhg1.png?width=2948&format=png&auto=webp&s=068c6308fb791c54a0bf79eb56213424a57d9784)\n\n# Key Parameter Findings\n\n**Network Dimensions**\n\n* Winner: `128, 64, 64, 32` (linear, linear\\_alpha, conv, conv\\_alpha) \\*\\*if you want some secret sauce: something I found across every base model I have trained on is that this combo is universally strong for training style LoRAs of any intent. Many other parameters have effects that are subject to the goal of the user and their taste.\n\nhttps://preview.redd.it/kuigiqhjilhg1.png?width=1988&format=png&auto=webp&s=34d667ceea37b5dc25546005077388222782d095\n\n* Past this = diminishing returns\n* Cranking all to 256 = images totally destroyed (honestly, it looks coo,l and it made me want to make some experimental models that are designed for extreme degradation and I'd like to test further, but for this use case: unusable)\n\n[256 universal rank degredationon the lower right images](https://preview.redd.it/hk96h5fyhlhg1.png?width=2171&format=png&auto=webp&s=0b68b45176697bab38e1f6fb83625fd6eb4bd7a7)\n\n**Decay**\n\n* Lowering decay by 10x from the default improved grain pickup and shadow texture. This is a parameter that had a huge enhancement in the low noise learning of grain patterns, but for illustrative and animation models, I would recommend the opposite, to increase this setting.\n* Highlights bloomed more naturally with visible halation\n* This was one of the biggest improvements\n\n[Decay lowered 5x \\(bottom\\) for the Dev model ](https://preview.redd.it/k2cmrzbfjlhg1.png?width=1179&format=png&auto=webp&s=896a880328cd37138a02c4f8b872500b1991f1d4)\n\n**Lower decay (left):**\n\n* Lifted black point\n* RGB channels bleed into each other\n* Less saturated, more washed-out look\n\n**Higher decay (right):**\n\n* Deeper blacks\n* More channel separation\n* Punchier saturation, more contrast\n\nNeither end is \"correct\". It's about understanding that these parameter changes, though mysterious computer math under the hood, produce measurable differences in the output. The waveform shows it's not placebo; decay has a real, visible effect on black point, channel separation, and saturation.\n\n[Far left - low decay, far right, high decay. ](https://preview.redd.it/1aoxdps5llhg1.png?width=2474&format=png&auto=webp&s=f28221b1f63645e1dae5b1438194fb4250c93987)\n\n**Timestep Type**\n\n* Tested sigmoid, linear, shift\n* Shift gave interesting outputs but defaults (balanced) were better overall for this look. I've noticed when training anime / illustrative LoRAs that training with Shift increased the prevalence of the brush strokes and medium-level noise learning.\n\nhttps://preview.redd.it/hv6a7yu1mlhg1.png?width=1959&format=png&auto=webp&s=c09065ac88ffbfe91eed0d09933c4d7e1116db68\n\n**FP32 vs FP8 Training**\n\n* For Flux 2 Klein specifically, FP8 training produced better film grain texture\n* Non-FP8 had better subject fidelity but the texture looked neural-network-generated rather than film-like\n* This might be model-specific, on others I found training with the dtype of fp32 gave a noticeably higher fidelity. (training time increases nearly 10x, though, it's often not worth the squeeze to test until the final iterations of the fine-tune)\n\n# Step Count\n\nAll parameter tests run at 3K steps (good enough to see if the config is working without burning compute).\n\nOnce I found a winning config (v47), I tested epochs from 1K → 10K+ steps:\n\n* **3K steps:** More optical artifacts, lens flares, aggressive degradation\n* **7K steps (dev winner):** Better subject retention while keeping grain, bloom, tinted shadows\n* Past 7k steps was a noticeable spike in degradation to the point of anatomical distortion that was not desirable.\n\nI'm releasing both\n\n[testing v47 of the dev model 1-10k steps at epochs every 250 steps. \\(1-8k depicted here\\)](https://preview.redd.it/2qqr23fxglhg1.png?width=2235&format=png&auto=webp&s=be178f4573c92d53cfb536b69b418e4585b0130e)\n\nIf you care to try any of the modes:\n\n**Recommended settings:**\n\n* Trigger word: `HerbstPhoto`\n* LoRA strength: 0.73 sweet spot (0.4-0.75 balanced, 0.8-1.0 max texture)\n* Sampler: `dpmpp_2s_ancestral` \\+ `sgm_uniform`\n* Resolution: up to 2K\n\nHappy to answer questions about methodology or specific parameter choices.",
      "is_original_content": false,
      "link_flair_text": "FLUX 2",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qwasra/50_flux_2_klein_lora_training_runs_dev_and_klein/",
      "domain": "self.FluxAI",
      "is_self": true,
      "comments": [
        {
          "id": "o3ox7ut",
          "author": "addandsubtract",
          "text": "Hey, I happened to skim through the video when you released it last week, and was hoping for a written version, so thanks for this post! Also thanks for putting the time into test the various scheduler's / sampler's. Do you plan on making more of these studies, maybe with a focus on the dataset or the other training parameters?",
          "score": 5,
          "created_utc": "2026-02-05 09:58:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pbyos",
              "author": "Significant-Scar2591",
              "text": "Ah cool :) oh absolutely, I have more in the process now. This was a test about emulation low noise analog details, moving forward I will be doing less emulation and more distillation + deliberate overfitting to create unique aesthetics. This I am much more excited about as the parameters become more of a playground when there isn't such a narrow qualifier of what \"passes and fails\". Training data and theory behind it is more important than this stuff so I wanted to release this first.",
              "score": 4,
              "created_utc": "2026-02-05 12:05:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vcrzm",
          "author": "Agreeable_Effect938",
          "text": "Hello. Nice test.\n\nHere's what you may find interesting: i have tried different approaches to training flux.2 klein, and found one particularly powerful trick. Training on downscaled 768x resolution for like 70% of the first steps, then go up to 1024-1256x resolution for the remaining 30%. it gave amazing style lora, and the only way I'm able to reach simillar quality is with this approach.  \nThe other trick is merging. It applies to all models, and is super powerful, yet no one seem to know about this. Training a bit different style loras and them merging them together gives much better result then the sum of it's parts. It's like SD1.5 merges, they have this amazing almost Flux-like quality, that none of the base models from the merges had by itself. I've made some very known Flux Loras like AntiBlur, and the secret was always to use merging of different variants to get a really polished Lora",
          "score": 3,
          "created_utc": "2026-02-06 08:44:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vtelv",
              "author": "Significant-Scar2591",
              "text": "Hey, thanks for sharing this, very interesting approach. I've used AntiBlur as well as SameFaceFix and am familiar with abit of your work :) I have a few questions if you can help me better understand: \n\nOn the resolution progression:\n\n* Are you preprocessing two separate datasets (one at 768x, one at 1024-1256x) and switching between them during training? Or are you changing the resolution settings in your training config at \\~70% completion while keeping the same source images?\n* If it's config-based: are you using resolution buckets that cap at 768x for the first phase, then expanding the bucket range?\n\nOn the merging:\n\n* When you say \"different style loras\" - are these trained on different subsets of your dataset, or the same images with different parameters? \n* \"a bit different\" - how different? \n* What's your merge method; how and when is this done? ",
              "score": 2,
              "created_utc": "2026-02-06 11:18:34",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3vzzl3",
              "author": "alb5357",
              "text": "I think it'd be great if AI toolkit had a setting for the first epoch to train at 256, then 512, 768, 1024 etc in order.",
              "score": 1,
              "created_utc": "2026-02-06 12:09:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3w6p67",
                  "author": "Significant-Scar2591",
                  "text": "It's a great idea. I'm sure there is a sweet spot on which resolutions to train through which steps. I'll add this to the list of tests to run and share what I find. ",
                  "score": 2,
                  "created_utc": "2026-02-06 12:54:57",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qvraev",
      "title": "Fine tuning flux 2 Klein 9b for unwrapped textures, UV maps",
      "subreddit": "FluxAI",
      "url": "https://www.reddit.com/gallery/1qvraev",
      "author": "Zealousideal-Check77",
      "created_utc": "2026-02-04 15:18:53",
      "score": 31,
      "num_comments": 6,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "LORAS, MODELS, etc  [Fine Tuned]",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qvraev/fine_tuning_flux_2_klein_9b_for_unwrapped/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3mpnay",
          "author": "orph_reup",
          "text": "You can try training an edit lora of paired images 1a The face to unwrap and 1b The unwrapped face.\n\nUse a consistent UV map across all your unwrapped images. \n\nThere are many lora training tools out there, local and cloud. \n\nI think you will get reasonable results.\n\nYou might even do a 2 stage process.\n\n1 unwrap the face.\n2 correct the face to the uv map.\n\nThe second stage of the process would be using the universal uv map as a reference and getting klein to correct your unwrapped uv to the landmarks on the universal map. You might train a lora on this part.\n\nImo Klein is accurate enough to make these experiments worth while.",
          "score": 6,
          "created_utc": "2026-02-05 00:26:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kr3bt",
          "author": "alb5357",
          "text": "I imagine people in the future seeing our unwrapped face textures and thinking it's our weird stylistic art.",
          "score": 2,
          "created_utc": "2026-02-04 18:37:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3laq1h",
              "author": "Zealousideal-Check77",
              "text": "Bro u everywhere xDDD",
              "score": 3,
              "created_utc": "2026-02-04 20:08:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lr6rk",
          "author": "Unis_Torvalds",
          "text": "You can't just give it the UV maps as a dataset.  You need to teach the model how to go from a face photograph to a UV unwrap.\n\nAre there not existing (non-AI) tools or apps which are already set up to do this?",
          "score": 1,
          "created_utc": "2026-02-04 21:26:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n47t7",
              "author": "Zealousideal-Check77",
              "text": "There are tools but we are looking for an AI model to speed up our pipeline",
              "score": 1,
              "created_utc": "2026-02-05 01:49:05",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3nbdwh",
              "author": "Unis_Torvalds",
              "text": "Stitched images (like before/after composites) might work for training. Would require testing of course.",
              "score": 1,
              "created_utc": "2026-02-05 02:30:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxoex2",
      "title": "World of Vulcan. A film made with Flux LoRAs trained on my own analog photography",
      "subreddit": "FluxAI",
      "url": "https://v.redd.it/w5zhdmktuwhg1",
      "author": "Significant-Scar2591",
      "created_utc": "2026-02-06 17:44:16",
      "score": 13,
      "num_comments": 2,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "VIDEO",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qxoex2/world_of_vulcan_a_film_made_with_flux_loras/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3ynw3d",
          "author": "pmp22",
          "text": "Amazing. As a lover of antiquity this tickles me. I have all these wonderful images in my head after reading old greek and roman writers and seeing the art and architecture but nothing on screen has been able to compare to that mental imagry. With these tools we'll be able to make it all come to life in ways never seen before.\nSome if these shots are fantastic, great job.",
          "score": 1,
          "created_utc": "2026-02-06 20:17:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yquc1",
              "author": "Significant-Scar2591",
              "text": "Thanks so much for the nice words. It’s amazing what these tools can visualize :)",
              "score": 1,
              "created_utc": "2026-02-06 20:32:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qu6bph",
      "title": "A sketchpad i am working for comfyui.",
      "subreddit": "FluxAI",
      "url": "https://v.redd.it/u4ttrlcl25hg1",
      "author": "Vivid-Loss9868",
      "created_utc": "2026-02-02 20:17:17",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Not Included",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qu6bph/a_sketchpad_i_am_working_for_comfyui/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qt6tcp",
      "title": "Create a consistent character animation sprite | No Workflow",
      "subreddit": "FluxAI",
      "url": "https://www.reddit.com/gallery/1qt4ku4",
      "author": "TawusGame",
      "created_utc": "2026-02-01 18:44:00",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qt6tcp/create_a_consistent_character_animation_sprite_no/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o35ia7c",
          "author": "TawusGame",
          "text": "I published a post explaining how the workflow works. Since I don’t know how to use sites like Civit AI, I uploaded it to the [Itch.io](http://itch.io/) site.\n\n[https://tawusgames.itch.io/ai-gen-sprite-tutorial](https://tawusgames.itch.io/ai-gen-sprite-tutorial)",
          "score": 1,
          "created_utc": "2026-02-02 13:36:29",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsib7a",
      "title": "Which base model should I use the quants of, Klein or Dev?",
      "subreddit": "FluxAI",
      "url": "https://www.reddit.com/r/FluxAI/comments/1qsib7a/which_base_model_should_i_use_the_quants_of_klein/",
      "author": "ts4m8r",
      "created_utc": "2026-01-31 23:48:55",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 0.86,
      "text": "I'm on a 3060 12GB, 32GB RAM. unsloth's Flux.2 Klein 9B Q8\\_0 is 9.98GB. Flux.2 Dev has a Q4\\_K\\_M for 20.1 GB. Considering that Klein is already a distilled model, does \"distilling it twice\" by making a quant of it cause enough degradation that I'd be better off just using a different base model? Would the Dev Q4 be too much for my system to handle practically? Am I better off just going with a 4B model for speed generation and then i2i with a higher model for quality later?",
      "is_original_content": false,
      "link_flair_text": "Question / Help",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qsib7a/which_base_model_should_i_use_the_quants_of_klein/",
      "domain": "self.FluxAI",
      "is_self": true,
      "comments": [
        {
          "id": "o2x4pqp",
          "author": "benkei_sudo",
          "text": "Use Flux.2 Klein 9B if you want speed and quality.  \nUse Flux.2 Dev if you want to use LoRA.\n\nQuantization and distillation are two different things:\n\n* Quantization is converting a model to a lower dtype, compressing its size.\n* In distillation, a model is trained in fewer steps. In Klein's case, this makes it capable of inferring in 4 steps (non-distilled needs 50 steps), but the size remains the same.",
          "score": 1,
          "created_utc": "2026-02-01 04:59:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsnd87",
      "title": "Classic Snowman in Winter Landscape",
      "subreddit": "FluxAI",
      "url": "https://i.redd.it/15djf0myxsgg1.png",
      "author": "ai_scribbles",
      "created_utc": "2026-02-01 03:33:09",
      "score": 2,
      "num_comments": 0,
      "upvote_ratio": 0.67,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Workflow Included",
      "permalink": "https://reddit.com/r/FluxAI/comments/1qsnd87/classic_snowman_in_winter_landscape/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    }
  ]
}