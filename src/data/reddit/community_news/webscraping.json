{
  "metadata": {
    "last_updated": "2026-02-25 03:09:26",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 13,
    "total_comments": 72,
    "file_size_bytes": 84596
  },
  "items": [
    {
      "id": "1rc9992",
      "title": "Built a stealth Chromium, what site should I try next?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rc9992/built_a_stealth_chromium_what_site_should_i_try/",
      "author": "duracula",
      "created_utc": "2026-02-23 05:46:48",
      "score": 99,
      "num_comments": 72,
      "upvote_ratio": 0.99,
      "text": "Last couple months I was automating browser tasks on sites behind Cloudflare and reCAPTCHA. Tried various tools and solutions out there, everything either broke on the next Chrome update, got detected, or died. I was duct-taping 4 tools together and something broke every other week.\n\nSo I patched Chromium itself at the C++ source level.\n\nCloakBrowser is a small Python wrapper around a custom Chromium binary with 16 fingerprint patches compiled into the source. Not JavaScript injection, not config flags, canvas, WebGL, audio, fonts, GPU strings, all modified before compilation.\n\nResults:\n\n\\- reCAPTCHA v3: 0.9 (server-verified)  \n\\- Cloudflare Turnstile: pass (managed + non-interactive)  \n\\- BrowserScan, FingerprintJS, deviceandbrowserinfo: all clean  \n\\- 14/14 detection tests passed (full results on GitHub)\n\n    pip install cloakbrowser \n    \n    from cloakbrowser import launch \n    browser = launch() \n    page = browser.new_page()\n\nSame Playwright API, binary auto-downloads on first run (\\~200MB, cached).\n\nHow it's different from Patchright/rebrowser: those patch the protocol layer. We patch the browser itself, fingerprint values baked in at compile time. TLS matches because it IS Chrome.\n\nWhat it does NOT do: no proxy rotation, no CAPTCHA solving, no fingerprint randomization per session (yet). It's a browser, not a scraping stack. Bring your own proxies.\n\nLinux x64 right now (even inside docker)  \nmacOS coming soon (let me know if you need it, helps me prioritize).\n\nGitHub: [https://github.com/CloakHQ/CloakBrowser](https://github.com/CloakHQ/CloakBrowser)\n\nPyPI: `pip install cloakbrowser`\n\nIf you have a site that blocks everything, throw it at CloakBrowser and let me know. I like the challenge. Hardest cases welcome.  \nPro tip: pair it with a residential proxy, the browser handles fingerprints, but your IP still matters.\n\nEarly days ‚Äî feedback, bugs, requests are welcome.\n\nEdit:\n\nJust shipped it! `npm install cloakbrowser` ‚Äî supports both Playwright and Puppeteer  \nSame stealth binary, same 14/14 detection results. TypeScript with full types.  \n\n     // Playwright (default)\n      import { launch } from 'cloakbrowser';\n      const browser = await launch();\n      const page = await browser.newPage();\n      await page.goto('https://example.com');\n    \n      // Or with Puppeteer\n      import { launch } from 'cloakbrowser/puppeteer';\n    \n\n ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rc9992/built_a_stealth_chromium_what_site_should_i_try/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6wtohv",
          "author": "Nice-Vermicelli6865",
          "text": "Try completing a survey on a survey platform and see if you are able to complete a survey, they have the toughest anti bot challenges",
          "score": 15,
          "created_utc": "2026-02-23 06:38:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wz8ra",
              "author": "duracula",
              "text": "SurveyMonkey worked, completed test survey (or its less protected?).  \nTypeform worked, the hard part was to find the damn next button.  \nSame for Qualtrics  \nYou have any other survey site in mind?  \n  \n  \n",
              "score": 2,
              "created_utc": "2026-02-23 07:28:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6wziim",
                  "author": "Nice-Vermicelli6865",
                  "text": "I meant on actual websites, like Prime Opinion that are actually guarded",
                  "score": 8,
                  "created_utc": "2026-02-23 07:31:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ws841",
          "author": "RandomPantsAppear",
          "text": "My dude, excellent. \n\nI was literally just pondering how to avoid having to do another chrome extension + python command server, and this fits the bill. \n\nFor every hour I do not have to do that, or code in C++ you are my hero an extra time.",
          "score": 8,
          "created_utc": "2026-02-23 06:25:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xfuvb",
          "author": "juhacz",
          "text": "After a quick test, I see that it causes captcha on the [allegro.pl](http://allegro.pl) website in headless mode.",
          "score": 5,
          "created_utc": "2026-02-23 10:10:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zdssy",
              "author": "duracula",
              "text": "Tested it ‚Äî Allegro uses DataDome, which is one of the more aggressive bot detection services. Took some digging, thanks for the challenge.\n\nFor sites with this level of protection, two things help: a residential proxy (datacenter IPs get flagged by IP reputation) and headed mode via Xvfb (some services detect headless-specific signals).   \nUpdated the README with instructions.  \nAfter implementing this 2 steps, I could enter the site without problems and navigate inside.  \n",
              "score": 5,
              "created_utc": "2026-02-23 17:14:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zcdlw",
          "author": "Pristine_Wind_2304",
          "text": "its giving ai generated text from your replies and your original post but if your tests are right then this seems like an awesome project!! i hope it gets developed further and not abandoned like the other five million chrome binary patches that just cant keep up with the like 100 leaks from every web api",
          "score": 6,
          "created_utc": "2026-02-23 17:07:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zh1o4",
              "author": "duracula",
              "text": "Thanks,   \nYeah I use AI heavily and I'm not gonna pretend otherwise. Without it I couldn't have patched Chromium to this level in a few weeks, it's a massive codebase and a lot of work. AI saved me months.  \n  \nSame with this thread, lots of replies that each deserve a proper testing and answer. I throw in my points and AI helps me write them up in proper English. It's a tool, like everything else.\n\nOn the abandonment concern, totally fair, I've seen the graveyard too. The difference here is this powers production automation I depend on every day. If it breaks, my stuff breaks.   \nI'll keep it going as long as I can ‚Äî but I won't lie, these things take a lot of time and dedication, and life happens.   \n  \nThe code and test results are real though ‚Äî `pip install cloakbrowser` and `python examples/stealth_test.py` hits 6 live detection sites with pass/fail verdicts.  \nThat's what matters.",
              "score": 0,
              "created_utc": "2026-02-23 17:29:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6x120h",
          "author": "theozero",
          "text": "This seems promising. Any plans to make it possible to use via JavaScript / puppeteer?",
          "score": 5,
          "created_utc": "2026-02-23 07:45:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x2o74",
              "author": "duracula",
              "text": "Thanks, adding js/puppeteer support to the roadmap!  \nThe stealth is in the Chromium binary itself, not the Python wrapper, so it's mainly packaging work.\n\nIn the meantime, you can already use it with Puppeteer today ‚Äî just point it at the binary:  \nHaven't tried it, but it should work, same parameters.\n\n    const puppeteer = require('puppeteer-core');\n    \n      const browser = await puppeteer.launch({\n        executablePath: '~/.cloakbrowser/chromium-142.0.7444.175/chrome',\n        args: [\n          '--no-sandbox',\n          '--disable-blink-features=AutomationControlled',\n          '--fingerprint=12345',\n          '--fingerprint-platform=windows',\n          '--fingerprint-hardware-concurrency=8',\n          '--fingerprint-gpu-vendor=NVIDIA Corporation',\n          '--fingerprint-gpu-renderer=NVIDIA GeForce RTX 3070',\n        ],\n        ignoreDefaultArgs: ['--enable-automation'],\n      });\n\nInstall the binary with:\n\n    pip install cloakbrowser && python -c \"from cloakbrowser.download import ensure_binary; ensure_binary()\"\n\nThen use the path above.\n\nAll the stealth passes through ‚Äî same 14/14 detection results.  \nProper npm package coming soon.",
              "score": 4,
              "created_utc": "2026-02-23 08:01:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zqvbo",
                  "author": "theozero",
                  "text": "Nice - I'll give it a go!",
                  "score": 2,
                  "created_utc": "2026-02-23 18:15:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o700sz8",
              "author": "duracula",
              "text": "Just shipped it! `npm install cloakbrowser` ‚Äî supports both Playwright and Puppeteer  \nSame stealth binary, same 14/14 detection results. TypeScript with full types.  \n\n     // Playwright (default)\n      import { launch } from 'cloakbrowser';\n      const browser = await launch();\n      const page = await browser.newPage();\n      await page.goto('https://example.com');\n    \n      // Or with Puppeteer\n      import { launch } from 'cloakbrowser/puppeteer';\n    \n\n Give it a try, let me know if there bugs or problems, have fun.",
              "score": 2,
              "created_utc": "2026-02-23 19:00:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o704xdi",
                  "author": "theozero",
                  "text": "Awesome. I‚Äôve got a docker based setup anyway so won‚Äôt be using npm but I‚Äôm sure others will find it super useful",
                  "score": 2,
                  "created_utc": "2026-02-23 19:19:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6y7giw",
          "author": "Objectdotuser",
          "text": "amazing work, but how could we possibly vet this patched chromium binary?",
          "score": 4,
          "created_utc": "2026-02-23 13:43:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z1iqe",
              "author": "duracula",
              "text": "Fair concern ‚Äî \"trust me bro\" doesn't cut it for a binary you're running.\n\n1. Check the hash ‚Äî every release has a SHA256 digest on GitHub, verify your download matches  \n2. Run it sandboxed ‚Äî Docker, strace, or a VM. Monitor network traffic, syscalls, file access. It's just Chromium ‚Äî it doesn't phone home or do anything a stock Chrome wouldn't  \n3. Scan it ‚Äî upload to VirusTotal, it passes clean  \n4. Read the wrapper ‚Äî fully open source MIT, you can see exactly what flags get passed and how the binary is launched\n\nAt the end of the day, you're in the same position as with any Chromium distribution (Brave, Vivaldi, Arc) ‚Äî you either trust the publisher, audit the behavior, or build your own.",
              "score": 5,
              "created_utc": "2026-02-23 16:17:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xcvu4",
          "author": "EnvironmentSome9274",
          "text": "Try Walmart, their anti bot is very aggressive",
          "score": 4,
          "created_utc": "2026-02-23 09:42:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zp7oy",
              "author": "duracula",
              "text": "Worked.",
              "score": 1,
              "created_utc": "2026-02-23 18:07:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zpg9t",
                  "author": "EnvironmentSome9274",
                  "text": "Can you be a bit more elaborate lol, please? \nHow many products did you try scraping? Did the bot flag you and you rotated or was it completely undetected? \nThank you",
                  "score": 1,
                  "created_utc": "2026-02-23 18:08:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6y7p2b",
          "author": "usamaejazch",
          "text": "how is the stealth browser binary compiled? no source of patches? \n\nit could even have malware, no?",
          "score": 4,
          "created_utc": "2026-02-23 13:45:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z2feu",
              "author": "duracula",
              "text": "You're right that you can't fully verify a closed binary ‚Äî same as Brave, Arc, or any Chromium fork that ships pre-built.\n\nIt's compiled from the official Chromium 142 source tree with our patches applied, using the standard Chromium build toolchain (gn + ninja).   \nSame process any Chromium fork uses. The patches modify fingerprint APIs (canvas, WebGL, audio, fonts, GPU strings).   \nThat's it.  No network changes, no data collection, no telemetry.\n\nWhat you can verify:  \n\\- Run it with strace or Wireshark ‚Äî it behaves identically to stock Chromium except fingerprint values differ  \n\\- Upload to VirusTotal, passes clean  \n\\- The wrapper is fully open source, you can read every line\n\nThe patches aren't open source because they're the core IP of the project. But the binary behavior is fully auditable ‚Äî that's where trust should come from.\n\nIf that's not enough for your threat model, that's completely fair. Not every tool is for everyone.",
              "score": 2,
              "created_utc": "2026-02-23 16:21:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xihbe",
          "author": "Zestyclose_Ad9943",
          "text": "Is it possible to use it on a Node project ?  \nI have a scraping script built on Node with Playwright, I wish I could use your browser instead.",
          "score": 3,
          "created_utc": "2026-02-23 10:35:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70119r",
              "author": "duracula",
              "text": "Just shipped it! `npm install cloakbrowser` ‚Äî supports both Playwright and Puppeteer  \nSame stealth binary, same 14/14 detection results. TypeScript with full types.  \n\n     // Playwright (default)\n      import { launch } from 'cloakbrowser';\n      const browser = await launch();\n      const page = await browser.newPage();\n      await page.goto('https://example.com');\n    \n      // Or with Puppeteer\n      import { launch } from 'cloakbrowser/puppeteer';\n    \n\n Give it a try, let me know if there bugs or problems, have fun.",
              "score": 3,
              "created_utc": "2026-02-23 19:01:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y6er5",
          "author": "Objectdotuser",
          "text": "hmm well companies are wising up to the evasion browsers. how does it handle the chrome versions? If you  have a stagnant chrome version this can be a sign that you are using a controlled browser. does the version update with the typical monthly chrome scheduled releases? how does the update cycle work? i read in the code that the first time it downloads the patched binary and then uses that cached version. that would imply it does not update and this was a one time thing. any ideas on how to handle the update cycles?",
          "score": 3,
          "created_utc": "2026-02-23 13:37:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z0f0r",
              "author": "duracula",
              "text": "Valid point ‚Äî a stale version is a detection signal.\n\nCurrently on Chromium 142, and 145 is being tested now.   \nThe 16 patches port cleanly since they're in isolated files (canvas, WebGL, audio, fonts) ‚Äî porting from 139‚Üí142 took about a day.\n\nThe plan is monthly builds minimum to stay within the normal version window, with CI automation for faster turnaround.   \nDetection services mostly check if you're within the last 2-3 major versions, so the window is forgiving. And a slightly older Chromium with correct TLS + consistent fingerprints still beats any JS injection tool on a current version.\n\nAuto-update in the wrapper is on the roadmap too ‚Äî check for new binary on launch, download in background.",
              "score": 2,
              "created_utc": "2026-02-23 16:12:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y6qqt",
          "author": "AltruisticHunt2941",
          "text": "This library can scrape this site MakeMyTrip.com ?",
          "score": 3,
          "created_utc": "2026-02-23 13:39:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wtjfs",
          "author": "brokedesigner0",
          "text": "Nice",
          "score": 2,
          "created_utc": "2026-02-23 06:36:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wtq87",
          "author": "ry8",
          "text": "This is awesome. I‚Äôll be using this. Thank you!",
          "score": 2,
          "created_utc": "2026-02-23 06:38:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wvqxf",
          "author": "kev_11_1",
          "text": "Zillow",
          "score": 2,
          "created_utc": "2026-02-23 06:56:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wxlta",
              "author": "duracula",
              "text": "Zillow worked without a problem.  \nDidn't scraped it all, but search, listing, apartments pages and photos works.",
              "score": 1,
              "created_utc": "2026-02-23 07:13:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6wzi16",
                  "author": "kev_11_1",
                  "text": "I heard it opens initially and blocks after hefty requests.\n\nBut sure i will give it a try",
                  "score": 2,
                  "created_utc": "2026-02-23 07:30:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o749108",
                  "author": "kev_11_1",
                  "text": "Hey, can you try [bizbuysell.com](http://bizbuysell.com), not the home page, but inside the deal page, where I am facing issues even after using camoufox.",
                  "score": 1,
                  "created_utc": "2026-02-24 11:17:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6wxrk4",
          "author": "dsjflkhs",
          "text": " Interesting",
          "score": 2,
          "created_utc": "2026-02-23 07:14:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xd6qt",
          "author": "deadcoder0904",
          "text": "LinkedIn & X are the hardest, no? Even Substack articles don't allow scraping throws \"Too Many Requests\"\n\nX had Bird CLI by OpenClaw creator that got taken down so that might be easy with cookie.\n\nLinkedIn might be the toughest but also one of the most useful ones.\n\nCool project though.",
          "score": 2,
          "created_utc": "2026-02-23 09:45:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xg5e1",
          "author": "inliberty_financials",
          "text": "Good job man ! Thanks this is what i wanted, I'll test out the solution.",
          "score": 2,
          "created_utc": "2026-02-23 10:13:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xgfpb",
          "author": "jagdish1o1",
          "text": "I will sure give it a try",
          "score": 2,
          "created_utc": "2026-02-23 10:16:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xgl5c",
              "author": "jagdish1o1",
              "text": "No mac is a setback for me",
              "score": 2,
              "created_utc": "2026-02-23 10:17:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xjovl",
          "author": "ChaandyMan",
          "text": "awesome work",
          "score": 2,
          "created_utc": "2026-02-23 10:47:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xrmzi",
          "author": "Double-Journalist-90",
          "text": "Can you create a user account on X",
          "score": 2,
          "created_utc": "2026-02-23 11:56:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70trx4",
              "author": "duracula",
              "text": "Tried it. Signup flow works fine until the Arkose CAPTCHA step ‚Äî it loads but shows an infinite spinner instead of a puzzle.   \nOur stealth passes all X's bot checks, but Arkose runs its own fingerprinting inside a cross-origin iframe. Currently investigating what's flagging us. Will update.",
              "score": 3,
              "created_utc": "2026-02-23 21:19:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o716mpw",
                  "author": "Double-Journalist-90",
                  "text": "Thanks let me know I appreciate the response",
                  "score": 3,
                  "created_utc": "2026-02-23 22:21:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xxtrl",
          "author": "orucreiss",
          "text": "Do you fingerprint webgl gpu?",
          "score": 2,
          "created_utc": "2026-02-23 12:43:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yznld",
              "author": "duracula",
              "text": "Yes ‚Äî GPU vendor and renderer strings are spoofed via CLI flags at launch:\n\n    --fingerprint-gpu-vendor=NVIDIA Corporation\n    --fingerprint-gpu-renderer=NVIDIA GeForce RTX 3070\n\n  These are patched at the C++ level in the binary, so `WebGLRenderingContext.getParameter(UNMASKED_VENDOR_WEBGL)` and `UNMASKED_RENDERER_WEBGL` both return the spoofed values.   \nNot JS injection ‚Äî the actual GPU reporting functions in Chromium are modified.\n\nThe --fingerprint seed also affects canvas and WebGL hash output, so each session produces a unique but consistent fingerprint.",
              "score": 3,
              "created_utc": "2026-02-23 16:08:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6z3fh2",
                  "author": "orucreiss",
                  "text": "This is sooo good",
                  "score": 2,
                  "created_utc": "2026-02-23 16:26:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yoj25",
          "author": "Broad-Apartment4747",
          "text": "Is there a plan to develop Windows x64?",
          "score": 2,
          "created_utc": "2026-02-23 15:15:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z4ou3",
              "author": "duracula",
              "text": "Yes ‚Äî macOS is next, then Windows.   \nThe patches are platform-agnostic C++ so it's the same code, just need to set up the build environments (Xcode for macOS, Visual Studio for Windows).   \nWe're finishing the Chromium 145 build now on Linux, other platforms will follow.   \nEach platform takes 3-6 hours to compile plus testing against all detection services, so it takes a bit ‚Äî but it's coming.\n\nIn the meantime, you can run it today via Docker on Windows/macOS ‚Äî there's a ready-made Dockerfile included:\n\n    docker build -t cloakbrowser  .\n    docker run --rm cloakbrowser python your_script.py",
              "score": 1,
              "created_utc": "2026-02-23 16:32:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6z5ikp",
                  "author": "usamaejazch",
                  "text": "I am sure you didn't do anything risky. But, I am just pointing it out from the perspective of a third party and because of security reasons. \n\nNPM modules get breached all the time. What if an update secretly ships a session logger or something? ",
                  "score": 2,
                  "created_utc": "2026-02-23 16:35:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yp42s",
          "author": "maher_bk",
          "text": "I'll definitly integrate it in my scraping at scale backend (for my ios app) :) However, I am not sure if it is supporting Ubuntu ARM64 ? (Basically ampere servers)",
          "score": 2,
          "created_utc": "2026-02-23 15:18:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z2xt2",
              "author": "duracula",
              "text": "Not yet ‚Äî currently Linux x64 only.   \nNext up is macOS (arm64 + x64), then Windows. ARM64 Linux is further out.  \n  \nFor scraping at scale, x64 servers work out of the box with pip install cloakbrowser.",
              "score": 1,
              "created_utc": "2026-02-23 16:24:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ypfjx",
          "author": "dud380",
          "text": "Very interesting, especially TLS fingerprinting since it can't be done from JS. Also binary-level hiding of CDP internals. ",
          "score": 2,
          "created_utc": "2026-02-23 15:20:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wyc6r",
          "author": "alexp9000",
          "text": "Ticke tmaster?",
          "score": 1,
          "created_utc": "2026-02-23 07:20:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x0bsb",
              "author": "duracula",
              "text": "With residental proxy, concerts discovery listings and item worked, same for starting ordering process of seats selection.  \nWith datacenter ip, blocked as expected from F5.",
              "score": 1,
              "created_utc": "2026-02-23 07:38:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zew8c",
                  "author": "alexp9000",
                  "text": "Amazing! Need Mac plz :)",
                  "score": 1,
                  "created_utc": "2026-02-23 17:19:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6znqac",
          "author": "letopeto",
          "text": "why is it stuck on v142? I think latest chrome is v145? I've found running an out of date version of chrome increases your flag risk",
          "score": 1,
          "created_utc": "2026-02-23 18:00:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o703ewd",
              "author": "duracula",
              "text": "Not stuck ‚Äî 145 is already built and in testing now on linux.   \n142 is still within the normal version window (detection services mostly flag browsers 3+ major versions behind), and I've been running it in production with solid results. But staying current matters, so 145 is the priority.\n\nStar the repo on GitHub to get notified when it drops.",
              "score": 1,
              "created_utc": "2026-02-23 19:12:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zwokr",
          "author": "pierreortega",
          "text": "and tiktok web? does it get captcha on logged out?",
          "score": 1,
          "created_utc": "2026-02-23 18:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zxd7e",
          "author": "PutHot606",
          "text": "Try www.bet365.bet.br",
          "score": 1,
          "created_utc": "2026-02-23 18:44:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71z36x",
          "author": "stratz_ken",
          "text": "Test it on windows server builds. If it works there it would be the first of its kind.  Almost all of them fail under server operating systems.",
          "score": 1,
          "created_utc": "2026-02-24 00:57:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73th1l",
          "author": "boomersruinall",
          "text": "How about indeed? I have been struggling with this particular target. Also chewy",
          "score": 1,
          "created_utc": "2026-02-24 08:53:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73xrhk",
          "author": "Bharath0224",
          "text": "How about [darty.com](http://darty.com) ?",
          "score": 1,
          "created_utc": "2026-02-24 09:34:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7728wy",
              "author": "duracula",
              "text": "Works with CloakBrowser in headed mode + residential proxy.  \nSee the headed mode section in our README for setup.",
              "score": 1,
              "created_utc": "2026-02-24 19:53:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76xr9t",
          "author": "Glittering_Turn_6971",
          "text": "I would love to try it on macOS with Apple silicon.",
          "score": 1,
          "created_utc": "2026-02-24 19:32:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8s88p",
      "title": "I web scraped 72,728 courses from the catalogs of 7 Universities",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r8s88p/i_web_scraped_72728_courses_from_the_catalogs_of/",
      "author": "digital__navigator",
      "created_utc": "2026-02-19 06:50:55",
      "score": 55,
      "num_comments": 12,
      "upvote_ratio": 0.97,
      "text": "Hey everyone,\n\n  \nI used Python Requests and bs4 and sometimes Selenium to write web scraping scripts to web scrape course catalogs.\n\nHeres a small part of Stanfords.\n\nhttps://preview.redd.it/hngsv9w8dekg1.png?width=1856&format=png&auto=webp&s=20a86e74eeb460ddea53188184c4bdf190c5602d\n\nThen created a system to organize the data, put it in databases, query some statistics, and pipeline it into html files which I present on a website I created, called DegreeView.\n\n  \nI am not selling anything on the site. Its ***currently*** just a project.\n\n  \nThis allowed me to get the number of courses and departments in a universities course catalog, the longest and shortest coursename, and sort all departments by how many courses they have revealing the biggest and smallest departments. \n\nhttps://preview.redd.it/c92z8zaeeekg1.png?width=2926&format=png&auto=webp&s=d67f2dcaff5918d0a05d13890883de45669c96b6\n\n  \nAnd create a page for each department in the course catalog where I do something similar:\n\n* Get the number of courses in the department\n* The shortest and longest course name\n* Other things like what percent are upper-division courses, what percent are lower, and what percent are grad courses\n\nhttps://preview.redd.it/d8gz568feekg1.png?width=2890&format=png&auto=webp&s=ff20bf4324107446c81debdd588deb8accff44ad\n\nFor each university I have to write a custom web scraping script but the general structure of every universities catalog I have scraped is similar, so I haven't had to change too much for any one of them. The hardest was the first one I did, UT Austin, and also the real hardest part was creating the system that handles everything once the data is obtained and allows me to work with differentiating data across different universities. \n\n  \nAlso Stanford was hard to scrape cause I had to use Selenium to get Javascript rendered data.\n\n  \nWeb scraping is definitely the backbone of this project so hopefully some of you guys here find this interesting. \n\nThe only reason I kept this project going and didn't give up is because I always had in my mind that it would be very scalable, and I think it is. I just need to do more web scraping.\n\n  \nCheck out the site at [degreeviewsite.com](https://www.degreeviewsite.com/)",
      "is_original_content": false,
      "link_flair_text": "Scaling up üöÄ",
      "permalink": "https://reddit.com/r/webscraping/comments/1r8s88p/i_web_scraped_72728_courses_from_the_catalogs_of/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o67i76g",
          "author": "SignificantMath9703",
          "text": "Check on the website's mobile responsiveness but generally the idea is so superb",
          "score": 5,
          "created_utc": "2026-02-19 08:06:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67jh1a",
              "author": "digital__navigator",
              "text": "LMAOO yeah the home page is not responsive.\n\nIts generally not good practice to make excuses but...college is busy lmao.\n\nI realllyyy wanted to stop delaying a multi university launch.\n\n  \nAt least most other pages of the site should have a basic level of responsiveness.\n\nThanks for your reply üôè",
              "score": 4,
              "created_utc": "2026-02-19 08:18:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o687iyv",
          "author": "divided_capture_bro",
          "text": "Not much to sell since they are the product. Lots of course descriptions are boilerplate too.\n\n\nNow if you also had syllabi and course materials...",
          "score": 3,
          "created_utc": "2026-02-19 11:59:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a8ysl",
          "author": "Jay6719t",
          "text": "That's awesome, I just started learning this stuff not long ago, any tips on how to get out of terminal and into dashboards?",
          "score": 3,
          "created_utc": "2026-02-19 18:29:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6c15h5",
              "author": "digital__navigator",
              "text": "Thanks, and I'd recommend learning basic HTML and CSS and then Jinja.\n\nThere are lots of ways to create websites. Jinja is just a way to place variables onto HTML files. And you can also write some sort of quasi-Python code. I think it's really useful for presenting data on a webpage and is not hard (at least as far as I'm learned but you dont know what you dont know)",
              "score": 1,
              "created_utc": "2026-02-19 23:54:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6dbfk9",
                  "author": "Jay6719t",
                  "text": "Thank you, I'll definitely have to check out jinja and yeah I currently am working on csv and excel files so it'd be nice to have a way to display data better or a more organized way.",
                  "score": 2,
                  "created_utc": "2026-02-20 04:44:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67bh9c",
          "author": "V01DDev",
          "text": "Amazing, saving this for later, good job",
          "score": 2,
          "created_utc": "2026-02-19 07:05:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67cgjb",
              "author": "digital__navigator",
              "text": "Firstly big thanks means a lot.\n\nWell hey if you wanna support you can sign up for a newsletter I made for the site.\n\n[https://degreeview.beehiiv.com/](https://degreeview.beehiiv.com/)\n\nI'll simply be sending out updates on how progress is going, like adding new universities, working on the backend to add more statistics, or making major stylistic changes.\n\nI could be wrong and over ambitious here but I think I should be able to web scrape at least 50 universities which would be like 300,000+ courses on the site, and a potential user base (but by user theyll probably just visit the site once and never come back) of around 500,000 assuming an average university size of 10,000 students. ",
              "score": 1,
              "created_utc": "2026-02-19 07:13:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o68fbf2",
          "author": "Emotional_Still4526",
          "text": "Nice project man I am in the middle of something like that but for PhD will be amazing if we chat sometimes :)",
          "score": 2,
          "created_utc": "2026-02-19 12:54:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6c0npq",
              "author": "digital__navigator",
              "text": "Thanks üôè",
              "score": 1,
              "created_utc": "2026-02-19 23:51:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rcb2p8",
      "title": "I curated a list of 100+ open-source proxy tools",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rcb2p8/i_curated_a_list_of_100_opensource_proxy_tools/",
      "author": "ciokan",
      "created_utc": "2026-02-23 07:31:27",
      "score": 30,
      "num_comments": 1,
      "upvote_ratio": 0.96,
      "text": "Been collecting proxy-related tools for a while and finally organized them into an awesome-list on GitHub. Covers proxy libraries (Python, Go, Node.js), forward/reverse proxies, SOCKS5 servers, Shadowsocks, Trojan, WireGuard, DNS proxies, scraping frameworks with proxy support, and proxy checkers.\n\nTried to include only actively maintained projects. Happy to add anything I missed ‚Äî PRs welcome.\n\n[https://github.com/drsoft-oss/awesome-proxy](https://github.com/drsoft-oss/awesome-proxy)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rcb2p8/i_curated_a_list_of_100_opensource_proxy_tools/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o78qjof",
          "author": "IWillBiteYourFace",
          "text": "Did you delete the repo? The link doesn't work.",
          "score": 0,
          "created_utc": "2026-02-25 00:53:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9qhse",
      "title": "Amazon WAF Solver API",
      "subreddit": "webscraping",
      "url": "https://github.com/jonathanyly/awswaf-solver-api",
      "author": "Jolle_",
      "created_utc": "2026-02-20 09:07:13",
      "score": 17,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1r9qhse/amazon_waf_solver_api/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6e78iw",
          "author": "matty_fu",
          "text": "u/xkiiann how does this differ to the one you published last year?",
          "score": 2,
          "created_utc": "2026-02-20 09:25:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e7qvl",
              "author": "Jolle_",
              "text": "its still the same version so i think its not differing when it comes to solving successfully. However checking with Kians script I do some calculuations a bit different (not using zlib) ",
              "score": 3,
              "created_utc": "2026-02-20 09:30:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6efr70",
                  "author": "xkiiann",
                  "text": "I added captcha solving, not just pow",
                  "score": 1,
                  "created_utc": "2026-02-20 10:43:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r93d69",
      "title": "Need recommendations for web scraping tools",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r93d69/need_recommendations_for_web_scraping_tools/",
      "author": "mustazafi",
      "created_utc": "2026-02-19 16:13:24",
      "score": 12,
      "num_comments": 14,
      "upvote_ratio": 0.89,
      "text": "Hey everyone,\n\nI'm trying to scrape data from a song lyrics website (specifically Turkish/Arabic ilahi/nasheed lyrics from ilahisozleri.net). I reached out to the site owner and got explicit permission to scrape the content for my personal project ‚Äì they said it's fine since the lyrics are mostly public domain or user-contributed, and they're okay with it as long as I don't overload the server.\n\nThe problem is, there's no public API available. I asked if they could provide one or even a data dump, but they replied something like: \"Sorry, I don't have time to set up an API or export the database right now. Just build your own scraper, it's straightforward since the site is simple HTML.\"\n\nI don't have much experience with web scraping, but I know Python and want to do this ethically (with delays, user-agent, etc.). Can you recommend some beginner-friendly tools or libraries?\n\n* Preferably Python-based (like BeautifulSoup, Scrapy, or Selenium if needed for JS).\n* Free/open-source.\n* Tips on handling pagination (site has multiple pages per artist) and extracting lyrics cleanly (they're in tags).\n* Any anti-scrape best practices to avoid issues, even with permission?\n\nGoal is to pull all lyrics into a JSON/CSV for my app. Thanks in advance!\n\n(If anyone has scraped similar sites, share your code snippets or gotchas!)",
      "is_original_content": false,
      "link_flair_text": "AI ‚ú®",
      "permalink": "https://reddit.com/r/webscraping/comments/1r93d69/need_recommendations_for_web_scraping_tools/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6fuyuo",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 16,
          "created_utc": "2026-02-20 15:51:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6vtzzq",
              "author": "edumbao",
              "text": "Thank you, trying this code. ",
              "score": 1,
              "created_utc": "2026-02-23 02:22:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6xhx38",
                  "author": "hasdata_com",
                  "text": "You're welcome, hope it helps",
                  "score": 8,
                  "created_utc": "2026-02-23 10:30:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6c0ig1",
          "author": "ScrapeAlchemist",
          "text": "Hi,\n\nSimple HTML, no JS rendering ‚Äî this is actually the easiest type of scraping to set up.\n\nHere's what I'd do: open the site in DevTools, grab the CSS selectors for the lyrics container, title, artist, and pagination links. Then paste those into ChatGPT/Claude with something like \"write me a Python scraper using requests + BeautifulSoup that extracts lyrics from this structure\" and share the HTML snippet. You'll get a working script in one shot that you can tweak from there.\n\nLLMs are surprisingly good at generating scrapers for static HTML sites. You describe the page structure, it writes the code. For a beginner this is the fastest path ‚Äî you'll learn the patterns as you adjust the output.\n\nA few tips:\n- Use `encoding='utf-8'` everywhere ‚Äî especially if the site has Turkish/Arabic text\n- Wrap each request in `try/except` so one failure doesn't kill the run\n- If pagination is per-artist, scrape the artist index first then loop through each\n\nFor a simple HTML site with permission, `requests` + `bs4` is all you need. No heavy frameworks.\n\nHope this helps!",
          "score": 5,
          "created_utc": "2026-02-19 23:51:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vtwgh",
          "author": "edumbao",
          "text": "Thanks for sharing this, and the comments really helped in my project as well.",
          "score": 3,
          "created_utc": "2026-02-23 02:21:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6aanh0",
          "author": "Jay6719t",
          "text": "Personally ive been using bs4 and requests, going to be going into APIs in a few weeks but if it is mostly html you want to target some parent elements and look inside those for pagination regex is pretty good and use loops to get through the pages, other than that you don't really need to worry about overloading servers for now. Selenium is for automation when theres logins or captchas usually, I'm currently working on a multi site Scraper if you'd like to keep in contact I could send you some notes when it's done on thing's ive learned",
          "score": 2,
          "created_utc": "2026-02-19 18:37:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bdvbw",
          "author": "Lemon_eats_orange",
          "text": "When scraping sites \"ethically\" when you've already been given permission, I'd say the main point is don't bring down the site. If we use ahref's traffic checker the site gets around 31,000 requests per month or around 0.7 requests to a webpage on its site per minute. This would mean depending on the site it would either be okay to spam 1000 requests per minute if they have a good backend or that could seem like a DDOS attack. Use your judgement and make a reasonable number of requests if using your own IP and know that perhaps different times have more traffic than other times.\n\nFor Scraping the site itself, you could use something simple like the following:  \n\\- Python Requests Library for making http requests.\n\n\\- Beautifulsoup for parsing the resulting html files.\n\nIf you need to get a list of all pages, the site has a sitemap. In the sitemap there are a list of links which lead to other links, though it does not give pagination: [https://ilahisozleri.net/sitemap.html](https://ilahisozleri.net/sitemap.html). You could create a script first to collect all pages by:\n\n\\- Making a request to the sitemap and then collecting each page. In the html, which you can find from clicking F12 you can see that each link is contained within a <a> element contained within a <td> element. You can then parse through and get each link and then use requests to visit these URLs and beautifulsoup to parse it.\n\nYou'll need to study the sitemap to see if it has everything you need.\n\nFor pagination, it seems the site may be quite simple. For example on [https://ilahisozleri.net/sanatci/hasan-dursun](https://ilahisozleri.net/sanatci/hasan-dursun) it seems that all pages are in elements underneath the class \"pages\". You can parse the URLs from the href for the next page, go to the next page, and then keep going until there are no more pages left.\n\nThe site itself doesn't seem to have many anti-bot defenses (no cloudflare, akamai, etc), and you can get to the site without the use of Javascript. If anything I think the only potential issue could be rate limiting or IP bans if you go crazy on the site with your own IP --> you can also purchase proxies to make your requests from multiple places.\n\nI'd say first make a script to make to an artist page using requests library and ensure you can collect the correct data from it by parsing it (URL's to song pages). After that make the script to collect the lyrics from the page and test it on a few pages. Once you've done that and gotten parsing down for these and getting the pages you can proceed to create the logic to collect from multiple pages.\n\nEdit: This is but one of many ways to go about it but there is so much more. If you don't know how to find elements in the developer tools you'll need to in order to correctly use any html parser to find the correct data.",
          "score": 1,
          "created_utc": "2026-02-19 21:47:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6egnnt",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-20 10:51:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ekl31",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-20 11:24:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6hrzxs",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-20 21:14:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ikvb1",
              "author": "webscraping-ModTeam",
              "text": "ü™ß Please review the sub rules üëâ",
              "score": 1,
              "created_utc": "2026-02-20 23:47:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6e1szh",
          "author": "jagdish1o1",
          "text": "Nothing is better than scrapy when it comes to large projects. It is a dedicated framework for web scraping only and since it‚Äôs a static website, scrapy is the best fit here. Use residential proxies and you‚Äôre good to go.",
          "score": -1,
          "created_utc": "2026-02-20 08:33:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rasgub",
      "title": "Can you scrape flight data ?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rasgub/can_you_scrape_flight_data/",
      "author": "malwaregeeek",
      "created_utc": "2026-02-21 14:28:53",
      "score": 5,
      "num_comments": 17,
      "upvote_ratio": 0.7,
      "text": "I am building a flight booking app. I was using Amadeus but they are deprecating next month and I am thinking of alternatives. Is there a way to scrape flight results ?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rasgub/can_you_scrape_flight_data/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6nbvo5",
          "author": "sohailglt",
          "text": "If you are building a booking app, you need to use the official API of Global Distribution Systems (GDS) like Amadeus, Sabre, and Travelport.",
          "score": 13,
          "created_utc": "2026-02-21 19:11:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6o2le2",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-21 21:31:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6pfqlc",
                  "author": "webscraping-ModTeam",
                  "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
                  "score": 1,
                  "created_utc": "2026-02-22 02:26:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6n1att",
          "author": "lp435",
          "text": "There should be APIs for this. No need to scrape",
          "score": 9,
          "created_utc": "2026-02-21 18:19:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n20q2",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-21 18:23:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6pfvox",
                  "author": "webscraping-ModTeam",
                  "text": "üëî Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
                  "score": 1,
                  "created_utc": "2026-02-22 02:27:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ny3uc",
          "author": "FranBattan",
          "text": "We are scraping booking.com, expedia.com, despegar, and other places like these and I can tell you that flights or hotels websites are hard to scrape. Also because of the cadence and the number of combinations you need to perform to get the results, it multiplies the amount of data you will get. Also, every website has anti bot systems like datadome and imperva.",
          "score": 4,
          "created_utc": "2026-02-21 21:08:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6o332h",
              "author": "malwaregeeek",
              "text": "Gotcha! What‚Äôs the latency of these endpoints you use to scrape data? Also what are the legal implications of scraping an OTA?",
              "score": 1,
              "created_utc": "2026-02-21 21:34:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6y1hda",
                  "author": "Comfortable_Camp9744",
                  "text": "In the US if its public without login, you can scrape no problem",
                  "score": 1,
                  "created_utc": "2026-02-23 13:07:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6nj3dq",
          "author": "baxet",
          "text": "there‚Äôs a whole industry for this (source: it‚Äôs my daily dev job). Although there are more and more legit API connections being offered every day, but mostly b2b so unless you‚Äôre serious about the business don‚Äôt get too excited",
          "score": 4,
          "created_utc": "2026-02-21 19:48:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tkyte",
          "author": "iamwasim094",
          "text": "You can build yourself if you have the technical know how, else there are plenty of APIs provider you can may check",
          "score": 1,
          "created_utc": "2026-02-22 19:04:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ttp9x",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-22 19:47:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6vuvio",
                  "author": "webscraping-ModTeam",
                  "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
                  "score": 1,
                  "created_utc": "2026-02-23 02:27:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yo5l5",
          "author": "New_Palpitation3128",
          "text": "I also need to scrape airline data, i used Amadeus but didn‚Äôt receive the production keys so it doesn‚Äôt give me the required data. Can someone guide me please. I am looking for another platform now.",
          "score": 1,
          "created_utc": "2026-02-23 15:13:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yo84t",
              "author": "New_Palpitation3128",
              "text": "It‚Äôs for my research about airline prices.",
              "score": 1,
              "created_utc": "2026-02-23 15:14:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9s0j9",
      "title": "Brave search does not scrape linkedin posts like google search?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r9s0j9/brave_search_does_not_scrape_linkedin_posts_like/",
      "author": "bravelogitex",
      "created_utc": "2026-02-20 10:38:01",
      "score": 5,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "I have this js code to show the latest hiring posts from Linkedin, but nothing shows up:\n\n`const url = new URL(\"https://api.search.brave.com/res/v1/web/search\");`  \n`url.searchParams.append(\"q\", \"hiring site:linkedin.com/posts\");`  \n`url.searchParams.append(\"freshness\", \"pw\");`  \n`url.searchParams.append(\"operators\", \"true\");`\n\nResults returned are 0. If I change the second line to just`hiring site:linkedin.com,` then a few results show but they are only linkedin profiles, not posts.  \n  \nWhat gives? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r9s0j9/brave_search_does_not_scrape_linkedin_posts_like/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6f1sr6",
          "author": "Quantum_Rage",
          "text": "LinkedIn is a hard target to scrape and Google might have special arrangements to access the data without scraping.",
          "score": 3,
          "created_utc": "2026-02-20 13:22:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i7lmq",
              "author": "cyber_scraper",
              "text": "well, integration with Google search console can immediately send signals about new pages while Brave  doesn't have such option",
              "score": 1,
              "created_utc": "2026-02-20 22:33:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6i9i5f",
              "author": "bravelogitex",
              "text": "Social media becoming walled gardens...",
              "score": 1,
              "created_utc": "2026-02-20 22:43:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rcxwo5",
      "title": "Scrape transcripts from Spotify",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rcxwo5/scrape_transcripts_from_spotify/",
      "author": "AnglePast1245",
      "created_utc": "2026-02-23 23:36:52",
      "score": 5,
      "num_comments": 4,
      "upvote_ratio": 0.86,
      "text": "Does anyone know a reliable way (via API, browser extension, script, or tool) to scrape or export full episode transcripts from Spotify podcasts?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rcxwo5/scrape_transcripts_from_spotify/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o73i0yn",
          "author": "statlawnna",
          "text": "This would be interesting also for me ",
          "score": 3,
          "created_utc": "2026-02-24 07:06:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73tadx",
          "author": "boomersruinall",
          "text": "Following, as I am also looking for answers regarding transcripts",
          "score": 3,
          "created_utc": "2026-02-24 08:51:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7577nl",
          "author": "yyavuz",
          "text": "In the same boat",
          "score": 1,
          "created_utc": "2026-02-24 14:49:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra7lkh",
      "title": "Avoiding Recaptcha Enterprise v3",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1ra7lkh/avoiding_recaptcha_enterprise_v3/",
      "author": "saadcarnot",
      "created_utc": "2026-02-20 21:09:08",
      "score": 4,
      "num_comments": 14,
      "upvote_ratio": 0.76,
      "text": "I am working on automating a time critical ticket booking however my last click brings up captcha. It is v3 Enterprise recaptcha. \n\nI can use solvers but it's time critical and i need to complete within 1second . Any ideas? I have tried patchright, playwright, selenium, pydoll. \n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1ra7lkh/avoiding_recaptcha_enterprise_v3/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6j6lwt",
          "author": "RandomPantsAppear",
          "text": "Important questions:¬†\n\n* when you do this manually does the captcha show up?\n\n* when you do this automated, using your real browser cookies does the captcha show up?\n",
          "score": 3,
          "created_utc": "2026-02-21 01:58:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kmpys",
              "author": "saadcarnot",
              "text": "Manually captcha never comes but running on automation even with stealth modes everytime it gets triggered. I tried using existing profile got captcha as well",
              "score": 1,
              "created_utc": "2026-02-21 08:45:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6mvp9x",
                  "author": "RandomPantsAppear",
                  "text": "Ok this is oddly enough great news. From this we can establish that your issue isn‚Äôt identity or identity length or reputation. \n\nYour issue is most likely the browser driver. Most aren‚Äôt perfect. I would load up an automated window, time.sleep(9999999), and then start visiting ‚ÄúWebdriver test‚Äù,‚Äùbot test‚Äù, etc. Keep looking until you fail one. \n\nSome of these are more/less verbose about the actual issue but it‚Äôs normally not too hard to pull the JavaScript and figure out why you‚Äôre failing.\n\nAs an alternative: if you‚Äôre running headless, stop and test it that way. Some anti bot platforms can detect headless. Use xvfb instead.",
                  "score": 2,
                  "created_utc": "2026-02-21 17:52:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6kk6sd",
          "author": "forklingo",
          "text": "if it is recaptcha enterprise v3 and especially on a ticketing site, they are likely scoring behavior over time and not just that final click, so trying to ‚Äúsolve‚Äù it in under a second probably won‚Äôt be reliable. most of the detection happens through fingerprinting, traffic patterns, and account reputation long before the captcha shows up. honestly for time critical booking your best bet is optimizing legit flows like having account, payment, and autofill ready rather than trying to bypass the captcha layer, since they are specifically designed to stop automation.",
          "score": 3,
          "created_utc": "2026-02-21 08:20:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ln6z9",
              "author": "saadcarnot",
              "text": "On the same platform, out competitors have heavy automations running, making me think it is somehow possible. I have tried creating a complete like browsing, clicks, random mouse movements, scrolls and waits on pages. Still when ran using script captcha comes and when using my regular browser manually it works.",
              "score": 1,
              "created_utc": "2026-02-21 13:57:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6pzhbk",
                  "author": "forklingo",
                  "text": "if it works manually but not via script, it‚Äôs almost always fingerprinting or browser environment differences. enterprise v3 scores the whole session, not just the last click. your automation probably looks clean to you but still ‚Äúoff‚Äù compared to a real user profile. you might need to focus less on fake interactions and more on matching a real browser context exactly.",
                  "score": 2,
                  "created_utc": "2026-02-22 04:42:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6sbrct",
          "author": "irrisolto",
          "text": "Recaptcha tokens last 2 minutes, farm them before even spawning the browser and then use them. No solver is gonna solve recaptcha v3 enterprise within 1s",
          "score": 2,
          "created_utc": "2026-02-22 15:39:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6suhvz",
              "author": "saadcarnot",
              "text": "That's a good point, how do I use them? Is there any place where we inject it in webdriver?",
              "score": 1,
              "created_utc": "2026-02-22 17:02:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6tbbg1",
          "author": "Puzzleheaded_Row3877",
          "text": "You need to farm the tokens before the ticketing starts ,I think they have an expiry of 1 minute ,then intercept the recapcha request on the browser, block it and inject your farmed token.",
          "score": 2,
          "created_utc": "2026-02-22 18:19:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6tqei4",
              "author": "saadcarnot",
              "text": "I will give it a shot, which automation framework gives best request interception capabilities? Currently I have playwright setup",
              "score": 1,
              "created_utc": "2026-02-22 19:30:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6u9tvu",
          "author": "ScrapeAlchemist",
          "text": "Hi,\n\nSince it works manually but triggers on automation, your issue is likely browser fingerprinting rather than the CAPTCHA itself. reCAPTCHA Enterprise v3 scores silently in the background ‚Äî by the time you see the challenge, you've already failed the score check.\n\nA few things to look at:\n\n- **Session warming**: Log in and browse the site normally before the critical click. v3 scores your entire session, not just the final action\n- **Fingerprint consistency**: Make sure your timezone, WebGL, canvas, and navigator properties match a real browser profile. Tools like patchright help but aren't perfect out of the box\n- **Cookie persistence**: Reuse cookies from a manual session where you passed. If you already have a good score tied to that session, the final click won't trigger a challenge\n\nThe 1-second window is realistic if the scoring already happened upstream and you're clean going in.\n\nI hope this helps",
          "score": 2,
          "created_utc": "2026-02-22 21:08:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6uaj6r",
              "author": "saadcarnot",
              "text": "Thank you for these details! On cookie reuse, would running using existing profile use the cookies?",
              "score": 1,
              "created_utc": "2026-02-22 21:12:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rbfrym",
      "title": "Any residential proxy providers with a free trial/credits?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rbfrym/any_residential_proxy_providers_with_a_free/",
      "author": "Srigbok_",
      "created_utc": "2026-02-22 07:41:58",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Does anyone know reputable residential proxy providers that offer a free trial or small free credits (enough for a quick test)?",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1rbfrym/any_residential_proxy_providers_with_a_free/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6qltlf",
          "author": "Drakula2k",
          "text": "Most of scraping APIs with residential proxy support offer free plans, try WebScraping.AI. There is also a proxy mode allowing you to connect to it as an HTTP proxy.",
          "score": 1,
          "created_utc": "2026-02-22 07:55:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qpk6r",
          "author": "Bitter_Caramel305",
          "text": "Thordata gives you 1Mb for testing their residential proxies, my one ran out while I was testing weather the IPs actually rotate or not with [httpbin.org/ip](http://httpbin.org/ip), :(",
          "score": 1,
          "created_utc": "2026-02-22 08:30:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra4xec",
      "title": "Newbie Looking For Advice",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1ra4xec/newbie_looking_for_advice/",
      "author": "PaintPractical4321",
      "created_utc": "2026-02-20 19:27:33",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 0.81,
      "text": "Hello all, I was looking for some advice... \n\n(for those who just want me to get straight to the point... im looking for a way of finding the email addresses of businesses such as pubs, bars, restaurants... I figured a scraper and google maps would be the way to do this)\n\nI have been experimenting with epoxy resin and ended up making glass/bottle art after seeing what others have done on social media. One person/account in particular which drew my attention is based in Germany and uses Etsy as one of the platforms to advertise and sell. I was surprised at how much they seem to have them listed for, and knowing how much its costing me to make each one on average, it appears their is some good profit to be made. I've been doing this as a hobby more than anything up to now but It would be great if I could sell some. I have a couple listed on ebay but I wanted to try being proactive and approaching businesses which would be the most likely to buy this sort of thing... bars, pubs, restaurants. I'm looking for a way to find the email addresses for these...i assume a scraper and google maps would be the way to do this.\n\nI found a couple of free chrome extensions but neither scrape emails as part of the free version. Does anybody know of any free extensions/software... that will?\n\n  \nThanks!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1ra4xec/newbie_looking_for_advice/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6nlxhi",
          "author": "kargie121",
          "text": "There‚Äôs a way you can scrape the emails but it depends on the location/city",
          "score": 1,
          "created_utc": "2026-02-21 20:03:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ua7jz",
          "author": "ScrapeAlchemist",
          "text": "Hi,\n\nGoogle Maps API is the right call for this. Since you're new to scraping, I'd suggest using an LLM (like ChatGPT or Claude) to help you build the whole setup step by step.\n\nThe approach:\n1. **Google Maps API** ‚Äî search for pubs/bars/restaurants by location. Free tier covers small-scale searches. Ask the LLM to write you a Python script that pulls business names, addresses, phone numbers, and website URLs.\n2. **Website scraping** ‚Äî have the LLM generate a second script that visits each business website and extracts email addresses from contact pages, mailto: links, etc.\n\nYou don't need to know how to code ‚Äî just describe what you want to the LLM and it'll generate working scripts you can run. It can also help you set up Python on your machine if you haven't already.\n\nSkip the Chrome extensions ‚Äî they're limited and unreliable. A simple script gives you full control and costs nothing.\n\nI hope this helps.",
          "score": 1,
          "created_utc": "2026-02-22 21:10:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdfm7a",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rdfm7a/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-02-24 13:01:04",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.81,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levels‚Äîwhether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) üå±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1rdfm7a/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o763nqx",
          "author": "jagdish1o1",
          "text": "Hey! I‚Äôm not sure what category I fall into when it comes to scraping, but I‚Äôve done plenty of scraping projects over the years and have gained solid knowledge of how to scrape various websites.\n\nHere are some tips from my side:\n\n1. Try to avoid using browsers for scraping unless it‚Äôs absolutely necessary. Even if you have to use one, capture the request headers from the browser and try to mimic the request using those headers instead.\n2. Use residential rotating proxies for recurring scraping tasks, especially when you need to scrape a site on a daily basis.\n3. Consider integrating AI into your HTML parsing. This can save you a lot of maintenance work in the long run. Just make sure to enforce structured output.\n4. Write modular code instead of putting everything into one or two scripts. This will save you time on future projects and make maintenance easier.\n5. Use exponential backoff instead of simple retries. Even better, use exponential backoff with jitter. This helps reduce bottlenecks and handle rate limiting more effectively.\n\nIf you already have strong scraping knowledge, consider building APIs for popular websites and selling them on RapidAPI.\n\nThese are the points that come to mind right now. I‚Äôll add more in a reply if I think of anything else.\n\nPeace ‚úåÔ∏è",
          "score": 1,
          "created_utc": "2026-02-24 17:17:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdbvik",
      "title": "What's working for you with proxy rotation?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rdbvik/whats_working_for_you_with_proxy_rotation/",
      "author": "Forsaken-Bobcat4065",
      "created_utc": "2026-02-24 09:40:07",
      "score": 3,
      "num_comments": 10,
      "upvote_ratio": 0.81,
      "text": "¬†I‚Äôve been down the scraping rabbit hole lately and honestly‚Ä¶ I‚Äôm spending way too much time dealing with rate limits, CAPTCHAs, random blocks, and instability. \n\nWhat are people using these days to manage proxies and keep things running smoothly? Rotating residential or datacenter proxies, specific libraries, browser automation, or a mix? \n\nI‚Äôm just looking for something that actually works in real-world projects without becoming a full-time maintenance job. Any tools or setups that have made things more stable and hands-off?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rdbvik/whats_working_for_you_with_proxy_rotation/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o75520e",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-02-24 14:38:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75axnp",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-02-24 15:07:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75xjf7",
          "author": "Gold_Emphasis1325",
          "text": "The working environment to operate like this is getting crushed. Enough ankle biters creating bots and \"Agents\" caused big players to finally invest the time into closing known gaps that allowed more TOS violations and scraping. It's only going to get more difficult to get away with things. Any long-term plan relying on scraping or having \"robust scraping\" is likely not a plan at all.",
          "score": 2,
          "created_utc": "2026-02-24 16:50:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74orx1",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-24 13:09:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74qoen",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-02-24 13:20:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75m3xf",
          "author": "VonDenBerg",
          "text": "GOOD providers, not trash. ",
          "score": 1,
          "created_utc": "2026-02-24 15:59:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77345g",
          "author": "Alone-Rub-4418",
          "text": "I feel you on the rabbit hole thing. I've had decent luck with a mix of residential proxies and a simple backoff/retry strategy in my scripts. Honestly, the biggest game changer for me was just accepting that some blocks are inevitable and building my scraper to fail gracefully and pick up where it left off",
          "score": 1,
          "created_utc": "2026-02-24 19:57:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}