{
  "metadata": {
    "last_updated": "2026-02-14 16:49:57",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 20,
    "total_comments": 81,
    "file_size_bytes": 105938
  },
  "items": [
    {
      "id": "1r0p4j2",
      "title": "Scrape Zillow Data - 2026",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r0p4j2/scrape_zillow_data_2026/",
      "author": "retardracer",
      "created_utc": "2026-02-10 02:56:50",
      "score": 68,
      "num_comments": 28,
      "upvote_ratio": 0.93,
      "text": "I recently accidentally found out a whitelisted way to scrape Zillow.\n\nI took the time to properly file a bug with them and let them resolve.\n\nThey told me to kick rocks, that they did not have enough information to reproduce.\n\nSo I told them I would come here and post less then what I gave them and see if other people can reproduce :)\n\nZillow has whitelisted a section of google servers. Anyone can utilize google sheets (or any google app with gapps scripting or similar), to fetch/scrape and pull into your sheets, from there you can do anything with your CSV/sheet from there.\n\nMy example was a spreadsheet I made to track houses during house hunting. Before I turned it into a cool proof of concept web app(that doesnt use Zillow)\n\nGod speed!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r0p4j2/scrape_zillow_data_2026/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4k22gz",
          "author": "ContributionEasy6513",
          "text": "Thanks OP\n\nGotta love organisations that whitelist AWS or an entire cloudprovider completely past their firewalls.",
          "score": 26,
          "created_utc": "2026-02-10 03:54:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v6ljt",
              "author": "Transformand",
              "text": "  \nWas anyone able to get a whitelisted VM via GCP? I couldnt, tried a few US regions...\n\nSeems only the IPs running app scripts are whitelisted, but those are limited to 6 minute runs, few runs per day\n\n",
              "score": 2,
              "created_utc": "2026-02-11 20:50:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4wgf2r",
                  "author": "retardracer",
                  "text": "How are you able to reproduce but bugcrowd couldn't?  :) and the type of details you got are exactly what bug crowd should be doing.  Zillow get a new bug vendor!",
                  "score": 1,
                  "created_utc": "2026-02-12 00:50:32",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4kdmkk",
          "author": "AdministrativeHost15",
          "text": "Same trick as scraping LinkedIn via running scripts on Azure. (Microsoft owns LinkedIn).",
          "score": 22,
          "created_utc": "2026-02-10 05:14:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ndf06",
              "author": "jinef_john",
              "text": "I don't think this works per se, I've gone and set up a script on azure but I get status 999. I'll probably try with internal endpoints but I don't see how this route is different from building a custom scraping pipeline ðŸ¤”",
              "score": 5,
              "created_utc": "2026-02-10 17:31:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4m5std",
              "author": "letopeto",
              "text": "Can you explain more? All you need is a Azure server or is there more to it?",
              "score": 3,
              "created_utc": "2026-02-10 13:58:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n3u65",
                  "author": "AdministrativeHost15",
                  "text": "PM/QA with Microsoft/LinkedIn often run scripts against LinkedIn for various purposes. Internal tools run on Azure so LinkedIn can't block the IP address ranges of the Azure data centers.",
                  "score": 6,
                  "created_utc": "2026-02-10 16:46:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4nhwis",
              "author": "yauhenrachkouski",
              "text": "I tried it also via azure vm and see auth wall. When did you try it last time? Tried with bing bot UA and with real",
              "score": 2,
              "created_utc": "2026-02-10 17:51:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ot5yr",
                  "author": "Transformand",
                  "text": "what Auth wall do you mean?",
                  "score": 1,
                  "created_utc": "2026-02-10 21:29:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4jw1r3",
          "author": "letopeto",
          "text": "how do you use google sheets to do the scraping itself? a bit confused by what you mean",
          "score": 4,
          "created_utc": "2026-02-10 03:16:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jxfp0",
              "author": "retardracer",
              "text": "Google Apps Script (GAS) enables advanced automation, custom functions, and interactive features within Google Sheets using JavaScript. Accessible via Extensions > Apps Script, it automates tasks like data formatting, menu creation, and connecting to other Google services (Gmail, Drive). Key capabilities include custom functions, macros, and API interactions. ",
              "score": 12,
              "created_utc": "2026-02-10 03:24:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qpr1k",
          "author": "SharpRule4025",
          "text": "Classic case of whitelisting an entire cloud provider's IP range. The same pattern exists with LinkedIn and Azure (Microsoft owns LinkedIn, so Azure IPs get trusted differently). Companies whitelist Google/AWS/Azure at the network level for legitimate integrations and forget that anyone can spin up a VM on those same networks.\n\nThe Google Sheets approach works because the request originates from Google's infrastructure, which Zillow trusts implicitly. Same reason Apps Script can hit APIs that block datacenter IPs from other providers.\n\nNot surprised they dismissed the bug report. Fixing it would mean auditing their entire IP whitelist, which probably breaks a dozen internal integrations.",
          "score": 4,
          "created_utc": "2026-02-11 03:59:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l5q6l",
          "author": "copyfringe",
          "text": "Thanks for sharing, OP. If they have whitelisted all of Google, then maybe one could spin up a linux VM in GCP and run a traditional wget based scraper script from there.",
          "score": 3,
          "created_utc": "2026-02-10 09:27:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4k73o3",
          "author": "Forward_Tackle_6487",
          "text": "frekking awesome",
          "score": 2,
          "created_utc": "2026-02-10 04:28:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4k9ojf",
          "author": "Bitter_Caramel305",
          "text": "You know what, you should also inform them to hide their backend API or at least obfuscate it using protobuf or something, as it's laying in open getting used by people like me.\n\nHowever, I don't think they'll be interested in fixing this either as most companies just don't bother to care about such insignificant acts, but rather prefers to focus on shipping new features, no matter how insignificant.\n\nOn a related note, remember their dev team might be to understaff and also their devs probably have a huge backlog.",
          "score": 4,
          "created_utc": "2026-02-10 04:46:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54fwv0",
              "author": "Even-Recording-1886",
              "text": "If they hide their backend api them What is the fallback process to scrape data from it?",
              "score": 1,
              "created_utc": "2026-02-13 06:46:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o54i000",
                  "author": "Bitter_Caramel305",
                  "text": "Uhh..., have you ever heard about browser automation?",
                  "score": 1,
                  "created_utc": "2026-02-13 07:04:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4knv0f",
          "author": "Fit_Temperature680",
          "text": "Thanks for the tip \n\n",
          "score": 1,
          "created_utc": "2026-02-10 06:38:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nh7ri",
          "author": "Even-Recording-1886",
          "text": "Donâ€™t we need proxy?",
          "score": 1,
          "created_utc": "2026-02-10 17:48:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nj62p",
              "author": "PresidentHoaks",
              "text": "Not if youre going through a google system that whitelists Google's IP block. They do this because they want Google to index their pages so theyre the top search result for homes. They would have to get stricter about their whitelist if they wanted to solve this",
              "score": 2,
              "created_utc": "2026-02-10 17:57:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4v6mcu",
          "author": "Transformand",
          "text": "  \nWas anyone able to get a whitelisted VM via GCP? I couldnt, tried a few US regions...\n\nSeems only the IPs running app scripts are whitelisted, but those are limited to 6 minute runs, few runs per day",
          "score": 1,
          "created_utc": "2026-02-11 20:50:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52i8n9",
          "author": "Ok-Taste-5844",
          "text": "I'm still running into 403, even when running it in Apps Script. I'm in Canada, could that be why?\n\nHere's my code. Thanks for help.\n\n    function testZillow() {\n    Â  var response = UrlFetchApp.fetch(\"https://www.zillow.com/homes/for_rent/792680_rid/\", {muteHttpExceptions: true});\n    Â  var code = response.getResponseCode();\n    Â  var html = response.getContentText();\n    Â  \n    Â  Logger.log(\"Response Code: \" + code);\n    Â  Logger.log(\"First 100 chars: \" + html.substring(0, 100));\n    Â  Logger.log(\"Contains 'denied': \" + (html.indexOf(\"denied\") > -1));\n    Â  Logger.log(\"Contains 'zpid': \" + (html.indexOf(\"zpid\") > -1));\n    }",
          "score": 1,
          "created_utc": "2026-02-12 23:10:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kstqw",
          "author": "error1212",
          "text": "That's interesting, thanks for sharing. I'm afraid you may be risking a lawsuit.",
          "score": -3,
          "created_utc": "2026-02-10 07:22:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1tmm1",
      "title": "Web scraping sandbox website - scrapingsandbox.com",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r1tmm1/web_scraping_sandbox_website_scrapingsandboxcom/",
      "author": "vickyrathee",
      "created_utc": "2026-02-11 10:14:57",
      "score": 27,
      "num_comments": 19,
      "upvote_ratio": 0.97,
      "text": "Hey guys,\n\nI have published a scraping sandbox website- [scrapingsandbox.com](https://scrapingsandbox.com/) to learn and practice web scraping using Playwright, Puppeteer etc.\n\nFor now, I just kept 500 products in e-commerce styled list, filter, pagination and details page to practicing product website scraping, later I will be adding more pages for different scenarios.\n\nIt's [open source on Github](https://github.com/Agenty/scrapingsandbox), please let me know if have have any suggestion or feedback.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r1tmm1/web_scraping_sandbox_website_scrapingsandboxcom/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4tovor",
          "author": "sakozzy",
          "text": "This is cool man! One suggestion: add a few â€œreal world painâ€ scenarios people actually run into. Stuff like lazy loading/infinite scroll, rate limits, basic anti-bot behavior, and messy HTML. Thatâ€™s where most beginners get stuck",
          "score": 8,
          "created_utc": "2026-02-11 16:37:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xj4rs",
              "author": "vickyrathee",
              "text": "Sure, I will add infinite scroll today and other example in upcoming week. PR welcome if you want to contribute. [https://github.com/Agenty/scrapingsandbox](https://github.com/Agenty/scrapingsandbox) ",
              "score": 4,
              "created_utc": "2026-02-12 04:57:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o55viwg",
              "author": "Gold_Emphasis1325",
              "text": "You can introduce simulated poor network conditions (latency, bandwidth, drops) on the client side.",
              "score": 1,
              "created_utc": "2026-02-13 13:46:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4s0ckj",
          "author": "lp435",
          "text": "Cool idea. How does it work? Have you implemented challenges?",
          "score": 1,
          "created_utc": "2026-02-11 10:39:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s0xu2",
              "author": "vickyrathee",
              "text": "it's hosed on Cloudflare workers to build scraping agents with any language you want. No copy right claim, legal issue. Learn, make videos, blog post etc.",
              "score": 1,
              "created_utc": "2026-02-11 10:44:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4tveec",
          "author": "AdministrativeHost15",
          "text": "Consider giving a prize to the user who can scrape the highly protected secret word.",
          "score": 1,
          "created_utc": "2026-02-11 17:07:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xizyf",
              "author": "vickyrathee",
              "text": "Who will pay for the prize? :-)",
              "score": 2,
              "created_utc": "2026-02-12 04:56:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4xugxs",
                  "author": "AdministrativeHost15",
                  "text": "Prize can just be bragging rights that will help you get freelance gigs scraping highly protected sites.",
                  "score": 1,
                  "created_utc": "2026-02-12 06:31:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4u80ee",
          "author": "Gold_Emphasis1325",
          "text": "Legal / TOS issues with provider? Otherwise, it would be nice to have a practice area for Captcha, and the \"click this box\" to prove you're human. Either way, it's all just an arms race between the scrapers and the scrape-ees....",
          "score": 1,
          "created_utc": "2026-02-11 18:06:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xjdx6",
              "author": "vickyrathee",
              "text": "Adding captcha etc on sandbox will be a TOS issue and might risk our account, so will add general examples for learning instead like infinit scroll, form submission, pagination, rate limit etc.",
              "score": 1,
              "created_utc": "2026-02-12 04:59:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o55vdcd",
                  "author": "Gold_Emphasis1325",
                  "text": "Yeah and the TOS are changing constantly. Allowed / gray area today and violation and possibly legal action later...",
                  "score": 1,
                  "created_utc": "2026-02-13 13:45:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4x17cu",
          "author": "Objective-Fun-4533",
          "text": "Cool. Like the UI",
          "score": 1,
          "created_utc": "2026-02-12 02:55:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xjfig",
              "author": "vickyrathee",
              "text": "thanks to Claud Opus 4.6",
              "score": 1,
              "created_utc": "2026-02-12 04:59:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4y8su5",
          "author": "Nel549",
          "text": "Add cloudfare to it or on specific pages",
          "score": 1,
          "created_utc": "2026-02-12 08:46:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yim0a",
          "author": "maxim-kulgin",
          "text": "add products variations? for example like size/color? Thanks man! ",
          "score": 1,
          "created_utc": "2026-02-12 10:23:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyx0zj",
      "title": "Holy Grail: Open Source Autonomous AI Agent With Custom WebScraper",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qyx0zj/holy_grail_open_source_autonomous_ai_agent_with/",
      "author": "AppropriateLeather63",
      "created_utc": "2026-02-08 02:35:45",
      "score": 16,
      "num_comments": 13,
      "upvote_ratio": 0.68,
      "text": "[https://github.com/dakotalock/holygrailopensource](https://github.com/dakotalock/holygrailopensource)\n\nReadme is included.\n\nWhat it does: This is my passion project. It is an end to end development pipeline that can run autonomously. It also has stateful memory, an in app IDE, live internet access, an in app internet browser, a pseudo self improvement loop, and more.\n\nThis is completely open source and free to use.\n\nIf you use this, please credit the original project. Iâ€™m open sourcing it to try to get attention and hopefully a job in the software development industry.\n\nTarget audience: Software developers\n\nComparison: Itâ€™s like replit if replit has stateful memory, an in app IDE, an in app internet browser, and improved the more you used it. Itâ€™s like replit but way better lol\n\nCodex can pilot this autonomously for hours at a time (see readme), and has. The core LLM I used is Gemini because itâ€™s free, but this can be changed to GPT very easily with very minimal alterations to the code (simply change the model used and the api call function). Llama could also be plugged in.",
      "is_original_content": false,
      "link_flair_text": "AI âœ¨",
      "permalink": "https://reddit.com/r/webscraping/comments/1qyx0zj/holy_grail_open_source_autonomous_ai_agent_with/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o47x3w7",
          "author": "SkratchyHole",
          "text": "Have you considered separating the code instead of having everything in one long file?",
          "score": 13,
          "created_utc": "2026-02-08 07:57:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o489egc",
              "author": "TylerDurdenJunior",
              "text": "Just a 9000 line long file.\n\nðŸ˜",
              "score": 15,
              "created_utc": "2026-02-08 09:54:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48usco",
          "author": "KickBack-Relax",
          "text": "Was AI used to build this?",
          "score": 7,
          "created_utc": "2026-02-08 13:00:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4acqln",
              "author": "mild_geese",
              "text": "One commit, one file, references 6 different models at the top. Pure slop",
              "score": 9,
              "created_utc": "2026-02-08 17:50:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4jaqjb",
                  "author": "ClockAppropriate4597",
                  "text": "Oh my fucking god why is every programming sub filled with these AI slop projects?",
                  "score": 3,
                  "created_utc": "2026-02-10 01:10:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o48zn7w",
              "author": "davak72",
              "text": "I canâ€™t imagine it wasnâ€™t. AI bros use AI",
              "score": 3,
              "created_utc": "2026-02-08 13:33:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4cmllz",
          "author": "taylorlistens",
          "text": "As a general rule, you should avoid targeting an audience you arenâ€™t a part of.",
          "score": 6,
          "created_utc": "2026-02-09 00:50:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4klm9a",
              "author": "fts_now",
              "text": "ðŸ˜„",
              "score": 1,
              "created_utc": "2026-02-10 06:18:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4rqz5q",
              "author": "RobSm",
              "text": "Damn right :)",
              "score": 1,
              "created_utc": "2026-02-11 09:12:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4bbqh8",
          "author": "Practical_Estate4971",
          "text": "The concept here is interesting, but the implementation is definitely more of a rough prototype than a production-ready tool. The backend is currently just one massive script instead of a structured application, and the memory system relies on a text file that rewrites itself entirely on every request, which is going to cause major lag as it grows. It also lacks security sandboxing and currently forces watermarks on the output, so it really needs a modular rewrite and a real database to be viable for actual use. Keep trying... Do not expect job interviews anytime soon!",
          "score": 2,
          "created_utc": "2026-02-08 20:37:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46w4c7",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 0,
          "created_utc": "2026-02-08 03:05:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48iqxg",
              "author": "9302462",
              "text": "Spam response that replies to every post.",
              "score": 2,
              "created_utc": "2026-02-08 11:21:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qzdo94",
      "title": "best alternative to Puppeteer that Google can't detect as a bot?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qzdo94/best_alternative_to_puppeteer_that_google_cant/",
      "author": "bs_ti",
      "created_utc": "2026-02-08 16:40:00",
      "score": 12,
      "num_comments": 17,
      "upvote_ratio": 0.76,
      "text": "Google now detects Puppeteer pretty easily. What are you using instead that works?\n\nI need something that passes as a real user and doesn't get flagged.\n\nWhat's actually working?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qzdo94/best_alternative_to_puppeteer_that_google_cant/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4aj2se",
          "author": "thePsychonautDad",
          "text": "100% success: Chrome + CDP. Ideally with xdotool or some other software that allows to move the mouse, click & type so you don't get flagged by emulated events.\n\nIt's not headless, but it skips all the automation flags and works.",
          "score": 15,
          "created_utc": "2026-02-08 18:19:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4c80jc",
              "author": "mastermog",
              "text": "> xdotool\n\nNot OP, but would you have any handy resources on Chrome + CDP + xdotool? A blog article or similar?\n\nI haven't been able to source much outside of the official xdotool repo.",
              "score": 2,
              "created_utc": "2026-02-08 23:27:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4cmmkv",
                  "author": "thePsychonautDad",
                  "text": "I don't have any link, but any LLM will give you a detailed explanation & code to implement.\n\nThe basics: Your script launches Chrome with CDP enabled. Then that allows it to take control of the browser. You make the script go to a page, get the DOM via CDP, figure out what to click or type into (either via events or compute the bounding box and send the cursor to click there).",
                  "score": 4,
                  "created_utc": "2026-02-09 00:50:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4dibux",
          "author": "Upper-Character-6743",
          "text": "For scraping Google (specifically Google Places), I've had success with Selenium's UC mode. Check it out if you're interested in a headless browser that has a similar API to Puppeteer.",
          "score": 2,
          "created_utc": "2026-02-09 03:43:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4it76x",
          "author": "CuriousCat7871",
          "text": "You have to use multiple technologies to bypass anti bots technologies. Donâ€™t run your browser headless, in headful mode. Use anti captcha tools and persist your user data between your browsing sessions. If you run in a vps, use residential proxies.\n\nIt is not guaranteed to work, but it will increase your chances.",
          "score": 2,
          "created_utc": "2026-02-09 23:31:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qp0ak",
          "author": "SharpRule4025",
          "text": "The tool isn't the problem, the fingerprint is. Puppeteer, Playwright, Selenium, they all work fine if you patch the detectable signals: navigator.webdriver flag, CDP leak, headless indicators in the user agent, missing plugins array, etc.\n\nPlaywright with a proper stealth config handles most of it out of the box. For Google specifically, the trick is residential proxies plus persistent browser profiles with real-looking fingerprints. Google's detection is more about behavioral signals and IP reputation than which automation framework you're running.\n\nIf you're doing high volume, the real answer is stop fighting the bot detection and use an approach that handles it at the infrastructure level. Rotating sessions with pre-solved challenges, fresh fingerprints per session, and IPs that haven't been burned.",
          "score": 2,
          "created_utc": "2026-02-11 03:54:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4aexjx",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-08 18:00:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4budyo",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 2,
              "created_utc": "2026-02-08 22:11:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4cangg",
          "author": "Curiouser666",
          "text": "Which site(s) are you wanting to access?",
          "score": 1,
          "created_utc": "2026-02-08 23:42:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hd3mv",
          "author": "OrchidKido",
          "text": "Playwright with custom fingerprints injection. Works like a charm without those botright, patchright, whateveresleright stuff you can find on Github. \n\nSpent a whole shitload of time, tested each and every repo untill discovered it. The success rate is not that good as I'd want it to, but this time it's just a matter of proxy. And I use pretty shitty datacenter proxy.",
          "score": 1,
          "created_utc": "2026-02-09 19:08:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hdb9f",
              "author": "OrchidKido",
              "text": "If speed is not the matter - go with the Selenium undetected. Else - Playwright is  the only way to go.",
              "score": 1,
              "created_utc": "2026-02-09 19:09:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4id7xh",
          "author": "simion_baws",
          "text": "nodriver / zendriver. Direct browser control via CDP, no traces or anything, undetectable.",
          "score": 1,
          "created_utc": "2026-02-09 22:07:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pkn31",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-10 23:51:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qh1t8",
              "author": "webscraping-ModTeam",
              "text": "âš¡ï¸ Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-02-11 03:03:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4sw07r",
          "author": "pepohte",
          "text": "Selenium + Undetected chrome driver",
          "score": 1,
          "created_utc": "2026-02-11 14:15:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3hy8h",
      "title": "Chrome extension that auto-detects and extracts data from any webpage",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r3hy8h/chrome_extension_that_autodetects_and_extracts/",
      "author": "mjiqbal",
      "created_utc": "2026-02-13 06:33:11",
      "score": 11,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "we got tired of writing one-off scripts for simple scraping jobs (product listings, search results, directories, tables ) so we built a Chrome extension called Detect and Extract that handles the common cases without writing a single line of code.\n\nHow it works:\n\n1.  Click the extension icon on any page, it automatically detects data structures (tables, lists, grids, card layouts, etc.)\n2.   Preview the data in a grid, rename or delete columns you don't need\n3.   Export as CSV, Excel, JSON, or copy straight to clipboard\n\n The part we are most proud of, multi-page crawling:\n\n*   Point it at the \"Next\" button or page numbers, and it'll auto-crawl through all pages\n*   Handles numbered pagination (1, 2, 3...) and \"Next >\" button patterns\n*   Auto-detects and dismisses modal popups that block the page during crawls (upgrade prompts, cookie banners, etc.)\n*   Built-in deduplication so you don't get repeat rows across pages\n\n  Other stuff:\n\n*  Visual element picker â€” if auto-detection misses something, click any element on the page and it finds all similar items\n*  Presets â€” save your scraping config per site so you don't have to set it up again next time\n*  Minimal permissions â€” only uses activeTab, no background data collection, no account required\n*  Works on most sites â€” e-commerce, directories, forums, search results, dashboards\n\n  What it's NOT:\n\n*   Not a Selenium/Puppeteer replacement â€” this is for visual, interactive scraping\n*   Won't bypass anti-bot measures or CAPTCHAs\n*   Not great for SPAs that require authentication flows or infinite nested navigation\n\n\n\nTrying to make it a solid free tool for people who need quick data without spinning up a whole scraping pipeline.\n\nWould love feedback from this community. What features would make this more useful for your workflows?\n\n[Detect and Extract](https://chromewebstore.google.com/detail/detect-and-extract/kkmibnjkdelljnkoibconbnaenpliefi)\n\nhttps://preview.redd.it/lj6mmc2xg7jg1.png?width=1681&format=png&auto=webp&s=9a0545a48f68145697e2a74d99da783ae07320da\n\n  \nHeads up about the install warning: If you have Chrome's Enhanced Safe Browsing turned on (Chrome Settings > Privacy), you'll see a \"Proceed with caution\" dialog when installing. This is completely normal for all newly published extensions it's not a security issue with the extension itself. Google flags every new extension until it builds up a trust score over time through installs and automated safety reviews. Just click \"Continue to install\" and it works fine. Users with Standard Safe Browsing won't see this at all.  \nReference URL [Regarding that warning](https://support.google.com/chrome/answer/2664769?visit_id=639065601437510841-3127678855&p=cws_enhanced_safe_browsing&rd=1#10745467)  \n",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r3hy8h/chrome_extension_that_autodetects_and_extracts/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o55xo30",
          "author": "Gold_Emphasis1325",
          "text": "I made a few versions of these and settled on one that I sometimes use. I was thinking about trying to productize and ideally make money or at least make connections, but with vibe coding out these days, a whole genre of apps and utilities and projects are now \"useless\" in the customer sense, but gold for the individual -- hyper personalized, \"free (minus your time and tokens)\" and something to showcase....\n\nPrivate-source open roadmap for anyone building their own:  \n\\- cloudflare box interaction  \n\\- old and 2025+ style captchas (and beta unseen human detection bypass)  \n\\- secure interaction with API layer / MCP - LLM - RAG - persistence  \n\\- auth/auth  \n\\- payments system  \n\\- thought through TOS of the plugin/API, privacy (what users will accept) and payments  \n\\- plan for dealing with fragility, constant updates",
          "score": 1,
          "created_utc": "2026-02-13 13:57:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5awbvi",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-14 06:51:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b4cax",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-14 08:06:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5bd2mc",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-14 09:32:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qyn0p8",
      "title": "Open sourced my business' data extraction framework",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qyn0p8/open_sourced_my_business_data_extraction_framework/",
      "author": "Apprehensive-File169",
      "created_utc": "2026-02-07 19:31:07",
      "score": 9,
      "num_comments": 1,
      "upvote_ratio": 0.92,
      "text": "Through years of webscraping, a huge issue I faced is discrepancy between data types and extraction types and varying website formats.\n\n\n\nA website that has an API, some html docs, json within the html, multiple potential formats and versions etc. all need code flows to extract the same data. And then, how do you have resiliency and consistency in data extraction when the value is usually in place A with an xpath, sometimes place B with a json, and as a last resort regex search for place C?\n\n\n\nMy framework, chadselect, pulls html json and raw text into one class that allows selection across all 4 extraction frameworks (xpath, css, regex, jmespath) to build consistent data collection.\n\n    cs = ChadSelect()\n    cs.add_html('<>some html</>')\n\n    result = cs.select_first([\n        (0, \"css:#exact-id\"),\n        (0, \"xpath://span[@class='alt']/text()\"),\n        (0, r\"regex:fallback:\\s*(.+)\"),\n    ])\n    \n\n  \nOne more addition, common xpath functions like normalize space, trim, substring, replace are built into all selectors - not only limited to xpath anymore. Callable with simple '>>' piping:\n\n    result = cs.select(0, \"css:.vin >> substring-after('VIN: ') >> substring(0, 3) >> lowercase()\")\n    \n\n  \nFuthermore, it's already preconfigured with what I've found to be the fastest engines for each type of querying (lxml, selectolax, re, and jmespath). So hopefully it will be a boost to consistency, dev convenience, and execution time.\n\n\n\nI'm trying to get into open sourcing some projects and frameworks I've built. It would mean the world to me if this was useful to anyone. Please leave issues or comments for any bugs or feature requests. \n\n\n\nThank you for your time\n\n  \n\n\n[https://github.com/markjacksoncerberus/chadselect](https://github.com/markjacksoncerberus/chadselect)\n\n[https://pypi.org/project/chadselect/](https://pypi.org/project/chadselect/)\n\n[https://crates.io/crates/chadselect](https://crates.io/crates/chadselect)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qyn0p8/open_sourced_my_business_data_extraction_framework/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4n1qu6",
          "author": "DataKazKN",
          "text": "Nice approach with the type discrepancy handling â€” that's genuinely one of the most annoying parts of scraping at scale.\n\nHow do you handle sites that rotate their HTML structure? I've been building scrapers for marketplace sites (Vinted, App Store) and the biggest pain is when they randomly change their Next.js RSC payload format. Ended up using a multi-strategy approach: try API first, fallback to HTML parsing, then ld+json extraction.\n\nStarred the repo. The modular extraction pipeline is clean.",
          "score": 1,
          "created_utc": "2026-02-10 16:37:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r16ix2",
      "title": "Is cookie reuse risky or necessary for modern scraping?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r16ix2/is_cookie_reuse_risky_or_necessary_for_modern/",
      "author": "JosephPRO_",
      "created_utc": "2026-02-10 17:07:40",
      "score": 9,
      "num_comments": 10,
      "upvote_ratio": 0.85,
      "text": "I keep seeing very mixed advice when it comes to cookies in scraping workflows. Some people say reusing cookies is important if you want sessions to look normal and avoid getting blocked, especially on sites with logins or multi-step flows. Others warn that reusing cookies is risky and can actually cause more problems if you carry over bad state or get a session flagged.\n\nFrom what Iâ€™ve seen so far, starting with a fresh session every time sometimes works, but other times it feels like sites expect continuity. Reusing cookies seems to make things more stable in those cases, but Iâ€™m never sure how long is too long or when a session should be thrown away and rebuilt.\n\nIâ€™m trying to figure out what actually works in real-world scraping, not just in theory. Do people here mostly reuse cookies, rotate them often, or avoid them unless absolutely needed? Have cookies been a bigger source of trouble than things like IPs or headers over time?\n\nCurious to hear how others approach this and what youâ€™ve learned from experience.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r16ix2/is_cookie_reuse_risky_or_necessary_for_modern/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4n90y3",
          "author": "v_maria",
          "text": "This honestly depends on the site, thats the fun thing about scraping. There is no real silver bullet you need to learn and understand web",
          "score": 11,
          "created_utc": "2026-02-10 17:10:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n9lh7",
          "author": "army_of_wan",
          "text": "You throw the session away when it expires  and you'll know it expires when you get a 403 instead of a 200. \n\nCatch the exception\n\nInitiate a solving process -> could be cookie farming api or something else\n\nUpdate your session with new cookies\n\nYou may also consider proxy rotation.\n\nGo again.",
          "score": 7,
          "created_utc": "2026-02-10 17:13:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nljif",
          "author": "RandomPantsAppear",
          "text": "Not in the slightest, especially as most sites wonâ€™t block you solely for being an unfamiliar entity. No history + one of (bad ip, bad history, bad headers) will get you blocked though. \n\nThere are multiple ways to handle this\n\n1) Just accept your 50% success rate and abuse celeryâ€™s retry functionality \n\n2) Forge the cookies (integer, timestamp, uuid)\n\n3) Build a history - easier than it sounds. Iâ€™ve actually had scrapers that zip up their chrome profile and upload to s3 when theyâ€™re done, then redownload it when they start again. \n\nâ€”â€”â€”â€”â€”-\n\nThe biggest advantage a scraper has vs history based blocking is that **they can only query data they can access in milliseconds**. It is not acceptable virtually anywhere to add 200ms to every request. \n\nWhat this means is that you only need the basics - a number of requests previously seen, an ideally distant start date, and an ideally recent end date. Thatâ€™s the kind of data most places can quickly cache in volume. \n\nSimilarly (for cookie forging) a remarkable number of places will accept a uuid as legitimate just because it should be a uuid and is a uuid, and cache misses and syncing between multiple nodes is a common issue with real users. \n\nThis can get a little hairy once captchas start rendering and you donâ€™t solve them. But for most solutions, a pretty basic history will go a long way.",
          "score": 3,
          "created_utc": "2026-02-10 18:08:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qoqqp",
          "author": "SharpRule4025",
          "text": "It depends entirely on the site and what their anti-bot is looking for. There's no universal rule.\n\nFor sites with Cloudflare or similar protection, reusing a valid session cookie avoids re-solving the challenge on every request. That's a net positive, fewer challenge pages, faster requests, lower detection risk. The key is rotating them before expiry and not sharing a single cookie across multiple IPs.\n\nFor simpler sites, fresh sessions work fine. The overhead of cookie management isn't worth it if the site doesn't track session state for bot detection.\n\nThe real risk with cookie reuse is carrying a flagged session forward. If one request gets soft-blocked (CAPTCHA, rate limit warning), that cookie is burned. You need logic to detect when a session goes stale and swap it out immediately rather than keep hammering with a poisoned session.",
          "score": 2,
          "created_utc": "2026-02-11 03:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zcirk",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-12 13:59:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zj75h",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-12 14:35:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o567v2g",
          "author": "sohailglt",
          "text": "It depends on the website youâ€™re scraping. Youâ€™ll need to test and try the cookies one by one to see which ones are required.",
          "score": 1,
          "created_utc": "2026-02-13 14:51:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qz50zq",
      "title": "Cloudflare suddenly blocking previously working excel download url.",
      "subreddit": "webscraping",
      "url": "https://i.redd.it/h0yo3uklt8ig1.jpeg",
      "author": "warshed77",
      "created_utc": "2026-02-08 09:53:46",
      "score": 8,
      "num_comments": 16,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qz50zq/cloudflare_suddenly_blocking_previously_working/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o491osr",
          "author": "Coding-Doctor-Omar",
          "text": "Are you using the standard \"requests\" library?",
          "score": 6,
          "created_utc": "2026-02-08 13:45:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o492juu",
              "author": "warshed77",
              "text": "Yes, I was hitting this link and excel was downloading.",
              "score": 3,
              "created_utc": "2026-02-08 13:51:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4930ai",
                  "author": "Coding-Doctor-Omar",
                  "text": "Use curl_cffi instead.\n\nRun in the terminal:\n\n```\npip install curl-cffi\n```\n\nIn your code, use this:\n\n```\nfrom curl_cffi import requests\n\ntarget_url = \"https://your.target.url.com/\"\n\nres = requests.get(target_url, impersonate=\"edge\")\n```\n\nDO NOT forget the \"impersonate\" argument.",
                  "score": 5,
                  "created_utc": "2026-02-08 13:54:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4j0m7z",
          "author": "SharpRule4025",
          "text": "Same thing happened to a bunch of scrapers in the last few weeks. Cloudflare rolled out stricter WAF defaults and a lot of sites got auto-upgraded without even asking for it.\n\nFor the requests library specifically, try curl_cffi instead. It matches the TLS fingerprint of a real browser which is what Cloudflare is actually checking. The standard requests library has a distinctive TLS signature that gets flagged instantly.\n\nIf the site needs JS rendering on top of that, you're looking at playwright with a stealth plugin or a scraping API that handles the anti-bot layer for you.",
          "score": 2,
          "created_utc": "2026-02-10 00:12:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48bo1w",
          "author": "Difficult-Cat-4631",
          "text": "They might have activated cloudflare with custom waf rules. Do you have permission for what you want to do? Maybe they want to prevent that",
          "score": 4,
          "created_utc": "2026-02-08 10:15:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48ezy9",
              "author": "warshed77",
              "text": "If ibho through selenium it works but I don't want that , is there any way I can do this using requests.",
              "score": 1,
              "created_utc": "2026-02-08 10:47:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48rzif",
                  "author": "Accomplished-Gap-748",
                  "text": "Many options depending on the rules on their side. You can try : residential proxy, tls spoofing, lower concurrency, headless browser (i know you don't want to, but sometimes it's the only solution), etc",
                  "score": 2,
                  "created_utc": "2026-02-08 12:40:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ae0pz",
          "author": "Accurate-Lie-957",
          "text": "Try using requests.get() with standard headers (as you were doing) but through a residential proxy. What kind of network are you on - residential or vps or something else? ",
          "score": 1,
          "created_utc": "2026-02-08 17:56:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4dzytx",
              "author": "warshed77",
              "text": "With residential also getting this 403 error.",
              "score": 1,
              "created_utc": "2026-02-09 05:47:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fpmkt",
          "author": "Krokzter",
          "text": "If requesting with your real IP or residential doesn't work, then the website most likely has a javascript proof of work. It will generate a token using your browser and send it along with the request.    \nIf that's the case, your options are:   \na. automate scraping with a browser using Playwright, Selenium, etc.   \nb. have a browser always open on the target page, and generate tokens on demand to be used in your requests.    \nOption b. is often better at scale, but a. is good enough for smaller scale.",
          "score": 1,
          "created_utc": "2026-02-09 14:19:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r103lp",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r103lp/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-02-10 13:01:11",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 0.85,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levelsâ€”whether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) ðŸŒ±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring ðŸ’°",
      "permalink": "https://reddit.com/r/webscraping/comments/1r103lp/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4u7yks",
          "author": "Finding_Away40",
          "text": "webscraping newbie here.  How do I scrape Google maps, or can I scrape google business profiles?  Scraping for a list of residential and commercial paining companies in the US.  Or, is it easiest to use a tool like Outscraper?  thx",
          "score": 1,
          "created_utc": "2026-02-11 18:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vaoah",
          "author": "skinonmychin",
          "text": "How would I scrape the files behind an old website (late aughties) from WebArchive? I recall seeing somewhere a way to download the text files behind it without fragmenting the content.",
          "score": 1,
          "created_utc": "2026-02-11 21:10:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4w4vur",
              "author": "HLCYSWAP",
              "text": "    wget \\\n      --mirror \\                # Mirror the site recursively\n      --convert-links \\         # Convert links to work offline\n      --adjust-extension \\      # Save files with proper extensions (like .html)\n      --page-requisites \\       # Download all resources (CSS, JS, images)\n      --no-parent \\             # Donâ€™t ascend to parent directories\n      -e robots=off \\           # Ignore robots.txt restrictions\n      -P ./local-copy \\         # Save to this local folder\n      \"https://example.com/path/\"",
              "score": 1,
              "created_utc": "2026-02-11 23:43:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o50bzja",
          "author": "Eyoba_19",
          "text": "Currently building a social media (tiktok and instagram mostly) to scrape profiles. The idea would be you can input specific keywords or description of what kind of influencers youâ€™re looking for with specific criteria like min/max follower count, region, engagement rate and so on, and retrieve a set of profiles.\n\nOver time a knowledge graph of profiles will be built and you can thus just fetch from a db instead of scraping.\n\nYou can then ofcourse sell the data back or train AI or whatever, you own your data.\n\nIâ€™m planning it as a Saas, but open to do it as a one-off product(although not sure how to handle constant support for when sites change their API).\n\nLet me know if anyone is interested, would love to talk to some people to kinda hit where this hurts and make sth out of it, DMs are open",
          "score": 1,
          "created_utc": "2026-02-12 16:53:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3uw0e",
      "title": "Built two scrapers for european markets what should I learn next?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r3uw0e/built_two_scrapers_for_european_markets_what/",
      "author": "Free-Path-5550",
      "created_utc": "2026-02-13 17:10:14",
      "score": 7,
      "num_comments": 4,
      "upvote_ratio": 0.82,
      "text": "Been working on scrapers for a couple of European marketplaces this past week. My first attempt in this space. I finished two that successfully \"work\"... for now.  Some things I picked up so far:\n\n\\- Raw data isn't enough. Adding computed fields like deal detection, engagement scoring, and price tracking across runs makes the output way more useful than a static dump.\n\n\\- European platforms have aggressive anti-bot compared to US sites from my research. Took real effort to get stable.\n\n\\- You don't need a browser for everything. Keeping it lightweight makes a huge difference.\n\n\\- Biggest lesson learned... was how much I hate DataDome. I was able to slip past it a few times, but usually blocked the next run. I eventually learned to just go around it if possible.\n\nStill early in this spcraping. What should I be learning next? What separates a decent scraper from a great one?  Thanks!",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r3uw0e/built_two_scrapers_for_european_markets_what/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o59r43b",
          "author": "Bitter_Caramel305",
          "text": "Try running your existing scrapers on a few hundred thousand product/items and you'll realize that scrapers without proxies and regular maintains are not stable at all. ",
          "score": 7,
          "created_utc": "2026-02-14 01:48:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5beg7v",
              "author": "Free-Path-5550",
              "text": "yeah learned that one fast! already looking into co-occurrence analysis and proximity scoring. just felt like the natural next step  didn't even know the terminology until i went down the rabbit hole, it clicked when i kept getting garbage results and thought 'there has to be a way to check these in combination.' this is for my 3rd scraper though, more of a complaint/intent mining thing vs marketplace scraping",
              "score": 1,
              "created_utc": "2026-02-14 09:46:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r06vvb",
      "title": "Help needed: scraping product URLs from El Corte InglÃ©s website",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r06vvb/help_needed_scraping_product_urls_from_el_corte/",
      "author": "frogmaxi",
      "created_utc": "2026-02-09 15:18:05",
      "score": 6,
      "num_comments": 9,
      "upvote_ratio": 0.81,
      "text": "Hi all,\n\nI work for Jack & Jones and manage the El Corte InglÃ©s (Spain) account. Iâ€™m trying to build a reliable way to list which styles and color variants are currently published on ECIâ€™s website, and extract two fields shown on the product page.\n\nSearch page (Jack & Jones menâ€™s clothing): via this [link ](https://www.elcorteingles.es/moda-hombre/ropa/search-nwx/1/?s=jack+%26+jones&stype=past_search)\n\n**What I need (output = CSV):**  \nFor each product on that search result (about **229 products/styles** when fully loaded), I need one row per **color variant** with:\n\n* **Modelo** (style code)\n* **Referencia** (variant/reference code)\n\nExample issue:  \nOne product can have multiple color swatches. Clicking swatches changes the URL query (e.g. `...&color=Negro`, `...&color=Azul`), and each color has a different â€œReferenciaâ€ while â€œModeloâ€ is shared.\n\n[for each colorway, we need the \\\\\"referencia\\\\\"](https://preview.redd.it/ggx6vi3uihig1.png?width=1901&format=png&auto=webp&s=6f82a326a98284309fb1778a55570ff02332ecdc)\n\n[This is found under \\\\\"detalle y cuidado\\\\\"](https://preview.redd.it/n6rdi0ywihig1.png?width=1817&format=png&auto=webp&s=e370f51ff77ac1bd1613caa7a4164c05830a1630)\n\n\n\nI have 0.5/10 python/web scraping knowledge but I guess this could be automated. I spent the last 8 hours trying back and forth with the help of chatGPT 5.2 (thinking) but without any luck. Is this achivable?\n\nThanks in advance!  \nCarlos",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r06vvb/help_needed_scraping_product_urls_from_el_corte/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4hl0rv",
          "author": "jeheda",
          "text": "I was bored so:\n\n[https://docs.google.com/spreadsheets/d/1McYT-ulwmsUp7zBmaTTXvlD-cMJBmYBBds\\_wqHAoKis/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1McYT-ulwmsUp7zBmaTTXvlD-cMJBmYBBds_wqHAoKis/edit?usp=sharing)",
          "score": 4,
          "created_utc": "2026-02-09 19:46:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hli0j",
              "author": "frogmaxi",
              "text": "What a fucking genius!! \nWould you pleeease be so kind and let me know how you did it??",
              "score": 1,
              "created_utc": "2026-02-09 19:49:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4homd1",
                  "author": "jeheda",
                  "text": "They have an API, go to the website search and product page press F12 and go to the network tab and you will see the calls the website does to these api endpoints.\n\nIn this case i used these two endpoints:\n\nThis one requires authorization but the web generates one you can just copy and paste it:  \n/products/ecommerce-pdp-product/v1/pdp/  \n\n\nThis is the search pagination doesn't seem to require authorization:  \n/api/firefly/vuestore/new-search/moda-hombre/ropa/1/?s=jack%20%26%20jones&showDimensions=none&stype=past\\_search&isBookSearch=false&isMultiSearchCategory=false",
                  "score": 2,
                  "created_utc": "2026-02-09 20:05:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4g1w6f",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-09 15:24:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4g7x84",
              "author": "webscraping-ModTeam",
              "text": "ðŸª§ Please review the sub rules ðŸ‘‰",
              "score": 1,
              "created_utc": "2026-02-09 15:53:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gmcfb",
          "author": "CouldBeNapping",
          "text": "Totally achievable, but you should know that Jack&Jones already have an agency for this. You should talk to your UK counterparts.",
          "score": 1,
          "created_utc": "2026-02-09 17:02:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gwpla",
              "author": "frogmaxi",
              "text": "I thought so :)\nI donâ€™t know anyone at the UK office. In fact, I barely know anyone outside the Spanish branch. \nAny idea how difficult is to do what I need?\nThanks!!",
              "score": 1,
              "created_utc": "2026-02-09 17:51:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4j0m9g",
          "author": "SharpRule4025",
          "text": "For ongoing monitoring like this you want to hit their internal API endpoints directly instead of scraping rendered pages. Open the network tab in your browser while navigating the Jack & Jones section, filter by XHR, and you'll probably see product catalog API calls returning JSON with all the data you need.\n\nIf they serve it through a GraphQL endpoint thats even better since you can query exactly the fields you want. Way more stable than parsing HTML which breaks every time they redesign.",
          "score": 1,
          "created_utc": "2026-02-10 00:12:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0xvwc",
      "title": "IMDB data scraping",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r0xvwc/imdb_data_scraping/",
      "author": "pavankalyanre",
      "created_utc": "2026-02-10 11:06:49",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "**My old post:**\n\n[ https://www.reddit.com/r/webscraping/comments/o7l9fw/here\\_is\\_how\\_i\\_scraped\\_everything\\_on\\_imdb\\_website/ ](https://www.reddit.com/r/webscraping/comments/o7l9fw/here_is_how_i_scraped_everything_on_imdb_website/?utm_source=chatgpt.com)\n\nHi webscrapers,\n\nA few years ago, I posted about how I scraped around 500k IMDb movies. Iâ€™ve since updated the package to extract data directly from imdb apis instead of using BeautifulSoup or Selenium.\n\nI created scripts to extract IMDb data.\n\nCheck out the package and feel free to fork it and try it out. If you find it useful, a â­ on the repo would be appreciated.\n\nGithub :  [ https://github.com/pavan412kalyan/imdb-movie-scraper/tree/main/ImdbDataExtraction ](https://github.com/pavan412kalyan/imdb-movie-scraper/tree/main/ImdbDataExtraction)\n\nHere is the IMDb movie dataset on Kaggle: updated with 700k + movies now\n\n[ https://www.kaggle.com/datasets/pavan4kalyan/imdb-dataset-of-600k-international-movies/data ](https://www.kaggle.com/datasets/pavan4kalyan/imdb-dataset-of-600k-international-movies/data)\n\nYou can also see the website I built using this data (with some AI):\n\n[ https://realimdb.com/ ](https://realimdb.com/)\n\nScripts included:\n\nImdbDataExtraction/\n\nâ”œâ”€â”€ pages\\_dowloader/           # Movie/TV bulk scraping\n\nâ”œâ”€â”€ search\\_by\\_id/              # Individual lookups\n\nâ”œâ”€â”€ search\\_by\\_string/          # Text-based search\n\nâ”œâ”€â”€ people\\_downloader/         # Celebrity/crew data\n\nâ”œâ”€â”€ videos\\_downloader/         # Video content\n\nâ”œâ”€â”€ images\\_dowloader/          # Image content\n\nâ”œâ”€â”€ review\\_downl",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r0xvwc/imdb_data_scraping/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qzxuaa",
      "title": "Can someone help me out with data scraping",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qzxuaa/can_someone_help_me_out_with_data_scraping/",
      "author": "Old_Bed_4408",
      "created_utc": "2026-02-09 07:24:57",
      "score": 5,
      "num_comments": 5,
      "upvote_ratio": 0.86,
      "text": "I deal with inventory all day to make my job easier I wanna be able to pull product image from walmart.ca I have an excel file with huge list of of webcodes/UPC.. What's the easiest way I can do this and hopefully free and the ability to do it on my mobile ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qzxuaa/can_someone_help_me_out_with_data_scraping/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4ejc1r",
          "author": "akashpanda29",
          "text": "You can use Puppeteer with chrome",
          "score": 1,
          "created_utc": "2026-02-09 08:44:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4j0mbg",
          "author": "SharpRule4025",
          "text": "Walmart.ca has pretty aggressive anti-bot so Puppeteer alone will probably get you blocked after a few requests.\n\nBefore you go down the rendering route, check if Walmart has internal API endpoints. Open a product page in your browser, look at the network tab and filter by XHR/Fetch. You'll likely see API calls that return product data as JSON including image URLs. You can hit those directly with the UPC and skip the page rendering entirely. Way faster for bulk pulls too.",
          "score": 1,
          "created_utc": "2026-02-10 00:12:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vc446",
          "author": "HLCYSWAP",
          "text": "serve cookies to yourself in headless browser, make sure all challenges are passed, then reuse session and cookies in curl\\_cffi. youâ€™d hit Walmartâ€™s search or product lookup endpoint and parse the response because UPCs can have multiple SKUs and dont map to URLs cleanly for url construction. The product image URLs typically come from JSON (`NEXT_DATA__`, GraphQL, or search API responses).\n\nyou would have to code it and then sideload it into an android phone (while this is still possible, fuck google)\n\n  \nif you dont have coding experience / dont want to do this project yourself you can find persons in the subreddit looking for work [here](https://www.reddit.com/r/webscraping/comments/1qsmo66/monthly_selfpromotion_february_2026/) in the monthly self-promotion thread.",
          "score": 1,
          "created_utc": "2026-02-11 21:17:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4mkpv",
      "title": "Data extraction quality / LLM cost",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r4mkpv/data_extraction_quality_llm_cost/",
      "author": "Playgroundmob",
      "created_utc": "2026-02-14 14:53:02",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I'm trying to get an idea of how much people/companies would pay per result when scraping websites that require structured data with a reliable JSON schema and high data quality - for example, e-commerce or job listings. especially when dealing with unknown sources.\n\nLLMs like Flash 2.5/3.0, which are actually reliable for consistent results, are not cheap - sometimes we might even reach $0.01â€“$0.03 per extraction.\n\nI'm trying to understand, in real world terms, how much people would pay for solutions that just work.\n\nIf anyone can share real world use cases, that would be awesome.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r4mkpv/data_extraction_quality_llm_cost/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5czh07",
          "author": "RandomPantsAppear",
          "text": "I think the biggest issue you will run into is people doing it cheaper, not using an LLM. You are kind of limiting yourself to one offs, because for any recurring task people will find something more traditional and exponentially cheaper.",
          "score": 1,
          "created_utc": "2026-02-14 16:22:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d1jzs",
              "author": "Playgroundmob",
              "text": "You're right. I'm wondering if there's a use case for massive scrape operation, where we just can't handle building specific scrapers per domain, as we don't know what volume of different websites to investigate and scrape.",
              "score": 1,
              "created_utc": "2026-02-14 16:33:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5d246w",
                  "author": "RandomPantsAppear",
                  "text": "If I were you, Iâ€™d focus on integrating. Thereâ€™s lots of options but I would first build something in that let it work with Google docs, pdf, and images as well as HTML. I would also add several export types. \n\nAfter that I would make a chrome extension that allowed people to either subscribe or top up their balances, and extract data on demand. \n\nI would name the app something that business people would likely search for, not something a coder would search for.",
                  "score": 1,
                  "created_utc": "2026-02-14 16:35:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r1wc9z",
      "title": "scrape data from site that loads data dynamically with javascript???",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r1wc9z/scrape_data_from_site_that_loads_data_dynamically/",
      "author": "Quiet_Dasy",
      "created_utc": "2026-02-11 12:41:37",
      "score": 4,
      "num_comments": 8,
      "upvote_ratio": 0.75,
      "text": "Project Overview: DeckMaster Scraper\n\nLive Site: domain-rec.web.app\n\nTech Stack: Flutter frontend with a Supabase backend.\n\nCurrent Access: Public REST API endpoint (No direct DB credentials).\n\n\nTarget Endpoint:\n https://kxkpdonptbxenljethns.supabase.co/rest/v1/PopularDeckMasters?select=*&limit=50\n\nThe Goal\n\nInstead of just pulling all cards\n, I need to extract the specific card name  ,not card data, contained within each individual page.\n\nThe Challenge\n\nI need a method to iterate through the IDs provided by the main API and scrape the specific card details associated with each entry.\n\nHow to Scrape the Data??\n\nSince the site uses Supabase, i don't actually need to \"scrape\" the HTML. \n",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r1wc9z/scrape_data_from_site_that_loads_data_dynamically/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4srk51",
          "author": "BeforeICry",
          "text": "Generally one of the best formats. Static sites require full HTML parsing whereas these you can look for the backend API and likely use a much smaller traffic bandwidth.",
          "score": 2,
          "created_utc": "2026-02-11 13:51:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wbfd8",
          "author": "WouldaCouldaBetta",
          "text": "Cant u just let the page load and scrape the loaded html?",
          "score": 1,
          "created_utc": "2026-02-12 00:21:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yt62h",
          "author": "SharpRule4025",
          "text": "This is actually way easier than you think. Flutter apps with Supabase backends expose the REST API directly, you already found the endpoint. You don't need to scrape the rendered page at all.\n\nJust hit that Supabase endpoint, grab the IDs from the main listing, then iterate through them hitting the detail endpoint for each one. Supabase REST follows PostgREST conventions so you can filter with ?id=eq.{id} to get individual records. No browser, no JS rendering needed.",
          "score": 1,
          "created_utc": "2026-02-12 11:55:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5676nq",
          "author": "sohailglt",
          "text": "Monitor the endpoints, headers, and cookies, then replicate the request exactly as they send it to retrieve the data.",
          "score": 1,
          "created_utc": "2026-02-13 14:47:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qz0fo3",
      "title": "Built a Python scraper for RSS and web pages",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qz0fo3/built_a_python_scraper_for_rss_and_web_pages/",
      "author": "Single-Bandicoot3617",
      "created_utc": "2026-02-08 05:24:38",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.71,
      "text": "Hi everyone,\n\nIâ€™ve been working on a Python scraping project and wanted to share it here for feedback.\n\nThe project started as a simple RSS based scraper for AI and ML news. Iâ€™ve since expanded it into a more flexible scraping tool that can handle different kinds of sources.\n\nWhat it currently does:\n\nIt accepts multiple URLs through a small interactive CLI  \nIt checks whether a URL is an RSS feed or a normal webpage  \nIt scrapes static HTML pages using BeautifulSoup  \nIt falls back to Playwright for JavaScript heavy pages  \nIt stores both raw and cleaned results in Excel  \nIt can optionally upload the data to Google Sheets  \nIt runs automatically using a built in scheduler  \nIt includes logging, rate limiting, and basic failure reporting\n\nThis is still a learning focused project. My main goal was to understand how to structure a scraper that works across different site types instead of writing one off scripts.\n\nI would really appreciate feedback on:\n\nScraping approach and reliability  \nWhen to prefer RSS vs HTML vs browser based scraping  \nHow to make this more robust or simpler  \nAny bad practices you notice\n\nRepository link:  \n[https://github.com/monish-exz/ai-daily-tech-news-automation](https://github.com/monish-exz/ai-daily-tech-news-automation)\n\nThanks for taking a look.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qz0fo3/built_a_python_scraper_for_rss_and_web_pages/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r0mir4",
      "title": "FB group post scraping",
      "subreddit": "webscraping",
      "url": "https://i.redd.it/helvk0jdgkig1.png",
      "author": "Capital_Towel_9219",
      "created_utc": "2026-02-10 01:01:51",
      "score": 4,
      "num_comments": 4,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r0mir4/fb_group_post_scraping/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4jn1v5",
          "author": "RandomPantsAppear",
          "text": "With FB and tasks like this the only real route Iâ€™ve found for selectors is finding what you *can* make a selector for that is nearby the element youâ€™re targeting, then going into parent/children and looking at the length of the non html text. \n\nSometimes this is more doable if you compress the dom down - so (as an example) if element and element.parent have the same total text rendered to them and both are div/span, eliminate either tag (in a way that maintains the rendered text). \n\nIt (again, sometimes) helps make the gratuitous and redundant html fb renders more manageable and makes selectors matching that you later call parent or child on a bit more predictable.",
          "score": 2,
          "created_utc": "2026-02-10 02:22:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jnip7",
              "author": "RandomPantsAppear",
              "text": "Another option might be the screenshot and send to AI approach but that gets pricey at scale",
              "score": 3,
              "created_utc": "2026-02-10 02:25:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mk51f",
                  "author": "Capital_Towel_9219",
                  "text": "It is really time consuming, thats the main issue.",
                  "score": 1,
                  "created_utc": "2026-02-10 15:14:15",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l6gpl",
          "author": "Hot_District_1164",
          "text": "I'm trying to solve exactly this issue right now. The UI contains deliberately malicious bait attributes, that make you think you can use them to extract the content, but new attributes are appearing an disappearing on a daily basis. Links might not have the href attribute at all unless the post is within viewport and hovered by mose. Crazy stuff. Most success I had was by just getting all the posts, getting innerText and processing by AI. I've also reverse engineered some successful browser extensions doing this Facebook stuff and apparently they are using the Facebook GraphQL to get the data.",
          "score": 1,
          "created_utc": "2026-02-10 09:34:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3lfdv",
      "title": "How do you handle scraping directory sites that cap results at ~200?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r3lfdv/how_do_you_handle_scraping_directory_sites_that/",
      "author": "deadlynightshade_x",
      "created_utc": "2026-02-13 10:08:34",
      "score": 4,
      "num_comments": 15,
      "upvote_ratio": 0.67,
      "text": "Hey \n\nI've been trying to pull data from a large online directory/phone book site (Swiss one, but I think the issue is pretty common across similar services like yellow pages, local directories, etc.).The site claims tens of thousands of matching entries (e.g., \\~50k private records for a region), but in practice:\n\n* URL params like &pages=500, &maxnum=500, or whatever don't actually fetch more and it hard-caps visible/returned results around 200.\n\nHas anyone here successfully scraped large volumes from these kinds of directory/phone book sites recently ?\n\n* Do most of them still enforce strict \\~100â€“200 result caps per query/page?\n* What tricks actually work to get around it without getting banned quickly?\n\nJust curious if it's still feasible for bigger datasets or if these sites have mostly locked it down. \n\n\n\nTips, tools, or experiences appreciated !Thanks!",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r3lfdv/how_do_you_handle_scraping_directory_sites_that/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o55w5y8",
          "author": "datamizer",
          "text": "They all set max results per query to normal defaults like 100. Even if you try to request more, the API basically has logic to protect it from serving a very large result set. \n\nAll you do is set a page number or offset, figure out the max result size, then send paginated requests at some interval. That's very standard.\n\nDepends on the site, some just have basic rate limits like no more than 10 requests over 10 seconds from the same IP. Most don't have robust prevention. Most you just pass an origin and a referer and that's good enough.",
          "score": 6,
          "created_utc": "2026-02-13 13:49:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o565cf3",
              "author": "leros",
              "text": "Lots of sites cap the results and don't let you paginate through them all. You need be more creative to scrape everything.Â ",
              "score": 2,
              "created_utc": "2026-02-13 14:38:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56d6i8",
                  "author": "datamizer",
                  "text": "It depends on the specifics of how they implemented it. Some sites there is no mechanism other than having a full sitemap if they offer one. Checking robots.txt to see what they are trying to obscure, using search engines that have their pages indexed etc. It's really just site by site basis.",
                  "score": 1,
                  "created_utc": "2026-02-13 15:17:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5bqpts",
              "author": "deadlynightshade_x",
              "text": "Thank you",
              "score": 1,
              "created_utc": "2026-02-14 11:44:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o595at0",
          "author": "forklingo",
          "text": "a lot of those caps arenâ€™t just ui limits, theyâ€™re intentional backend limits to stop bulk extraction. if the api itself only returns \\~200 per query, tweaking page params usually wonâ€™t help.\n\nwhat tends to work better is slicing the query space instead of trying to increase the page size. for example, split by smaller geographic areas, postal codes, name prefixes, or categories so each query stays under the cap but collectively covers more ground.\n\nalso be careful with â€œgetting aroundâ€ protections. many directory sites have pretty strict terms and rate limits, and they do monitor unusual access patterns. rotating ips or hammering endpoints might work short term but usually gets you blocked fast.\n\nfor larger datasets, sometimes itâ€™s more realistic to look for official data sources, public datasets, or paid access if itâ€™s business critical. scraping big directories at scale has gotten a lot harder over the years.",
          "score": 2,
          "created_utc": "2026-02-13 23:34:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bquvy",
              "author": "deadlynightshade_x",
              "text": "I appreciate your input",
              "score": 1,
              "created_utc": "2026-02-14 11:45:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o57vvg6",
          "author": "Free-Path-5550",
          "text": "Break your one big query into many smaller ones that each return under the cap., then deduplicate after merging since slices will overlap. Well this is how i solved against the site i was trying.",
          "score": 1,
          "created_utc": "2026-02-13 19:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5br7pq",
              "author": "deadlynightshade_x",
              "text": "That what i did and it worked ! Thanks",
              "score": 1,
              "created_utc": "2026-02-14 11:48:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5a85n5",
          "author": "pauldm7",
          "text": "You will likely need to make a lot more queries. Ie instead of searching for widgets, you will search for widgets a, widgets b, widgets aa widgets ab and so on. Cover every combo of letters until the results stop.\n\nIf you can click on the individual results, how is the URL? Numerical IDs?",
          "score": 1,
          "created_utc": "2026-02-14 03:39:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bra4d",
              "author": "deadlynightshade_x",
              "text": "That's exactly what I did and it gave me good results! Thanks",
              "score": 1,
              "created_utc": "2026-02-14 11:49:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bfme2",
          "author": "uncivilized_human",
          "text": "one thing i've found helps - check if there's an internal api the frontend uses. the caps are often enforced at the ui layer but the underlying api might have different limits. open network tab, search, and see what endpoints get hit. mobile apps are worth checking too since their apis tend to be less locked down.",
          "score": 1,
          "created_utc": "2026-02-14 09:57:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5brd9y",
              "author": "deadlynightshade_x",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-02-14 11:49:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o56bp02",
          "author": "Hour_Analyst_7765",
          "text": "Yes most sites cap request sites.. whether its too big for a client (probably historic reasons), or anti-scraping measure.\n\nHowever, 100 to 200 per call is imo quite good still. If the dataset is 50k, thats 500 pages, and even if you wait a minute per call you'll have that dataset in just over 8 hours. If you refresh daily thats quick enough.\n\nFor me <10 hours is plenty quick. But I often don't have very strict latency requirements. Most of my jobs need to keep up on a \"daily basis\"\n\nThe difficulty is often:\n\n1. Walking pages linearly may get you blocked on WAF nowadays.\n2. Sites that stop serving data after N amount of records, because they don't expect anyone to realistically scan through literal hunrdeds or thousands of records.\n\nIn this case, you'll have to deal with navigation through categories, parametric filters, etc. to reduce the dataset that can be crawled completely.\n\n3. Sites that don't have a way of putting new/modified records in front. I fetch quite a lot of listings and always will look for a \"newest first\" kind of sorting. Fortunately for many sites this is their default. I stop crawling as soon as I see data that I've already crawled.",
          "score": 1,
          "created_utc": "2026-02-13 15:10:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58kdvb",
              "author": "ObjectIndependent827",
              "text": "Yeah this matches what most people run into now itâ€™s less about brute forcing pages and more about breaking the dataset into smaller slices through filters or categories\n\nNewest first sorting is huge too since it lets you maintain coverage without re crawling everything and helps avoid triggering WAF limits over time",
              "score": 2,
              "created_utc": "2026-02-13 21:42:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5br5j3",
              "author": "deadlynightshade_x",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-02-14 11:47:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3htxr",
      "title": "[Selenium/C#] \"Cannot start the driver service\" in Windows Service",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r3htxr/seleniumc_cannot_start_the_driver_service_in/",
      "author": "AromaticLocksmith662",
      "created_utc": "2026-02-13 06:26:28",
      "score": 3,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nIâ€™ve been banging my head against a wall for a week with a Selenium ChromeDriver issue and could use some fresh eyes.\n\nThe Context:\n\nI have a web scraping tool running as a background Windows Service. It processes license data for different states.\n\nScale: We have about 20 separate Windows Services running in parallel on the same server, each scraping different data sources.\n\nTech Stack: C# .NET, Selenium WebDriver, Chrome (Headless).\n\nVersion: Chrome & Driver are both version 144.0.x.x (Versions are matched).\n\nThe Issue:\n\nEverything was running smoothly until recently. Now, I am getting a WebDriverException claiming it cannot start the driver service on a specific localhost port.\n\nthe exception:\n\nCannot start the driver service on http://localhost:54853/\n\nThe Stack Trace:\n\nat OpenQA.Selenium.DriverService.Start()\n\nat OpenQA.Selenium.Remote.DriverServiceCommandExecutor.Execute(Command commandToExecute)\n\nat OpenQA.Selenium.WebDriver.Execute(String driverCommandToExecute, Dictionary\\`2 parameters)\n\nat OpenQA.Selenium.WebDriver.StartSession(ICapabilities desiredCapabilities)\n\nat OpenQA.Selenium.Chromium.ChromiumDriver..ctor(ChromiumDriverService service, ChromiumOptions options, TimeSpan commandTimeout)\n\nat MyNamespace.LicenseProject.Business.Vermont.VermontLicenseService.ProcessLicense() in ...\\\\VermontLicenseService.cs:line 228\n\ncode:\n\nvar options = new ChromeOptions();\n\noptions.AddArgument(\"--headless\");\n\noptions.AddArgument(\"--no-sandbox\");\n\noptions.AddArgument(\"--disable-dev-shm-usage\");\n\n// I am explicitly setting the driver directory\n\nvar service = ChromeDriverService.CreateDefaultService(driverPath);\n\nservice.HideCommandPromptWindow = true;\n\n// Error implies it fails right here:\n\nusing (var driver = new ChromeDriver(service, options, TimeSpan.FromMinutes(2)))\n\n{\n\n// scraping logic\n\n}\n\nWhat I've Tried/Verified:\n\nVersion Mismatch: Double-checked that the chromedriver.exe version matches the installed Chrome browser version (144.0.x.x).\n\nManual Run: The scraper works fine when I run it as a console app/user mode. It only fails when running as a Windows Service.\n\nCleanup: I suspected \"zombie\" chrome processes were eating up ports, so I added logic to kill orphaned chrome processes, but the issue persists.\n\nHas anyone managed high-volume Selenium instances in a Windows Service environment and seen this port binding error?\n\nAny pointers would be appreciated!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r3htxr/seleniumc_cannot_start_the_driver_service_in/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5b4f4r",
          "author": "jinef_john",
          "text": "I would explicitly enable verbose logging and log that to a file. But here is what I think is happening, with 20 parallel services, they're likely clashing on the default user data directory. You should try and set unique user data directories per service instance.\n\nAlso I suspect the driver might be failing to bind, you could let chromedriver pick its own port, by setting service.Port=0.\n\nIf neither of these help, I'd bet it's a session 0 isolation problem. I believe even in headless, chrome sometimes needs to create a window station or access GDI resources that are restricted in Session 0. So you could try running the service under a specific user account with 'allow service to interact with service'\n\nOr it could even be the headless flags too. \nTry the new headless mode\n```\n--headless=new\n```\nYou could also add other options, disable gpu, extensions,software rasterizer,force color profile to be srgb\n\nBut I would mostly look into the user data directory or session 0 isolation.",
          "score": 1,
          "created_utc": "2026-02-14 08:07:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o568ljp",
          "author": "Stunning_Cry_6673",
          "text": "Lol. Selenium was a technology popular 15 years ago . Cant believe someone would use it today from scraping ðŸ˜‚ðŸ˜‚ðŸ˜‚",
          "score": -4,
          "created_utc": "2026-02-13 14:54:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b4tkt",
              "author": "jinef_john",
              "text": "Your favorite tool today, probably uses selenium under the hood ðŸ˜‰",
              "score": 0,
              "created_utc": "2026-02-14 08:11:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5b52xt",
                  "author": "Stunning_Cry_6673",
                  "text": "Lol. In these days when is so simple to get information you come with this garbage affirmation ðŸ¤£ðŸ¤£ðŸ¤£",
                  "score": 0,
                  "created_utc": "2026-02-14 08:14:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qzbk1v",
      "title": "Turnstile keeps blocking my daily scraper. Any help?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qzbk1v/turnstile_keeps_blocking_my_daily_scraper_any_help/",
      "author": "No-Card-2312",
      "created_utc": "2026-02-08 15:19:05",
      "score": 2,
      "num_comments": 16,
      "upvote_ratio": 0.63,
      "text": "Hey folks, \n\nIâ€™m kind of stuck and looking for some realâ€‘world advice. \n\nI have a small tool that grabs public HTML pages from a site protected by Cloudflare Turnstile. \n\nThereâ€™s no API, no hidden endpoints, the data is literally just what a browser sees.\n\nThe funny part: It runs once a day One page No parallel requests No hammering Stillâ€¦ Turnstile every time ðŸ˜….\n\nIâ€™ve tried the usual stuff: Playwright / Puppeteer with a real browser (not headless) Reasonable headers, UA, viewport Slowing everything way down Even Firefoxâ€‘based setups The tool runs on a VPS, so Iâ€™m starting to wonder if that alone is enough for Cloudflare to go â€œnopeâ€. \n\nIâ€™m not trying to abuse anything, just need a reliable way to fetch this page for internal processing. \n\nBefore I overâ€‘engineer this or move to paid services, Iâ€™m curious: Is scraping from a VPS basically doomed with Turnstile? Have people had better luck running this kind of thing from a â€œrealâ€ environment? Or is the honest answer: if Turnstile is there, automation just isnâ€™t welcome? Would love to hear how others have dealt with this in practice. \n\nThanks ðŸ™",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qzbk1v/turnstile_keeps_blocking_my_daily_scraper_any_help/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4a0sda",
          "author": "LessBadger4273",
          "text": "Have you tried with residential proxies? \n\nAlso, does the data requires JS rendering? Perhaps you could get the data way easier with curl_cffi only",
          "score": 3,
          "created_utc": "2026-02-08 16:52:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4av1t7",
              "author": "No-Card-2312",
              "text": "I havenâ€™t tried residential proxies, and this is actually the first time Iâ€™ve heard about them.\n\nThe data I need doesnâ€™t require JavaScript rendering at all. I only need to scrape the HTML, extract the href value for a PDF link, and then retrieve that value.",
              "score": 1,
              "created_utc": "2026-02-08 19:15:15",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4aw5pf",
              "author": "No-Card-2312",
              "text": "I looked into residential proxies, but the client wonâ€™t pay ðŸ¤«\n\nIâ€™m hoping for a free option or something with a free trial that doesnâ€™t ask for a payment method.\n\nMy crawler is tiny and only runs three times a day.",
              "score": 0,
              "created_utc": "2026-02-08 19:20:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4bcu7g",
          "author": "FinalDescription6553",
          "text": "You could use this to try and solve the Cloudflare challenge\n\nhttps://github.com/FlareSolverr/FlareSolverr",
          "score": 3,
          "created_utc": "2026-02-08 20:43:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4bfqp0",
          "author": "OkTry9715",
          "text": "Well try it on your own home IP address, if it works. Then you for sure know that it is because of VPS and not your script. Then you can create private VPN from your or your clients network if you do not want to pay for residential VPN. Or cheapest option is to create VPN from mobile internet SIM card from your local phone service provider. There are already routers that allows you to run private VPN on them. Mikrotik is one, but you need to learn how to configure it, there are probably plug and play solutions too.",
          "score": 3,
          "created_utc": "2026-02-08 20:58:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49jaxj",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-08 15:25:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49jm4m",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 3,
              "created_utc": "2026-02-08 15:27:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4b116x",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-08 19:44:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4bc9ax",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-08 20:40:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4iz2rd",
          "author": "SharpRule4025",
          "text": "Its almost certainly the VPS IP. Cloudflare maintains lists of datacenter IP ranges and flags them differently from residential. Even a perfect browser fingerprint from a known datacenter block gets challenged.\n\nCheapest fix for one page per day is probably a WireGuard tunnel from your VPS to your home network. Request goes out through a residential IP, Turnstile doesnt trigger. No need to pay for residential proxies for a single daily fetch.",
          "score": 1,
          "created_utc": "2026-02-10 00:03:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pcjzn",
          "author": "Ralphc360",
          "text": "The Main problem when scraping from a VPS is going to be the data center IP. Find a high quality proxy provider and give it a try",
          "score": 1,
          "created_utc": "2026-02-10 23:06:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}