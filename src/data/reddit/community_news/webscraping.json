{
  "metadata": {
    "last_updated": "2026-01-05 02:46:09",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 14,
    "total_comments": 65,
    "file_size_bytes": 79542
  },
  "items": [
    {
      "id": "1q0fqza",
      "title": "Deploying scrapers",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q0fqza/deploying_scrapers/",
      "author": "async-lambda",
      "created_utc": "2025-12-31 15:34:08",
      "score": 15,
      "num_comments": 31,
      "upvote_ratio": 0.86,
      "text": "I know this is, asking a question in very bad faith. I'm a student and I dont have money to spend.\n\nIs there a way I can deploy a headless browser for free? what i mean to ask is, having the convenience to hit an endpoint, and for it to run the scraper and show me results. Its just for personal use. Any services that offer this- or have a generous free tier?\n\nI can learn/am willing to learn stacks, am familiar with most web driver runners selenium/scrapy/playwright/cypress/puppeteer.\n\nThanks for reading \n\nEdit: tasks that I require are very minimal, 2-3 requests per day, with a few button clicks ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q0fqza/deploying_scrapers/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwxgh41",
          "author": "v_maria",
          "text": "Nothing is free. You can host on your own computer for the price of electricity",
          "score": 14,
          "created_utc": "2025-12-31 15:41:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxgijg",
          "author": "No-Appointment9068",
          "text": "Pick up an old laptop for free or get a raspberry pi or something?",
          "score": 7,
          "created_utc": "2025-12-31 15:41:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxmr8e",
          "author": "divided_capture_bro",
          "text": "GitHub Actions with public repo.",
          "score": 4,
          "created_utc": "2025-12-31 16:12:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxkdeh",
          "author": "816shows",
          "text": "Don't spin up a full ec2 instance for something you run 2-3 times a day.  Instead build an AWS lambda that you can either trigger on demand via an API gateway call or tie to an Eventbridge schedule. The container that you deploy in the lambda is quite simple, you don't need a lot of sophisticated layers just to have selenium and your script to work.  Hit me up if you want a sample Dockerfile and config details.",
          "score": 3,
          "created_utc": "2025-12-31 16:00:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy126g",
              "author": "Ordinary-Coconut7752",
              "text": "hey, would you mind sending me your docker and config files? Wanna try to deploy my scrapers on Lambda",
              "score": 1,
              "created_utc": "2025-12-31 17:23:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwya8sh",
                  "author": "816shows",
                  "text": "Done!",
                  "score": 1,
                  "created_utc": "2025-12-31 18:08:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwyexmy",
              "author": "73tada",
              "text": "Same here, please! I need an excuse to learn lambda!",
              "score": 1,
              "created_utc": "2025-12-31 18:32:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwypzg5",
                  "author": "816shows",
                  "text": "Sure thing - I added some notes on the configuration and for the uninitiated, you will have to create the new function based on a container image. This means you'll also have to get the Elastic Container Registry setup, and the rest of the details are outlined in this repo:  \n  \n[https://github.com/816shows/public/tree/main/selenium-lambda](https://github.com/816shows/public/tree/main/selenium-lambda)",
                  "score": 2,
                  "created_utc": "2025-12-31 19:28:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwyi4mx",
              "author": "pachinkomachine101",
              "text": "This sounds interesting, could you share it with me too? I'd love to study your setup!",
              "score": 1,
              "created_utc": "2025-12-31 18:48:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxino6",
          "author": "yangshunz",
          "text": "You can run Puppeteer on Vercel functions",
          "score": 2,
          "created_utc": "2025-12-31 15:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxreyz",
          "author": "RandomPantsAppear",
          "text": "AWS has a free tier for EC2, micro instances I believe.¬†\n\nFor very low cost you can also use a lambda outside of a vpc I believe (to dodge nat and internet gateway costs). Highly recommend Zappa for flask/django. Don‚Äôt forget to protect those endpoints.¬†\n\nAnother super low cost setup that I haven‚Äôt implemented but could work is an ec2 trigger on S3 uploads. Make the file your url list. Make the upload trigger a script that launches a small fargate instance that shuts down when it‚Äôs done. 0.25 vcpu and 256m of ram. That will cost you less than 2 cents per hour.¬†",
          "score": 2,
          "created_utc": "2025-12-31 16:35:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxhyd3",
          "author": "goranculibrk",
          "text": "Amazon should have free tier with some ec2 micro instance. Maybe look into that?",
          "score": 1,
          "created_utc": "2025-12-31 15:48:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxv4v6",
          "author": "Low-Clerk-3419",
          "text": "You can easily deploy something like that in vercel functions, fly.io, railway etc. but you have to keep in mind those free tiers are not meant for scraping; it will be very slow and limited experience.",
          "score": 1,
          "created_utc": "2025-12-31 16:54:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy61v9",
          "author": "Daniel_triathlete",
          "text": "Can‚Äôt you handle this task with JDownloader and an old laptop?",
          "score": 1,
          "created_utc": "2025-12-31 17:48:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy70bv",
          "author": "Intelligent_Area_135",
          "text": "Just wrap an endpoint around it and run it on your computer",
          "score": 1,
          "created_utc": "2025-12-31 17:52:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy79vv",
              "author": "Intelligent_Area_135",
              "text": "I use express js as the api around my web scraper, but if you are concerned about your ip getting banned or something, I have deployed to gcp and it‚Äôs very cheap but I think there might be better options",
              "score": 1,
              "created_utc": "2025-12-31 17:54:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwybc31",
          "author": "_i3urnsy_",
          "text": "I think GitHub Actions can do this for free if you are cool with the repo being public",
          "score": 1,
          "created_utc": "2025-12-31 18:14:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwycbfh",
          "author": "automationexperts",
          "text": "Try here https://www.pythonanywhere.com/",
          "score": 1,
          "created_utc": "2025-12-31 18:18:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx104rn",
          "author": "hatemjaber",
          "text": "Oracle has a free tier, 4 CPU and 24 GB ram. You can create a couple VMs.",
          "score": 1,
          "created_utc": "2026-01-01 03:40:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6tw33",
          "author": "Rorschache00714",
          "text": "Github offers a student essentials package with a shit ton of free resources. Look into that if you have a school email you can use.",
          "score": 1,
          "created_utc": "2026-01-02 02:59:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8237q",
          "author": "Training-Dinner3340",
          "text": "Depending on what you‚Äôre doing, Cloudflare Workers + Browser Rendering may get the job done:\nhttps://developers.cloudflare.com/browser-rendering/\n\nFree tier is okay:\nhttps://developers.cloudflare.com/browser-rendering/pricing/\n\nChatGPT is decent at writing Cloudflare worker scripts. Good luck!",
          "score": 1,
          "created_utc": "2026-01-02 08:37:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9kwa4",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-02 15:24:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9rg0u",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-02 15:55:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxbchys",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-02 20:23:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxckia9",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-03 00:09:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdklgv",
          "author": "Foodforbrain101",
          "text": "I could have sworn that GitHub Actions previously had a \na certain amount of free minutes even for private repos. \n\nRegardless, you can also take a look at using Azure DevOps' equivalent, Azure Pipelines, with 1800 minutes free per month, 60 min per run max for private repos but you have to request it (which is fairly easy and quick).\n\nIf you do go with Azure Pipelines, I suggest using the Microsoft Playwright for Python container image for your pipeline. There's ways to make this setup more metadata-driven too, as you can parameterize Azure Pipelines and use the Azure DevOps API to trigger runs, and you can easily tack on Azure Logic Apps (4000 free actions per month) as a simple orchestrator, use any kind of blob or table storage (even Google Drive) to store and fetch your metadata table containing the schedules and info about which scripts to run. Might be overkill for your needs though, but it's honestly one of the easiest and cheapest ways I've found to run headless browsers.",
          "score": 1,
          "created_utc": "2026-01-03 03:38:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxe6d6k",
          "author": "BeigeBolt",
          "text": "What is most money made by scraping",
          "score": 1,
          "created_utc": "2026-01-03 06:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxnc8d5",
          "author": "Ok_Sir_1814",
          "text": "People saying it costs money when you can use Google colabs and run the script whenever you need to execute the scrapping.",
          "score": 1,
          "created_utc": "2026-01-04 16:30:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxo71py",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-04 18:48:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxol7ag",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-04 19:51:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q1y5gh",
      "title": "Is human-like automation actually possible today",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q1y5gh/is_humanlike_automation_actually_possible_today/",
      "author": "learning_linuxsystem",
      "created_utc": "2026-01-02 13:07:07",
      "score": 11,
      "num_comments": 10,
      "upvote_ratio": 0.87,
      "text": "I‚Äôm trying to understand the limits of collecting publicly available information from online platforms (social networks, professional networks, job platforms, etc.), especially for **OSINT, market analysis, or workforce research**.\n\nWhen attempting to collect data directly from platforms, I quickly run into **behavioral detection systems**. This raises a few fundamental questions for me.\n\nAt an intuitive level, it seems possible to:\n\n* add randomness (scrolling, delays, mouse movement),\n* simulate exploration instead of direct actions,\n* or hide client-side activity,\n\nand therefore make an automated actor look human.\n\nBut in practice, this approach seems to break down very quickly.\n\nWhat I‚Äôm trying to understand is **why**, and whether people actually solve this problem differently today.\n\nMy questions are:\n\n1. **Why doesn‚Äôt adding randomness make automation behave like a real human?** What parts of human behavior (intent, context, timing, correlation) are hard to reproduce even if actions look human on the surface?\n2. **What do modern platforms analyze beyond basic signals like IP, cookies, or user-agent?** At a conceptual level, what kinds of behavioral patterns make automation detectable?\n3. **Why isn‚Äôt hiding or masking client-side actions enough?** Even if visual interactions are hidden, what timing or state-level signals still reveal automation?\n4. **Is this problem mainly technical, or statistical and economic?** Is human-like automation theoretically possible but impractical at scale, or effectively impossible in real-world conditions?\n5. **From an OSINT perspective, how is platform data actually collected today?**\n   * Do people still use automation in any form?\n   * Do they rely more on aggregated or secondary data sources?\n   * Or is the work mostly manual and selective?\n6. **Are these systems truly being ‚Äúbypassed,‚Äù or are people simply avoiding platforms and using different data paths altogether?**\n\nI‚Äôm not looking for instructions on bypassing protections.  \nI want to understand **how behavioral detection works at a high level**, what it can and cannot infer, and what **realistic, sustainable approaches** exist if the goal is insight rather than evasion.\n\n**Note:**  \nSorry in advance ‚Äî I used AI assistance to help write this question. My English isn‚Äôt strong enough to clearly express technical ideas, but I genuinely want to understand how these systems work.",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1q1y5gh/is_humanlike_automation_actually_possible_today/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nxe49t2",
          "author": "RandomPantsAppear",
          "text": "You don't have to deal with mouse movement, because people own tablets and tablets don't have mice, they just jump from destination to destination.\n\nTime is the new currency. Spin up profiles that have browsing history and months behind them, and you'll be doing great. Spin up accounts with these, and you'll be doing even better.\n\nVerifiability is second only to time. Link that identity with emails, with addresses, with names, and you will almost never have problems if you can scale.\n\nAlso it's absurdly easy to detect bot traffic. What's hard is detecting it before the page loads, at scale, within an acceptable delay. Hindsight is 20/20.\n\nIn 10 years, we will all basically be professional schizophrenics, with infinite personalities jealously guarded, curated, protected, and used.",
          "score": 9,
          "created_utc": "2026-01-03 05:52:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfaakv",
              "author": "Soren_Professor",
              "text": "I couldn't have said it better",
              "score": 2,
              "created_utc": "2026-01-03 11:47:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxla243",
              "author": "RobSm",
              "text": "> Also it's absurdly easy to detect bot traffic\n\nHow do you know? Maybe half of the 'human traffic' you see in your server are also bots, you just don't detect them? 0_o",
              "score": 1,
              "created_utc": "2026-01-04 07:51:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxllqsc",
              "author": "polygraph-net",
              "text": "> it's absurdly easy to detect bot traffic\n\nThis isn't true. I'm a bot detection researcher and many modern bots are extremely difficult to detect.\n\nAre you sure you're detecting stealth bots and not just basic things like puppeteer, playwright, selenium, etc.?",
              "score": 1,
              "created_utc": "2026-01-04 09:37:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxls6ga",
                  "author": "learning_linuxsystem",
                  "text": "Out of curiosity, how do you actually detect the more advanced, stealthy bots you mentioned? What gives them away?",
                  "score": 1,
                  "created_utc": "2026-01-04 10:35:09",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nxov1vf",
                  "author": "RandomPantsAppear",
                  "text": "I feel like I‚Äôm being misunderstood a bit, I could have worded this better. Your field is incredibly challenging. \n\nThe reason I said bot detection is easy after the fact, is that you then have the time needed to analyze behavioral traits and complex data, and for the relevant data to exist in the first place. \n\nOnce the data exists, it‚Äôs easy to spot if you want a quarterly summary of ‚Äúwho was a bot‚Äù. But this is different than blocking bots in real time. \n\nLet‚Äôs say I load Facebook, login, and start looking for group names with the goal of finding all relevant groups related to {keyword_list}. \n\nLoading Facebook is normal, logging in is normal, triggering the autocomplete is normal. \n\nWhat is abnormal, is that every time I login, I go immediately to the groups tab and trigger this autocomplete. Or that I spend X time on the groups page, vs Y on the feed. Or one of many non behavioral indicators (mismatch between events firing and device type, etc)\n\nThat takes time to develop, and resources to summarize individually. It takes even more resources to summarize what ‚Äúnormal‚Äù is.\n\nThere are not many places that can do this in realtime, for every page load, before sending response headers, or before the bots have already done significant work.",
                  "score": 1,
                  "created_utc": "2026-01-04 20:36:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxfeqg5",
              "author": "learning_linuxsystem",
              "text": "Thanks for the perspective ‚Äî I‚Äôll take this into account and rethink what‚Äôs realistically possible given the time and identity side of things. Appreciated.",
              "score": 0,
              "created_utc": "2026-01-03 12:22:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pz1zvp",
      "title": "Scraping reddit?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pz1zvp/scraping_reddit/",
      "author": "AdhesivenessEven7287",
      "created_utc": "2025-12-29 23:39:13",
      "score": 7,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Over time I save up pages of articles and comments I think will be interesting. But I've not gotten around to it yet. \n\nHow can I have the links but easily download the page? Baring in mind to view all comments I need to scroll down the page. ",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1pz1zvp/scraping_reddit/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwmyser",
          "author": "g4m3r1",
          "text": "If you just want to scrape the actual content of the reddit post then its quite easy. Just add .json at the end of the URL and reddit will return the content of the post + all comments as beautifully formatted json.\n\nE.g. Try it with your own post here: [https://www.reddit.com/r/webscraping/comments/1pz1zvp/scraping\\_reddit.json](https://www.reddit.com/r/webscraping/comments/1pz1zvp/scraping_reddit.json)",
          "score": 9,
          "created_utc": "2025-12-29 23:50:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwn0m6n",
              "author": "PresidentHoaks",
              "text": "Lol mobile sends me back to this page, will have to check on my lappy",
              "score": 2,
              "created_utc": "2025-12-30 00:00:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwn3ws2",
                  "author": "g4m3r1",
                  "text": "Never tried it on mobile but on desktop this works fine :).",
                  "score": 1,
                  "created_utc": "2025-12-30 00:18:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pzyqo0",
      "title": "open-source userscript for google map scraper (it works again)",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pzyqo0/opensource_userscript_for_google_map_scraper_it/",
      "author": "Asleep-Patience-3686",
      "created_utc": "2025-12-31 00:33:36",
      "score": 7,
      "num_comments": 8,
      "upvote_ratio": 0.9,
      "text": "I built this script about six months ago, and it worked well until two months ago when it suddenly stopped functioning. I spent the entire night yesterday and finally resolved the issue.\n\n  \nFunctionality:\n\n1. Automatically scroll to load more results\n2. Retrieve email addresses and Plus Codes\n3. Export in more formats\n4. Support all subdomains of Google Maps sites.\n\nChange logs:\n\n1. The collection button cannot be displayed due to the Google Maps UI redesign.\n2. The POI request data cannot be intercepted.\n3. Added logs to assist with debugging.\n\n[https://greasyfork.org/en/scripts/537223-google-map-scraper](https://greasyfork.org/en/scripts/537223-google-map-scraper)\n\nJust enjoy with free and unlimited leads!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1pzyqo0/opensource_userscript_for_google_map_scraper_it/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwvj8to",
          "author": "Training_Hand_1685",
          "text": "Wow, something like this is exactly what I searched Reddit for. So I can search for non-profits and housing programs in different states (that are listed on google maps) with this? For free? Do I need to be technically savvy (know how to web scrape) or can a newbie/end user, use this?",
          "score": 1,
          "created_utc": "2025-12-31 06:52:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwx6520",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2025-12-31 14:47:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxn57fq",
                  "author": "webscraping-ModTeam",
                  "text": "ü™ß Please review the sub rules üëâ",
                  "score": 1,
                  "created_utc": "2026-01-04 15:57:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx0yra9",
              "author": "Asleep-Patience-3686",
              "text": "I set up a GitHub repository. It will guide you on how to use it. [https://github.com/webAutomationLover/google-map-scraper](https://github.com/webAutomationLover/google-map-scraper)",
              "score": 1,
              "created_utc": "2026-01-01 03:30:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxitnel",
          "author": "Twitty-slapping",
          "text": "Where are you getting the email address lol ?",
          "score": 1,
          "created_utc": "2026-01-03 22:45:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmg3b8",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-04 13:41:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxn593z",
                  "author": "webscraping-ModTeam",
                  "text": "ü™ß Please review the sub rules üëâ",
                  "score": 1,
                  "created_utc": "2026-01-04 15:57:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q2pu29",
      "title": "[Hiring] Looking for Automation Expert ‚Äì Paid",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q2pu29/hiring_looking_for_automation_expert_paid/",
      "author": "pageforsource",
      "created_utc": "2026-01-03 08:53:07",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 0.77,
      "text": "Hey everyone,\n\nI‚Äôm working on a personal web automation project (Node.js‚Äìbased) where I need to automate interactions on a few modern websites for data processing / internal tooling purposes.\n\nThe automation involves:\n\nHeadless / real browser automation \n\nHandling anti-bot protections\n\nSolving or bypassing captchas.\n\nRequirements:\nComfortable working with Node.js automation stacks \n\nDm for more details",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1q2pu29/hiring_looking_for_automation_expert_paid/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q09vdc",
      "title": "Bypassing DataDome",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q09vdc/bypassing_datadome/",
      "author": "Vlad_Beletskiy",
      "created_utc": "2025-12-31 10:25:38",
      "score": 7,
      "num_comments": 7,
      "upvote_ratio": 0.77,
      "text": "Hello, dear community!\n\nI‚Äôve got an issue being detected by DataDome (403 status) while scraping a big resource.\n\n\n\n**What works**\n\nI use Zendriver pointing to my local MacOS Chrome. Navigating to site‚Äôs main page -> waiting for the DataDome endpoint that returns DataDome token -> making subsequent requests via curl\\_cffi (on my local MacOS machine) with that token being sent as a DataDome cookie.  \nI‚Äôve checked that this token lives quite long - is valid for at least several hours, but assume even more (managed to make requests after multiple days).\n\n\n\n**What I want to do that doesn‚Äôt work**\n\nI want to deploy it and opted for Docker. Installed Chrome (not Chromium) within the Docker. Tried the same algorithm as above. The outcome is that I‚Äôm able to get token from the DataDome endpoint. But subsequent curl\\_cffi requests fail with 403. Tried curl\\_cffi requests from Docker and locally - both fail, issued token is not valid.\n\nNext thing I‚Äôve enabled xvfb that resulted in a bit better outcome. Namely, after obtaining the token the next request via curl\\_cffi succeeds, while subsequent ones fail with 403. So, it‚Äôs basically single use.\n\nNext I‚Äôve played with different user agents, set timezone, but the outcome is the same.\n\n\n\nOne more observation - there‚Äôs another request which exposes DataDome token via Set-Cookie response header. If done with Zendriver under Docker, Set-Cookie header for that same endpoint is missing.\n\nSo, my assumption is that my trust score by DataDome is higher than to show me captcha, but lower than to issue a long-living token.\n\n\n\nAnd one more observation - both locally and under Docker requests via curl\\_cffi work with 131st Chrome version being impersonated. Though, 143rd latest Chrome version is used to obtain this token. Any other curl\\_cffi impersonation options just don‚Äôt work (result in 403). Why does that happen?\n\nAnd I see that curl\\_cffi supports impersonation of the following OSes only: Win10, MacOS (different versions), iOS. So, in theory it shouldn‚Äôt work at all combined with Docker setup?\n\n\n\n**Question** \\- could you please point me in the right direction what to investigate and try next. How do you solve such deployment problems and reliably deploy scraping solutions? And probably you can share advice how to enhance my DataDome bypassing strategy?\n\n\n\nThank you for any input and advices!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q09vdc/bypassing_datadome/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwwa7vq",
          "author": "infaticaIo",
          "text": "DataDome tokens are usually bound to more than just a cookie. They often get tied to the full client ‚Äúshape‚Äù (TLS and HTTP2 fingerprint, IP reputation, timing, browser signals, sometimes even local storage) so a token minted in one environment can be useless in another. That explains why local Mac works but Docker fails, and why you see ‚Äúsingle use‚Äù behavior.\n\nWhat to investigate, at a high level:\n\n* Fingerprint consistency: the environment that mints the token needs to match the environment that reuses it. If you mint in a real Chrome and replay with curl, any mismatch in TLS or HTTP2 can invalidate quickly.\n* IP consistency: tokens can be scoped to IP or ASN. Local IP vs Docker egress often differs even on the same machine if you run through different routes.\n* Header and cookie jar completeness: missing Set-Cookie under Docker usually means the JS flow or redirects differ, or a required request wasn‚Äôt executed the same way.\n* Version coupling: the fact that only one curl\\_cffi impersonation works suggests the backend is keying on a very specific TLS stack and ordering.\n\nFor deployment, the reliable pattern is usually to keep the whole flow in one place. Either keep requests inside the same browser context that earned the session, or run the replay client with a fingerprint that is as close as possible to that browser and network path. Mixing ‚Äúreal browser to get token‚Äù with a very different HTTP client is where these systems tend to break.\n\nIf this is for a legitimate use case, the sustainable option is getting approved access or using an official feed. Trying to ‚Äúenhance bypass‚Äù is a cat and mouse game and will keep changing.",
          "score": 6,
          "created_utc": "2025-12-31 11:03:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwww0zk",
              "author": "Vlad_Beletskiy",
              "text": "Thank you.\n\nIt seems that IP binding doesn't matter that much for cookie issuing. Because I've tried to obtain DataDome cookie token both with & without proxies (running locally without Docker). And then use via curl\\_cffi for subsequent requests. And it works regardless of the proxy presence while obtaining the token.\n\nInteresting point here - I've obtained cookie using different Chrome version and different OS (MacOS) version compared to that used subsequently during curl\\_cffi impersonation, and that 131rd version still worked. However should have failed. That still seems strange.\n\n  \n\"keep requests inside the same browser context that earned the session\" - yeah, was thinking similarly.",
              "score": 1,
              "created_utc": "2025-12-31 13:49:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwlfqv",
          "author": "Dismal_Pilot_1561",
          "text": "Personally, it works great for me in Docker using a fairly heavy Linux base image, but one that helps boost my Datadome trust score.\n\nJust like you, I first warm up the proxy using a real automated browser combined with a custom captcha solver. Then, I use curl_cffi with the cookies generated by the actual browser, and I save any new cookies if they get updated (which happens quite often).\n\nThe main difference is probably that I'm forced to solve a captcha (not the main page), which significantly increases the Datadome trust score. Also, I make sure to use the correct cookie data and headers to mimic the browser I used as closely as possible.\n\nI use this method for high-frequency scraping. Without pushing it too hard and on a fairly modest machine, I scrape about 15,000 URLs in 4 hours.",
          "score": 2,
          "created_utc": "2025-12-31 12:38:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx15tfk",
          "author": "_mackody",
          "text": "Look at JA3",
          "score": 1,
          "created_utc": "2026-01-01 04:19:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhflci",
          "author": "abdullah-shaheer",
          "text": "It might be then an IP issue, maybe datacenter IP. The second thing is to use pydoll python, go to the website, make 100 or 200 requests using its inbuilt request maker that copies everything from the browser that is already opened, then do some random actions, send 200 to 400 requests again and this way, you can scale up. You can also integrate different residential proxies for doing this in scale. This is far better than copying the token and sending it. The website might be blocking you via some other way other than TLS fingerprinting, if you use pydoll, it will mimic everything from the actual session you're using and requests will never be rejected most probably. I personally have tried this method.",
          "score": 1,
          "created_utc": "2026-01-03 18:43:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxi4t9f",
          "author": "GillesQuenot",
          "text": "Like @Dismal_Pilot_1561, I use my own Datadome captcha solver which works pretty well.\n\nI use an automated browser to scrape Datadome websites. If you have a pool of resid IP, you can even avoid the use to solve captchas.\n\nHave you checked that the version match between Chrome and curl_ffi ?",
          "score": 1,
          "created_utc": "2026-01-03 20:43:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxip56m",
              "author": "Twitty-slapping",
              "text": "You mean you built your own CAPTCHA solver?",
              "score": 1,
              "created_utc": "2026-01-03 22:23:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q1uq5w",
      "title": "Turnstiles, geetest, automation in Rust?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q1uq5w/turnstiles_geetest_automation_in_rust/",
      "author": "Normal-Middle3719",
      "created_utc": "2026-01-02 09:55:40",
      "score": 7,
      "num_comments": 3,
      "upvote_ratio": 0.74,
      "text": "Hey guys,\n\nI‚Äôve been benefiting from the open-source projects here for a while, so I wanted to give back. I‚Äôm a big fan of compiled languages, and I needed a way to handle browser tasks (specifically CAPTCHAs) in Rust without getting flagged.\n\nI forked chromiumoxide and ported the stealth patches from `rebrowser` and `puppeteer-real-browser`. I also built dedicated solvers for Cloudflare and GeeTest.\n\nüß™ The Proof (Detection Results)\n\nI‚Äôve tested this against common scanners and it‚Äôs passing:\n\n* **Intoli / WebDriver Advanced:** Passed (WebDriver hidden, Permissions default).\n* **Fingerprint Scanner:** `PHANTOM_UA`, `PHANTOM_PROPERTIES`, and `SELENIUM_DRIVER` all return **OK**.\n* **Canvas/WebGL:** Properly spoofing Google Inc. (NVIDIA) with no broken dimensions.\n* **Stack Traces:** `PHANTOM_OVERFLOW` depth and error names match real Chrome behavior.\n\nüõ† The Repos\n\n* [**chaser-oxide**](https://github.com/ccheshirecat/chaser-oxide)‚Äì Chromiumoxide fork with stealth/impersonation patches.\n* [**chaser-cf**](https://github.com/ccheshirecat/chaser-cf)‚Äì Rust implementation for Cloudflare Turnstile.\n* [**chaser-gt**](https://github.com/ccheshirecat/chaser-gt)‚Äì GeeTest solver using deobfuscation (via `rquests`/`curl_cffi`).\n\n**Note:** I shipped these with **C FFI bindings**, so you can use them in **Python, Go, or Node** if you just want the Rust performance/stealth without writing Rust code. I personally prefer this over managing a separate microservice.\n\nüí¨ Curious about your workflows:\n\n1. **Third-party APIs:** For those using paid solvers (Capsolver, etc.), is it for the convenience, or because you don't want to maintain stealth patches yourself?\n2. **Scraping Use Cases:** What are you guys actually building? I‚Äôll go first: I‚Äôm overengineering automation for crypto casinos because I found some gaps in their flow lol.\n3. **Differentiators:** What actually makes a solver \"good\" in 2026? Is it raw solve speed, or just the success rate on high-entropy challenges?\n\nIt‚Äôs still early, so feel free to contribute, roast my code, or reach out to collaborate. Happy New Year!",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1q1uq5w/turnstiles_geetest_automation_in_rust/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nxa1jbi",
          "author": "TinyBeing8001",
          "text": "this is cool and thanks for it\n\nit might help spacing your post content out so it‚Äôs more consumable",
          "score": 1,
          "created_utc": "2026-01-02 16:43:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbbgrl",
              "author": "Normal-Middle3719",
              "text": "thanks for the feedback! took your advice and updated the post",
              "score": 1,
              "created_utc": "2026-01-02 20:18:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxlqjis",
          "author": "uzzalhme",
          "text": "Thanks for your awesome work , im also interested to know about the crypto casino gaps .",
          "score": 1,
          "created_utc": "2026-01-04 10:20:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pz8czz",
      "title": "Is it just me or playwright incredibly unstable",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pz8czz/is_it_just_me_or_playwright_incredibly_unstable/",
      "author": "kilobrew",
      "created_utc": "2025-12-30 04:24:02",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 0.8,
      "text": "I‚Äôve been using playwright in the AWS environment and having nothing but trouble getting it to run without randomly disconnecting, ‚Äúfailed to get world‚Äù, or timeouts that really shouldn‚Äôt have happened. Hell, Even running AWS‚Äôs SAAS bedrock agent_core browser tool has the same issue. \n\nIt seems the only time I can actually use it is if it‚Äôs installed on a full blown windows install with a GPU. \n\n\nIs it just me?\n",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1pz8czz/is_it_just_me_or_playwright_incredibly_unstable/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwoupx0",
          "author": "RandomPantsAppear",
          "text": "I ran multiple playwright instances on Fargate instances with 0.25 vcpu and 256m of ram, that were also running redis and celery. \n\nSomething is very wrong if this is the behavior you are getting. \n\n\nHow are you detecting the page load completion success/fail?\n\nHave you checked the process list to make sure processes are successfully exiting?\n\nAre you taking screenshots on the theoretical page load fails? (I am not sure how this works headless, I often run it with xvfb)",
          "score": 1,
          "created_utc": "2025-12-30 06:42:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpm1qz",
          "author": "bluemangodub",
          "text": "Not my experience at all. I have had untold issues with AWS, especially the low price instances. \n\nRun it locally, or on a proper VPS and see if you have the same issues. If not, it's AWS",
          "score": 1,
          "created_utc": "2025-12-30 10:52:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpnb6t",
          "author": "Lafftar",
          "text": "Are you using proxies?",
          "score": 1,
          "created_utc": "2025-12-30 11:04:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzhjmn",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pzhjmn/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2025-12-30 13:01:09",
      "score": 5,
      "num_comments": 8,
      "upvote_ratio": 0.79,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levels‚Äîwhether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) üå±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1pzhjmn/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nx78umh",
          "author": "convicted_redditor",
          "text": "# Created 3 web scraping pypi libs on python. Any way to monetise?\n\n[](https://www.reddit.com/r/webscraping/?f=flair_name%3A%22Scaling%20up%20%F0%9F%9A%80%22)In 2025, I developed 3 pypi libs around webscraping-\n\n1. stealthkit - wrapper over curl\\_cffi with human-like fingerprinting with header rotations and cookie management.\n2. amzpy - built on top of curl\\_cffi (but before stealthkit), scrapes amazon search and product data.\n3. pnsea - built over stealthkit to scrape stock exchange data of India (NSE).\n\nReason to build them was for my personal usage as I developed an amazon related web app last year so I built amzpy. I was building a lot streamlit data based apps (and more) to play with NSE data - like options chain, insider data, etc.\n\nHow can I monetise this skill? Should I build FastAPI and turn into saas?\n\nHow do you guys monetise your web scraping skills?",
          "score": 2,
          "created_utc": "2026-01-02 04:36:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvep5p",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2025-12-31 06:14:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvsb8v",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2025-12-31 08:14:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx082gy",
          "author": "LessBadger4273",
          "text": "Here ya go, if someone can help me with this ‚Äî Libs such as ‚Äúnodriver‚Äù seems to be able to completely bypass some antibots like shopee.* ones that also requires js rendering. I guess this is because you are basically using your browser ‚Äúas is‚Äù, without any automation flag, right? \n\nIf so, why it‚Äôs so hard to replicate this at scale using residential proxies? My guess is that once you move this to AWS ec2, for example, those antibots can detect you are in a vm environment and block you, right? Would it be be possible to run this at scale by having an in house farm of old desktops/laptops? Or maybe using some rdp tools? Is it a price constraint that we are not able to bypass these antibots at scale or am I missing something?",
          "score": 1,
          "created_utc": "2026-01-01 00:36:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1cfkv",
              "author": "QuinsZouls",
              "text": "Shopee antibot is heavily dependent of the hardware fingerprint  + ip, I have succeeded experience using a local farm of macbook devices that run google chrome , usually th vm can easily  detected by proof of work and webgl + canvas fingerprint. \nAlso I have succeeded with some cloud VM instances with a GPU",
              "score": 1,
              "created_utc": "2026-01-01 05:12:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx88x0k",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-02 09:42:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8upl6",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-01-02 12:50:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q12pcv",
      "title": "Scraping in Google Scholar",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q12pcv/scraping_in_google_scholar/",
      "author": "Cuaternion",
      "created_utc": "2026-01-01 12:02:19",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 0.86,
      "text": "Hi, I'm trying to do scraping with some academic profiles in Google Scholar, but maybe the server has restrictions for this activity.\nAny suggestions?\nThanks",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q12pcv/scraping_in_google_scholar/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nx60svn",
          "author": "bootlegDonDraper",
          "text": "hey OP\n\nyou'll hit rate limits everywhere when web scraping, but it's easy to get through\n\n**first solution**, throttle your requests and add random delays between requests.\n\n**second,** instead of scraping it in one go, create a scraper that scrapes a chunk of URLs every hour or so with the rate limiting in first solution\n\n*you don't want to wait?*\n\n**third and most effective,** rotate proxies. if you use a large proxy pool you can run concurrent requests to scrape tens of pages at once without ever being rate limited.\n\nif your proxies are low quality DC proxies, your requests will get blocked. if more than half of your requests aren't blocked, introduce error handling to re-request the same page with another ip if it gets blocked.\n\nvoila",
          "score": 4,
          "created_utc": "2026-01-02 00:07:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0atdm",
      "title": "Scraping market data CS2/CSGO",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q0atdm/scraping_market_data_cs2csgo/",
      "author": "Short_Bus_6284",
      "created_utc": "2025-12-31 11:24:18",
      "score": 5,
      "num_comments": 7,
      "upvote_ratio": 0.86,
      "text": "Good evening! Hope this is the right place to ask. I've reached a point where I need metadata and, especially, up to date prices for Counter Strike 2 skins. I understand that there are paid APIs and the Steam API that provide real-time metadata and prices, but to be honest, I‚Äôd prefer to go with free solutions. This brings me to scrapers, since I haven‚Äôt been able to find any free APIs that meet my needs. I‚Äôve dug through GitHub and found some repos, but most of them either don‚Äôt work with modern JavaScript heavy sites, or they only scrape limited metadata. The only repo I found that works well is¬†[this one](https://github.com/eovacius/csgodatabase-scraper/), which returns both prices and metadata fairly quickly. However, the project is missing some content, like souvenirs, stickers, cases, etc. It looks like it‚Äôs still pretty new, so I‚Äôm sure the content will be updated soon, but I don‚Äôt want to wait too long. So, I was hoping some of you might know of any resources or public databases/sites that would let me scrape CS2 skin information. Or, if there are any other free methods to get this info without scraping, that would be super helpful too. Thanks in advance!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q0atdm/scraping_market_data_cs2csgo/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nx6pmh6",
          "author": "scraping-test",
          "text": "*\"free methods to get this info without scraping\"* \n\ni can think of two: an intern... or divine intervention xd\n\n  \nto be real, the repo you've mentioned looks really well-built. I'm sure you can somewhat easily modify it to extract the additional stuff you want. if you don't know how, clone to cursor and get claude on it",
          "score": 3,
          "created_utc": "2026-01-02 02:34:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8ss17",
              "author": "Short_Bus_6284",
              "text": "Yes, config looks promising but unfortunately im not familiar with Golang. So i guess i would go with claude. Thanks",
              "score": 1,
              "created_utc": "2026-01-02 12:36:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5pmdf",
          "author": "_i3urnsy_",
          "text": "Is your issue with scraping the site or do you need a better data source to scrape info from?",
          "score": 1,
          "created_utc": "2026-01-01 23:04:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8spp9",
              "author": "Short_Bus_6284",
              "text": "Both scraper and target site are fine. Just need more data",
              "score": 2,
              "created_utc": "2026-01-02 12:36:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8k5jl",
          "author": "bluemangodub",
          "text": "you either figure out how to do it yourself, or you pay someone to do.\n\nIF you don't want to wait for the repo you are using, reach out to the dev and offer them money to compete the parts you are needing.",
          "score": 1,
          "created_utc": "2026-01-02 11:25:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8t214",
              "author": "Short_Bus_6284",
              "text": "I'll contant them, either we scale it together or i pay and they do the job. honestly might be cheaper than paid Api or scraper services.",
              "score": 1,
              "created_utc": "2026-01-02 12:38:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdz8vu",
          "author": "Afraid-Solid-7239",
          "text": "What information in specific are you looking to get that the current repo doesn't have?   \nThe files in /json/ of the repo seem fairly extensive? I'm able to find souvenirs and cases in some json files.\n\nThough the dev of that repo in specific, is just scraping csgodatabase.  \nIs the data you want available on csgodatabase.com? I don't mind writing up a scraper for it, for you.",
          "score": 1,
          "created_utc": "2026-01-03 05:14:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0u4vg",
      "title": "Monthly Self-Promotion - January 2026",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q0u4vg/monthly_selfpromotion_january_2026/",
      "author": "AutoModerator",
      "created_utc": "2026-01-01 03:00:42",
      "score": 4,
      "num_comments": 24,
      "upvote_ratio": 0.76,
      "text": "Hello and howdy, digital miners of¬†r/webscraping!\n\nThe moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!\n\n* Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?\n* Maybe you've got a ground-breaking product in need of some intrepid testers?\n* Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?\n* Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?\n\nWell, this is your time to shine and shout from the digital rooftops - Welcome to your haven!\n\nJust a friendly reminder, we like to keep all our self-promotion in one handy place, so any promotional posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q0u4vg/monthly_selfpromotion_january_2026/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nx1mea7",
          "author": "Jonathan_Geiger",
          "text": "I‚Äôm building [SocialKit](https://socialkit.dev) it‚Äôs a social media scraping API for extracting transcripts, summaries, stats, comments, and more from YouTube, TikTok, Instagram, and Shorts (:\n\nJust reached $700 in MRR!!",
          "score": 4,
          "created_utc": "2026-01-01 06:40:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx642u9",
              "author": "bootlegDonDraper",
              "text": "very cool! will need something like this for my personal project\n\ntwo things maybe as advice:\n\n**1** meta is veeery nosy about scraping, especially for instagram. i would expect to get an email from them as your business grows, and not invest heavily in instagram (publicly at least ;) ) but stay on youtube and tiktok side of things\n\n**2** i dont see any dedicated product for scraping social media ranking for \"tiktok comment scraper\". I would go from [https://www.socialkit.dev/tiktok-comments-api](https://www.socialkit.dev/tiktok-comments-api) this landing page to .../tiktok-comment-scraper-api and optimize for the scraper keyword as it has muuuch higher volume",
              "score": 1,
              "created_utc": "2026-01-02 00:25:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1h6ai",
          "author": "Bitter_Caramel305",
          "text": "Do people actually come here to read this?",
          "score": 3,
          "created_utc": "2026-01-01 05:52:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1syyp",
              "author": "Jonathan_Geiger",
              "text": "I did :)",
              "score": 2,
              "created_utc": "2026-01-01 07:44:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx21k2a",
                  "author": "Bitter_Caramel305",
                  "text": "And are you also going to buy other people's products/services?",
                  "score": 1,
                  "created_utc": "2026-01-01 09:15:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx1ymcy",
              "author": "indicava",
              "text": "Me too",
              "score": 1,
              "created_utc": "2026-01-01 08:44:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx5iki6",
              "author": "matty_fu",
              "text": "this thread gets 10k+ views each month, and can be helpful for SEO/GEO. also it helps the mod team get less abuse from people wanting to advertise their products - a win for everyone",
              "score": 1,
              "created_utc": "2026-01-01 22:25:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1h7oe",
          "author": "Bitter_Caramel305",
          "text": "# I can scrape that website for you\n\nHi everyone,  \nI‚Äôm Vishwas Batra, feel free to call me Vishwas.\n\nBy background and passion, I‚Äôm a full stack developer. Over time, project needs pushed me deeper into web scraping and I ended up genuinely enjoying it.\n\n**A bit of context**\n\nLike most people, I started with browser automation using tools like Playwright and Selenium. Then I moved on to crawlers with Scrapy. Today, my first approach is reverse engineering exposed backend APIs whenever possible.\n\nI have successfully reverse engineered Amazon‚Äôs search API, Instagram‚Äôs profile API and DuckDuckGo‚Äôs¬†`/html`¬†endpoint to extract raw JSON data. This approach is far easier to parse than HTML and significantly more resource efficient compared to full browser automation.\n\nThat said, I‚Äôm also realistic. Not every website exposes usable API endpoints. In those cases, I fall back to traditional browser automation or crawler based solutions to meet business requirements.\n\nIf you ever need clean, structured spreadsheets filled with reliable data, I‚Äôm confident I can deliver. I charge nothing upfront and only ask for payment once the work is completed and approved.\n\n**How I approach a project**\n\n* You clarify the data you need such as product name, company name, price, email and the target websites.\n* I audit the sites to identify exposed API endpoints. This usually takes around 30 minutes per typical website.\n* If an API is available, I use it. Otherwise, I choose between browser automation or crawlers depending on the site. I then share the scraping strategy, estimated infrastructure costs and total time required.\n* Once agreed, you provide a BRD or I create one myself, which I usually do as a best practice to stay within clear boundaries.\n* I build the scraper, often within the same day for simple to mid sized projects.\n* I scrape a 100 row sample and share it for review.\n* After approval, you provide credentials for your preferred proxy and infrastructure vendors. I can also recommend suitable vendors and plans if needed.\n* I run the full scrape and stop once the agreed volume is reached, for example 5000 products.\n* I hand over the data in CSV, Google Sheets and XLSX formats along with the scripts.\n\nOnce everything is approved, I request the due payment. For one off projects, we part ways professionally. If you like my work, we continue collaborating on future projects.\n\nA clear win for both sides.\n\nIf this sounds useful, feel free to reach out via¬†[LinkedIn](https://www.linkedin.com/in/vishwas-batra/)¬†or just send me a DM here.",
          "score": 1,
          "created_utc": "2026-01-01 05:52:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1ic2g",
          "author": "ConstIsNull",
          "text": "Building a JSON API for getting jobs listed on company career pages only, no LinkedIn or indeed. Saves you time instead of building your own job scraper. Check it out at jobven.com",
          "score": 1,
          "created_utc": "2026-01-01 06:02:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1tkco",
          "author": "Dear-Cable-5339",
          "text": "**Crawlbase Smart Proxy**¬†‚Äì scrape any public website without worrying about blocks, CAPTCHAs, or IP bans.  \nWorks great for e-commerce, SERPs, social platforms, and more.\n\nYou send the URL ‚Üí we handle proxies, rotation, retries, and geo.\n\nüëâ Try it free here:¬†[https://crawlbase.com/?s=5qGcKLCR](https://crawlbase.com/?s=5qGcKLCR)",
          "score": 1,
          "created_utc": "2026-01-01 07:50:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx20w02",
          "author": "Silentkindfromsauna",
          "text": "[Lindra.ai](www.lindra.ai) for turning any website into an api, still early but already works great for scraping",
          "score": 1,
          "created_utc": "2026-01-01 09:08:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx21jrx",
          "author": "Classic_Exam7405",
          "text": "We built the SOTA AI Web Agent rtrvr.ai for vibe scraping. Just prompt our agent to fill forms, extract data or monitor sites.\n\nScrape strict antibot sites like LinkedIn with our chrome extension.\n\nScale on our cloud/api platform",
          "score": 1,
          "created_utc": "2026-01-01 09:15:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx251r6",
          "author": "Ok-Skirt8939",
          "text": "im building \nhttps://clearproxy.io/\n\nan instant proxy validator.\n(u can check 100 Millions of proxies in less than 1 minute.)\nand.. more features just check it out (free 1M trial Checks)",
          "score": 1,
          "created_utc": "2026-01-01 09:53:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx260us",
          "author": "Long-Movie-2207",
          "text": "It's a real-time IP intel API that detects proxies, VPNs, Tor, datacenter vs residential, even specific providers, and gives a solid risk score (0-100) plus some device fingerprinting stuff.\n\nSuper handy for spotting if your proxies are likely to get flagged. There's a free tier with 5k requests to test it out, no strings attached.\n\nIf anyone's dealing with detection issues, might be worth a quick look: [https://ping0.xyz](https://ping0.xyz)",
          "score": 1,
          "created_utc": "2026-01-01 10:04:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx283xq",
          "author": "renegat0x0",
          "text": "https://github.com/rumca-js/Internet-Places-Database - data\n\n\nhttps://github.com/rumca-js/crawler-buddy - crawler\n\n\nhttps://github.com/rumca-js/webtoolkit - crawling library",
          "score": 1,
          "created_utc": "2026-01-01 10:26:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4ylgo",
          "author": "ziddec",
          "text": "I'm building https://sociavault.com \nA real-time social media scraping API. JSON response, 1 API key, 25+ platforms.",
          "score": 1,
          "created_utc": "2026-01-01 20:41:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5x6vx",
          "author": "scrape-do",
          "text": "***Hey scrapers,***\n\n  \nFirst off, happy new year to everyone :)\n\n\n\nWe're [Scrape.do](http://Scrape.do) üëã  an aggressively agile team behind a robust web scraping API that we've been building (very quietly since we were mainly focused on product) for the last 5 years.\n\n  \n2025 has been THE year for us, as we:\n\n* silently rolled out **next-gen infrastructure changes** that makes us the fastest and the most reliable on **a lot of** target domains, i.e. [async Scrape.do](https://scrape.do/documentation/async-api/)\n* **redesigned** our [dashboard with added playground](https://dashboard.scrape.do/playground) and [reworked documentation](https://scrape.do/documentation/),\n* [launched our new website](https://scrape.do/),\n* published **ready, free, and** [open-source scrapers](https://github.com/scrape-do/scrapedo-scrapers) **for 30 of the toughest domains** (we used SDO to bypass anti-bot, BUT will work with any API or your own setup)\n* an [Amazon Scraper API](https://scrape.do/products/ready-api/amazon-scraper/) as a product, but also a [free Amazon scraping repository](https://github.com/scrape-do/amazon-scraper)(**best one out there**, can easily be used w/o SDO same as above)\n\n  \n*Our support, bypass capabilities, pricing, and speed create an unmatched combination.* ‚≠ê\n\n\n\n***Yet, we're not satisfied.***\n\n\n\nMore products, features, and most importantly open-source resources are coming your way very soon.\n\n\n\nHappy scraping,\n\nSDO team",
          "score": 1,
          "created_utc": "2026-01-01 23:46:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7taq4",
          "author": "deepwalker_hq",
          "text": "\nHey r/webscraping,\n\nWe‚Äôve been building [Deepwalker](https://deepwalker.xyz) to help overcome the challenges of scraping modern mobile apps. Think automating interactions on TikTok, Instagram, Spotify, and more using real devices.\n\nOur latest post details how we tackled Spotify: [https://deepwalker.xyz/blog/scraping-spotify-is-ez](https://deepwalker.xyz/blog/scraping-spotify-is-ez)\n\nDeepWalker is built for professionals tackling large-scale data extraction and automation.",
          "score": 1,
          "created_utc": "2026-01-02 07:14:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9t67k",
          "author": "Past-Refrigerator803",
          "text": "**Browser4 ‚Äî Agentic browser engine for web scraping & automation**\n\nHi ,\n\nI‚Äôm building **Browser4**, an open-source, **agentic browser engine** for serious web scraping and automation‚Äîespecially where traditional tools start to break down.\n\nBrowser4 runs **inside your own infrastructure** and is designed for teams and individuals who need **control, stability, and scale**.\n\n# What Browser4 offers\n\n* **Agent-native architecture** Built for autonomous browser agents that reason, retry, branch, and adapt.\n* **Deep CDP control** JVM-native, coroutine-safe Chrome DevTools Protocol implementation. Capable of processing **\\~10k‚Äì20k highly complex pages per node per day**.\n* **Reliable scraping on dynamic sites** Handles logins, SPAs, infinite scroll, and JS-heavy workflows.\n* **Machine-learning extraction agent** Learns field structures across complex pages **without consuming LLM tokens**.\n* **Multi-strategy data extraction** LLMs, LM agents, selectors, DOM access, JavaScript execution, network interception, and screenshots.\n* **Scheduling & long-running jobs** Designed for production pipelines, not just one-off scripts.\n* **Built-in observability** Explicit state management, retries, and failure handling.\n\nBrowser4 can operate as a **fully autonomous browser agent** that plans and executes end-to-end tasks. At the same time, if you‚Äôve found Selenium or Playwright brittle, RPA tools too heavy, or scraping frameworks too opinionated, Browser4 also works at a **lower abstraction level** and gives you back control.\n\n# Use cases\n\n* AI agents that browse and interact with websites\n* Automation workflows that must survive frequent UI changes\n* Large-scale web scraping systems\n\nThe project is **open source** and under active development. Feedback, issues, and real-world use cases are very welcome.\n\n**GitHub:** [https://github.com/platonai/browser4](https://github.com/platonai/browser4)",
          "score": 1,
          "created_utc": "2026-01-02 16:04:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxe5dr7",
          "author": "Livid-Ad-8185",
          "text": "I'm a just starting my business as a software developer. As a self-taught (interested in all) covers from computer vision, web scraping, browser automation, data science, networking, DOM/Browser environment and deep learning. **Providing High Quality - Borderline CUTTING EDGE Solution & Software. ASK ME anything.**\n\n**Author of:**\n\nPropScanner - **Fanduel, Draftkings, Pinnacle** Prop Scanner, supports up to real-time updates.\n\n**Boob World Super TLS Client - C++ TLS client with BoringSSL built from scratch, Supports 1:1 Chrome143+ TLS fingerprints**\n\nJob Auto-Fill Pro - High Performance LLM-LEAD/Generic Job Apply/Filling Chrome Extension supports ALL From-based Platforms including **Linkedin, WorkDay, GreenHouse and Lever**. And you can use it for free now!\n\n\\*\\* CAPTCHA Solver - 100% Accuracy, Generating ProtonMail Accounts\\*\\*\n\n**Others Including: Yelp/Amazon/Walmart ENTIRE US Scrapers and Others 100+ | Universal-V 140M Language Model and MORE.**\n\n**CHECK MY PORTFOLIO:**\n\n[https://gitlab.com/JJ-GitRepo/](https://gitlab.com/JJ-GitRepo/)\n\n**Discord:**\n\n[https://discord.gg/jfdck6EE](https://discord.gg/jfdck6EE)\n\nDO NOT HELP INDIVIDUAL - $50+ PER SOLUTION, $40/H Per Session (INDIVIDUAL), Enteprise DIFFER.\n\nSoftware Development: $50 Deposit, 70% - 100% after data/software (trial) verified upon your choice. Can go through Upwork if legitment.",
          "score": 1,
          "created_utc": "2026-01-03 06:00:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxf2bsc",
          "author": "DataiziNet",
          "text": "Stop building scrapers. Start consuming data.\n\nMost tools here sell you the infrastructure (proxies/browsers) but leave you with the maintenance.[Dataizi](https://dataizi.net/)is a fully managed data extraction service. We don't sell the shovel; we dig the hole.\n\nWhy us?\n\n* 0 Maintenance: We monitor feed health 24/7. When layouts change, we fix the parsers, not you.\n* Complex Targets: We handle JS-heavy sites, hidden APIs, and aggressive anti-bot protections.\n\nPerfect for:\n\n* Price Intelligence: High-frequency SKU tracking for dynamic pricing strategies.\n* Machine Learning/AI: Clean, structured, normalized data ready for model training.\n* SaaS & Agencies: Reliable data feeds without hiring a dedicated scraping engineer.\n\nThe Offer: You provide the URL + Schema. We deliver the API or CSV.\n\n[Get a quote: dataizi.net](https://dataizi.net/)",
          "score": 1,
          "created_utc": "2026-01-03 10:40:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjkene",
          "author": "jobswithgptcom",
          "text": "[https://jobswithgpt.com](https://jobswithgpt.com) \\- job search",
          "score": 1,
          "created_utc": "2026-01-04 01:06:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxl1swy",
          "author": "DinnerStraight9753",
          "text": "Happy new year guys! How was your webscraping experience last year?\n\n# Want some real updates? Here comes the new [**PYPROXY**](http://www.pyproxy.com/?utm-source=zr&utm-keyword=?99) for 2026:\n\n1. **Expanded the unlimited residential IP pool,** key resources of high-demand countries (US, GB, FR, CA...) are now availableÔºÅ\n\n2. [Static ISP proxies](http://www.pyproxy.com/staticisp/?utm-source=nh&utm-keyword=?01) now **support custom IP range selection,** giving you greater control and flexibility in managing your proxy resources.\n\n3.¬†[Rotating Residential Proxies](http://www.pyproxy.com/residential/?utm-source=cs&utm-keyword=?99) billing has been updated to a **monthly subscription** model, with unused balances now **eligible for rollover.**\n\nDiscounts are available now for residential proxies, ISP proxies, and SOCKS5 proxies. Don't miss out!",
          "score": 1,
          "created_utc": "2026-01-04 06:39:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0kacg",
      "title": "Amazon \"shop other stores\" Beta",
      "subreddit": "webscraping",
      "url": "https://i.redd.it/zlndg2yq4lag1.jpeg",
      "author": "ZanofArc",
      "created_utc": "2025-12-31 18:44:32",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.71,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q0kacg/amazon_shop_other_stores_beta/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q0o0z1",
      "title": "TLS fingerprint websocket client to bypass cloudflare?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q0o0z1/tls_fingerprint_websocket_client_to_bypass/",
      "author": "vroemboem",
      "created_utc": "2025-12-31 21:34:36",
      "score": 4,
      "num_comments": 4,
      "upvote_ratio": 0.76,
      "text": "What are the best stealth websocket clients (that work with nodejs)?",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1q0o0z1/tls_fingerprint_websocket_client_to_bypass/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwzg735",
          "author": "Afraid-Solid-7239",
          "text": "What's the url for the socket you're trying to connect to? I'll play around with it",
          "score": 1,
          "created_utc": "2025-12-31 21:50:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1n7fo",
          "author": "army_of_wan",
          "text": "have you tried building a stealth client using golang's tls\\_library, i bet you could do that using gemini 3 or Claude opus",
          "score": 1,
          "created_utc": "2026-01-01 06:47:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxe4ygl",
          "author": "cq_in_unison",
          "text": "\\`curl\\_cffi\\` can usually get around it, but you mind need to go up to \\`nodriver\\` or scripted playwright. don't forget that it's not always just cloudflare, but a number of prevention tactics.",
          "score": 1,
          "created_utc": "2026-01-03 05:57:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}