{
  "metadata": {
    "last_updated": "2026-03-01 16:49:58",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 14,
    "total_comments": 108,
    "file_size_bytes": 119387
  },
  "items": [
    {
      "id": "1rc9992",
      "title": "Built a stealth Chromium, what site should I try next?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rc9992/built_a_stealth_chromium_what_site_should_i_try/",
      "author": "duracula",
      "created_utc": "2026-02-23 05:46:48",
      "score": 120,
      "num_comments": 92,
      "upvote_ratio": 0.99,
      "text": "Last couple months I was automating browser tasks on sites behind Cloudflare and reCAPTCHA. Tried various tools and solutions out there, everything either broke on the next Chrome update, got detected, or died. I was duct-taping 4 tools together and something broke every other week.\n\nSo I patched Chromium itself at the C++ source level.\n\nCloakBrowser is a small Python wrapper around a custom Chromium binary with 16 fingerprint patches compiled into the source. Not JavaScript injection, not config flags, canvas, WebGL, audio, fonts, GPU strings, all modified before compilation.\n\nResults:\n\n\\- reCAPTCHA v3: 0.9 (server-verified)  \n\\- Cloudflare Turnstile: pass (managed + non-interactive)  \n\\- BrowserScan, FingerprintJS, deviceandbrowserinfo: all clean  \n\\- 30/30 detection tests passed (full results on GitHub)\n\n    pip install cloakbrowser \n    \n    from cloakbrowser import launch \n    browser = launch() \n    page = browser.new_page()\n\nSame Playwright API, binary auto-downloads on first run (\\~200MB, cached).\n\nHow it's different from Patchright/rebrowser: those patch the protocol layer. We patch the browser itself, fingerprint values baked in at compile time. TLS matches because it IS Chrome.\n\nWhat it does NOT do: no proxy rotation, no CAPTCHA solving, no fingerprint randomization per session (yet). It's a browser, not a scraping stack. Bring your own proxies.  \nWe don't bypass reCAPTCHA. reCAPTCHA just thinks we're a normal browser â€” because we are one.\n\nLinux x64 and macOS (Silicon + Intel) are live now, even inside Docker.\n\n[https://github.com/CloakHQ/CloakBrowser](https://github.com/CloakHQ/CloakBrowser)  \n[https://cloakbrowser.dev/](https://cloakbrowser.dev/)  \nPyPI: [https://pypi.org/project/cloakbrowser/](https://pypi.org/project/cloakbrowser/) (`pip install cloakbrowser)`  \nnpm: [https://www.npmjs.com/package/cloakbrowser](https://www.npmjs.com/package/cloakbrowser) (`npm install cloakbrowser)`\n\nIf you have a site that blocks everything, throw it at CloakBrowser and let me know. I like the challenge. Hardest cases welcome.  \nPro tip: pair it with a residential proxy, the browser handles fingerprints, but your IP still matters.\n\nEarly days â€” feedback, bugs, requests are welcome.\n\n**Update 1:**\n\nJust shipped it! npm install cloakbrowser â€” supports both Playwright and Puppeteer\n\nSame stealth binary, same 30/30 detection results. TypeScript with full types.\n\n     // Playwright (default)\n      import { launch } from 'cloakbrowser';\n      const browser = await launch();\n      const page = await browser.newPage();\n      await page.goto('https://example.com');\n    \n      // Or with Puppeteer\n      import { launch } from 'cloakbrowser/puppeteer';\n\n**Update 2**:\n\nOur GitHub organization was temporarily flagged by an automated system.  \nWe were reorganizing repositories today, and the bulk activity on a new org, combined with a large binary on Releases and a traffic spike from this post â€” triggered GitHub's automated moderation.  \nWe've filed an appeal and expect it to be restored soon (at least hoping so).\n\nIn the meantime:  \n\\- `pip install cloakbrowser` and `npm install cloakbrowser` still work â€” binary downloads from our mirror  \n\\- GitLab Mirror - [https://gitlab.com/CloakHQ/cloakbrowser](https://gitlab.com/CloakHQ/cloakbrowser)  \n\\- And simple site - [https://cloakbrowser.dev/](https://cloakbrowser.dev/)  \n\\- GitHub repo is temporarily 404, should be back soon.  \n\\- [Posted about the situation in r/github](https://www.reddit.com/r/github/comments/1reu95z/github_flagged_our_opensource_new_born_org_with/)\n\nNothing changed with the project itself. Sorry for the inconvenience.\n\n**Update 3:**  \nGitHub org is restored â€” back to normal.  \nThanks everyone who reached out and helped during the downtime.\n\n**Update 4:**  \nmacOS builds are live!! Apple Silicon and Intel.  \nIf you tried before and got a download error, that's fixed now.  \nSame `pip install cloakbrowser` /  `npm install cloakbrowser` \\-  binary auto-downloads for your platform.\n\nEarly access, tested on 30 tests, but not yet battle-tested at scale like Linux.  \nIf you hit anything on Mac, open a GitHub issue.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rc9992/built_a_stealth_chromium_what_site_should_i_try/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6wtohv",
          "author": "Nice-Vermicelli6865",
          "text": "Try completing a survey on a survey platform and see if you are able to complete a survey, they have the toughest anti bot challenges",
          "score": 15,
          "created_utc": "2026-02-23 06:38:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wz8ra",
              "author": "duracula",
              "text": "SurveyMonkey worked, completed test survey (or its less protected?).  \nTypeform worked, the hard part was to find the damn next button.  \nSame for Qualtrics  \nYou have any other survey site in mind?  \n  \n  \n",
              "score": 2,
              "created_utc": "2026-02-23 07:28:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6wziim",
                  "author": "Nice-Vermicelli6865",
                  "text": "I meant on actual websites, like Prime Opinion that are actually guarded",
                  "score": 9,
                  "created_utc": "2026-02-23 07:31:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ws841",
          "author": "RandomPantsAppear",
          "text": "My dude, excellent. \n\nI was literally just pondering how to avoid having to do another chrome extension + python command server, and this fits the bill. \n\nFor every hour I do not have to do that, or code in C++ you are my hero an extra time.",
          "score": 10,
          "created_utc": "2026-02-23 06:25:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x120h",
          "author": "theozero",
          "text": "This seems promising. Any plans to make it possible to use via JavaScript / puppeteer?",
          "score": 6,
          "created_utc": "2026-02-23 07:45:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x2o74",
              "author": "duracula",
              "text": "Thanks, adding js/puppeteer support to the roadmap!  \nThe stealth is in the Chromium binary itself, not the Python wrapper, so it's mainly packaging work.\n\nIn the meantime, you can already use it with Puppeteer today â€” just point it at the binary:  \nHaven't tried it, but it should work, same parameters.\n\n    const puppeteer = require('puppeteer-core');\n    \n      const browser = await puppeteer.launch({\n        executablePath: '~/.cloakbrowser/chromium-142.0.7444.175/chrome',\n        args: [\n          '--no-sandbox',\n          '--disable-blink-features=AutomationControlled',\n          '--fingerprint=12345',\n          '--fingerprint-platform=windows',\n          '--fingerprint-hardware-concurrency=8',\n          '--fingerprint-gpu-vendor=NVIDIA Corporation',\n          '--fingerprint-gpu-renderer=NVIDIA GeForce RTX 3070',\n        ],\n        ignoreDefaultArgs: ['--enable-automation'],\n      });\n\nInstall the binary with:\n\n    pip install cloakbrowser && python -c \"from cloakbrowser.download import ensure_binary; ensure_binary()\"\n\nThen use the path above.\n\nAll the stealth passes through â€” same 14/14 detection results.  \nProper npm package coming soon.",
              "score": 4,
              "created_utc": "2026-02-23 08:01:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zqvbo",
                  "author": "theozero",
                  "text": "Nice - I'll give it a go!",
                  "score": 2,
                  "created_utc": "2026-02-23 18:15:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o700sz8",
              "author": "duracula",
              "text": "Just shipped it! `npm install cloakbrowser` â€” supports both Playwright and Puppeteer  \nSame stealth binary, same 14/14 detection results. TypeScript with full types.  \n\n     // Playwright (default)\n      import { launch } from 'cloakbrowser';\n      const browser = await launch();\n      const page = await browser.newPage();\n      await page.goto('https://example.com');\n    \n      // Or with Puppeteer\n      import { launch } from 'cloakbrowser/puppeteer';\n    \n\n Give it a try, let me know if there bugs or problems, have fun.",
              "score": 2,
              "created_utc": "2026-02-23 19:00:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o704xdi",
                  "author": "theozero",
                  "text": "Awesome. Iâ€™ve got a docker based setup anyway so wonâ€™t be using npm but Iâ€™m sure others will find it super useful",
                  "score": 2,
                  "created_utc": "2026-02-23 19:19:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xfuvb",
          "author": "juhacz",
          "text": "After a quick test, I see that it causes captcha on the [allegro.pl](http://allegro.pl) website in headless mode.",
          "score": 6,
          "created_utc": "2026-02-23 10:10:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zdssy",
              "author": "duracula",
              "text": "Tested it â€” Allegro uses DataDome, which is one of the more aggressive bot detection services. Took some digging, thanks for the challenge.\n\nFor sites with this level of protection, two things help: a residential proxy (datacenter IPs get flagged by IP reputation) and headed mode via Xvfb (some services detect headless-specific signals).   \nUpdated the README with instructions.  \nAfter implementing this 2 steps, I could enter the site without problems and navigate inside.  \n",
              "score": 6,
              "created_utc": "2026-02-23 17:14:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y7giw",
          "author": "Objectdotuser",
          "text": "amazing work, but how could we possibly vet this patched chromium binary?",
          "score": 6,
          "created_utc": "2026-02-23 13:43:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z1iqe",
              "author": "duracula",
              "text": "Fair concern â€” \"trust me bro\" doesn't cut it for a binary you're running.\n\n1. Check the hash â€” every release has a SHA256 digest on GitHub, verify your download matches  \n2. Run it sandboxed â€” Docker, strace, or a VM. Monitor network traffic, syscalls, file access. It's just Chromium â€” it doesn't phone home or do anything a stock Chrome wouldn't  \n3. Scan it â€” upload to VirusTotal, it passes clean  \n4. Read the wrapper â€” fully open source MIT, you can see exactly what flags get passed and how the binary is launched\n\nAt the end of the day, you're in the same position as with any Chromium distribution (Brave, Vivaldi, Arc) â€” you either trust the publisher, audit the behavior, or build your own.",
              "score": 4,
              "created_utc": "2026-02-23 16:17:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zcdlw",
          "author": "Pristine_Wind_2304",
          "text": "its giving ai generated text from your replies and your original post but if your tests are right then this seems like an awesome project!! i hope it gets developed further and not abandoned like the other five million chrome binary patches that just cant keep up with the like 100 leaks from every web api",
          "score": 6,
          "created_utc": "2026-02-23 17:07:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zh1o4",
              "author": "duracula",
              "text": "Thanks,   \nYeah I use AI heavily and I'm not gonna pretend otherwise. Without it I couldn't have patched Chromium to this level in a few weeks, it's a massive codebase and a lot of work. AI saved me months.  \n  \nSame with this thread, lots of replies that each deserve a proper testing and answer. I throw in my points and AI helps me write them up in proper English. It's a tool, like everything else.\n\nOn the abandonment concern, totally fair, I've seen the graveyard too. The difference here is this powers production automation I depend on every day. If it breaks, my stuff breaks.   \nI'll keep it going as long as I can â€” but I won't lie, these things take a lot of time and dedication, and life happens.   \n  \nThe code and test results are real though â€” `pip install cloakbrowser` and `python examples/stealth_test.py` hits 6 live detection sites with pass/fail verdicts.  \nThat's what matters.",
              "score": 1,
              "created_utc": "2026-02-23 17:29:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y7p2b",
          "author": "usamaejazch",
          "text": "how is the stealth browser binary compiled? no source of patches? \n\nit could even have malware, no?",
          "score": 5,
          "created_utc": "2026-02-23 13:45:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z2feu",
              "author": "duracula",
              "text": "You're right that you can't fully verify a closed binary â€” same as Brave, Arc, or any Chromium fork that ships pre-built.\n\nIt's compiled from the official Chromium 142 source tree with our patches applied, using the standard Chromium build toolchain (gn + ninja).   \nSame process any Chromium fork uses. The patches modify fingerprint APIs (canvas, WebGL, audio, fonts, GPU strings).   \nThat's it.  No network changes, no data collection, no telemetry.\n\nWhat you can verify:  \n\\- Run it with strace or Wireshark â€” it behaves identically to stock Chromium except fingerprint values differ  \n\\- Upload to VirusTotal, passes clean  \n\\- The wrapper is fully open source, you can read every line\n\nThe patches aren't open source because they're the core IP of the project. But the binary behavior is fully auditable â€” that's where trust should come from.\n\nIf that's not enough for your threat model, that's completely fair. Not every tool is for everyone.",
              "score": 2,
              "created_utc": "2026-02-23 16:21:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xcvu4",
          "author": "EnvironmentSome9274",
          "text": "Try Walmart, their anti bot is very aggressive",
          "score": 3,
          "created_utc": "2026-02-23 09:42:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zp7oy",
              "author": "duracula",
              "text": "Worked.",
              "score": 1,
              "created_utc": "2026-02-23 18:07:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zpg9t",
                  "author": "EnvironmentSome9274",
                  "text": "Can you be a bit more elaborate lol, please? \nHow many products did you try scraping? Did the bot flag you and you rotated or was it completely undetected? \nThank you",
                  "score": 1,
                  "created_utc": "2026-02-23 18:08:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xihbe",
          "author": "Zestyclose_Ad9943",
          "text": "Is it possible to use it on a Node project ?  \nI have a scraping script built on Node with Playwright, I wish I could use your browser instead.",
          "score": 4,
          "created_utc": "2026-02-23 10:35:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70119r",
              "author": "duracula",
              "text": "Just shipped it! `npm install cloakbrowser` â€” supports both Playwright and Puppeteer  \nSame stealth binary, same 14/14 detection results. TypeScript with full types.  \n\n     // Playwright (default)\n      import { launch } from 'cloakbrowser';\n      const browser = await launch();\n      const page = await browser.newPage();\n      await page.goto('https://example.com');\n    \n      // Or with Puppeteer\n      import { launch } from 'cloakbrowser/puppeteer';\n    \n\n Give it a try, let me know if there bugs or problems, have fun.",
              "score": 3,
              "created_utc": "2026-02-23 19:01:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y6er5",
          "author": "Objectdotuser",
          "text": "hmm well companies are wising up to the evasion browsers. how does it handle the chrome versions? If you  have a stagnant chrome version this can be a sign that you are using a controlled browser. does the version update with the typical monthly chrome scheduled releases? how does the update cycle work? i read in the code that the first time it downloads the patched binary and then uses that cached version. that would imply it does not update and this was a one time thing. any ideas on how to handle the update cycles?",
          "score": 3,
          "created_utc": "2026-02-23 13:37:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z0f0r",
              "author": "duracula",
              "text": "Valid point â€” a stale version is a detection signal.\n\nCurrently on Chromium 142, and 145 is being tested now.   \nThe 16 patches port cleanly since they're in isolated files (canvas, WebGL, audio, fonts) â€” porting from 139â†’142 took about a day.\n\nThe plan is monthly builds minimum to stay within the normal version window, with CI automation for faster turnaround.   \nDetection services mostly check if you're within the last 2-3 major versions, so the window is forgiving. And a slightly older Chromium with correct TLS + consistent fingerprints still beats any JS injection tool on a current version.\n\nAuto-update in the wrapper is on the roadmap too â€” check for new binary on launch, download in background.",
              "score": 2,
              "created_utc": "2026-02-23 16:12:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y6qqt",
          "author": "AltruisticHunt2941",
          "text": "This library can scrape this site MakeMyTrip.com ?",
          "score": 3,
          "created_utc": "2026-02-23 13:39:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wtjfs",
          "author": "brokedesigner0",
          "text": "Nice",
          "score": 2,
          "created_utc": "2026-02-23 06:36:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wtq87",
          "author": "ry8",
          "text": "This is awesome. Iâ€™ll be using this. Thank you!",
          "score": 2,
          "created_utc": "2026-02-23 06:38:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wvqxf",
          "author": "kev_11_1",
          "text": "Zillow",
          "score": 2,
          "created_utc": "2026-02-23 06:56:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wxlta",
              "author": "duracula",
              "text": "Zillow worked without a problem.  \nDidn't scraped it all, but search, listing, apartments pages and photos works.",
              "score": 1,
              "created_utc": "2026-02-23 07:13:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6wzi16",
                  "author": "kev_11_1",
                  "text": "I heard it opens initially and blocks after hefty requests.\n\nBut sure i will give it a try",
                  "score": 2,
                  "created_utc": "2026-02-23 07:30:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o749108",
                  "author": "kev_11_1",
                  "text": "Hey, can you try [bizbuysell.com](http://bizbuysell.com), not the home page, but inside the deal page, where I am facing issues even after using camoufox.",
                  "score": 2,
                  "created_utc": "2026-02-24 11:17:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6wxrk4",
          "author": "dsjflkhs",
          "text": " Interesting",
          "score": 2,
          "created_utc": "2026-02-23 07:14:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xd6qt",
          "author": "deadcoder0904",
          "text": "LinkedIn & X are the hardest, no? Even Substack articles don't allow scraping throws \"Too Many Requests\"\n\nX had Bird CLI by OpenClaw creator that got taken down so that might be easy with cookie.\n\nLinkedIn might be the toughest but also one of the most useful ones.\n\nCool project though.",
          "score": 2,
          "created_utc": "2026-02-23 09:45:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xg5e1",
          "author": "inliberty_financials",
          "text": "Good job man ! Thanks this is what i wanted, I'll test out the solution.",
          "score": 2,
          "created_utc": "2026-02-23 10:13:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xgfpb",
          "author": "jagdish1o1",
          "text": "I will sure give it a try",
          "score": 2,
          "created_utc": "2026-02-23 10:16:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xgl5c",
              "author": "jagdish1o1",
              "text": "No mac is a setback for me",
              "score": 2,
              "created_utc": "2026-02-23 10:17:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7fdzyc",
                  "author": "duracula",
                  "text": "macOS Apple Silicon build is in progress â€” it'll come with the Chromium 145 release we currently working on.  \nIn the meantime you can use CloakBrowser on Mac via Docker.",
                  "score": 1,
                  "created_utc": "2026-02-26 00:22:48",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o7rsyoe",
                  "author": "duracula",
                  "text": "macOS is up!  \nApple Silicon and Intel. Same install, binary auto-downloads for your platform now.\n\n This is early access for macOS, so if you run into anything let me know here.",
                  "score": 1,
                  "created_utc": "2026-02-27 21:25:41",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xjovl",
          "author": "ChaandyMan",
          "text": "awesome work",
          "score": 2,
          "created_utc": "2026-02-23 10:47:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xrmzi",
          "author": "Double-Journalist-90",
          "text": "Can you create a user account on X",
          "score": 2,
          "created_utc": "2026-02-23 11:56:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70trx4",
              "author": "duracula",
              "text": "Tried it. Signup flow works fine until the Arkose CAPTCHA step â€” it loads but shows an infinite spinner instead of a puzzle.   \nOur stealth passes all X's bot checks, but Arkose runs its own fingerprinting inside a cross-origin iframe. Currently investigating what's flagging us. Will update.",
              "score": 3,
              "created_utc": "2026-02-23 21:19:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o716mpw",
                  "author": "Double-Journalist-90",
                  "text": "Thanks let me know I appreciate the response",
                  "score": 3,
                  "created_utc": "2026-02-23 22:21:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xxtrl",
          "author": "orucreiss",
          "text": "Do you fingerprint webgl gpu?",
          "score": 2,
          "created_utc": "2026-02-23 12:43:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yznld",
              "author": "duracula",
              "text": "Yes â€” GPU vendor and renderer strings are spoofed via CLI flags at launch:\n\n    --fingerprint-gpu-vendor=NVIDIA Corporation\n    --fingerprint-gpu-renderer=NVIDIA GeForce RTX 3070\n\n  These are patched at the C++ level in the binary, so `WebGLRenderingContext.getParameter(UNMASKED_VENDOR_WEBGL)` and `UNMASKED_RENDERER_WEBGL` both return the spoofed values.   \nNot JS injection â€” the actual GPU reporting functions in Chromium are modified.\n\nThe --fingerprint seed also affects canvas and WebGL hash output, so each session produces a unique but consistent fingerprint.",
              "score": 3,
              "created_utc": "2026-02-23 16:08:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6z3fh2",
                  "author": "orucreiss",
                  "text": "This is sooo good",
                  "score": 2,
                  "created_utc": "2026-02-23 16:26:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yoj25",
          "author": "Broad-Apartment4747",
          "text": "Is there a plan to develop Windows x64?",
          "score": 2,
          "created_utc": "2026-02-23 15:15:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z4ou3",
              "author": "duracula",
              "text": "Yes â€” macOS is next, then Windows.   \nThe patches are platform-agnostic C++ so it's the same code, just need to set up the build environments (Xcode for macOS, Visual Studio for Windows).   \nWe're finishing the Chromium 145 build now on Linux, other platforms will follow.   \nEach platform takes 3-6 hours to compile plus testing against all detection services, so it takes a bit â€” but it's coming.\n\nIn the meantime, you can run it today via Docker on Windows/macOS â€” there's a ready-made Dockerfile included:\n\n    docker build -t cloakbrowser  .\n    docker run --rm cloakbrowser python your_script.py",
              "score": 1,
              "created_utc": "2026-02-23 16:32:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6z5ikp",
                  "author": "usamaejazch",
                  "text": "I am sure you didn't do anything risky. But, I am just pointing it out from the perspective of a third party and because of security reasons. \n\nNPM modules get breached all the time. What if an update secretly ships a session logger or something? ",
                  "score": 2,
                  "created_utc": "2026-02-23 16:35:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yp42s",
          "author": "maher_bk",
          "text": "I'll definitly integrate it in my scraping at scale backend (for my ios app) :) However, I am not sure if it is supporting Ubuntu ARM64 ? (Basically ampere servers)",
          "score": 2,
          "created_utc": "2026-02-23 15:18:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z2xt2",
              "author": "duracula",
              "text": "Not yet â€” currently Linux x64 only.   \nNext up is macOS (arm64 + x64), then Windows. ARM64 Linux is further out.  \n  \nFor scraping at scale, x64 servers work out of the box with pip install cloakbrowser.",
              "score": 1,
              "created_utc": "2026-02-23 16:24:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7yp1o8",
                  "author": "maher_bk",
                  "text": "Hello again! So I moved my scraping servers from an ARM64 to a x86 (AMD) machine and hence enabled cloakbrowser! For now looking really good (already had 6 scrapers chained so kinda see him performing quite well in the chain.  \nI was looking for suggestions on how to approach scrolling on heavy js websites (by the way the goal of such task is to gather the links then I use heuristics + AI to filter out the one that I'm looking for).  \nBelow my approach to make sure the whole page is rendered:\n\n    # RENDER_READY_TIMEOUT_SECONDS = 8\n    # RENDER_STABILITY_POLL_SECONDS = 0.5\n    \n    async def _wait_for_render_ready(\n            \n    self\n    ,\n            \n    page\n    ,\n            \n    timeout_seconds\n    : float = RENDER_READY_TIMEOUT_SECONDS,\n            \n    min_text_length\n    : int = 150,\n        ) -> bool:\n            start = time.time()\n            while (time.time() - start) < \n    timeout_seconds\n    :\n                try:\n                    ready_state = await \n    page\n    .evaluate(\"document.readyState || ''\")\n                    if ready_state in (\"interactive\", \"complete\"):\n                        break\n                except Exception:\n                    pass\n                await asyncio.sleep(RENDER_STABILITY_POLL_SECONDS)\n    \n    \n            stable_samples = 0\n            prev_text_len = -1\n            prev_html_len = -1\n            while (time.time() - start) < \n    timeout_seconds\n    :\n                try:\n                    text_len = await \n    page\n    .evaluate(\n                        \"() => document.body?.innerText?.length || 0\"\n                    )\n                    html_len = await \n    page\n    .evaluate(\n                        \"() => document.documentElement?.outerHTML?.length || 0\"\n                    )\n                    if await \n    self\n    ._has_content_selector(\n    page\n    ):\n                        if text_len >= max(50, \n    min_text_length\n     // 2):\n                            return True\n                    if text_len >= \n    min_text_length\n     and prev_text_len >= 0:\n                        text_delta = abs(text_len - prev_text_len)\n                        html_delta = abs(html_len - prev_html_len)\n                        if text_delta <= 5 and html_delta <= 200:\n                            stable_samples += 1\n                        else:\n                            stable_samples = 0\n                        if stable_samples >= max(1, RENDER_STABILITY_REQUIRED_SAMPLES):\n                            return True\n                    prev_text_len = text_len\n                    prev_html_len = html_len\n                except Exception:\n                    pass\n                await asyncio.sleep(RENDER_STABILITY_POLL_SECONDS)\n            return False",
                  "score": 2,
                  "created_utc": "2026-02-28 23:29:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ypfjx",
          "author": "dud380",
          "text": "Very interesting, especially TLS fingerprinting since it can't be done from JS. Also binary-level hiding of CDP internals. ",
          "score": 2,
          "created_utc": "2026-02-23 15:20:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75fanb",
          "author": "bluemangodub",
          "text": "How do you score on :\n\nhttps://fingerprint-scan.com/\n\nhttps://abrahamjuliot.github.io/creepjs/\n\n\nAre you aiming only for headfulll or aim to provide a passing headless implemented (the hardest of all due to base missing functionality). \n\nBest I could get on fingerprint-scan was 50% likely to be a bot and 30% like headless due to:\n\n1.    noTaskbar: true\n2.   noContentIndex: true\n3.    noContactsManager: true\n4.    noDownlinkMax: true\n\n\nAnyway, looks like a good project, good luck :-)",
          "score": 2,
          "created_utc": "2026-02-24 15:27:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7e95o2",
              "author": "duracula",
              "text": "Thanks for testing! Here are our current scores:\n\nCreepJS:  \n\\- headless: 0%  \n\\- stealth: 0%  \n\\- like-headless: 31% â€” fixes in progress to bring this under 20%\n\nfingerprint-scan.com:  \n\\- Bot Detection: 4/4 PASS (WebDriver, Selenium, CDP, Playwright all false)  \n\\- Bot Risk Score: 45/100 â€” working on lowering this further\n\nWe're targeting full headless pass, not just headful. The remaining signals need C++ stubs in the Chromium build â€” on our roadmap with 145 build.",
              "score": 2,
              "created_utc": "2026-02-25 20:56:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76xr9t",
          "author": "Glittering_Turn_6971",
          "text": "I would love to try it on macOS with Apple silicon.",
          "score": 2,
          "created_utc": "2026-02-24 19:32:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ebqy2",
              "author": "duracula",
              "text": "macOS Apple Silicon build is in progress â€” it'll come with the Chromium 145 release we currently working on.   \nIn the meantime you can use CloakBrowser on Mac via Docker.",
              "score": 2,
              "created_utc": "2026-02-25 21:08:32",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7rtdgo",
              "author": "duracula",
              "text": "macOS builds are now live!  \nBoth Apple Silicon (arm64) and Intel (x64).\n\nSame `pip install cloakbrowser`, the binary will auto-download for your platform now.\n\n This is early access for macOS, so if you run into anything let me know here.",
              "score": 1,
              "created_utc": "2026-02-27 21:27:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ekovd",
          "author": "TheCrandelticSociety",
          "text": "wow.... Camoufox 2.0... this is awesome. getting it up and running in docker on Mac was a breeze. very much appreciate the hardwork! passes akamai without issue. excited for future updates",
          "score": 2,
          "created_utc": "2026-02-25 21:49:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7h5l5x",
          "author": "CptLancia",
          "text": "Hey, is there any profile/fingerprinting management/creation? \nOr is it a single fingerprint that is being used?",
          "score": 2,
          "created_utc": "2026-02-26 07:15:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7h8m1a",
              "author": "duracula",
              "text": "Each launch automatically gets a unique fingerprint â€” a random seed drives all the Chromium-level patches (canvas, WebGL, audio, fonts, client rects) so they stay internally consistent.\n\nFor persistent profiles, pin a seed: `launch(args=[\"--fingerprint=42069\"])`  \nsame seed = same fingerprint every time. You can also customize GPU vendor/renderer, platform, timezone, geolocation, and more via 10 available flags.\n\nWe just documented all of this: [https://github.com/CloakHQ/CloakBrowser#fingerprint-management](https://github.com/CloakHQ/CloakBrowser#fingerprint-management)\n\nPlease update me if there is problem with them.",
              "score": 1,
              "created_utc": "2026-02-26 07:43:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6wyc6r",
          "author": "alexp9000",
          "text": "Ticke tmaster?",
          "score": 1,
          "created_utc": "2026-02-23 07:20:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x0bsb",
              "author": "duracula",
              "text": "With residental proxy, concerts discovery listings and item worked, same for starting ordering process of seats selection.  \nWith datacenter ip, blocked as expected from F5.",
              "score": 1,
              "created_utc": "2026-02-23 07:38:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zew8c",
                  "author": "alexp9000",
                  "text": "Amazing! Need Mac plz :)",
                  "score": 1,
                  "created_utc": "2026-02-23 17:19:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6znqac",
          "author": "letopeto",
          "text": "why is it stuck on v142? I think latest chrome is v145? I've found running an out of date version of chrome increases your flag risk",
          "score": 1,
          "created_utc": "2026-02-23 18:00:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o703ewd",
              "author": "duracula",
              "text": "Not stuck â€” 145 is already built and in testing now on linux.   \n142 is still within the normal version window (detection services mostly flag browsers 3+ major versions behind), and I've been running it in production with solid results. But staying current matters, so 145 is the priority.\n\nStar the repo on GitHub to get notified when it drops.",
              "score": 1,
              "created_utc": "2026-02-23 19:12:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zwokr",
          "author": "pierreortega",
          "text": "and tiktok web? does it get captcha on logged out?",
          "score": 1,
          "created_utc": "2026-02-23 18:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zxd7e",
          "author": "PutHot606",
          "text": "Try www.bet365.bet.br",
          "score": 1,
          "created_utc": "2026-02-23 18:44:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71z36x",
          "author": "stratz_ken",
          "text": "Test it on windows server builds. If it works there it would be the first of its kind.  Almost all of them fail under server operating systems.",
          "score": 1,
          "created_utc": "2026-02-24 00:57:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73th1l",
          "author": "boomersruinall",
          "text": "How about indeed? I have been struggling with this particular target. Also chewy",
          "score": 1,
          "created_utc": "2026-02-24 08:53:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73xrhk",
          "author": "Bharath0224",
          "text": "How about [darty.com](http://darty.com) ?",
          "score": 1,
          "created_utc": "2026-02-24 09:34:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7728wy",
              "author": "duracula",
              "text": "Works with CloakBrowser in headed mode + residential proxy.  \nSee the headed mode section in our README for setup.",
              "score": 1,
              "created_utc": "2026-02-24 19:53:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o79gp1o",
          "author": "Fit-Molasses-8050",
          "text": "I am getting error while installing the CloakBrowser using docker.  \nERROR: error during connect: Head \"http://%2F%2F.%2Fpipe%2FdockerDesktopLinuxEngine/\\_ping\": open //./pipe/dockerDesktopLinuxEngine: The system cannot find the file specified.\n\nhave anyone got the same issue? ",
          "score": 1,
          "created_utc": "2026-02-25 03:21:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7eb9ph",
              "author": "duracula",
              "text": "That's a Docker Desktop issue, not CloakBrowser-specific.  \nDocker Desktop isn't running on your machine â€” start it first, then retry.   \nIf you're on Windows, make sure the Docker Desktop app is open and the engine is fully started before running any docker command",
              "score": 2,
              "created_utc": "2026-02-25 21:06:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o79thbz",
          "author": "Mammoth_Gazelle_9921",
          "text": "No pass recaptcha v3 enterpise invisible",
          "score": 1,
          "created_utc": "2026-02-25 04:41:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7eb2o7",
              "author": "duracula",
              "text": "Hey,  \nwe actually talked about this in a GitHub issues?   \nThe problem was Puppeteer specifically, its CDP protocol leaks automation signals that reCAPTCHA Enterprise picks up.   \nSwitching to the Playwright wrapper fixes it, works great. We've documented it in the README now too.\n\nThanks for testing!",
              "score": 1,
              "created_utc": "2026-02-25 21:05:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ivtlj",
          "author": "Automatic_Bus7109",
          "text": "This looks pretty much like a malware to me.",
          "score": 1,
          "created_utc": "2026-02-26 15:01:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o81wmdr",
          "author": "Popular-Exchange823",
          "text": "Try aa.com",
          "score": 1,
          "created_utc": "2026-03-01 13:58:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcb2p8",
      "title": "I curated a list of 100+ open-source proxy tools",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rcb2p8/i_curated_a_list_of_100_opensource_proxy_tools/",
      "author": "ciokan",
      "created_utc": "2026-02-23 07:31:27",
      "score": 36,
      "num_comments": 1,
      "upvote_ratio": 0.98,
      "text": "Been collecting proxy-related tools for a while and finally organized them into an awesome-list on GitHub. Covers proxy libraries (Python, Go, Node.js), forward/reverse proxies, SOCKS5 servers, Shadowsocks, Trojan, WireGuard, DNS proxies, scraping frameworks with proxy support, and proxy checkers.\n\nTried to include only actively maintained projects. Happy to add anything I missed â€” PRs welcome.\n\n[https://github.com/drsoft-oss/awesome-proxy](https://github.com/drsoft-oss/awesome-proxy)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rcb2p8/i_curated_a_list_of_100_opensource_proxy_tools/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o78qjof",
          "author": "IWillBiteYourFace",
          "text": "Did you delete the repo? The link doesn't work.",
          "score": -1,
          "created_utc": "2026-02-25 00:53:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rf0nd0",
      "title": "I built an open-source no code web scraper Chrome extension",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rf0nd0/i_built_an_opensource_no_code_web_scraper_chrome/",
      "author": "Dry-Raspberry-3608",
      "created_utc": "2026-02-26 04:34:32",
      "score": 33,
      "num_comments": 12,
      "upvote_ratio": 0.91,
      "text": "Hey everyone,\n\nI do a fair bit of data collection for my own side projects. Usually, I just write a quick Python script with BeautifulSoup, but sometimes I just want to visit a webpage, click on a few elements, and download a CSV without having to open my terminal or fight with CORS.\n\nI tried a few of the existing visual scraping tools out there, but almost all of them lock you into expensive monthly subscriptions. I really hate the idea of paying a recurring fee just to extract public text, and I don't love my data passing through a random third-party server.\n\nSo I spent the last few weeks building my own alternative. Itâ€™s a completely free, open-source no code web scraper that runs entirely locally in your browser.\n\nHere is how the workflow looks right now:\n\n* You open the extension on the page you want to scrape.\n* You click on the elements you want to grab (it auto-detects repeating patterns like lists, grids, or tables).\n* You name your columns (e.g., \"Price\", \"Product Title\").\n* Hit export, and it generates a clean CSV or JSON file instantly.\n\nBecause it runs locally in your browser, it uses your own IP and session state. This means it doesn't get instantly blocked by standard anti-bot protections the way server-side scrapers do.\n\nSince it's open source, you don't have to worry about sudden paywalls, API caps, or vendor lock-in.\n\nYou can install it directly from the Chrome Web Store here:[https://chromewebstore.google.com/detail/no-code-web-scraper/cogbfdcdnohnoknnogniplimgkdoohea](https://chromewebstore.google.com/detail/no-code-web-scraper/cogbfdcdnohnoknnogniplimgkdoohea)\n\n(The GitHub repo with all the source code is linked on the store page, but let me know if you want me to drop it in the comments).\n\nI'm still actively working on it, so please let me know if you run into bugs. It struggles a bit with deeply nested shadow DOMs right now, but I'm trying to figure out a fix for the next update. Honest feedback or feature ideas are super welcome!",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1rf0nd0/i_built_an_opensource_no_code_web_scraper_chrome/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o7gl5pj",
          "author": "Dry-Raspberry-3608",
          "text": "I am not too proud of it yet so please please tell me if u like it and found bugs i will fix everything by next update. ",
          "score": 2,
          "created_utc": "2026-02-26 04:35:53",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o7gmzbo",
          "author": "Connect-Soil-7277",
          "text": "will definitely give it a try",
          "score": 2,
          "created_utc": "2026-02-26 04:48:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gnhmr",
              "author": "Dry-Raspberry-3608",
              "text": "Aye Thanks",
              "score": 1,
              "created_utc": "2026-02-26 04:52:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7h4zwf",
          "author": "angelarose210",
          "text": "Can you share the repo?",
          "score": 2,
          "created_utc": "2026-02-26 07:10:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gmh75",
          "author": "Accomplished_Ear4947",
          "text": "thanks!",
          "score": 1,
          "created_utc": "2026-02-26 04:45:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gxl8w",
          "author": "XxxHAMZAxxX",
          "text": "Thanks for sharing",
          "score": 1,
          "created_utc": "2026-02-26 06:07:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7i7b1r",
          "author": "Kikoness",
          "text": "Seems very interesting for what I do daily! I'll give it a try. Mind sharing the repo? I couldn't find it on the store page.",
          "score": 1,
          "created_utc": "2026-02-26 12:46:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lqrgo",
          "author": "Anxious_Ad2885",
          "text": "what tech stack do you use for build that extension?",
          "score": 1,
          "created_utc": "2026-02-26 23:09:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mr2qk",
          "author": "the__solo__legend",
          "text": "Thanks will definitely helps mee",
          "score": 1,
          "created_utc": "2026-02-27 02:34:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfwvga",
      "title": "Looking for a Simple Scraper for a Simple Need",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rfwvga/looking_for_a_simple_scraper_for_a_simple_need/",
      "author": "amikigu",
      "created_utc": "2026-02-27 04:29:04",
      "score": 11,
      "num_comments": 25,
      "upvote_ratio": 0.77,
      "text": "Hi all, it seems that most web scraping tools do far more than what I want to do, which is to just scrape the header, main/first image link, tags, and text, of specific articles from various websites, and then put that data in a database of some sort that's usable by Wordpress (or even just into a .csv file at minimum). My goal is to then reformat/summarize said text/data later in a newsletter format. Is there any tool with a relatively simple GUI (or in which the coding isn't outlandishly difficult to use) and with decent tutorials that people would recommend for this? Given that scraping has been a thing for years, and given the clear time and effort that have been spent developing the tools I've already explored, I'm hoping what I want is already out there, and I'm just not finding the right tutorials/links. Thanks in advance for any guidance.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rfwvga/looking_for_a_simple_scraper_for_a_simple_need/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o7nqdkx",
          "author": "Picatrixter",
          "text": "It's funny reading this, because this is exactly what I do for a decent income (mainly Adsense and affiliate): I built a scraper specifically for Wordpress sites (via the standard API). I get the articles from various sources (also following the external links in the body to get more context), rewrite them with AI, resize the featured images, publish them on my own WP sites. Once that's done, phase 2: other scripts start sharing the content on Facebook and other social nets, while another script generates the newsletter from the latest articles, while also adding some affiliate links to it (to books that the readers might be interested in). \n\nMy advice: break this large process into smaller steps, so that it doesn't feel overwhelming, tgen assemble everything into one big pipeline.",
          "score": 5,
          "created_utc": "2026-02-27 06:34:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o1jr6",
              "author": "lp435",
              "text": "I always wondered if this really works. It's unimaginable to me that people read an article and then feel the sudden urge to click on a link an buy a book. But I'm happy that it works for you!",
              "score": 1,
              "created_utc": "2026-02-27 08:11:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7o4yic",
                  "author": "Picatrixter",
                  "text": "It's not just a random book. The books are chosen by a similarity ranking (say 80% of the articles are about recent scientific discoveries - the book will be about something related). For a 100% automated workflow, I am happy with the results.",
                  "score": 1,
                  "created_utc": "2026-02-27 08:44:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7oan8r",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-27 09:38:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7odgef",
                  "author": "webscraping-ModTeam",
                  "text": "ðŸ‘” Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
                  "score": 1,
                  "created_utc": "2026-02-27 10:05:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ochy8",
              "author": "Naive-Highway-4733",
              "text": "That's sounds awesome. Where do you host your applications may I ask. Is there a way to host it as cheap as possible or do you use your own computer. Is there a way to do this Self-gosted. If you could write an article about it, that would be worth to read",
              "score": 1,
              "created_utc": "2026-02-27 09:56:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7uak34",
                  "author": "Picatrixter",
                  "text": "I host it at home, on a dedicated Ubuntu server (cheap SFF computer). Scripts run on a schedule via cron.",
                  "score": 1,
                  "created_utc": "2026-02-28 06:55:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7t7wmg",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-28 02:16:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7t9qvo",
                  "author": "webscraping-ModTeam",
                  "text": "ðŸª§ Please review the sub rules ðŸ‘‰",
                  "score": 1,
                  "created_utc": "2026-02-28 02:27:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7q8ycr",
          "author": "Proof_Resource7669",
          "text": "Honestly, you might be overthinking it. For something that specific, you could probably get a working Python script with Beautiful Soup and Requests in an afternoon. There are tons of straightforward tutorials for exactly that kind of targeted scraping",
          "score": 2,
          "created_utc": "2026-02-27 16:53:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sztdv",
          "author": "alex3321xxx",
          "text": "Just ask Claude or Gemini",
          "score": 2,
          "created_utc": "2026-02-28 01:25:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7odj4b",
          "author": "eurobosch",
          "text": "You can check out [n8n](https://n8n.io), it's a no-code automation tool that you can use for example for scraping, extracting data, reshape it, store it, etc. You can create flows from very simple (2-3 nodes) up to very complex ones (tens or hundreds) with lots of integrations. You can schedule your jobs to run at specific intervals, you can connect it to hundreds of services and tools (GMail, Google Sheets, Postgres databases, etc).   \nI know they had a free plan or you can self-host it with their docker images.  \nCheck out their templates library where you can find already built flows for many scraping scenarios.\n\nSimilar tools are [make.com](http://make.com), zapier.",
          "score": 1,
          "created_utc": "2026-02-27 10:06:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7omwfv",
              "author": "havoc2k10",
              "text": "do you have template for n8n webscraping?",
              "score": 1,
              "created_utc": "2026-02-27 11:29:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7pqybx",
                  "author": "eurobosch",
                  "text": "yeah I have several but they are too complicated for your needs and they are not published on the n8n site, I use them privately for other projects. \n\nyou can search for scraping in the template library [here](https://n8n.io/workflows/?q=scraping) ",
                  "score": 1,
                  "created_utc": "2026-02-27 15:28:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7razlu",
          "author": "justincampbelldesign",
          "text": "Can you provide more details.\n\n* What are a few example sites?\n* Is [this example](https://www.figma.com/proto/cF2LKiZ9DpdxPmmm1cBh1m/Article-extraction?node-id=4-550&t=79IWLzf03CSb3xbs-1&scaling=scale-down&content-scaling=fixed&page-id=0%3A1) of what you are trying to pull correct? (Best viewed on a laptop or desktop computer, use the toolbar in the upper right to zoom in and out.)\n\nYou might want to use the RSS feed. RSS is stable, legal/expected, and doesnâ€™t break when page HTML changes.  \nMany publishers expose RSS feeds that already include:\n\n* title (header)\n* link\n* publish date\n* excerpt / sometimes full content\n* sometimes featured image (varies)\n\nYou might be able to skip the database  \nRSS feed > WP posts or a custom post type > newsletter formatting later.  \nThis avoids building a separate database and might make this a bit simpler.",
          "score": 1,
          "created_utc": "2026-02-27 19:55:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7t9v3d",
              "author": "amikigu",
              "text": "Thanks for your reply. Some sample articles are these two, with differing degrees of complexity: [https://en.antaranews.com/news/399809/indonesia-allocates-rp335-trillion-for-free-meals-program-in-2026](https://en.antaranews.com/news/399809/indonesia-allocates-rp335-trillion-for-free-meals-program-in-2026)\n\n[https://sahellibertynews.com/2026/02/23/burkina-faso-fighting-terrorism-the-need-to-avoid-the-pitfalls-of-media-misinformation/](https://sahellibertynews.com/2026/02/23/burkina-faso-fighting-terrorism-the-need-to-avoid-the-pitfalls-of-media-misinformation/)\n\nAnd yes, the Figma link shows the bare bones of what I'm trying to do (I like things complicated so I'm hoping to do far more than what's shown, but that link definitely shows the basic gist).\n\nAnd I think pumping an RSS feed into my project is great if I was creating something like an app that feeds you news, or even to creating like a sidebar showing updates from various sources, but since I'm hoping to self-host this content and drive traffic to me, I think I'd still have to add a bit of AI summarization or other \"creative touches\" so that my work isn't viewed as pure copyright infringement. And not all sites have RSS feeds. But since many do, maybe I'll start developing a workflow with RSS and then branch out into scraping and other tools later.",
              "score": 2,
              "created_utc": "2026-02-28 02:28:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7whjg3",
                  "author": "justincampbelldesign",
                  "text": "Got it makes sense thanks for the info. When you say \"you like things complicated\" you mean the process of pulling the data? \nYou also said you're trying to do more than what's shown, does that mean you have additional pieces of data you are pulling or is there more you're trying to do than what's in the original post? \nHave you already tried using replit or similar to quickly build something that can pull this data for you or are you set on finding a pre built option?",
                  "score": 1,
                  "created_utc": "2026-02-28 16:32:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7to40v",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-28 04:00:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7u82xb",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-28 06:33:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rhm1oq",
      "title": "Monthly Self-Promotion - March 2026",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rhm1oq/monthly_selfpromotion_march_2026/",
      "author": "AutoModerator",
      "created_utc": "2026-03-01 03:00:29",
      "score": 9,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "Hello and howdy, digital miners ofÂ r/webscraping!\n\nThe moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!\n\n* Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?\n* Maybe you've got a ground-breaking product in need of some intrepid testers?\n* Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?\n* Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?\n\nWell, this is your time to shine and shout from the digital rooftops - Welcome to your haven!\n\nJust a friendly reminder, we like to keep all our self-promotion in one handy place, so any promotional posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rhm1oq/monthly_selfpromotion_march_2026/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o7zvcsg",
          "author": "HLCYSWAP",
          "text": "I have defeated datadome, cloudflare, akamai and perimeterX at scale. (ticketmaster, meta, x, cia's FOIA, live, dozens of smaller client specific targets)\n\nETL (scraping), databasing, ML training AI (captcha bypass, voice, transformers - LLMs), standard desktop apps like DAWs as seen on my github. my past successes here:\n\n[https://github.com/matthew-fornear](https://github.com/matthew-fornear)\n\nopen to short or long work. you can reach me via reddit, x ([https://www.x.com/fixitorgotojail](https://www.x.com/fixitorgotojail)) or discord",
          "score": 6,
          "created_utc": "2026-03-01 03:48:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o80f9u8",
              "author": "Salty-Mouse7235",
              "text": "Hi,\n\nI have a project. How can i reach out to you? X wont let me message you as I dont have premium",
              "score": 2,
              "created_utc": "2026-03-01 06:17:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o81y3nb",
                  "author": "HLCYSWAP",
                  "text": "I sent you a reddit chat request",
                  "score": 1,
                  "created_utc": "2026-03-01 14:07:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o8053bx",
          "author": "rishiilahoti",
          "text": "Created an open source job scraper for Ashby Hq Jobs.\n\nI was tired of manually checking career pages every day, so I built a full-stack job intelligence platform that scrapes AshbyHQ's public API (used by OpenAI, Notion, Ramp, Cursor, Snowflake, etc.), stores everything in PostgreSQL, and surfaces the best opportunities through a Next.js frontend.\n\nWhat it does:\n\n\\* Scrapes 53+ companies every 12 hours via cron\n\n\\* User can add company via pasting url with slug (jobs.ashbyhq.com/{company})\n\n\\* Detects new, updated, and removed postings using content hashing\n\n\\* Scores every job based on keywords, location, remote preference, and freshness\n\n\\* Lets you filter, search, and mark jobs as applied/ignored (stored locally per browser)\n\nTech: Node.js backend, Neon PostgreSQL, Next.js 16 with Server Components, Tailwind CSS. Hosted for $0 (Vercel + Neon free tier + GitHub Actions for the cron).\n\nWould love suggestions on the project.  \n\nGithub Repo: \\[https://github.com/rishilahoti/ashbyhq-scraper\\](https://github.com/rishilahoti/ashbyhq-scraper)  \n\nLive Website: \\[https://ashbyhq-scraper.vercel.app/\\](https://ashbyhq-scraper.vercel.app/)",
          "score": 2,
          "created_utc": "2026-03-01 04:57:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80ow1b",
          "author": "Hot-Muscle-7021",
          "text": "We are now offering access to Bet365 data via our platform: [https://pulsescore.net/](https://pulsescore.net/)\n\nOur goal is to provide structured, reliable, and low-latency Bet365 data for developers, analysts, and businesses that need high-quality sports data feeds.\n\nYou can explore whatâ€™s currently available through our **free subscription**, which gives you direct access to data and lets you evaluate the quality, structure, and coverage.\n\nMore datasets, expanded coverage, and additional features are coming soon.\n\nIf youâ€™re building trading models, analytics tools, betting platforms, or data-driven applications, this can serve as a solid foundation.\n\nVisit: [https://pulsescore.net/](https://pulsescore.net/)  \nFree access available.\n\nIf you have questions or need specific datasets, feel free to reach out.",
          "score": 2,
          "created_utc": "2026-03-01 07:44:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80znvi",
          "author": "CouldBeNapping",
          "text": "Signed my 8th retail client last week for price scraping, PDP checking and competitor promotion reviews.  \nMeans ARR will jump to Â£110,000. Almost at the point where I'm considering ditching the day job.  \n",
          "score": 2,
          "created_utc": "2026-03-01 09:26:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o813ni8",
              "author": "No-Appointment9068",
              "text": "Congratulations! If you wouldn't mind, can you give me some details on what you offer your clients? I'm looking to move into a similar niche.",
              "score": 1,
              "created_utc": "2026-03-01 10:04:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o816b0n",
                  "author": "CouldBeNapping",
                  "text": "I'm not going to give the secret sauce another person in the UK I'm afraid. It's already a crowded market so I don't want more noise ;)",
                  "score": 1,
                  "created_utc": "2026-03-01 10:30:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o80zu1g",
          "author": "deepwalker_hq",
          "text": "Deepwalker - the ultimate mobile AI agent\n\nhttps://deepwalker.xyz",
          "score": 2,
          "created_utc": "2026-03-01 09:28:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80mlm3",
          "author": "digital__navigator",
          "text": "Noob compared to others, web scraped 86,467 courses from 9 universities, you can see course catalog stats like longest and shortest coursenames for them all.\n\n  \nPut everything on a website.\n\n[https://www.degreeviewsite.com/](https://www.degreeviewsite.com/)\n\nIm a uni student trying to do entreprenuership one day.\n\n",
          "score": 1,
          "created_utc": "2026-03-01 07:22:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80p1no",
          "author": "Jonathan_Geiger",
          "text": "Social media API for extracting transcripts, summaries, video data, and more (:\n\n[SocialKit](https://socialkit.dev)",
          "score": 1,
          "created_utc": "2026-03-01 07:45:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o81gxc5",
          "author": "scorpiock",
          "text": "Geekflare API to scrape any website into Markdown, HTML and JSON.\n\n\nhttps://geekflare.com/api/webscraping/\n\n\n500 free credits to try.Â ",
          "score": 1,
          "created_utc": "2026-03-01 12:07:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcxwo5",
      "title": "Scrape transcripts from Spotify",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rcxwo5/scrape_transcripts_from_spotify/",
      "author": "AnglePast1245",
      "created_utc": "2026-02-23 23:36:52",
      "score": 6,
      "num_comments": 9,
      "upvote_ratio": 0.81,
      "text": "Does anyone know a reliable way (via API, browser extension, script, or tool) to scrape or export full episode transcripts from Spotify podcasts?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rcxwo5/scrape_transcripts_from_spotify/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o73tadx",
          "author": "boomersruinall",
          "text": "Following, as I am also looking for answers regarding transcripts",
          "score": 3,
          "created_utc": "2026-02-24 08:51:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7delxm",
          "author": "jagdish1o1",
          "text": "Have you tried charles to intercept the api requests?",
          "score": 2,
          "created_utc": "2026-02-25 18:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7577nl",
          "author": "yyavuz",
          "text": "In the same boat",
          "score": 1,
          "created_utc": "2026-02-24 14:49:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ak2f2",
          "author": "vorty212",
          "text": "Following",
          "score": 1,
          "created_utc": "2026-02-25 08:20:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rffmmh",
      "title": "Should I focus on bypassing Cloudflare or finding the internal API?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rffmmh/should_i_focus_on_bypassing_cloudflare_or_finding/",
      "author": "Otherwise-Advance466",
      "created_utc": "2026-02-26 16:59:16",
      "score": 6,
      "num_comments": 18,
      "upvote_ratio": 0.8,
      "text": "Hey r/webscraping,\n\nI've been researching web scraping with Cloudflare protection for a while now and I'm at a crossroads. I've done a lot of reading (Stack Overflow threads, GitHub issues, etc.) and I understand the landscape pretty well at this point â€“ but I can't decide which approach to actually invest my time in.\n\n# What I've already learned / tried conceptually:\n\n* `undetected_chromedriver` works against basic Cloudflare **but not in headless mode**\n* The workaround for headless on Linux is **Xvfb** (virtual display) with SeleniumBase UC Mode\n* `playwright-stealth`, manually copying cookies/headers, FlareSolverr â€“ all **unreliable** against aggressive Cloudflare configs\n* Copying `cf_clearance` cookies into Scrapy requests **doesn't work** because Cloudflare binds them to the original TLS fingerprint (JA3)\n* For serious Cloudflare (Enterprise tier) basically nothing open-source works reliably\n\n# My actual question:\n\nI've heard that many sites using Cloudflare on their frontend actually have **internal APIs** (XHR/Fetch calls) that are either less protected or protected differently (e.g. just an API key).\n\nShould I:\n\n**Option A)** Focus on bypassing Cloudflare using SeleniumBase UC Mode + Xvfb, accepting that it might break at any time and requires a non-headless setup\n\n**Option B)** Dig into the Network tab of the target site, find the internal API calls, and try to replicate those directly with Python requests â€“ potentially avoiding Cloudflare entirely\n\n**Option C)** Something else entirely that I'm missing?\n\n# My constraints:\n\n* Running on Linux server (so headless environment)\n* Python preferred\n* Want something reasonably stable, not something that breaks every 2 weeks when Cloudflare updates\n\nWhat would you do in my position? Has anyone had success finding internal APIs on heavily Cloudflare-protected sites? Any tips on what to look for in the Network tab?\n\nThanks in advance\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rffmmh/should_i_focus_on_bypassing_cloudflare_or_finding/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o7ju9ju",
          "author": "Flojomojo0",
          "text": "You should always try to find the internal apis as it simplifies scraping by a lot",
          "score": 8,
          "created_utc": "2026-02-26 17:41:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jw297",
          "author": "Objectdotuser",
          "text": "headless mode will never work, too easy to detect. just commit to running a bunch of machines and browsers",
          "score": 4,
          "created_utc": "2026-02-26 17:49:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lbnu7",
          "author": "Hopeless_Scraping",
          "text": "Use the TLS fingerprints used for the clearance and replicate them in your request",
          "score": 3,
          "created_utc": "2026-02-26 21:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k3plx",
          "author": "scrape-do",
          "text": "*Completely depends on your target domain, but I'm going to assume you have a specific domain in mind that's very aggressively protected by CF.*\n\n  \nSeems like you've tried your luck with the bypass approach, so I would go for the internal API if I were you. They usually have light-protection and you can mimic a legitimate backend call with a few cookies or the right payload.\n\n  \nIf you're building scrapers for multiple domains, go for the internal API first **EVERY TIME** unless it's a basic server-rendered site, you'll build a muscle for it and save huge time on setup and maintenance, not mentioning performance. Compared to front-end changes and CF updates, backend API scrapers rarely breaks.",
          "score": 1,
          "created_utc": "2026-02-26 18:24:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7k3zn6",
              "author": "scrape-do",
              "text": "Although there might be times where the backend is virtually impossible to crack at large-scale, so keep Selenium as an option at all times :)",
              "score": 1,
              "created_utc": "2026-02-26 18:26:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7k3thm",
          "author": "Pauloedsonjk",
          "text": "c + b, a.\nA)I use uc with xvfb in production, we have PHP + python, with proxy, captcha\nand\nC+B) too solved in this way when I need it.",
          "score": 1,
          "created_utc": "2026-02-26 18:25:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kctcp",
          "author": "bluewhalefunk",
          "text": "cloudflare is fine but you good IPs.  Playright patched works fine. IF you get it. tab tab enter (or space) and it will solve\n\n> I've heard that many sites using Cloudflare on their frontend actually have internal APIs (XHR/Fetch calls) that are either less protected or protected differently (e.g. just an API key).\n\nNo one can tell you. That's the thing with scraping. 9/10 no one knows the issues you will fsce until they have done exactly what you want to do. You just have to do it, discover the issues and bang your head against the wall until you solve them.  I've done this for 15+ years, trust me, knowing how you bang your head against the way for hours days weeks until you solve it is the most useful skill you can have.",
          "score": 1,
          "created_utc": "2026-02-26 19:06:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kvlrb",
          "author": "AdministrativeHost15",
          "text": "API calls are protected too. I investigated an error and instead of JSON it was returning the HTML of the captcha page.",
          "score": 1,
          "created_utc": "2026-02-26 20:36:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l4nsp",
              "author": "Otherwise-Advance466",
              "text": "so how do website monitors work then? especially like sneaker retailer monitors, how do they bypass cloudflare ",
              "score": 1,
              "created_utc": "2026-02-26 21:19:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lfxw6",
          "author": "TabbyTyper",
          "text": "Not sure it can answered here, but what sites are running enterprise-grade cloudflare? Are those tougher than casino sites to avoid detection?",
          "score": 1,
          "created_utc": "2026-02-26 22:13:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mm8kx",
          "author": "cyber_scraper",
          "text": "1. Option B should be always priority where it's feasible.\n2. Compare your costs for all scraping infra to run Option A with some 3rd party services who could provide API to protected websites you need (of course if exist). Maybe it's not worth to build/maintain.\n3. If B and C don't fit - experiment with A",
          "score": 1,
          "created_utc": "2026-02-27 02:06:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7n7cxm",
          "author": "Coding-Doctor-Omar",
          "text": "For browser automation, try strong automation libraries/frameworks such as Scrapling and cloakbrowser. If you want to use scrapy, use scrapy impersonate in order to spoof TLS fingerprints. If you want to use requests, use curl_cffi instead to spoof TLS fingerprints. Obviously, if you can find internal APIs, go for them since that's the best option.",
          "score": 1,
          "created_utc": "2026-02-27 04:13:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nm22n",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-27 05:58:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7odso2",
              "author": "webscraping-ModTeam",
              "text": "ðŸ‘” Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
              "score": 1,
              "created_utc": "2026-02-27 10:08:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7nzi5k",
          "author": "irrisolto",
          "text": "Apis are behind cloudfare waf too. You need to find out what kind of cloudfare challenge the website has. If itâ€™s just jsd you can use an opensource solver to get cf_clearence and use it for api requests. If the site itâ€™s low sec you can get cf_clearence just by making a req to the home page and use it for the api requests. If the page is under UAM you need a solver / browser to get cf cookie",
          "score": 1,
          "created_utc": "2026-02-27 07:53:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7oaixq",
          "author": "Curious_Anteater7293",
          "text": "If there is no API you can use, then bypassing cloudflare is the only option (if you want it free of course. If you dont care about paying for acraping, just use services that do all the work for you)\n\nHonestly, after a week of delwing inside of the antibor systems, making your own bypass is not that hard. However, it requires a lot of skill and time. The best way to bypass the cloudflare is to firstly understand what it does to detect a bot. It scans a lot of params in your browser, using fingerprints and many other things.\n\nWhen I developed my bot system, I made a MITM proxy chain gate that replaced TLS and HTTP/2 fingerprints with valid ones based on the bot's useragent. Then, you need to spoof a ton of fingerprints: webgl, canvas, fonts, useragent, screen resolution, timezone, locale, even Audio.\n\nAlso, you should understand that a real user never can type the whole sentence in a millisecond or click right in the center of the button. Behavioral patterns are important, too\n\nI can only wish you luck xD",
          "score": 1,
          "created_utc": "2026-02-27 09:37:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7q1iko",
          "author": "kingxx773",
          "text": "go with internal api i do same thing too. it will make things faster",
          "score": 1,
          "created_utc": "2026-02-27 16:18:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7u97cv",
          "author": "jagdish1o1",
          "text": "Try to find internal apis first, and if thatâ€™s protected too try making request from cf worker ;) \n\nJust recently i had this issue where the api was returning html but in the network tab itâ€™s showing JSON response. Iâ€™ve created an api on cf worker which simply makes the request to the given url and it surprisingly worked! \n\nMy guess is the api whitelisted the cf network.",
          "score": 1,
          "created_utc": "2026-02-28 06:43:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfdqsb",
      "title": "Web Scraper / Researcher Needed â€“ Pre-Opening  Business Leads",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rfdqsb/web_scraper_researcher_needed_preopening_business/",
      "author": "techguyfl17",
      "created_utc": "2026-02-26 15:51:03",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "\n\nDescription:\n\nIâ€™m looking for an experienced web scraper or researcher to help identify brick-and-mortar SMB businesses that are under construction or preparing to open in Florida (starting South Florida/ Florida).\n\nObjective:  \nGenerate weekly leads of businesses BEFORE they launch so I can offer MSP / full-suite technology services.\n\nPrimary Sources:  \nâ€¢ County & city permit databases (Tenant Improvement, Buildout, Commercial Remodel, New Construction)  \nâ€¢ Business license filings  \nâ€¢ Local business journals  \nâ€¢ â€œComing Soonâ€ storefronts  \nâ€¢ Commercial lease announcements\n\nRequired Data:  \nâ€¢ Business name  \nâ€¢ Address  \nâ€¢ Industry/type  \nâ€¢ Permit date + status  \nâ€¢ Estimated opening date (if available)  \nâ€¢ Email/contact (or source link for enrichment)  \nâ€¢ Direct source link\n\nDeliverables:  \nâ€¢ Weekly Google Sheet or CSV  \nâ€¢ No duplicates  \nâ€¢ Fresh leads (last 30 days)  \nâ€¢ Organized + structured format\n\nTo apply:\n\n1. Describe your experience scraping government portals.\n2. Tell me what tools you use (Python, BeautifulSoup, Scrapy, etc.).\n3. Share a sample output (if available).\n4. Quote hourly rate or per-lead pricing.\n\nThis will  become ongoing weekly work for the right candidate.\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Hiring ðŸ’°",
      "permalink": "https://reddit.com/r/webscraping/comments/1rfdqsb/web_scraper_researcher_needed_preopening_business/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rh4kcy",
      "title": "webscraping websites for arbitrage",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rh4kcy/webscraping_websites_for_arbitrage/",
      "author": "misterno123",
      "created_utc": "2026-02-28 14:51:43",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.76,
      "text": "Currently I am running a webscraper from home using data center proxies. I scrape only the ASINs in websites where same item has low rank on amazon. It is scraping sites with items for sale in bulk and I buy them on the cheap and sell them on amazon as new. This is just 1 item so to expand , I tried this with electronics and auto parts but most sites asking for physical location to buy in bulk\n\nIt does not have to be on amazon I can sell on ebay also, but I am looking for websites to buy in bulk. Any ideas? or is there a better subreddit to ask this question?\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rh4kcy/webscraping_websites_for_arbitrage/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o81apk7",
          "author": "glowandgo_",
          "text": "not really my lane, but pure scraping arb usually compresses fast. once you automate it, others do too. margins go to zero, esp in electronics.,sites asking for physical location is prob screening resellers, signal about supply control...youâ€™ll get better answers in r/FulfillmentByAmazon or r/Flipping. this sub isnâ€™t super ops focused..if expanding, think moat not just more sites. pure arb becomes a race.",
          "score": 2,
          "created_utc": "2026-03-01 11:12:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdbvik",
      "title": "What's working for you with proxy rotation?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rdbvik/whats_working_for_you_with_proxy_rotation/",
      "author": "Forsaken-Bobcat4065",
      "created_utc": "2026-02-24 09:40:07",
      "score": 3,
      "num_comments": 24,
      "upvote_ratio": 0.81,
      "text": "Â Iâ€™ve been down the scraping rabbit hole lately and honestlyâ€¦ Iâ€™m spending way too much time dealing with rate limits, CAPTCHAs, random blocks, and instability. \n\nWhat are people using these days to manage proxies and keep things running smoothly? Rotating residential or datacenter proxies, specific libraries, browser automation, or a mix? \n\nIâ€™m just looking for something that actually works in real-world projects without becoming a full-time maintenance job. Any tools or setups that have made things more stable and hands-off?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rdbvik/whats_working_for_you_with_proxy_rotation/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o75xjf7",
          "author": "Gold_Emphasis1325",
          "text": "The working environment to operate like this is getting crushed. Enough ankle biters creating bots and \"Agents\" caused big players to finally invest the time into closing known gaps that allowed more TOS violations and scraping. It's only going to get more difficult to get away with things. Any long-term plan relying on scraping or having \"robust scraping\" is likely not a plan at all.",
          "score": 6,
          "created_utc": "2026-02-24 16:50:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75520e",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-02-24 14:38:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75axnp",
              "author": "webscraping-ModTeam",
              "text": "âš¡ï¸ Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-02-24 15:07:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ahux6",
          "author": "thomas_estate",
          "text": "Residential proxies are the way to go for anything with serious anti-bot protection. Datacenter IPs get flagged almost immediately on most major sites.\n\nFor rotation, I've had good results with backconnect proxy services that handle the rotation on their end â€” you just hit one endpoint and they cycle IPs automatically. Way less headache than managing your own pool.\n\nBrowser automation (Playwright/Puppeteer) with stealth plugins helps a ton with CAPTCHAs. Some sites still require manual solving services, but between that and residential IPs, most blocks disappear.\n\nWhat scale are you running at? And what types of sites mainly? The approach shifts a lot depending on whether you're hitting a few targets hard vs. scraping broadly across many domains.",
          "score": 2,
          "created_utc": "2026-02-25 07:59:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74orx1",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-24 13:09:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74qoen",
              "author": "webscraping-ModTeam",
              "text": "âš¡ï¸ Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-02-24 13:20:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75m3xf",
          "author": "VonDenBerg",
          "text": "GOOD providers, not trash. ",
          "score": 1,
          "created_utc": "2026-02-24 15:59:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7f5vkf",
              "author": "HardReload",
              "text": "And how would one find those?",
              "score": 1,
              "created_utc": "2026-02-25 23:37:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7h7uag",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 1,
                  "created_utc": "2026-02-26 07:36:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77345g",
          "author": "Alone-Rub-4418",
          "text": "I feel you on the rabbit hole thing. I've had decent luck with a mix of residential proxies and a simple backoff/retry strategy in my scripts. Honestly, the biggest game changer for me was just accepting that some blocks are inevitable and building my scraper to fail gracefully and pick up where it left off",
          "score": 1,
          "created_utc": "2026-02-24 19:57:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78ychh",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-25 01:37:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79dj43",
              "author": "webscraping-ModTeam",
              "text": "ðŸš«ðŸ¤– No bots",
              "score": 1,
              "created_utc": "2026-02-25 03:02:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o790hj1",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-25 01:49:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79dkep",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-25 03:02:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79ttfj",
          "author": "OwnPrize7838",
          "text": "I only use static clean 0 fraud ISP",
          "score": 1,
          "created_utc": "2026-02-25 04:44:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7f5jdm",
              "author": "HardReload",
              "text": "Can you elaborate?",
              "score": 1,
              "created_utc": "2026-02-25 23:35:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rcad6w",
      "title": "Anyone else seeing more blocking from cloud IPs lately?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rcad6w/anyone_else_seeing_more_blocking_from_cloud_ips/",
      "author": "bertdida",
      "created_utc": "2026-02-23 06:49:33",
      "score": 3,
      "num_comments": 9,
      "upvote_ratio": 0.81,
      "text": "Not sure if it's just me, but Iâ€™ve been building scraping-heavy automation lately and noticed something.\n\nEverything works fine locally. Once I deploy to AWS or other cloud providers, some sites start blocking almost immediately.\n\nI already tried adjusting headers, user agents, delays between requests. Still inconsistent. Feels like datacenter IPs are getting flagged much faster now compared to before.\n\nHow are you guys handling this in production? Are datacenter IPs basically unreliable now for certain sites?\n\nJust curious what others are doing.",
      "is_original_content": false,
      "link_flair_text": "Bot detection ðŸ¤–",
      "permalink": "https://reddit.com/r/webscraping/comments/1rcad6w/anyone_else_seeing_more_blocking_from_cloud_ips/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6wvj6f",
          "author": "irrisolto",
          "text": "Datacenter ips were always been flagged. Try using proxies",
          "score": 7,
          "created_utc": "2026-02-23 06:54:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wxtd7",
              "author": "bertdida",
              "text": "True. I guess I'm just noticing it feels even more aggressive lately. ",
              "score": 1,
              "created_utc": "2026-02-23 07:15:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6wwb1y",
          "author": "albert_in_vine",
          "text": "Most of the datacenter proxies gets flagged easily, try usin isp or residential proxies",
          "score": 2,
          "created_utc": "2026-02-23 07:01:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wxjl0",
              "author": "bertdida",
              "text": "Yea, residential works in some cases, but it can get expensive. Still experimenting to find a stable setup.",
              "score": 1,
              "created_utc": "2026-02-23 07:12:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xk3xt",
          "author": "glowandgo_",
          "text": "oh oh thatâ€™s pretty common now. a lot of sites just blanket flag known datacenter ranges, especially from aws/gcp....headers and delays help a bit, but if the ip reputation is burned youâ€™re fighting uphill. some teams move toward residential or proxy rotation, others rethink whether scraping is worth the arms race at all. depends a lot on the target and how aggressive they are.",
          "score": 2,
          "created_utc": "2026-02-23 10:51:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z4yg6",
          "author": "illusivejosiah",
          "text": "Yep, Iâ€™ve seen the same thing. It works locally because your home ISP looks like a normal user, then you deploy to AWS/GCP and youâ€™re suddenly coming from a known datacenter range, so some sites will challenge or block you almost immediately. Tweaking headers and adding delays can help once you get past IP reputation, but it wonâ€™t rescue a cloud IP thatâ€™s already scored as â€œhosting.â€ In my experience, if you get blocked in the first few requests itâ€™s mostly the IP range, and if it runs for a while then starts throwing 403/429/CAPTCHA pages itâ€™s more rate limits or bot detection (sessions, fingerprints, headless). Most production setups either keep compute in AWS and route outbound traffic through residential/ISP/mobile proxies (and keep the same IP for a short session instead of rotating every request), or they run the scraper from residential/ISP egress and just ship results back to the cloud. Datacenter IPs are still fine for easy targets, but once a site is running Cloudflare or one of the big bot vendors you usually need higher-trust IPs and often a real browser. If you can paste one example response (status code plus a couple headers, redact cookies), itâ€™s usually pretty obvious what kind of block youâ€™re hitting.",
          "score": 1,
          "created_utc": "2026-02-23 16:33:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fzm88",
          "author": "Resident-Piano-1663",
          "text": "I'm getting my surf data blocked when I make requests from my droplet but not from my home computer",
          "score": 1,
          "created_utc": "2026-02-26 02:24:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kb5ym",
          "author": "scrape-do",
          "text": "https://i.redd.it/pz6vz7t6zvlg1.gif\n\nIn a nutshell.",
          "score": 1,
          "created_utc": "2026-02-26 18:58:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdfm7a",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rdfm7a/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-02-24 13:01:04",
      "score": 3,
      "num_comments": 8,
      "upvote_ratio": 0.67,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levelsâ€”whether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) ðŸŒ±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring ðŸ’°",
      "permalink": "https://reddit.com/r/webscraping/comments/1rdfm7a/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o763nqx",
          "author": "jagdish1o1",
          "text": "Hey! Iâ€™m not sure what category I fall into when it comes to scraping, but Iâ€™ve done plenty of scraping projects over the years and have gained solid knowledge of how to scrape various websites.\n\nHere are some tips from my side:\n\n1. Try to avoid using browsers for scraping unless itâ€™s absolutely necessary. Even if you have to use one, capture the request headers from the browser and try to mimic the request using those headers instead.\n2. Use residential rotating proxies for recurring scraping tasks, especially when you need to scrape a site on a daily basis.\n3. Consider integrating AI into your HTML parsing. This can save you a lot of maintenance work in the long run. Just make sure to enforce structured output.\n4. Write modular code instead of putting everything into one or two scripts. This will save you time on future projects and make maintenance easier.\n5. Use exponential backoff instead of simple retries. Even better, use exponential backoff with jitter. This helps reduce bottlenecks and handle rate limiting more effectively.\n\nIf you already have strong scraping knowledge, consider building APIs for popular websites and selling them on RapidAPI.\n\nThese are the points that come to mind right now. Iâ€™ll add more in a reply if I think of anything else.\n\nPeace âœŒï¸",
          "score": 5,
          "created_utc": "2026-02-24 17:17:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7anwp8",
              "author": "GoingGeek",
              "text": "ai is good but which local small model would u recommend for fast parsing.",
              "score": 1,
              "created_utc": "2026-02-25 08:56:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ar31k",
                  "author": "jagdish1o1",
                  "text": "I use openai or gemini models, havenâ€™t tried ai models locally. Apis works just fine.",
                  "score": 1,
                  "created_utc": "2026-02-25 09:26:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ew0xq",
                  "author": "Azuriteh",
                  "text": "Pretty much any SLM post 2025, e.g. Qwen3 4b 2507 should work pretty well",
                  "score": 1,
                  "created_utc": "2026-02-25 22:45:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ao4a7",
              "author": "GoingGeek",
              "text": "and do u mind me knocking u in dm\n\n",
              "score": 1,
              "created_utc": "2026-02-25 08:58:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ar6ju",
                  "author": "jagdish1o1",
                  "text": "Sure as long as youâ€™re not selling something.",
                  "score": 2,
                  "created_utc": "2026-02-25 09:27:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7tu4of",
          "author": "mnlaowai",
          "text": "Iâ€™m looking for someone to help me build something to scrape a relatively simple association directory on a website. Effectively, you need to click on A, and then all of the members with the last name of A come up. Click on each name to get their position and contact info. I want an excel file just listing all of this data. Iâ€™m not a coder but using regular LLMs, I could get A and B done. Would prefer not to spend a couple hours on it though and figure someone here might be able to make something relatively quickly.",
          "score": 1,
          "created_utc": "2026-02-28 04:42:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zvboq",
              "author": "OkAd6989",
              "text": "Which directory is this? Might be able to assist. ",
              "score": 1,
              "created_utc": "2026-03-01 03:48:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rgfbv7",
      "title": "[HELP] How to scrape dynamic webistes with pagination",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rgfbv7/help_how_to_scrape_dynamic_webistes_with/",
      "author": "nirvana_49",
      "created_utc": "2026-02-27 18:50:49",
      "score": 3,
      "num_comments": 11,
      "upvote_ratio": 0.81,
      "text": "Scraping this URL: \\`https://www.myntra.com/sneakers?rawQuery=sneakers\\`\n\nPagination is working fine â€” the meta text updates (\\`Page 1 of 802 â†’ Page 2 of 802\\`) after clicking \\`li.pagination-next\\`, but \\`window.\\_\\_myx.searchData.results.products\\` always returns the same 32 product IDs regardless of which page I'm on.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rgfbv7/help_how_to_scrape_dynamic_webistes_with/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o7ruj8z",
          "author": "cyber_scraper",
          "text": "If you check network tab in dev tools you would see that there are calls to internal api gateway like:  \n[https://www.myntra.com/gateway/v4/search/sneakers%60?rawQuery=sneakers%60&rows=50&o=99&plaEnabled=true&xdEnabled=false&isFacet=true&p=3](https://www.myntra.com/gateway/v4/search/sneakers%60?rawQuery=sneakers%60&rows=50&o=99&plaEnabled=true&xdEnabled=false&isFacet=true&p=3)   \n  \nSo you just need to handle cookies and change page",
          "score": 4,
          "created_utc": "2026-02-27 21:33:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7x827a",
              "author": "nirvana_49",
              "text": "Thanks",
              "score": 2,
              "created_utc": "2026-02-28 18:44:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7shuwc",
          "author": "abdullah-shaheer",
          "text": "I think you can easily use it's API as it is very prominent in the network requests. Here is an example to get the data:-\n\nimport requests\n\ncookies = {your cookies here from the website}\n\nheaders = {your headers}\n\n\\# use these params to query\n\nparams = {\n\n'rawQuery': 'sneakers\\`',\n\n'rows': '50',\n\n'o': '99',\n\n'plaEnabled': 'true',\n\n'xdEnabled': 'false',\n\n'isFacet': 'true',\n\n'p': '3',\n\n}\n\n\n\nresponse = requests.get('https://www.myntra.com/gateway/v4/search/sneakers%60', params=params, cookies=cookies, headers=headers)\n\nprint(response.text)",
          "score": 2,
          "created_utc": "2026-02-27 23:39:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7rmp9x",
          "author": "BrightProgrammer9590",
          "text": "On each result page, wait for the products to become available. Then parse the list. You may even have to keep track of one of your last product elements to make sure it is gone before assuming new product list is loaded",
          "score": 1,
          "created_utc": "2026-02-27 20:54:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7s0r4e",
          "author": "bootlegDonDraper",
          "text": "I got it working through Playwright.\n\n`window.__myx.searchData.results.products` is set once on page load, and won't update with pagination clicks.\n\nWhen you click next, the browser fires an XHR to \\`/gateway/v4/search/sneakers?rawQuery=sneakers&rows=50&o=49&...\\` which has the next page of products. The frontend updates the DOM from it but doesn't write back to myx, weird choice on Myntra's end for sure.\n\nSo you should intercept that network response instead of reading myx by listening to responses matching \\`/gateway/v4/search/\\` and read .products from the JSON body.",
          "score": 1,
          "created_utc": "2026-02-27 22:04:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7s50lw",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-27 22:27:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7u33gc",
          "author": "jagdish1o1",
          "text": "Alway look for internal apis first, donâ€™t rely on html parsing.",
          "score": 1,
          "created_utc": "2026-02-28 05:51:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7wwe7i",
          "author": "EntranceSorry7225",
          "text": "Sounds like the pagination might be loading new content via JavaScript but the data object isn't being refreshed. Try checking the network tab in dev tools to see if there's a separate API call being made when you click next sometimes the product data gets fetched from a different endpoint",
          "score": 1,
          "created_utc": "2026-02-28 17:47:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1reg5wg",
      "title": "How to scrape ios/android top downloaded apps for a specific country?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1reg5wg/how_to_scrape_iosandroid_top_downloaded_apps_for/",
      "author": "letopeto",
      "created_utc": "2026-02-25 15:24:02",
      "score": 2,
      "num_comments": 6,
      "upvote_ratio": 0.63,
      "text": "How do you scrape ios/android top downloaded apps (free & paid) for a specific country (Sweden)? beyond 100+ results? The endpoint people use (rss.applemarketingtools.com and itunes) only seem to return 100 results.\n\nI can't figure out what the correct api url to query is and if any auth is required. Any help would be greatly appreciated.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1reg5wg/how_to_scrape_iosandroid_top_downloaded_apps_for/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o7cejqy",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-02-25 15:51:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cj961",
              "author": "letopeto",
              "text": "yes im aware of that website, hence why I made this post. I already made a free trial for the website to get the rankings beyond 30 (they only give 30 w/ a free account, and you get the full list on a paid account/trial). But clearly they are able to scrape beyond 100 but all the endpoints I'm trying is limited to 100 results:\n\ne.g. https://itunes.apple.com/se/rss/topfreeapplications/limit=200/genre=36/json \n\n(only gives 100 despite limit set at 200). Apparently this used to return 200 results but they have limited it to 100.\n\nI think there is a new endpoint url to scrape results from because obviously that website is able to get a list beyond 100 result. How are they doing it?\n\nMy goal isn't just to grab the results today, its a daily scrape to see how rankings have changed over time, and ideally i scrape myself vs having to rely on a 3rd party to do it for me that i have to pay for.",
              "score": 1,
              "created_utc": "2026-02-25 16:13:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7cs5b2",
                  "author": "CouldBeNapping",
                  "text": "I wouldnâ€™t waste your time or energy trying to hack through Appleâ€™s security/obscurity for the App Store. \n\nYouâ€™ll likely find that the site I linked to has setup loads of virtual iPhones in Xcode and uses OCR to parse the data. \n\nCould do it yourself but effort and cost vs. impact",
                  "score": 2,
                  "created_utc": "2026-02-25 16:53:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7cyoed",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-25 17:23:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cqate",
          "author": "Medical-Road-5690",
          "text": "Yeah, the 100 result limit on those public endpoints is a known headache. I've had luck using the official App Store Connect API for iOS data it requires a dev account for authentication but can give you deeper country specific charts. For Android, scraping the Play Store directly with something like undetected requests might be your best bet to get past the first page",
          "score": 1,
          "created_utc": "2026-02-25 16:45:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}