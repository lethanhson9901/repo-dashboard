{
  "metadata": {
    "last_updated": "2025-12-30 22:29:59",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 11,
    "total_comments": 54,
    "file_size_bytes": 66033
  },
  "items": [
    {
      "id": "1pvobl0",
      "title": "Why do people think web scraping is a free service?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pvobl0/why_do_people_think_web_scraping_is_a_free_service/",
      "author": "unstopablex5",
      "created_utc": "2025-12-25 22:17:53",
      "score": 96,
      "num_comments": 23,
      "upvote_ratio": 0.91,
      "text": "I‚Äôve been on this sub for years, and I‚Äôm consistently surprised by how many posts ask for basic scraping help without any prior effort.\n\nIt‚Äôs rarely questions like ‚Äúhow do I avoid advanced fingerprinting or bot detection.‚Äù Instead, it‚Äôs almost always ‚Äúhow do I scrape this static HTML page.‚Äù These are problems that have been answered hundreds of times and are easily searchable.\n\nScraping can be complex, but not every problem is. When someone hasn‚Äôt tried searching past threads, Googling, or even using ChatGPT before posting, it lowers the overall quality of discussion here.\n\nI‚Äôm not saying beginners shouldn‚Äôt ask questions. But low effort questions with no context or attempted solution shouldn‚Äôt be the norm.\n\nWhat‚Äôs more frustrating are requests that implicitly expect a full pipeline. Scraping, data cleaning, storage, and reliability are not a single snippet of code. That is a product, not a quick favor.\n\nIf someone needs that level of work, the options are to invest time into learning or pay someone who already has the expertise. **Scraping is not a trivial skill. It borrows heavily from data engineering and software engineering, and treating it as free labor undervalues the work involved.**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1pvobl0/why_do_people_think_web_scraping_is_a_free_service/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nvxn469",
          "author": "cgoldberg",
          "text": "This is nothing specific to web scraping... It's just the state of technical and programming questions in general, and it always has been. Visit any programming community or forum anywhere on the internet and it's full of newbies with misconceptions and unrealistic expectations asking questions that have been answered a thousand times before, and a bunch of frustrated veterans shouting \"RTFM\" or telling them what they think is actually wrong.",
          "score": 42,
          "created_utc": "2025-12-25 22:27:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxp0x6",
              "author": "unstopablex5",
              "text": "good point but it feels more egregious for web scraping since the offenders are usually other developers and engineers trivializing what we do",
              "score": 2,
              "created_utc": "2025-12-25 22:39:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvywdan",
              "author": "Exp5000",
              "text": "It's like this in life as well. At every company I've worked with the help desk is full of these types that couldn't be bothered to troubleshoot and immediately escalate the easiest of issues. It's just human nature. There are those who need zero direction and will succeed on their own. Then there's those who just couldn't make it very far without guidance.",
              "score": 1,
              "created_utc": "2025-12-26 03:26:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvxmx16",
          "author": "divided_capture_bro",
          "text": "Can you please find all open ports on AWS for me?¬†",
          "score": 16,
          "created_utc": "2025-12-25 22:25:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxorvn",
              "author": "nameless_pattern",
              "text": "Here you go¬†\n\n\n\n\nlocalhost:3000",
              "score": 6,
              "created_utc": "2025-12-25 22:37:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvxxe7h",
          "author": "MonsieurFizzle",
          "text": "Not gonna lie, this also feels full of irony given the subject matter.",
          "score": 7,
          "created_utc": "2025-12-25 23:33:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyifvg",
          "author": "99ducks",
          "text": "This field seems to attract a lot of novice developers who just aren't good yet at solving problems or asking good questions.",
          "score": 7,
          "created_utc": "2025-12-26 01:51:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1ns2y",
          "author": "hasdata_com",
          "text": "Tbh, helping isn't hard. Most folks replying here are happy to help. The real problem is when someone asks to scrape something super abstract like \"an online store\" and that's it. No site, no details, no constraints. At that point it's like‚Ä¶ help with what, exactly? And how?",
          "score": 3,
          "created_utc": "2025-12-26 16:40:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxozbz",
          "author": "nameless_pattern",
          "text": "The same people who don't think to look at the wiki and don't think to Google it themselves and don't think to learn the skill on their own.¬†\n\n\nAnd there's more people like this now because of chat bots, although strangely they don't always just ask the chat bots first either",
          "score": 2,
          "created_utc": "2025-12-25 22:38:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw07jy7",
          "author": "v_maria",
          "text": "\"idea people\"",
          "score": 2,
          "created_utc": "2025-12-26 10:33:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8jk4l",
          "author": "dot_py",
          "text": "Never heard of RTFM? Noob being mad at noobs... the effort you put in before a question likely has someone thinking the same m8. Humble thy self lol",
          "score": 2,
          "created_utc": "2025-12-27 19:14:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwbdxkx",
              "author": "unstopablex5",
              "text": "congrats, you won the award for taking the most Tylenol ever in 1 day. Your prize is in the mail!\n\nEdit: A kill tony fan. yep it all tracks",
              "score": 1,
              "created_utc": "2025-12-28 05:04:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwq3c4x",
          "author": "amemingfullife",
          "text": "It‚Äôs the same part of the brain that makes your boss think that he can ‚Äòvibe code an app‚Äô and have it work in production.",
          "score": 2,
          "created_utc": "2025-12-30 13:09:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxtahz",
          "author": "Dorkits",
          "text": "It's hard bro, unfortunately.",
          "score": 1,
          "created_utc": "2025-12-25 23:06:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy3lie",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2025-12-26 00:13:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvy51vt",
              "author": "webscraping-ModTeam",
              "text": "üëî Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
              "score": 1,
              "created_utc": "2025-12-26 00:22:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvy7l0k",
          "author": "node77",
          "text": "I agree, you can almost make it a consulting gig. I learned it by my self with scrappy. I have used PowerShell to do some work,, with just regular expressions.",
          "score": 1,
          "created_utc": "2025-12-26 00:37:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzsoug",
          "author": "Virsenas",
          "text": "If you see usernames in Word_Word_4numbers that bother you with this topic, then ignore them, because they are likely 99% a bot. Unless you have a real example of your topic and can give a link to a comment/post, then I think people could help discuss about it.",
          "score": 1,
          "created_utc": "2025-12-26 07:58:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3sq56",
          "author": "VastEnergy4724",
          "text": "I don't know why I see this post recommended. But I built a functioning web scraper for 2 products and 5+ shops, because I can't buy TCG products at msrp prices. Had to cancel Amazon because it's not worth it I got the keep shopping challenge too often. I don't use proxies. I startet with cycles every few seconds but switched now to minutes. Btw all with chatgpt,i know nextjs and js for web developing but didn't use it for a while.",
          "score": 1,
          "created_utc": "2025-12-26 23:39:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiezr8",
          "author": "crowpng",
          "text": "Basic HTML scraping is easy to demo, which sets false expectations.. Reliability, scale, and maintenance are where the real work starts. That gap is what most people miss.",
          "score": 1,
          "created_utc": "2025-12-29 07:54:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvz2dm4",
          "author": "oloshoslut12",
          "text": "Would be free if search providers didnt have strict anti bot methodologies which created a whole market for web scraping",
          "score": 0,
          "created_utc": "2025-12-26 04:09:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzl22m",
          "author": "Haikal019",
          "text": "people like you are the reason stack overflow receive less visit, smart and expert yet underplay new joiner/begineer engineer. what is high quality to you doesnt mean high quality to others. we better respect new joiner as it show scraping is for everyone and be glad this community is growing",
          "score": -2,
          "created_utc": "2025-12-26 06:43:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1rrg5",
              "author": "unstopablex5",
              "text": "Stack overflow receives less visits because you can ask chatgpt, claude or simply google any question you have. If you are going to stack overflow to ask simplistic questions at this point you are not cut out for this field",
              "score": 3,
              "created_utc": "2025-12-26 17:02:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwk7xw",
      "title": "I deployed a side project scraping 5000 dispensaries.",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pwk7xw/i_deployed_a_side_project_scraping_5000/",
      "author": "Round_Method_5140",
      "created_utc": "2025-12-27 00:53:28",
      "score": 37,
      "num_comments": 13,
      "upvote_ratio": 0.92,
      "text": "This is a project where I learned some basics through self teaching and generative assistance from Antigravity. I started by sniffing network on their web pages. Location search, product search, etc. It was all there. Next was understanding the most lightweight and efficient way to get information. Using curl cffi was able to directly call the endpoints repetitively. Next was refinement. How can I capture all stores with the least number of calls? I'll look to incorporate stores and products from iheartjane next.\n\nEdit: I forgot. https://1-zip.com",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1pwk7xw/i_deployed_a_side_project_scraping_5000/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nw49r1n",
          "author": "Grouchy_Brain_1641",
          "text": "The second time I did it I found the one json file for all dispensaries in Weedmap's head. They called me and said hey your web site looks sort of like ours. I said no it looks exactly like yours. Have you ever seen that variable in your head? He was like oh shit. Clonesbayarea is just my daily selenium scrape.",
          "score": 22,
          "created_utc": "2025-12-27 01:22:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4ahxc",
              "author": "Round_Method_5140",
              "text": "Awesome thanks for sharing. I did not look into weedmaps yet. Did they ever fix the information being exposed?",
              "score": 5,
              "created_utc": "2025-12-27 01:27:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw52gvn",
                  "author": "Grouchy_Brain_1641",
                  "text": "oh ya they fixed it soon after.",
                  "score": 1,
                  "created_utc": "2025-12-27 04:32:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw58wvj",
          "author": "AdministrativeHost15",
          "text": "Scraping the resin from 5000 dispensaries you should have accumulated some good stuff.",
          "score": 12,
          "created_utc": "2025-12-27 05:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw77ytx",
          "author": "noodlesallaround",
          "text": "This is cool.",
          "score": 2,
          "created_utc": "2025-12-27 15:12:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9fjlv",
          "author": "JohnnyOmmm",
          "text": "What‚Äôs this for, do they have prices or quantity availability up or something?",
          "score": 2,
          "created_utc": "2025-12-27 22:06:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdufhb",
              "author": "Round_Method_5140",
              "text": "The main use case is for finding products near you, without having to go to multiple different dispensary websites. \n\nAlso you can correlate the same product to different dispensaries and compare prices.\n\nYou can view labs (terpenes etc), how many in stock, specials. Filter out low quality bulk and shake.",
              "score": 2,
              "created_utc": "2025-12-28 16:27:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwegbiw",
                  "author": "JohnnyOmmm",
                  "text": "That‚Äôs awesome broo, I wish they had something similar for the Marshalls Ross Burlington conglomerate cause I‚Äôm tired of driving to each one multiple times a day for work ü§£",
                  "score": 3,
                  "created_utc": "2025-12-28 18:15:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwq7j92",
                  "author": "mortalhal",
                  "text": "Where do you find the lab reports? Many of the listings don‚Äôt include the terpene breakdown",
                  "score": 1,
                  "created_utc": "2025-12-30 13:34:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwbouw8",
          "author": "humble_cyrus",
          "text": "Dude, hella tight.",
          "score": 2,
          "created_utc": "2025-12-28 06:31:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwh92nv",
          "author": "ajawnoutofwater",
          "text": "Excellent name",
          "score": 2,
          "created_utc": "2025-12-29 02:53:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzit0y",
      "title": "Technical SEO baseline that moved DA 0 to 28 in 90 days",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pzit0y/technical_seo_baseline_that_moved_da_0_to_28_in/",
      "author": "Alive_Helicopter_597",
      "created_utc": "2025-12-30 13:59:05",
      "score": 20,
      "num_comments": 12,
      "upvote_ratio": 0.83,
      "text": "Ran controlled technical SEO experiment across 12 new domains to isolate what actually moves domain authority fast versus SEO mythology. All sites launched same week with identical technical setup. Ninety days later average DA reached 28.3 with tightest performing at DA 31 and weakest at DA 24. The test methodology controlled variables to isolate technical SEO impact. All 12 sites used identical hosting (Cloudflare + Webflow), same site structure (homepage, 8 service pages, blog), identical technical optimization (Core Web Vitals, schema markup, XML sitemaps), and same backlink foundation (200 directory submissions via [directory submission service](http://getmorebacklinks.org/) for consistency).\n\nWeek one established technical baseline. Configured Cloudflare for speed optimization achieving 1.2s average load time. Implemented proper schema markup for Organization, Service, and Article types. Created logical URL structure with proper hierarchy. Fixed all technical issues in Search Console. Submitted to directories establishing initial backlink profile.\n\nDays 8-30 focused on content foundation. Published 8 foundational pages targeting primary keywords. Ensured proper internal linking architecture with logical topical clusters. Optimized images for speed without quality loss. All sites achieved 95+ PageSpeed scores mobile and desktop. Average DA reached 12.7 by day 30.\n\nDays 31-60 showed backlink indexing phase. Directory submissions started appearing in Search Console. Published 12 additional blog posts across all sites. Content from days 8-30 started ranking pages 2-3. Technical monitoring showed zero crawl errors. Average DA reached 21.4 by day 60.\n\nDays 61-90 demonstrated compound technical effects. Earlier content moved to page one for longtail terms. Added FAQ schema generating featured snippets for 6 sites. Implemented breadcrumb navigation improving crawlability. Optimized Core Web Vitals achieving perfect scores. Average DA reached 28.3 by day 90.\n\nPerformance variation analysis showed tight clustering. Highest performing site hit DA 31, lowest reached DA 24, with 8 sites in DA 26-29 range. The 7-point spread suggests technical baseline plus directory backlinks produces consistent results when variables are controlled. Individual content quality explained most variation.\n\nTechnical factors that correlated with higher DA were perfect Core Web Vitals scores (all three metrics green), proper schema implementation generating rich results, zero crawl errors in Search Console, logical internal linking structure, and fast initial backlink indexing within first 20 days. What surprisingly didn't impact DA significantly in 90-day window was exact hosting choice beyond basic speed, specific CMS platform (tested Webflow, Wordpress, custom), design quality or UX refinement, or social media presence. These may matter long-term but didn't affect initial DA climb.\n\nThe technical SEO lesson is new sites can reach DA 25-30 in 90 days with proper baseline. This provides foundation for content to rank and guest posting to succeed. Most sites fail because they skip technical foundation and jump to content, then wonder why nothing ranks despite \"good content.\" Cost efficiency for agencies is clear. Each site cost $127 directory service, $60 for 3 months hosting, $90 for tools (Search Console free, Ahrefs trial). Total $277 per site to establish DA 25-30 baseline. That foundation makes all subsequent SEO work more effective.\n\nFor technical SEO practitioners the recommendation is establish perfect technical baseline first (week 1), layer in directory backlinks for initial authority (weeks 2-4), publish content targeting keywords while maintaining technical excellence (weeks 5-12), and monitor Search Console religiously fixing any crawl issues immediately. The broader strategic point is technical SEO isn't glamorous but it's the foundation everything else builds on. Sites with perfect technical setup and mediocre content outperform sites with amazing content and poor technical foundation. Get the boring baseline right first.  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1pzit0y/technical_seo_baseline_that_moved_da_0_to_28_in/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwqs35v",
          "author": "mantepbanget",
          "text": "bullshit and spam",
          "score": 2,
          "created_utc": "2025-12-30 15:27:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqurgy",
          "author": "RandomPantsAppear",
          "text": "I am calling shenanigans on this post. \n\nI find it wildly unlikely that you found any real amount of success with directories. This is not 2004 anymore. \n\nThey‚Äôre not technically worthless links(close), but there‚Äôs not a godamn way in hell they‚Äôre worth anything like $127.\n\nThis seems like a lowkey advertisement looking for DMs",
          "score": 2,
          "created_utc": "2025-12-30 15:40:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrz207",
              "author": "99ducks",
              "text": "OP seems to block these types of posts from being visible on their account, but if you google \"Alive_Helicopter_597 directory submission service\" you can see they spam this pretty regularly.",
              "score": 2,
              "created_utc": "2025-12-30 18:48:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws29mq",
                  "author": "RandomPantsAppear",
                  "text": "I reported op. But smart spammer.",
                  "score": 2,
                  "created_utc": "2025-12-30 19:02:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrzsza",
              "author": "jamesmundy",
              "text": "I used a directory submission service recently and DR when up +20 in one week, genuinely. I got quite a lot of traffic as the submission sites were highly relevant.\nWhether this leads to justifying the expense I don‚Äôt know but it doesn‚Äôt feel worthless",
              "score": 1,
              "created_utc": "2025-12-30 18:51:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws1lby",
                  "author": "RandomPantsAppear",
                  "text": "In some insanely niche directories it‚Äôs possible, but even then paying $120 per directory link is insane. \n\nDirect traffic seems even more unlikely than positive SEO results from directory links. \n\nLike tbh I would consider bot traffic to be more likely than real traffic. \n\nI‚Äôve done SEO for a long time, at one point ran one of the most popular gray/blackhat SEO blogs out there, spoke at conferences from my late teen to early 20s years. I would prefer outright link spam to directory submissions - I do know what I‚Äôm talking about here. \n\nMost directory submission services are outright scams. The ones that do work, the results are likely to be temporary. You‚Äôre normally looking at a domain with a couple aged domains redirected back at it, supported by a few other thin directory sites, that are supported by outright link spam and other filthy links. It‚Äôs link juice laundering, and it‚Äôs fine but most site networks fall eventually. Ones that are part of paid submission packages even more rapidly.\n\nThe way it works is that normally the lowest tier of supporting site will work for a short time promoting the cleaner directory. But one by one, they get penalized and the link juice stops flowing. So for the better directory or thin site, it peaks and then has a slow fall while they build a new network of sites.",
                  "score": 1,
                  "created_utc": "2025-12-30 18:59:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwqcsku",
          "author": "Embarrassed_Poem9556",
          "text": "What schema types did you find generated featured snippets most consistently? FAQ schema seems obvious but wondering if you tested others like HowTo or Product schema",
          "score": 1,
          "created_utc": "2025-12-30 14:05:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqf4vz",
              "author": "Alive_Helicopter_597",
              "text": "What schema types did you find generated featured snippets most consistently? FAQ schema seems obvious but wondering if you tested others like HowTo or Product schema",
              "score": 0,
              "created_utc": "2025-12-30 14:19:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqe155",
          "author": "WearySpecialist7660",
          "text": "\nyou mentioned directory backlinks started appearing in Search Console around days 31-60. Did you see any immediate DA movement or did it take time after indexing for DA to actually update?",
          "score": 1,
          "created_utc": "2025-12-30 14:12:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqgeri",
              "author": "Alive_Helicopter_597",
              "text": "There's definitely a lag. Links appeared in Search Console around day 20-25, but DA didn't jump until day 35-40. So roughly 2 weeks between indexing and DA update.   \nIt happens in waves - links index ‚Üí wait 10-15 days ‚Üí DA bumps up. Patience is key",
              "score": 1,
              "created_utc": "2025-12-30 14:26:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwrr4ab",
          "author": "RobSm",
          "text": "'Most sites fail because they skip technical foundation and jump to content, then wonder why nothing ranks despite \"good content.\"' - how do you know? You did not include a site in the mix which did not have good technical foundation, so there is no comparison here.",
          "score": 1,
          "created_utc": "2025-12-30 18:11:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqc4tf",
          "author": "Ok_Cauliflower5526",
          "text": "\n\nDA 28 in 90 days is solid especially with consistency across all 12 sites. The tight clustering (DA 24-31) really proves your point about technical baseline mattering more than people think.",
          "score": 0,
          "created_utc": "2025-12-30 14:01:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1puf7hb",
      "title": "AutoHealing Crawlers/Scrappers",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1puf7hb/autohealing_crawlersscrappers/",
      "author": "Urten",
      "created_utc": "2025-12-24 05:11:11",
      "score": 11,
      "num_comments": 17,
      "upvote_ratio": 1.0,
      "text": "Hello, Just as the title - has anyone ever build any autohealing scrapper, there are few github libraries but they don't seem to be working or inaccurate, if the api changes the scraper breaks. So I want to ask if anyone had any luck building a fully functional autohealing scraper. ",
      "is_original_content": false,
      "link_flair_text": "Scaling up üöÄ",
      "permalink": "https://reddit.com/r/webscraping/comments/1puf7hb/autohealing_crawlersscrappers/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nvpz913",
          "author": "RandomPantsAppear",
          "text": "I‚Äôve had luck doing it, but the code was kind of a learning experience so it‚Äôs not clean. \n\nThe hardest part is getting it to play well with multiple page types, distributed systems, and making sure that your updated selectors work on all matching pages and not just the one that it was broken on. \n\nThe solutions are URL routers, locking systems for config updates, cloud hosted config, and temporary caching of results so you can quickly pull multiple recent example of that url option.",
          "score": 3,
          "created_utc": "2025-12-24 14:27:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvret73",
              "author": "Urten",
              "text": "\\`\\`\\`The hardest part is getting it to play well with multiple page types, distributed systems, and making sure that your updated selectors work on all matching pages and not just the one that it was broken on.\\`\\`\\`\n\nwait so you will update the selectors manually? I don't get it",
              "score": 1,
              "created_utc": "2025-12-24 19:08:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvrezcm",
                  "author": "RandomPantsAppear",
                  "text": "No, the way it heals is by detecting failures and rewriting its own rules and selectors, saving them to a config and sharing it with the other scrapers",
                  "score": 1,
                  "created_utc": "2025-12-24 19:09:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvr5c87",
          "author": "viciousDellicious",
          "text": "yes, if the crawlers cant extract the data anymore (signal triggered by crawl volume trends), then my service go to previous crawls check the data gotten there and see if its still in the markup, if it is not then its probably blocked for which it does some fuzzing with several waf bypassing techniques until it sees one of the old datapoints or run out of techniques.\n\n\nif the change is just the security then this self healing will update the profile(headers, fingerprints, etc) for that domain and go on.\nif the issue is not on the security side but that the markup changed i will do reverse lookups on the data, to find the html node containing them, once i am at the lowest level node i traverse parents getting classes or id's until i get a unique selector for my data, i retest this with the other urls/expected datapoints, if they all work, then the service updates the list of selectors.\nif this fails, it uses ai to find a selector for the value i have.",
          "score": 3,
          "created_utc": "2025-12-24 18:16:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrdqfo",
              "author": "Urten",
              "text": "Hoo lee shit, that's genius. This seems like a viable solution. If you don't mind, can you share what kind of tools you are using for this workflow, i kinda have slight idea, But i wanna have a clear picture on this method.",
              "score": 2,
              "created_utc": "2025-12-24 19:02:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvre5i9",
                  "author": "viciousDellicious",
                  "text": "its all in golang and a wrapper around ai tools (claude/chatgpt/ollama) based on volume, budget and urgency we change the model",
                  "score": 1,
                  "created_utc": "2025-12-24 19:04:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pz1zvp",
      "title": "Scraping reddit?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pz1zvp/scraping_reddit/",
      "author": "AdhesivenessEven7287",
      "created_utc": "2025-12-29 23:39:13",
      "score": 6,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Over time I save up pages of articles and comments I think will be interesting. But I've not gotten around to it yet. \n\nHow can I have the links but easily download the page? Baring in mind to view all comments I need to scroll down the page. ",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1pz1zvp/scraping_reddit/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwmyser",
          "author": "g4m3r1",
          "text": "If you just want to scrape the actual content of the reddit post then its quite easy. Just add .json at the end of the URL and reddit will return the content of the post + all comments as beautifully formatted json.\n\nE.g. Try it with your own post here: [https://www.reddit.com/r/webscraping/comments/1pz1zvp/scraping\\_reddit.json](https://www.reddit.com/r/webscraping/comments/1pz1zvp/scraping_reddit.json)",
          "score": 6,
          "created_utc": "2025-12-29 23:50:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwn0m6n",
              "author": "PresidentHoaks",
              "text": "Lol mobile sends me back to this page, will have to check on my lappy",
              "score": 1,
              "created_utc": "2025-12-30 00:00:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwn3ws2",
                  "author": "g4m3r1",
                  "text": "Never tried it on mobile but on desktop this works fine :).",
                  "score": 1,
                  "created_utc": "2025-12-30 00:18:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pz8czz",
      "title": "Is it just me or playwright incredibly unstable",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pz8czz/is_it_just_me_or_playwright_incredibly_unstable/",
      "author": "kilobrew",
      "created_utc": "2025-12-30 04:24:02",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 0.8,
      "text": "I‚Äôve been using playwright in the AWS environment and having nothing but trouble getting it to run without randomly disconnecting, ‚Äúfailed to get world‚Äù, or timeouts that really shouldn‚Äôt have happened. Hell, Even running AWS‚Äôs SAAS bedrock agent_core browser tool has the same issue. \n\nIt seems the only time I can actually use it is if it‚Äôs installed on a full blown windows install with a GPU. \n\n\nIs it just me?\n",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1pz8czz/is_it_just_me_or_playwright_incredibly_unstable/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwoupx0",
          "author": "RandomPantsAppear",
          "text": "I ran multiple playwright instances on Fargate instances with 0.25 vcpu and 256m of ram, that were also running redis and celery. \n\nSomething is very wrong if this is the behavior you are getting. \n\n\nHow are you detecting the page load completion success/fail?\n\nHave you checked the process list to make sure processes are successfully exiting?\n\nAre you taking screenshots on the theoretical page load fails? (I am not sure how this works headless, I often run it with xvfb)",
          "score": 1,
          "created_utc": "2025-12-30 06:42:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpm1qz",
          "author": "bluemangodub",
          "text": "Not my experience at all. I have had untold issues with AWS, especially the low price instances. \n\nRun it locally, or on a proper VPS and see if you have the same issues. If not, it's AWS",
          "score": 1,
          "created_utc": "2025-12-30 10:52:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1px553f",
      "title": "Legal implications of this sort of scraping",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1px553f/legal_implications_of_this_sort_of_scraping/",
      "author": "artnote1337",
      "created_utc": "2025-12-27 18:41:10",
      "score": 5,
      "num_comments": 14,
      "upvote_ratio": 0.86,
      "text": "So, I'm scraping data from a website that has a paywall on some of its data, BUT the endpoint that returns this data was easily found in the source code and does not require any special cookies besides the ones from a free account. Its data from census from a country that were digitalized, the census itself is public but the way this data is being provided may not be I guess. I'm using proxy, a few accounts and browsers to scrape the data using this found endpoint (respecting 429s). Will/Can I be in trouble? What are your opinion on the moral/ethics in this sort of scraping?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1px553f/legal_implications_of_this_sort_of_scraping/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nw93rmc",
          "author": "Fragrant_Ad3054",
          "text": "If your requests don't affect the site's functionality, and your project isn't for commercial purposes, then you can rest easy.\n\nIf your project is for commercial purposes, then the data must be \"transformed\" in the sense that you can't simply copy and paste its content. For example, using scraped data to create a graph is fine, but scraping and offering the same data to clients without prior transformation exposes you to a \"risk.\" In this case, a lawyer specializing in digital law can draft valuable documents such as terms of service, disclaimers, etc.",
          "score": 9,
          "created_utc": "2025-12-27 21:03:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw94qad",
              "author": "artnote1337",
              "text": "I do normalize the data to a tabular form thats not exactly like the site presents it. I‚Äôm not sure what the client will be doing with this data tho.",
              "score": 3,
              "created_utc": "2025-12-27 21:09:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw94y8w",
                  "author": "Fragrant_Ad3054",
                  "text": "May I ask if you are in the EU zone or outside the EU zone?",
                  "score": 2,
                  "created_utc": "2025-12-27 21:10:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwa7gsh",
          "author": "Informal_Stock_6166",
          "text": "You are fine mate.... most companies don't care or waste the time to go after scrapers now days, they just tidy up their spam/anti bot system and that's it",
          "score": 2,
          "created_utc": "2025-12-28 00:42:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8k8qw",
          "author": "RandomPantsAppear",
          "text": "Anytime you‚Äôre making an account you‚Äôre agreeing to their terms of service. There have been successful court cases specific to using logged in accounts. \n\nThat said, the practice is common and repercussions are extremely rare. Like lottery jackpot amounts of rare. \n\nAs far as ethics go, I can only give mine personally. \n\n* Scrape respectfully, do not slam their servers and degrade service for their real users. \n\n* Use the data in a transformative way. Don‚Äôt just copy the data and remake the site you‚Äôre scraping from. \n\n* Scrape in the least invasive way possible. ie if they have data with 100 results cached and fast, but you figure out you can query 2000, just iterate pages at the 100 and keep hitting that cache. \n\nThis slips by a lot of people, but scrapers that don‚Äôt cause problems for the place you are scraping are infinitely easier to maintain. Doing it right also means that you don‚Äôt end up as a support ticket in their engineering department, where you will have personal attention.\n\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n\nFor your specific purpose, census data I would have zero issues scraping it. It was paid for by the people, the people having access to it is morally right in my view.",
          "score": 4,
          "created_utc": "2025-12-27 19:18:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8lb1r",
              "author": "artnote1337",
              "text": "Yeah, I'm scraping it in a way that if a few 429s appear when requesting I backoff so the actual website and its frontend don't suffer. But a client requested this data (maybe for research purposes?) and I'll get paid. This data is really old, its from the 19th century actually so it's likely research related.",
              "score": 2,
              "created_utc": "2025-12-27 19:23:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw8lz82",
                  "author": "RandomPantsAppear",
                  "text": "If I were you, I would do a little poking and prodding to figure out what exactly the rate limit is, or what a safe request speed is and stick to it. \n\nAnytime a response code is not 200, it‚Äôs getting logged somewhere. It‚Äôs also likely showing up on charts and graphs somewhere.\n\nAlso, it‚Äôs a lot easier to write and maintain the code if you don‚Äôt have to expect 429s and regular retries.",
                  "score": 5,
                  "created_utc": "2025-12-27 19:27:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw931k8",
          "author": "TurbulentSoup5082",
          "text": "How would a website know your scraped data came from their site specifically? Will they include dummy data to catch you out?",
          "score": 1,
          "created_utc": "2025-12-27 20:59:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pv756v",
      "title": "Any serious consequences?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pv756v/any_serious_consequences/",
      "author": "reddit_user4u",
      "created_utc": "2025-12-25 06:30:11",
      "score": 5,
      "num_comments": 19,
      "upvote_ratio": 0.86,
      "text": "Thinking about webscraping fragrantica for all their male perfumes for a machine learning perfume recommender project.\n\nNow I want to document everything on github as I'm doing this in attempt to get a coop (also bc its super cool). However, their ToS say web scraping is prohibited but Ive seen people in the past scrape their data and post on github. Theres also a old scraped fragrantica dataset on kaggle.\n\nI just dont want to get into any legal trouble or anything so does anyone have any advice? Anything appreciated!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1pv756v/any_serious_consequences/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nvuhswt",
          "author": "divided_capture_bro",
          "text": "One consequence will be that everyone thinks you're really into male perfume.\n\n\nToS = these are only suggestions.\n\n\nIf you want to have fun, set it up as a public repo and use GitHub Actions to do the scraping.",
          "score": 8,
          "created_utc": "2025-12-25 08:58:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwpaj8",
              "author": "Ecstatic_Vacation37",
              "text": "Don‚Äôt a lot of websites block the ip that comes from Gh actions ?",
              "score": 2,
              "created_utc": "2025-12-25 19:00:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwpxct",
                  "author": "divided_capture_bro",
                  "text": "Some but not all. Only one way to find out, and if need be use a proxy.\n\n\nI just checked and the site is accessible via Tor, so you could use that.",
                  "score": 1,
                  "created_utc": "2025-12-25 19:04:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvxfrxh",
              "author": "reddit_user4u",
              "text": "I was thinking about using rotating proxies with requests to scrape the data with 20 concurrent workers in python. Will this be too much for their servers or should be fine? \n\n  \nAlso just to confirm, no major consequences lol?",
              "score": 1,
              "created_utc": "2025-12-25 21:42:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvxkdw2",
                  "author": "divided_capture_bro",
                  "text": "I can't really say what their servers can handle, but 20 concurrent requests sounds light (it's not like you're sending all requests at once; that would likely cause problems!)\n\n\nAnd yes, there are likely no major consequences unless you're literally attacking them. There is a large body of recent case law affirming the legality of scraping - even doing so flagrantly like with BrightData.",
                  "score": 1,
                  "created_utc": "2025-12-25 22:10:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvvqyp2",
          "author": "leros",
          "text": "If they tell you to stop and you keep scraping, you could get into trouble. If you're hammering them with massive traffic and evading blocks, you could get into trouble. If you're publicly profiting off the data or harming them, you could get into trouble.¬†\n\n\nOtherwise you're probably fine.¬†",
          "score": 2,
          "created_utc": "2025-12-25 15:37:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvwaxe",
          "author": "Tasty_While_8076",
          "text": "Someone from fragrantica might break through your door like the kool-aid man and steal your computer. Be careful, it happened to a friend of a friend of mine.\n\nYou'll be fine.",
          "score": 1,
          "created_utc": "2025-12-25 16:09:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwd78m",
          "author": "RandomPantsAppear",
          "text": "If you were going to have an issue (which is unlikely) they would just send a DMCA takedown to GitHub, github would take it down and that‚Äôs the end of it.",
          "score": 1,
          "created_utc": "2025-12-25 17:51:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvyy1ky",
              "author": "divided_capture_bro",
              "text": "It's not copyrighted information, so DMCA doesn't apply.",
              "score": 1,
              "created_utc": "2025-12-26 03:38:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw39wnm",
                  "author": "RandomPantsAppear",
                  "text": "I personally agree with you, but this is how I‚Äôve seen similar things be taken down from GitHub.",
                  "score": 1,
                  "created_utc": "2025-12-26 21:51:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvy68il",
          "author": "zoransa",
          "text": ">Owner here: don‚Äôt scrape Fragrantica. It violates our ToS, it‚Äôs unauthorized use of our IP, and it disrupts our operations. We don‚Äôt provide an API and we don‚Äôt license our content for datasets/ML. If you publish or commercialize scraped Fragrantica data, expect a lawsuit.",
          "score": 1,
          "created_utc": "2025-12-26 00:29:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvyelk0",
              "author": "reddit_user4u",
              "text": "Okay understood. just curious, why is it that other githubs which use fragrantica webscraped data are still up, and also the kaggle dataset which is derived from fragrantica as well?",
              "score": 1,
              "created_utc": "2025-12-26 01:24:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvyp0p8",
                  "author": "zoransa",
                  "text": "Stolen data is still illegal, and when we discover it, we send DMCA takedown notices.\n\nWe had a case where an aggressive crawler hammered our service for almost two weeks; we were literally going offline for minutes at a time. A few months later, a researcher from Imperial College tried to ‚Äúlegalize‚Äù the theft by asking permission to use the data for his PhD, after he had already completed the project and written the thesis. When we found out, we objected and documented the disruption and downtime it caused. The PhD was ultimately rejected.\n\nWe will not allow scraping of our website, not even for educational purposes. If anyone attempts to use scraped Fragrantica data commercially, we will take legal action.",
                  "score": 1,
                  "created_utc": "2025-12-26 02:36:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw032kf",
          "author": "Ladline69",
          "text": "Just do it fuck it!",
          "score": 1,
          "created_utc": "2025-12-26 09:47:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvu5huj",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2025-12-25 06:49:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvuhydd",
              "author": "divided_capture_bro",
              "text": "It's not illegal to profit from publicly available information. All of the recent cases point to this same conclusion, that the law as it stands allows for scraping.",
              "score": 8,
              "created_utc": "2025-12-25 09:00:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvuldh1",
                  "author": "[deleted]",
                  "text": "Right. Google is basically a giant web scraper and making a ton of money from it. If tjey block scrapping then Google will be the first one to get hit.",
                  "score": 3,
                  "created_utc": "2025-12-25 09:38:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvw3hcl",
                  "author": "leros",
                  "text": "That doesn't mean a big company won't take legal action against you that costs you a bunch of money. Companies get sued for scraping, stop, and settle for a payment.",
                  "score": 2,
                  "created_utc": "2025-12-25 16:52:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pvg8zy",
      "title": "Tool for tracking product photos + prices from multiple shops?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pvg8zy/tool_for_tracking_product_photos_prices_from/",
      "author": "blera",
      "created_utc": "2025-12-25 15:58:12",
      "score": 3,
      "num_comments": 10,
      "upvote_ratio": 0.81,
      "text": "I‚Äôm looking for a ToS friendly way to monitor product listings on multiple lingerie retailers. I follow around 10‚Äì15 shops (Hunkem√∂ller, Women‚Äôsecret, Intimissimi, VS, etc.) and manually checking category pages is taking too much time.\n\nWhat I want is basically ‚Äúwatch these category URLs‚Äù and collect product name, product link, main photo, and current price. Then keep it organized by shop and category (bras, bodysuits, sets), and ideally notify me when prices drop.\n\nDoes something like this already exist (library, service, framework, or a common approach people use)? I‚Äôm not trying to bypass protections or do heavy scraping, just personal tracking, ideally polite and low frequency. If you‚Äôve built something similar, what worked well for e-commerce sites?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1pvg8zy/tool_for_tracking_product_photos_prices_from/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nw3ni8f",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2025-12-26 23:08:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9mgty",
              "author": "webscraping-ModTeam",
              "text": "ü™ß Please review the sub rules üëâ",
              "score": 1,
              "created_utc": "2025-12-27 22:44:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwmut1",
          "author": "abdullah-shaheer",
          "text": "You can make a personalized system for yourself",
          "score": 1,
          "created_utc": "2025-12-25 18:46:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwo2k0",
              "author": "blera",
              "text": "I can. I have experience building software, just not web scraping specifically. I‚Äôm scouting to see if something already exists or if there‚Äôs a good base to build on, rather than reinventing the wheel.\n\nDo you have any advice? I‚Äôm specifically trying to download all product images from a page, including images for the other color variants too. For example (SFW): https://www.intimissimi.com/it/product/maglia_in_seta_a_taglio_uomo-CLD92A.html?dwvar_CLD92A_Z_COL_INT=651I\n\nHow would you approach getting all the image URLs (and the other colors‚Äô images) in a robust way? Machine to machine obviously",
              "score": 1,
              "created_utc": "2025-12-25 18:53:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvwt9ue",
                  "author": "abdullah-shaheer",
                  "text": "Yes, there are a lot of ways to do so. Sometimes, all the information gets loaded initially, you can search for application/json+ld classes in the html to see the json information. If you can't find a information regarding variant image urls in the main page, then the most easiest way without making more requests will be making image urls with reference to the base product image urls, there will be a small difference in base product and it's variant image urls like variant code or something like that. Find the information in the base product's html, however it will be hard for beginners to do so (I think). You can also track network requests and make a data pipeline to retrieve all the information. There will be a lot other ways, depending on the website. I am not entering the website as most of it's content is NSFW based. Let me know, if there is something else, you wanna ask. I will be happy to help (only for SFW).",
                  "score": 1,
                  "created_utc": "2025-12-25 19:24:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvx7n7t",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2025-12-25 20:51:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxcu9o",
              "author": "webscraping-ModTeam",
              "text": "üëî Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
              "score": 1,
              "created_utc": "2025-12-25 21:24:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvxikmg",
          "author": "unstopablex5",
          "text": "Theres no product like this to my knowledge, you either hire someone with scraping experience to build you the pipeline or you do it yourself",
          "score": 1,
          "created_utc": "2025-12-25 21:59:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzzc2f",
          "author": "RobSm",
          "text": "If you hire someone to do 'one time job' you will have to maintain it when sites change structure. So better learn yourself, especially if this is 'nothing big'.",
          "score": 1,
          "created_utc": "2025-12-26 09:07:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzhjmn",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pzhjmn/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2025-12-30 13:01:09",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.81,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levels‚Äîwhether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) üå±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1pzhjmn/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwsa57x",
          "author": "yousephx",
          "text": "I'm available for hire!   \n  \nI recently developed the most accurate solution on the internet to scrape Google Maps 360 images: [gsvp-dl](https://github.com/yousephzidan/gsvp-dl). It handle edge cases and problems that previous solutions don't. It has a documentation on how Google assemble and build it's 360 images, which you won't find anywhere online, even by Google themselves!  \n\nPortfolio: [yousephzidan.dev](http://yousephzidan.dev)",
          "score": 2,
          "created_utc": "2025-12-30 19:40:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pv93nw",
      "title": "Scraping Job on Indeed",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pv93nw/scraping_job_on_indeed/",
      "author": "Haikal019",
      "created_utc": "2025-12-25 08:44:14",
      "score": 1,
      "num_comments": 5,
      "upvote_ratio": 0.57,
      "text": "thinking about web scraping indeed using playwright to collect job data like job title, job description, and salary for a data engineering / analytics project.\n\nare there any good github repos using playwright or similar tools that i can refer to for scraping job listings.\n\nIssue on my side is to get the job description, needing to click on the left panel everytime is not a problem but somehow on playwright, it only show the first job description despite after highlight/select other job card. not sure what went wrong.\n\nany advice would be appreciated",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1pv93nw/scraping_job_on_indeed/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nvujwd4",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2025-12-25 09:22:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvus6me",
              "author": "webscraping-ModTeam",
              "text": "üëî Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
              "score": 0,
              "created_utc": "2025-12-25 10:53:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwe82p",
          "author": "i-am-hustling",
          "text": "You can try opening the job URL in a new tab",
          "score": 1,
          "created_utc": "2025-12-25 17:57:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7679b",
              "author": "Haikal019",
              "text": "i did but the problem is it detect bot. how do i bypass that then?",
              "score": 1,
              "created_utc": "2025-12-27 15:02:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw9f5mx",
          "author": "about7cars",
          "text": "Ive been using selenium, having troubles with bot detection too, worked good for a day. Pretty sure fingerprinting got me. Haven‚Äôt tried spoofing users and headers yet. The robots.txt shows /jobs are disallowed except for a few like google and twitter bots. \n\nInspect element, youll see the job listings, including the card stacks are pretty deeply nested, i found being a level higher and doing a css search worked best. Im not sure how playwright works, but I hope the info is helpful for figuring it out.",
          "score": 1,
          "created_utc": "2025-12-27 22:04:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}