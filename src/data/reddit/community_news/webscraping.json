{
  "metadata": {
    "last_updated": "2026-02-27 03:04:00",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 17,
    "total_comments": 97,
    "file_size_bytes": 108603
  },
  "items": [
    {
      "id": "1rc9992",
      "title": "Built a stealth Chromium, what site should I try next?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rc9992/built_a_stealth_chromium_what_site_should_i_try/",
      "author": "duracula",
      "created_utc": "2026-02-23 05:46:48",
      "score": 111,
      "num_comments": 85,
      "upvote_ratio": 0.99,
      "text": "Last couple months I was automating browser tasks on sites behind Cloudflare and reCAPTCHA. Tried various tools and solutions out there, everything either broke on the next Chrome update, got detected, or died. I was duct-taping 4 tools together and something broke every other week.\n\nSo I patched Chromium itself at the C++ source level.\n\nCloakBrowser is a small Python wrapper around a custom Chromium binary with 16 fingerprint patches compiled into the source. Not JavaScript injection, not config flags, canvas, WebGL, audio, fonts, GPU strings, all modified before compilation.\n\nResults:\n\n\\- reCAPTCHA v3: 0.9 (server-verified)  \n\\- Cloudflare Turnstile: pass (managed + non-interactive)  \n\\- BrowserScan, FingerprintJS, deviceandbrowserinfo: all clean  \n\\- 14/14 detection tests passed (full results on GitHub)\n\n    pip install cloakbrowser \n    \n    from cloakbrowser import launch \n    browser = launch() \n    page = browser.new_page()\n\nSame Playwright API, binary auto-downloads on first run (\\~200MB, cached).\n\nHow it's different from Patchright/rebrowser: those patch the protocol layer. We patch the browser itself, fingerprint values baked in at compile time. TLS matches because it IS Chrome.\n\nWhat it does NOT do: no proxy rotation, no CAPTCHA solving, no fingerprint randomization per session (yet). It's a browser, not a scraping stack. Bring your own proxies.  \nWe don't bypass reCAPTCHA. reCAPTCHA just thinks we're a normal browser â€” because we are one.\n\nLinux x64 right now (even inside docker)  \nmacOS coming soon (let me know if you need it, helps me prioritize).\n\n[https://github.com/CloakHQ/CloakBrowser](https://github.com/CloakHQ/CloakBrowser)  \n[https://cloakbrowser.dev/](https://cloakbrowser.dev/)  \nPyPI: [https://pypi.org/project/cloakbrowser/](https://pypi.org/project/cloakbrowser/) (`pip install cloakbrowser)`  \nnpm: [https://www.npmjs.com/package/cloakbrowser](https://www.npmjs.com/package/cloakbrowser) (`npm install cloakbrowser)`\n\nIf you have a site that blocks everything, throw it at CloakBrowser and let me know. I like the challenge. Hardest cases welcome.  \nPro tip: pair it with a residential proxy, the browser handles fingerprints, but your IP still matters.\n\nEarly days â€” feedback, bugs, requests are welcome.\n\n**Update 1:**\n\nJust shipped it! npm install cloakbrowser â€” supports both Playwright and Puppeteer\n\nSame stealth binary, same 14/14 detection results. TypeScript with full types.\n\n     // Playwright (default)\n      import { launch } from 'cloakbrowser';\n      const browser = await launch();\n      const page = await browser.newPage();\n      await page.goto('https://example.com');\n    \n      // Or with Puppeteer\n      import { launch } from 'cloakbrowser/puppeteer';\n\n**Update 2**:\n\nOur GitHub organization was temporarily flagged by an automated system.  \nWe were reorganizing repositories today, and the bulk activity on a new org, combined with a large binary on Releases and a traffic spike from this post â€” triggered GitHub's automated moderation.  \nWe've filed an appeal and expect it to be restored soon (at least hoping so).\n\nIn the meantime:  \n\\- `pip install cloakbrowser` and `npm install cloakbrowser` still work â€” binary downloads from our mirror  \n\\- GitLab Mirror - [https://gitlab.com/CloakHQ/cloakbrowser](https://gitlab.com/CloakHQ/cloakbrowser)  \n\\- And simple site - [https://cloakbrowser.dev/](https://cloakbrowser.dev/)  \n\\- GitHub repo is temporarily 404, should be back soon.  \n\\- [Posted about the situation in r/github](https://www.reddit.com/r/github/comments/1reu95z/github_flagged_our_opensource_new_born_org_with/)\n\nNothing changed with the project itself. Sorry for the inconvenience.\n\n**Update 3:**  \nGitHub org is restored â€” back to normal.   \nThanks everyone who reached out and helped during the downtime.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rc9992/built_a_stealth_chromium_what_site_should_i_try/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6wtohv",
          "author": "Nice-Vermicelli6865",
          "text": "Try completing a survey on a survey platform and see if you are able to complete a survey, they have the toughest anti bot challenges",
          "score": 15,
          "created_utc": "2026-02-23 06:38:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wz8ra",
              "author": "duracula",
              "text": "SurveyMonkey worked, completed test survey (or its less protected?).  \nTypeform worked, the hard part was to find the damn next button.  \nSame for Qualtrics  \nYou have any other survey site in mind?  \n  \n  \n",
              "score": 2,
              "created_utc": "2026-02-23 07:28:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6wziim",
                  "author": "Nice-Vermicelli6865",
                  "text": "I meant on actual websites, like Prime Opinion that are actually guarded",
                  "score": 8,
                  "created_utc": "2026-02-23 07:31:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ws841",
          "author": "RandomPantsAppear",
          "text": "My dude, excellent. \n\nI was literally just pondering how to avoid having to do another chrome extension + python command server, and this fits the bill. \n\nFor every hour I do not have to do that, or code in C++ you are my hero an extra time.",
          "score": 9,
          "created_utc": "2026-02-23 06:25:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x120h",
          "author": "theozero",
          "text": "This seems promising. Any plans to make it possible to use via JavaScript / puppeteer?",
          "score": 5,
          "created_utc": "2026-02-23 07:45:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x2o74",
              "author": "duracula",
              "text": "Thanks, adding js/puppeteer support to the roadmap!  \nThe stealth is in the Chromium binary itself, not the Python wrapper, so it's mainly packaging work.\n\nIn the meantime, you can already use it with Puppeteer today â€” just point it at the binary:  \nHaven't tried it, but it should work, same parameters.\n\n    const puppeteer = require('puppeteer-core');\n    \n      const browser = await puppeteer.launch({\n        executablePath: '~/.cloakbrowser/chromium-142.0.7444.175/chrome',\n        args: [\n          '--no-sandbox',\n          '--disable-blink-features=AutomationControlled',\n          '--fingerprint=12345',\n          '--fingerprint-platform=windows',\n          '--fingerprint-hardware-concurrency=8',\n          '--fingerprint-gpu-vendor=NVIDIA Corporation',\n          '--fingerprint-gpu-renderer=NVIDIA GeForce RTX 3070',\n        ],\n        ignoreDefaultArgs: ['--enable-automation'],\n      });\n\nInstall the binary with:\n\n    pip install cloakbrowser && python -c \"from cloakbrowser.download import ensure_binary; ensure_binary()\"\n\nThen use the path above.\n\nAll the stealth passes through â€” same 14/14 detection results.  \nProper npm package coming soon.",
              "score": 3,
              "created_utc": "2026-02-23 08:01:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zqvbo",
                  "author": "theozero",
                  "text": "Nice - I'll give it a go!",
                  "score": 2,
                  "created_utc": "2026-02-23 18:15:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o700sz8",
              "author": "duracula",
              "text": "Just shipped it! `npm install cloakbrowser` â€” supports both Playwright and Puppeteer  \nSame stealth binary, same 14/14 detection results. TypeScript with full types.  \n\n     // Playwright (default)\n      import { launch } from 'cloakbrowser';\n      const browser = await launch();\n      const page = await browser.newPage();\n      await page.goto('https://example.com');\n    \n      // Or with Puppeteer\n      import { launch } from 'cloakbrowser/puppeteer';\n    \n\n Give it a try, let me know if there bugs or problems, have fun.",
              "score": 2,
              "created_utc": "2026-02-23 19:00:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o704xdi",
                  "author": "theozero",
                  "text": "Awesome. Iâ€™ve got a docker based setup anyway so wonâ€™t be using npm but Iâ€™m sure others will find it super useful",
                  "score": 2,
                  "created_utc": "2026-02-23 19:19:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xfuvb",
          "author": "juhacz",
          "text": "After a quick test, I see that it causes captcha on the [allegro.pl](http://allegro.pl) website in headless mode.",
          "score": 6,
          "created_utc": "2026-02-23 10:10:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zdssy",
              "author": "duracula",
              "text": "Tested it â€” Allegro uses DataDome, which is one of the more aggressive bot detection services. Took some digging, thanks for the challenge.\n\nFor sites with this level of protection, two things help: a residential proxy (datacenter IPs get flagged by IP reputation) and headed mode via Xvfb (some services detect headless-specific signals).   \nUpdated the README with instructions.  \nAfter implementing this 2 steps, I could enter the site without problems and navigate inside.  \n",
              "score": 5,
              "created_utc": "2026-02-23 17:14:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y7giw",
          "author": "Objectdotuser",
          "text": "amazing work, but how could we possibly vet this patched chromium binary?",
          "score": 6,
          "created_utc": "2026-02-23 13:43:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z1iqe",
              "author": "duracula",
              "text": "Fair concern â€” \"trust me bro\" doesn't cut it for a binary you're running.\n\n1. Check the hash â€” every release has a SHA256 digest on GitHub, verify your download matches  \n2. Run it sandboxed â€” Docker, strace, or a VM. Monitor network traffic, syscalls, file access. It's just Chromium â€” it doesn't phone home or do anything a stock Chrome wouldn't  \n3. Scan it â€” upload to VirusTotal, it passes clean  \n4. Read the wrapper â€” fully open source MIT, you can see exactly what flags get passed and how the binary is launched\n\nAt the end of the day, you're in the same position as with any Chromium distribution (Brave, Vivaldi, Arc) â€” you either trust the publisher, audit the behavior, or build your own.",
              "score": 5,
              "created_utc": "2026-02-23 16:17:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zcdlw",
          "author": "Pristine_Wind_2304",
          "text": "its giving ai generated text from your replies and your original post but if your tests are right then this seems like an awesome project!! i hope it gets developed further and not abandoned like the other five million chrome binary patches that just cant keep up with the like 100 leaks from every web api",
          "score": 6,
          "created_utc": "2026-02-23 17:07:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zh1o4",
              "author": "duracula",
              "text": "Thanks,   \nYeah I use AI heavily and I'm not gonna pretend otherwise. Without it I couldn't have patched Chromium to this level in a few weeks, it's a massive codebase and a lot of work. AI saved me months.  \n  \nSame with this thread, lots of replies that each deserve a proper testing and answer. I throw in my points and AI helps me write them up in proper English. It's a tool, like everything else.\n\nOn the abandonment concern, totally fair, I've seen the graveyard too. The difference here is this powers production automation I depend on every day. If it breaks, my stuff breaks.   \nI'll keep it going as long as I can â€” but I won't lie, these things take a lot of time and dedication, and life happens.   \n  \nThe code and test results are real though â€” `pip install cloakbrowser` and `python examples/stealth_test.py` hits 6 live detection sites with pass/fail verdicts.  \nThat's what matters.",
              "score": 1,
              "created_utc": "2026-02-23 17:29:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y7p2b",
          "author": "usamaejazch",
          "text": "how is the stealth browser binary compiled? no source of patches? \n\nit could even have malware, no?",
          "score": 5,
          "created_utc": "2026-02-23 13:45:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z2feu",
              "author": "duracula",
              "text": "You're right that you can't fully verify a closed binary â€” same as Brave, Arc, or any Chromium fork that ships pre-built.\n\nIt's compiled from the official Chromium 142 source tree with our patches applied, using the standard Chromium build toolchain (gn + ninja).   \nSame process any Chromium fork uses. The patches modify fingerprint APIs (canvas, WebGL, audio, fonts, GPU strings).   \nThat's it.  No network changes, no data collection, no telemetry.\n\nWhat you can verify:  \n\\- Run it with strace or Wireshark â€” it behaves identically to stock Chromium except fingerprint values differ  \n\\- Upload to VirusTotal, passes clean  \n\\- The wrapper is fully open source, you can read every line\n\nThe patches aren't open source because they're the core IP of the project. But the binary behavior is fully auditable â€” that's where trust should come from.\n\nIf that's not enough for your threat model, that's completely fair. Not every tool is for everyone.",
              "score": 2,
              "created_utc": "2026-02-23 16:21:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xcvu4",
          "author": "EnvironmentSome9274",
          "text": "Try Walmart, their anti bot is very aggressive",
          "score": 3,
          "created_utc": "2026-02-23 09:42:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zp7oy",
              "author": "duracula",
              "text": "Worked.",
              "score": 1,
              "created_utc": "2026-02-23 18:07:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zpg9t",
                  "author": "EnvironmentSome9274",
                  "text": "Can you be a bit more elaborate lol, please? \nHow many products did you try scraping? Did the bot flag you and you rotated or was it completely undetected? \nThank you",
                  "score": 1,
                  "created_utc": "2026-02-23 18:08:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xihbe",
          "author": "Zestyclose_Ad9943",
          "text": "Is it possible to use it on a Node project ?  \nI have a scraping script built on Node with Playwright, I wish I could use your browser instead.",
          "score": 4,
          "created_utc": "2026-02-23 10:35:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70119r",
              "author": "duracula",
              "text": "Just shipped it! `npm install cloakbrowser` â€” supports both Playwright and Puppeteer  \nSame stealth binary, same 14/14 detection results. TypeScript with full types.  \n\n     // Playwright (default)\n      import { launch } from 'cloakbrowser';\n      const browser = await launch();\n      const page = await browser.newPage();\n      await page.goto('https://example.com');\n    \n      // Or with Puppeteer\n      import { launch } from 'cloakbrowser/puppeteer';\n    \n\n Give it a try, let me know if there bugs or problems, have fun.",
              "score": 3,
              "created_utc": "2026-02-23 19:01:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y6er5",
          "author": "Objectdotuser",
          "text": "hmm well companies are wising up to the evasion browsers. how does it handle the chrome versions? If you  have a stagnant chrome version this can be a sign that you are using a controlled browser. does the version update with the typical monthly chrome scheduled releases? how does the update cycle work? i read in the code that the first time it downloads the patched binary and then uses that cached version. that would imply it does not update and this was a one time thing. any ideas on how to handle the update cycles?",
          "score": 3,
          "created_utc": "2026-02-23 13:37:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z0f0r",
              "author": "duracula",
              "text": "Valid point â€” a stale version is a detection signal.\n\nCurrently on Chromium 142, and 145 is being tested now.   \nThe 16 patches port cleanly since they're in isolated files (canvas, WebGL, audio, fonts) â€” porting from 139â†’142 took about a day.\n\nThe plan is monthly builds minimum to stay within the normal version window, with CI automation for faster turnaround.   \nDetection services mostly check if you're within the last 2-3 major versions, so the window is forgiving. And a slightly older Chromium with correct TLS + consistent fingerprints still beats any JS injection tool on a current version.\n\nAuto-update in the wrapper is on the roadmap too â€” check for new binary on launch, download in background.",
              "score": 2,
              "created_utc": "2026-02-23 16:12:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y6qqt",
          "author": "AltruisticHunt2941",
          "text": "This library can scrape this site MakeMyTrip.com ?",
          "score": 3,
          "created_utc": "2026-02-23 13:39:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wtjfs",
          "author": "brokedesigner0",
          "text": "Nice",
          "score": 2,
          "created_utc": "2026-02-23 06:36:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wtq87",
          "author": "ry8",
          "text": "This is awesome. Iâ€™ll be using this. Thank you!",
          "score": 2,
          "created_utc": "2026-02-23 06:38:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wvqxf",
          "author": "kev_11_1",
          "text": "Zillow",
          "score": 2,
          "created_utc": "2026-02-23 06:56:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wxlta",
              "author": "duracula",
              "text": "Zillow worked without a problem.  \nDidn't scraped it all, but search, listing, apartments pages and photos works.",
              "score": 1,
              "created_utc": "2026-02-23 07:13:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6wzi16",
                  "author": "kev_11_1",
                  "text": "I heard it opens initially and blocks after hefty requests.\n\nBut sure i will give it a try",
                  "score": 2,
                  "created_utc": "2026-02-23 07:30:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o749108",
                  "author": "kev_11_1",
                  "text": "Hey, can you try [bizbuysell.com](http://bizbuysell.com), not the home page, but inside the deal page, where I am facing issues even after using camoufox.",
                  "score": 2,
                  "created_utc": "2026-02-24 11:17:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6wxrk4",
          "author": "dsjflkhs",
          "text": " Interesting",
          "score": 2,
          "created_utc": "2026-02-23 07:14:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xd6qt",
          "author": "deadcoder0904",
          "text": "LinkedIn & X are the hardest, no? Even Substack articles don't allow scraping throws \"Too Many Requests\"\n\nX had Bird CLI by OpenClaw creator that got taken down so that might be easy with cookie.\n\nLinkedIn might be the toughest but also one of the most useful ones.\n\nCool project though.",
          "score": 2,
          "created_utc": "2026-02-23 09:45:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xg5e1",
          "author": "inliberty_financials",
          "text": "Good job man ! Thanks this is what i wanted, I'll test out the solution.",
          "score": 2,
          "created_utc": "2026-02-23 10:13:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xgfpb",
          "author": "jagdish1o1",
          "text": "I will sure give it a try",
          "score": 2,
          "created_utc": "2026-02-23 10:16:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xgl5c",
              "author": "jagdish1o1",
              "text": "No mac is a setback for me",
              "score": 2,
              "created_utc": "2026-02-23 10:17:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7fdzyc",
                  "author": "duracula",
                  "text": "macOS Apple Silicon build is in progress â€” it'll come with the Chromium 145 release we currently working on.  \nIn the meantime you can use CloakBrowser on Mac via Docker.",
                  "score": 1,
                  "created_utc": "2026-02-26 00:22:48",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xjovl",
          "author": "ChaandyMan",
          "text": "awesome work",
          "score": 2,
          "created_utc": "2026-02-23 10:47:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xrmzi",
          "author": "Double-Journalist-90",
          "text": "Can you create a user account on X",
          "score": 2,
          "created_utc": "2026-02-23 11:56:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70trx4",
              "author": "duracula",
              "text": "Tried it. Signup flow works fine until the Arkose CAPTCHA step â€” it loads but shows an infinite spinner instead of a puzzle.   \nOur stealth passes all X's bot checks, but Arkose runs its own fingerprinting inside a cross-origin iframe. Currently investigating what's flagging us. Will update.",
              "score": 3,
              "created_utc": "2026-02-23 21:19:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o716mpw",
                  "author": "Double-Journalist-90",
                  "text": "Thanks let me know I appreciate the response",
                  "score": 3,
                  "created_utc": "2026-02-23 22:21:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xxtrl",
          "author": "orucreiss",
          "text": "Do you fingerprint webgl gpu?",
          "score": 2,
          "created_utc": "2026-02-23 12:43:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yznld",
              "author": "duracula",
              "text": "Yes â€” GPU vendor and renderer strings are spoofed via CLI flags at launch:\n\n    --fingerprint-gpu-vendor=NVIDIA Corporation\n    --fingerprint-gpu-renderer=NVIDIA GeForce RTX 3070\n\n  These are patched at the C++ level in the binary, so `WebGLRenderingContext.getParameter(UNMASKED_VENDOR_WEBGL)` and `UNMASKED_RENDERER_WEBGL` both return the spoofed values.   \nNot JS injection â€” the actual GPU reporting functions in Chromium are modified.\n\nThe --fingerprint seed also affects canvas and WebGL hash output, so each session produces a unique but consistent fingerprint.",
              "score": 3,
              "created_utc": "2026-02-23 16:08:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6z3fh2",
                  "author": "orucreiss",
                  "text": "This is sooo good",
                  "score": 2,
                  "created_utc": "2026-02-23 16:26:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yoj25",
          "author": "Broad-Apartment4747",
          "text": "Is there a plan to develop Windows x64?",
          "score": 2,
          "created_utc": "2026-02-23 15:15:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z4ou3",
              "author": "duracula",
              "text": "Yes â€” macOS is next, then Windows.   \nThe patches are platform-agnostic C++ so it's the same code, just need to set up the build environments (Xcode for macOS, Visual Studio for Windows).   \nWe're finishing the Chromium 145 build now on Linux, other platforms will follow.   \nEach platform takes 3-6 hours to compile plus testing against all detection services, so it takes a bit â€” but it's coming.\n\nIn the meantime, you can run it today via Docker on Windows/macOS â€” there's a ready-made Dockerfile included:\n\n    docker build -t cloakbrowser  .\n    docker run --rm cloakbrowser python your_script.py",
              "score": 1,
              "created_utc": "2026-02-23 16:32:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6z5ikp",
                  "author": "usamaejazch",
                  "text": "I am sure you didn't do anything risky. But, I am just pointing it out from the perspective of a third party and because of security reasons. \n\nNPM modules get breached all the time. What if an update secretly ships a session logger or something? ",
                  "score": 2,
                  "created_utc": "2026-02-23 16:35:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yp42s",
          "author": "maher_bk",
          "text": "I'll definitly integrate it in my scraping at scale backend (for my ios app) :) However, I am not sure if it is supporting Ubuntu ARM64 ? (Basically ampere servers)",
          "score": 2,
          "created_utc": "2026-02-23 15:18:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z2xt2",
              "author": "duracula",
              "text": "Not yet â€” currently Linux x64 only.   \nNext up is macOS (arm64 + x64), then Windows. ARM64 Linux is further out.  \n  \nFor scraping at scale, x64 servers work out of the box with pip install cloakbrowser.",
              "score": 1,
              "created_utc": "2026-02-23 16:24:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ypfjx",
          "author": "dud380",
          "text": "Very interesting, especially TLS fingerprinting since it can't be done from JS. Also binary-level hiding of CDP internals. ",
          "score": 2,
          "created_utc": "2026-02-23 15:20:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75fanb",
          "author": "bluemangodub",
          "text": "How do you score on :\n\nhttps://fingerprint-scan.com/\n\nhttps://abrahamjuliot.github.io/creepjs/\n\n\nAre you aiming only for headfulll or aim to provide a passing headless implemented (the hardest of all due to base missing functionality). \n\nBest I could get on fingerprint-scan was 50% likely to be a bot and 30% like headless due to:\n\n1.    noTaskbar: true\n2.   noContentIndex: true\n3.    noContactsManager: true\n4.    noDownlinkMax: true\n\n\nAnyway, looks like a good project, good luck :-)",
          "score": 2,
          "created_utc": "2026-02-24 15:27:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7e95o2",
              "author": "duracula",
              "text": "Thanks for testing! Here are our current scores:\n\nCreepJS:  \n\\- headless: 0%  \n\\- stealth: 0%  \n\\- like-headless: 31% â€” fixes in progress to bring this under 20%\n\nfingerprint-scan.com:  \n\\- Bot Detection: 4/4 PASS (WebDriver, Selenium, CDP, Playwright all false)  \n\\- Bot Risk Score: 45/100 â€” working on lowering this further\n\nWe're targeting full headless pass, not just headful. The remaining signals need C++ stubs in the Chromium build â€” on our roadmap with 145 build.",
              "score": 1,
              "created_utc": "2026-02-25 20:56:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o76xr9t",
          "author": "Glittering_Turn_6971",
          "text": "I would love to try it on macOS with Apple silicon.",
          "score": 2,
          "created_utc": "2026-02-24 19:32:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ebqy2",
              "author": "duracula",
              "text": "macOS Apple Silicon build is in progress â€” it'll come with the Chromium 145 release we currently working on.   \nIn the meantime you can use CloakBrowser on Mac via Docker.",
              "score": 2,
              "created_utc": "2026-02-25 21:08:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ekovd",
          "author": "TheCrandelticSociety",
          "text": "wow.... Camoufox 2.0... this is awesome. getting it up and running in docker on Mac was a breeze. very much appreciate the hardwork! passes akamai without issue. excited for future updates",
          "score": 2,
          "created_utc": "2026-02-25 21:49:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7h5l5x",
          "author": "CptLancia",
          "text": "Hey, is there any profile/fingerprinting management/creation? \nOr is it a single fingerprint that is being used?",
          "score": 2,
          "created_utc": "2026-02-26 07:15:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7h8m1a",
              "author": "duracula",
              "text": "Each launch automatically gets a unique fingerprint â€” a random seed drives all the Chromium-level patches (canvas, WebGL, audio, fonts, client rects) so they stay internally consistent.\n\nFor persistent profiles, pin a seed: `launch(args=[\"--fingerprint=42069\"])`  \nsame seed = same fingerprint every time. You can also customize GPU vendor/renderer, platform, timezone, geolocation, and more via 10 available flags.\n\nWe just documented all of this: [https://github.com/CloakHQ/CloakBrowser#fingerprint-management](https://github.com/CloakHQ/CloakBrowser#fingerprint-management)\n\nPlease update me if there is problem with them.",
              "score": 1,
              "created_utc": "2026-02-26 07:43:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6wyc6r",
          "author": "alexp9000",
          "text": "Ticke tmaster?",
          "score": 1,
          "created_utc": "2026-02-23 07:20:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x0bsb",
              "author": "duracula",
              "text": "With residental proxy, concerts discovery listings and item worked, same for starting ordering process of seats selection.  \nWith datacenter ip, blocked as expected from F5.",
              "score": 1,
              "created_utc": "2026-02-23 07:38:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zew8c",
                  "author": "alexp9000",
                  "text": "Amazing! Need Mac plz :)",
                  "score": 1,
                  "created_utc": "2026-02-23 17:19:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6znqac",
          "author": "letopeto",
          "text": "why is it stuck on v142? I think latest chrome is v145? I've found running an out of date version of chrome increases your flag risk",
          "score": 1,
          "created_utc": "2026-02-23 18:00:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o703ewd",
              "author": "duracula",
              "text": "Not stuck â€” 145 is already built and in testing now on linux.   \n142 is still within the normal version window (detection services mostly flag browsers 3+ major versions behind), and I've been running it in production with solid results. But staying current matters, so 145 is the priority.\n\nStar the repo on GitHub to get notified when it drops.",
              "score": 1,
              "created_utc": "2026-02-23 19:12:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zwokr",
          "author": "pierreortega",
          "text": "and tiktok web? does it get captcha on logged out?",
          "score": 1,
          "created_utc": "2026-02-23 18:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zxd7e",
          "author": "PutHot606",
          "text": "Try www.bet365.bet.br",
          "score": 1,
          "created_utc": "2026-02-23 18:44:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71z36x",
          "author": "stratz_ken",
          "text": "Test it on windows server builds. If it works there it would be the first of its kind.  Almost all of them fail under server operating systems.",
          "score": 1,
          "created_utc": "2026-02-24 00:57:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73th1l",
          "author": "boomersruinall",
          "text": "How about indeed? I have been struggling with this particular target. Also chewy",
          "score": 1,
          "created_utc": "2026-02-24 08:53:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73xrhk",
          "author": "Bharath0224",
          "text": "How about [darty.com](http://darty.com) ?",
          "score": 1,
          "created_utc": "2026-02-24 09:34:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7728wy",
              "author": "duracula",
              "text": "Works with CloakBrowser in headed mode + residential proxy.  \nSee the headed mode section in our README for setup.",
              "score": 1,
              "created_utc": "2026-02-24 19:53:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o79gp1o",
          "author": "Fit-Molasses-8050",
          "text": "I am getting error while installing the CloakBrowser using docker.  \nERROR: error during connect: Head \"http://%2F%2F.%2Fpipe%2FdockerDesktopLinuxEngine/\\_ping\": open //./pipe/dockerDesktopLinuxEngine: The system cannot find the file specified.\n\nhave anyone got the same issue? ",
          "score": 1,
          "created_utc": "2026-02-25 03:21:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7eb9ph",
              "author": "duracula",
              "text": "That's a Docker Desktop issue, not CloakBrowser-specific.  \nDocker Desktop isn't running on your machine â€” start it first, then retry.   \nIf you're on Windows, make sure the Docker Desktop app is open and the engine is fully started before running any docker command",
              "score": 2,
              "created_utc": "2026-02-25 21:06:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o79thbz",
          "author": "Mammoth_Gazelle_9921",
          "text": "No pass recaptcha v3 enterpise invisible",
          "score": 1,
          "created_utc": "2026-02-25 04:41:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7eb2o7",
              "author": "duracula",
              "text": "Hey,  \nwe actually talked about this in a GitHub issues?   \nThe problem was Puppeteer specifically, its CDP protocol leaks automation signals that reCAPTCHA Enterprise picks up.   \nSwitching to the Playwright wrapper fixes it, works great. We've documented it in the README now too.\n\nThanks for testing!",
              "score": 1,
              "created_utc": "2026-02-25 21:05:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ivtlj",
          "author": "Automatic_Bus7109",
          "text": "This looks pretty much like a malware to me.",
          "score": 1,
          "created_utc": "2026-02-26 15:01:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcb2p8",
      "title": "I curated a list of 100+ open-source proxy tools",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rcb2p8/i_curated_a_list_of_100_opensource_proxy_tools/",
      "author": "ciokan",
      "created_utc": "2026-02-23 07:31:27",
      "score": 34,
      "num_comments": 1,
      "upvote_ratio": 0.98,
      "text": "Been collecting proxy-related tools for a while and finally organized them into an awesome-list on GitHub. Covers proxy libraries (Python, Go, Node.js), forward/reverse proxies, SOCKS5 servers, Shadowsocks, Trojan, WireGuard, DNS proxies, scraping frameworks with proxy support, and proxy checkers.\n\nTried to include only actively maintained projects. Happy to add anything I missed â€” PRs welcome.\n\n[https://github.com/drsoft-oss/awesome-proxy](https://github.com/drsoft-oss/awesome-proxy)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rcb2p8/i_curated_a_list_of_100_opensource_proxy_tools/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o78qjof",
          "author": "IWillBiteYourFace",
          "text": "Did you delete the repo? The link doesn't work.",
          "score": -1,
          "created_utc": "2026-02-25 00:53:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rf0nd0",
      "title": "I built an open-source no code web scraper Chrome extension",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rf0nd0/i_built_an_opensource_no_code_web_scraper_chrome/",
      "author": "Dry-Raspberry-3608",
      "created_utc": "2026-02-26 04:34:32",
      "score": 25,
      "num_comments": 11,
      "upvote_ratio": 0.96,
      "text": "Hey everyone,\n\nI do a fair bit of data collection for my own side projects. Usually, I just write a quick Python script with BeautifulSoup, but sometimes I just want to visit a webpage, click on a few elements, and download a CSV without having to open my terminal or fight with CORS.\n\nI tried a few of the existing visual scraping tools out there, but almost all of them lock you into expensive monthly subscriptions. I really hate the idea of paying a recurring fee just to extract public text, and I don't love my data passing through a random third-party server.\n\nSo I spent the last few weeks building my own alternative. Itâ€™s a completely free, open-source no code web scraper that runs entirely locally in your browser.\n\nHere is how the workflow looks right now:\n\n* You open the extension on the page you want to scrape.\n* You click on the elements you want to grab (it auto-detects repeating patterns like lists, grids, or tables).\n* You name your columns (e.g., \"Price\", \"Product Title\").\n* Hit export, and it generates a clean CSV or JSON file instantly.\n\nBecause it runs locally in your browser, it uses your own IP and session state. This means it doesn't get instantly blocked by standard anti-bot protections the way server-side scrapers do.\n\nSince it's open source, you don't have to worry about sudden paywalls, API caps, or vendor lock-in.\n\nYou can install it directly from the Chrome Web Store here:[https://chromewebstore.google.com/detail/no-code-web-scraper/cogbfdcdnohnoknnogniplimgkdoohea](https://chromewebstore.google.com/detail/no-code-web-scraper/cogbfdcdnohnoknnogniplimgkdoohea)\n\n(The GitHub repo with all the source code is linked on the store page, but let me know if you want me to drop it in the comments).\n\nI'm still actively working on it, so please let me know if you run into bugs. It struggles a bit with deeply nested shadow DOMs right now, but I'm trying to figure out a fix for the next update. Honest feedback or feature ideas are super welcome!",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1rf0nd0/i_built_an_opensource_no_code_web_scraper_chrome/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o7gl5pj",
          "author": "Dry-Raspberry-3608",
          "text": "I am not too proud of it yet so please please tell me if u like it and found bugs i will fix everything by next update. ",
          "score": 2,
          "created_utc": "2026-02-26 04:35:53",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o7gmzbo",
          "author": "Connect-Soil-7277",
          "text": "will definitely give it a try",
          "score": 2,
          "created_utc": "2026-02-26 04:48:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gnhmr",
              "author": "Dry-Raspberry-3608",
              "text": "Aye Thanks",
              "score": 1,
              "created_utc": "2026-02-26 04:52:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7h4zwf",
          "author": "angelarose210",
          "text": "Can you share the repo?",
          "score": 2,
          "created_utc": "2026-02-26 07:10:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gmh75",
          "author": "Accomplished_Ear4947",
          "text": "thanks!",
          "score": 1,
          "created_utc": "2026-02-26 04:45:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gxl8w",
          "author": "XxxHAMZAxxX",
          "text": "Thanks for sharing",
          "score": 1,
          "created_utc": "2026-02-26 06:07:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7i7b1r",
          "author": "Kikoness",
          "text": "Seems very interesting for what I do daily! I'll give it a try. Mind sharing the repo? I couldn't find it on the store page.",
          "score": 1,
          "created_utc": "2026-02-26 12:46:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lqrgo",
          "author": "Anxious_Ad2885",
          "text": "what tech stack do you use for build that extension?",
          "score": 1,
          "created_utc": "2026-02-26 23:09:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mr2qk",
          "author": "the__solo__legend",
          "text": "Thanks will definitely helps mee",
          "score": 1,
          "created_utc": "2026-02-27 02:34:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9qhse",
      "title": "Amazon WAF Solver API",
      "subreddit": "webscraping",
      "url": "https://github.com/jonathanyly/awswaf-solver-api",
      "author": "Jolle_",
      "created_utc": "2026-02-20 09:07:13",
      "score": 15,
      "num_comments": 3,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Bot detection ðŸ¤–",
      "permalink": "https://reddit.com/r/webscraping/comments/1r9qhse/amazon_waf_solver_api/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6e78iw",
          "author": "matty_fu",
          "text": "u/xkiiann how does this differ to the one you published last year?",
          "score": 2,
          "created_utc": "2026-02-20 09:25:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e7qvl",
              "author": "Jolle_",
              "text": "its still the same version so i think its not differing when it comes to solving successfully. However checking with Kians script I do some calculuations a bit different (not using zlib) ",
              "score": 3,
              "created_utc": "2026-02-20 09:30:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6efr70",
                  "author": "xkiiann",
                  "text": "I added captcha solving, not just pow",
                  "score": 1,
                  "created_utc": "2026-02-20 10:43:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rcxwo5",
      "title": "Scrape transcripts from Spotify",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rcxwo5/scrape_transcripts_from_spotify/",
      "author": "AnglePast1245",
      "created_utc": "2026-02-23 23:36:52",
      "score": 5,
      "num_comments": 7,
      "upvote_ratio": 0.79,
      "text": "Does anyone know a reliable way (via API, browser extension, script, or tool) to scrape or export full episode transcripts from Spotify podcasts?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rcxwo5/scrape_transcripts_from_spotify/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o73tadx",
          "author": "boomersruinall",
          "text": "Following, as I am also looking for answers regarding transcripts",
          "score": 3,
          "created_utc": "2026-02-24 08:51:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7delxm",
          "author": "jagdish1o1",
          "text": "Have you tried charles to intercept the api requests?",
          "score": 2,
          "created_utc": "2026-02-25 18:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7577nl",
          "author": "yyavuz",
          "text": "In the same boat",
          "score": 1,
          "created_utc": "2026-02-24 14:49:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ak2f2",
          "author": "vorty212",
          "text": "Following",
          "score": 1,
          "created_utc": "2026-02-25 08:20:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rasgub",
      "title": "Can you scrape flight data ?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rasgub/can_you_scrape_flight_data/",
      "author": "malwaregeeek",
      "created_utc": "2026-02-21 14:28:53",
      "score": 5,
      "num_comments": 17,
      "upvote_ratio": 0.73,
      "text": "I am building a flight booking app. I was using Amadeus but they are deprecating next month and I am thinking of alternatives. Is there a way to scrape flight results ?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rasgub/can_you_scrape_flight_data/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6nbvo5",
          "author": "sohailglt",
          "text": "If you are building a booking app, you need to use the official API of Global Distribution Systems (GDS) like Amadeus, Sabre, and Travelport.",
          "score": 13,
          "created_utc": "2026-02-21 19:11:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6o2le2",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-21 21:31:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6pfqlc",
                  "author": "webscraping-ModTeam",
                  "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
                  "score": 1,
                  "created_utc": "2026-02-22 02:26:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6n1att",
          "author": "lp435",
          "text": "There should be APIs for this. No need to scrape",
          "score": 8,
          "created_utc": "2026-02-21 18:19:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n20q2",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-21 18:23:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6pfvox",
                  "author": "webscraping-ModTeam",
                  "text": "ðŸ‘” Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
                  "score": 1,
                  "created_utc": "2026-02-22 02:27:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ny3uc",
          "author": "FranBattan",
          "text": "We are scraping booking.com, expedia.com, despegar, and other places like these and I can tell you that flights or hotels websites are hard to scrape. Also because of the cadence and the number of combinations you need to perform to get the results, it multiplies the amount of data you will get. Also, every website has anti bot systems like datadome and imperva.",
          "score": 5,
          "created_utc": "2026-02-21 21:08:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6o332h",
              "author": "malwaregeeek",
              "text": "Gotcha! Whatâ€™s the latency of these endpoints you use to scrape data? Also what are the legal implications of scraping an OTA?",
              "score": 1,
              "created_utc": "2026-02-21 21:34:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6y1hda",
                  "author": "Comfortable_Camp9744",
                  "text": "In the US if its public without login, you can scrape no problem",
                  "score": 1,
                  "created_utc": "2026-02-23 13:07:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6nj3dq",
          "author": "baxet",
          "text": "thereâ€™s a whole industry for this (source: itâ€™s my daily dev job). Although there are more and more legit API connections being offered every day, but mostly b2b so unless youâ€™re serious about the business donâ€™t get too excited",
          "score": 3,
          "created_utc": "2026-02-21 19:48:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tkyte",
          "author": "iamwasim094",
          "text": "You can build yourself if you have the technical know how, else there are plenty of APIs provider you can may check",
          "score": 1,
          "created_utc": "2026-02-22 19:04:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ttp9x",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-22 19:47:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6vuvio",
                  "author": "webscraping-ModTeam",
                  "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
                  "score": 1,
                  "created_utc": "2026-02-23 02:27:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yo5l5",
          "author": "New_Palpitation3128",
          "text": "I also need to scrape airline data, i used Amadeus but didnâ€™t receive the production keys so it doesnâ€™t give me the required data. Can someone guide me please. I am looking for another platform now.",
          "score": 1,
          "created_utc": "2026-02-23 15:13:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yo84t",
              "author": "New_Palpitation3128",
              "text": "Itâ€™s for my research about airline prices.",
              "score": 1,
              "created_utc": "2026-02-23 15:14:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ra7lkh",
      "title": "Avoiding Recaptcha Enterprise v3",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1ra7lkh/avoiding_recaptcha_enterprise_v3/",
      "author": "saadcarnot",
      "created_utc": "2026-02-20 21:09:08",
      "score": 4,
      "num_comments": 14,
      "upvote_ratio": 1.0,
      "text": "I am working on automating a time critical ticket booking however my last click brings up captcha. It is v3 Enterprise recaptcha. \n\nI can use solvers but it's time critical and i need to complete within 1second . Any ideas? I have tried patchright, playwright, selenium, pydoll. \n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1ra7lkh/avoiding_recaptcha_enterprise_v3/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6j6lwt",
          "author": "RandomPantsAppear",
          "text": "Important questions:Â \n\n* when you do this manually does the captcha show up?\n\n* when you do this automated, using your real browser cookies does the captcha show up?\n",
          "score": 3,
          "created_utc": "2026-02-21 01:58:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kmpys",
              "author": "saadcarnot",
              "text": "Manually captcha never comes but running on automation even with stealth modes everytime it gets triggered. I tried using existing profile got captcha as well",
              "score": 1,
              "created_utc": "2026-02-21 08:45:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6mvp9x",
                  "author": "RandomPantsAppear",
                  "text": "Ok this is oddly enough great news. From this we can establish that your issue isnâ€™t identity or identity length or reputation. \n\nYour issue is most likely the browser driver. Most arenâ€™t perfect. I would load up an automated window, time.sleep(9999999), and then start visiting â€œWebdriver testâ€,â€bot testâ€, etc. Keep looking until you fail one. \n\nSome of these are more/less verbose about the actual issue but itâ€™s normally not too hard to pull the JavaScript and figure out why youâ€™re failing.\n\nAs an alternative: if youâ€™re running headless, stop and test it that way. Some anti bot platforms can detect headless. Use xvfb instead.",
                  "score": 2,
                  "created_utc": "2026-02-21 17:52:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6kk6sd",
          "author": "forklingo",
          "text": "if it is recaptcha enterprise v3 and especially on a ticketing site, they are likely scoring behavior over time and not just that final click, so trying to â€œsolveâ€ it in under a second probably wonâ€™t be reliable. most of the detection happens through fingerprinting, traffic patterns, and account reputation long before the captcha shows up. honestly for time critical booking your best bet is optimizing legit flows like having account, payment, and autofill ready rather than trying to bypass the captcha layer, since they are specifically designed to stop automation.",
          "score": 3,
          "created_utc": "2026-02-21 08:20:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ln6z9",
              "author": "saadcarnot",
              "text": "On the same platform, out competitors have heavy automations running, making me think it is somehow possible. I have tried creating a complete like browsing, clicks, random mouse movements, scrolls and waits on pages. Still when ran using script captcha comes and when using my regular browser manually it works.",
              "score": 1,
              "created_utc": "2026-02-21 13:57:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6pzhbk",
                  "author": "forklingo",
                  "text": "if it works manually but not via script, itâ€™s almost always fingerprinting or browser environment differences. enterprise v3 scores the whole session, not just the last click. your automation probably looks clean to you but still â€œoffâ€ compared to a real user profile. you might need to focus less on fake interactions and more on matching a real browser context exactly.",
                  "score": 2,
                  "created_utc": "2026-02-22 04:42:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6sbrct",
          "author": "irrisolto",
          "text": "Recaptcha tokens last 2 minutes, farm them before even spawning the browser and then use them. No solver is gonna solve recaptcha v3 enterprise within 1s",
          "score": 2,
          "created_utc": "2026-02-22 15:39:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6suhvz",
              "author": "saadcarnot",
              "text": "That's a good point, how do I use them? Is there any place where we inject it in webdriver?",
              "score": 1,
              "created_utc": "2026-02-22 17:02:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6tbbg1",
          "author": "Puzzleheaded_Row3877",
          "text": "You need to farm the tokens before the ticketing starts ,I think they have an expiry of 1 minute ,then intercept the recapcha request on the browser, block it and inject your farmed token.",
          "score": 2,
          "created_utc": "2026-02-22 18:19:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6tqei4",
              "author": "saadcarnot",
              "text": "I will give it a shot, which automation framework gives best request interception capabilities? Currently I have playwright setup",
              "score": 1,
              "created_utc": "2026-02-22 19:30:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6u9tvu",
          "author": "ScrapeAlchemist",
          "text": "Hi,\n\nSince it works manually but triggers on automation, your issue is likely browser fingerprinting rather than the CAPTCHA itself. reCAPTCHA Enterprise v3 scores silently in the background â€” by the time you see the challenge, you've already failed the score check.\n\nA few things to look at:\n\n- **Session warming**: Log in and browse the site normally before the critical click. v3 scores your entire session, not just the final action\n- **Fingerprint consistency**: Make sure your timezone, WebGL, canvas, and navigator properties match a real browser profile. Tools like patchright help but aren't perfect out of the box\n- **Cookie persistence**: Reuse cookies from a manual session where you passed. If you already have a good score tied to that session, the final click won't trigger a challenge\n\nThe 1-second window is realistic if the scoring already happened upstream and you're clean going in.\n\nI hope this helps",
          "score": 2,
          "created_utc": "2026-02-22 21:08:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6uaj6r",
              "author": "saadcarnot",
              "text": "Thank you for these details! On cookie reuse, would running using existing profile use the cookies?",
              "score": 1,
              "created_utc": "2026-02-22 21:12:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdfm7a",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rdfm7a/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-02-24 13:01:04",
      "score": 4,
      "num_comments": 6,
      "upvote_ratio": 0.83,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levelsâ€”whether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) ðŸŒ±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring ðŸ’°",
      "permalink": "https://reddit.com/r/webscraping/comments/1rdfm7a/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o763nqx",
          "author": "jagdish1o1",
          "text": "Hey! Iâ€™m not sure what category I fall into when it comes to scraping, but Iâ€™ve done plenty of scraping projects over the years and have gained solid knowledge of how to scrape various websites.\n\nHere are some tips from my side:\n\n1. Try to avoid using browsers for scraping unless itâ€™s absolutely necessary. Even if you have to use one, capture the request headers from the browser and try to mimic the request using those headers instead.\n2. Use residential rotating proxies for recurring scraping tasks, especially when you need to scrape a site on a daily basis.\n3. Consider integrating AI into your HTML parsing. This can save you a lot of maintenance work in the long run. Just make sure to enforce structured output.\n4. Write modular code instead of putting everything into one or two scripts. This will save you time on future projects and make maintenance easier.\n5. Use exponential backoff instead of simple retries. Even better, use exponential backoff with jitter. This helps reduce bottlenecks and handle rate limiting more effectively.\n\nIf you already have strong scraping knowledge, consider building APIs for popular websites and selling them on RapidAPI.\n\nThese are the points that come to mind right now. Iâ€™ll add more in a reply if I think of anything else.\n\nPeace âœŒï¸",
          "score": 5,
          "created_utc": "2026-02-24 17:17:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7anwp8",
              "author": "GoingGeek",
              "text": "ai is good but which local small model would u recommend for fast parsing.",
              "score": 1,
              "created_utc": "2026-02-25 08:56:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ar31k",
                  "author": "jagdish1o1",
                  "text": "I use openai or gemini models, havenâ€™t tried ai models locally. Apis works just fine.",
                  "score": 1,
                  "created_utc": "2026-02-25 09:26:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ew0xq",
                  "author": "Azuriteh",
                  "text": "Pretty much any SLM post 2025, e.g. Qwen3 4b 2507 should work pretty well",
                  "score": 1,
                  "created_utc": "2026-02-25 22:45:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ao4a7",
              "author": "GoingGeek",
              "text": "and do u mind me knocking u in dm\n\n",
              "score": 1,
              "created_utc": "2026-02-25 08:58:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ar6ju",
                  "author": "jagdish1o1",
                  "text": "Sure as long as youâ€™re not selling something.",
                  "score": 2,
                  "created_utc": "2026-02-25 09:27:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rfdqsb",
      "title": "Web Scraper / Researcher Needed â€“ Pre-Opening  Business Leads",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rfdqsb/web_scraper_researcher_needed_preopening_business/",
      "author": "techguyfl17",
      "created_utc": "2026-02-26 15:51:03",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.83,
      "text": "\n\nDescription:\n\nIâ€™m looking for an experienced web scraper or researcher to help identify brick-and-mortar SMB businesses that are under construction or preparing to open in Florida (starting South Florida/ Florida).\n\nObjective:  \nGenerate weekly leads of businesses BEFORE they launch so I can offer MSP / full-suite technology services.\n\nPrimary Sources:  \nâ€¢ County & city permit databases (Tenant Improvement, Buildout, Commercial Remodel, New Construction)  \nâ€¢ Business license filings  \nâ€¢ Local business journals  \nâ€¢ â€œComing Soonâ€ storefronts  \nâ€¢ Commercial lease announcements\n\nRequired Data:  \nâ€¢ Business name  \nâ€¢ Address  \nâ€¢ Industry/type  \nâ€¢ Permit date + status  \nâ€¢ Estimated opening date (if available)  \nâ€¢ Email/contact (or source link for enrichment)  \nâ€¢ Direct source link\n\nDeliverables:  \nâ€¢ Weekly Google Sheet or CSV  \nâ€¢ No duplicates  \nâ€¢ Fresh leads (last 30 days)  \nâ€¢ Organized + structured format\n\nTo apply:\n\n1. Describe your experience scraping government portals.\n2. Tell me what tools you use (Python, BeautifulSoup, Scrapy, etc.).\n3. Share a sample output (if available).\n4. Quote hourly rate or per-lead pricing.\n\nThis will  become ongoing weekly work for the right candidate.\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Hiring ðŸ’°",
      "permalink": "https://reddit.com/r/webscraping/comments/1rfdqsb/web_scraper_researcher_needed_preopening_business/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1reg5wg",
      "title": "How to scrape ios/android top downloaded apps for a specific country?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1reg5wg/how_to_scrape_iosandroid_top_downloaded_apps_for/",
      "author": "letopeto",
      "created_utc": "2026-02-25 15:24:02",
      "score": 3,
      "num_comments": 6,
      "upvote_ratio": 0.72,
      "text": "How do you scrape ios/android top downloaded apps (free & paid) for a specific country (Sweden)? beyond 100+ results? The endpoint people use (rss.applemarketingtools.com and itunes) only seem to return 100 results.\n\nI can't figure out what the correct api url to query is and if any auth is required. Any help would be greatly appreciated.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1reg5wg/how_to_scrape_iosandroid_top_downloaded_apps_for/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o7cejqy",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-02-25 15:51:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cj961",
              "author": "letopeto",
              "text": "yes im aware of that website, hence why I made this post. I already made a free trial for the website to get the rankings beyond 30 (they only give 30 w/ a free account, and you get the full list on a paid account/trial). But clearly they are able to scrape beyond 100 but all the endpoints I'm trying is limited to 100 results:\n\ne.g. https://itunes.apple.com/se/rss/topfreeapplications/limit=200/genre=36/json \n\n(only gives 100 despite limit set at 200). Apparently this used to return 200 results but they have limited it to 100.\n\nI think there is a new endpoint url to scrape results from because obviously that website is able to get a list beyond 100 result. How are they doing it?\n\nMy goal isn't just to grab the results today, its a daily scrape to see how rankings have changed over time, and ideally i scrape myself vs having to rely on a 3rd party to do it for me that i have to pay for.",
              "score": 1,
              "created_utc": "2026-02-25 16:13:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7cs5b2",
                  "author": "CouldBeNapping",
                  "text": "I wouldnâ€™t waste your time or energy trying to hack through Appleâ€™s security/obscurity for the App Store. \n\nYouâ€™ll likely find that the site I linked to has setup loads of virtual iPhones in Xcode and uses OCR to parse the data. \n\nCould do it yourself but effort and cost vs. impact",
                  "score": 2,
                  "created_utc": "2026-02-25 16:53:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7cyoed",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-25 17:23:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cqate",
          "author": "Medical-Road-5690",
          "text": "Yeah, the 100 result limit on those public endpoints is a known headache. I've had luck using the official App Store Connect API for iOS data it requires a dev account for authentication but can give you deeper country specific charts. For Android, scraping the Play Store directly with something like undetected requests might be your best bet to get past the first page",
          "score": 1,
          "created_utc": "2026-02-25 16:45:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbfrym",
      "title": "Any residential proxy providers with a free trial/credits?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rbfrym/any_residential_proxy_providers_with_a_free/",
      "author": "Srigbok_",
      "created_utc": "2026-02-22 07:41:58",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Does anyone know reputable residential proxy providers that offer a free trial or small free credits (enough for a quick test)?",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1rbfrym/any_residential_proxy_providers_with_a_free/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6qltlf",
          "author": "Drakula2k",
          "text": "Most of scraping APIs with residential proxy support offer free plans, try WebScraping.AI. There is also a proxy mode allowing you to connect to it as an HTTP proxy.",
          "score": 1,
          "created_utc": "2026-02-22 07:55:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qpk6r",
          "author": "Bitter_Caramel305",
          "text": "Thordata gives you 1Mb for testing their residential proxies, my one ran out while I was testing weather the IPs actually rotate or not with [httpbin.org/ip](http://httpbin.org/ip), :(",
          "score": 1,
          "created_utc": "2026-02-22 08:30:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra4xec",
      "title": "Newbie Looking For Advice",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1ra4xec/newbie_looking_for_advice/",
      "author": "PaintPractical4321",
      "created_utc": "2026-02-20 19:27:33",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 0.81,
      "text": "Hello all, I was looking for some advice... \n\n(for those who just want me to get straight to the point... im looking for a way of finding the email addresses of businesses such as pubs, bars, restaurants... I figured a scraper and google maps would be the way to do this)\n\nI have been experimenting with epoxy resin and ended up making glass/bottle art after seeing what others have done on social media. One person/account in particular which drew my attention is based in Germany and uses Etsy as one of the platforms to advertise and sell. I was surprised at how much they seem to have them listed for, and knowing how much its costing me to make each one on average, it appears their is some good profit to be made. I've been doing this as a hobby more than anything up to now but It would be great if I could sell some. I have a couple listed on ebay but I wanted to try being proactive and approaching businesses which would be the most likely to buy this sort of thing... bars, pubs, restaurants. I'm looking for a way to find the email addresses for these...i assume a scraper and google maps would be the way to do this.\n\nI found a couple of free chrome extensions but neither scrape emails as part of the free version. Does anybody know of any free extensions/software... that will?\n\n  \nThanks!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1ra4xec/newbie_looking_for_advice/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6nlxhi",
          "author": "kargie121",
          "text": "Thereâ€™s a way you can scrape the emails but it depends on the location/city",
          "score": 1,
          "created_utc": "2026-02-21 20:03:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ua7jz",
          "author": "ScrapeAlchemist",
          "text": "Hi,\n\nGoogle Maps API is the right call for this. Since you're new to scraping, I'd suggest using an LLM (like ChatGPT or Claude) to help you build the whole setup step by step.\n\nThe approach:\n1. **Google Maps API** â€” search for pubs/bars/restaurants by location. Free tier covers small-scale searches. Ask the LLM to write you a Python script that pulls business names, addresses, phone numbers, and website URLs.\n2. **Website scraping** â€” have the LLM generate a second script that visits each business website and extracts email addresses from contact pages, mailto: links, etc.\n\nYou don't need to know how to code â€” just describe what you want to the LLM and it'll generate working scripts you can run. It can also help you set up Python on your machine if you haven't already.\n\nSkip the Chrome extensions â€” they're limited and unreliable. A simple script gives you full control and costs nothing.\n\nI hope this helps.",
          "score": 1,
          "created_utc": "2026-02-22 21:10:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdbvik",
      "title": "What's working for you with proxy rotation?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rdbvik/whats_working_for_you_with_proxy_rotation/",
      "author": "Forsaken-Bobcat4065",
      "created_utc": "2026-02-24 09:40:07",
      "score": 3,
      "num_comments": 23,
      "upvote_ratio": 0.81,
      "text": "Â Iâ€™ve been down the scraping rabbit hole lately and honestlyâ€¦ Iâ€™m spending way too much time dealing with rate limits, CAPTCHAs, random blocks, and instability. \n\nWhat are people using these days to manage proxies and keep things running smoothly? Rotating residential or datacenter proxies, specific libraries, browser automation, or a mix? \n\nIâ€™m just looking for something that actually works in real-world projects without becoming a full-time maintenance job. Any tools or setups that have made things more stable and hands-off?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rdbvik/whats_working_for_you_with_proxy_rotation/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o75xjf7",
          "author": "Gold_Emphasis1325",
          "text": "The working environment to operate like this is getting crushed. Enough ankle biters creating bots and \"Agents\" caused big players to finally invest the time into closing known gaps that allowed more TOS violations and scraping. It's only going to get more difficult to get away with things. Any long-term plan relying on scraping or having \"robust scraping\" is likely not a plan at all.",
          "score": 4,
          "created_utc": "2026-02-24 16:50:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75520e",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-02-24 14:38:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75axnp",
              "author": "webscraping-ModTeam",
              "text": "âš¡ï¸ Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-02-24 15:07:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ahux6",
          "author": "thomas_estate",
          "text": "Residential proxies are the way to go for anything with serious anti-bot protection. Datacenter IPs get flagged almost immediately on most major sites.\n\nFor rotation, I've had good results with backconnect proxy services that handle the rotation on their end â€” you just hit one endpoint and they cycle IPs automatically. Way less headache than managing your own pool.\n\nBrowser automation (Playwright/Puppeteer) with stealth plugins helps a ton with CAPTCHAs. Some sites still require manual solving services, but between that and residential IPs, most blocks disappear.\n\nWhat scale are you running at? And what types of sites mainly? The approach shifts a lot depending on whether you're hitting a few targets hard vs. scraping broadly across many domains.",
          "score": 2,
          "created_utc": "2026-02-25 07:59:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74orx1",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-24 13:09:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74qoen",
              "author": "webscraping-ModTeam",
              "text": "âš¡ï¸ Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-02-24 13:20:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75m3xf",
          "author": "VonDenBerg",
          "text": "GOOD providers, not trash. ",
          "score": 1,
          "created_utc": "2026-02-24 15:59:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7f5vkf",
              "author": "HardReload",
              "text": "And how would one find those?",
              "score": 1,
              "created_utc": "2026-02-25 23:37:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7h7uag",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 1,
                  "created_utc": "2026-02-26 07:36:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77345g",
          "author": "Alone-Rub-4418",
          "text": "I feel you on the rabbit hole thing. I've had decent luck with a mix of residential proxies and a simple backoff/retry strategy in my scripts. Honestly, the biggest game changer for me was just accepting that some blocks are inevitable and building my scraper to fail gracefully and pick up where it left off",
          "score": 1,
          "created_utc": "2026-02-24 19:57:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78ychh",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-25 01:37:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79dj43",
              "author": "webscraping-ModTeam",
              "text": "ðŸš«ðŸ¤– No bots",
              "score": 1,
              "created_utc": "2026-02-25 03:02:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o790hj1",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-25 01:49:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79dkep",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-25 03:02:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79ttfj",
          "author": "OwnPrize7838",
          "text": "I only use static clean 0 fraud ISP",
          "score": 1,
          "created_utc": "2026-02-25 04:44:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7f5jdm",
              "author": "HardReload",
              "text": "Can you elaborate?",
              "score": 1,
              "created_utc": "2026-02-25 23:35:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rcad6w",
      "title": "Anyone else seeing more blocking from cloud IPs lately?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rcad6w/anyone_else_seeing_more_blocking_from_cloud_ips/",
      "author": "bertdida",
      "created_utc": "2026-02-23 06:49:33",
      "score": 3,
      "num_comments": 8,
      "upvote_ratio": 0.81,
      "text": "Not sure if it's just me, but Iâ€™ve been building scraping-heavy automation lately and noticed something.\n\nEverything works fine locally. Once I deploy to AWS or other cloud providers, some sites start blocking almost immediately.\n\nI already tried adjusting headers, user agents, delays between requests. Still inconsistent. Feels like datacenter IPs are getting flagged much faster now compared to before.\n\nHow are you guys handling this in production? Are datacenter IPs basically unreliable now for certain sites?\n\nJust curious what others are doing.",
      "is_original_content": false,
      "link_flair_text": "Bot detection ðŸ¤–",
      "permalink": "https://reddit.com/r/webscraping/comments/1rcad6w/anyone_else_seeing_more_blocking_from_cloud_ips/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6wvj6f",
          "author": "irrisolto",
          "text": "Datacenter ips were always been flagged. Try using proxies",
          "score": 5,
          "created_utc": "2026-02-23 06:54:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wxtd7",
              "author": "bertdida",
              "text": "True. I guess I'm just noticing it feels even more aggressive lately. ",
              "score": 1,
              "created_utc": "2026-02-23 07:15:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6wwb1y",
          "author": "albert_in_vine",
          "text": "Most of the datacenter proxies gets flagged easily, try usin isp or residential proxies",
          "score": 2,
          "created_utc": "2026-02-23 07:01:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wxjl0",
              "author": "bertdida",
              "text": "Yea, residential works in some cases, but it can get expensive. Still experimenting to find a stable setup.",
              "score": 1,
              "created_utc": "2026-02-23 07:12:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xk3xt",
          "author": "glowandgo_",
          "text": "oh oh thatâ€™s pretty common now. a lot of sites just blanket flag known datacenter ranges, especially from aws/gcp....headers and delays help a bit, but if the ip reputation is burned youâ€™re fighting uphill. some teams move toward residential or proxy rotation, others rethink whether scraping is worth the arms race at all. depends a lot on the target and how aggressive they are.",
          "score": 2,
          "created_utc": "2026-02-23 10:51:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z4yg6",
          "author": "illusivejosiah",
          "text": "Yep, Iâ€™ve seen the same thing. It works locally because your home ISP looks like a normal user, then you deploy to AWS/GCP and youâ€™re suddenly coming from a known datacenter range, so some sites will challenge or block you almost immediately. Tweaking headers and adding delays can help once you get past IP reputation, but it wonâ€™t rescue a cloud IP thatâ€™s already scored as â€œhosting.â€ In my experience, if you get blocked in the first few requests itâ€™s mostly the IP range, and if it runs for a while then starts throwing 403/429/CAPTCHA pages itâ€™s more rate limits or bot detection (sessions, fingerprints, headless). Most production setups either keep compute in AWS and route outbound traffic through residential/ISP/mobile proxies (and keep the same IP for a short session instead of rotating every request), or they run the scraper from residential/ISP egress and just ship results back to the cloud. Datacenter IPs are still fine for easy targets, but once a site is running Cloudflare or one of the big bot vendors you usually need higher-trust IPs and often a real browser. If you can paste one example response (status code plus a couple headers, redact cookies), itâ€™s usually pretty obvious what kind of block youâ€™re hitting.",
          "score": 1,
          "created_utc": "2026-02-23 16:33:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fzm88",
          "author": "Resident-Piano-1663",
          "text": "I'm getting my surf data blocked when I make requests from my droplet but not from my home computer",
          "score": 1,
          "created_utc": "2026-02-26 02:24:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rffmmh",
      "title": "Should I focus on bypassing Cloudflare or finding the internal API?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rffmmh/should_i_focus_on_bypassing_cloudflare_or_finding/",
      "author": "Otherwise-Advance466",
      "created_utc": "2026-02-26 16:59:16",
      "score": 3,
      "num_comments": 11,
      "upvote_ratio": 0.81,
      "text": "Hey r/webscraping,\n\nI've been researching web scraping with Cloudflare protection for a while now and I'm at a crossroads. I've done a lot of reading (Stack Overflow threads, GitHub issues, etc.) and I understand the landscape pretty well at this point â€“ but I can't decide which approach to actually invest my time in.\n\n# What I've already learned / tried conceptually:\n\n* `undetected_chromedriver` works against basic Cloudflare **but not in headless mode**\n* The workaround for headless on Linux is **Xvfb** (virtual display) with SeleniumBase UC Mode\n* `playwright-stealth`, manually copying cookies/headers, FlareSolverr â€“ all **unreliable** against aggressive Cloudflare configs\n* Copying `cf_clearance` cookies into Scrapy requests **doesn't work** because Cloudflare binds them to the original TLS fingerprint (JA3)\n* For serious Cloudflare (Enterprise tier) basically nothing open-source works reliably\n\n# My actual question:\n\nI've heard that many sites using Cloudflare on their frontend actually have **internal APIs** (XHR/Fetch calls) that are either less protected or protected differently (e.g. just an API key).\n\nShould I:\n\n**Option A)** Focus on bypassing Cloudflare using SeleniumBase UC Mode + Xvfb, accepting that it might break at any time and requires a non-headless setup\n\n**Option B)** Dig into the Network tab of the target site, find the internal API calls, and try to replicate those directly with Python requests â€“ potentially avoiding Cloudflare entirely\n\n**Option C)** Something else entirely that I'm missing?\n\n# My constraints:\n\n* Running on Linux server (so headless environment)\n* Python preferred\n* Want something reasonably stable, not something that breaks every 2 weeks when Cloudflare updates\n\nWhat would you do in my position? Has anyone had success finding internal APIs on heavily Cloudflare-protected sites? Any tips on what to look for in the Network tab?\n\nThanks in advance\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rffmmh/should_i_focus_on_bypassing_cloudflare_or_finding/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o7ju9ju",
          "author": "Flojomojo0",
          "text": "You should always try to find the internal apis as it simplifies scraping by a lot",
          "score": 7,
          "created_utc": "2026-02-26 17:41:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jw297",
          "author": "Objectdotuser",
          "text": "headless mode will never work, too easy to detect. just commit to running a bunch of machines and browsers",
          "score": 3,
          "created_utc": "2026-02-26 17:49:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lbnu7",
          "author": "Hopeless_Scraping",
          "text": "Use the TLS fingerprints used for the clearance and replicate them in your request",
          "score": 3,
          "created_utc": "2026-02-26 21:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k3plx",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-26 18:24:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k3thm",
          "author": "Pauloedsonjk",
          "text": "c + b, a.\nA)I use uc with xvfb in production, we have PHP + python, with proxy, captcha\nand\nC+B) too solved in this way when I need it.",
          "score": 1,
          "created_utc": "2026-02-26 18:25:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kvlrb",
          "author": "AdministrativeHost15",
          "text": "API calls are protected too. I investigated an error and instead of JSON it was returning the HTML of the captcha page.",
          "score": 1,
          "created_utc": "2026-02-26 20:36:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lfxw6",
          "author": "TabbyTyper",
          "text": "Not sure it can answered here, but what sites are running enterprise-grade cloudflare? Are those tougher than casino sites to avoid detection?",
          "score": 1,
          "created_utc": "2026-02-26 22:13:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9s0j9",
      "title": "Brave search does not scrape linkedin posts like google search?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r9s0j9/brave_search_does_not_scrape_linkedin_posts_like/",
      "author": "bravelogitex",
      "created_utc": "2026-02-20 10:38:01",
      "score": 2,
      "num_comments": 4,
      "upvote_ratio": 0.67,
      "text": "I have this js code to show the latest hiring posts from Linkedin, but nothing shows up:\n\n`const url = new URL(\"https://api.search.brave.com/res/v1/web/search\");`  \n`url.searchParams.append(\"q\", \"hiring site:linkedin.com/posts\");`  \n`url.searchParams.append(\"freshness\", \"pw\");`  \n`url.searchParams.append(\"operators\", \"true\");`\n\nResults returned are 0. If I change the second line to just`hiring site:linkedin.com,` then a few results show but they are only linkedin profiles, not posts.  \n  \nWhat gives? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r9s0j9/brave_search_does_not_scrape_linkedin_posts_like/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6f1sr6",
          "author": "Quantum_Rage",
          "text": "LinkedIn is a hard target to scrape and Google might have special arrangements to access the data without scraping.",
          "score": 3,
          "created_utc": "2026-02-20 13:22:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i7lmq",
              "author": "cyber_scraper",
              "text": "well, integration with Google search console can immediately send signals about new pages while Brave  doesn't have such option",
              "score": 1,
              "created_utc": "2026-02-20 22:33:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6i9i5f",
              "author": "bravelogitex",
              "text": "Social media becoming walled gardens...",
              "score": 1,
              "created_utc": "2026-02-20 22:43:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rb50m5",
      "title": "Anyone succesfull scraping Idealista websites?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rb50m5/anyone_succesfull_scraping_idealista_websites/",
      "author": "aaronn2",
      "created_utc": "2026-02-21 22:49:54",
      "score": 2,
      "num_comments": 15,
      "upvote_ratio": 0.63,
      "text": "Hello,\n\nas the title says, is anyone successful recently with scrpaing data from the Idealista websites? If so, what is your setup/what kind of proxies do you use?\n\nThey have been pretty aggresive with their protections and nothing seems to be working anymore.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rb50m5/anyone_succesfull_scraping_idealista_websites/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6utbr1",
          "author": "ScrapeAlchemist",
          "text": "Hi,\n\nIdealista uses DataDome, which is one of the more aggressive anti-bot solutions out there. A few things that matter:\n\n- **Residential proxies** are practically required â€” datacenter IPs get flagged instantly\n- **Browser fingerprinting** is the real challenge. They check TLS fingerprint, canvas, WebGL, and navigator properties. Tools like undetected-chromedriver or Playwright with stealth plugins help, but you need to keep them updated\n- **Rate limiting** â€” slow down your requests significantly. DataDome tracks request patterns, so randomized delays between 5-15s per page help\n- **Cookie/session management** â€” solve the initial challenge once, then reuse the session cookies for subsequent requests\n\nThe people who succeed consistently use a combination of residential rotation + proper browser emulation rather than just throwing proxies at it.\n\nI hope this helps.",
          "score": 5,
          "created_utc": "2026-02-22 22:49:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6y6wpn",
              "author": "Kikoness",
              "text": "basically what he said",
              "score": 1,
              "created_utc": "2026-02-23 13:40:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7241sx",
              "author": "Krokzter",
              "text": "How do I avoid request pattern detection at scale? 1 thread without delays would have the same effect as 2 threads with delays, no? And at that point I'd be detected",
              "score": 1,
              "created_utc": "2026-02-24 01:26:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73olop",
                  "author": "ScrapeAlchemist",
                  "text": "Random delays per session, make sure you have mouse movment, make sure you type and scroll like a human, random type speed, random scroll speed. Each session needs to look unique. \n\nIf you can randomize  navigation pattern thats even better. \n\nI ussualy have 3 -4 paths of navigation, then each session gets random path with all other aspects randomized as well..",
                  "score": 1,
                  "created_utc": "2026-02-24 08:07:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6qtbrw",
          "author": "army_of_wan",
          "text": "Yeah, thats just datadome. Its very bypassable",
          "score": 2,
          "created_utc": "2026-02-22 09:06:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ra6aa",
              "author": "aaronn2",
              "text": "Shall I ask you on how can it be done?",
              "score": 1,
              "created_utc": "2026-02-22 11:47:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6u9ns7",
          "author": "klavado",
          "text": "I'm working on an open source project that will let you do that:\n\n[https://scraper.propertywebbuilder.com/listings/ab1dfcf55375](https://scraper.propertywebbuilder.com/listings/ab1dfcf55375)\n\nFor idealista though you will have to paste in the html.\n\nOr you can use the chrome extension that comes with the project - you need to get it directly from the github repo.\n\nI'm actively working on the project now so it will improve over the next few days.  For example I need to retrieve more photos.",
          "score": 2,
          "created_utc": "2026-02-22 21:07:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6um34w",
              "author": "aaronn2",
              "text": "How do you scrape Idealista with Datadome?",
              "score": 1,
              "created_utc": "2026-02-22 22:10:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6wozj6",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-23 05:57:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wqx9b",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-23 06:14:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o72576m",
          "author": "Krokzter",
          "text": "Unfortunately it's aggressive enough that it might be too expensive or hard to scrape at scale. Your best bet would be to use a paid service if you want to make more than 1 request every few seconds. I use a premium service at scale for a pretty steep price and it still struggles. \nIf you still want the challenge or it's a small scale, residential proxies are pretty much mandatory. Using a headful browser also helps if you're able, but you'll be detected if you don't rotate fingerprints. \nEDIT: Alternatively, if this is for Spain only you could target Yaencontre. After they got acquired by Idealista, it got a lot of the Idealista content with more lax protections.",
          "score": 1,
          "created_utc": "2026-02-24 01:32:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}