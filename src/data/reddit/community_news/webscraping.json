{
  "metadata": {
    "last_updated": "2026-01-29 17:09:57",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 11,
    "total_comments": 50,
    "file_size_bytes": 59228
  },
  "items": [
    {
      "id": "1qmd4qm",
      "title": "osn-selenium: An open-source Selenium-based framework",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qmd4qm/osnselenium_an_opensource_seleniumbased_framework/",
      "author": "oddshellnick",
      "created_utc": "2026-01-25 07:57:44",
      "score": 16,
      "num_comments": 3,
      "upvote_ratio": 0.8,
      "text": "Standard Selenium WebDriver implementations often face significant limitations in production environments, primarily due to blocking I/O execution patterns which increase overhead when scaling via threading. Furthermore, the lack of native, typed interfaces for Chrome DevTools Protocol (CDP) domains complicates low-level browser control. Additionally, standard automation signatures are easily identified by advanced anti-bot solutions through browser fingerprint analysis.\n\nTo address these issues, I have developed¬†**osn-selenium**, an asynchronous automation framework built on top of Selenium. Specifically architected for Blink-based browsers (Chrome, Edge, Yandex), it utilizes the¬†Trio¬†library to provide structured concurrency. The framework employs a modular Mixin-based architecture that maintains 99% backward compatibility with standard Selenium calls while exposing advanced control interfaces.\n\n**Core Technical Features:**\n\n* **Structured Concurrency (Trio):**¬†Native integration with the Trio event loop via¬†TrioThreadMixin, enabling efficient concurrent management of multiple browser instances.\n* **Typed CDP Executors:**¬†High-level, typed access to all Chrome DevTools Protocol domains. This allows for real-time network request interception and response manipulation directly from Python.\n* **Advanced Fingerprint Spoofing Engine:**¬†Features a built-in registry of over 200 parameters (Canvas, WebGL, AudioContext, etc.). Detection can be enabled in two lines of code. Supports spoofing via static/random values, static/random noise injection, and dynamic modification of value sequences. Additionally, the registry of parameters can be expanded.\n* **Dedicated dev\\_tools Package:**¬†A module designed for background browser event processing. It features specialized loggers for CDP and fingerprinting activity, alongside advanced request interception handlers.\n* **Full Instance Wrappers:**¬†Custom high-level wrappers for all Selenium objects including WebElements, Alerts, ShadowRoots, etc. These are 100% drop-in compatible with vanilla Selenium logic.\n* **Human-Like Interaction Layer:**¬†Implementation of natural mouse movements using Bezier curves with jitter, smooth scrolling algorithms, and human-like typing simulation.\n\nI am currently expanding the framework's capabilities. Short-term goals include automated parameter aggregation for¬†all flags managers, implementing higher-level logic for¬†Network,¬†Page, and¬†Runtime¬†domains in the¬†dev\\_tools¬†package, refining Human-Like movement patterns, and supporting a hybrid driver interface (both mixins and component-attributes). Support for additional Chromium-based browsers is also underway.\n\nThe long-term roadmap includes support for Gecko-based browsers (Firefox) and developing true internal concurrency for single browser instances using Trio memory channels and direct CDP pipe management. I am looking for technical feedback and contributors to help refine the architecture.\n\nIf you are interested in modernizing your Selenium-based infrastructure, I invite you to explore the [repository](https://github.com/oddshellnick/osn-selenium) and contribute to its development.",
      "is_original_content": false,
      "link_flair_text": "Scaling up üöÄ",
      "permalink": "https://reddit.com/r/webscraping/comments/1qmd4qm/osnselenium_an_opensource_seleniumbased_framework/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1mz7t2",
          "author": "cgoldberg",
          "text": "Using `setup.py` for a new package in 2026 is pretty odd.",
          "score": 1,
          "created_utc": "2026-01-25 16:05:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zk413",
          "author": "kts_2001",
          "text": "Good bro I will test it",
          "score": 1,
          "created_utc": "2026-01-27 09:26:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlewai",
      "title": "I built a CLI that turns websites into real Playwright scrapers",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qlewai/i_built_a_cli_that_turns_websites_into_real/",
      "author": "Acceptable_Grass2812",
      "created_utc": "2026-01-24 05:51:51",
      "score": 16,
      "num_comments": 4,
      "upvote_ratio": 0.73,
      "text": "I built **ScrapeWizard** because using LLMs to write scrapers is slow and expensive ‚Äî you keep generating code, running it, fixing it, and burning API credits.\n\nScrapeWizard does it differently.  \nIt scans the website (DOM, JS, network calls, selectors, pagination) and uses AI **only to generate and fix the scraper code**.  \nThe actual scraping runs locally with Playwright.\n\nSo even if data extraction fails, you still get a full working script with all the site details that you can edit and reuse.\n\nGitHub:  \n[https://github.com/pras-ops/ScrapeWizard](https://github.com/pras-ops/ScrapeWizard)\n\nWould love feedback from people who scrape or automate.",
      "is_original_content": false,
      "link_flair_text": "AI ‚ú®",
      "permalink": "https://reddit.com/r/webscraping/comments/1qlewai/i_built_a_cli_that_turns_websites_into_real/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1mkt89",
          "author": "Habitualcaveman",
          "text": "Web Scraping Copilot for VScode and Scrapy is similar to (re)generate spiders. It guides the LLM to follow a workflow that generates the spiders and creates test fixtures and abstracts the parsing code into something called pageObjects so you get Scrapy spider code you can run anywhere, and just regenerate when the underlying site changes/breaks.",
          "score": 1,
          "created_utc": "2026-01-25 14:58:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uc30g",
              "author": "Acceptable_Grass2812",
              "text": "Web Scraping Copilot for VScode¬†it wroks for static website not js heavy as of now i think",
              "score": 1,
              "created_utc": "2026-01-26 16:21:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ue2ui",
                  "author": "Habitualcaveman",
                  "text": "Yeah you need a browser plugin for the heavy stuff.¬†",
                  "score": 1,
                  "created_utc": "2026-01-26 16:29:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fghkd",
          "author": "_i3urnsy_",
          "text": "Interesting definitely gonna take a look",
          "score": 0,
          "created_utc": "2026-01-24 14:20:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkygpn",
      "title": "What VPS provider do you use for large scale crawling ?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qkygpn/what_vps_provider_do_you_use_for_large_scale/",
      "author": "damienlusson",
      "created_utc": "2026-01-23 18:18:31",
      "score": 11,
      "num_comments": 17,
      "upvote_ratio": 0.92,
      "text": "Hi,\n\n  \nI'm crawling at a large volume (400 millions pages per month), but hetzner and now netcup blocked my vps because of port scanning.\n\n(they don't allow port scanning fair, but when you show them your script and the use case they don't want to change position and maintain that crawling like Google or Bing does is not possible on their infra, fair they are private company and don't want to take any risk i guess)\n\n  \nWhat provider would you recommend, you use ?\n\n  \nThanks in advance",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qkygpn/what_vps_provider_do_you_use_for_large_scale/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1aa2ws",
          "author": "amemingfullife",
          "text": "Why do you need to port scan for crawling? For websites everything is on 80/443. Sounds dodgy tbh.",
          "score": 9,
          "created_utc": "2026-01-23 18:39:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1agkf7",
              "author": "damienlusson",
              "text": "Hetzner and netcup have blocked my server because they are considering i'm doing netscan and port scanning, both of them are not allowed on their infrastructure.\n\nEven after showing them my scrapy setup and code and arguing that i'm doing what a lot of people like Google or Bing do > crawling they still are saying the above to me\n\nI guess i have the same problem as this guy : [https://www.reddit.com/r/hetzner/comments/18u09s3/hetzner\\_says\\_search\\_engine\\_crawlers\\_like\\_google/](https://www.reddit.com/r/hetzner/comments/18u09s3/hetzner_says_search_engine_crawlers_like_google/)",
              "score": 1,
              "created_utc": "2026-01-23 19:08:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1aiti6",
                  "author": "amemingfullife",
                  "text": "Ah, I see, they *consider* it port scanning even if it isn‚Äôt. Fair enough. \n\nI use Vultr, DigitalOcean and GCP and never had issue crawling around 80m sites.",
                  "score": 3,
                  "created_utc": "2026-01-23 19:19:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1anats",
          "author": "Professional-Fox4161",
          "text": "A few years ago I was able to crawl half a billion pages/day on a cluster of scaleway dedicated servers. Don't know if it's still possible.",
          "score": 1,
          "created_utc": "2026-01-23 19:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1btfp1",
              "author": "unteth",
              "text": "Mind explaining your setup?",
              "score": 1,
              "created_utc": "2026-01-23 23:01:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1c1ukt",
                  "author": "Professional-Fox4161",
                  "text": "If I remember correctly there was something like 12 servers with 6 cores and 32Gb RAM, with 3 x 1TB disks for the crawling DB. 6 servers 4 cores with 32gb for the crawlers. Also 3 servers for the message queue and 1 for the config server.",
                  "score": 3,
                  "created_utc": "2026-01-23 23:46:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1bf0t2",
              "author": "Loud_Bathroom_8023",
              "text": "wtf were you scraping haha",
              "score": 0,
              "created_utc": "2026-01-23 21:50:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1bgkix",
                  "author": "Professional-Fox4161",
                  "text": "The web. Hence \"crawling\", not scraping. As OP said, it's the same thing Google and Bing and many others do and it's totally legit if you respect robots.txt, use a user-agent that clearly state your purpose by providing a link to a web page, and use a list of im addresses that is described somewhere.",
                  "score": 2,
                  "created_utc": "2026-01-23 21:57:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1angi6",
          "author": "Difficult-Cat-4631",
          "text": "Leaseweb",
          "score": 1,
          "created_utc": "2026-01-23 19:41:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1av3ek",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-23 20:16:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bikdv",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-23 22:07:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1lxgvp",
          "author": "ScrapeAlchemist",
          "text": "Hi,\n\nAt 400M pages/month, the real issue is that crawling domains pointing to unreachable IPs gets flagged as \"port scanning\" by automated abuse systems ‚Äî even though it isn't.\n\nOptions that actually work:\n\n1. **Linode** ‚Äî explicitly security-research friendly. Open a ticket explaining your use case and they'll whitelist the activity. Starts at $5/mo.\n\n2. **Smaller dedicated server providers with human support** (IOFlood, etc.) ‚Äî unlike Hetzner's automated systems, you can actually talk to someone and explain your use case. ~$60-130/mo for a dedicated box.\n\n3. **Proxy provider with large IP pool** ‚Äî at your scale you need tens of thousands of rotating IPs anyway. This offloads abuse complaints to infrastructure built for it.\n\nHope this helps.",
          "score": 1,
          "created_utc": "2026-01-25 12:43:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nmkfo",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-25 17:45:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r9321",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-26 03:52:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2bu7m0",
          "author": "swiftbursteli",
          "text": "Have you considered spinning up a small device, connecting to business internet and self-hosting?",
          "score": 1,
          "created_utc": "2026-01-29 01:18:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qofvmx",
      "title": "Akamai anti-bot blocking flight search scraping (403/418)",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qofvmx/akamai_antibot_blocking_flight_search_scraping/",
      "author": "Individual-Ship-7587",
      "created_utc": "2026-01-27 14:49:48",
      "score": 8,
      "num_comments": 18,
      "upvote_ratio": 0.79,
      "text": "Hi all,\n\nI‚Äôm attempting to collect public flight search data (routes, dates, mileage pricing) for personal research, at low request rates and without commercial intent.\n\nAirline websites (Azul / LATAM) consistently return 403 and 418 responses, and traffic analysis strongly suggests Akamai Bot Manager / sensor-based protection.\n\n# Environment & attempts so far\n\n* Python and Go\n* Multiple HTTP clients and browser automation frameworks\n* Headless and non-headless browsers\n* Mobile and rotating proxies\n* Header replication (UA, sec-ch-ua, accept, etc.)\n* Session persistence, realistic delays, low RPS\n\nDespite matching headers and basic browser behavior, sessions eventually fail.\n\n# Observed behavior\n\nFrom inspecting network traffic:\n\n* Initial page load sets temporary cookies\n* A follow-up request sends browser fingerprint / behavioral telemetry\n* Only after successful validation are long-lived cookies issued\n* Missing or inconsistent telemetry leads to 403/418 shortly after\n\nThis looks consistent with client-side sensor collection (JS-generated signals rather than static tokens).\n\n# Conceptual question\n\nAt this level of protection, is it generally realistic to:\n\n* Attempt to reproduce sensor payloads manually (outside a real browser), or\n* Does this usually indicate that:\n   * Traditional HTTP-level scraping is no longer viable?\n   * Only full browser execution with real user interaction scales reliably?\n   * Or that the correct approach is to seek alternative data sources (official APIs, licensed feeds, partnerships)?\n\nI‚Äôm not asking for bypass techniques or ToS violations ‚Äî I‚Äôm trying to understand where the practical boundary is for scraping when dealing with modern, behavior-based bot defenses.\n\nAny insight from people who‚Äôve dealt with Akamai or similar systems would be greatly appreciated.\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1qofvmx/akamai_antibot_blocking_flight_search_scraping/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o21hix7",
          "author": "army_of_wan",
          "text": "Have you tried using fire fox automation ? ( puppeteer , camafoux ?)\nHarvest cookies with a browser and then reuse them to scrape the site.",
          "score": 4,
          "created_utc": "2026-01-27 16:27:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24xvrd",
              "author": "letopeto",
              "text": "Isn‚Äôt camoufox widely outdated? Repo hasn‚Äôt been update in a while it‚Äôs still running a very old version of Firefox and last time I tried it it couldn‚Äôt even bypass a simple Cloudflare challenge",
              "score": 1,
              "created_utc": "2026-01-28 02:04:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o265xra",
              "author": "irrisolto",
              "text": "Not gonna work at scale",
              "score": 1,
              "created_utc": "2026-01-28 06:38:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o266nm3",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 1,
                  "created_utc": "2026-01-28 06:43:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21k1xl",
          "author": "Specialist-Egg-3720",
          "text": "once the js challenge is completed in real browser, it sets a cookie, you need to capture that cookie then move on to do your api calls, its very difficult to reproduce a sensor payload without the real browser as it checks for various things such as webgl rendering, viewport, cpu info +gpu info + screen info + other params of your pc to create a unique device specifc signature. Its good option to just solve this challenge in real browser , get the cookie then move on to http request based scraping.",
          "score": 3,
          "created_utc": "2026-01-27 16:38:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22tuoh",
          "author": "Afraid-Solid-7239",
          "text": "reply with the page, and data you want to scrape?",
          "score": 1,
          "created_utc": "2026-01-27 19:56:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27bgei",
              "author": "Individual-Ship-7587",
              "text": "The page is a public flight search results page, for example:\n\n[https://www.latamairlines.com/br/pt/oferta-voos?origin=POA&destination=SCL&outbound=2026-06-17&inbound=2026-03-26&trip=RT&cabin=Economy&redemption=true]()\n\nI‚Äôm only trying to collect the data shown to any user on that page:  \nflight options (route, dates, airline), mileage prices, taxes/fees, cabin class, and basic availability information. No booking, no account access, and no private user data.",
              "score": 1,
              "created_utc": "2026-01-28 12:29:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o265wps",
          "author": "irrisolto",
          "text": "Use a solver to get akamai cookies",
          "score": 1,
          "created_utc": "2026-01-28 06:37:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26nn9p",
              "author": "scraperouter-com",
              "text": "what solver?",
              "score": 1,
              "created_utc": "2026-01-28 09:12:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o279qaa",
                  "author": "irrisolto",
                  "text": "A solver for akamai that gives you cookies",
                  "score": 1,
                  "created_utc": "2026-01-28 12:17:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2b1zbx",
          "author": "nez1rat",
          "text": "The only way to scrape successfully the data is by using a valid TLS fingerprint with valid Akamai cookies in your request.  \n  \nTo test it simply copy paste from your browser the cookies that are being used in the fetch flights request",
          "score": 1,
          "created_utc": "2026-01-28 22:51:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qod78a",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qod78a/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-01-27 13:01:04",
      "score": 8,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levels‚Äîwhether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) üå±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1qod78a/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2b5ims",
          "author": "Working_Map379",
          "text": "I am looking to hire web scraping expert. Please DM me.",
          "score": 2,
          "created_utc": "2026-01-28 23:09:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20j8it",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-27 13:45:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20v3bm",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-01-27 14:46:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21d8jp",
          "author": "xRazar",
          "text": "I'm currently in the process of scraping e-sim sites wherever possible I try to find the public APIs for this but it does not seem that effective for most of the sites. Anyone has experience with scraping E-Sim sites (Saily, Nomad as few examples to go off)",
          "score": 1,
          "created_utc": "2026-01-27 16:09:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21ndjn",
          "author": "EstablishmentOver202",
          "text": "How do you guys deal with cloudflare? Turnstile is killing me",
          "score": 1,
          "created_utc": "2026-01-27 16:53:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24ff3z",
              "author": "Mean_Professional529",
              "text": "Try a scraping API that handles JavaScript rendering and proxy rotation. Some services include built-in CAPTCHA solving for Turnstile. This can help bypass Cloudflare without managing it yourself",
              "score": 1,
              "created_utc": "2026-01-28 00:28:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2e6w04",
              "author": "jonfy98",
              "text": "You could try APIScraper which is efficient but not free,   \nanother idea could be NoDriver for better successrate which i often use for dealing with this kind of problem.",
              "score": 1,
              "created_utc": "2026-01-29 11:37:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o28bhyx",
          "author": "Either_Height7010",
          "text": "I'm hiring a US-based senior+ reverse engineer. At an incredibly high level, think bypassing anti-bot systems, large-scale web scraping/login automation, and JavaScript-based reverse engineering of web apps. \n\nI'm a third-party recruiter sourcing on behalf of my client. Message me if intrigued!",
          "score": 1,
          "created_utc": "2026-01-28 15:38:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnimgt",
      "title": "Scrape a webpage that uses Akamai",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qnimgt/scrape_a_webpage_that_uses_akamai/",
      "author": "flstckdev",
      "created_utc": "2026-01-26 15:11:09",
      "score": 7,
      "num_comments": 13,
      "upvote_ratio": 0.72,
      "text": "I‚Äôm trying to scrape a webpage that uses Akamai bot protection and need to understand how to properly make HTTP requests that comply with Akamai‚Äôs requirements without using Selenium or Playwright.\n\nDoes anyone have general guidance on how Akamai detects non-browser traffic, what headers/cookies/flows are typically required, or how to structure requests so they behave like a normal browser? Any high-level advice or references would be helpful.\n\nThanks in advance.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qnimgt/scrape_a_webpage_that_uses_akamai/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1u2244",
          "author": "Acrobatic_Idea_3358",
          "text": "Not my write up but this guy has some headers and user agents that worked at some point. Probably  a good starting point, however I have not tested any of these. https://substack.thewebscraping.club/p/scraping-akamai-protected-websites",
          "score": 4,
          "created_utc": "2026-01-26 15:38:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zdfw3",
              "author": "flstckdev",
              "text": "Thanks! Definitely a good starting point.",
              "score": 2,
              "created_utc": "2026-01-27 08:23:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ud3cn",
          "author": "Odd-Lychee1248",
          "text": "I faced akamai bot detection and blocked my requests even I used selenium, but I solved the problem using SeleniumBase undetected mode and I sent the request using ex_script function inside chrome devtools using fetch()",
          "score": 5,
          "created_utc": "2026-01-26 16:25:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zdcgo",
              "author": "flstckdev",
              "text": "That one I did not know of. Thank you, i‚Äôll look into it!",
              "score": 1,
              "created_utc": "2026-01-27 08:22:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1u3epk",
          "author": "SurlyJason",
          "text": "I've done a few, and found they vary from stupidly easy (usually HTTP headers) to more complex where I could get some requests to work, but an IP would be blocked after just a few successful requests. \n\nIf you're okay sharing the URL, I have a little utility I made to test sites. I could see if I can tell where on the spectrum it lies.",
          "score": 2,
          "created_utc": "2026-01-26 15:44:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vjxdu",
              "author": "brnbs_dev",
              "text": "How does that utility tool work?",
              "score": 1,
              "created_utc": "2026-01-26 19:28:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21osch",
          "author": "ScrapeAlchemist",
          "text": "Hey!\n\nAkamai's detection goes way beyond headers - it fingerprints TLS settings (JA3/JA4), HTTP/2 frame ordering, and behavioral signals from their sensor script.\n\nFor pure HTTP without a browser, you'd need to:\n\n- Match TLS fingerprint exactly (curl-impersonate can help here)\n- Replicate HTTP/2 frame ordering and priority settings\n- Handle their cookie validation flow (initial request ‚Üí sensor challenge ‚Üí validated session)\n- Rotate IPs before behavioral patterns get flagged\n\nThe substack article someone linked is a good starting point for headers. But honestly, if the site is running Akamai Bot Manager (not just basic Akamai CDN), pure HTTP scraping becomes a cat-and-mouse game. Their sensor.js collects browser fingerprints that are hard to fake without actual JS execution.\n\nYou might want to check if the site has a mobile app or public API - sometimes those have lighter protections than the web frontend.\n\nGood luck!",
          "score": 2,
          "created_utc": "2026-01-27 16:59:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21loks",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-27 16:45:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21pzgd",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-27 17:04:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qpdyt6",
      "title": "Internal Google Maps API endpoints",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qpdyt6/internal_google_maps_api_endpoints/",
      "author": "LouisDeconinck",
      "created_utc": "2026-01-28 15:11:48",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 0.89,
      "text": "I build a scraper that extracts place IDs from the protobuf tiling api. Now I would like to fetch details from each place using this place id (I also have the S2 tile id). Are rhere any good endpoints to do this with?",
      "is_original_content": false,
      "link_flair_text": "Scaling up üöÄ",
      "permalink": "https://reddit.com/r/webscraping/comments/1qpdyt6/internal_google_maps_api_endpoints/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qmysbt",
      "title": "I'm starting a web scraping project. Need advices.",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qmysbt/im_starting_a_web_scraping_project_need_advices/",
      "author": "Papenguito",
      "created_utc": "2026-01-25 23:11:50",
      "score": 6,
      "num_comments": 15,
      "upvote_ratio": 0.72,
      "text": "I am going to start a project of web scraping. Is playwright with TS the best option to start i want to scrape some pages o news from my city i need advices to start with this pls ",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qmysbt/im_starting_a_web_scraping_project_need_advices/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1pr5a0",
          "author": "hikingsticks",
          "text": "You need to investigate the pages and see what required to get the data you want. Only use headless browser if you have to, it's much more preferable to not use one if possible.\n\nOpen the network tab and check the requests being made by your browser, see which one(s) have the data you need, and try to replicate them.",
          "score": 5,
          "created_utc": "2026-01-25 23:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ptnwn",
              "author": "Papenguito",
              "text": "i want to get the news from the web pages",
              "score": 0,
              "created_utc": "2026-01-25 23:29:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1q1pwi",
                  "author": "hikingsticks",
                  "text": "Yes... You'd be well served by learning some html basics, and becoming familiar with the network tab. Then watch some John Watson Rooney on YouTube for scraping techniques.\n\nOr just throw AI at it and learn nothing.",
                  "score": 4,
                  "created_utc": "2026-01-26 00:08:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1q0vmy",
                  "author": "Own_Relationship9794",
                  "text": "which website is it?",
                  "score": 1,
                  "created_utc": "2026-01-26 00:04:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ut9su",
          "author": "hasdata_com",
          "text": "Have you looked into Google News RSS? That's usually the easiest starting point if you just need the headlines. For the actual sites, it really comes down to how they load data. If it's simple static HTML, basic request libs work fine. But for anything with JS rendering, you're right, you will need heavier tools like Playwright to handle the dynamic content",
          "score": 7,
          "created_utc": "2026-01-26 17:35:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rxmyb",
          "author": "Key_Investment_6818",
          "text": "basic html parsing with curl\\_cffi should do the job , just make sure you know what elements you want to scrape..",
          "score": 2,
          "created_utc": "2026-01-26 06:41:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1u2hig",
          "author": "bluemangodub",
          "text": "Depends on the site. Really it's trial and error.  Try HTTP requests. If that works, great. If not, do really need a browser? If so, try a browser. Does it work? Great? If not, then finger the anti bot detections and by pass that",
          "score": 2,
          "created_utc": "2026-01-26 15:39:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1u63vt",
              "author": "Papenguito",
              "text": "Thanks mate",
              "score": 1,
              "created_utc": "2026-01-26 15:55:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o22xdua",
          "author": "elixon",
          "text": "Start by opening the Network tab in your browser and search for the data in the requests there Ctrl+Shift+F in Chrome. Find the request that contains the information you need, right click it, and copy it as curl or fetch. Learn how to make simple, effective HTTP requests directly.\n\nSkip Playwright. It is expensive and unnecessary for 99% of scraping use cases. Only beginners rely on it because they never scale and keep their ambitions low.",
          "score": 2,
          "created_utc": "2026-01-27 20:12:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2au4lc",
          "author": "No-Incident5783",
          "text": "From experience, try not to overcomplicate things. Depending on the complexity of the website it might be better to simply use htpp request or selenium. Also, if you are a beginner, don‚Äôt necessarily use headless browser with selenium or playwright. This way you can see what your code is doing and through which tabs, elements etc it is going through.",
          "score": 2,
          "created_utc": "2026-01-28 22:14:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1q0zqy",
          "author": "Rorschache00714",
          "text": "If if you download Antigravity you can tell the agent to use the browser and do that for you. Have it create a json file with all the scrape data.",
          "score": 1,
          "created_utc": "2026-01-26 00:04:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qat0f",
          "author": "akashpanda29",
          "text": "You can do it from playwright. But playwright is a overkill for most of the website where you can just get html of api data as json directly through a fetch call. \nSo investigating website is the primary step",
          "score": 1,
          "created_utc": "2026-01-26 00:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r11cx",
          "author": "Holiday-Tonight5626",
          "text": "every news site is different. sites know ppl r scraping, so they all have measures to deal with that. some use apis, like npr i think..\nif you want to scrape popular news sites yeah you will have to use pw for a lot of it. wait for the js to render then grab that shit",
          "score": 1,
          "created_utc": "2026-01-26 03:06:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qk5u1r",
      "title": "Reservation Alerts",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qk5u1r/reservation_alerts/",
      "author": "Fearless_Space_2822",
      "created_utc": "2026-01-22 20:30:01",
      "score": 5,
      "num_comments": 22,
      "upvote_ratio": 0.69,
      "text": "Looking to build a scraper that alerts me via discord webhook whenever a reservation opens up for a place that uses [waitwhile.com](http://waitwhile.com) . I don't have much coding experience besides data languages but figured I could code this via AI. Looking for how possible and easy this could be or any tips that you experts have.\n\nThe bot would need to essentially monitor and refresh the site, then as cancellations occur or new times open up, the bot would send some sort of custom webhook to alert me of the time/day available with a link to book. I would probably have it poll every 2-3 minutes and use proxies to avoid IP ban. I was checking around github and other sites to see if something has been made already since this is a very commonly used reservation host. Thanks for all the help in advance and I could provide more information if needed.   \n  \nEDIT: The main error I'm running into is that the bot sends a webhook every time it checks rather than filtering to only when its available, then populating the webhook with info.",
      "is_original_content": false,
      "link_flair_text": "AI ‚ú®",
      "permalink": "https://reddit.com/r/webscraping/comments/1qk5u1r/reservation_alerts/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o17ku9h",
          "author": "Kbot__",
          "text": "Hi,\n\nGood news ‚Äî waitwhile makes this easier than you think. The booking data is embedded directly in the page's HTML as a JSON blob (inside a tag called `__NEXT_DATA__`). No headless browser or JS rendering needed.\n\nSince you're already using n8n, here's what I'd do:\n\n1. **HTTP Request node** ‚Äî simple GET to the waitwhile URL\n2. **HTML Extract node** ‚Äî pull the JSON from the `<script id=\"__NEXT_DATA__\">` tag\n3. **Check the field** `props.ssrLocationData.availableBookingResourceIds` ‚Äî when this array is not empty, slots are open\n4. **IF node** ‚Äî compare to the previous result (store it with a Set node or a file). Only continue if it changed from empty to non-empty\n5. **Discord webhook node** ‚Äî fires only when new slots appear\n\nThat solves your \"webhook on every refresh\" problem ‚Äî you're only alerting on state changes, not on every poll.\n\nI tested it just now with a plain GET request ‚Äî the data comes back without needing proxies or anything special.",
          "score": 5,
          "created_utc": "2026-01-23 09:29:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18sxrn",
              "author": "Super_Refuse_2415",
              "text": "üî•",
              "score": 1,
              "created_utc": "2026-01-23 14:34:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o19ejmb",
              "author": "Fearless_Space_2822",
              "text": "i'll try this",
              "score": 1,
              "created_utc": "2026-01-23 16:16:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o149vc7",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-01-22 21:05:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14enjz",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-22 21:28:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1540ll",
                  "author": "webscraping-ModTeam",
                  "text": "ü™ß Please review the sub rules üëâ",
                  "score": 2,
                  "created_utc": "2026-01-22 23:38:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o157ny3",
          "author": "davak72",
          "text": "When you say ‚Äúthe bot‚Äù sends a webhook every time, what are you referring to?? You mention GitHub, but not that you‚Äôve actually started implementing a bot in any particular language or based on any particular project. It‚Äôs hard to help with 0 context",
          "score": 1,
          "created_utc": "2026-01-22 23:58:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15bbip",
              "author": "Fearless_Space_2822",
              "text": "Sorry for lack of context, as I really don't code. I'm just looking to solve a problem with automation. I want  the bot to send a webhook message into a designated discord channel alerting me of an open reservation on the website, with the link to book said reservation.  \n  \nI mentioned github just as a site that I scoured for possible solutions that have already been made by other people. So far, through json I have a bot that sends messages through a webhook when it refreshes the site, but lacks the actual useful info in the webhook. I have no preferences just want to get this solved.",
              "score": 1,
              "created_utc": "2026-01-23 00:17:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o15bnod",
                  "author": "matty_fu",
                  "text": "If you‚Äôre not comfortable sharing the URL of the specific service you‚Äôre scraping, can you find another one that uses waitwhile? Hopefully the same techniques will apply to both",
                  "score": 1,
                  "created_utc": "2026-01-23 00:19:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o15vuzc",
                  "author": "davak72",
                  "text": "Ohhh. Gotcha. You‚Äôll honestly probably have good success working even with ChatGPT free. What language is your minimal bot in?",
                  "score": 1,
                  "created_utc": "2026-01-23 02:10:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o15r26r",
          "author": "_i3urnsy_",
          "text": "Is your solutions browser or request based? It‚Äôs good you‚Äôre getting the webhook, but sounds like you just are missing whatever the identifier for open or reserved bookings is.\n\nWithout the specific url or a similar one it‚Äôs hard to assist further.",
          "score": 1,
          "created_utc": "2026-01-23 01:43:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o167j3b",
              "author": "Fearless_Space_2822",
              "text": "Im trying to make it work on [https://waitwhile.com/locations/chromehearts/services?registration=booking](https://waitwhile.com/locations/chromehearts/services?registration=booking)\n\nwaitwhile is a popular host for waitlists and reservations.",
              "score": 2,
              "created_utc": "2026-01-23 03:15:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qo77gd",
      "title": "Advice needed: scraping company websites in Python",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qo77gd/advice_needed_scraping_company_websites_in_python/",
      "author": "Working_Taste9458",
      "created_utc": "2026-01-27 07:26:44",
      "score": 5,
      "num_comments": 14,
      "upvote_ratio": 0.86,
      "text": "I‚Äôm building a small project that needs to scrape company websites (manufacturers, suppliers, distributors, traders) to collect basic business information. I‚Äôm using Python and want to know what the best approach and tools are today for reliable web scraping. For example, should I start with requests + BeautifulSoup, or go straight to something like Playwright? Also, any general tips or common mistakes to avoid when scraping multiple websites would be really helpful.",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qo77gd/advice_needed_scraping_company_websites_in_python/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1zauu4",
          "author": "Bitter_Caramel305",
          "text": "Playwright is not the choice of any expert it's the choice of dumb beginners.\n\nRequests and bs4 is fine but replace requests with the requests module of curl\\_cffi.  \nThe syntax will be the same, but you'll get TSL fingerprinting of a real browser (Thanks to C) and an optional but powerful request param (impersonate=\"any browser of your choice\").\n\nExample:\n\n    from curl_cffi import requests\n    r = requests.get(url, cookies, headers, impersonate=\"chrome\")\n\n\n\nAlso, always reverse engineer the exposed backend API first and use this as a fallback not primary method.  \nHappy scraping!",
          "score": 12,
          "created_utc": "2026-01-27 08:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zn2lq",
              "author": "scraperouter-com",
              "text": "if curl\\_cffi is blocked you can try scrapling stealthmode but only if you are sure you need the browser (much slower way)",
              "score": 3,
              "created_utc": "2026-01-27 09:53:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o23uv95",
                  "author": "askolein",
                  "text": "But isn‚Äôt most websites not directly rendering html via http requests. I struggle to see any relevant website to scrape without selenium?",
                  "score": 1,
                  "created_utc": "2026-01-27 22:44:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o277s0v",
              "author": "husayd",
              "text": "I feel offended by the first sentence xd. I use both, and sometimes playwright (or selenium) is inevitable, or i am just a dumb beginner.",
              "score": 1,
              "created_utc": "2026-01-28 12:04:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o279aau",
                  "author": "Bitter_Caramel305",
                  "text": "Sorry about that ;) but to be honest, sometimes I reverse engineer the entire website while reverse engineering the API, just so I can avoid the inevitable browser automation.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:14:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o208fu0",
          "author": "Responsible-Fly-990",
          "text": "go with requests + BeautifulSoup if you r a beginner",
          "score": 2,
          "created_utc": "2026-01-27 12:43:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24byq2",
          "author": "Hungry-Working26",
          "text": "For company sites, start with requests and BeautifulSoup. Switch to Playwright only if you see dynamic content. Rotate user agents and add delays between requests to be respectful.\n\nHere's a basic pattern using the requests library:\n\npython\n\nimport requests\n\nfrom bs4 import BeautifulSoup\n\nresponse = requests.get('your\\_url\\_here')\n\nsoup = BeautifulSoup(response.content, 'html.parser')\n\nAlways check the site's robots.txt first",
          "score": 2,
          "created_utc": "2026-01-28 00:11:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2818mg",
          "author": "New-Independence5780",
          "text": "use cheriooCrawlee if it just simple websites that doesnt need js rendering if yes use playwrightCrawlee or puppeterCrawlee",
          "score": 1,
          "created_utc": "2026-01-28 14:51:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28rt0d",
          "author": "wequatimi",
          "text": "So you got ai businessidea=make cash fast.\nMight be entertaining. And educative..",
          "score": 1,
          "created_utc": "2026-01-28 16:49:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2azs7r",
          "author": "nez1rat",
          "text": "Honestly it depends on what are your target sites tho, I can suggest you to use [https://pypi.org/project/curl-cffi/](https://pypi.org/project/curl-cffi/) with BeautifulSoup as you mentioned",
          "score": 1,
          "created_utc": "2026-01-28 22:40:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eubfr",
          "author": "byte_knight_",
          "text": "Definetely start from with requests and bs4 or speed and simplicity, i'd use Playwright only for something JS heavy maybe",
          "score": 1,
          "created_utc": "2026-01-29 14:03:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmbyps",
      "title": "Browser Code: Coding agent for user scripts",
      "subreddit": "webscraping",
      "url": "https://github.com/chebykinn/browser-code",
      "author": "heraldev",
      "created_utc": "2026-01-25 06:51:59",
      "score": 4,
      "num_comments": 5,
      "upvote_ratio": 0.71,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qmbyps/browser_code_coding_agent_for_user_scripts/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1l1q21",
          "author": "RandomPantsAppear",
          "text": "Good damn work. I love the idea of making a virtual fs to represent the webpage",
          "score": 3,
          "created_utc": "2026-01-25 08:11:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l58mh",
          "author": "BodybuilderLost328",
          "text": "Its all fine till the html of all the page exceed the llm context, how are you handling this?\n\nSo like for bigger webpages like amazon this tool wont work right?",
          "score": 3,
          "created_utc": "2026-01-25 08:42:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ncqle",
              "author": "heraldev",
              "text": "It will! The agent in the extension reads the page as a file. This file is formatted and cleaned up - I add spaces and newlines around each html tag, this allows for reading only the parts of it. Then the agent has 3 tools to explore the file - read with offset and limit, grep, and as a last resort it can execute JS to filter elements.",
              "score": 1,
              "created_utc": "2026-01-25 17:03:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wwdei",
          "author": "quarkcarbon",
          "text": "Reducing a semantic rich webpage as a file with some offsets doesn't it lead to inefficient reads and highly expensive ops ? Also it's all fun and games in testing with browser webpage/extension storage. But the min you do it on the real user's browser - their device type/ram + the number of tabs they open and how full is storage is gonna blow out with browser storage errors soon esp since you take the html of pages. Now if you often fallback to user's device FS, then it's another CLI agent with web access.",
          "score": 1,
          "created_utc": "2026-01-26 23:06:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ksdns",
          "author": "heraldev",
          "text": "I‚Äôve been experimenting with embedding an Claude Code-style coding agent directly into the browser.\n\nAt a high level, the agent generates and maintains userscripts and CSS that are re-applied on page load. Rather than just editing DOM via JS in console the agent is treating the page, and the DOM as a **file**.\n\nThe models are often trained in RL sandboxes with full access to the filesystem and bash, so they are really good at using it. So to make the agent behave well, I've simulated this environment.\n\nThe whole state of a page and scripts is implemented as a virtual filesystem hacked on top of `browser.local` storage. URL is mapped to directories, and the agent starts inside this directory. It has the tools to read/edit files, grep around and a fake bash command that is just used for running scripts and executing JS code.\n\nI've tested only with Opus 4.5 so far, and it works pretty reliably.  \nThe state of the file system can be synced to FS, although because Firefox doesn't support Filesystem API, you need to manually import the FS contents first.\n\nThis agent is \\*really\\* useful for extracting things to CSV.",
          "score": 1,
          "created_utc": "2026-01-25 06:52:28",
          "is_submitter": true,
          "replies": []
        }
      ]
    }
  ]
}