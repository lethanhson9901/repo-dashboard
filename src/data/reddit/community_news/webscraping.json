{
  "metadata": {
    "last_updated": "2026-02-05 09:15:02",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 14,
    "total_comments": 68,
    "file_size_bytes": 70636
  },
  "items": [
    {
      "id": "1qs66k0",
      "title": "Couldn't find proxy directory with filters so built one",
      "subreddit": "webscraping",
      "url": "https://i.redd.it/xo702jo5kpgg1.png",
      "author": "Consistent-Feed-7323",
      "created_utc": "2026-01-31 16:04:15",
      "score": 34,
      "num_comments": 28,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qs66k0/couldnt_find_proxy_directory_with_filters_so/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2uiaup",
          "author": "Important-Seat-1882",
          "text": "Do you have here information on whether proxies can be used behind login and auth? ",
          "score": 3,
          "created_utc": "2026-01-31 20:06:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2v3efn",
              "author": "Consistent-Feed-7323",
              "text": "If you mean whenever proxies auth is by IP or login;password - yeah, there's a filter for that.",
              "score": 3,
              "created_utc": "2026-01-31 21:50:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2xewcb",
                  "author": "Lemon_eats_orange",
                  "text": "Important Seat may be referring to if the IP's themselves can be used for a login use case where the data isn't public. Some providers won't allow logging into some accounts with their proxies. Could be wrong though.",
                  "score": 1,
                  "created_utc": "2026-02-01 06:17:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2wjdbx",
          "author": "scrapingtryhard",
          "text": "EPICCCCC, the footer is still 2025 though u should fix it",
          "score": 3,
          "created_utc": "2026-02-01 02:41:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wjz7n",
              "author": "Consistent-Feed-7323",
              "text": "Dear god, that's probably the last thing I would ever think of lmao. Thank you!",
              "score": 1,
              "created_utc": "2026-02-01 02:45:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ywrbm",
                  "author": "scrapingtryhard",
                  "text": "add proxyon to it lol",
                  "score": 2,
                  "created_utc": "2026-02-01 13:50:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xf4q8",
          "author": "Lemon_eats_orange",
          "text": "Overall this is pretty good! No marketing just a list of proxies and what they can be used for. And you can check their trust pilots.",
          "score": 3,
          "created_utc": "2026-02-01 06:19:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2zfmfm",
          "author": "zrsca",
          "text": "This is so cool! Maybe add a sort by $/GB option. Really good site üôè",
          "score": 3,
          "created_utc": "2026-02-01 15:31:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zspeq",
              "author": "Consistent-Feed-7323",
              "text": "Thank you, I will try to add something like that in the future, biggest problem is to define price, since I'm putting minimal available for purchase price and services are often providing wholesale deals",
              "score": 1,
              "created_utc": "2026-02-01 16:32:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31asxq",
                  "author": "zrsca",
                  "text": "Yeah I figured that‚Äòd make it difficult. You could add a Filter option for how many GB the user wants to buy and use the according pricing they provide. But I think that‚Äòd be a pain in the a** to implement let alone maintain.",
                  "score": 1,
                  "created_utc": "2026-02-01 20:42:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o31o11p",
          "author": "illusivejosiah",
          "text": "Really well built. I run Illusory (mobile proxy provider) and I actually found this useful. Didn't realize we're the only service with dedicated IPs, IPv6, UDP, and unlimited bandwidth all-in-one. Appreciate the insight",
          "score": 3,
          "created_utc": "2026-02-01 21:47:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32bmgj",
              "author": "Consistent-Feed-7323",
              "text": "Thank you! The idea actually came when I was helping with marker research for my friend's proxy service and there was no place with such information.",
              "score": 1,
              "created_utc": "2026-02-01 23:50:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o32rzdg",
                  "author": "illusivejosiah",
                  "text": "Makes sense. Thanks for adding us!",
                  "score": 1,
                  "created_utc": "2026-02-02 01:21:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xjubp",
          "author": "SilentlySufferingZ",
          "text": "Which require KYC? Accept Bitcoin?",
          "score": 2,
          "created_utc": "2026-02-01 06:59:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ywymi",
              "author": "scrapingtryhard",
              "text": "proxyon",
              "score": 1,
              "created_utc": "2026-02-01 13:51:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o33cdcn",
          "author": "FranBattan",
          "text": "This is gold man, really appreciate it. Thanks",
          "score": 2,
          "created_utc": "2026-02-02 03:18:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t6bbc",
          "author": "akashpanda29",
          "text": "This is nice one. Kudos dude",
          "score": 1,
          "created_utc": "2026-01-31 16:17:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2t7gdm",
              "author": "Consistent-Feed-7323",
              "text": "Thank you very much, right now doing a round of \"marketing\", than will grab some more providers and start testing them",
              "score": 3,
              "created_utc": "2026-01-31 16:23:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2t9jt4",
          "author": "Fragrant_Ad3054",
          "text": "I really like the font used.",
          "score": 1,
          "created_utc": "2026-01-31 16:33:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2tbzmo",
              "author": "Consistent-Feed-7323",
              "text": "Thank you, it's monospace, I'm used to use it in my projects although it's not the best practice and is less readable than default.",
              "score": 1,
              "created_utc": "2026-01-31 16:44:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tg2n6",
          "author": "Alarmed_Scar_925",
          "text": "Thank you!!",
          "score": 1,
          "created_utc": "2026-01-31 17:04:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2v630c",
          "author": "Sergiowild",
          "text": "Good idea! Did you put your ref links on there?",
          "score": 1,
          "created_utc": "2026-01-31 22:03:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vq8af",
              "author": "Consistent-Feed-7323",
              "text": "Thank you. Not yet, but will do in some future. Ref links are requiring registration in most services and it's such a headache.",
              "score": 1,
              "created_utc": "2026-01-31 23:51:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2xxdz5",
          "author": "Mean_Yam5488",
          "text": "Nice work on the directory. Been looking for something like this when comparing providers.\n\nOne thing that would be super helpful - a filter for whether they allow account logins. Some providers block you from logging into social media or other accounts with their IPs, which kills a lot of use cases.",
          "score": 1,
          "created_utc": "2026-02-01 09:03:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y739u",
              "author": "Consistent-Feed-7323",
              "text": "Thank you! I do have similar to this info in restrictions, but many providers are either not disclosing it or referring as \"and other restrictions\" which can't be verified until testing. It's definitely something worth a separate filter, just can't make it right now.",
              "score": 1,
              "created_utc": "2026-02-01 10:32:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qttig8",
      "title": "How are you using AI to help build scrapers?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qttig8/how_are_you_using_ai_to_help_build_scrapers/",
      "author": "lieutenant_lowercase",
      "created_utc": "2026-02-02 12:18:06",
      "score": 15,
      "num_comments": 19,
      "upvote_ratio": 0.81,
      "text": "I use Claude Code for a lot of my programming but doesn't seem particularily useful when I'm writing web scrapers. I still have to load up the site, go to dev tools, inspect all the requests, find the private API's, figure out headers / cookies, check if its protected by Cloudflare / Akamai etc.. Perhaps once I have that I can dump all my learnings into claude code with some scaffolding at get it to write the scraper, but its still quite painful to do. My major time sink is understanding the structure of the site/app and its protections rather than writing the actual code. \n\nI'm not talking about using AI to parse websites, thats the easy bit tbh. I'm talking about the actual code generation. Do people give their LLM's access to the browser and let it figure it out? Anything else you guys are doing?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qttig8/how_are_you_using_ai_to_help_build_scrapers/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o363xue",
          "author": "somedude4949",
          "text": "Pass har file with requests I need to use give custom prompt on how to built and voila after few minutes everything working and integrate it depends on my use case",
          "score": 3,
          "created_utc": "2026-02-02 15:30:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35d5s1",
          "author": "sjmanzur",
          "text": "I started using antigravity with playwright and it‚Äôs a game changer really",
          "score": 4,
          "created_utc": "2026-02-02 13:05:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35jpsl",
          "author": "balletpaths",
          "text": "I point it to a URL, give a sample code format and let it rip! Then I adjust and make minor modifications.",
          "score": 2,
          "created_utc": "2026-02-02 13:44:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hy7di",
          "author": "Forsaken_Lie_8606",
          "text": "honestly the biggest win for me was using ai to generate the initial selector logic and then manually tweaking it. saves tons of time on boilerplate",
          "score": 2,
          "created_utc": "2026-02-04 08:36:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35e014",
          "author": "No-Appointment9068",
          "text": "I sometimes download the page source, set up a test for the output I want and then let AI have a crack at getting the selectors correct, they often produce quite brittle selectors but it's very easy to then fix with the same process.",
          "score": 3,
          "created_utc": "2026-02-02 13:10:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35reg9",
              "author": "jwrzyte",
              "text": "this - almost all my parsing code is generated by copilot, then i can test against it within pytest & scrapy. and make any changes as needed",
              "score": 1,
              "created_utc": "2026-02-02 14:26:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36cmpi",
                  "author": "No-Appointment9068",
                  "text": "A little tip! Chrome dev tools let's you copy selectors if you right click the element within the inspect view, for one off scripts I just use those. I Don't bother getting any fancier with AI.",
                  "score": 3,
                  "created_utc": "2026-02-02 16:11:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36mtp6",
          "author": "Tharnwell",
          "text": "Following. I'm currently building a web platform almost entirely with AI. Development and content creation are fully automated.\n\nThe only part AI still struggles with in my workflow is sourcing images from the web. I‚Äôm aware of copyright concerns, but in my specific use case this isn‚Äôt a major issue.\n\nWhile AI can generate images, they don‚Äôt work well for my needs.",
          "score": 1,
          "created_utc": "2026-02-02 16:57:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o374yiq",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-02 18:20:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3aco0k",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-03 04:25:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37jkzh",
          "author": "builderbycuriosity",
          "text": "Give your Claude code access to MCP servers like Playwright, which can automate the browser. It may not be perfect, but it will do the job.",
          "score": 1,
          "created_utc": "2026-02-02 19:27:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38sqni",
          "author": "orthogonal-ghost",
          "text": "I've thought about this problem a lot. The main challenge as you've noted is given the coding agent the proper context (HTML, network requests, javascript, etc.).  \n  \nTo address this, we built a specialized agent to programmatically \"inspect\" a web site for that context and to generate a Python script to scrape it. With that comes its own share of challenges (e.g., naively passing in all the HTML on a given web page can very quickly eat up an LLM's context), but we've found that it's been quite successful in building scrapers once it has the right things to look at.",
          "score": 1,
          "created_utc": "2026-02-02 23:04:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39eeeu",
          "author": "xRazar",
          "text": "I had a lot of success using Agent-Browser with Skills to integrate it into the models into OpenCode. The agents scan through the site trying to find public APIs if that fails it goes back to classic scraping.",
          "score": 1,
          "created_utc": "2026-02-03 01:03:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39j6il",
          "author": "calimovetips",
          "text": "i mostly use ai after i‚Äôve mapped the network calls. it‚Äôs good for turning notes into clean request code, retries, backoff, and a sane pipeline. the hard part is still modeling sessions and state, plus deciding what‚Äôs stable. are you scraping mostly xhr/json endpoints or full browser flows?",
          "score": 1,
          "created_utc": "2026-02-03 01:30:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3b2w3c",
              "author": "lieutenant_lowercase",
              "text": "XHR endpoints where available but fall back to full browser if i need",
              "score": 1,
              "created_utc": "2026-02-03 07:58:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3cphek",
          "author": "Hundreds-Of-Beavers",
          "text": "Built a Playwright agent to help with this - we gave it access to both a live browser session and a Typescript environment, so it can inspect the DOM, then write & execute Playwright code to test out the implementation against the browser.  And gave it tools for data extraction/screenshots/etc.\n\nBasically, our approach is we let the LLM do the majority of the work (and give it the tools to do so), but can then go in and troubleshoot the scraper as necessary",
          "score": 1,
          "created_utc": "2026-02-03 15:09:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3m79m3",
          "author": "Imaginary_Gate_698",
          "text": " AI helps more after you‚Äôve already done the hard part. We use it to scaffold Scrapy spiders, normalize responses, write retry and parsing glue, that kind of stuff. The discovery phase you described, mapping requests, session behavior, which calls actually matter, is still very manual and very site specific.\n\nGiving an LLM a browser sounds nice, but in practice it‚Äôs slow and it misses why things break under volume. It won‚Äôt notice session churn patterns, subtle header dependencies, or why a flow works once and then degrades. Where it‚Äôs been useful is once you‚Äôve identified the right endpoints, you can dump a clean HAR or request samples in and let it generate a first pass, then you tune from there. The real time sink is still understanding how the app behaves over time, not writing the code.",
          "score": 1,
          "created_utc": "2026-02-04 22:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35jwhf",
          "author": "AdministrativeHost15",
          "text": "Have the AI generate the scraping script. Don't code review it or try to fix it. Just have a test that determines if it returns any useful data or not. If it doesn't have the AI regenerate it.",
          "score": 0,
          "created_utc": "2026-02-02 13:45:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqxam9",
      "title": "Do I need a residential proxy to mass scrape menus?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qqxam9/do_i_need_a_residential_proxy_to_mass_scrape_menus/",
      "author": "z420a",
      "created_utc": "2026-01-30 05:58:34",
      "score": 12,
      "num_comments": 12,
      "upvote_ratio": 0.89,
      "text": "I have about 30,000 restaurants for which I need to scrape their menus. As far as I know a good chunk of those use services such as uber eats, DoorDash, toasttab, etc to host their menus. \n\nIs it possible to scrape all of that with just my laptop? Or will I get IP banned?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qqxam9/do_i_need_a_residential_proxy_to_mass_scrape_menus/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2k1su0",
          "author": "Puzzleheaded_Row3877",
          "text": "with uber eats you get upto 2000 requests per hour ,I don't think the rest of the sites you mentioned care about rate limiting that much ; and if they do , mix it up by scraping some of the menus from the wayback machine.",
          "score": 9,
          "created_utc": "2026-01-30 06:08:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kihz0",
          "author": "Bosschopper",
          "text": "Can‚Äôt answer you question but I‚Äôm working on a similar project. Are you doing a menu database? My project is pretty far along the dev pipeline and I‚Äôm hosted on the internet (well, sorta) so if you‚Äôre open to collaborating hit me up.\n\nI‚Äôm far along with UI stuff and features but frankly have no data to feed. DM me about where you‚Äôre at too",
          "score": 3,
          "created_utc": "2026-01-30 08:28:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lxvby",
          "author": "Pooria_P",
          "text": "Don't scrape all with your laptop if you'll be doing it at once. if the websites are not sophisticated, just use the cheapest proxy you can find (typically, static datacenter, but HIGH block rates), or static residentials.  \nUber eats and etc though, I think you have a better chance with rotating residential.",
          "score": 3,
          "created_utc": "2026-01-30 14:35:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mbao3",
          "author": "bluemangodub",
          "text": "Do you? try it. And if you do, you will find out.\n\nThat's it. No one knows, unless they have done it. You can guess. \n\nBut typically, if you are scraping 1 site, and you want 30k pages, if you want to do it slow you might get away with it. Otherwise, proxies allow you to hit it harder.",
          "score": 2,
          "created_utc": "2026-01-30 15:39:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2k7x8c",
          "author": "bigtakeoff",
          "text": "naw... guess it depends on where the menus are at",
          "score": 1,
          "created_utc": "2026-01-30 06:56:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lncto",
          "author": "HaphazardlyOrganized",
          "text": "Probably not, just don't pound their servers. If you check like once a day at 2am you should be fine",
          "score": 1,
          "created_utc": "2026-01-30 13:41:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs08kg",
      "title": "I upgraded my YouTube data tool ‚Äî (much faster + simpler API)",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qs08kg/i_upgraded_my_youtube_data_tool_much_faster/",
      "author": "nagmee",
      "created_utc": "2026-01-31 11:43:24",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 0.73,
      "text": "A few months ago I shared my Python tool for fetching YouTube data. After feedback, I refactored everything and added some features with 2.0 version.\n\nHere's the new features:\n\n* Get structured comments alongside with transcript and metadata.\n* `ytfetcher`¬†is now fully synchronous, simplifying usage and architecture.\n* Pre-Filter videos based on metadata such as¬†`view_count`,¬†`duration`¬†and¬†`title`.\n* Fetch data with playlist id or search query to similar to Youtube Search Bar.\n* Simpler CLI usage.\n\nI also solved a very critical bug with this version which is metadata and transcripts are might not be aligned properly.\n\nI still have a lot of futures to add. So if you guys have any suggestions I'd love to hear.\n\nHere's the full changelog if you want to check;¬†\n\n[https://github.com/kaya70875/ytfetcher/releases/tag/v2.0](https://github.com/kaya70875/ytfetcher/releases/tag/v2.0)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qs08kg/i_upgraded_my_youtube_data_tool_much_faster/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qrdn9r",
      "title": "Need help",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qrdn9r/need_help/",
      "author": "imvdave",
      "created_utc": "2026-01-30 18:27:57",
      "score": 8,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "I have a list of 2M+ online stores for which I want to detect the technology.\n\n  \nI have the script, but I often face 429 errors due to many websites belonging to Shopify.\n\n  \nIs there any way to speed this up?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qrdn9r/need_help/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2nkqci",
          "author": "scraperouter-com",
          "text": "use rotating proxies",
          "score": 2,
          "created_utc": "2026-01-30 19:00:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nsf8c",
          "author": "Puzzleheaded_Row3877",
          "text": "rotate the IP's. Also organize your list so that you are not hitting shopify 50 times in a row.",
          "score": 2,
          "created_utc": "2026-01-30 19:35:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35dhmu",
          "author": "ScrapeAlchemist",
          "text": "Hey! At 2M+ URLs you're going to need a large pool of rotating proxies - the other comments are right about that. Depending on how fast you want to go, you might need 100-1000+ IPs to avoid getting rate-limited by Shopify's shared infrastructure.\n\nAlso shuffle your list so you're not hammering Shopify back-to-back - interleave domains by provider.\n\nFor Shopify detection specifically - try hitting `/products.json` on each domain. Shopify stores expose this endpoint by default, so a 200 response with valid JSON is a quick confirm without parsing HTML. Same idea for other platforms that have predictable endpoints.\n\nGood luck!",
          "score": 2,
          "created_utc": "2026-02-02 13:07:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nrwgu",
          "author": "greg-randall",
          "text": "Can you do a DNS lookup on your domains and build a list of Shopify owned IPs?",
          "score": 1,
          "created_utc": "2026-01-30 19:32:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nzkav",
          "author": "NZRedditUser",
          "text": "well if you get a 429 (if you dont wanna solve the proxy issue) just check where the redirect goes if you do domain/admin if it goes -> x myshopify com then you know its shopify and can make assessments via that?",
          "score": 1,
          "created_utc": "2026-01-30 20:08:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oczsh",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-30 21:12:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2odeps",
              "author": "webscraping-ModTeam",
              "text": "üëî Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
              "score": 1,
              "created_utc": "2026-01-30 21:14:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37jmfw",
          "author": "scorpiock",
          "text": "Two things you could try:\n\n1. Slow down so you don't hit the rate limit  \n2. Rotating proxies",
          "score": 1,
          "created_utc": "2026-02-02 19:27:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qt7pju",
      "title": "litecrawl - minimal async crawler for targeted, incremental scraping",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qt7pju/litecrawl_minimal_async_crawler_for_targeted/",
      "author": "Little_Ant_3459",
      "created_utc": "2026-02-01 19:14:58",
      "score": 7,
      "num_comments": 4,
      "upvote_ratio": 0.9,
      "text": "I kept hitting the same pattern at work: \"we need to index this specific section of this website, with these rules, on this schedule.\" Each case was slightly different - different URL patterns, different update frequencies, different extraction logic.\n\nScrapy felt like overkill. I didn't need a framework with spiders and pipelines and middleware. I needed a tool I could call with parameters and forget about.\n\nSo I built litecrawl: one async function that manages its own state in SQLite.\n\nThe idea is you spin up a separate instance per use case. Each gets its own DB file, its own cron job, its own config. No orchestration, no shared state, no central scheduler. Just isolated, idempotent processes that pick up where they left off.\n\n    from litecrawl import litecrawl\n    \n    litecrawl(\n        sqlite_path=\"council.db\",\n        start_urls=[\"https://example.com/minutes\"],\n        include_patterns=[r\"https://example\\.com/minutes/\\d+\"],\n        n_concurrent=5,\n        fresh_factor=0.5\n    )\n\nIt handles the boring-but-important stuff:\n\n* Adaptive scheduling - backs off for static pages, speeds up for frequently changing content\n* Crash recovery - claims pages with row-level locking, releases stalled jobs automatically\n* Content hashing - only flags pages as \"fresh\" when something actually changed\n* SSRF protection - validates all resolved IPs, not just the first one\n* robots.txt - cached per domain with async fetching\n* Downloads - catches PDFs/ZIPs that trigger downloads instead of navigation\n\nDesigned to run via cron wrapped in `timeout`. If it crashes or gets killed, the next run continues where it left off.\n\n`pip install litecrawl`\n\nGitHub: [https://github.com/jakobmwang/litecrawl](https://github.com/jakobmwang/litecrawl)\n\nBuilt this for various data projects. Would love feedback - especially if you spot edge cases I haven't considered.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qt7pju/litecrawl_minimal_async_crawler_for_targeted/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o34s1j6",
          "author": "WindScripter",
          "text": "this looks awesome. gonna give it a try and let you know how it works, i generally have a similar use case as you",
          "score": 1,
          "created_utc": "2026-02-02 10:17:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36c9no",
              "author": "Little_Ant_3459",
              "text": "Cool! Looking forward to hear from you.",
              "score": 1,
              "created_utc": "2026-02-02 16:09:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o34y20u",
          "author": "renegat0x0",
          "text": " \\- I am not an expert, but it looks as if it used \"functional\" programming approach, but where everything is in a one file. I don't like projects where everything is in one file\n\n \\- was it vibe coded? I do not really see any relevant commit history",
          "score": 1,
          "created_utc": "2026-02-02 11:12:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36d4c9",
              "author": "Little_Ant_3459",
              "text": "A valid criticism. I am beginning to get use cases where a few link-related add-ons would be handy. When I get around to add that functionality, I should split it into more than one file for readability.\n\nAnd while I use LLM's intensively in most of my work, this one I actually hand-held more than usual, since they didn't understand the paradigm/approach.",
              "score": 2,
              "created_utc": "2026-02-02 16:13:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qtv2ob",
      "title": "How to scrape Instagram followers/followings in chronological order?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qtv2ob/how_to_scrape_instagram_followersfollowings_in/",
      "author": "Any_Independent375",
      "created_utc": "2026-02-02 13:30:16",
      "score": 6,
      "num_comments": 10,
      "upvote_ratio": 0.88,
      "text": "Hi everyone,\n\nI‚Äôm trying to understand how some websites are able to show Instagram followers or followings in **chronological order** for **public accounts**.\n\nI already looked into this:\n\n* When opening the followers/following popup on Instagram, the list is **not** shown in chronological order.\n* The web request [https://www.instagram.com/api/v1/friendships/{USER\\_ID}/following/?count=12](https://www.instagram.com/api/v1/friendships/{USER_ID}/following/?count=12) returns users in **exactly the same order** as shown in the popup, which again is **not chronological**.\n* The response does **not** include any obvious timestamp like followed\\_at, nor an incrementing ID that would allow sorting by time.\n\nI‚Äôm interested in **how this is technically possible at all**.\n\nAny insights from people who have looked into this would be really appreciated. \n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qtv2ob/how_to_scrape_instagram_followersfollowings_in/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o3e1btn",
          "author": "Forsaken_Lie_8606",
          "text": "so i was trying to do something similar a while back and i stumbled upon this thing where if you use the instagram graph api, you can get the followers in a specific order, but its not exactly chronological, its more like a ranked order based on teh users interaction with the account, but if you combine that with the users profile info, like when they joined instagram, you can kinda sort it out, ngl its a lot of work and not very accurate, but i did manage to get a list of around 80% of the followers in the correct order, tbh i dont know if its worth the effort tho, lol",
          "score": 2,
          "created_utc": "2026-02-03 18:50:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36fmsn",
          "author": "HLCYSWAP",
          "text": "on the assumption you‚Äôve truly exhausted all web api: they are getting different data via the mobile API, stuffing the headers of the request so they get more data than intended, or simply lying ü§•\n\nedit: could also be scraping daily and adding in new accounts via falsified timestamps but correct date",
          "score": 1,
          "created_utc": "2026-02-02 16:24:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38tryl",
              "author": "Any_Independent375",
              "text": "I don't think they are lying or scraping random accounts every day. I created an account, followed 6 profiles and the websites were able to list all followings in chronological order. There must be a way to achieve this.\n\nCould you clarify what you mean by mobile API and how to access it?",
              "score": 1,
              "created_utc": "2026-02-02 23:10:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o38z77j",
                  "author": "HLCYSWAP",
                  "text": "are they able to do this for any account at any moment? if so, they have access to APIs (likely mobile) that are exposing more data. is it for specific accounts only? if so, might just be a notification scraper.\n\n\n\nthe mobile app hits different endpoints than web. youre going to need to run the mobile traffic through a network request sniffer ala burp suite, http toolkit, etc. but these apps use certificate pinning meaning if you use the burp CA cert to sniff traffic itll deny you access to any traffic on the app so youll need a rooted/jailbroken mobile or an emulator with the ability to unpin certs ala frida/objection/manual APK patching with as old a build of instragram as you can get.",
                  "score": 1,
                  "created_utc": "2026-02-02 23:39:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36iopj",
          "author": "Hour_Analyst_7765",
          "text": "It could be they are checking over and over for months to years.\n\nWithout ground truth its also hard to check if any site is 100% accurate too.",
          "score": 1,
          "created_utc": "2026-02-02 16:38:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38teuw",
              "author": "Any_Independent375",
              "text": "I created a fresh Instagram account, followed 6 profiles and then checked the followers/followings with 3 different websites. They were able to list the followers/followings in chronological order. \n\nI also tried a bunch of other profiles and it was accurate.\n\nThere must be a way to achieve this.",
              "score": 2,
              "created_utc": "2026-02-02 23:08:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3bjvy7",
                  "author": "Hour_Analyst_7765",
                  "text": "Well, like I said.. create a difference list from the previously known data and then add these events with a timestamp of the scrape event (if the site doesn't report it). The accuracy of the order then depends on how often they can scrape the list.\n\nYou could try follow some accounts, but do it in different orders from multiple of your own accounts.\n\nI presume it takes a while to refresh the data, so if you follow with different order it should either confirm whether its invisible or not to this site. If it still has perfect correlation then that suggests they are getting the data from somewhere else.\n\nI apply a similar technique for scraping articles from sites. I will look for a post & modify dates in the <meta> tags, search engine JSON or a custom parser from the HTML. If there is nothing found, then I will default the date tag to the time of the page load, which has limited value but at least its timestamped. I also need these dates for orderings and the like.",
                  "score": 1,
                  "created_utc": "2026-02-03 10:42:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o377pbv",
          "author": "Affectionate_Cold209",
          "text": "Doesn't it needs proxy?\n\n(completely a question not the answer your are intereted in)",
          "score": 1,
          "created_utc": "2026-02-02 18:33:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3cd3nn",
          "author": "janalpadi",
          "text": "I‚Äôm currently expanding the scraper to include bios, follower counts, and following counts for each profile. Also, is it still possible to sort by chronological order on the Instagram mobile app? I haven‚Äôt used the app in ages since I‚Äôve been focusing on the web version for scraping. Let me know if you manage to figure that out",
          "score": 1,
          "created_utc": "2026-02-03 14:04:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrdd1y",
      "title": "Asking for advice and tips.",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qrdd1y/asking_for_advice_and_tips/",
      "author": "LowDiscount6694",
      "created_utc": "2026-01-30 18:18:10",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 0.73,
      "text": "Context: former software engineer and data analyst.\n\nGood morning to all of my master,\n\nI would like to seek an advice how to make become a better web scraper. I am using python selenium web scraping, pandas for data manipulation and third party vendor. I am not comfortable to my scraping skills I used to create a scraping in first quarter of last year. And currently I've been able to apply to a company. Since they hiring for web scraping engineer. I am confident that I will passed the exercises. Since I got the asking data. Now, what do I need to make my scraping become undetectable? I used the residential proxies provided Also the captcha bypass. I just wanted to learn how to apply the fingerprinting and etc. because I wanted to got hired so I can pay house bills. :( anything advice that you want to share.\n\nThank you for listening to me.",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qrdd1y/asking_for_advice_and_tips/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2nesmn",
          "author": "Active_Winner",
          "text": "Use playwright stealth, seleniumbase and drissionpage",
          "score": 1,
          "created_utc": "2026-01-30 18:34:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p1dg4",
          "author": "johnster929",
          "text": "Zendriver kept me under the radar, no need for proxy IP service",
          "score": 1,
          "created_utc": "2026-01-30 23:12:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsmo66",
      "title": "Monthly Self-Promotion - February 2026",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qsmo66/monthly_selfpromotion_february_2026/",
      "author": "AutoModerator",
      "created_utc": "2026-02-01 03:00:42",
      "score": 5,
      "num_comments": 11,
      "upvote_ratio": 0.86,
      "text": "Hello and howdy, digital miners of¬†r/webscraping!\n\nThe moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!\n\n* Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?\n* Maybe you've got a ground-breaking product in need of some intrepid testers?\n* Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?\n* Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?\n\nWell, this is your time to shine and shout from the digital rooftops - Welcome to your haven!\n\nJust a friendly reminder, we like to keep all our self-promotion in one handy place, so any promotional posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qsmo66/monthly_selfpromotion_february_2026/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2x5kjy",
          "author": "HLCYSWAP",
          "text": "ETL (scraping), databasing, ML training AI (captcha bypass, voice, transformers - LLMs), standard desktop apps like DAWs as seen on my github. my past successes here:\n\n[https://github.com/matthew-fornear](https://github.com/matthew-fornear)\n\nopen to short or long work. you can reach me via reddit, x (https://www.x.com/fixitorgotojail) or discord",
          "score": 4,
          "created_utc": "2026-02-01 05:05:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31w4uw",
          "author": "malvads",
          "text": "Hi! After some time, researching into complex crawlers for web-to-llm data, I created [https://github.com/malvads/mojo](https://github.com/malvads/mojo). Mojo is an extremely fast C++ web scraper with multi-depth capabilities ready to ingest data into RAG-like systems, it scans entire websites and converts them to Markdown format. It also downloads artifacts like PDFs/others . It‚Äôs incredibly fast, so it can run on AWS Lambdas or any Cloud Provider, and it supports rendering (via Chrome CDP, if --render flag). Additionally, it has an internal reverse proxy with proxy rotation to facilitate scraping (when CDP, to not relaunch chrome instances, this is not for normal HTTP reqs). Mojo can scrape full websites in seconds while using very low CPU and RAM. Precompiled binaries are also available.",
          "score": 2,
          "created_utc": "2026-02-01 22:27:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3079iu",
          "author": "LessBadger4273",
          "text": "Giving out 100M+ Shopee and Aliexpress product page dataset for free for educational purposes. Reach out to Octaprice.com to learn how to get it",
          "score": 1,
          "created_utc": "2026-02-01 17:39:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34x6ou",
          "author": "Dear-Cable-5339",
          "text": "If you‚Äôre tired of managing proxy pools and fighting blocks,¬†**Crawlbase Smart Proxy**¬†might help.\n\nIt lets you fetch public pages at scale while we handle IP rotation, retries, geo-targeting, and CAPTCHAs in the background.\n\n**Simple pricing:**  \n‚Ä¢ TCP / non-JS request =¬†**1 credit**  \n‚Ä¢ JavaScript-rendered request =¬†**2 credits**\n\nFree credits available if you want to test it out:  \nüëâ¬†[https://crawlbase.com/?s=5qGcKLCR](https://crawlbase.com/?s=5qGcKLCR)",
          "score": 1,
          "created_utc": "2026-02-02 11:04:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34ynae",
          "author": "renegat0x0",
          "text": "I am still scraping Internet links meta.\n\n[https://github.com/rumca-js/Internet-Places-Database](https://github.com/rumca-js/Internet-Places-Database)",
          "score": 1,
          "created_utc": "2026-02-02 11:17:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38aiog",
          "author": "PyScrapePro",
          "text": "Hello everyone üëã  \nI‚Äôm a **Python Web Scraping & Backend Engineer** specializing in **production-grade scraping systems** and data pipelines.\n\nI work mostly on projects where scraping is not a one-off script, but a **system that needs to run reliably under real constraints**.\n\n**What I focus on:**  \nüõ°Ô∏è scraping protected and JS-heavy websites (Cloudflare, modern anti-bot systems)  \n‚ö° high-performance pipelines (asyncio + multiprocessing, 5√ó speedups)  \nüìä stability and observability (95‚Äì99% successful runs, retries, graceful degradation)  \nüîÅ incremental updates and cost-aware scraping  \nüß† backend APIs around scraped data (FastAPI, auth, rate limits, PostgreSQL)\n\nMost of my recent work is under **NDA**, so I can‚Äôt share code or client names, but I *can* discuss:\n\n* architecture decisions\n* performance trade-offs\n* reliability strategies\n* long-term maintenance of scraping systems\n\nI also document my engineering approach and long-form technical articles here:  \nüëâ [https://pyscrapepro.netlify.app/](https://pyscrapepro.netlify.app/)\n\n**What I‚Äôm looking for:**\n\n* high-impact, **well-paid remote projects**\n* long-term contracts or full-time roles\n* teams that treat scraping as engineering, not ‚Äúquick scripts‚Äù\n\nIf you‚Äôre building or maintaining **serious scraping infrastructure** and need someone who cares about **stability, performance, and ownership**, feel free to reach out or comment here.\n\nHappy scraping üöÄ",
          "score": 1,
          "created_utc": "2026-02-02 21:34:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3buav9",
          "author": "SatisfactionClear654",
          "text": "Since it's a new project, our IP pool is still fresh and clean. Plus, we're offering Web Unlocking for free right now!\n\nAnyone looking to scrape data? Feel free to try it out and share your feedback!\n\n[https://cafescraper.com/](https://cafescraper.com/)",
          "score": 1,
          "created_utc": "2026-02-03 12:08:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ccsys",
          "author": "woldhack",
          "text": "Hello :)\nBuilt guerrillamail-client-rs ‚Äì an async Rust client for GuerrillaMail's temporary email service.\n\nUseful for scraping and automation setups where sign-ups block access. Makes it easy to deal with email verification, test registration flows, or spin up temp accounts without using real mail accounts.\n\nFeatures:\n- Async API built on reqwest\n- Create disposable emails\n- Poll inboxes for new messages\n- Fetch full email bodies\n- Download attachments\n\nCurrently in beta, API is pretty much done. Open to feedback before pushing 1.0.\nGitHub: https://github.com/11philip22/guerrillamail-client-rs",
          "score": 1,
          "created_utc": "2026-02-03 14:03:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrw98m",
      "title": "Data Scraping - What to use?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qrw98m/data_scraping_what_to_use/",
      "author": "Fabulous_Variety_256",
      "created_utc": "2026-01-31 07:47:26",
      "score": 4,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "My tech stack - NextJS 16, Typescript, Prisma 7, Postgres, Zod 4, RHF, Tailwindcss, ShadCN, Better-Auth, Resend, Vercel\n\nI'm working on a project to add to my cv. It shows data for gaming - matches, teams, games, leagues etc and also I provide predictions.\n\nMy goal is to get into my first job as a junior full stack web developer.\n\nI‚Äôm not done yet, I have at least 2 months to work on this project.\n\n\n\nThe thing is - I have another thing to do.\n\nI need to scrape data from another site. I want to get all the matches, the teams etc.\n\nWhen I enter a match there, it will not load everything. It will start loading the match details one by one when I'm scrolling.\n\n\n\nHow should I do it:\n\n\n\nIn the same project I'm building?\n\n\n\nIn a different project?\n\n\n\nIf 2, maybe I should show that I can handle another technologies besides next?:\n\n\n\nShould I do it with NextJS also\n\n\n\nShould I do it with NodeJS+Express?\n\n\n\nAnything else?\n\n\n\n\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qrw98m/data_scraping_what_to_use/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2rmxc0",
          "author": "hikingsticks",
          "text": "It sounds like you need to set up a headless browser based web scraper to get the data you need, then process it and stick it in a database.\n\nWhere are you deploying it? If you're using a VPS, consider one docker container running the database, one running the API to serve up the data, and one that gets started up periodically to run the scraper and insert the data into the database.\n\nPlaywright is a common choice for headless scraping, it can be done with javascript.",
          "score": 7,
          "created_utc": "2026-01-31 10:14:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3744q5",
          "author": "hasdata_com",
          "text": "Separate it. Definitely.\n\nRegarding the library, since the target site has infinite scroll, you need a headless browser like Puppeteer or Playwright (easier for beginners).",
          "score": 9,
          "created_utc": "2026-02-02 18:16:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sau1y",
          "author": "ketopraktanjungduren",
          "text": "If you're on NodeJS then use Playwright.¬†\n\n\nI'm on Python, and I use requests other tha Playwright. Maybe you also need a library that's like requests in python\n\n\nThese two are more than enough to start with",
          "score": 1,
          "created_utc": "2026-01-31 13:28:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tuqra",
          "author": "RandomPantsAppear",
          "text": "You‚Äôre going to find a lot more support for scraping related activities in Python, not JavaScript. \n\nPython is the language of choice for data processing and analysis, so it‚Äôs also the language of choice for acquisition.",
          "score": 1,
          "created_utc": "2026-01-31 18:14:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3503qu",
          "author": "ScrapeAlchemist",
          "text": "Hi,\n\nFor lazy-loading sites (content loads on scroll), you'll need browser automation since the data is rendered via JavaScript.\n\n**Tech choice:**\nSince you're already using NextJS, I'd suggest a **separate Node.js project** for the scraper. It keeps concerns separated and shows you can work outside the Next ecosystem ‚Äî good for CV. You can run it on a schedule and push data to your Postgres DB.\n\n**Tools for JS-rendered pages:**\n- **Playwright** ‚Äî modern, fast, great for scrolling/waiting for content\n- **Puppeteer** ‚Äî also solid, works well\n\n**Basic approach:**\n1. Load page with Playwright\n2. Scroll to trigger lazy loading (`page.evaluate(() => window.scrollBy(0, 1000))`)\n3. Wait for new content (`page.waitForSelector`)\n4. Extract data\n5. Repeat until all content loaded\n\n**Example pattern:**\n```javascript\nwhile (hasMoreContent) {\n  await page.evaluate(() => window.scrollTo(0, document.body.scrollHeight));\n  await page.waitForTimeout(1000);\n  // check if new items appeared\n}\n```\n\nGood luck with the project!",
          "score": 1,
          "created_utc": "2026-02-02 11:30:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38qkws",
          "author": "somedude4949",
          "text": "Before you use the browser automation , playwright or puppeteer make to try find if there any Public endpoints you can fetch directly with some minor adjustments",
          "score": 1,
          "created_utc": "2026-02-02 22:53:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eqyce",
          "author": "Forsaken_Lie_8606",
          "text": "ime ive worked on a similar project before and i found that using a headless browser like puppeteer was way more effective for scraping data than traditional methods, especially when dealing with dynamic content that loads on scroll, ngl it was a game changer for me, i was able to scrape all the match details and teams in like a fraction of the time it wouldve taken otherwise, imo its definitely worth considering, especially since youre trying to showcase your skills as a junior dev, maybe look into it and see if its a good fit for your project",
          "score": 1,
          "created_utc": "2026-02-03 20:50:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qurakm",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qurakm/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-02-03 13:00:35",
      "score": 4,
      "num_comments": 2,
      "upvote_ratio": 0.84,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levels‚Äîwhether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) üå±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1qurakm/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o3iswjr",
          "author": "Ok-Tower-7808",
          "text": "Best Way to Scrape LinkedIn for Thesis Research?\n\n  \nHi everyone!\n\nI‚Äôm currently working on my thesis, where I‚Äôm researching how a specific master‚Äôs degree is valued on the labour market.\n\nTo do this, I‚Äôm looking to gather data from LinkedIn‚Äîspecifically employment status, job titles, and ideally get their tags and the ‚Äúabout‚Äù section used by people who hold this degree.\n\nDoes anyone know of any good tools or methods to scrape this kind of data from LinkedIn? Any advice or experience would be greatly appreciated!\n\nThanks in advance üòä\n\n¬†",
          "score": 2,
          "created_utc": "2026-02-04 12:54:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3k16rm",
              "author": "Fragrant_Ad3054",
              "text": "Hello,\n\nThere are online services available that often allow you, using a Chrome extension for example, to save and export all sorts of data, including the data you're looking for.\n\nThese services are usually paid, and you pay based on the amount of information you export using the extension.\n\nHow much data would you need approximately (in thousands)? This question is to guide you based on your answer, not to sell a product I created or a product for which I receive a commission (I thought I'd mention it, haha).\n\nEdit: I advise against creating your own program to do this because, having done it a few years ago, it takes a lot of time and money. It's best to use existing free or paid tools; this will save you considerable time and money, unless the program is specifically designed for your thesis.",
              "score": 1,
              "created_utc": "2026-02-04 16:39:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1quoftu",
      "title": "HTML parser to query on computed CSS rather than class selectors",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1quoftu/html_parser_to_query_on_computed_css_rather_than/",
      "author": "Hour_Analyst_7765",
      "created_utc": "2026-02-03 10:26:54",
      "score": 3,
      "num_comments": 8,
      "upvote_ratio": 0.81,
      "text": "Some websites try to obfuscate HTML DOM by changing CSS class names to random gibberish, but also move CSS modifiers all around.\n\nFor example, I have 1 site that prints some data with <b> to create bold text, but with a page load they generate several nested divs which each get a random CSS class, some of them containing bullshit modifications, and then set the bold font that way. And F5, you're right, the DOM changed again.\n\nSo basically, I need a HTML DOM parser that folds all these CSS classes together and makes CSS properties accessible. Much alike the \"Computed\" tab in the element inspector of a browser. If I can then write a tree selector query for these properties, then I think I'm golden.\n\nI'm using C# by the way. I've looked at AngleSharp with its CSS extension, but it actually crashes on this HTML DOM when trying to \"Render\" the website. It may perhaps be fixable but I'm interested in hearing other suggestions, because I'm certainly not the only one with this issue.\n\nI'm open to libraries from other languages, although, I haven't tried using them so far for this site.\n\nI'm not that interested in AI or Playwright/headless browser solutions, because of overhead costs.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1quoftu/html_parser_to_query_on_computed_css_rather_than/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o3emqnp",
          "author": "Resident-Piano-1663",
          "text": "Puppeteer works great for me I use the eval$$ on IG to get usernames and send dms and that website has the worst classes and nested class names they randomly change and I created a scraper that sends dms and 3nmonths later I haven't had to change any code it still works",
          "score": 3,
          "created_utc": "2026-02-03 20:30:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bwybo",
          "author": "matty_fu",
          "text": "URL?",
          "score": 1,
          "created_utc": "2026-02-03 12:27:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3f0m64",
              "author": "Hour_Analyst_7765",
              "text": "Sites like [carousell.com](http://carousell.com)",
              "score": 1,
              "created_utc": "2026-02-03 21:35:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3glixt",
                  "author": "matty_fu",
                  "text": "are there any more details you can offer? eg. which pieces of data are you trying to lift off the page, and maybe given an example of the before and after HTML",
                  "score": 1,
                  "created_utc": "2026-02-04 02:38:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3fbde1",
          "author": "worldtest2k",
          "text": "Sometimes when they change the class names the tree hierarchy stays the same. So if you know the value you want is in the 5th nested div then just count down to it to locate it",
          "score": 1,
          "created_utc": "2026-02-03 22:26:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3m0rpr",
          "author": "prehensilemullet",
          "text": "This isn‚Äôt necessarily for obfuscation purposes in all cases where you see gibberish class names. ¬†CSS-in-JS libraries generate class names, not always in a deterministic manner.",
          "score": 1,
          "created_utc": "2026-02-04 22:12:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1quhl21",
      "title": "Tadpole - A modular and extensible DSL built for web scraping",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1quhl21/tadpole_a_modular_and_extensible_dsl_built_for/",
      "author": "tadpolehq",
      "created_utc": "2026-02-03 03:55:04",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.81,
      "text": "Hello!\n\nI wanted to share my recent project: Tadpole. It is a custom DSL built on top of [KDL](https://kdl.dev/) specifically for web scraping and browser automation.\n\nCheck out the documentation: https://tadpolehq.com/\nGithub Repo: https://github.com/tadpolehq/tadpole\n\n## Why?\nIt is designed to be modular and allows local and remote imports from git repositories. It also allows you to compose and slot complex `actions` and `evaluators`. There's tons of built-in functionality already to build on top of!\n\n### Example\n```kdl\nimport \"modules/redfin/mod.kdl\" repo=\"github.com/tadpolehq/community\"\n\nmain {\n  new_page {\n    redfin.search text=\"=text\"\n    wait_until\n    redfin.extract_from_card extract_to=\"addresses\" {\n      address {\n        redfin.extract_address_from_card\n      }\n    }\n  }\n}\n```\n\nand to run it:\n```bash\ntadpole run redfin.kdl --input '{\"text\": \"Seattle, WA\"}' --auto --output output.json\n```\n\nand the output:\n```json\n{\n  \"addresses\": [\n      {\n        \"address\": \"2011 E James St, Seattle, WA 98122\"\n      },\n      {\n        \"address\": \"8020 17th Ave NW, Seattle, WA 98117\"\n      },\n      {\n        \"address\": \"4015 SW Donovan St, Seattle, WA 98136\"\n      },\n      {\n        \"address\": \"116 13th Ave, Seattle, WA 98122\"\n      }\n      ...\n    ]\n}\n```\n\nIt is incredibly powerful to be able to now easily share and reuse scraper code the community creates! There's finally a way to standardize this logic.\n\n## Why not AI?\nAI is not doing a great job in this area, it's also incredibly inefficient and having noticeable environmental impact. People actually like to code.\n\n## Why not just Puppeteer?\nTadpole doesn't just call `Input.dispatchMouseEvent`, commands like `click` and `hover` are actually composed of several actions that use a bezier curve, and ease out functions to try to simulate human behavior. You get the ability to easily abstract away everything into the DSL.  The decentralized package manager also lets you share your code without the additional overhead and complexity that comes with npm or pip.\n\n**Note:** Tadpole is not built on Puppeteer, it implements CDP method calls and manages its own websocket.\n\nThe package was just released! Had a great time dealing with changesets not replacing the `workspace:` prefix. There will be bugs, but I will be actively releasing new features. Hope you guys enjoy this project!\n\nAlso, I created a repository: https://github.com/tadpolehq/community\nfor people to share their scraper code if they want to!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1quhl21/tadpole_a_modular_and_extensible_dsl_built_for/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qtcyok",
      "title": "Non sucking, easy tool to convert websites to LLM ready data, Mojo",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qtcyok/non_sucking_easy_tool_to_convert_websites_to_llm/",
      "author": "malvads",
      "created_utc": "2026-02-01 22:29:29",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.81,
      "text": "Hey all! After running into¬†*only paid tools or overly complicated setups*¬†for turning web pages into structured data for LLMs, I built¬†**Mojo,**¬†a¬†**simple, free, open-source tool**¬†that does exactly that. It‚Äôs designed to be easy to use and integrate into real workflows.\n\nIf you‚Äôve ever needed to prepare site content for an AI workflow without shelling out for paid services or wrestling with complex scrapers, this might help. Would love feedback, issues, contributions, use cases, etc. <3\n\n[https://github.com/malvads/mojo](https://github.com/malvads/mojo)¬†(and it's MIT licensed)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qtcyok/non_sucking_easy_tool_to_convert_websites_to_llm/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    }
  ]
}