{
  "metadata": {
    "last_updated": "2026-02-03 02:59:57",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 14,
    "total_comments": 77,
    "file_size_bytes": 81279
  },
  "items": [
    {
      "id": "1qs66k0",
      "title": "Couldn't find proxy directory with filters so built one",
      "subreddit": "webscraping",
      "url": "https://i.redd.it/xo702jo5kpgg1.png",
      "author": "Consistent-Feed-7323",
      "created_utc": "2026-01-31 16:04:15",
      "score": 28,
      "num_comments": 28,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qs66k0/couldnt_find_proxy_directory_with_filters_so/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2uiaup",
          "author": "Important-Seat-1882",
          "text": "Do you have here information on whether proxies can be used behind login and auth? ",
          "score": 3,
          "created_utc": "2026-01-31 20:06:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2v3efn",
              "author": "Consistent-Feed-7323",
              "text": "If you mean whenever proxies auth is by IP or login;password - yeah, there's a filter for that.",
              "score": 3,
              "created_utc": "2026-01-31 21:50:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2xewcb",
                  "author": "Lemon_eats_orange",
                  "text": "Important Seat may be referring to if the IP's themselves can be used for a login use case where the data isn't public. Some providers won't allow logging into some accounts with their proxies. Could be wrong though.",
                  "score": 1,
                  "created_utc": "2026-02-01 06:17:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2wjdbx",
          "author": "scrapingtryhard",
          "text": "EPICCCCC, the footer is still 2025 though u should fix it",
          "score": 3,
          "created_utc": "2026-02-01 02:41:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wjz7n",
              "author": "Consistent-Feed-7323",
              "text": "Dear god, that's probably the last thing I would ever think of lmao. Thank you!",
              "score": 1,
              "created_utc": "2026-02-01 02:45:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ywrbm",
                  "author": "scrapingtryhard",
                  "text": "add proxyon to it lol",
                  "score": 2,
                  "created_utc": "2026-02-01 13:50:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xf4q8",
          "author": "Lemon_eats_orange",
          "text": "Overall this is pretty good! No marketing just a list of proxies and what they can be used for. And you can check their trust pilots.",
          "score": 3,
          "created_utc": "2026-02-01 06:19:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2zfmfm",
          "author": "zrsca",
          "text": "This is so cool! Maybe add a sort by $/GB option. Really good site üôè",
          "score": 3,
          "created_utc": "2026-02-01 15:31:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zspeq",
              "author": "Consistent-Feed-7323",
              "text": "Thank you, I will try to add something like that in the future, biggest problem is to define price, since I'm putting minimal available for purchase price and services are often providing wholesale deals",
              "score": 1,
              "created_utc": "2026-02-01 16:32:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31asxq",
                  "author": "zrsca",
                  "text": "Yeah I figured that‚Äòd make it difficult. You could add a Filter option for how many GB the user wants to buy and use the according pricing they provide. But I think that‚Äòd be a pain in the a** to implement let alone maintain.",
                  "score": 1,
                  "created_utc": "2026-02-01 20:42:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o31o11p",
          "author": "illusivejosiah",
          "text": "Really well built. I run Illusory (mobile proxy provider) and I actually found this useful. Didn't realize we're the only service with dedicated IPs, IPv6, UDP, and unlimited bandwidth all-in-one. Appreciate the insight",
          "score": 3,
          "created_utc": "2026-02-01 21:47:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32bmgj",
              "author": "Consistent-Feed-7323",
              "text": "Thank you! The idea actually came when I was helping with marker research for my friend's proxy service and there was no place with such information.",
              "score": 1,
              "created_utc": "2026-02-01 23:50:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o32rzdg",
                  "author": "illusivejosiah",
                  "text": "Makes sense. Thanks for adding us!",
                  "score": 1,
                  "created_utc": "2026-02-02 01:21:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xjubp",
          "author": "SilentlySufferingZ",
          "text": "Which require KYC? Accept Bitcoin?",
          "score": 2,
          "created_utc": "2026-02-01 06:59:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ywymi",
              "author": "scrapingtryhard",
              "text": "proxyon",
              "score": 1,
              "created_utc": "2026-02-01 13:51:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o33cdcn",
          "author": "FranBattan",
          "text": "This is gold man, really appreciate it. Thanks",
          "score": 2,
          "created_utc": "2026-02-02 03:18:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t6bbc",
          "author": "akashpanda29",
          "text": "This is nice one. Kudos dude",
          "score": 1,
          "created_utc": "2026-01-31 16:17:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2t7gdm",
              "author": "Consistent-Feed-7323",
              "text": "Thank you very much, right now doing a round of \"marketing\", than will grab some more providers and start testing them",
              "score": 3,
              "created_utc": "2026-01-31 16:23:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2t9jt4",
          "author": "Fragrant_Ad3054",
          "text": "I really like the font used.",
          "score": 1,
          "created_utc": "2026-01-31 16:33:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2tbzmo",
              "author": "Consistent-Feed-7323",
              "text": "Thank you, it's monospace, I'm used to use it in my projects although it's not the best practice and is less readable than default.",
              "score": 1,
              "created_utc": "2026-01-31 16:44:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tg2n6",
          "author": "Alarmed_Scar_925",
          "text": "Thank you!!",
          "score": 1,
          "created_utc": "2026-01-31 17:04:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2v630c",
          "author": "Sergiowild",
          "text": "Good idea! Did you put your ref links on there?",
          "score": 1,
          "created_utc": "2026-01-31 22:03:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vq8af",
              "author": "Consistent-Feed-7323",
              "text": "Thank you. Not yet, but will do in some future. Ref links are requiring registration in most services and it's such a headache.",
              "score": 1,
              "created_utc": "2026-01-31 23:51:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2xxdz5",
          "author": "Mean_Yam5488",
          "text": "Nice work on the directory. Been looking for something like this when comparing providers.\n\nOne thing that would be super helpful - a filter for whether they allow account logins. Some providers block you from logging into social media or other accounts with their IPs, which kills a lot of use cases.",
          "score": 1,
          "created_utc": "2026-02-01 09:03:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y739u",
              "author": "Consistent-Feed-7323",
              "text": "Thank you! I do have similar to this info in restrictions, but many providers are either not disclosing it or referring as \"and other restrictions\" which can't be verified until testing. It's definitely something worth a separate filter, just can't make it right now.",
              "score": 1,
              "created_utc": "2026-02-01 10:32:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qqxam9",
      "title": "Do I need a residential proxy to mass scrape menus?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qqxam9/do_i_need_a_residential_proxy_to_mass_scrape_menus/",
      "author": "z420a",
      "created_utc": "2026-01-30 05:58:34",
      "score": 12,
      "num_comments": 10,
      "upvote_ratio": 0.89,
      "text": "I have about 30,000 restaurants for which I need to scrape their menus. As far as I know a good chunk of those use services such as uber eats, DoorDash, toasttab, etc to host their menus. \n\nIs it possible to scrape all of that with just my laptop? Or will I get IP banned?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qqxam9/do_i_need_a_residential_proxy_to_mass_scrape_menus/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2k1su0",
          "author": "Puzzleheaded_Row3877",
          "text": "with uber eats you get upto 2000 requests per hour ,I don't think the rest of the sites you mentioned care about rate limiting that much ; and if they do , mix it up by scraping some of the menus from the wayback machine.",
          "score": 8,
          "created_utc": "2026-01-30 06:08:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lxvby",
          "author": "Pooria_P",
          "text": "Don't scrape all with your laptop if you'll be doing it at once. if the websites are not sophisticated, just use the cheapest proxy you can find (typically, static datacenter, but HIGH block rates), or static residentials.  \nUber eats and etc though, I think you have a better chance with rotating residential.",
          "score": 3,
          "created_utc": "2026-01-30 14:35:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kihz0",
          "author": "Bosschopper",
          "text": "Can‚Äôt answer you question but I‚Äôm working on a similar project. Are you doing a menu database? My project is pretty far along the dev pipeline and I‚Äôm hosted on the internet (well, sorta) so if you‚Äôre open to collaborating hit me up.\n\nI‚Äôm far along with UI stuff and features but frankly have no data to feed. DM me about where you‚Äôre at too",
          "score": 2,
          "created_utc": "2026-01-30 08:28:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mbao3",
          "author": "bluemangodub",
          "text": "Do you? try it. And if you do, you will find out.\n\nThat's it. No one knows, unless they have done it. You can guess. \n\nBut typically, if you are scraping 1 site, and you want 30k pages, if you want to do it slow you might get away with it. Otherwise, proxies allow you to hit it harder.",
          "score": 2,
          "created_utc": "2026-01-30 15:39:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2k7x8c",
          "author": "bigtakeoff",
          "text": "naw... guess it depends on where the menus are at",
          "score": 1,
          "created_utc": "2026-01-30 06:56:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lncto",
          "author": "HaphazardlyOrganized",
          "text": "Probably not, just don't pound their servers. If you check like once a day at 2am you should be fine",
          "score": 1,
          "created_utc": "2026-01-30 13:41:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs08kg",
      "title": "I upgraded my YouTube data tool ‚Äî (much faster + simpler API)",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qs08kg/i_upgraded_my_youtube_data_tool_much_faster/",
      "author": "nagmee",
      "created_utc": "2026-01-31 11:43:24",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 0.81,
      "text": "A few months ago I shared my Python tool for fetching YouTube data. After feedback, I refactored everything and added some features with 2.0 version.\n\nHere's the new features:\n\n* Get structured comments alongside with transcript and metadata.\n* `ytfetcher`¬†is now fully synchronous, simplifying usage and architecture.\n* Pre-Filter videos based on metadata such as¬†`view_count`,¬†`duration`¬†and¬†`title`.\n* Fetch data with playlist id or search query to similar to Youtube Search Bar.\n* Simpler CLI usage.\n\nI also solved a very critical bug with this version which is metadata and transcripts are might not be aligned properly.\n\nI still have a lot of futures to add. So if you guys have any suggestions I'd love to hear.\n\nHere's the full changelog if you want to check;¬†\n\n[https://github.com/kaya70875/ytfetcher/releases/tag/v2.0](https://github.com/kaya70875/ytfetcher/releases/tag/v2.0)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qs08kg/i_upgraded_my_youtube_data_tool_much_faster/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qttig8",
      "title": "How are you using AI to help build scrapers?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qttig8/how_are_you_using_ai_to_help_build_scrapers/",
      "author": "lieutenant_lowercase",
      "created_utc": "2026-02-02 12:18:06",
      "score": 9,
      "num_comments": 13,
      "upvote_ratio": 0.81,
      "text": "I use Claude Code for a lot of my programming but doesn't seem particularily useful when I'm writing web scrapers. I still have to load up the site, go to dev tools, inspect all the requests, find the private API's, figure out headers / cookies, check if its protected by Cloudflare / Akamai etc.. Perhaps once I have that I can dump all my learnings into claude code with some scaffolding at get it to write the scraper, but its still quite painful to do. My major time sink is understanding the structure of the site/app and its protections rather than writing the actual code. \n\nI'm not talking about using AI to parse websites, thats the easy bit tbh. I'm talking about the actual code generation. Do people give their LLM's access to the browser and let it figure it out? Anything else you guys are doing?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qttig8/how_are_you_using_ai_to_help_build_scrapers/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o35jpsl",
          "author": "balletpaths",
          "text": "I point it to a URL, give a sample code format and let it rip! Then I adjust and make minor modifications.",
          "score": 2,
          "created_utc": "2026-02-02 13:44:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o363xue",
          "author": "somedude4949",
          "text": "Pass har file with requests I need to use give custom prompt on how to built and voila after few minutes everything working and integrate it depends on my use case",
          "score": 2,
          "created_utc": "2026-02-02 15:30:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35d5s1",
          "author": "sjmanzur",
          "text": "I started using antigravity with playwright and it‚Äôs a game changer really",
          "score": 2,
          "created_utc": "2026-02-02 13:05:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36mtp6",
          "author": "Tharnwell",
          "text": "Following. I'm currently building a web platform almost entirely with AI. Development and content creation are fully automated.\n\nThe only part AI still struggles with in my workflow is sourcing images from the web. I‚Äôm aware of copyright concerns, but in my specific use case this isn‚Äôt a major issue.\n\nWhile AI can generate images, they don‚Äôt work well for my needs.",
          "score": 1,
          "created_utc": "2026-02-02 16:57:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37jkzh",
          "author": "builderbycuriosity",
          "text": "Give your Claude code access to MCP servers like Playwright, which can automate the browser. It may not be perfect, but it will do the job.",
          "score": 1,
          "created_utc": "2026-02-02 19:27:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38sqni",
          "author": "orthogonal-ghost",
          "text": "I've thought about this problem a lot. The main challenge as you've noted is given the coding agent the proper context (HTML, network requests, javascript, etc.).  \n  \nTo address this, we built a specialized agent to programmatically \"inspect\" a web site for that context and to generate a Python script to scrape it. With that comes its own share of challenges (e.g., naively passing in all the HTML on a given web page can very quickly eat up an LLM's context), but we've found that it's been quite successful in building scrapers once it has the right things to look at.",
          "score": 1,
          "created_utc": "2026-02-02 23:04:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39eeeu",
          "author": "xRazar",
          "text": "I had a lot of success using Agent-Browser with Skills to integrate it into the models into OpenCode. The agents scan through the site trying to find public APIs if that fails it goes back to classic scraping.",
          "score": 1,
          "created_utc": "2026-02-03 01:03:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39j6il",
          "author": "calimovetips",
          "text": "i mostly use ai after i‚Äôve mapped the network calls. it‚Äôs good for turning notes into clean request code, retries, backoff, and a sane pipeline. the hard part is still modeling sessions and state, plus deciding what‚Äôs stable. are you scraping mostly xhr/json endpoints or full browser flows?",
          "score": 1,
          "created_utc": "2026-02-03 01:30:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35e014",
          "author": "No-Appointment9068",
          "text": "I sometimes download the page source, set up a test for the output I want and then let AI have a crack at getting the selectors correct, they often produce quite brittle selectors but it's very easy to then fix with the same process.",
          "score": 1,
          "created_utc": "2026-02-02 13:10:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35reg9",
              "author": "jwrzyte",
              "text": "this - almost all my parsing code is generated by copilot, then i can test against it within pytest & scrapy. and make any changes as needed",
              "score": 1,
              "created_utc": "2026-02-02 14:26:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36cmpi",
                  "author": "No-Appointment9068",
                  "text": "A little tip! Chrome dev tools let's you copy selectors if you right click the element within the inspect view, for one off scripts I just use those. I Don't bother getting any fancier with AI.",
                  "score": 3,
                  "created_utc": "2026-02-02 16:11:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35jwhf",
          "author": "AdministrativeHost15",
          "text": "Have the AI generate the scraping script. Don't code review it or try to fix it. Just have a test that determines if it returns any useful data or not. If it doesn't have the AI regenerate it.",
          "score": 0,
          "created_utc": "2026-02-02 13:45:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qofvmx",
      "title": "Akamai anti-bot blocking flight search scraping (403/418)",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qofvmx/akamai_antibot_blocking_flight_search_scraping/",
      "author": "Individual-Ship-7587",
      "created_utc": "2026-01-27 14:49:48",
      "score": 8,
      "num_comments": 23,
      "upvote_ratio": 0.75,
      "text": "Hi all,\n\nI‚Äôm attempting to collect public flight search data (routes, dates, mileage pricing) for personal research, at low request rates and without commercial intent.\n\nAirline websites (Azul / LATAM) consistently return 403 and 418 responses, and traffic analysis strongly suggests Akamai Bot Manager / sensor-based protection.\n\n# Environment & attempts so far\n\n* Python and Go\n* Multiple HTTP clients and browser automation frameworks\n* Headless and non-headless browsers\n* Mobile and rotating proxies\n* Header replication (UA, sec-ch-ua, accept, etc.)\n* Session persistence, realistic delays, low RPS\n\nDespite matching headers and basic browser behavior, sessions eventually fail.\n\n# Observed behavior\n\nFrom inspecting network traffic:\n\n* Initial page load sets temporary cookies\n* A follow-up request sends browser fingerprint / behavioral telemetry\n* Only after successful validation are long-lived cookies issued\n* Missing or inconsistent telemetry leads to 403/418 shortly after\n\nThis looks consistent with client-side sensor collection (JS-generated signals rather than static tokens).\n\n# Conceptual question\n\nAt this level of protection, is it generally realistic to:\n\n* Attempt to reproduce sensor payloads manually (outside a real browser), or\n* Does this usually indicate that:\n   * Traditional HTTP-level scraping is no longer viable?\n   * Only full browser execution with real user interaction scales reliably?\n   * Or that the correct approach is to seek alternative data sources (official APIs, licensed feeds, partnerships)?\n\nI‚Äôm not asking for bypass techniques or ToS violations ‚Äî I‚Äôm trying to understand where the practical boundary is for scraping when dealing with modern, behavior-based bot defenses.\n\nAny insight from people who‚Äôve dealt with Akamai or similar systems would be greatly appreciated.\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1qofvmx/akamai_antibot_blocking_flight_search_scraping/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o21hix7",
          "author": "army_of_wan",
          "text": "Have you tried using fire fox automation ? ( puppeteer , camafoux ?)\nHarvest cookies with a browser and then reuse them to scrape the site.",
          "score": 6,
          "created_utc": "2026-01-27 16:27:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24xvrd",
              "author": "letopeto",
              "text": "Isn‚Äôt camoufox widely outdated? Repo hasn‚Äôt been update in a while it‚Äôs still running a very old version of Firefox and last time I tried it it couldn‚Äôt even bypass a simple Cloudflare challenge",
              "score": 2,
              "created_utc": "2026-01-28 02:04:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2tzfuj",
                  "author": "Coding-Doctor-Omar",
                  "text": "Yes, it is outdated now, but its owner recently announced that it's back to development and should return back to its performance this year.",
                  "score": 1,
                  "created_utc": "2026-01-31 18:36:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o265xra",
              "author": "irrisolto",
              "text": "Not gonna work at scale",
              "score": 1,
              "created_utc": "2026-01-28 06:38:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o266nm3",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 1,
                  "created_utc": "2026-01-28 06:43:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21k1xl",
          "author": "Specialist-Egg-3720",
          "text": "once the js challenge is completed in real browser, it sets a cookie, you need to capture that cookie then move on to do your api calls, its very difficult to reproduce a sensor payload without the real browser as it checks for various things such as webgl rendering, viewport, cpu info +gpu info + screen info + other params of your pc to create a unique device specifc signature. Its good option to just solve this challenge in real browser , get the cookie then move on to http request based scraping.",
          "score": 3,
          "created_utc": "2026-01-27 16:38:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o226p1d",
          "author": "ScrapeAlchemist",
          "text": "Hey!\n\nYou've basically hit the wall where HTTP-level scraping stops being practical. Akamai Bot Manager with sensor.js is specifically designed to detect the difference between real browsers and automation.\n\nTo answer your conceptual question directly:\n\n1. **Reproducing sensor payloads manually** - theoretically possible but a nightmare to maintain. The fingerprint includes WebGL rendering, canvas, audio context, and dozens of other signals that change with Akamai updates\n\n2. **Full browser with real interaction** - this works but you need the browser to actually execute the JS and pass the challenges. The cookie harvesting approach others mentioned is the right idea\n\n3. **Alternative data sources** - for flight data specifically, Google Flights pulls from the same GDS systems. Some airlines also have affiliate/partner APIs that are less protected\n\nThe practical boundary: if a site invests in Akamai Bot Manager, they've decided to make scraping hard. You either invest equally in bypassing it (real browsers, residential IPs, proper session handling) or find a different data source.\n\nFor personal research at low volume, a real browser with residential IP and human-like delays might work. But it'll be fragile.\n\nGood luck!",
          "score": 2,
          "created_utc": "2026-01-27 18:16:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22tuoh",
          "author": "Afraid-Solid-7239",
          "text": "reply with the page, and data you want to scrape?",
          "score": 1,
          "created_utc": "2026-01-27 19:56:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27bgei",
              "author": "Individual-Ship-7587",
              "text": "The page is a public flight search results page, for example:\n\n[https://www.latamairlines.com/br/pt/oferta-voos?origin=POA&destination=SCL&outbound=2026-06-17&inbound=2026-03-26&trip=RT&cabin=Economy&redemption=true]()\n\nI‚Äôm only trying to collect the data shown to any user on that page:  \nflight options (route, dates, airline), mileage prices, taxes/fees, cabin class, and basic availability information. No booking, no account access, and no private user data.",
              "score": 2,
              "created_utc": "2026-01-28 12:29:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o265wps",
          "author": "irrisolto",
          "text": "Use a solver to get akamai cookies",
          "score": 1,
          "created_utc": "2026-01-28 06:37:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26nn9p",
              "author": "scraperouter-com",
              "text": "what solver?",
              "score": 1,
              "created_utc": "2026-01-28 09:12:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o279qaa",
                  "author": "irrisolto",
                  "text": "A solver for akamai that gives you cookies",
                  "score": 1,
                  "created_utc": "2026-01-28 12:17:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2b1zbx",
          "author": "nez1rat",
          "text": "The only way to scrape successfully the data is by using a valid TLS fingerprint with valid Akamai cookies in your request.  \n  \nTo test it simply copy paste from your browser the cookies that are being used in the fetch flights request",
          "score": 1,
          "created_utc": "2026-01-28 22:51:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u009e",
          "author": "Coding-Doctor-Omar",
          "text": "Have u tried using curl_cffi with impersonate=\"edge\" or \"firefox\" and with session cookies? I bet it will bypass 90% of anti-bot protections. The only things it will fail to bypass are interactive challenges.",
          "score": 1,
          "created_utc": "2026-01-31 18:39:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zr8z9",
              "author": "Financial-Image6455",
              "text": "That worked, thank you very much.",
              "score": 1,
              "created_utc": "2026-02-01 16:26:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o39krjr",
          "author": "HLCYSWAP",
          "text": "you should not serve the Portuguese site to a 100% english subreddit. make the cURL with TLS impersonation + akamai cookies included to get the json return with the data you want. at scale this means a headful browser to dump cookies before mass requests.\n\n  \n\"\"\"  \nRequires cookies from the browser. Set LATAM\\_COOKIES or create cookies.txt.  \n\"\"\"  \nimport os  \nimport sys  \nfrom pathlib import Path  \nfrom curl\\_cffi import requests  \n  \n  \ndef parse\\_cookies(cookie\\_header: str) -> dict\\[str, str\\]:  \n\"\"\"Parse 'name=value; name2=value2' into a dict. Handles values containing '='.\"\"\"  \ncookies = {}  \nfor part in cookie\\_header.split(\"; \"):  \npart = part.strip()  \nif not part:  \ncontinue  \nif \"=\" in part:  \nname, \\_, value = part.partition(\"=\")  \ncookies\\[name.strip()\\] = value.strip()  \nreturn cookies  \n  \n  \n  \ndef main() -> None:  \norigin = os.environ.get(\"LATAM\\_ORIGIN\", \"POA\")  \ndestination = os.environ.get(\"LATAM\\_DESTINATION\", \"BHZ\")  \noutbound = os.environ.get(\"LATAM\\_OUTBOUND\", \"2026-02-05T12%3A00%3A00.000Z\")  \n  \n  \nurl = (  \n\"https://www.latamairlines.com/us/en/flight-offers\"  \nf\"?origin={origin}&outbound={outbound}&destination={destination}\"  \n\"&inbound=null&adt=1&chd=0&inf=0&trip=OW&cabin=Economy\"  \n\"&redemption=false&sort=RECOMMENDED\"  \n)  \n  \n  \nheaders = {  \n\"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,\\*/\\*;q=0.8\",  \n\"accept-language\": \"en-US,en;q=0.7\",  \n\"cache-control\": \"max-age=0\",  \n\"priority\": \"u=0, i\",  \n\"referer\": \"https://www.latamairlines.com/us/en\",  \n\"sec-ch-ua\": '\"Not(A:Brand\";v=\"8\", \"Chromium\";v=\"144\", \"Brave\";v=\"144\"',  \n\"sec-ch-ua-mobile\": \"?0\",  \n\"sec-ch-ua-platform\": '\"Linux\"',  \n\"sec-fetch-dest\": \"document\",  \n\"sec-fetch-mode\": \"navigate\",  \n\"sec-fetch-site\": \"same-origin\",  \n\"sec-fetch-user\": \"?1\",  \n\"sec-gpc\": \"1\",  \n\"upgrade-insecure-requests\": \"1\",  \n}  \n  \n  \ncookie\\_str = os.environ.get(\"LATAM\\_COOKIES\")  \nif not cookie\\_str and Path(\"cookies.txt\").exists():  \ncookie\\_str = Path(\"cookies.txt\").read\\_text(encoding=\"utf-8\").strip()  \nif not cookie\\_str:  \nprint(\"Set LATAM\\_COOKIES or create cookies.txt with Cookie header from the flight-offers page.\", file=sys.stderr)  \nsys.exit(1)  \ncookies = parse\\_cookies(cookie\\_str)  \nwith requests.Session(impersonate=\"chrome\") as session:  \nresp = session.get(  \nurl,  \nheaders=headers,  \ncookies=cookies,  \ntimeout=30,  \n)  \nprint(f\"Status: {resp.status\\_code}\")  \n  \n  \nif \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":  \nmain()",
          "score": 1,
          "created_utc": "2026-02-03 01:39:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrdn9r",
      "title": "Need help",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qrdn9r/need_help/",
      "author": "imvdave",
      "created_utc": "2026-01-30 18:27:57",
      "score": 7,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "I have a list of 2M+ online stores for which I want to detect the technology.\n\n  \nI have the script, but I often face 429 errors due to many websites belonging to Shopify.\n\n  \nIs there any way to speed this up?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qrdn9r/need_help/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2nkqci",
          "author": "scraperouter-com",
          "text": "use rotating proxies",
          "score": 2,
          "created_utc": "2026-01-30 19:00:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nsf8c",
          "author": "Puzzleheaded_Row3877",
          "text": "rotate the IP's. Also organize your list so that you are not hitting shopify 50 times in a row.",
          "score": 2,
          "created_utc": "2026-01-30 19:35:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nrwgu",
          "author": "greg-randall",
          "text": "Can you do a DNS lookup on your domains and build a list of Shopify owned IPs?",
          "score": 1,
          "created_utc": "2026-01-30 19:32:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nzkav",
          "author": "NZRedditUser",
          "text": "well if you get a 429 (if you dont wanna solve the proxy issue) just check where the redirect goes if you do domain/admin if it goes -> x myshopify com then you know its shopify and can make assessments via that?",
          "score": 1,
          "created_utc": "2026-01-30 20:08:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oczsh",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-30 21:12:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2odeps",
              "author": "webscraping-ModTeam",
              "text": "üëî Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
              "score": 1,
              "created_utc": "2026-01-30 21:14:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35dhmu",
          "author": "ScrapeAlchemist",
          "text": "Hey! At 2M+ URLs you're going to need a large pool of rotating proxies - the other comments are right about that. Depending on how fast you want to go, you might need 100-1000+ IPs to avoid getting rate-limited by Shopify's shared infrastructure.\n\nAlso shuffle your list so you're not hammering Shopify back-to-back - interleave domains by provider.\n\nFor Shopify detection specifically - try hitting `/products.json` on each domain. Shopify stores expose this endpoint by default, so a 200 response with valid JSON is a quick confirm without parsing HTML. Same idea for other platforms that have predictable endpoints.\n\nGood luck!",
          "score": 1,
          "created_utc": "2026-02-02 13:07:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37jmfw",
          "author": "scorpiock",
          "text": "Two things you could try:\n\n1. Slow down so you don't hit the rate limit  \n2. Rotating proxies",
          "score": 1,
          "created_utc": "2026-02-02 19:27:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qod78a",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qod78a/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-01-27 13:01:04",
      "score": 7,
      "num_comments": 14,
      "upvote_ratio": 1.0,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levels‚Äîwhether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) üå±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1qod78a/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2b5ims",
          "author": "Working_Map379",
          "text": "I am looking to hire web scraping expert. Please DM me.",
          "score": 3,
          "created_utc": "2026-01-28 23:09:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35hh1c",
          "author": "Any_Independent375",
          "text": "I have a question since my post was deleted (for whatever reason): \n\n**How to scrape Instagram followers/followings in chronological order?**\n\nHi everyone,\n\nI‚Äôm trying to understand how some websites are able to show Instagram followers or followings in¬†**chronological order**¬†for¬†**public accounts**.\n\nI already looked into this:\n\n* When opening the followers/following popup on Instagram, the list is¬†**not**¬†shown in chronological order.\n* The web request¬†[https://www.instagram.com/api/v1/friendships/{USER\\_ID}/following/?count=12](https://www.instagram.com/api/v1/friendships/%7BUSER_ID%7D/following/?count=12)¬†returns users in¬†**exactly the same order**¬†as shown in the popup, which again is¬†**not chronological**.\n* The response does¬†**not**¬†include any obvious timestamp like followed\\_at, nor an incrementing ID that would allow sorting by time.\n\nI‚Äôm interested in¬†**how this is technically possible at all**.\n\nAny insights from people who have looked into this would be really appreciated.\n\nThanks![](https://www.reddit.com/r/webscraping/?f=flair_name%3A%22Getting%20started%20%F0%9F%8C%B1%22)",
          "score": 2,
          "created_utc": "2026-02-02 13:31:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20j8it",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-27 13:45:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20v3bm",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-01-27 14:46:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21d8jp",
          "author": "xRazar",
          "text": "I'm currently in the process of scraping e-sim sites wherever possible I try to find the public APIs for this but it does not seem that effective for most of the sites. Anyone has experience with scraping E-Sim sites (Saily, Nomad as few examples to go off)",
          "score": 1,
          "created_utc": "2026-01-27 16:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hodl0",
              "author": "error1212",
              "text": "I have",
              "score": 1,
              "created_utc": "2026-01-29 21:58:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21ndjn",
          "author": "EstablishmentOver202",
          "text": "How do you guys deal with cloudflare? Turnstile is killing me",
          "score": 1,
          "created_utc": "2026-01-27 16:53:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24ff3z",
              "author": "Mean_Professional529",
              "text": "Try a scraping API that handles JavaScript rendering and proxy rotation. Some services include built-in CAPTCHA solving for Turnstile. This can help bypass Cloudflare without managing it yourself",
              "score": 1,
              "created_utc": "2026-01-28 00:28:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2e6w04",
              "author": "jonfy98",
              "text": "You could try APIScraper which is efficient but not free,   \nanother idea could be NoDriver for better successrate which i often use for dealing with this kind of problem.",
              "score": 1,
              "created_utc": "2026-01-29 11:37:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2gq3ee",
              "author": "gbertb",
              "text": "not free, but very cost is use spider cloud",
              "score": 1,
              "created_utc": "2026-01-29 19:14:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2p35i9",
              "author": "Open_Passage_7351",
              "text": "[https://github.com/lexiforest/curl\\_cffi](https://github.com/lexiforest/curl_cffi) wrapped with FastAPI endpoint.\n\n\n\nI run my \\`app.py\\` which exposes a single \\`post /api/forward\\` endpoint. I can call that endpoint from any other service passing the target URL. The request is passed through curl\\_cffi, and response returned. I'm using this at a scale of thousands of requests a day with pretty decent success rate (well above 80%).",
              "score": 1,
              "created_utc": "2026-01-30 23:22:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o39f0fe",
          "author": "DesignerWar3820",
          "text": "i'm trying to find a way to get the urls for an entire saved public collection of mine. the collection has 591 videos in it and i've trying making it work with a code in the console (that i can't find it anymore) but it gave me urls that were not within this collection but were a part of other collections i had. what code can i include in the console to make sure i get only the urls for the public collection i have opened.\n\ni also tried using apify but it did not give me the correct urls even when i gave it the saved public collection url. what are some codes or tips to be able to get all the urls from 1 collection ?",
          "score": 1,
          "created_utc": "2026-02-03 01:06:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28bhyx",
          "author": "Either_Height7010",
          "text": "I'm hiring a US-based senior+ reverse engineer. At an incredibly high level, think bypassing anti-bot systems, large-scale web scraping/login automation, and JavaScript-based reverse engineering of web apps. \n\nI'm a third-party recruiter sourcing on behalf of my client. Message me if intrigued!",
          "score": 1,
          "created_utc": "2026-01-28 15:38:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo77gd",
      "title": "Advice needed: scraping company websites in Python",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qo77gd/advice_needed_scraping_company_websites_in_python/",
      "author": "Working_Taste9458",
      "created_utc": "2026-01-27 07:26:44",
      "score": 7,
      "num_comments": 15,
      "upvote_ratio": 0.89,
      "text": "I‚Äôm building a small project that needs to scrape company websites (manufacturers, suppliers, distributors, traders) to collect basic business information. I‚Äôm using Python and want to know what the best approach and tools are today for reliable web scraping. For example, should I start with requests + BeautifulSoup, or go straight to something like Playwright? Also, any general tips or common mistakes to avoid when scraping multiple websites would be really helpful.",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qo77gd/advice_needed_scraping_company_websites_in_python/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1zauu4",
          "author": "Bitter_Caramel305",
          "text": "Playwright is not the choice of any expert it's the choice of dumb beginners.\n\nRequests and bs4 is fine but replace requests with the requests module of curl\\_cffi.  \nThe syntax will be the same, but you'll get TSL fingerprinting of a real browser (Thanks to C) and an optional but powerful request param (impersonate=\"any browser of your choice\").\n\nExample:\n\n    from curl_cffi import requests\n    r = requests.get(url, cookies, headers, impersonate=\"chrome\")\n\n\n\nAlso, always reverse engineer the exposed backend API first and use this as a fallback not primary method.  \nHappy scraping!",
          "score": 13,
          "created_utc": "2026-01-27 08:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zn2lq",
              "author": "scraperouter-com",
              "text": "if curl\\_cffi is blocked you can try scrapling stealthmode but only if you are sure you need the browser (much slower way)",
              "score": 3,
              "created_utc": "2026-01-27 09:53:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o23uv95",
                  "author": "askolein",
                  "text": "But isn‚Äôt most websites not directly rendering html via http requests. I struggle to see any relevant website to scrape without selenium?",
                  "score": 1,
                  "created_utc": "2026-01-27 22:44:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o277s0v",
              "author": "husayd",
              "text": "I feel offended by the first sentence xd. I use both, and sometimes playwright (or selenium) is inevitable, or i am just a dumb beginner.",
              "score": 1,
              "created_utc": "2026-01-28 12:04:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o279aau",
                  "author": "Bitter_Caramel305",
                  "text": "Sorry about that ;) but to be honest, sometimes I reverse engineer the entire website while reverse engineering the API, just so I can avoid the inevitable browser automation.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:14:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o208fu0",
          "author": "Responsible-Fly-990",
          "text": "go with requests + BeautifulSoup if you r a beginner",
          "score": 2,
          "created_utc": "2026-01-27 12:43:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24byq2",
          "author": "Hungry-Working26",
          "text": "For company sites, start with requests and BeautifulSoup. Switch to Playwright only if you see dynamic content. Rotate user agents and add delays between requests to be respectful.\n\nHere's a basic pattern using the requests library:\n\npython\n\nimport requests\n\nfrom bs4 import BeautifulSoup\n\nresponse = requests.get('your\\_url\\_here')\n\nsoup = BeautifulSoup(response.content, 'html.parser')\n\nAlways check the site's robots.txt first",
          "score": 2,
          "created_utc": "2026-01-28 00:11:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2818mg",
          "author": "New-Independence5780",
          "text": "use cheriooCrawlee if it just simple websites that doesnt need js rendering if yes use playwrightCrawlee or puppeterCrawlee",
          "score": 1,
          "created_utc": "2026-01-28 14:51:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28rt0d",
          "author": "wequatimi",
          "text": "So you got ai businessidea=make cash fast.\nMight be entertaining. And educative..",
          "score": 1,
          "created_utc": "2026-01-28 16:49:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2azs7r",
          "author": "nez1rat",
          "text": "Honestly it depends on what are your target sites tho, I can suggest you to use [https://pypi.org/project/curl-cffi/](https://pypi.org/project/curl-cffi/) with BeautifulSoup as you mentioned",
          "score": 1,
          "created_utc": "2026-01-28 22:40:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eubfr",
          "author": "byte_knight_",
          "text": "Definetely start from with requests and bs4 or speed and simplicity, i'd use Playwright only for something JS heavy maybe",
          "score": 1,
          "created_utc": "2026-01-29 14:03:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpmlka",
      "title": "Help: BeautifulSoup/Playwright Parsing Logic",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/gallery/1qpmlka",
      "author": "TapProfessional4535",
      "created_utc": "2026-01-28 20:14:09",
      "score": 5,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qpmlka/help_beautifulsoupplaywright_parsing_logic/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o2fm5s5",
          "author": "Business-Cherry1883",
          "text": "If your end goal is ‚Äú3000 players + rankings‚Äù, I‚Äôd seriously consider¬†**avoiding DOM parsing**¬†for the core dataset and only parsing HTML for the few fields that aren‚Äôt available elsewhere.\n\n* There‚Äôs already a Python package on pip called¬†`twofourseven`¬†that can scrape 247Sports recruiting data and includes a¬†`TransferPortal`¬†class with¬†`getFootballData(year)`¬†that returns a dataframe of everyone who entered the football transfer portal for a given year.‚Äã\n* Once you have that baseline list, use Playwright only for the ‚Äúdetail page‚Äù fields you¬†*must*¬†render (e.g., banners/commitment blocks) and keep the HTML parsing minimal and label-driven (parse ‚ÄúOVR/NATL/ST/Pos‚Äù by the nearby label text, not by assuming order/position).\n\nFor your brittle cases (stars, state rank vs position rank, JUCO variance), don‚Äôt do ‚Äúany number not X must be Y‚Äù. Instead: extract the small text chunk for each section (‚ÄúAs a Transfer‚Äù / ‚ÄúAs a Prospect‚Äù), then regex-match explicit patterns (`OVR`,¬†`NATL`,¬†`ST`,¬†`JUCO`,¬†`KS: 8`, etc.) and treat anything unmatched as ‚Äúunknown‚Äù rather than forcing it into a column.",
          "score": 3,
          "created_utc": "2026-01-29 16:15:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2n9oyk",
              "author": "TapProfessional4535",
              "text": "If I am using GitHub, Databricks, Google Collab, or another LLM, does that change your reco? \n\nI‚Äôm on a machine that is strict with what I can do in native Python app on it. \n\nThis is the first scraping project I‚Äôve ever done FWIW.",
              "score": 1,
              "created_utc": "2026-01-30 18:12:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ck3u1",
          "author": "jlrich10",
          "text": "I would look at scripts. I didnt want to take the time to do it but I would look at this and it see if it has what you need. \\_\\_INITIAL\\_DATA\\_\\_. Give that to Claude or Chatgpt and it will write the parser if it has what you need in it. Using json is usually better if its in there.",
          "score": 2,
          "created_utc": "2026-01-29 03:43:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2n8ciq",
              "author": "TapProfessional4535",
              "text": "I‚Äôve tried ChatGPT and Gemini both. Many hours of back and forth. I‚Äôve got enterprise licenses for both. I‚Äôm a decent promoter for regression modeling and forest modeling so this is making me pull my hair out",
              "score": 1,
              "created_utc": "2026-01-30 18:06:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ecmrt",
          "author": "scraperouter-com",
          "text": "Did you check JSON data available in the source code?\n\nhttps://preview.redd.it/x5irlakw5agg1.png?width=442&format=png&auto=webp&s=0b99b76c15763e484568f4b6ac05ddaaa2b80a79\n\nand other similar tags with structured data.",
          "score": 2,
          "created_utc": "2026-01-29 12:19:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2n8nx7",
              "author": "TapProfessional4535",
              "text": "I‚Äôd be lying if I knew what I‚Äôm doing. Savvy in advanced analytics, but this is the first scraping project I‚Äôve ever worked on.",
              "score": 0,
              "created_utc": "2026-01-30 18:07:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o34b0zm",
                  "author": "kev_11_1",
                  "text": "yeah try looking at this script tag in the html. Just click inspect on the page and then search for sthe cript tag that has json data and tell Gemini or GPT to take it from there.",
                  "score": 1,
                  "created_utc": "2026-02-02 07:34:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2a3fbl",
          "author": "TapProfessional4535",
          "text": "The Code Snippet: Here is the full parsing function. Is there a more robust way to handle these dynamic ranking boxes and to separate out Transfer vs. Prospect?\n\ndef parse_profile(html, url, player_id):\n    soup = BeautifulSoup(html, 'lxml')\n    data = {}\n   \n    # Locate the text nodes to find the correct containers\n    transfer_node = soup.find(string=re.compile(\"As a Transfer\"))\n    prospect_node = soup.find(string=re.compile(\"As a Prospect\"))\n\n    # --- 1. Parsing Transfer Section ---\n    if transfer_node:\n        t_container = transfer_node.find_parent('section') or transfer_node.find_parent('div')\n        if t_container:\n            # Stars & Rating\n            stars = t_container.select('.icon-starsolid.yellow')\n            data['Transfer Stars'] = len(stars)\n            rating = t_container.select_one('.rating')\n            if rating: data['Transfer Rating'] = rating.text.strip()\n           \n            # Ranks (Negative Logic)\n            for li in t_container.select('li'):\n                label = li.select_one('h5').get_text(strip=True).upper()\n                val = li.select_one('strong').get_text(strip=True)\n               \n                if 'OVR' in label:\n                    data['Transfer Overall Rank'] = val\n                # If NOT Overall/National/State, assume Position Rank (Fixes WR vs S mismatch)\n                elif label not in ['NATL', 'NATIONAL', 'ST', 'STATE']:\n                    data['Transfer Position Rank'] = val\n\n    # --- 2. Parsing Prospect Section (JUCO Logic) ---\n    if prospect_node:\n        p_container = prospect_node.find_parent('section') or prospect_node.find_parent('div')\n        if p_container:\n            # Check for JUCO header\n            is_juco = \"JUCO\" in p_container.get_text().upper()\n           \n            # Stars (Flag JUCO if empty)\n            stars = p_container.select('.icon-starsolid.yellow')\n            data['Prospect Stars'] = f\"{len(stars)} JUCO\" if is_juco else len(stars)\n\n            # Ranks (Prioritize National, then Position)\n            for li in p_container.select('li'):\n                label = li.select_one('h5').get_text(strip=True).upper()\n                val = li.select_one('strong').get_text(strip=True)\n               \n                if 'NATL' in label or 'NATIONAL' in label:\n                    data['Prospect National Rank'] = f\"{val} JUCO\" if is_juco else val\n                # Filter out State abbreviations (AK, AL, ... TX, etc) to find Position Rank\n                elif label not in ['OVR', 'ST', 'STATE', 'TX', 'FL', 'CA', 'GA']:\n                    data['Prospect Position Rank'] = f\"{val} JUCO\" if is_juco else val\n\n    return data",
          "score": 1,
          "created_utc": "2026-01-28 20:17:07",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrw98m",
      "title": "Data Scraping - What to use?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qrw98m/data_scraping_what_to_use/",
      "author": "Fabulous_Variety_256",
      "created_utc": "2026-01-31 07:47:26",
      "score": 5,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "My tech stack - NextJS 16, Typescript, Prisma 7, Postgres, Zod 4, RHF, Tailwindcss, ShadCN, Better-Auth, Resend, Vercel\n\nI'm working on a project to add to my cv. It shows data for gaming - matches, teams, games, leagues etc and also I provide predictions.\n\nMy goal is to get into my first job as a junior full stack web developer.\n\nI‚Äôm not done yet, I have at least 2 months to work on this project.\n\n\n\nThe thing is - I have another thing to do.\n\nI need to scrape data from another site. I want to get all the matches, the teams etc.\n\nWhen I enter a match there, it will not load everything. It will start loading the match details one by one when I'm scrolling.\n\n\n\nHow should I do it:\n\n\n\nIn the same project I'm building?\n\n\n\nIn a different project?\n\n\n\nIf 2, maybe I should show that I can handle another technologies besides next?:\n\n\n\nShould I do it with NextJS also\n\n\n\nShould I do it with NodeJS+Express?\n\n\n\nAnything else?\n\n\n\n\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qrw98m/data_scraping_what_to_use/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2rmxc0",
          "author": "hikingsticks",
          "text": "It sounds like you need to set up a headless browser based web scraper to get the data you need, then process it and stick it in a database.\n\nWhere are you deploying it? If you're using a VPS, consider one docker container running the database, one running the API to serve up the data, and one that gets started up periodically to run the scraper and insert the data into the database.\n\nPlaywright is a common choice for headless scraping, it can be done with javascript.",
          "score": 6,
          "created_utc": "2026-01-31 10:14:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3744q5",
          "author": "hasdata_com",
          "text": "Separate it. Definitely.\n\nRegarding the library, since the target site has infinite scroll, you need a headless browser like Puppeteer or Playwright (easier for beginners).",
          "score": 3,
          "created_utc": "2026-02-02 18:16:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sau1y",
          "author": "ketopraktanjungduren",
          "text": "If you're on NodeJS then use Playwright.¬†\n\n\nI'm on Python, and I use requests other tha Playwright. Maybe you also need a library that's like requests in python\n\n\nThese two are more than enough to start with",
          "score": 1,
          "created_utc": "2026-01-31 13:28:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tuqra",
          "author": "RandomPantsAppear",
          "text": "You‚Äôre going to find a lot more support for scraping related activities in Python, not JavaScript. \n\nPython is the language of choice for data processing and analysis, so it‚Äôs also the language of choice for acquisition.",
          "score": 1,
          "created_utc": "2026-01-31 18:14:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3503qu",
          "author": "ScrapeAlchemist",
          "text": "Hi,\n\nFor lazy-loading sites (content loads on scroll), you'll need browser automation since the data is rendered via JavaScript.\n\n**Tech choice:**\nSince you're already using NextJS, I'd suggest a **separate Node.js project** for the scraper. It keeps concerns separated and shows you can work outside the Next ecosystem ‚Äî good for CV. You can run it on a schedule and push data to your Postgres DB.\n\n**Tools for JS-rendered pages:**\n- **Playwright** ‚Äî modern, fast, great for scrolling/waiting for content\n- **Puppeteer** ‚Äî also solid, works well\n\n**Basic approach:**\n1. Load page with Playwright\n2. Scroll to trigger lazy loading (`page.evaluate(() => window.scrollBy(0, 1000))`)\n3. Wait for new content (`page.waitForSelector`)\n4. Extract data\n5. Repeat until all content loaded\n\n**Example pattern:**\n```javascript\nwhile (hasMoreContent) {\n  await page.evaluate(() => window.scrollTo(0, document.body.scrollHeight));\n  await page.waitForTimeout(1000);\n  // check if new items appeared\n}\n```\n\nGood luck with the project!",
          "score": 1,
          "created_utc": "2026-02-02 11:30:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38qkws",
          "author": "somedude4949",
          "text": "Before you use the browser automation , playwright or puppeteer make to try find if there any Public endpoints you can fetch directly with some minor adjustments",
          "score": 1,
          "created_utc": "2026-02-02 22:53:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qt7pju",
      "title": "litecrawl - minimal async crawler for targeted, incremental scraping",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qt7pju/litecrawl_minimal_async_crawler_for_targeted/",
      "author": "Little_Ant_3459",
      "created_utc": "2026-02-01 19:14:58",
      "score": 5,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "I kept hitting the same pattern at work: \"we need to index this specific section of this website, with these rules, on this schedule.\" Each case was slightly different - different URL patterns, different update frequencies, different extraction logic.\n\nScrapy felt like overkill. I didn't need a framework with spiders and pipelines and middleware. I needed a tool I could call with parameters and forget about.\n\nSo I built litecrawl: one async function that manages its own state in SQLite.\n\nThe idea is you spin up a separate instance per use case. Each gets its own DB file, its own cron job, its own config. No orchestration, no shared state, no central scheduler. Just isolated, idempotent processes that pick up where they left off.\n\n    from litecrawl import litecrawl\n    \n    litecrawl(\n        sqlite_path=\"council.db\",\n        start_urls=[\"https://example.com/minutes\"],\n        include_patterns=[r\"https://example\\.com/minutes/\\d+\"],\n        n_concurrent=5,\n        fresh_factor=0.5\n    )\n\nIt handles the boring-but-important stuff:\n\n* Adaptive scheduling - backs off for static pages, speeds up for frequently changing content\n* Crash recovery - claims pages with row-level locking, releases stalled jobs automatically\n* Content hashing - only flags pages as \"fresh\" when something actually changed\n* SSRF protection - validates all resolved IPs, not just the first one\n* robots.txt - cached per domain with async fetching\n* Downloads - catches PDFs/ZIPs that trigger downloads instead of navigation\n\nDesigned to run via cron wrapped in `timeout`. If it crashes or gets killed, the next run continues where it left off.\n\n`pip install litecrawl`\n\nGitHub: [https://github.com/jakobmwang/litecrawl](https://github.com/jakobmwang/litecrawl)\n\nBuilt this for various data projects. Would love feedback - especially if you spot edge cases I haven't considered.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qt7pju/litecrawl_minimal_async_crawler_for_targeted/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o34s1j6",
          "author": "WindScripter",
          "text": "this looks awesome. gonna give it a try and let you know how it works, i generally have a similar use case as you",
          "score": 1,
          "created_utc": "2026-02-02 10:17:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36c9no",
              "author": "Little_Ant_3459",
              "text": "Cool! Looking forward to hear from you.",
              "score": 1,
              "created_utc": "2026-02-02 16:09:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o34y20u",
          "author": "renegat0x0",
          "text": " \\- I am not an expert, but it looks as if it used \"functional\" programming approach, but where everything is in a one file. I don't like projects where everything is in one file\n\n \\- was it vibe coded? I do not really see any relevant commit history",
          "score": 1,
          "created_utc": "2026-02-02 11:12:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36d4c9",
              "author": "Little_Ant_3459",
              "text": "A valid criticism. I am beginning to get use cases where a few link-related add-ons would be handy. When I get around to add that functionality, I should split it into more than one file for readability.\n\nAnd while I use LLM's intensively in most of my work, this one I actually hand-held more than usual, since they didn't understand the paradigm/approach.",
              "score": 2,
              "created_utc": "2026-02-02 16:13:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qpdyt6",
      "title": "Internal Google Maps API endpoints",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qpdyt6/internal_google_maps_api_endpoints/",
      "author": "LouisDeconinck",
      "created_utc": "2026-01-28 15:11:48",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.71,
      "text": "I build a scraper that extracts place IDs from the protobuf tiling api. Now I would like to fetch details from each place using this place id (I also have the S2 tile id). Are rhere any good endpoints to do this with?",
      "is_original_content": false,
      "link_flair_text": "Scaling up üöÄ",
      "permalink": "https://reddit.com/r/webscraping/comments/1qpdyt6/internal_google_maps_api_endpoints/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qsmo66",
      "title": "Monthly Self-Promotion - February 2026",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qsmo66/monthly_selfpromotion_february_2026/",
      "author": "AutoModerator",
      "created_utc": "2026-02-01 03:00:42",
      "score": 4,
      "num_comments": 7,
      "upvote_ratio": 0.76,
      "text": "Hello and howdy, digital miners of¬†r/webscraping!\n\nThe moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!\n\n* Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?\n* Maybe you've got a ground-breaking product in need of some intrepid testers?\n* Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?\n* Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?\n\nWell, this is your time to shine and shout from the digital rooftops - Welcome to your haven!\n\nJust a friendly reminder, we like to keep all our self-promotion in one handy place, so any promotional posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qsmo66/monthly_selfpromotion_february_2026/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2x5kjy",
          "author": "HLCYSWAP",
          "text": "ETL (scraping), databasing, ML training AI (captcha bypass, voice, transformers - LLMs), standard desktop apps like DAWs as seen on my github. my past successes here:\n\n[https://github.com/matthew-fornear](https://github.com/matthew-fornear)\n\nopen to short or long work. you can reach me via reddit, x (https://www.x.com/fixitorgotojail) or discord",
          "score": 4,
          "created_utc": "2026-02-01 05:05:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31w4uw",
          "author": "malvads",
          "text": "Hi! After some time, researching into complex crawlers for web-to-llm data, I created [https://github.com/malvads/mojo](https://github.com/malvads/mojo). Mojo is an extremely fast C++ web scraper with multi-depth capabilities ready to ingest data into RAG-like systems, it scans entire websites and converts them to Markdown format. It also downloads artifacts like PDFs/others . It‚Äôs incredibly fast, so it can run on AWS Lambdas or any Cloud Provider, and it supports rendering (via Chrome CDP, if --render flag). Additionally, it has an internal reverse proxy with proxy rotation to facilitate scraping (when CDP, to not relaunch chrome instances, this is not for normal HTTP reqs). Mojo can scrape full websites in seconds while using very low CPU and RAM. Precompiled binaries are also available.",
          "score": 2,
          "created_utc": "2026-02-01 22:27:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3079iu",
          "author": "LessBadger4273",
          "text": "Giving out 100M+ Shopee and Aliexpress product page dataset for free for educational purposes. Reach out to Octaprice.com to learn how to get it",
          "score": 1,
          "created_utc": "2026-02-01 17:39:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34x6ou",
          "author": "Dear-Cable-5339",
          "text": "If you‚Äôre tired of managing proxy pools and fighting blocks,¬†**Crawlbase Smart Proxy**¬†might help.\n\nIt lets you fetch public pages at scale while we handle IP rotation, retries, geo-targeting, and CAPTCHAs in the background.\n\n**Simple pricing:**  \n‚Ä¢ TCP / non-JS request =¬†**1 credit**  \n‚Ä¢ JavaScript-rendered request =¬†**2 credits**\n\nFree credits available if you want to test it out:  \nüëâ¬†[https://crawlbase.com/?s=5qGcKLCR](https://crawlbase.com/?s=5qGcKLCR)",
          "score": 1,
          "created_utc": "2026-02-02 11:04:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34ynae",
          "author": "renegat0x0",
          "text": "I am still scraping Internet links meta.\n\n[https://github.com/rumca-js/Internet-Places-Database](https://github.com/rumca-js/Internet-Places-Database)",
          "score": 1,
          "created_utc": "2026-02-02 11:17:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38aiog",
          "author": "PyScrapePro",
          "text": "Hello everyone üëã  \nI‚Äôm a **Python Web Scraping & Backend Engineer** specializing in **production-grade scraping systems** and data pipelines.\n\nI work mostly on projects where scraping is not a one-off script, but a **system that needs to run reliably under real constraints**.\n\n**What I focus on:**  \nüõ°Ô∏è scraping protected and JS-heavy websites (Cloudflare, modern anti-bot systems)  \n‚ö° high-performance pipelines (asyncio + multiprocessing, 5√ó speedups)  \nüìä stability and observability (95‚Äì99% successful runs, retries, graceful degradation)  \nüîÅ incremental updates and cost-aware scraping  \nüß† backend APIs around scraped data (FastAPI, auth, rate limits, PostgreSQL)\n\nMost of my recent work is under **NDA**, so I can‚Äôt share code or client names, but I *can* discuss:\n\n* architecture decisions\n* performance trade-offs\n* reliability strategies\n* long-term maintenance of scraping systems\n\nI also document my engineering approach and long-form technical articles here:  \nüëâ [https://pyscrapepro.netlify.app/](https://pyscrapepro.netlify.app/)\n\n**What I‚Äôm looking for:**\n\n* high-impact, **well-paid remote projects**\n* long-term contracts or full-time roles\n* teams that treat scraping as engineering, not ‚Äúquick scripts‚Äù\n\nIf you‚Äôre building or maintaining **serious scraping infrastructure** and need someone who cares about **stability, performance, and ownership**, feel free to reach out or comment here.\n\nHappy scraping üöÄ",
          "score": 1,
          "created_utc": "2026-02-02 21:34:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrdd1y",
      "title": "Asking for advice and tips.",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qrdd1y/asking_for_advice_and_tips/",
      "author": "LowDiscount6694",
      "created_utc": "2026-01-30 18:18:10",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 0.67,
      "text": "Context: former software engineer and data analyst.\n\nGood morning to all of my master,\n\nI would like to seek an advice how to make become a better web scraper. I am using python selenium web scraping, pandas for data manipulation and third party vendor. I am not comfortable to my scraping skills I used to create a scraping in first quarter of last year. And currently I've been able to apply to a company. Since they hiring for web scraping engineer. I am confident that I will passed the exercises. Since I got the asking data. Now, what do I need to make my scraping become undetectable? I used the residential proxies provided Also the captcha bypass. I just wanted to learn how to apply the fingerprinting and etc. because I wanted to got hired so I can pay house bills. :( anything advice that you want to share.\n\nThank you for listening to me.",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qrdd1y/asking_for_advice_and_tips/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2nesmn",
          "author": "Active_Winner",
          "text": "Use playwright stealth, seleniumbase and drissionpage",
          "score": 1,
          "created_utc": "2026-01-30 18:34:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p1dg4",
          "author": "johnster929",
          "text": "Zendriver kept me under the radar, no need for proxy IP service",
          "score": 1,
          "created_utc": "2026-01-30 23:12:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}