{
  "metadata": {
    "last_updated": "2026-01-03 12:29:58",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 21,
    "total_comments": 113,
    "file_size_bytes": 129243
  },
  "items": [
    {
      "id": "1pvobl0",
      "title": "Why do people think web scraping is a free service?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pvobl0/why_do_people_think_web_scraping_is_a_free_service/",
      "author": "unstopablex5",
      "created_utc": "2025-12-25 22:17:53",
      "score": 96,
      "num_comments": 23,
      "upvote_ratio": 0.91,
      "text": "I‚Äôve been on this sub for years, and I‚Äôm consistently surprised by how many posts ask for basic scraping help without any prior effort.\n\nIt‚Äôs rarely questions like ‚Äúhow do I avoid advanced fingerprinting or bot detection.‚Äù Instead, it‚Äôs almost always ‚Äúhow do I scrape this static HTML page.‚Äù These are problems that have been answered hundreds of times and are easily searchable.\n\nScraping can be complex, but not every problem is. When someone hasn‚Äôt tried searching past threads, Googling, or even using ChatGPT before posting, it lowers the overall quality of discussion here.\n\nI‚Äôm not saying beginners shouldn‚Äôt ask questions. But low effort questions with no context or attempted solution shouldn‚Äôt be the norm.\n\nWhat‚Äôs more frustrating are requests that implicitly expect a full pipeline. Scraping, data cleaning, storage, and reliability are not a single snippet of code. That is a product, not a quick favor.\n\nIf someone needs that level of work, the options are to invest time into learning or pay someone who already has the expertise. **Scraping is not a trivial skill. It borrows heavily from data engineering and software engineering, and treating it as free labor undervalues the work involved.**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1pvobl0/why_do_people_think_web_scraping_is_a_free_service/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nvxn469",
          "author": "cgoldberg",
          "text": "This is nothing specific to web scraping... It's just the state of technical and programming questions in general, and it always has been. Visit any programming community or forum anywhere on the internet and it's full of newbies with misconceptions and unrealistic expectations asking questions that have been answered a thousand times before, and a bunch of frustrated veterans shouting \"RTFM\" or telling them what they think is actually wrong.",
          "score": 41,
          "created_utc": "2025-12-25 22:27:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxp0x6",
              "author": "unstopablex5",
              "text": "good point but it feels more egregious for web scraping since the offenders are usually other developers and engineers trivializing what we do",
              "score": 3,
              "created_utc": "2025-12-25 22:39:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvywdan",
              "author": "Exp5000",
              "text": "It's like this in life as well. At every company I've worked with the help desk is full of these types that couldn't be bothered to troubleshoot and immediately escalate the easiest of issues. It's just human nature. There are those who need zero direction and will succeed on their own. Then there's those who just couldn't make it very far without guidance.",
              "score": 1,
              "created_utc": "2025-12-26 03:26:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvxmx16",
          "author": "divided_capture_bro",
          "text": "Can you please find all open ports on AWS for me?¬†",
          "score": 16,
          "created_utc": "2025-12-25 22:25:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxorvn",
              "author": "nameless_pattern",
              "text": "Here you go¬†\n\n\n\n\nlocalhost:3000",
              "score": 6,
              "created_utc": "2025-12-25 22:37:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvxxe7h",
          "author": "MonsieurFizzle",
          "text": "Not gonna lie, this also feels full of irony given the subject matter.",
          "score": 6,
          "created_utc": "2025-12-25 23:33:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyifvg",
          "author": "99ducks",
          "text": "This field seems to attract a lot of novice developers who just aren't good yet at solving problems or asking good questions.",
          "score": 6,
          "created_utc": "2025-12-26 01:51:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1ns2y",
          "author": "hasdata_com",
          "text": "Tbh, helping isn't hard. Most folks replying here are happy to help. The real problem is when someone asks to scrape something super abstract like \"an online store\" and that's it. No site, no details, no constraints. At that point it's like‚Ä¶ help with what, exactly? And how?",
          "score": 3,
          "created_utc": "2025-12-26 16:40:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxozbz",
          "author": "nameless_pattern",
          "text": "The same people who don't think to look at the wiki and don't think to Google it themselves and don't think to learn the skill on their own.¬†\n\n\nAnd there's more people like this now because of chat bots, although strangely they don't always just ask the chat bots first either",
          "score": 2,
          "created_utc": "2025-12-25 22:38:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw07jy7",
          "author": "v_maria",
          "text": "\"idea people\"",
          "score": 2,
          "created_utc": "2025-12-26 10:33:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8jk4l",
          "author": "dot_py",
          "text": "Never heard of RTFM? Noob being mad at noobs... the effort you put in before a question likely has someone thinking the same m8. Humble thy self lol",
          "score": 2,
          "created_utc": "2025-12-27 19:14:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwbdxkx",
              "author": "unstopablex5",
              "text": "congrats, you won the award for taking the most Tylenol ever in 1 day. Your prize is in the mail!\n\nEdit: A kill tony fan. yep it all tracks",
              "score": 1,
              "created_utc": "2025-12-28 05:04:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwq3c4x",
          "author": "amemingfullife",
          "text": "It‚Äôs the same part of the brain that makes your boss think that he can ‚Äòvibe code an app‚Äô and have it work in production.",
          "score": 2,
          "created_utc": "2025-12-30 13:09:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxtahz",
          "author": "Dorkits",
          "text": "It's hard bro, unfortunately.",
          "score": 1,
          "created_utc": "2025-12-25 23:06:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy3lie",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2025-12-26 00:13:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvy51vt",
              "author": "webscraping-ModTeam",
              "text": "üëî Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
              "score": 1,
              "created_utc": "2025-12-26 00:22:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvy7l0k",
          "author": "node77",
          "text": "I agree, you can almost make it a consulting gig. I learned it by my self with scrappy. I have used PowerShell to do some work,, with just regular expressions.",
          "score": 1,
          "created_utc": "2025-12-26 00:37:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzsoug",
          "author": "Virsenas",
          "text": "If you see usernames in Word_Word_4numbers that bother you with this topic, then ignore them, because they are likely 99% a bot. Unless you have a real example of your topic and can give a link to a comment/post, then I think people could help discuss about it.",
          "score": 1,
          "created_utc": "2025-12-26 07:58:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3sq56",
          "author": "VastEnergy4724",
          "text": "I don't know why I see this post recommended. But I built a functioning web scraper for 2 products and 5+ shops, because I can't buy TCG products at msrp prices. Had to cancel Amazon because it's not worth it I got the keep shopping challenge too often. I don't use proxies. I startet with cycles every few seconds but switched now to minutes. Btw all with chatgpt,i know nextjs and js for web developing but didn't use it for a while.",
          "score": 1,
          "created_utc": "2025-12-26 23:39:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiezr8",
          "author": "crowpng",
          "text": "Basic HTML scraping is easy to demo, which sets false expectations.. Reliability, scale, and maintenance are where the real work starts. That gap is what most people miss.",
          "score": 1,
          "created_utc": "2025-12-29 07:54:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvz2dm4",
          "author": "oloshoslut12",
          "text": "Would be free if search providers didnt have strict anti bot methodologies which created a whole market for web scraping",
          "score": 0,
          "created_utc": "2025-12-26 04:09:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzl22m",
          "author": "Haikal019",
          "text": "people like you are the reason stack overflow receive less visit, smart and expert yet underplay new joiner/begineer engineer. what is high quality to you doesnt mean high quality to others. we better respect new joiner as it show scraping is for everyone and be glad this community is growing",
          "score": -2,
          "created_utc": "2025-12-26 06:43:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1rrg5",
              "author": "unstopablex5",
              "text": "Stack overflow receives less visits because you can ask chatgpt, claude or simply google any question you have. If you are going to stack overflow to ask simplistic questions at this point you are not cut out for this field",
              "score": 3,
              "created_utc": "2025-12-26 17:02:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwk7xw",
      "title": "I deployed a side project scraping 5000 dispensaries.",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pwk7xw/i_deployed_a_side_project_scraping_5000/",
      "author": "Round_Method_5140",
      "created_utc": "2025-12-27 00:53:28",
      "score": 38,
      "num_comments": 16,
      "upvote_ratio": 0.93,
      "text": "This is a project where I learned some basics through self teaching and generative assistance from Antigravity. I started by sniffing network on their web pages. Location search, product search, etc. It was all there. Next was understanding the most lightweight and efficient way to get information. Using curl cffi was able to directly call the endpoints repetitively. Next was refinement. How can I capture all stores with the least number of calls? I'll look to incorporate stores and products from iheartjane next.\n\nEdit: I forgot. https://1-zip.com",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1pwk7xw/i_deployed_a_side_project_scraping_5000/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nw49r1n",
          "author": "Grouchy_Brain_1641",
          "text": "The second time I did it I found the one json file for all dispensaries in Weedmap's head. They called me and said hey your web site looks sort of like ours. I said no it looks exactly like yours. Have you ever seen that variable in your head? He was like oh shit. Clonesbayarea is just my daily selenium scrape.",
          "score": 23,
          "created_utc": "2025-12-27 01:22:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4ahxc",
              "author": "Round_Method_5140",
              "text": "Awesome thanks for sharing. I did not look into weedmaps yet. Did they ever fix the information being exposed?",
              "score": 4,
              "created_utc": "2025-12-27 01:27:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw52gvn",
                  "author": "Grouchy_Brain_1641",
                  "text": "oh ya they fixed it soon after.",
                  "score": 1,
                  "created_utc": "2025-12-27 04:32:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw58wvj",
          "author": "AdministrativeHost15",
          "text": "Scraping the resin from 5000 dispensaries you should have accumulated some good stuff.",
          "score": 14,
          "created_utc": "2025-12-27 05:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw77ytx",
          "author": "noodlesallaround",
          "text": "This is cool.",
          "score": 2,
          "created_utc": "2025-12-27 15:12:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9fjlv",
          "author": "JohnnyOmmm",
          "text": "What‚Äôs this for, do they have prices or quantity availability up or something?",
          "score": 2,
          "created_utc": "2025-12-27 22:06:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdufhb",
              "author": "Round_Method_5140",
              "text": "The main use case is for finding products near you, without having to go to multiple different dispensary websites. \n\nAlso you can correlate the same product to different dispensaries and compare prices.\n\nYou can view labs (terpenes etc), how many in stock, specials. Filter out low quality bulk and shake.",
              "score": 2,
              "created_utc": "2025-12-28 16:27:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwegbiw",
                  "author": "JohnnyOmmm",
                  "text": "That‚Äôs awesome broo, I wish they had something similar for the Marshalls Ross Burlington conglomerate cause I‚Äôm tired of driving to each one multiple times a day for work ü§£",
                  "score": 3,
                  "created_utc": "2025-12-28 18:15:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwq7j92",
                  "author": "mortalhal",
                  "text": "Where do you find the lab reports? Many of the listings don‚Äôt include the terpene breakdown",
                  "score": 1,
                  "created_utc": "2025-12-30 13:34:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwbouw8",
          "author": "humble_cyrus",
          "text": "Dude, hella tight.",
          "score": 2,
          "created_utc": "2025-12-28 06:31:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwh92nv",
          "author": "ajawnoutofwater",
          "text": "Excellent name",
          "score": 2,
          "created_utc": "2025-12-29 02:53:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx61y9v",
          "author": "Long-Stop-5732",
          "text": "Fire bro, I‚Äôm in the weed biz in LA this could make a lot of money",
          "score": 1,
          "created_utc": "2026-01-02 00:13:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzit0y",
      "title": "Technical SEO baseline that moved DA 0 to 28 in 90 days",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pzit0y/technical_seo_baseline_that_moved_da_0_to_28_in/",
      "author": "Alive_Helicopter_597",
      "created_utc": "2025-12-30 13:59:05",
      "score": 20,
      "num_comments": 12,
      "upvote_ratio": 0.83,
      "text": "Ran controlled technical SEO experiment across 12 new domains to isolate what actually moves domain authority fast versus SEO mythology. All sites launched same week with identical technical setup. Ninety days later average DA reached 28.3 with tightest performing at DA 31 and weakest at DA 24. The test methodology controlled variables to isolate technical SEO impact. All 12 sites used identical hosting (Cloudflare + Webflow), same site structure (homepage, 8 service pages, blog), identical technical optimization (Core Web Vitals, schema markup, XML sitemaps), and same backlink foundation (200 directory submissions via [directory submission service](http://getmorebacklinks.org/) for consistency).\n\nWeek one established technical baseline. Configured Cloudflare for speed optimization achieving 1.2s average load time. Implemented proper schema markup for Organization, Service, and Article types. Created logical URL structure with proper hierarchy. Fixed all technical issues in Search Console. Submitted to directories establishing initial backlink profile.\n\nDays 8-30 focused on content foundation. Published 8 foundational pages targeting primary keywords. Ensured proper internal linking architecture with logical topical clusters. Optimized images for speed without quality loss. All sites achieved 95+ PageSpeed scores mobile and desktop. Average DA reached 12.7 by day 30.\n\nDays 31-60 showed backlink indexing phase. Directory submissions started appearing in Search Console. Published 12 additional blog posts across all sites. Content from days 8-30 started ranking pages 2-3. Technical monitoring showed zero crawl errors. Average DA reached 21.4 by day 60.\n\nDays 61-90 demonstrated compound technical effects. Earlier content moved to page one for longtail terms. Added FAQ schema generating featured snippets for 6 sites. Implemented breadcrumb navigation improving crawlability. Optimized Core Web Vitals achieving perfect scores. Average DA reached 28.3 by day 90.\n\nPerformance variation analysis showed tight clustering. Highest performing site hit DA 31, lowest reached DA 24, with 8 sites in DA 26-29 range. The 7-point spread suggests technical baseline plus directory backlinks produces consistent results when variables are controlled. Individual content quality explained most variation.\n\nTechnical factors that correlated with higher DA were perfect Core Web Vitals scores (all three metrics green), proper schema implementation generating rich results, zero crawl errors in Search Console, logical internal linking structure, and fast initial backlink indexing within first 20 days. What surprisingly didn't impact DA significantly in 90-day window was exact hosting choice beyond basic speed, specific CMS platform (tested Webflow, Wordpress, custom), design quality or UX refinement, or social media presence. These may matter long-term but didn't affect initial DA climb.\n\nThe technical SEO lesson is new sites can reach DA 25-30 in 90 days with proper baseline. This provides foundation for content to rank and guest posting to succeed. Most sites fail because they skip technical foundation and jump to content, then wonder why nothing ranks despite \"good content.\" Cost efficiency for agencies is clear. Each site cost $127 directory service, $60 for 3 months hosting, $90 for tools (Search Console free, Ahrefs trial). Total $277 per site to establish DA 25-30 baseline. That foundation makes all subsequent SEO work more effective.\n\nFor technical SEO practitioners the recommendation is establish perfect technical baseline first (week 1), layer in directory backlinks for initial authority (weeks 2-4), publish content targeting keywords while maintaining technical excellence (weeks 5-12), and monitor Search Console religiously fixing any crawl issues immediately. The broader strategic point is technical SEO isn't glamorous but it's the foundation everything else builds on. Sites with perfect technical setup and mediocre content outperform sites with amazing content and poor technical foundation. Get the boring baseline right first.  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1pzit0y/technical_seo_baseline_that_moved_da_0_to_28_in/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwqs35v",
          "author": "mantepbanget",
          "text": "bullshit and spam",
          "score": 2,
          "created_utc": "2025-12-30 15:27:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqurgy",
          "author": "RandomPantsAppear",
          "text": "I am calling shenanigans on this post. \n\nI find it wildly unlikely that you found any real amount of success with directories. This is not 2004 anymore. \n\nThey‚Äôre not technically worthless links(close), but there‚Äôs not a godamn way in hell they‚Äôre worth anything like $127.\n\nThis seems like a lowkey advertisement looking for DMs",
          "score": 2,
          "created_utc": "2025-12-30 15:40:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrz207",
              "author": "99ducks",
              "text": "OP seems to block these types of posts from being visible on their account, but if you google \"Alive_Helicopter_597 directory submission service\" you can see they spam this pretty regularly.",
              "score": 2,
              "created_utc": "2025-12-30 18:48:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws29mq",
                  "author": "RandomPantsAppear",
                  "text": "I reported op. But smart spammer.",
                  "score": 2,
                  "created_utc": "2025-12-30 19:02:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrzsza",
              "author": "jamesmundy",
              "text": "I used a directory submission service recently and DR when up +20 in one week, genuinely. I got quite a lot of traffic as the submission sites were highly relevant.\nWhether this leads to justifying the expense I don‚Äôt know but it doesn‚Äôt feel worthless",
              "score": 1,
              "created_utc": "2025-12-30 18:51:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws1lby",
                  "author": "RandomPantsAppear",
                  "text": "In some insanely niche directories it‚Äôs possible, but even then paying $120 per directory link is insane. \n\nDirect traffic seems even more unlikely than positive SEO results from directory links. \n\nLike tbh I would consider bot traffic to be more likely than real traffic. \n\nI‚Äôve done SEO for a long time, at one point ran one of the most popular gray/blackhat SEO blogs out there, spoke at conferences from my late teen to early 20s years. I would prefer outright link spam to directory submissions - I do know what I‚Äôm talking about here. \n\nMost directory submission services are outright scams. The ones that do work, the results are likely to be temporary. You‚Äôre normally looking at a domain with a couple aged domains redirected back at it, supported by a few other thin directory sites, that are supported by outright link spam and other filthy links. It‚Äôs link juice laundering, and it‚Äôs fine but most site networks fall eventually. Ones that are part of paid submission packages even more rapidly.\n\nThe way it works is that normally the lowest tier of supporting site will work for a short time promoting the cleaner directory. But one by one, they get penalized and the link juice stops flowing. So for the better directory or thin site, it peaks and then has a slow fall while they build a new network of sites.",
                  "score": 1,
                  "created_utc": "2025-12-30 18:59:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwqcsku",
          "author": "Embarrassed_Poem9556",
          "text": "What schema types did you find generated featured snippets most consistently? FAQ schema seems obvious but wondering if you tested others like HowTo or Product schema",
          "score": 1,
          "created_utc": "2025-12-30 14:05:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqf4vz",
              "author": "Alive_Helicopter_597",
              "text": "What schema types did you find generated featured snippets most consistently? FAQ schema seems obvious but wondering if you tested others like HowTo or Product schema",
              "score": 0,
              "created_utc": "2025-12-30 14:19:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqe155",
          "author": "WearySpecialist7660",
          "text": "\nyou mentioned directory backlinks started appearing in Search Console around days 31-60. Did you see any immediate DA movement or did it take time after indexing for DA to actually update?",
          "score": 1,
          "created_utc": "2025-12-30 14:12:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqgeri",
              "author": "Alive_Helicopter_597",
              "text": "There's definitely a lag. Links appeared in Search Console around day 20-25, but DA didn't jump until day 35-40. So roughly 2 weeks between indexing and DA update.   \nIt happens in waves - links index ‚Üí wait 10-15 days ‚Üí DA bumps up. Patience is key",
              "score": 1,
              "created_utc": "2025-12-30 14:26:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwrr4ab",
          "author": "RobSm",
          "text": "'Most sites fail because they skip technical foundation and jump to content, then wonder why nothing ranks despite \"good content.\"' - how do you know? You did not include a site in the mix which did not have good technical foundation, so there is no comparison here.",
          "score": 1,
          "created_utc": "2025-12-30 18:11:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqc4tf",
          "author": "Ok_Cauliflower5526",
          "text": "\n\nDA 28 in 90 days is solid especially with consistency across all 12 sites. The tight clustering (DA 24-31) really proves your point about technical baseline mattering more than people think.",
          "score": 0,
          "created_utc": "2025-12-30 14:01:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0fqza",
      "title": "Deploying scrapers",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q0fqza/deploying_scrapers/",
      "author": "async-lambda",
      "created_utc": "2025-12-31 15:34:08",
      "score": 13,
      "num_comments": 28,
      "upvote_ratio": 0.85,
      "text": "I know this is, asking a question in very bad faith. I'm a student and I dont have money to spend.\n\nIs there a way I can deploy a headless browser for free? what i mean to ask is, having the convenience to hit an endpoint, and for it to run the scraper and show me results. Its just for personal use. Any services that offer this- or have a generous free tier?\n\nI can learn/am willing to learn stacks, am familiar with most web driver runners selenium/scrapy/playwright/cypress/puppeteer.\n\nThanks for reading \n\nEdit: tasks that I require are very minimal, 2-3 requests per day, with a few button clicks ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q0fqza/deploying_scrapers/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwxgh41",
          "author": "v_maria",
          "text": "Nothing is free. You can host on your own computer for the price of electricity",
          "score": 14,
          "created_utc": "2025-12-31 15:41:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxgijg",
          "author": "No-Appointment9068",
          "text": "Pick up an old laptop for free or get a raspberry pi or something?",
          "score": 7,
          "created_utc": "2025-12-31 15:41:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxmr8e",
          "author": "divided_capture_bro",
          "text": "GitHub Actions with public repo.",
          "score": 4,
          "created_utc": "2025-12-31 16:12:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxkdeh",
          "author": "816shows",
          "text": "Don't spin up a full ec2 instance for something you run 2-3 times a day.  Instead build an AWS lambda that you can either trigger on demand via an API gateway call or tie to an Eventbridge schedule. The container that you deploy in the lambda is quite simple, you don't need a lot of sophisticated layers just to have selenium and your script to work.  Hit me up if you want a sample Dockerfile and config details.",
          "score": 3,
          "created_utc": "2025-12-31 16:00:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy126g",
              "author": "Ordinary-Coconut7752",
              "text": "hey, would you mind sending me your docker and config files? Wanna try to deploy my scrapers on Lambda",
              "score": 1,
              "created_utc": "2025-12-31 17:23:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwya8sh",
                  "author": "816shows",
                  "text": "Done!",
                  "score": 1,
                  "created_utc": "2025-12-31 18:08:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwyexmy",
              "author": "73tada",
              "text": "Same here, please! I need an excuse to learn lambda!",
              "score": 1,
              "created_utc": "2025-12-31 18:32:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwypzg5",
                  "author": "816shows",
                  "text": "Sure thing - I added some notes on the configuration and for the uninitiated, you will have to create the new function based on a container image. This means you'll also have to get the Elastic Container Registry setup, and the rest of the details are outlined in this repo:  \n  \n[https://github.com/816shows/public/tree/main/selenium-lambda](https://github.com/816shows/public/tree/main/selenium-lambda)",
                  "score": 2,
                  "created_utc": "2025-12-31 19:28:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwyi4mx",
              "author": "pachinkomachine101",
              "text": "This sounds interesting, could you share it with me too? I'd love to study your setup!",
              "score": 1,
              "created_utc": "2025-12-31 18:48:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxino6",
          "author": "yangshunz",
          "text": "You can run Puppeteer on Vercel functions",
          "score": 2,
          "created_utc": "2025-12-31 15:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxreyz",
          "author": "RandomPantsAppear",
          "text": "AWS has a free tier for EC2, micro instances I believe.¬†\n\nFor very low cost you can also use a lambda outside of a vpc I believe (to dodge nat and internet gateway costs). Highly recommend Zappa for flask/django. Don‚Äôt forget to protect those endpoints.¬†\n\nAnother super low cost setup that I haven‚Äôt implemented but could work is an ec2 trigger on S3 uploads. Make the file your url list. Make the upload trigger a script that launches a small fargate instance that shuts down when it‚Äôs done. 0.25 vcpu and 256m of ram. That will cost you less than 2 cents per hour.¬†",
          "score": 2,
          "created_utc": "2025-12-31 16:35:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxhyd3",
          "author": "goranculibrk",
          "text": "Amazon should have free tier with some ec2 micro instance. Maybe look into that?",
          "score": 1,
          "created_utc": "2025-12-31 15:48:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxv4v6",
          "author": "Low-Clerk-3419",
          "text": "You can easily deploy something like that in vercel functions, fly.io, railway etc. but you have to keep in mind those free tiers are not meant for scraping; it will be very slow and limited experience.",
          "score": 1,
          "created_utc": "2025-12-31 16:54:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy61v9",
          "author": "Daniel_triathlete",
          "text": "Can‚Äôt you handle this task with JDownloader and an old laptop?",
          "score": 1,
          "created_utc": "2025-12-31 17:48:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy70bv",
          "author": "Intelligent_Area_135",
          "text": "Just wrap an endpoint around it and run it on your computer",
          "score": 1,
          "created_utc": "2025-12-31 17:52:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy79vv",
              "author": "Intelligent_Area_135",
              "text": "I use express js as the api around my web scraper, but if you are concerned about your ip getting banned or something, I have deployed to gcp and it‚Äôs very cheap but I think there might be better options",
              "score": 1,
              "created_utc": "2025-12-31 17:54:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwybc31",
          "author": "_i3urnsy_",
          "text": "I think GitHub Actions can do this for free if you are cool with the repo being public",
          "score": 1,
          "created_utc": "2025-12-31 18:14:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwycbfh",
          "author": "automationexperts",
          "text": "Try here https://www.pythonanywhere.com/",
          "score": 1,
          "created_utc": "2025-12-31 18:18:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx104rn",
          "author": "hatemjaber",
          "text": "Oracle has a free tier, 4 CPU and 24 GB ram. You can create a couple VMs.",
          "score": 1,
          "created_utc": "2026-01-01 03:40:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6tw33",
          "author": "Rorschache00714",
          "text": "Github offers a student essentials package with a shit ton of free resources. Look into that if you have a school email you can use.",
          "score": 1,
          "created_utc": "2026-01-02 02:59:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8237q",
          "author": "Training-Dinner3340",
          "text": "Depending on what you‚Äôre doing, Cloudflare Workers + Browser Rendering may get the job done:\nhttps://developers.cloudflare.com/browser-rendering/\n\nFree tier is okay:\nhttps://developers.cloudflare.com/browser-rendering/pricing/\n\nChatGPT is decent at writing Cloudflare worker scripts. Good luck!",
          "score": 1,
          "created_utc": "2026-01-02 08:37:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9kwa4",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-02 15:24:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9rg0u",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-02 15:55:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxbchys",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-02 20:23:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxckia9",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-03 00:09:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdklgv",
          "author": "Foodforbrain101",
          "text": "I could have sworn that GitHub Actions previously had a \na certain amount of free minutes even for private repos. \n\nRegardless, you can also take a look at using Azure DevOps' equivalent, Azure Pipelines, with 1800 minutes free per month, 60 min per run max for private repos but you have to request it (which is fairly easy and quick).\n\nIf you do go with Azure Pipelines, I suggest using the Microsoft Playwright for Python container image for your pipeline. There's ways to make this setup more metadata-driven too, as you can parameterize Azure Pipelines and use the Azure DevOps API to trigger runs, and you can easily tack on Azure Logic Apps (4000 free actions per month) as a simple orchestrator, use any kind of blob or table storage (even Google Drive) to store and fetch your metadata table containing the schedules and info about which scripts to run. Might be overkill for your needs though, but it's honestly one of the easiest and cheapest ways I've found to run headless browsers.",
          "score": 1,
          "created_utc": "2026-01-03 03:38:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxe6d6k",
          "author": "BeigeBolt",
          "text": "What is most money made by scraping",
          "score": 1,
          "created_utc": "2026-01-03 06:08:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1puf7hb",
      "title": "AutoHealing Crawlers/Scrappers",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1puf7hb/autohealing_crawlersscrappers/",
      "author": "Urten",
      "created_utc": "2025-12-24 05:11:11",
      "score": 10,
      "num_comments": 17,
      "upvote_ratio": 0.92,
      "text": "Hello, Just as the title - has anyone ever build any autohealing scrapper, there are few github libraries but they don't seem to be working or inaccurate, if the api changes the scraper breaks. So I want to ask if anyone had any luck building a fully functional autohealing scraper. ",
      "is_original_content": false,
      "link_flair_text": "Scaling up üöÄ",
      "permalink": "https://reddit.com/r/webscraping/comments/1puf7hb/autohealing_crawlersscrappers/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nvpz913",
          "author": "RandomPantsAppear",
          "text": "I‚Äôve had luck doing it, but the code was kind of a learning experience so it‚Äôs not clean. \n\nThe hardest part is getting it to play well with multiple page types, distributed systems, and making sure that your updated selectors work on all matching pages and not just the one that it was broken on. \n\nThe solutions are URL routers, locking systems for config updates, cloud hosted config, and temporary caching of results so you can quickly pull multiple recent example of that url option.",
          "score": 3,
          "created_utc": "2025-12-24 14:27:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvret73",
              "author": "Urten",
              "text": "\\`\\`\\`The hardest part is getting it to play well with multiple page types, distributed systems, and making sure that your updated selectors work on all matching pages and not just the one that it was broken on.\\`\\`\\`\n\nwait so you will update the selectors manually? I don't get it",
              "score": 1,
              "created_utc": "2025-12-24 19:08:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvrezcm",
                  "author": "RandomPantsAppear",
                  "text": "No, the way it heals is by detecting failures and rewriting its own rules and selectors, saving them to a config and sharing it with the other scrapers",
                  "score": 1,
                  "created_utc": "2025-12-24 19:09:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvr5c87",
          "author": "viciousDellicious",
          "text": "yes, if the crawlers cant extract the data anymore (signal triggered by crawl volume trends), then my service go to previous crawls check the data gotten there and see if its still in the markup, if it is not then its probably blocked for which it does some fuzzing with several waf bypassing techniques until it sees one of the old datapoints or run out of techniques.\n\n\nif the change is just the security then this self healing will update the profile(headers, fingerprints, etc) for that domain and go on.\nif the issue is not on the security side but that the markup changed i will do reverse lookups on the data, to find the html node containing them, once i am at the lowest level node i traverse parents getting classes or id's until i get a unique selector for my data, i retest this with the other urls/expected datapoints, if they all work, then the service updates the list of selectors.\nif this fails, it uses ai to find a selector for the value i have.",
          "score": 3,
          "created_utc": "2025-12-24 18:16:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrdqfo",
              "author": "Urten",
              "text": "Hoo lee shit, that's genius. This seems like a viable solution. If you don't mind, can you share what kind of tools you are using for this workflow, i kinda have slight idea, But i wanna have a clear picture on this method.",
              "score": 2,
              "created_utc": "2025-12-24 19:02:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvre5i9",
                  "author": "viciousDellicious",
                  "text": "its all in golang and a wrapper around ai tools (claude/chatgpt/ollama) based on volume, budget and urgency we change the model",
                  "score": 1,
                  "created_utc": "2025-12-24 19:04:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pz1zvp",
      "title": "Scraping reddit?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pz1zvp/scraping_reddit/",
      "author": "AdhesivenessEven7287",
      "created_utc": "2025-12-29 23:39:13",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 0.91,
      "text": "Over time I save up pages of articles and comments I think will be interesting. But I've not gotten around to it yet. \n\nHow can I have the links but easily download the page? Baring in mind to view all comments I need to scroll down the page. ",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1pz1zvp/scraping_reddit/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwmyser",
          "author": "g4m3r1",
          "text": "If you just want to scrape the actual content of the reddit post then its quite easy. Just add .json at the end of the URL and reddit will return the content of the post + all comments as beautifully formatted json.\n\nE.g. Try it with your own post here: [https://www.reddit.com/r/webscraping/comments/1pz1zvp/scraping\\_reddit.json](https://www.reddit.com/r/webscraping/comments/1pz1zvp/scraping_reddit.json)",
          "score": 10,
          "created_utc": "2025-12-29 23:50:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwn0m6n",
              "author": "PresidentHoaks",
              "text": "Lol mobile sends me back to this page, will have to check on my lappy",
              "score": 2,
              "created_utc": "2025-12-30 00:00:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwn3ws2",
                  "author": "g4m3r1",
                  "text": "Never tried it on mobile but on desktop this works fine :).",
                  "score": 1,
                  "created_utc": "2025-12-30 00:18:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pzyqo0",
      "title": "open-source userscript for google map scraper (it works again)",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pzyqo0/opensource_userscript_for_google_map_scraper_it/",
      "author": "Asleep-Patience-3686",
      "created_utc": "2025-12-31 00:33:36",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "I built this script about six months ago, and it worked well until two months ago when it suddenly stopped functioning. I spent the entire night yesterday and finally resolved the issue.\n\n  \nFunctionality:\n\n1. Automatically scroll to load more results\n2. Retrieve email addresses and Plus Codes\n3. Export in more formats\n4. Support all subdomains of Google Maps sites.\n\nChange logs:\n\n1. The collection button cannot be displayed due to the Google Maps UI redesign.\n2. The POI request data cannot be intercepted.\n3. Added logs to assist with debugging.\n\n[https://greasyfork.org/en/scripts/537223-google-map-scraper](https://greasyfork.org/en/scripts/537223-google-map-scraper)\n\nJust enjoy with free and unlimited leads!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1pzyqo0/opensource_userscript_for_google_map_scraper_it/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwvj8to",
          "author": "Training_Hand_1685",
          "text": "Wow, something like this is exactly what I searched Reddit for. So I can search for non-profits and housing programs in different states (that are listed on google maps) with this? For free? Do I need to be technically savvy (know how to web scrape) or can a newbie/end user, use this?",
          "score": 1,
          "created_utc": "2025-12-31 06:52:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0yra9",
              "author": "Asleep-Patience-3686",
              "text": "I set up a GitHub repository. It will guide you on how to use it. [https://github.com/webAutomationLover/google-map-scraper](https://github.com/webAutomationLover/google-map-scraper)",
              "score": 1,
              "created_utc": "2026-01-01 03:30:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1px553f",
      "title": "Legal implications of this sort of scraping",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1px553f/legal_implications_of_this_sort_of_scraping/",
      "author": "artnote1337",
      "created_utc": "2025-12-27 18:41:10",
      "score": 7,
      "num_comments": 14,
      "upvote_ratio": 0.9,
      "text": "So, I'm scraping data from a website that has a paywall on some of its data, BUT the endpoint that returns this data was easily found in the source code and does not require any special cookies besides the ones from a free account. Its data from census from a country that were digitalized, the census itself is public but the way this data is being provided may not be I guess. I'm using proxy, a few accounts and browsers to scrape the data using this found endpoint (respecting 429s). Will/Can I be in trouble? What are your opinion on the moral/ethics in this sort of scraping?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1px553f/legal_implications_of_this_sort_of_scraping/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nw93rmc",
          "author": "Fragrant_Ad3054",
          "text": "If your requests don't affect the site's functionality, and your project isn't for commercial purposes, then you can rest easy.\n\nIf your project is for commercial purposes, then the data must be \"transformed\" in the sense that you can't simply copy and paste its content. For example, using scraped data to create a graph is fine, but scraping and offering the same data to clients without prior transformation exposes you to a \"risk.\" In this case, a lawyer specializing in digital law can draft valuable documents such as terms of service, disclaimers, etc.",
          "score": 9,
          "created_utc": "2025-12-27 21:03:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw94qad",
              "author": "artnote1337",
              "text": "I do normalize the data to a tabular form thats not exactly like the site presents it. I‚Äôm not sure what the client will be doing with this data tho.",
              "score": 3,
              "created_utc": "2025-12-27 21:09:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw94y8w",
                  "author": "Fragrant_Ad3054",
                  "text": "May I ask if you are in the EU zone or outside the EU zone?",
                  "score": 2,
                  "created_utc": "2025-12-27 21:10:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwa7gsh",
          "author": "Informal_Stock_6166",
          "text": "You are fine mate.... most companies don't care or waste the time to go after scrapers now days, they just tidy up their spam/anti bot system and that's it",
          "score": 2,
          "created_utc": "2025-12-28 00:42:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8k8qw",
          "author": "RandomPantsAppear",
          "text": "Anytime you‚Äôre making an account you‚Äôre agreeing to their terms of service. There have been successful court cases specific to using logged in accounts. \n\nThat said, the practice is common and repercussions are extremely rare. Like lottery jackpot amounts of rare. \n\nAs far as ethics go, I can only give mine personally. \n\n* Scrape respectfully, do not slam their servers and degrade service for their real users. \n\n* Use the data in a transformative way. Don‚Äôt just copy the data and remake the site you‚Äôre scraping from. \n\n* Scrape in the least invasive way possible. ie if they have data with 100 results cached and fast, but you figure out you can query 2000, just iterate pages at the 100 and keep hitting that cache. \n\nThis slips by a lot of people, but scrapers that don‚Äôt cause problems for the place you are scraping are infinitely easier to maintain. Doing it right also means that you don‚Äôt end up as a support ticket in their engineering department, where you will have personal attention.\n\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n\nFor your specific purpose, census data I would have zero issues scraping it. It was paid for by the people, the people having access to it is morally right in my view.",
          "score": 3,
          "created_utc": "2025-12-27 19:18:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8lb1r",
              "author": "artnote1337",
              "text": "Yeah, I'm scraping it in a way that if a few 429s appear when requesting I backoff so the actual website and its frontend don't suffer. But a client requested this data (maybe for research purposes?) and I'll get paid. This data is really old, its from the 19th century actually so it's likely research related.",
              "score": 2,
              "created_utc": "2025-12-27 19:23:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw8lz82",
                  "author": "RandomPantsAppear",
                  "text": "If I were you, I would do a little poking and prodding to figure out what exactly the rate limit is, or what a safe request speed is and stick to it. \n\nAnytime a response code is not 200, it‚Äôs getting logged somewhere. It‚Äôs also likely showing up on charts and graphs somewhere.\n\nAlso, it‚Äôs a lot easier to write and maintain the code if you don‚Äôt have to expect 429s and regular retries.",
                  "score": 5,
                  "created_utc": "2025-12-27 19:27:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw931k8",
          "author": "TurbulentSoup5082",
          "text": "How would a website know your scraped data came from their site specifically? Will they include dummy data to catch you out?",
          "score": 1,
          "created_utc": "2025-12-27 20:59:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzhjmn",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pzhjmn/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2025-12-30 13:01:09",
      "score": 6,
      "num_comments": 8,
      "upvote_ratio": 0.88,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levels‚Äîwhether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) üå±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1pzhjmn/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nx78umh",
          "author": "convicted_redditor",
          "text": "# Created 3 web scraping pypi libs on python. Any way to monetise?\n\n[](https://www.reddit.com/r/webscraping/?f=flair_name%3A%22Scaling%20up%20%F0%9F%9A%80%22)In 2025, I developed 3 pypi libs around webscraping-\n\n1. stealthkit - wrapper over curl\\_cffi with human-like fingerprinting with header rotations and cookie management.\n2. amzpy - built on top of curl\\_cffi (but before stealthkit), scrapes amazon search and product data.\n3. pnsea - built over stealthkit to scrape stock exchange data of India (NSE).\n\nReason to build them was for my personal usage as I developed an amazon related web app last year so I built amzpy. I was building a lot streamlit data based apps (and more) to play with NSE data - like options chain, insider data, etc.\n\nHow can I monetise this skill? Should I build FastAPI and turn into saas?\n\nHow do you guys monetise your web scraping skills?",
          "score": 2,
          "created_utc": "2026-01-02 04:36:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvep5p",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2025-12-31 06:14:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvsb8v",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2025-12-31 08:14:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx082gy",
          "author": "LessBadger4273",
          "text": "Here ya go, if someone can help me with this ‚Äî Libs such as ‚Äúnodriver‚Äù seems to be able to completely bypass some antibots like shopee.* ones that also requires js rendering. I guess this is because you are basically using your browser ‚Äúas is‚Äù, without any automation flag, right? \n\nIf so, why it‚Äôs so hard to replicate this at scale using residential proxies? My guess is that once you move this to AWS ec2, for example, those antibots can detect you are in a vm environment and block you, right? Would it be be possible to run this at scale by having an in house farm of old desktops/laptops? Or maybe using some rdp tools? Is it a price constraint that we are not able to bypass these antibots at scale or am I missing something?",
          "score": 1,
          "created_utc": "2026-01-01 00:36:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1cfkv",
              "author": "QuinsZouls",
              "text": "Shopee antibot is heavily dependent of the hardware fingerprint  + ip, I have succeeded experience using a local farm of macbook devices that run google chrome , usually th vm can easily  detected by proof of work and webgl + canvas fingerprint. \nAlso I have succeeded with some cloud VM instances with a GPU",
              "score": 1,
              "created_utc": "2026-01-01 05:12:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx88x0k",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-02 09:42:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8upl6",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-01-02 12:50:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pv756v",
      "title": "Any serious consequences?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pv756v/any_serious_consequences/",
      "author": "reddit_user4u",
      "created_utc": "2025-12-25 06:30:11",
      "score": 6,
      "num_comments": 22,
      "upvote_ratio": 1.0,
      "text": "Thinking about webscraping fragrantica for all their male perfumes for a machine learning perfume recommender project.\n\nNow I want to document everything on github as I'm doing this in attempt to get a coop (also bc its super cool). However, their ToS say web scraping is prohibited but Ive seen people in the past scrape their data and post on github. Theres also a old scraped fragrantica dataset on kaggle.\n\nI just dont want to get into any legal trouble or anything so does anyone have any advice? Anything appreciated!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1pv756v/any_serious_consequences/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nvuhswt",
          "author": "divided_capture_bro",
          "text": "One consequence will be that everyone thinks you're really into male perfume.\n\n\nToS = these are only suggestions.\n\n\nIf you want to have fun, set it up as a public repo and use GitHub Actions to do the scraping.",
          "score": 7,
          "created_utc": "2025-12-25 08:58:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwpaj8",
              "author": "Ecstatic_Vacation37",
              "text": "Don‚Äôt a lot of websites block the ip that comes from Gh actions ?",
              "score": 2,
              "created_utc": "2025-12-25 19:00:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwpxct",
                  "author": "divided_capture_bro",
                  "text": "Some but not all. Only one way to find out, and if need be use a proxy.\n\n\nI just checked and the site is accessible via Tor, so you could use that.",
                  "score": 1,
                  "created_utc": "2025-12-25 19:04:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvxfrxh",
              "author": "reddit_user4u",
              "text": "I was thinking about using rotating proxies with requests to scrape the data with 20 concurrent workers in python. Will this be too much for their servers or should be fine? \n\n  \nAlso just to confirm, no major consequences lol?",
              "score": 1,
              "created_utc": "2025-12-25 21:42:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvxkdw2",
                  "author": "divided_capture_bro",
                  "text": "I can't really say what their servers can handle, but 20 concurrent requests sounds light (it's not like you're sending all requests at once; that would likely cause problems!)\n\n\nAnd yes, there are likely no major consequences unless you're literally attacking them. There is a large body of recent case law affirming the legality of scraping - even doing so flagrantly like with BrightData.",
                  "score": 1,
                  "created_utc": "2025-12-25 22:10:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvvqyp2",
          "author": "leros",
          "text": "If they tell you to stop and you keep scraping, you could get into trouble. If you're hammering them with massive traffic and evading blocks, you could get into trouble. If you're publicly profiting off the data or harming them, you could get into trouble.¬†\n\n\nOtherwise you're probably fine.¬†",
          "score": 2,
          "created_utc": "2025-12-25 15:37:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwd78m",
          "author": "RandomPantsAppear",
          "text": "If you were going to have an issue (which is unlikely) they would just send a DMCA takedown to GitHub, github would take it down and that‚Äôs the end of it.",
          "score": 2,
          "created_utc": "2025-12-25 17:51:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvyy1ky",
              "author": "divided_capture_bro",
              "text": "It's not copyrighted information, so DMCA doesn't apply.",
              "score": 1,
              "created_utc": "2025-12-26 03:38:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw39wnm",
                  "author": "RandomPantsAppear",
                  "text": "I personally agree with you, but this is how I‚Äôve seen similar things be taken down from GitHub.",
                  "score": 1,
                  "created_utc": "2025-12-26 21:51:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw032kf",
          "author": "Ladline69",
          "text": "Just do it fuck it!",
          "score": 2,
          "created_utc": "2025-12-26 09:47:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvwaxe",
          "author": "Tasty_While_8076",
          "text": "Someone from fragrantica might break through your door like the kool-aid man and steal your computer. Be careful, it happened to a friend of a friend of mine.\n\nYou'll be fine.",
          "score": 1,
          "created_utc": "2025-12-25 16:09:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy68il",
          "author": "zoransa",
          "text": ">Owner here: don‚Äôt scrape Fragrantica. It violates our ToS, it‚Äôs unauthorized use of our IP, and it disrupts our operations. We don‚Äôt provide an API and we don‚Äôt license our content for datasets/ML. If you publish or commercialize scraped Fragrantica data, expect a lawsuit.",
          "score": 1,
          "created_utc": "2025-12-26 00:29:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvyelk0",
              "author": "reddit_user4u",
              "text": "Okay understood. just curious, why is it that other githubs which use fragrantica webscraped data are still up, and also the kaggle dataset which is derived from fragrantica as well?",
              "score": 1,
              "created_utc": "2025-12-26 01:24:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvyp0p8",
                  "author": "zoransa",
                  "text": "Stolen data is still illegal, and when we discover it, we send DMCA takedown notices.\n\nWe had a case where an aggressive crawler hammered our service for almost two weeks; we were literally going offline for minutes at a time. A few months later, a researcher from Imperial College tried to ‚Äúlegalize‚Äù the theft by asking permission to use the data for his PhD, after he had already completed the project and written the thesis. When we found out, we objected and documented the disruption and downtime it caused. The PhD was ultimately rejected.\n\nWe will not allow scraping of our website, not even for educational purposes. If anyone attempts to use scraped Fragrantica data commercially, we will take legal action.",
                  "score": 1,
                  "created_utc": "2025-12-26 02:36:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwcweo",
          "author": "bluemangodub",
          "text": "IF you do it for profit then in the US that's proven to be a problem. In the EU less so.\n\nScraping tends to be alright, logging into account areas, can be more problematic.\n\nScraping a data set and putting it on github, probably at worst all that happens is the page gets taken down. If you can be found easily, you may get a cease and desist form letter from some lawyer.  In which case it's probably time to take it down.",
          "score": 1,
          "created_utc": "2025-12-31 11:28:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvu5huj",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2025-12-25 06:49:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvuhydd",
              "author": "divided_capture_bro",
              "text": "It's not illegal to profit from publicly available information. All of the recent cases point to this same conclusion, that the law as it stands allows for scraping.",
              "score": 6,
              "created_utc": "2025-12-25 09:00:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvuldh1",
                  "author": "[deleted]",
                  "text": "Right. Google is basically a giant web scraper and making a ton of money from it. If tjey block scrapping then Google will be the first one to get hit.",
                  "score": 4,
                  "created_utc": "2025-12-25 09:38:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvw3hcl",
                  "author": "leros",
                  "text": "That doesn't mean a big company won't take legal action against you that costs you a bunch of money. Companies get sued for scraping, stop, and settle for a payment.",
                  "score": 2,
                  "created_utc": "2025-12-25 16:52:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q0u4vg",
      "title": "Monthly Self-Promotion - January 2026",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q0u4vg/monthly_selfpromotion_january_2026/",
      "author": "AutoModerator",
      "created_utc": "2026-01-01 03:00:42",
      "score": 6,
      "num_comments": 21,
      "upvote_ratio": 0.88,
      "text": "Hello and howdy, digital miners of¬†r/webscraping!\n\nThe moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!\n\n* Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?\n* Maybe you've got a ground-breaking product in need of some intrepid testers?\n* Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?\n* Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?\n\nWell, this is your time to shine and shout from the digital rooftops - Welcome to your haven!\n\nJust a friendly reminder, we like to keep all our self-promotion in one handy place, so any promotional posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q0u4vg/monthly_selfpromotion_january_2026/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nx1mea7",
          "author": "Jonathan_Geiger",
          "text": "I‚Äôm building [SocialKit](https://socialkit.dev) it‚Äôs a social media scraping API for extracting transcripts, summaries, stats, comments, and more from YouTube, TikTok, Instagram, and Shorts (:\n\nJust reached $700 in MRR!!",
          "score": 4,
          "created_utc": "2026-01-01 06:40:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx642u9",
              "author": "bootlegDonDraper",
              "text": "very cool! will need something like this for my personal project\n\ntwo things maybe as advice:\n\n**1** meta is veeery nosy about scraping, especially for instagram. i would expect to get an email from them as your business grows, and not invest heavily in instagram (publicly at least ;) ) but stay on youtube and tiktok side of things\n\n**2** i dont see any dedicated product for scraping social media ranking for \"tiktok comment scraper\". I would go from [https://www.socialkit.dev/tiktok-comments-api](https://www.socialkit.dev/tiktok-comments-api) this landing page to .../tiktok-comment-scraper-api and optimize for the scraper keyword as it has muuuch higher volume",
              "score": 1,
              "created_utc": "2026-01-02 00:25:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1h6ai",
          "author": "Bitter_Caramel305",
          "text": "Do people actually come here to read this?",
          "score": 3,
          "created_utc": "2026-01-01 05:52:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1syyp",
              "author": "Jonathan_Geiger",
              "text": "I did :)",
              "score": 2,
              "created_utc": "2026-01-01 07:44:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx21k2a",
                  "author": "Bitter_Caramel305",
                  "text": "And are you also going to buy other people's products/services?",
                  "score": 1,
                  "created_utc": "2026-01-01 09:15:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx1ymcy",
              "author": "indicava",
              "text": "Me too",
              "score": 1,
              "created_utc": "2026-01-01 08:44:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx5iki6",
              "author": "matty_fu",
              "text": "this thread gets 10k+ views each month, and can be helpful for SEO/GEO. also it helps the mod team get less abuse from people wanting to advertise their products - a win for everyone",
              "score": 1,
              "created_utc": "2026-01-01 22:25:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1h7oe",
          "author": "Bitter_Caramel305",
          "text": "# I can scrape that website for you\n\nHi everyone,  \nI‚Äôm Vishwas Batra, feel free to call me Vishwas.\n\nBy background and passion, I‚Äôm a full stack developer. Over time, project needs pushed me deeper into web scraping and I ended up genuinely enjoying it.\n\n**A bit of context**\n\nLike most people, I started with browser automation using tools like Playwright and Selenium. Then I moved on to crawlers with Scrapy. Today, my first approach is reverse engineering exposed backend APIs whenever possible.\n\nI have successfully reverse engineered Amazon‚Äôs search API, Instagram‚Äôs profile API and DuckDuckGo‚Äôs¬†`/html`¬†endpoint to extract raw JSON data. This approach is far easier to parse than HTML and significantly more resource efficient compared to full browser automation.\n\nThat said, I‚Äôm also realistic. Not every website exposes usable API endpoints. In those cases, I fall back to traditional browser automation or crawler based solutions to meet business requirements.\n\nIf you ever need clean, structured spreadsheets filled with reliable data, I‚Äôm confident I can deliver. I charge nothing upfront and only ask for payment once the work is completed and approved.\n\n**How I approach a project**\n\n* You clarify the data you need such as product name, company name, price, email and the target websites.\n* I audit the sites to identify exposed API endpoints. This usually takes around 30 minutes per typical website.\n* If an API is available, I use it. Otherwise, I choose between browser automation or crawlers depending on the site. I then share the scraping strategy, estimated infrastructure costs and total time required.\n* Once agreed, you provide a BRD or I create one myself, which I usually do as a best practice to stay within clear boundaries.\n* I build the scraper, often within the same day for simple to mid sized projects.\n* I scrape a 100 row sample and share it for review.\n* After approval, you provide credentials for your preferred proxy and infrastructure vendors. I can also recommend suitable vendors and plans if needed.\n* I run the full scrape and stop once the agreed volume is reached, for example 5000 products.\n* I hand over the data in CSV, Google Sheets and XLSX formats along with the scripts.\n\nOnce everything is approved, I request the due payment. For one off projects, we part ways professionally. If you like my work, we continue collaborating on future projects.\n\nA clear win for both sides.\n\nIf this sounds useful, feel free to reach out via¬†[LinkedIn](https://www.linkedin.com/in/vishwas-batra/)¬†or just send me a DM here.",
          "score": 1,
          "created_utc": "2026-01-01 05:52:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1ic2g",
          "author": "ConstIsNull",
          "text": "Building a JSON API for getting jobs listed on company career pages only, no LinkedIn or indeed. Saves you time instead of building your own job scraper. Check it out at jobven.com",
          "score": 1,
          "created_utc": "2026-01-01 06:02:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1tkco",
          "author": "Dear-Cable-5339",
          "text": "**Crawlbase Smart Proxy**¬†‚Äì scrape any public website without worrying about blocks, CAPTCHAs, or IP bans.  \nWorks great for e-commerce, SERPs, social platforms, and more.\n\nYou send the URL ‚Üí we handle proxies, rotation, retries, and geo.\n\nüëâ Try it free here:¬†[https://crawlbase.com/?s=5qGcKLCR](https://crawlbase.com/?s=5qGcKLCR)",
          "score": 1,
          "created_utc": "2026-01-01 07:50:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx20w02",
          "author": "Silentkindfromsauna",
          "text": "[Lindra.ai](www.lindra.ai) for turning any website into an api, still early but already works great for scraping",
          "score": 1,
          "created_utc": "2026-01-01 09:08:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx21jrx",
          "author": "Classic_Exam7405",
          "text": "We built the SOTA AI Web Agent rtrvr.ai for vibe scraping. Just prompt our agent to fill forms, extract data or monitor sites.\n\nScrape strict antibot sites like LinkedIn with our chrome extension.\n\nScale on our cloud/api platform",
          "score": 1,
          "created_utc": "2026-01-01 09:15:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx251r6",
          "author": "Ok-Skirt8939",
          "text": "im building \nhttps://clearproxy.io/\n\nan instant proxy validator.\n(u can check 100 Millions of proxies in less than 1 minute.)\nand.. more features just check it out (free 1M trial Checks)",
          "score": 1,
          "created_utc": "2026-01-01 09:53:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx260us",
          "author": "Long-Movie-2207",
          "text": "It's a real-time IP intel API that detects proxies, VPNs, Tor, datacenter vs residential, even specific providers, and gives a solid risk score (0-100) plus some device fingerprinting stuff.\n\nSuper handy for spotting if your proxies are likely to get flagged. There's a free tier with 5k requests to test it out, no strings attached.\n\nIf anyone's dealing with detection issues, might be worth a quick look: [https://ping0.xyz](https://ping0.xyz)",
          "score": 1,
          "created_utc": "2026-01-01 10:04:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx283xq",
          "author": "renegat0x0",
          "text": "https://github.com/rumca-js/Internet-Places-Database - data\n\n\nhttps://github.com/rumca-js/crawler-buddy - crawler\n\n\nhttps://github.com/rumca-js/webtoolkit - crawling library",
          "score": 1,
          "created_utc": "2026-01-01 10:26:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4ylgo",
          "author": "ziddec",
          "text": "I'm building https://sociavault.com \nA real-time social media scraping API. JSON response, 1 API key, 25+ platforms.",
          "score": 1,
          "created_utc": "2026-01-01 20:41:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5x6vx",
          "author": "scrape-do",
          "text": "***Hey scrapers,***\n\n  \nFirst off, happy new year to everyone :)\n\n\n\nWe're [Scrape.do](http://Scrape.do) üëã  an aggressively agile team behind a robust web scraping API that we've been building (very quietly since we were mainly focused on product) for the last 5 years.\n\n  \n2025 has been THE year for us, as we:\n\n* silently rolled out **next-gen infrastructure changes** that makes us the fastest and the most reliable on **a lot of** target domains, i.e. [async Scrape.do](https://scrape.do/documentation/async-api/)\n* **redesigned** our [dashboard with added playground](https://dashboard.scrape.do/playground) and [reworked documentation](https://scrape.do/documentation/),\n* [launched our new website](https://scrape.do/),\n* published **ready, free, and** [open-source scrapers](https://github.com/scrape-do/scrapedo-scrapers) **for 30 of the toughest domains** (we used SDO to bypass anti-bot, BUT will work with any API or your own setup)\n* an [Amazon Scraper API](https://scrape.do/products/ready-api/amazon-scraper/) as a product, but also a [free Amazon scraping repository](https://github.com/scrape-do/amazon-scraper)(**best one out there**, can easily be used w/o SDO same as above)\n\n  \n*Our support, bypass capabilities, pricing, and speed create an unmatched combination.* ‚≠ê\n\n\n\n***Yet, we're not satisfied.***\n\n\n\nMore products, features, and most importantly open-source resources are coming your way very soon.\n\n\n\nHappy scraping,\n\nSDO team",
          "score": 1,
          "created_utc": "2026-01-01 23:46:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7taq4",
          "author": "deepwalker_hq",
          "text": "\nHey r/webscraping,\n\nWe‚Äôve been building [Deepwalker](https://deepwalker.xyz) to help overcome the challenges of scraping modern mobile apps. Think automating interactions on TikTok, Instagram, Spotify, and more using real devices.\n\nOur latest post details how we tackled Spotify: [https://deepwalker.xyz/blog/scraping-spotify-is-ez](https://deepwalker.xyz/blog/scraping-spotify-is-ez)\n\nDeepWalker is built for professionals tackling large-scale data extraction and automation.",
          "score": 1,
          "created_utc": "2026-01-02 07:14:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9t67k",
          "author": "Past-Refrigerator803",
          "text": "**Browser4 ‚Äî Agentic browser engine for web scraping & automation**\n\nHi ,\n\nI‚Äôm building **Browser4**, an open-source, **agentic browser engine** for serious web scraping and automation‚Äîespecially where traditional tools start to break down.\n\nBrowser4 runs **inside your own infrastructure** and is designed for teams and individuals who need **control, stability, and scale**.\n\n# What Browser4 offers\n\n* **Agent-native architecture** Built for autonomous browser agents that reason, retry, branch, and adapt.\n* **Deep CDP control** JVM-native, coroutine-safe Chrome DevTools Protocol implementation. Capable of processing **\\~10k‚Äì20k highly complex pages per node per day**.\n* **Reliable scraping on dynamic sites** Handles logins, SPAs, infinite scroll, and JS-heavy workflows.\n* **Machine-learning extraction agent** Learns field structures across complex pages **without consuming LLM tokens**.\n* **Multi-strategy data extraction** LLMs, LM agents, selectors, DOM access, JavaScript execution, network interception, and screenshots.\n* **Scheduling & long-running jobs** Designed for production pipelines, not just one-off scripts.\n* **Built-in observability** Explicit state management, retries, and failure handling.\n\nBrowser4 can operate as a **fully autonomous browser agent** that plans and executes end-to-end tasks. At the same time, if you‚Äôve found Selenium or Playwright brittle, RPA tools too heavy, or scraping frameworks too opinionated, Browser4 also works at a **lower abstraction level** and gives you back control.\n\n# Use cases\n\n* AI agents that browse and interact with websites\n* Automation workflows that must survive frequent UI changes\n* Large-scale web scraping systems\n\nThe project is **open source** and under active development. Feedback, issues, and real-world use cases are very welcome.\n\n**GitHub:** [https://github.com/platonai/browser4](https://github.com/platonai/browser4)",
          "score": 1,
          "created_utc": "2026-01-02 16:04:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxe5dr7",
          "author": "Livid-Ad-8185",
          "text": "I'm a just starting my business as a software developer. As a self-taught (interested in all) covers from computer vision, web scraping, browser automation, data science, networking, DOM/Browser environment and deep learning. **Providing High Quality - Borderline CUTTING EDGE Solution & Software. ASK ME anything.**\n\n**Author of:**\n\nPropScanner - **Fanduel, Draftkings, Pinnacle** Prop Scanner, supports up to real-time updates.\n\n**BW TLS Client - Chrome143 TLS fingerprints**, **better than curl-cffi, curl-impresonation**, curl, requests. | Web Scraping [**Nike.com.br**](http://Nike.com.br) **(Akamai)**, **Snkers**, Amazon **ANY BOTDETECTION, ANY WEBSITE UNSTOPPABLE AT REQUEST LEVEL**.\n\nJob Auto-Fill Pro - High Performance LLM-LEAD/Generic Job Apply/Filling Chrome Extension supports ALL From-based Platforms including **Linkedin, WorkDay, GreenHouse and Lever**. And you can use it for free now!\n\n** CAPTCHA Solver - 100% Accuracy, Generating ProtonMail Accounts**\n\n**Small Zombie - Chrome CDP Driver Connected with Undetectable Browser for PASSIVE & ACTIVE BOTDETECTION BYPASSSING**\n\n**Others Including: Yelp/Amazon/Walmart ENTIRE US Scrapers and Others 100+ | Universal-V 140M Language Model and MORE.**\n\n**CHECK MY PORTFOLIO:**\n\n[https://gitlab.com/JJ-GitRepo/](https://gitlab.com/JJ-GitRepo/)\n\n**Discord:**\n\n[https://discord.gg/jfdck6EE](https://discord.gg/jfdck6EE)\n\nDO NOT HELP INDIVIDUAL - $50+ PER SOLUTION, $40/H Per Session (INDIVIDUAL), Enteprise DIFFER.\n\nSoftware Development: $50 Deposit, 70% - 100% after data/software (trial) verified upon your choice. Can go through Upwork if legitment.",
          "score": 1,
          "created_utc": "2026-01-03 06:00:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxf2bsc",
          "author": "DataiziNet",
          "text": "Stop building scrapers. Start consuming data.\n\nMost tools here sell you the infrastructure (proxies/browsers) but leave you with the maintenance.[Dataizi](https://dataizi.net/)is a fully managed data extraction service. We don't sell the shovel; we dig the hole.\n\nWhy us?\n\n* 0 Maintenance: We monitor feed health 24/7. When layouts change, we fix the parsers, not you.\n* Complex Targets: We handle JS-heavy sites, hidden APIs, and aggressive anti-bot protections.\n\nPerfect for:\n\n* Price Intelligence: High-frequency SKU tracking for dynamic pricing strategies.\n* Machine Learning/AI: Clean, structured, normalized data ready for model training.\n* SaaS & Agencies: Reliable data feeds without hiring a dedicated scraping engineer.\n\nThe Offer: You provide the URL + Schema. We deliver the API or CSV.\n\n[Get a quote: dataizi.net](https://dataizi.net/)",
          "score": 1,
          "created_utc": "2026-01-03 10:40:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pz8czz",
      "title": "Is it just me or playwright incredibly unstable",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pz8czz/is_it_just_me_or_playwright_incredibly_unstable/",
      "author": "kilobrew",
      "created_utc": "2025-12-30 04:24:02",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 0.8,
      "text": "I‚Äôve been using playwright in the AWS environment and having nothing but trouble getting it to run without randomly disconnecting, ‚Äúfailed to get world‚Äù, or timeouts that really shouldn‚Äôt have happened. Hell, Even running AWS‚Äôs SAAS bedrock agent_core browser tool has the same issue. \n\nIt seems the only time I can actually use it is if it‚Äôs installed on a full blown windows install with a GPU. \n\n\nIs it just me?\n",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1pz8czz/is_it_just_me_or_playwright_incredibly_unstable/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwoupx0",
          "author": "RandomPantsAppear",
          "text": "I ran multiple playwright instances on Fargate instances with 0.25 vcpu and 256m of ram, that were also running redis and celery. \n\nSomething is very wrong if this is the behavior you are getting. \n\n\nHow are you detecting the page load completion success/fail?\n\nHave you checked the process list to make sure processes are successfully exiting?\n\nAre you taking screenshots on the theoretical page load fails? (I am not sure how this works headless, I often run it with xvfb)",
          "score": 1,
          "created_utc": "2025-12-30 06:42:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpm1qz",
          "author": "bluemangodub",
          "text": "Not my experience at all. I have had untold issues with AWS, especially the low price instances. \n\nRun it locally, or on a proper VPS and see if you have the same issues. If not, it's AWS",
          "score": 1,
          "created_utc": "2025-12-30 10:52:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpnb6t",
          "author": "Lafftar",
          "text": "Are you using proxies?",
          "score": 1,
          "created_utc": "2025-12-30 11:04:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0kacg",
      "title": "Amazon \"shop other stores\" Beta",
      "subreddit": "webscraping",
      "url": "https://i.redd.it/zlndg2yq4lag1.jpeg",
      "author": "ZanofArc",
      "created_utc": "2025-12-31 18:44:32",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q0kacg/amazon_shop_other_stores_beta/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q09vdc",
      "title": "Bypassing DataDome",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q09vdc/bypassing_datadome/",
      "author": "Vlad_Beletskiy",
      "created_utc": "2025-12-31 10:25:38",
      "score": 4,
      "num_comments": 4,
      "upvote_ratio": 0.65,
      "text": "Hello, dear community!\n\nI‚Äôve got an issue being detected by DataDome (403 status) while scraping a big resource.\n\n\n\n**What works**\n\nI use Zendriver pointing to my local MacOS Chrome. Navigating to site‚Äôs main page -> waiting for the DataDome endpoint that returns DataDome token -> making subsequent requests via curl\\_cffi (on my local MacOS machine) with that token being sent as a DataDome cookie.  \nI‚Äôve checked that this token lives quite long - is valid for at least several hours, but assume even more (managed to make requests after multiple days).\n\n\n\n**What I want to do that doesn‚Äôt work**\n\nI want to deploy it and opted for Docker. Installed Chrome (not Chromium) within the Docker. Tried the same algorithm as above. The outcome is that I‚Äôm able to get token from the DataDome endpoint. But subsequent curl\\_cffi requests fail with 403. Tried curl\\_cffi requests from Docker and locally - both fail, issued token is not valid.\n\nNext thing I‚Äôve enabled xvfb that resulted in a bit better outcome. Namely, after obtaining the token the next request via curl\\_cffi succeeds, while subsequent ones fail with 403. So, it‚Äôs basically single use.\n\nNext I‚Äôve played with different user agents, set timezone, but the outcome is the same.\n\n\n\nOne more observation - there‚Äôs another request which exposes DataDome token via Set-Cookie response header. If done with Zendriver under Docker, Set-Cookie header for that same endpoint is missing.\n\nSo, my assumption is that my trust score by DataDome is higher than to show me captcha, but lower than to issue a long-living token.\n\n\n\nAnd one more observation - both locally and under Docker requests via curl\\_cffi work with 131st Chrome version being impersonated. Though, 143rd latest Chrome version is used to obtain this token. Any other curl\\_cffi impersonation options just don‚Äôt work (result in 403). Why does that happen?\n\nAnd I see that curl\\_cffi supports impersonation of the following OSes only: Win10, MacOS (different versions), iOS. So, in theory it shouldn‚Äôt work at all combined with Docker setup?\n\n\n\n**Question** \\- could you please point me in the right direction what to investigate and try next. How do you solve such deployment problems and reliably deploy scraping solutions? And probably you can share advice how to enhance my DataDome bypassing strategy?\n\n\n\nThank you for any input and advices!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q09vdc/bypassing_datadome/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwwa7vq",
          "author": "infaticaIo",
          "text": "DataDome tokens are usually bound to more than just a cookie. They often get tied to the full client ‚Äúshape‚Äù (TLS and HTTP2 fingerprint, IP reputation, timing, browser signals, sometimes even local storage) so a token minted in one environment can be useless in another. That explains why local Mac works but Docker fails, and why you see ‚Äúsingle use‚Äù behavior.\n\nWhat to investigate, at a high level:\n\n* Fingerprint consistency: the environment that mints the token needs to match the environment that reuses it. If you mint in a real Chrome and replay with curl, any mismatch in TLS or HTTP2 can invalidate quickly.\n* IP consistency: tokens can be scoped to IP or ASN. Local IP vs Docker egress often differs even on the same machine if you run through different routes.\n* Header and cookie jar completeness: missing Set-Cookie under Docker usually means the JS flow or redirects differ, or a required request wasn‚Äôt executed the same way.\n* Version coupling: the fact that only one curl\\_cffi impersonation works suggests the backend is keying on a very specific TLS stack and ordering.\n\nFor deployment, the reliable pattern is usually to keep the whole flow in one place. Either keep requests inside the same browser context that earned the session, or run the replay client with a fingerprint that is as close as possible to that browser and network path. Mixing ‚Äúreal browser to get token‚Äù with a very different HTTP client is where these systems tend to break.\n\nIf this is for a legitimate use case, the sustainable option is getting approved access or using an official feed. Trying to ‚Äúenhance bypass‚Äù is a cat and mouse game and will keep changing.",
          "score": 5,
          "created_utc": "2025-12-31 11:03:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwww0zk",
              "author": "Vlad_Beletskiy",
              "text": "Thank you.\n\nIt seems that IP binding doesn't matter that much for cookie issuing. Because I've tried to obtain DataDome cookie token both with & without proxies (running locally without Docker). And then use via curl\\_cffi for subsequent requests. And it works regardless of the proxy presence while obtaining the token.\n\nInteresting point here - I've obtained cookie using different Chrome version and different OS (MacOS) version compared to that used subsequently during curl\\_cffi impersonation, and that 131rd version still worked. However should have failed. That still seems strange.\n\n  \n\"keep requests inside the same browser context that earned the session\" - yeah, was thinking similarly.",
              "score": 1,
              "created_utc": "2025-12-31 13:49:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwlfqv",
          "author": "Dismal_Pilot_1561",
          "text": "Personally, it works great for me in Docker using a fairly heavy Linux base image, but one that helps boost my Datadome trust score.\n\nJust like you, I first warm up the proxy using a real automated browser combined with a custom captcha solver. Then, I use curl_cffi with the cookies generated by the actual browser, and I save any new cookies if they get updated (which happens quite often).\n\nThe main difference is probably that I'm forced to solve a captcha (not the main page), which significantly increases the Datadome trust score. Also, I make sure to use the correct cookie data and headers to mimic the browser I used as closely as possible.\n\nI use this method for high-frequency scraping. Without pushing it too hard and on a fairly modest machine, I scrape about 15,000 URLs in 4 hours.",
          "score": 2,
          "created_utc": "2025-12-31 12:38:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx15tfk",
          "author": "_mackody",
          "text": "Look at JA3",
          "score": 1,
          "created_utc": "2026-01-01 04:19:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0atdm",
      "title": "Scraping market data CS2/CSGO",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q0atdm/scraping_market_data_cs2csgo/",
      "author": "Short_Bus_6284",
      "created_utc": "2025-12-31 11:24:18",
      "score": 4,
      "num_comments": 7,
      "upvote_ratio": 0.83,
      "text": "Good evening! Hope this is the right place to ask. I've reached a point where I need metadata and, especially, up to date prices for Counter Strike 2 skins. I understand that there are paid APIs and the Steam API that provide real-time metadata and prices, but to be honest, I‚Äôd prefer to go with free solutions. This brings me to scrapers, since I haven‚Äôt been able to find any free APIs that meet my needs. I‚Äôve dug through GitHub and found some repos, but most of them either don‚Äôt work with modern JavaScript heavy sites, or they only scrape limited metadata. The only repo I found that works well is¬†[this one](https://github.com/eovacius/csgodatabase-scraper/), which returns both prices and metadata fairly quickly. However, the project is missing some content, like souvenirs, stickers, cases, etc. It looks like it‚Äôs still pretty new, so I‚Äôm sure the content will be updated soon, but I don‚Äôt want to wait too long. So, I was hoping some of you might know of any resources or public databases/sites that would let me scrape CS2 skin information. Or, if there are any other free methods to get this info without scraping, that would be super helpful too. Thanks in advance!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q0atdm/scraping_market_data_cs2csgo/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nx6pmh6",
          "author": "scraping-test",
          "text": "*\"free methods to get this info without scraping\"* \n\ni can think of two: an intern... or divine intervention xd\n\n  \nto be real, the repo you've mentioned looks really well-built. I'm sure you can somewhat easily modify it to extract the additional stuff you want. if you don't know how, clone to cursor and get claude on it",
          "score": 3,
          "created_utc": "2026-01-02 02:34:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8ss17",
              "author": "Short_Bus_6284",
              "text": "Yes, config looks promising but unfortunately im not familiar with Golang. So i guess i would go with claude. Thanks",
              "score": 1,
              "created_utc": "2026-01-02 12:36:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5pmdf",
          "author": "_i3urnsy_",
          "text": "Is your issue with scraping the site or do you need a better data source to scrape info from?",
          "score": 1,
          "created_utc": "2026-01-01 23:04:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8spp9",
              "author": "Short_Bus_6284",
              "text": "Both scraper and target site are fine. Just need more data",
              "score": 2,
              "created_utc": "2026-01-02 12:36:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8k5jl",
          "author": "bluemangodub",
          "text": "you either figure out how to do it yourself, or you pay someone to do.\n\nIF you don't want to wait for the repo you are using, reach out to the dev and offer them money to compete the parts you are needing.",
          "score": 1,
          "created_utc": "2026-01-02 11:25:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8t214",
              "author": "Short_Bus_6284",
              "text": "I'll contant them, either we scale it together or i pay and they do the job. honestly might be cheaper than paid Api or scraper services.",
              "score": 1,
              "created_utc": "2026-01-02 12:38:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdz8vu",
          "author": "Afraid-Solid-7239",
          "text": "What information in specific are you looking to get that the current repo doesn't have?   \nThe files in /json/ of the repo seem fairly extensive? I'm able to find souvenirs and cases in some json files.\n\nThough the dev of that repo in specific, is just scraping csgodatabase.  \nIs the data you want available on csgodatabase.com? I don't mind writing up a scraper for it, for you.",
          "score": 1,
          "created_utc": "2026-01-03 05:14:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q12pcv",
      "title": "Scraping in Google Scholar",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q12pcv/scraping_in_google_scholar/",
      "author": "Cuaternion",
      "created_utc": "2026-01-01 12:02:19",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.76,
      "text": "Hi, I'm trying to do scraping with some academic profiles in Google Scholar, but maybe the server has restrictions for this activity.\nAny suggestions?\nThanks",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q12pcv/scraping_in_google_scholar/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nx60svn",
          "author": "bootlegDonDraper",
          "text": "hey OP\n\nyou'll hit rate limits everywhere when web scraping, but it's easy to get through\n\n**first solution**, throttle your requests and add random delays between requests.\n\n**second,** instead of scraping it in one go, create a scraper that scrapes a chunk of URLs every hour or so with the rate limiting in first solution\n\n*you don't want to wait?*\n\n**third and most effective,** rotate proxies. if you use a large proxy pool you can run concurrent requests to scrape tens of pages at once without ever being rate limited.\n\nif your proxies are low quality DC proxies, your requests will get blocked. if more than half of your requests aren't blocked, introduce error handling to re-request the same page with another ip if it gets blocked.\n\nvoila",
          "score": 5,
          "created_utc": "2026-01-02 00:07:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0o0z1",
      "title": "TLS fingerprint websocket client to bypass cloudflare?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q0o0z1/tls_fingerprint_websocket_client_to_bypass/",
      "author": "vroemboem",
      "created_utc": "2025-12-31 21:34:36",
      "score": 4,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "What are the best stealth websocket clients (that work with nodejs)?",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1q0o0z1/tls_fingerprint_websocket_client_to_bypass/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwzg735",
          "author": "Afraid-Solid-7239",
          "text": "What's the url for the socket you're trying to connect to? I'll play around with it",
          "score": 1,
          "created_utc": "2025-12-31 21:50:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1n7fo",
          "author": "army_of_wan",
          "text": "have you tried building a stealth client using golang's tls\\_library, i bet you could do that using gemini 3 or Claude opus",
          "score": 1,
          "created_utc": "2026-01-01 06:47:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxe4ygl",
          "author": "cq_in_unison",
          "text": "\\`curl\\_cffi\\` can usually get around it, but you mind need to go up to \\`nodriver\\` or scripted playwright. don't forget that it's not always just cloudflare, but a number of prevention tactics.",
          "score": 1,
          "created_utc": "2026-01-03 05:57:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1uq5w",
      "title": "Turnstiles, geetest, automation in Rust?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q1uq5w/turnstiles_geetest_automation_in_rust/",
      "author": "Normal-Middle3719",
      "created_utc": "2026-01-02 09:55:40",
      "score": 4,
      "num_comments": 2,
      "upvote_ratio": 0.7,
      "text": "Hey guys,\n\nI‚Äôve been benefiting from the open-source projects here for a while, so I wanted to give back. I‚Äôm a big fan of compiled languages, and I needed a way to handle browser tasks (specifically CAPTCHAs) in Rust without getting flagged.\n\nI forked chromiumoxide and ported the stealth patches from `rebrowser` and `puppeteer-real-browser`. I also built dedicated solvers for Cloudflare and GeeTest.\n\nüß™ The Proof (Detection Results)\n\nI‚Äôve tested this against common scanners and it‚Äôs passing:\n\n* **Intoli / WebDriver Advanced:** Passed (WebDriver hidden, Permissions default).\n* **Fingerprint Scanner:** `PHANTOM_UA`, `PHANTOM_PROPERTIES`, and `SELENIUM_DRIVER` all return **OK**.\n* **Canvas/WebGL:** Properly spoofing Google Inc. (NVIDIA) with no broken dimensions.\n* **Stack Traces:** `PHANTOM_OVERFLOW` depth and error names match real Chrome behavior.\n\nüõ† The Repos\n\n* [**chaser-oxide**](https://github.com/ccheshirecat/chaser-oxide)‚Äì Chromiumoxide fork with stealth/impersonation patches.\n* [**chaser-cf**](https://github.com/ccheshirecat/chaser-cf)‚Äì Rust implementation for Cloudflare Turnstile.\n* [**chaser-gt**](https://github.com/ccheshirecat/chaser-gt)‚Äì GeeTest solver using deobfuscation (via `rquests`/`curl_cffi`).\n\n**Note:** I shipped these with **C FFI bindings**, so you can use them in **Python, Go, or Node** if you just want the Rust performance/stealth without writing Rust code. I personally prefer this over managing a separate microservice.\n\nüí¨ Curious about your workflows:\n\n1. **Third-party APIs:** For those using paid solvers (Capsolver, etc.), is it for the convenience, or because you don't want to maintain stealth patches yourself?\n2. **Scraping Use Cases:** What are you guys actually building? I‚Äôll go first: I‚Äôm overengineering automation for crypto casinos because I found some gaps in their flow lol.\n3. **Differentiators:** What actually makes a solver \"good\" in 2026? Is it raw solve speed, or just the success rate on high-entropy challenges?\n\nIt‚Äôs still early, so feel free to contribute, roast my code, or reach out to collaborate. Happy New Year!",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1q1uq5w/turnstiles_geetest_automation_in_rust/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nxa1jbi",
          "author": "TinyBeing8001",
          "text": "this is cool and thanks for it\n\nit might help spacing your post content out so it‚Äôs more consumable",
          "score": 1,
          "created_utc": "2026-01-02 16:43:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbbgrl",
              "author": "Normal-Middle3719",
              "text": "thanks for the feedback! took your advice and updated the post",
              "score": 1,
              "created_utc": "2026-01-02 20:18:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pvg8zy",
      "title": "Tool for tracking product photos + prices from multiple shops?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pvg8zy/tool_for_tracking_product_photos_prices_from/",
      "author": "blera",
      "created_utc": "2025-12-25 15:58:12",
      "score": 3,
      "num_comments": 10,
      "upvote_ratio": 0.81,
      "text": "I‚Äôm looking for a ToS friendly way to monitor product listings on multiple lingerie retailers. I follow around 10‚Äì15 shops (Hunkem√∂ller, Women‚Äôsecret, Intimissimi, VS, etc.) and manually checking category pages is taking too much time.\n\nWhat I want is basically ‚Äúwatch these category URLs‚Äù and collect product name, product link, main photo, and current price. Then keep it organized by shop and category (bras, bodysuits, sets), and ideally notify me when prices drop.\n\nDoes something like this already exist (library, service, framework, or a common approach people use)? I‚Äôm not trying to bypass protections or do heavy scraping, just personal tracking, ideally polite and low frequency. If you‚Äôve built something similar, what worked well for e-commerce sites?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1pvg8zy/tool_for_tracking_product_photos_prices_from/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nw3ni8f",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2025-12-26 23:08:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9mgty",
              "author": "webscraping-ModTeam",
              "text": "ü™ß Please review the sub rules üëâ",
              "score": 1,
              "created_utc": "2025-12-27 22:44:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwmut1",
          "author": "abdullah-shaheer",
          "text": "You can make a personalized system for yourself",
          "score": 1,
          "created_utc": "2025-12-25 18:46:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwo2k0",
              "author": "blera",
              "text": "I can. I have experience building software, just not web scraping specifically. I‚Äôm scouting to see if something already exists or if there‚Äôs a good base to build on, rather than reinventing the wheel.\n\nDo you have any advice? I‚Äôm specifically trying to download all product images from a page, including images for the other color variants too. For example (SFW): https://www.intimissimi.com/it/product/maglia_in_seta_a_taglio_uomo-CLD92A.html?dwvar_CLD92A_Z_COL_INT=651I\n\nHow would you approach getting all the image URLs (and the other colors‚Äô images) in a robust way? Machine to machine obviously",
              "score": 1,
              "created_utc": "2025-12-25 18:53:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvwt9ue",
                  "author": "abdullah-shaheer",
                  "text": "Yes, there are a lot of ways to do so. Sometimes, all the information gets loaded initially, you can search for application/json+ld classes in the html to see the json information. If you can't find a information regarding variant image urls in the main page, then the most easiest way without making more requests will be making image urls with reference to the base product image urls, there will be a small difference in base product and it's variant image urls like variant code or something like that. Find the information in the base product's html, however it will be hard for beginners to do so (I think). You can also track network requests and make a data pipeline to retrieve all the information. There will be a lot other ways, depending on the website. I am not entering the website as most of it's content is NSFW based. Let me know, if there is something else, you wanna ask. I will be happy to help (only for SFW).",
                  "score": 1,
                  "created_utc": "2025-12-25 19:24:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvx7n7t",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2025-12-25 20:51:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxcu9o",
              "author": "webscraping-ModTeam",
              "text": "üëî Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
              "score": 1,
              "created_utc": "2025-12-25 21:24:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvxikmg",
          "author": "unstopablex5",
          "text": "Theres no product like this to my knowledge, you either hire someone with scraping experience to build you the pipeline or you do it yourself",
          "score": 1,
          "created_utc": "2025-12-25 21:59:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzzc2f",
          "author": "RobSm",
          "text": "If you hire someone to do 'one time job' you will have to maintain it when sites change structure. So better learn yourself, especially if this is 'nothing big'.",
          "score": 1,
          "created_utc": "2025-12-26 09:07:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pv93nw",
      "title": "Scraping Job on Indeed",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pv93nw/scraping_job_on_indeed/",
      "author": "Haikal019",
      "created_utc": "2025-12-25 08:44:14",
      "score": 2,
      "num_comments": 5,
      "upvote_ratio": 0.63,
      "text": "thinking about web scraping indeed using playwright to collect job data like job title, job description, and salary for a data engineering / analytics project.\n\nare there any good github repos using playwright or similar tools that i can refer to for scraping job listings.\n\nIssue on my side is to get the job description, needing to click on the left panel everytime is not a problem but somehow on playwright, it only show the first job description despite after highlight/select other job card. not sure what went wrong.\n\nany advice would be appreciated",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1pv93nw/scraping_job_on_indeed/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nvujwd4",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2025-12-25 09:22:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvus6me",
              "author": "webscraping-ModTeam",
              "text": "üëî Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
              "score": 0,
              "created_utc": "2025-12-25 10:53:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwe82p",
          "author": "i-am-hustling",
          "text": "You can try opening the job URL in a new tab",
          "score": 1,
          "created_utc": "2025-12-25 17:57:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7679b",
              "author": "Haikal019",
              "text": "i did but the problem is it detect bot. how do i bypass that then?",
              "score": 1,
              "created_utc": "2025-12-27 15:02:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw9f5mx",
          "author": "about7cars",
          "text": "Ive been using selenium, having troubles with bot detection too, worked good for a day. Pretty sure fingerprinting got me. Haven‚Äôt tried spoofing users and headers yet. The robots.txt shows /jobs are disallowed except for a few like google and twitter bots. \n\nInspect element, youll see the job listings, including the card stacks are pretty deeply nested, i found being a level higher and doing a css search worked best. Im not sure how playwright works, but I hope the info is helpful for figuring it out.",
          "score": 1,
          "created_utc": "2025-12-27 22:04:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxoz80",
      "title": "Webscraping with selenium",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pxoz80/webscraping_with_selenium/",
      "author": "Afedzi",
      "created_utc": "2025-12-28 11:14:24",
      "score": 0,
      "num_comments": 13,
      "upvote_ratio": 0.44,
      "text": "I am looking for a youtube tutorial playlist for using selenium to scrape website.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1pxoz80/webscraping_with_selenium/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwgaqa4",
          "author": "PresidentHoaks",
          "text": "Youtube search for scraping with selenium then.",
          "score": 10,
          "created_utc": "2025-12-28 23:42:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiarvd",
              "author": "abdullah-shaheer",
              "text": "Exactly üòÖ",
              "score": 3,
              "created_utc": "2025-12-29 07:16:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwjl55b",
              "author": "Relative_Rope4234",
              "text": "ü§£ü§£ü§£",
              "score": 1,
              "created_utc": "2025-12-29 13:46:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwk2gat",
              "author": "Afedzi",
              "text": "üòÇüòÇ",
              "score": 1,
              "created_utc": "2025-12-29 15:21:50",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwk2jwc",
              "author": "Afedzi",
              "text": "I don‚Äôt get the content I want",
              "score": 1,
              "created_utc": "2025-12-29 15:22:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwitoln",
          "author": "EitherAlternative985",
          "text": "Honestly, it doesn't seem very viable anymore because of its size; I think it's more useful for testing. You should use Puppiter or something else. Even the new one called PyDoll works perfectly.",
          "score": 1,
          "created_utc": "2025-12-29 10:12:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj9km6",
          "author": "v_maria",
          "text": "you dont need a youtube tutorial you can just copy paste from the official website https://www.selenium.dev/documentation/webdriver/getting_started/first_script/",
          "score": 1,
          "created_utc": "2025-12-29 12:29:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwk2f28",
              "author": "Afedzi",
              "text": "Thank you",
              "score": 1,
              "created_utc": "2025-12-29 15:21:40",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwmh5tk",
              "author": "ClockOfDeathTicks",
              "text": "Much better since the videos will either only cover the very surface of what you can do or just grab said documentation and read it in front of you like a child while typing the code",
              "score": 1,
              "created_utc": "2025-12-29 22:16:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwllzhj",
          "author": "hulleyrob",
          "text": "r/seleniumbase lots of examples on the site too",
          "score": 1,
          "created_utc": "2025-12-29 19:43:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrc334",
              "author": "Afedzi",
              "text": "Thank you..",
              "score": 2,
              "created_utc": "2025-12-30 17:01:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsf0pp",
          "author": "Significant-Body2932",
          "text": "I recommend using Playwright or DrissionPage for browser automation in Python. They support an asynchronous context.",
          "score": 1,
          "created_utc": "2025-12-30 20:03:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}