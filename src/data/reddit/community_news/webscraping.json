{
  "metadata": {
    "last_updated": "2026-02-21 16:49:58",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 19,
    "total_comments": 172,
    "file_size_bytes": 143961
  },
  "items": [
    {
      "id": "1r5712p",
      "title": "Scrapling v0.4 is here - Effortless Web Scraping for the Modern Web",
      "subreddit": "webscraping",
      "url": "https://i.redd.it/chi2m8gjjljg1.png",
      "author": "0xReaper",
      "created_utc": "2026-02-15 06:01:04",
      "score": 258,
      "num_comments": 39,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r5712p/scrapling_v04_is_here_effortless_web_scraping_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5h0uzf",
          "author": "Reddit_User_Original",
          "text": "Nice job. Been familiar with your project since v0.3. It's the best of its kind as far as i can tell. I use scrapling when using curl cffi is insufficient, and i need something more powerful. How do you stay on top of the anti bot tech? Have you had to implement changes in response to any new anti bot tech recently? Thanks so much for building this tool.",
          "score": 16,
          "created_utc": "2026-02-15 07:24:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5i40el",
              "author": "0xReaper",
              "text": "Thanks, mate. That means a lot to me. \n\nThe thing is, I have been working in the Web Scraping field for years, and since I made the library, I use it every day. So it's always under heavy testing from me; most of the time, I find issues before users report them because of that.\n\nRegarding security, before switching to Web Scraping, I spent about 8 years in the information security field, including bug hunting. So I was an ethical hacker before all of that. And I spent some time working as backend.",
              "score": 15,
              "created_utc": "2026-02-15 13:19:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5k4fsw",
          "author": "NoN4meBoy",
          "text": "Does it handle datadome ?",
          "score": 4,
          "created_utc": "2026-02-15 19:31:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hjjyo",
          "author": "Satobarri",
          "text": "Why can‚Äôt I decline your cookies on your page?",
          "score": 3,
          "created_utc": "2026-02-15 10:26:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5i30l4",
              "author": "0xReaper",
              "text": "I have fixed it, thanks for pointing that out",
              "score": 10,
              "created_utc": "2026-02-15 13:12:19",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5hwu1b",
              "author": "0xReaper",
              "text": "Oh, I didn‚Äôt notice that. Let me have a look at it, I have just switched to zensical with this update so I might have missed something in the configuration.",
              "score": 3,
              "created_utc": "2026-02-15 12:25:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5i31e4",
                  "author": "Satobarri",
                  "text": "Thanks. Not a biggie but makes it suspicious for European visitors.",
                  "score": 4,
                  "created_utc": "2026-02-15 13:12:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kfwlh",
          "author": "24props",
          "text": "I‚Äôm currently on my phone and will review this later. I believe that for many people today, due to the widespread use of AI coding, it will be beneficial to create a skill (agentskills.io) to assist users who utilize AI for development or integration. Only because LLMs are never trained on immediate new versions of anything and have knowledge gaps/cutoffs.",
          "score": 3,
          "created_utc": "2026-02-15 20:29:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kk0cb",
              "author": "0xReaper",
              "text": "Yes, I agree, I will work on this soon. I'm just taking a well-deserved rest before working on the next version. There is a lot more to add.",
              "score": 5,
              "created_utc": "2026-02-15 20:51:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nz6jy",
          "author": "Flat_Agent_9174",
          "text": "Wow, it's an amazing tool !",
          "score": 3,
          "created_utc": "2026-02-16 11:20:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o0e3k",
          "author": "Flat_Agent_9174",
          "text": "Can it bypass Datadome ?",
          "score": 3,
          "created_utc": "2026-02-16 11:31:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jn3on",
          "author": "JerryBond106",
          "text": "Should i use some vpn for this as well, so i don't get ip banned? (I'm new to this, i read proxy is included but don't know the big picture in scraping yet, as it changes rapidly and i wasn't ready to start safely yet)",
          "score": 2,
          "created_utc": "2026-02-15 18:06:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kepjy",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-15 20:23:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5mfpfp",
                  "author": "webscraping-ModTeam",
                  "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
                  "score": 1,
                  "created_utc": "2026-02-16 03:27:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kyzi6",
          "author": "515051505150",
          "text": "One thing I‚Äôve struggled with is determining the maximum number of requests per minute I can send to a site before getting rate limited or blocked. Is there a feature within scrapling that can help automatically determine the max threshold of scrapes before a site‚Äôs counter-measures kick in?",
          "score": 2,
          "created_utc": "2026-02-15 22:07:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o2dcz",
          "author": "imbuilding",
          "text": "Will be trying it out! Thanks",
          "score": 2,
          "created_utc": "2026-02-16 11:47:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5t5bjw",
          "author": "mischiefs",
          "text": "Great project mate! i'm not well versed in scrapping but i'm doing a pet project and got to use it. Got me impressed. Same feeling i got when i installed and tested tailscale, clickhouse or duckdb (more of a data engineer myself lol). it just work!",
          "score": 2,
          "created_utc": "2026-02-17 04:07:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xxi05",
              "author": "0xReaper",
              "text": "Thanks mate! that made my day :D",
              "score": 1,
              "created_utc": "2026-02-17 21:54:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o63g0q2",
          "author": "Careful_Ring2461",
          "text": "Made an Instagram and Tripadvisor scraper using Opus and your scrapling MCP without any issues. You're doing amazing work for newbies like me! ",
          "score": 2,
          "created_utc": "2026-02-18 18:09:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6agnrf",
          "author": "Afedzi",
          "text": "Sounds interesting. I will give it a try in my personal project and when I am able to navigate, will start informing my colleagues at work",
          "score": 2,
          "created_utc": "2026-02-19 19:05:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i11sy",
          "author": "Overall-Suit-5531",
          "text": "Interesting! Does it manage JavaScript too?",
          "score": 2,
          "created_utc": "2026-02-15 12:58:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5i31c0",
              "author": "0xReaper",
              "text": "yup",
              "score": 2,
              "created_utc": "2026-02-15 13:12:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hchnq",
          "author": "One-Spend379",
          "text": "Great job üëç \nCan it scrap allegro. pl ?",
          "score": 1,
          "created_utc": "2026-02-15 09:17:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jynxn",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-15 19:02:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mfrf7",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-16 03:27:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kdbhy",
          "author": "strasbourg69",
          "text": "Could i use this to scan for emails and phone numbers of for example plumbers, regionally targetted",
          "score": 1,
          "created_utc": "2026-02-15 20:16:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5krjs1",
          "author": "saadcarnot",
          "text": "Can it avoid anti bot stuff like google enterprise v3 captcha?",
          "score": 1,
          "created_utc": "2026-02-15 21:29:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nfd1o",
          "author": "ChallengeEmergency11",
          "text": "How free?",
          "score": 1,
          "created_utc": "2026-02-16 08:15:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5oqdnx",
          "author": "mayodoctur",
          "text": "Does this work for scraping news articles like Al Jazeera, Substack, blogs etc ?",
          "score": 1,
          "created_utc": "2026-02-16 14:21:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ot7op",
          "author": "RageQuitNub",
          "text": "very interesting, does it manage a list of proxy or we have to supply the proxy list?",
          "score": 1,
          "created_utc": "2026-02-16 14:36:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65a9vn",
              "author": "0xReaper",
              "text": "You have to supply it",
              "score": 1,
              "created_utc": "2026-02-18 23:18:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5p82uk",
          "author": "SnooFloofs641",
          "text": "How good is this with anti bot checks and stuff?",
          "score": 1,
          "created_utc": "2026-02-16 15:50:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5t3ae0",
          "author": "Muhammadwaleed",
          "text": "If I want to download videos from a social media site such as facebook such as my saved videos so I can clear my saved list, can it do that?",
          "score": 1,
          "created_utc": "2026-02-17 03:54:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uv0wy",
          "author": "Sensitive_Nobody409",
          "text": "Works with reCaptcha v3 Enterprise?",
          "score": 1,
          "created_utc": "2026-02-17 12:46:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wz6cb",
          "author": "arvcpl",
          "text": "will try it out, thanks",
          "score": 1,
          "created_utc": "2026-02-17 19:12:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o613430",
          "author": "Sparklist",
          "text": "Can I use to scrap photos from a airbnb accomodation page ?",
          "score": 1,
          "created_utc": "2026-02-18 10:25:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pjf4g",
          "author": "mikeb550",
          "text": "how do you deal with companies who forbid scraping their sites?  any of you customers get taken to court?",
          "score": 0,
          "created_utc": "2026-02-16 16:43:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5ptfe",
      "title": "I can scrape anything",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r5ptfe/i_can_scrape_anything/",
      "author": "0xMassii",
      "created_utc": "2026-02-15 20:57:12",
      "score": 68,
      "num_comments": 103,
      "upvote_ratio": 0.71,
      "text": "No selenium, playwright or puppeteer shit, I can scrape anything in full request mode, bypassing every bot protection. It doesn't matter if is Cloudflare, Akamai, PerimeterX etc  \nAfter years in this filed I believe it's time to give something back to the community. I'll start to release open source stuff for the people who want to learn. Let me know which one is the most interesting topic.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r5ptfe/i_can_scrape_anything/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5kqgqc",
          "author": "Raidrew",
          "text": "My body is ready",
          "score": 36,
          "created_utc": "2026-02-15 21:24:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kufee",
              "author": "heelstoo",
              "text": "Can you scrape me, Focker?",
              "score": 22,
              "created_utc": "2026-02-15 21:44:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5l5g7s",
                  "author": "emprezario",
                  "text": "You can scrape anything with nipples. ü§∑‚Äç‚ôÇÔ∏è",
                  "score": 12,
                  "created_utc": "2026-02-15 22:42:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kx6se",
          "author": "mizhgun",
          "text": "I can type 1000 characters per minute. It's just total gibberish, though.",
          "score": 17,
          "created_utc": "2026-02-15 21:58:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kytyf",
              "author": "0xMassii",
              "text": "No jokes here, do a check on me",
              "score": -8,
              "created_utc": "2026-02-15 22:07:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l2skh",
          "author": "indicava",
          "text": "Why the hell is this crap being upvoted?",
          "score": 32,
          "created_utc": "2026-02-15 22:28:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l3vi5",
              "author": "0xMassii",
              "text": "The real question is why not?",
              "score": -27,
              "created_utc": "2026-02-15 22:34:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l3e21",
          "author": "beachandbyte",
          "text": "Press X to doubt. \n\nAkami and Cloudflare inspect the TLS stack. It would be possible to spoof that but challenging. But then they pass back js that profiles the browser apis to suspicious requests and the payload in that js is constantly changing. So at least for those requests you would need to either mock the entire surface of the browser Api and return what they would consider reasonable values (at that point just using a browser is likely far easier). Won‚Äôt say it‚Äôs 100% not possible but unless your trick is proxying through some other process that is actually getting around all the protection you won‚Äôt be bypassing  cloudflare or Akami purely with requests.",
          "score": 8,
          "created_utc": "2026-02-15 22:31:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l45m1",
              "author": "0xMassii",
              "text": "Bro, bro, bro bot protection are my bitches\nI‚Äôm the man who destroy the whole ‚Ä¶‚Ä¶‚Ä¶‚Ä¶ website :)",
              "score": -16,
              "created_utc": "2026-02-15 22:35:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l9umx",
                  "author": "You_Cant_Win_This",
                  "text": "ok no we know you are fraud for sure",
                  "score": 5,
                  "created_utc": "2026-02-15 23:07:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kmtz5",
          "author": "EffectiveSeat1505",
          "text": "CF",
          "score": 3,
          "created_utc": "2026-02-15 21:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kymuh",
              "author": "0xMassii",
              "text": "I have a custom solver for CF, soon will be OS, I‚Äôll start from tomorrow",
              "score": 3,
              "created_utc": "2026-02-15 22:06:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5lqmym",
                  "author": "MurkBRA",
                  "text": "Everyone who tried to make this open source received a DMCA takedown notice.",
                  "score": 3,
                  "created_utc": "2026-02-16 00:46:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5kr2r4",
              "author": "UnlikelyLikably",
              "text": "Second that",
              "score": 1,
              "created_utc": "2026-02-15 21:27:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l632w",
          "author": "who_am_i_to_say_so",
          "text": "Your mom scraped deezenuts with her snaggletooth. Scraping must run in the family.",
          "score": 11,
          "created_utc": "2026-02-15 22:46:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l6f8f",
              "author": "0xMassii",
              "text": "No sense but good for u ü§£",
              "score": -3,
              "created_utc": "2026-02-15 22:47:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l6jx3",
                  "author": "who_am_i_to_say_so",
                  "text": "I couldn‚Äôt resist. Haha",
                  "score": 1,
                  "created_utc": "2026-02-15 22:48:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kllaj",
          "author": "yyavuz",
          "text": "let's gooooooo",
          "score": 8,
          "created_utc": "2026-02-15 20:59:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kp5vy",
          "author": "PTBKoo",
          "text": "Cloudflare turnstile, I have to open a chrome browser for a every single turnstile solve and scraping over 1k daily and my poor server can‚Äôt keep up.",
          "score": 3,
          "created_utc": "2026-02-15 21:17:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lnyvm",
          "author": "BossDailyGaming",
          "text": "Talk is cheap, make that gh repo",
          "score": 3,
          "created_utc": "2026-02-16 00:30:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kntfi",
          "author": "askolein",
          "text": "Interested. Def the non selenium request thing. How come? Direct webassembly?",
          "score": 2,
          "created_utc": "2026-02-15 21:10:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kpbqg",
          "author": "No-Exchange2961",
          "text": "Please post the link to the open source!",
          "score": 2,
          "created_utc": "2026-02-15 21:18:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ksxhi",
          "author": "sudbull",
          "text": "LinkedIn",
          "score": 2,
          "created_utc": "2026-02-15 21:36:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kyolu",
              "author": "0xMassii",
              "text": "Ez brother",
              "score": 2,
              "created_utc": "2026-02-15 22:06:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5lb6q1",
          "author": "EntrepreneurSea4283",
          "text": "What's the the hardest thing to scrape",
          "score": 2,
          "created_utc": "2026-02-15 23:14:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lbgki",
              "author": "0xMassii",
              "text": "It depends, for me few POST requests on Akamai gave me hard times",
              "score": 1,
              "created_utc": "2026-02-15 23:16:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5lufaj",
          "author": "crawford5002",
          "text": "Teach me your ways",
          "score": 2,
          "created_utc": "2026-02-16 01:09:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kpi7j",
          "author": "san-vicente",
          "text": "Let me know",
          "score": 1,
          "created_utc": "2026-02-15 21:19:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kq3i9",
          "author": "Any-Dig-3384",
          "text": "DM if legit",
          "score": 1,
          "created_utc": "2026-02-15 21:22:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kri7l",
          "author": "Ladytron2",
          "text": "Facebook events?",
          "score": 1,
          "created_utc": "2026-02-15 21:29:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ks7mg",
          "author": "Srijaa",
          "text": "I‚Äôm in!",
          "score": 1,
          "created_utc": "2026-02-15 21:33:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ku6zv",
          "author": "uneatenbreakfast",
          "text": "The chosen one",
          "score": 1,
          "created_utc": "2026-02-15 21:43:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kup2g",
          "author": "puzz-User",
          "text": "Let‚Äôs go!",
          "score": 1,
          "created_utc": "2026-02-15 21:45:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kvsse",
          "author": "living_david_aloca",
          "text": "Well let‚Äôs see it",
          "score": 1,
          "created_utc": "2026-02-15 21:51:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kw0gx",
          "author": "Sensitive_Nobody409",
          "text": "Recaptcha v3 pls ‚ù§Ô∏èüëèüèø",
          "score": 1,
          "created_utc": "2026-02-15 21:52:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kyx8n",
              "author": "0xMassii",
              "text": "Ez, but if you want to start from somewhere check solver online",
              "score": 1,
              "created_utc": "2026-02-15 22:07:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kw4m0",
          "author": "Sensitive_Nobody409",
          "text": "Build your own chromium?",
          "score": 1,
          "created_utc": "2026-02-15 21:53:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kx1va",
          "author": "slumdogbi",
          "text": "Amazon with sponsored products",
          "score": 1,
          "created_utc": "2026-02-15 21:57:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kyy8v",
              "author": "0xMassii",
              "text": "Ez",
              "score": 1,
              "created_utc": "2026-02-15 22:07:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ld0o2",
                  "author": "slumdogbi",
                  "text": "üëèüëèüëèüëè",
                  "score": 1,
                  "created_utc": "2026-02-15 23:25:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5lh0ov",
                  "author": "RealAmerik",
                  "text": "I'm interested in this as well.",
                  "score": 1,
                  "created_utc": "2026-02-15 23:49:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kydj8",
          "author": "Overall-Suit-5531",
          "text": "Do it",
          "score": 1,
          "created_utc": "2026-02-15 22:04:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l0cai",
          "author": "Dorkits",
          "text": "Where",
          "score": 1,
          "created_utc": "2026-02-15 22:15:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l0f36",
              "author": "0xMassii",
              "text": "Everywhere",
              "score": 0,
              "created_utc": "2026-02-15 22:15:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l0kvi",
                  "author": "Dorkits",
                  "text": "The link bro",
                  "score": 1,
                  "created_utc": "2026-02-15 22:16:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l1vnf",
          "author": "sojufles",
          "text": "How not to get rate limited by Cloudflare, with a scraper in a container using 1 IP?",
          "score": 1,
          "created_utc": "2026-02-15 22:23:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l2hlv",
              "author": "0xMassii",
              "text": "That‚Äôs is related to the website, you need to find vulns on the target or misconfig, otherwise cf will block you",
              "score": 1,
              "created_utc": "2026-02-15 22:26:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l34c6",
                  "author": "sojufles",
                  "text": "Thanks for the reply, it also seems that whenever my scraper in container is rate limited. I can access the site from my local browser. Do you have any idea or experiences why this happens?",
                  "score": 1,
                  "created_utc": "2026-02-15 22:29:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l2jsb",
          "author": "Antop90",
          "text": "The problem isn't emulating the browser, there are already plenty of super effective libraries for that. The real issue is having a pool of very expensive IPv4 addresses to rotate.",
          "score": 1,
          "created_utc": "2026-02-15 22:26:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l2xi4",
              "author": "0xMassii",
              "text": "Yeah, but you can scrape also with cheap resi, but you will be slower obv",
              "score": 1,
              "created_utc": "2026-02-15 22:28:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l3hye",
                  "author": "Antop90",
                  "text": "Sometimes speed is essential, and the only way to achieve it is with a solid proxy pool. no magical alternatives exist",
                  "score": 1,
                  "created_utc": "2026-02-15 22:32:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l2stb",
          "author": "Eudaimonic_me",
          "text": "Google trends",
          "score": 1,
          "created_utc": "2026-02-15 22:28:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l3qzm",
          "author": "saadcarnot",
          "text": "I am working on automating an time critical workflow, however it's protected with Recaptcha v3 Enterprise. I can't afford to wait for solver to get back. I need to avoid it all together. \n\nAny suggestions for me?",
          "score": 1,
          "created_utc": "2026-02-15 22:33:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l4a1w",
              "author": "0xMassii",
              "text": "Look for vulns on the website target",
              "score": 2,
              "created_utc": "2026-02-15 22:36:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l54i8",
                  "author": "saadcarnot",
                  "text": "Couldn't find any, can you list down what to look for? Site is recreation.gov",
                  "score": 1,
                  "created_utc": "2026-02-15 22:40:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5lawia",
          "author": "Inner_Grape_211",
          "text": "can u explain each of ur bypasses? with general tips pls?",
          "score": 1,
          "created_utc": "2026-02-15 23:13:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lbhn3",
              "author": "0xMassii",
              "text": "I‚Äôll do",
              "score": 1,
              "created_utc": "2026-02-15 23:16:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ls78m",
          "author": "TheKillerScope",
          "text": "Can you scrape Dune or GMGN? PolyMarket? If so, I'd be VERY interested in connecting with you and discussing a possible partnership.",
          "score": 1,
          "created_utc": "2026-02-16 00:55:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lus8f",
          "author": "sunrise_zc",
          "text": "customized browserÔºü",
          "score": 1,
          "created_utc": "2026-02-16 01:12:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m1ddn",
          "author": "RaiseRuntimeError",
          "text": "Is this r/webscrapingcirclejerk",
          "score": 1,
          "created_utc": "2026-02-16 01:54:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m7qli",
          "author": "Villain_99",
          "text": "LinkedIn please",
          "score": 1,
          "created_utc": "2026-02-16 02:35:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mas5b",
          "author": "G_S_7_wiz",
          "text": "How do you get all amzon reviews..you need cookies for all reviews",
          "score": 1,
          "created_utc": "2026-02-16 02:54:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mb88o",
          "author": "LifeShmucksSoMuch",
          "text": "can you bypass shape?",
          "score": 1,
          "created_utc": "2026-02-16 02:57:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l60ag",
          "author": "DotEnvironmental4718",
          "text": "Can you scrape my butt? It‚Äôs itching",
          "score": 1,
          "created_utc": "2026-02-15 22:45:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l6atr",
              "author": "0xMassii",
              "text": "Crazy statement",
              "score": 2,
              "created_utc": "2026-02-15 22:47:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l9cbf",
          "author": "tradegreek",
          "text": "How do you scrape stuff which requires constantly refreshed cookies (say every 5-10 mins) without using a tool like selenium to obtain the new cookies?",
          "score": 1,
          "created_utc": "2026-02-15 23:04:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l9ki9",
              "author": "0xMassii",
              "text": "Maybe i don‚Äôt need cookies either to get the data :)",
              "score": 2,
              "created_utc": "2026-02-15 23:05:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l9n2v",
                  "author": "tradegreek",
                  "text": "I don‚Äôt get how is that possible?",
                  "score": 0,
                  "created_utc": "2026-02-15 23:05:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5la1z9",
                  "author": "You_Cant_Win_This",
                  "text": "You do though",
                  "score": 0,
                  "created_utc": "2026-02-15 23:08:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5lg4cf",
          "author": "Afriendlywhiteguy",
          "text": "Teach me your ways",
          "score": 1,
          "created_utc": "2026-02-15 23:44:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lgsti",
          "author": "dca12345",
          "text": "What‚Äôs wrong with PlayWright?",
          "score": 1,
          "created_utc": "2026-02-15 23:48:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l1e5u",
          "author": "hulleyrob",
          "text": "Docker support?",
          "score": 0,
          "created_utc": "2026-02-15 22:20:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l2be9",
              "author": "0xMassii",
              "text": "We don‚Äôt need, but we can",
              "score": 2,
              "created_utc": "2026-02-15 22:25:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l2s5l",
                  "author": "hulleyrob",
                  "text": "Could be very interesting if it will still beat all the tech in a docker container I don‚Äôt think there is anything out there that can do that right now.",
                  "score": 0,
                  "created_utc": "2026-02-15 22:28:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l790o",
          "author": "Acceptable-Sea-3248",
          "text": "LinkedIn",
          "score": 0,
          "created_utc": "2026-02-15 22:52:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l7aws",
          "author": "Acceptable-Sea-3248",
          "text": "LinkedIn seems to be the most difficult",
          "score": 0,
          "created_utc": "2026-02-15 22:52:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ldt2m",
          "author": "TillOk5563",
          "text": "The ServiceNow variables fields?",
          "score": 0,
          "created_utc": "2026-02-15 23:30:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5liydm",
          "author": "oizysplutus",
          "text": "Enterprise hcap",
          "score": 0,
          "created_utc": "2026-02-16 00:01:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lk4i8",
          "author": "tuttipazzo",
          "text": "Ok.  You got me interested as well.",
          "score": 0,
          "created_utc": "2026-02-16 00:07:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ll7qd",
          "author": "Sensitive-Finger-404",
          "text": "facebook marketplace",
          "score": 0,
          "created_utc": "2026-02-16 00:14:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lmb1u",
          "author": "bgj556",
          "text": "‚Ä¶ yes please",
          "score": 0,
          "created_utc": "2026-02-16 00:20:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l7598",
          "author": "AdhesivenessEven7287",
          "text": "Reddit",
          "score": -1,
          "created_utc": "2026-02-15 22:51:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8s88p",
      "title": "I web scraped 72,728 courses from the catalogs of 7 Universities",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r8s88p/i_web_scraped_72728_courses_from_the_catalogs_of/",
      "author": "digital__navigator",
      "created_utc": "2026-02-19 06:50:55",
      "score": 49,
      "num_comments": 11,
      "upvote_ratio": 0.98,
      "text": "Hey everyone,\n\n  \nI used Python Requests and bs4 and sometimes Selenium to write web scraping scripts to web scrape course catalogs.\n\nHeres a small part of Stanfords.\n\nhttps://preview.redd.it/hngsv9w8dekg1.png?width=1856&format=png&auto=webp&s=20a86e74eeb460ddea53188184c4bdf190c5602d\n\nThen created a system to organize the data, put it in databases, query some statistics, and pipeline it into html files which I present on a website I created, called DegreeView.\n\n  \nI am not selling anything on the site. Its ***currently*** just a project.\n\n  \nThis allowed me to get the number of courses and departments in a universities course catalog, the longest and shortest coursename, and sort all departments by how many courses they have revealing the biggest and smallest departments. \n\nhttps://preview.redd.it/c92z8zaeeekg1.png?width=2926&format=png&auto=webp&s=d67f2dcaff5918d0a05d13890883de45669c96b6\n\n  \nAnd create a page for each department in the course catalog where I do something similar:\n\n* Get the number of courses in the department\n* The shortest and longest course name\n* Other things like what percent are upper-division courses, what percent are lower, and what percent are grad courses\n\nhttps://preview.redd.it/d8gz568feekg1.png?width=2890&format=png&auto=webp&s=ff20bf4324107446c81debdd588deb8accff44ad\n\nFor each university I have to write a custom web scraping script but the general structure of every universities catalog I have scraped is similar, so I haven't had to change too much for any one of them. The hardest was the first one I did, UT Austin, and also the real hardest part was creating the system that handles everything once the data is obtained and allows me to work with differentiating data across different universities. \n\n  \nAlso Stanford was hard to scrape cause I had to use Selenium to get Javascript rendered data.\n\n  \nWeb scraping is definitely the backbone of this project so hopefully some of you guys here find this interesting. \n\nThe only reason I kept this project going and didn't give up is because I always had in my mind that it would be very scalable, and I think it is. I just need to do more web scraping.\n\n  \nCheck out the site at [degreeviewsite.com](https://www.degreeviewsite.com/)",
      "is_original_content": false,
      "link_flair_text": "Scaling up üöÄ",
      "permalink": "https://reddit.com/r/webscraping/comments/1r8s88p/i_web_scraped_72728_courses_from_the_catalogs_of/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o67i76g",
          "author": "SignificantMath9703",
          "text": "Check on the website's mobile responsiveness but generally the idea is so superb",
          "score": 3,
          "created_utc": "2026-02-19 08:06:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67jh1a",
              "author": "digital__navigator",
              "text": "LMAOO yeah the home page is not responsive.\n\nIts generally not good practice to make excuses but...college is busy lmao.\n\nI realllyyy wanted to stop delaying a multi university launch.\n\n  \nAt least most other pages of the site should have a basic level of responsiveness.\n\nThanks for your reply üôè",
              "score": 5,
              "created_utc": "2026-02-19 08:18:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o687iyv",
          "author": "divided_capture_bro",
          "text": "Not much to sell since they are the product. Lots of course descriptions are boilerplate too.\n\n\nNow if you also had syllabi and course materials...",
          "score": 3,
          "created_utc": "2026-02-19 11:59:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a8ysl",
          "author": "Jay6719t",
          "text": "That's awesome, I just started learning this stuff not long ago, any tips on how to get out of terminal and into dashboards?",
          "score": 3,
          "created_utc": "2026-02-19 18:29:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6c15h5",
              "author": "digital__navigator",
              "text": "Thanks, and I'd recommend learning basic HTML and CSS and then Jinja.\n\nThere are lots of ways to create websites. Jinja is just a way to place variables onto HTML files. And you can also write some sort of quasi-Python code. I think it's really useful for presenting data on a webpage and is not hard (at least as far as I'm learned but you dont know what you dont know)",
              "score": 1,
              "created_utc": "2026-02-19 23:54:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6dbfk9",
                  "author": "Jay6719t",
                  "text": "Thank you, I'll definitely have to check out jinja and yeah I currently am working on csv and excel files so it'd be nice to have a way to display data better or a more organized way.",
                  "score": 1,
                  "created_utc": "2026-02-20 04:44:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67bh9c",
          "author": "V01DDev",
          "text": "Amazing, saving this for later, good job",
          "score": 2,
          "created_utc": "2026-02-19 07:05:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67cgjb",
              "author": "digital__navigator",
              "text": "Firstly big thanks means a lot.\n\nWell hey if you wanna support you can sign up for a newsletter I made for the site.\n\n[https://degreeview.beehiiv.com/](https://degreeview.beehiiv.com/)\n\nI'll simply be sending out updates on how progress is going, like adding new universities, working on the backend to add more statistics, or making major stylistic changes.\n\nI could be wrong and over ambitious here but I think I should be able to web scrape at least 50 universities which would be like 300,000+ courses on the site, and a potential user base (but by user theyll probably just visit the site once and never come back) of around 500,000 assuming an average university size of 10,000 students. ",
              "score": 1,
              "created_utc": "2026-02-19 07:13:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o68fbf2",
          "author": "Emotional_Still4526",
          "text": "Nice project man I am in the middle of something like that but for PhD will be amazing if we chat sometimes :)",
          "score": 2,
          "created_utc": "2026-02-19 12:54:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6c0npq",
              "author": "digital__navigator",
              "text": "Thanks üôè",
              "score": 1,
              "created_utc": "2026-02-19 23:51:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r7crb3",
      "title": "Reverse engineering the new Datadome VM üî•",
      "subreddit": "webscraping",
      "url": "https://github.com/xKiian/datadome-vm",
      "author": "xkiiann",
      "created_utc": "2026-02-17 17:34:44",
      "score": 33,
      "num_comments": 5,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r7crb3/reverse_engineering_the_new_datadome_vm/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5wsih5",
          "author": "ragingpot",
          "text": "I was literally just doing this and this popped up, awesome stuff!!",
          "score": 7,
          "created_utc": "2026-02-17 18:41:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5z47bi",
          "author": "Krokzter",
          "text": "Good work, good luck with the scholarship!",
          "score": 2,
          "created_utc": "2026-02-18 01:41:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o683oum",
          "author": "deepwalker_hq",
          "text": "great job",
          "score": 2,
          "created_utc": "2026-02-19 11:29:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wzmpi",
          "author": "arvcpl",
          "text": "that's very handy. thank you!",
          "score": 1,
          "created_utc": "2026-02-17 19:14:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67j4nr",
          "author": "RobSm",
          "text": "Good work",
          "score": 1,
          "created_utc": "2026-02-19 08:15:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7c3e9",
      "title": "WebMCP is insane....",
      "subreddit": "webscraping",
      "url": "https://v.redd.it/gzt3djhw73kg1",
      "author": "GeobotPY",
      "created_utc": "2026-02-17 17:11:54",
      "score": 22,
      "num_comments": 5,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "AI ‚ú®",
      "permalink": "https://reddit.com/r/webscraping/comments/1r7c3e9/webmcp_is_insane/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o60cp6h",
          "author": "THenrich",
          "text": "WebMCP works only with websites that voluntarily expose functions that expose their data.  \nSince popular websites protect their data they most probably won't expose their data through webMCP to the world. ",
          "score": 5,
          "created_utc": "2026-02-18 06:24:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60le1c",
              "author": "GeobotPY",
              "text": "At the current state yes, but cooked up a open-source community hub where you can upload configs for sites and then when other agents visit that site a chrome extension fetches webMCP's for that particular site. \n\n[webmcp-hub.com](http://webmcp-hub.com)",
              "score": 2,
              "created_utc": "2026-02-18 07:41:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wqdcy",
          "author": "somedude4949",
          "text": "If it becomes a thing I don't think everyone will adapt it because you are exposing internal endpoints to automations this is not gonna work for every website",
          "score": 2,
          "created_utc": "2026-02-17 18:31:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65e98f",
          "author": "Robertusit",
          "text": "You know how to use an AI, local or in cloud, to connect to Google Chrome and help to make a web scraping ?",
          "score": 1,
          "created_utc": "2026-02-18 23:39:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9qhse",
      "title": "Amazon WAF Solver API",
      "subreddit": "webscraping",
      "url": "https://github.com/jonathanyly/awswaf-solver-api",
      "author": "Jolle_",
      "created_utc": "2026-02-20 09:07:13",
      "score": 12,
      "num_comments": 3,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1r9qhse/amazon_waf_solver_api/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6e78iw",
          "author": "matty_fu",
          "text": "u/xkiiann how does this differ to the one you published last year?",
          "score": 2,
          "created_utc": "2026-02-20 09:25:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e7qvl",
              "author": "Jolle_",
              "text": "its still the same version so i think its not differing when it comes to solving successfully. However checking with Kians script I do some calculuations a bit different (not using zlib) ",
              "score": 2,
              "created_utc": "2026-02-20 09:30:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6efr70",
                  "author": "xkiiann",
                  "text": "I added captcha solving, not just pow",
                  "score": 1,
                  "created_utc": "2026-02-20 10:43:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r93d69",
      "title": "Need recommendations for web scraping tools",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r93d69/need_recommendations_for_web_scraping_tools/",
      "author": "mustazafi",
      "created_utc": "2026-02-19 16:13:24",
      "score": 12,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nI'm trying to scrape data from a song lyrics website (specifically Turkish/Arabic ilahi/nasheed lyrics from ilahisozleri.net). I reached out to the site owner and got explicit permission to scrape the content for my personal project ‚Äì they said it's fine since the lyrics are mostly public domain or user-contributed, and they're okay with it as long as I don't overload the server.\n\nThe problem is, there's no public API available. I asked if they could provide one or even a data dump, but they replied something like: \"Sorry, I don't have time to set up an API or export the database right now. Just build your own scraper, it's straightforward since the site is simple HTML.\"\n\nI don't have much experience with web scraping, but I know Python and want to do this ethically (with delays, user-agent, etc.). Can you recommend some beginner-friendly tools or libraries?\n\n* Preferably Python-based (like BeautifulSoup, Scrapy, or Selenium if needed for JS).\n* Free/open-source.\n* Tips on handling pagination (site has multiple pages per artist) and extracting lyrics cleanly (they're in tags).\n* Any anti-scrape best practices to avoid issues, even with permission?\n\nGoal is to pull all lyrics into a JSON/CSV for my app. Thanks in advance!\n\n(If anyone has scraped similar sites, share your code snippets or gotchas!)",
      "is_original_content": false,
      "link_flair_text": "AI ‚ú®",
      "permalink": "https://reddit.com/r/webscraping/comments/1r93d69/need_recommendations_for_web_scraping_tools/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6fuyuo",
          "author": "hasdata_com",
          "text": "Since you got explicit permission, ignore anyone telling u to buy residential proxies or heavy tools. Complete overkill. The structure is clean HTML, so:\n\n* Loop through alphabet (`?harf=A` to `?harf=Z`), extract artist links from `<div class=\"artistCard\">`\n* For each artist page, extract song links from `<article>` tags in `<div class=\"listing\">`\n* For each song, get lyrics from `<div class=\"reading\"><p>` and save to files organized by artist folders \n\nOr just use it :)\n\n    import requests\n    from bs4 import BeautifulSoup\n    import os\n    import time\n    import string\n    import re\n    \n    \n    BASE_URL = \"https://ilahisozleri.net\"\n    \n    \n    def safe_filename(name):\n    ¬† ¬† return re.sub(r'[^\\w\\s-]', '', name).strip()[:100]\n    \n    \n    def get_artists():\n    ¬† ¬† artists = []\n    ¬† ¬† letters = list(string.ascii_uppercase) + ['√á', 'ƒû', 'ƒ∞', '√ñ', '≈û', '√ú']\n    ¬† ¬† \n    ¬† ¬† for letter in letters:\n    ¬† ¬† ¬† ¬† url = f\"{BASE_URL}/sanatcilar?harf={letter}\"\n    ¬† ¬† ¬† ¬† print(f\"Scraping letter {letter}...\")\n    ¬† ¬† ¬† ¬† \n    ¬† ¬† ¬† ¬† try:\n    ¬† ¬† ¬† ¬† ¬† ¬† response = requests.get(url, timeout=10)\n    ¬† ¬† ¬† ¬† ¬† ¬† response.encoding = 'utf-8'\n    ¬† ¬† ¬† ¬† ¬† ¬† soup = BeautifulSoup(response.text, 'html.parser')\n    ¬† ¬† ¬† ¬† ¬† ¬† \n    ¬† ¬† ¬† ¬† ¬† ¬† for card in soup.find_all('div', class_='artistCard'):\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† link = card.find('a')\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† name = card.find('h2').get_text(strip=True)\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† if link:\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† artists.append({'name': name, 'url': link['href']})\n    ¬† ¬† ¬† ¬† ¬† ¬† \n    ¬† ¬† ¬† ¬† ¬† ¬† time.sleep(0.5)\n    ¬† ¬† ¬† ¬† except Exception as e:\n    ¬† ¬† ¬† ¬† ¬† ¬† print(f\"Error: {e}\")\n    ¬† ¬† \n    ¬† ¬† return artists\n    \n    \n    def get_songs(artist_url):\n    ¬† ¬† songs = []\n    ¬† ¬† try:\n    ¬† ¬† ¬† ¬† response = requests.get(artist_url, timeout=10)\n    ¬† ¬† ¬† ¬† response.encoding = 'utf-8'\n    ¬† ¬† ¬† ¬† soup = BeautifulSoup(response.text, 'html.parser')\n    ¬† ¬† ¬† ¬† \n    ¬† ¬† ¬† ¬† for article in soup.find_all('article'):\n    ¬† ¬† ¬† ¬† ¬† ¬† link = article.find('a')\n    ¬† ¬† ¬† ¬† ¬† ¬† if link:\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† title = link.get_text(strip=True)\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† href = link['href']\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† if not href.startswith('http'):\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† href = BASE_URL + href\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† songs.append({'title': title, 'url': href})\n    ¬† ¬† ¬† ¬† \n    ¬† ¬† ¬† ¬† time.sleep(0.5)\n    ¬† ¬† except Exception as e:\n    ¬† ¬† ¬† ¬† print(f\"Error: {e}\")\n    ¬† ¬† \n    ¬† ¬† return songs\n    \n    \n    def get_lyrics(song_url):\n    ¬† ¬† try:\n    ¬† ¬† ¬† ¬† response = requests.get(song_url, timeout=10)\n    ¬† ¬† ¬† ¬† response.encoding = 'utf-8'\n    ¬† ¬† ¬† ¬† soup = BeautifulSoup(response.text, 'html.parser')\n    ¬† ¬† ¬† ¬† \n    ¬† ¬† ¬† ¬† reading = soup.find('div', class_='reading')\n    ¬† ¬† ¬† ¬† if reading:\n    ¬† ¬† ¬† ¬† ¬† ¬† for ad in reading.find_all(['div', 'ins', 'iframe']):\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ad.decompose()\n    ¬† ¬† ¬† ¬† ¬† ¬† \n    ¬† ¬† ¬† ¬† ¬† ¬† paragraphs = reading.find_all('p')\n    ¬† ¬† ¬† ¬† ¬† ¬† lyrics = []\n    ¬† ¬† ¬† ¬† ¬† ¬† \n    ¬† ¬† ¬† ¬† ¬† ¬† for p in paragraphs:\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† text = p.decode_contents()\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† lines = re.split(r'<br\\s*/?>', text)\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† for line in lines:\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† clean = BeautifulSoup(line, 'html.parser').get_text(strip=True)\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† if clean and clean != '****': ¬†\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† lyrics.append(clean)\n    ¬† ¬† ¬† ¬† ¬† ¬† \n    ¬† ¬† ¬† ¬† ¬† ¬† return '\\n'.join(lyrics)\n    ¬† ¬† ¬† ¬† \n    ¬† ¬† except Exception as e:\n    ¬† ¬† ¬† ¬† print(f\"Error: {e}\")\n    ¬† ¬† \n    ¬† ¬† return None\n    \n    \n    def main():\n    ¬† ¬† os.makedirs('lyrics', exist_ok=True)\n    ¬† ¬† \n    ¬† ¬† print(\"Getting artists...\")\n    ¬† ¬† artists = get_artists()\n    ¬† ¬† print(f\"Found {len(artists)} artists\\n\")\n    ¬† ¬† \n    ¬† ¬† for i, artist in enumerate(artists, 1): \n    ¬† ¬† ¬† ¬† name = artist['name']\n    ¬† ¬† ¬† ¬† url = artist['url']\n    ¬† ¬† ¬† ¬† \n    ¬† ¬† ¬† ¬† print(f\"[{i}] {name}\")\n    ¬† ¬† ¬† ¬† \n    ¬† ¬† ¬† ¬† folder = os.path.join('lyrics', safe_filename(name))\n    ¬† ¬† ¬† ¬† os.makedirs(folder, exist_ok=True)\n    ¬† ¬† ¬† ¬† \n    ¬† ¬† ¬† ¬† songs = get_songs(url)\n    ¬† ¬† ¬† ¬† print(f\" ¬†Found {len(songs)} songs\")\n    ¬† ¬† ¬† ¬† \n    ¬† ¬† ¬† ¬† for j, song in enumerate(songs, 1): ¬†\n    ¬† ¬† ¬† ¬† ¬† ¬† title = song['title']\n    ¬† ¬† ¬† ¬† ¬† ¬† print(f\" ¬† ¬†[{j}] {title}\")\n    ¬† ¬† ¬† ¬† ¬† ¬† \n    ¬† ¬† ¬† ¬† ¬† ¬† lyrics = get_lyrics(song['url'])\n    ¬† ¬† ¬† ¬† ¬† ¬† \n    ¬† ¬† ¬† ¬† ¬† ¬† if lyrics:\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† filename = os.path.join(folder, f\"{safe_filename(title)}.txt\")\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† with open(filename, 'w', encoding='utf-8') as f:\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† f.write(f\"{title}\\n{'='*50}\\n\\n{lyrics}\")\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print(f\" ¬† ¬†Saved\")\n    ¬† ¬† ¬† ¬† ¬† ¬† else:\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print(f\" ¬† ¬†No lyrics\")\n    ¬† ¬† ¬† ¬† ¬† ¬† \n    ¬† ¬† ¬† ¬† ¬† ¬† time.sleep(0.5)\n    \n    \n    if __name__ == \"__main__\":\n    ¬† ¬† main()",
          "score": 14,
          "created_utc": "2026-02-20 15:51:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c0ig1",
          "author": "ScrapeAlchemist",
          "text": "Hi,\n\nSimple HTML, no JS rendering ‚Äî this is actually the easiest type of scraping to set up.\n\nHere's what I'd do: open the site in DevTools, grab the CSS selectors for the lyrics container, title, artist, and pagination links. Then paste those into ChatGPT/Claude with something like \"write me a Python scraper using requests + BeautifulSoup that extracts lyrics from this structure\" and share the HTML snippet. You'll get a working script in one shot that you can tweak from there.\n\nLLMs are surprisingly good at generating scrapers for static HTML sites. You describe the page structure, it writes the code. For a beginner this is the fastest path ‚Äî you'll learn the patterns as you adjust the output.\n\nA few tips:\n- Use `encoding='utf-8'` everywhere ‚Äî especially if the site has Turkish/Arabic text\n- Wrap each request in `try/except` so one failure doesn't kill the run\n- If pagination is per-artist, scrape the artist index first then loop through each\n\nFor a simple HTML site with permission, `requests` + `bs4` is all you need. No heavy frameworks.\n\nHope this helps!",
          "score": 4,
          "created_utc": "2026-02-19 23:51:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6aanh0",
          "author": "Jay6719t",
          "text": "Personally ive been using bs4 and requests, going to be going into APIs in a few weeks but if it is mostly html you want to target some parent elements and look inside those for pagination regex is pretty good and use loops to get through the pages, other than that you don't really need to worry about overloading servers for now. Selenium is for automation when theres logins or captchas usually, I'm currently working on a multi site Scraper if you'd like to keep in contact I could send you some notes when it's done on thing's ive learned",
          "score": 2,
          "created_utc": "2026-02-19 18:37:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bdvbw",
          "author": "Lemon_eats_orange",
          "text": "When scraping sites \"ethically\" when you've already been given permission, I'd say the main point is don't bring down the site. If we use ahref's traffic checker the site gets around 31,000 requests per month or around 0.7 requests to a webpage on its site per minute. This would mean depending on the site it would either be okay to spam 1000 requests per minute if they have a good backend or that could seem like a DDOS attack. Use your judgement and make a reasonable number of requests if using your own IP and know that perhaps different times have more traffic than other times.\n\nFor Scraping the site itself, you could use something simple like the following:  \n\\- Python Requests Library for making http requests.\n\n\\- Beautifulsoup for parsing the resulting html files.\n\nIf you need to get a list of all pages, the site has a sitemap. In the sitemap there are a list of links which lead to other links, though it does not give pagination: [https://ilahisozleri.net/sitemap.html](https://ilahisozleri.net/sitemap.html). You could create a script first to collect all pages by:\n\n\\- Making a request to the sitemap and then collecting each page. In the html, which you can find from clicking F12 you can see that each link is contained within a <a> element contained within a <td> element. You can then parse through and get each link and then use requests to visit these URLs and beautifulsoup to parse it.\n\nYou'll need to study the sitemap to see if it has everything you need.\n\nFor pagination, it seems the site may be quite simple. For example on [https://ilahisozleri.net/sanatci/hasan-dursun](https://ilahisozleri.net/sanatci/hasan-dursun) it seems that all pages are in elements underneath the class \"pages\". You can parse the URLs from the href for the next page, go to the next page, and then keep going until there are no more pages left.\n\nThe site itself doesn't seem to have many anti-bot defenses (no cloudflare, akamai, etc), and you can get to the site without the use of Javascript. If anything I think the only potential issue could be rate limiting or IP bans if you go crazy on the site with your own IP --> you can also purchase proxies to make your requests from multiple places.\n\nI'd say first make a script to make to an artist page using requests library and ensure you can collect the correct data from it by parsing it (URL's to song pages). After that make the script to collect the lyrics from the page and test it on a few pages. Once you've done that and gotten parsing down for these and getting the pages you can proceed to create the logic to collect from multiple pages.\n\nEdit: This is but one of many ways to go about it but there is so much more. If you don't know how to find elements in the developer tools you'll need to in order to correctly use any html parser to find the correct data.",
          "score": 1,
          "created_utc": "2026-02-19 21:47:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6egnnt",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-20 10:51:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ekl31",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-20 11:24:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6hrzxs",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-20 21:14:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ikvb1",
              "author": "webscraping-ModTeam",
              "text": "ü™ß Please review the sub rules üëâ",
              "score": 1,
              "created_utc": "2026-02-20 23:47:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6e1szh",
          "author": "jagdish1o1",
          "text": "Nothing is better than scrapy when it comes to large projects. It is a dedicated framework for web scraping only and since it‚Äôs a static website, scrapy is the best fit here. Use residential proxies and you‚Äôre good to go.",
          "score": -1,
          "created_utc": "2026-02-20 08:33:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r75el6",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r75el6/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-02-17 13:02:04",
      "score": 10,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levels‚Äîwhether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) üå±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1r75el6/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o60rdvm",
          "author": "Hopeless_Scraping",
          "text": "Identity ‚Äúcrisis‚Äù - how to monetize my ‚Äúskills‚Äù?\n\nhey guys\n\nbasically what the title says. i am currently lost on how to actually monetize my reversing and scraping skills and could use some advice from people actually doing this.\n\na bit about me... i don‚Äôt have a degree or any professional dev background. i'm 100% self taught. honestly since LLMs got so good i‚Äôve been able to \"code\" a ton of tools that were way out of my league before. yeah i still have to rely on them for the heavy lifting but it works.\n\nwhenever i see someone post a coding task i try to jump on it if i can handle it but it‚Äôs super inconsistent. over a whole year it barely adds up to anything because the tasks just aren't there frequently enough.\n\nthe thing is i have the setup. i have great resources for digital goods, unique proxy providers, the whole nine yards. i'm confident enough that i can answer like 95% of the questions in this sub without even touching an LLM.\n\ni know my way around rest apis and tls stuff (using curl_cffi with my own fingerprints etc). i'm familiar with cdp and camoufox for browser automation and i get how anti-bot mechanisms work. token harvesting, bypassing wafs/captchas, header construction... i can do all of that. i can reverse web apps and desktop apps too.\n\nso what now? i feel like i have the toolkit but no blueprint on how to turn this into a real income stream. \n\nanyone else been in this spot? how did you transition from just \"knowing stuff\" to actually getting paid?",
          "score": 5,
          "created_utc": "2026-02-18 08:37:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6914u3",
              "author": "themasterofbation",
              "text": "You need to start a SaaS.\n\nFind a site that is heavily protected that has valuable information - package it for B2B, sell access to that at a few hundred bucks per month\n\nMany businesses pay for \"leads\"\n\nFind sites where \"leads\" are available - a lead is someone that could/would/should be interested in yoru product.\n\nExample: Companies scrape local sites that show houses that have pulled a permit in the US. Then sell those to other contractors (i.e. I pull a permit to renovate my roof. That can mean that I will also need a connected service which is XYZ\").\n\nDont do this exactly, as others are doing it...but do something similar.\n\nI have a few ideas in my notes that I'll never get to...if you want - DM me",
              "score": 1,
              "created_utc": "2026-02-19 14:58:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o613w8n",
          "author": "Moiz_khurram",
          "text": "Looking for a technical co-founder not a hire, actual equity partner.\n\nBuilt a B2B lead data platform to $35K+ ARR with my brother. We aggregate data from GMB, BuiltWith, Crunchbase, and about ten other sources and deliver it through Slack to agencies. Two people, mostly manual, 450+ sales calls, 370+ community members.\n\nThe product needs to be rebuilt properly. The roadmap involves scraping infrastructure at scale, proxy rotation, anti bot evasion, LinkedIn data extraction, email verification pipelines, and eventually intent-based scraping from job boards. These are the exact problems this community solves every day.\n\nLooking for someone who's done this at real scale and wants co founder equity rather than a freelance rate. 15-30% equity, 4-year vest, 1 year cliff, legal entity already registered.\n\nIf that sounds interesting DM me with something you've built.",
          "score": 2,
          "created_utc": "2026-02-18 10:32:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67mth0",
              "author": "fts_now",
              "text": "Which markets do you serve right now?",
              "score": 1,
              "created_utc": "2026-02-19 08:51:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o69039d",
              "author": "themasterofbation",
              "text": "35k ARR (2.9k MRR) from 450+ sales calls?\n\nWhat is your CAC? ARPU? \n\nSeems like you're going the B2B acquisition route (Sales calls) but not really charging B2B prices or have not found PMF",
              "score": 1,
              "created_utc": "2026-02-19 14:52:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ykeff",
          "author": "Koyaanisquatsi_",
          "text": "Interesting thread, looking forward to connecting to fellow scrappers",
          "score": 1,
          "created_utc": "2026-02-17 23:54:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65swxd",
              "author": "Bitter_Caramel305",
              "text": "or fellow marketers",
              "score": 1,
              "created_utc": "2026-02-19 01:01:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o64k4c2",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-18 21:12:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o64qlxy",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-02-18 21:42:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6g1vsv",
          "author": "Moiz_khurram",
          "text": "Hiring Senior Python Engineer US-scale business intelligence pipeline maps plus websites plus scoring plus minimal API UI 500 fixed\n\n  \nWe run Scrapeamax, a B2B lead intelligence platform with paying customers. We need one senior Python engineer to build a production-ready pipeline that turns niche plus location into a clean business intelligence output CSV and JSON.\n\nBudget and timeline  \n$500 fixed  \n3 to 4 weeks  \nMilestones paid after we test and approve each one\n\nMust haves  \nPython  \nPlaywright or Botasaurus preferred  \nQueue based architecture with concurrent workers  \nResumable jobs with checkpointing  \nDeploy on our Google Cloud VM  \nDocumented code plus setup instructions\n\nScope  \nModule 1 Business source collector with location splitting for high US coverage  \nModule 2 Website crawler contact extraction social links tech stack basic website signals  \nModule 3 Scoring layer opportunity score plus data quality flags  \nModule 4 Minimal API create job status download CSV JSON optional webhook docs  \nModule 5 Minimal UI create job monitor queue download outputs\n\nCompliance note  \nProject must follow applicable laws and third party Terms of Service. Prefer official APIs or licensed sources where available.\n\nHow to apply  \nSend a DM with answers to these 5 questions and include proof links where possible  \n1 Describe a similar pipeline you built and share sample output or GitHub  \n2 How do you design reliability at scale queue retries rate limiting checkpointing  \n3 How would you do location splitting for high coverage across a large geography  \n4 What infrastructure approach do you recommend to balance cost and reliability  \n5 When upstream changes happen, what is your maintenance process and typical turnaround time\n\nIf your DM includes a short Loom walkthrough of a similar system, you go to the top of the list.",
          "score": 0,
          "created_utc": "2026-02-20 16:23:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6uo0k",
      "title": "Reddit Scraping for Academia",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r6uo0k/reddit_scraping_for_academia/",
      "author": "yousufq9",
      "created_utc": "2026-02-17 03:07:44",
      "score": 10,
      "num_comments": 8,
      "upvote_ratio": 0.86,
      "text": "Hey guys! Ive been trying to collect Reddit data for a project Im doing in my course and wanted to get some advice. I applied for the official API access using my institute email but my request was rejected. So I tried alternate methods such as Pushshift but it seems to have been restricted now. Also tried using reddit's JSON endpoints but that only gave me around 1000 of the most recent posts of a sub. Im trying to get all posts in 2024 and 2025 so that method doesnt work for me. Also tried using selenium on old reddit but havent been successful so far.  \n  \nDoes anyone have any suggestions for alternative methods to scrape subreddits or tips on how to get official API access? Any help would be appreciated!",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r6uo0k/reddit_scraping_for_academia/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5t093s",
          "author": "RandomPantsAppear",
          "text": "There is such sad irony here. Aaron Swartz - a cofounder of Reddit - was driven to suicide by criminal charges around illicitly scraping academic data. \n\nAnd now Reddit is rejecting applications from academia to access Reddit, and must be driven to scraping. \n\nFor those of us that believe in the free flow of information, this is unspeakably sad.",
          "score": 17,
          "created_utc": "2026-02-17 03:34:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5t0jtu",
          "author": "swapripper",
          "text": "You don‚Äôt need to scrape. Check pushshift sub for dumps on academictorrents. Download what you need & process/filter locally.",
          "score": 6,
          "created_utc": "2026-02-17 03:36:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xaue3",
              "author": "yousufq9",
              "text": "Thanks for the suggestion looking into this!",
              "score": 1,
              "created_utc": "2026-02-17 20:07:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tnleu",
          "author": "jackie-nohashtag",
          "text": "The real academic finding here is that Reddit would rather die than let you read it programmatically. Publish that.",
          "score": 3,
          "created_utc": "2026-02-17 06:25:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vheio",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-17 14:52:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vpd1s",
              "author": "webscraping-ModTeam",
              "text": "ü™ß Please review the sub rules üëâ",
              "score": 2,
              "created_utc": "2026-02-17 15:32:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5us8z2",
          "author": "Gold_Emphasis1325",
          "text": "Sounds like you're trying to do stuff you're not supposed to. Maybe find another way to apply your craft/learning. There are so many open source datasets... Scraping reddit, bots... these are all increasingly frowned upon as people are experience very real, extreme impacts to their lives as a result of oversold and misused automation, Agents, AI and LLMs",
          "score": 1,
          "created_utc": "2026-02-17 12:27:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r642hh",
      "title": "anyone else tired of ai driven web automation breaking every week?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r642hh/anyone_else_tired_of_ai_driven_web_automation/",
      "author": "Ok_Abrocoma_6369",
      "created_utc": "2026-02-16 08:20:11",
      "score": 10,
      "num_comments": 30,
      "upvote_ratio": 0.76,
      "text": "Seriously, my python scrapers fall apart the moment a site changes a class name or restructures a div.   \nwe mainly monitor competitor pricing, collect public data, and automate internal dashboards but maintaining scripts is killing productivity.   \ni have heard ai can make scrapers more resilient, teaching a system to understand a page and find data on its own.\n\ni am curious what people are actually running in production:   \nwhat does your stack look like?  \ndo you use ai powered web interaction or llms to control browsers?   \nhow do you handle scaling and avoiding blocks in the cloud?\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r642hh/anyone_else_tired_of_ai_driven_web_automation/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5nhyub",
          "author": "CouldBeNapping",
          "text": "We solved it by automating browsers in Windows. 25 VPS‚Äô with a bare bones Windows 10 or 11 install. \nRotating VPNs and residential proxies. \n\nBeen 100% successful for the last 18 months.",
          "score": 6,
          "created_utc": "2026-02-16 08:40:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nuhcs",
              "author": "HarambeTenSei",
              "text": "Not even Linux browsers?",
              "score": 1,
              "created_utc": "2026-02-16 10:38:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5nxerj",
                  "author": "CouldBeNapping",
                  "text": "Most \"real\" people who browse online stores are on Windows.   \nJust adds to the legitimacy of the user profile.",
                  "score": 4,
                  "created_utc": "2026-02-16 11:05:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6gv7fs",
              "author": "VonDenBerg",
              "text": "Can you elaborate on this? ",
              "score": 1,
              "created_utc": "2026-02-20 18:36:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6kvw3c",
                  "author": "CouldBeNapping",
                  "text": "Which bit specifically, it's a big operation",
                  "score": 1,
                  "created_utc": "2026-02-21 10:16:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ngnov",
          "author": "ProgrammerRadiant847",
          "text": "we have been testing a few different frameworks and tbh its still a bit of a patchwork.",
          "score": 2,
          "created_utc": "2026-02-16 08:27:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nwflt",
          "author": "AdhesivenessOld8612",
          "text": "ran into the exact same issue with our pricing monitoring. we have been using anchor browser for about 6 months now and its been amazing for stability. its built around ai native automation platform principles, so the agent actually understands page structure instead of relying on selectors. We set it up to track 50+ e-commerce sites and it adapts to minor layout changes on its own. The biggest win is not having a developer on-call just to fix broken selectors every week",
          "score": 2,
          "created_utc": "2026-02-16 10:56:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nhm4x",
          "author": "Azuriteh",
          "text": "I don't recommend AI scrapers at all, the cost scales a lot.\n\nIn any case, if you want to truly do it like that, I personally haven't found any library and actually delivers, you have to build your own agentic harness, but you can take inspiration on already existing harnesses for other tools to make things easier for you.\n\nAnother problem is that even for existing scraping AI libraries, they use easily detected browser emulators, e.g. Playwright, which adds another level of me not wanting to use them.",
          "score": 2,
          "created_utc": "2026-02-16 08:37:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nhvdo",
              "author": "Azuriteh",
              "text": "For scaling it's the same old story: use proxies according to the sophistication of the anti-bot defenses of the site and use camouflaged browsers, keeping track of antibot cookies per browser session.\n\nI usually try to reverse engineer the websites to instead just use TLS fingerprinting mimicry though, as it scales much better, but not always possible sadly (due to IP quality when deploying in cloud most of the time!) ",
              "score": 2,
              "created_utc": "2026-02-16 08:39:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5nirxq",
              "author": "One_Development8489",
              "text": "Playwrath + claude code with chrome plugin (which can auto open chrome and debug as a agent)",
              "score": 1,
              "created_utc": "2026-02-16 08:48:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nhroz",
          "author": "Worth-Culture5131",
          "text": "cost is the killer. we built a prototype with an llm orchestrator, and it was brilliant... until we saw the api bill for 10k pages a day. ",
          "score": 1,
          "created_utc": "2026-02-16 08:38:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nvak6",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-16 10:46:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nycyr",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-02-16 11:13:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5p62jm",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-16 15:41:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qkd9y",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-02-16 19:34:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5pmxjh",
          "author": "Coding-Doctor-Omar",
          "text": "Dont rely on normal html parsing. Look for either internal API requests or json blobs inside script tags in the page source.",
          "score": 1,
          "created_utc": "2026-02-16 16:59:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hcfgc",
              "author": "Ok-Taste-5844",
              "text": "Do you have any good resources to learn how to scrape using internal API requests? I‚Äôve been managing with chatgpt but don‚Äôt fully understand it.",
              "score": 2,
              "created_utc": "2026-02-20 19:57:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6k4fwm",
                  "author": "Coding-Doctor-Omar",
                  "text": "The  YT channel that taught me the fundamentals is called John Watson Rooney. This channel literally revolutionized my scraping ability. I highly recommend it. Also, out of curiosity, I would like to know in what ways chatgpt helps you do this.",
                  "score": 1,
                  "created_utc": "2026-02-21 05:54:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o61pxk2",
          "author": "jagdish1o1",
          "text": "We‚Äôve also created an internal tool to track prices of our competitors, I‚Äôve built the tool using cloudflare browser rendering it has a built in AI endpoint which let you describe schema with a prompt. \n\nThe cloudflare browser rendering respect bot protections, if sites that you‚Äôre scraping has strong bot protection that this might not work.",
          "score": 1,
          "created_utc": "2026-02-18 13:13:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63v9tb",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-18 19:17:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63zeps",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-18 19:36:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o65u30y",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-19 01:08:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6654y2",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-19 02:12:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6grf5q",
          "author": "VonDenBerg",
          "text": "working on getting a github agent to detect anomalies within the db to then auto patch/heal. it's literally what llm's are perfect for. ",
          "score": 1,
          "created_utc": "2026-02-20 18:19:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ofrca",
          "author": "forklingo",
          "text": "yeah this is basically the tax you pay for scraping anything modern. in my experience ai doesn‚Äôt magically fix brittle selectors, it just moves the brittleness up a layer unless you‚Äôre really thoughtful about how you structure it. we‚Äôve had better luck combining solid dom heuristics, fallback selectors, and some light semantic matching rather than full llm driven browsing. for scaling and blocks it‚Äôs mostly about boring stuff like good proxy rotation, sane request rates, and making traffic look human instead of blasting endpoints. curious if anyone here is actually running llm controlled browsers in prod without the costs getting wild.",
          "score": 0,
          "created_utc": "2026-02-16 13:22:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7i5gt",
      "title": "How do i deal with cloudflare turnstile anti-bot using curl cffi?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r7i5gt/how_do_i_deal_with_cloudflare_turnstile_antibot/",
      "author": "letopeto",
      "created_utc": "2026-02-17 20:43:46",
      "score": 9,
      "num_comments": 7,
      "upvote_ratio": 0.86,
      "text": "Hey folks I'm trying to do some light scraping against a Cloudflare-protected site and I‚Äôm running into issues. Was wondering if anyone experienced can provide some advice/tips.\n\n**What I‚Äôm doing**\n\nUse a stealth browser (e.g., nodriver) to load the target page and complete whatever Cloudflare presents (no issues with this, i get the cf bypassed and cf clearance cookie).\n\nAfter the browser run, I extract cookies (notably cf_clearance, plus any other set cookies) and then switch to a lightweight HTTP client (curl-cffi) for the actual requests.\n\nThe browser is pinned to a specific UA / UA-latest (e.g., ‚ÄúChrome v144‚Äù UA string).\n\nIn curl-cffi, I attach the cookie jar + headers and use an impersonation profile like impersonate=\"chrome-latest\".\n\n**The issue**\n\nEven with the cookies present, several times the curl-cffi request still gets hit with a Cloudflare challenge again even though the cookies has not expired (could have been just retrieved 5 seconds ago).\n\n**any idea why this is happening? my current hypothesis right now is -**\n\nIs this happening because the clearance/session is bound to signals beyond cookies, like:\n\n* UA + TLS fingerprint mismatch (stealth browser chrome-latest profile might be say ‚ÄúChrome 144‚Äù vs curl-cffi ‚Äúchrome-latest‚Äù might be Chrome 143 or something?)\n* Or could it be something else?\n\nQuestions\n\n* How important is it to match the stealth browser version with the curl-cffi version? If this is indeed the underlying issue, whats the best way to \"synchronize\" the chrome version profile in the stealth browser vs the curl-cffi chrome version profile it is using? I don't want to pin it specifically to a version like chrome v144 because then i have to go every single time and update the version manually (and if the version gets too old it likely will trigger an anti-bot challenge as well).\n\n* Is this potential mismatch an issue, or is curl-cffi just not great and triggers the cf challenge often?",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1r7i5gt/how_do_i_deal_with_cloudflare_turnstile_antibot/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5xje0c",
          "author": "GillesQuenot",
          "text": "You need to stick to the same version on both Chrome and curl_ffi, because TLS fingerprinting reveal the ciphers order that is different from version to version",
          "score": 4,
          "created_utc": "2026-02-17 20:48:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61s4fr",
              "author": "WhyWontThisWork",
              "text": "They are doing that level of inspection? And it never changes the order?",
              "score": 1,
              "created_utc": "2026-02-18 13:26:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o62lbkp",
                  "author": "Hour_Analyst_7765",
                  "text": "The TLS is kind of like the headers for the HTTPS session/tunnel.\n\nYou can also check a browser footprint by checking the HTTP headers, in particular user agent, the Accept and Sec headers. But also the order. What curl-cffi does for you, is spoof the same TLS algorithms/options and HTTP header values, including the orderening.\n\nNow there is 1 caveat: some TLS browsers (Chrome‚Äö do some \"greasing\" to randomize TLS order iirc. But HTTP does not. So there are various fingerprints like JA3 and JA4(H) that represents these properties.\n\nWAF look at these properties to determine if a browser is real. If any of these is off, its a giant red flag. TLS and HTTP fingerprints don't change per user or OS, but it can change per browser type and version.\n\nOne problem with curl-cffi though: the spoofing is quite static. And in their infinite wisedom in the latest versions, they're hard coding their headers \\*inside\\* the curl executable.\n\nSec-Fetch-Dest for example, specifies what kind of page the browser is expecting to be requested. For HTML pages this is \"document\", but for CSS, javascript or images this is different. Even iframes get denoted as such. I personally haven't encountered many sites that directly blocks access if you get these wrong (maybe video embeds that get denied if you open that page in a separate tab).. but I won't be surprised if some WAF monitor these headers for bot detection, and we'll see more 403's if you get them wrong.",
                  "score": 1,
                  "created_utc": "2026-02-18 15:51:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o60k9xl",
          "author": "sojufles",
          "text": "Followed!",
          "score": 1,
          "created_utc": "2026-02-18 07:31:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c8g8o",
          "author": "ScrapeAlchemist",
          "text": "Hi,\n\nYour hypothesis is right ‚Äî TLS fingerprint mismatch.\n\n`cf_clearance` is bound to a session fingerprint: TLS cipher suite order, HTTP/2 settings frame, header order, User-Agent. When curl-cffi sends `impersonate=\"chrome-latest\"`, its TLS handshake might present Chrome 131 ciphers while your browser was fingerprinted as Chrome 144. Cloudflare sees the mismatch and re-challenges.\n\nFixes:\n\n1. **Pin both to the same Chrome version explicitly.** Annoying to maintain but reliable.\n2. **Check what curl-cffi supports.** Run `curl_cffi.requests.BrowserType` ‚Äî if latest is `chrome124` and your browser is on 144, there's always a gap.\n3. **Skip the split.** Keep everything in the headless browser and use its network layer directly.\n4. **If you must split:** Extract JA3/JA4 fingerprint from your browser session and compare to curl-cffi's output.\n\nAlso watch HTTP/2 pseudo-header order ‚Äî Cloudflare checks that too.",
          "score": 1,
          "created_utc": "2026-02-20 00:36:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r61a90",
      "title": "Need tought websites to scrape for testing",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r61a90/need_tought_websites_to_scrape_for_testing/",
      "author": "Transformand",
      "created_utc": "2026-02-16 05:39:35",
      "score": 9,
      "num_comments": 28,
      "upvote_ratio": 0.85,
      "text": "I've been developing my own piece of code, that so far has been able to bypass anti-bot security I had a tough time cracking before at scale (such as PerimiterX).\n\nCan you share what sites you think are difficult to access/scrape?\n\nI want to test out my scraper more before open sourcing it",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r61a90/need_tought_websites_to_scrape_for_testing/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5mz52h",
          "author": "ry8",
          "text": "Try BassProShop.",
          "score": 5,
          "created_utc": "2026-02-16 05:50:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mz7p3",
          "author": "Lemon_eats_orange",
          "text": "Jokingly if you can get past the anti bot defenses on piracy anime sites that would be a site to see. Though like seriously you open the chrome developer tools on some of those sites and they will feedback loop you and run your resources dry. \n\nOff the top of my head these sites might be difficult. \nWalmart.com, zoro.com, naver.com and it's subdomains can be incredibly difficult, hermes.com, safeway.com. Definitely shopee.",
          "score": 3,
          "created_utc": "2026-02-16 05:51:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61uds4",
              "author": "FerencS",
              "text": "Yeah, I‚Äôve heard of some fucking dark magic on anime sites",
              "score": 2,
              "created_utc": "2026-02-18 13:38:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6c5vl8",
                  "author": "Lemon_eats_orange",
                  "text": "If any normal major site used what they are using I'd bet no one could scrape them. I tried opening developer tools on one and it just crashed. Once it was closed poof all good. Not even google does that.",
                  "score": 1,
                  "created_utc": "2026-02-20 00:21:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5n45v8",
          "author": "thePsychonautDad",
          "text": "Protonmail email creation steps.",
          "score": 5,
          "created_utc": "2026-02-16 06:33:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xblq9",
              "author": "GillesQuenot",
              "text": "Did you managed to pass the captcha? Looks tough. IA required, maybe a local VLM. Do you have some hints?",
              "score": 2,
              "created_utc": "2026-02-17 20:10:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ap4uq",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-19 19:46:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b5j9v",
                  "author": "webscraping-ModTeam",
                  "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
                  "score": 1,
                  "created_utc": "2026-02-19 21:06:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5r6ztk",
          "author": "ertostik",
          "text": "Let me know if someone pass sahibinden.com protection.",
          "score": 2,
          "created_utc": "2026-02-16 21:25:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5naddf",
          "author": "LT823",
          "text": "Instagram followers of profile",
          "score": 1,
          "created_utc": "2026-02-16 07:29:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nc08b",
          "author": "GuNiKz",
          "text": "reuters, I tried once, but I wasn't successful ",
          "score": 1,
          "created_utc": "2026-02-16 07:44:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ndn62",
          "author": "Little-traveler-1995",
          "text": "Try Hyatt hotel, I guess without using any browser automation tool it is highly impossible to crawl any link.",
          "score": 1,
          "created_utc": "2026-02-16 07:59:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nleq8",
          "author": "trololololol",
          "text": "Google Shopping",
          "score": 1,
          "created_utc": "2026-02-16 09:13:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nm4zv",
          "author": "Unlikely1529",
          "text": "bet365 hehe",
          "score": 1,
          "created_utc": "2026-02-16 09:20:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5no5cn",
          "author": "brateq",
          "text": "Aliexpress, Darty(dot)com",
          "score": 1,
          "created_utc": "2026-02-16 09:39:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5omyc2",
          "author": "vorty212",
          "text": "builtwith",
          "score": 1,
          "created_utc": "2026-02-16 14:02:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5op22g",
          "author": "Krokzter",
          "text": "Idealista is the most aggressive use of Datadome I've seen",
          "score": 1,
          "created_utc": "2026-02-16 14:14:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xbya5",
              "author": "GillesQuenot",
              "text": "Take a look at mobile.de `^^`",
              "score": 1,
              "created_utc": "2026-02-17 20:12:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5uxgdw",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-17 13:01:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vpyg2",
              "author": "webscraping-ModTeam",
              "text": "üëî Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
              "score": 1,
              "created_utc": "2026-02-17 15:34:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5w1c83",
          "author": "jagdish1o1",
          "text": "try this one \"extDOTto\" it's a torrent related website with very strong bot protection you can also try with \"hianimeDOTto\" \n\nlet me know once you make it live i'll try it. ",
          "score": 1,
          "created_utc": "2026-02-17 16:31:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ykluc",
          "author": "Legend_Troll_007",
          "text": "LinkedIn",
          "score": 1,
          "created_utc": "2026-02-17 23:55:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65u92l",
          "author": "Gold_Emphasis1325",
          "text": "Indeed, Facebook, [msn.com](http://msn.com)",
          "score": 1,
          "created_utc": "2026-02-19 01:09:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n08hq",
          "author": "nofilmincamera",
          "text": "Sounds kind of insane but I like banks for this. Specifically self service portals that are public. Tended to have recaptcha 3 plus latered protection.",
          "score": 0,
          "created_utc": "2026-02-16 05:59:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n7b3a",
          "author": "scorpiock",
          "text": "Try payment, social media sites",
          "score": 0,
          "created_utc": "2026-02-16 07:01:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7gbwk",
      "title": "how do you decide when something truly requires proxies?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r7gbwk/how_do_you_decide_when_something_truly_requires/",
      "author": "Free-Path-5550",
      "created_utc": "2026-02-17 19:38:16",
      "score": 8,
      "num_comments": 13,
      "upvote_ratio": 0.8,
      "text": "Ok so Im still new and learning this space. I got into it because I was building another app and realized data was the moat. Two weeks later my hyper focus has me deep in this.\n\nSo far Ive built about a dozen tools for different sites at different difficulty levels and theyve worked... mostly. Now I hit a site that seems like it might require a proxy.\n\nBut my real question is not just ‚Äúshould I use a proxy‚Äù ..its how do you reason about access patterns and anti-bot defenses before deciding to add infrastructure like proxies?\n\nE.g. Recently I ran into another harder site and most advice online just said use proxies. I didnt want to jump straight to paying for infrastructure so I kept digging. Eventually I found a post suggesting trying the mobile app. I did a MITM looked at the mobile API and that ended up working with a high success rate.\n\nThat made me realize if I had just followed the first advice I saw I wouldnt have learned anything.\n\nSo how do you decide when something truly requires proxies versus when you just havent found the right access pattern yet. Are there signals you look for or is it mostly experience.",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r7gbwk/how_do_you_decide_when_something_truly_requires/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5xalxu",
          "author": "Azuriteh",
          "text": "A small tip: everyone says \"just use proxies\" because they sell proxies.\n\n  \nNow, true scalable web scraping starts from tricks like the one you did, but most of the time if the app is serious enough it won't work at all. More and more apps have anti-bot protected endpoints to the point the MITM gets detected straight away unless you're using an actual Android device connected through ADB to your computer.\n\nMy rule of thumb is, you ONLY use proxies when you need to scale or you need more concurrent requests and you start getting blocked because of that usage pattern. It doesn't make sense to start using proxies if you can't reliably bypass protections first.\n\nAlso don't go straight for \"residential\" or \"mobile\" proxies. Try cheap datacenter proxies first, they can take you very far for quite a lot of things!! But of course they have their limitations too, mainly due to their IP reputation. For example, in your use case, since I'm guessing you found an unprotected endpoint, they might still have some rate limiting in place, so you'll get 429's pretty often, the way to bypass that is to have for example 100 datacenter proxies and making requests in each of those proxies. If you found out that the limit of requests per IP is of about 50 reqs/min, then you can have 50 reqs/min \\* amount of proxies you have.",
          "score": 14,
          "created_utc": "2026-02-17 20:05:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xdb4z",
              "author": "scrapingtryhard",
              "text": "also want to add some sites support ipv6 proxies which are wayyy cheaper its a trick many people dont know, but most cloudflare sites have ipv6 on by default",
              "score": 8,
              "created_utc": "2026-02-17 20:18:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o62uiyj",
          "author": "hasdata_com",
          "text": "Proxies are for when you've tried everything else and still get blocked. Or when volume is high enough that you hit rate limits no matter what. Also if you need specific geo, like SERP data changes by country, so you need proxies in those locations. But try everything else first.",
          "score": 11,
          "created_utc": "2026-02-18 16:33:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5z1yee",
          "author": "Wise_Top4267",
          "text": "You start using proxies when the website starts blocking you. Similarly, if you're going to do a massive scraping, be careful who you scrape and where, because if your ISP doesn't change your IP address, they could add you to blacklists, etc., resulting in a massive block for something as simple as accessing Amazon.",
          "score": 6,
          "created_utc": "2026-02-18 01:29:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60eyr2",
          "author": "CriticalOfSociety",
          "text": "If you have to ask this question then you probably don't need proxies.\n\nProxies are only ever needed when you need concurrency or any type of high scale scraping, since any scraper is much easier to detect when it's sending 10+ requests/s through the same IP. And even this depends on the site's antibot system and it's not always the case.",
          "score": 2,
          "created_utc": "2026-02-18 06:44:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60z5ny",
          "author": "Pauloedsonjk",
          "text": "Always use a proxy, because you can be blocked and easily identified.\nI analyze dev tools without proxy, and get blocked in any sites for any minutes or hours, then I did a hot spot in my smartphone to analyze the dev tools again.\nIt is possible to use a proxy in your browser too.",
          "score": 2,
          "created_utc": "2026-02-18 09:49:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62cqvi",
          "author": "justincampbelldesign",
          "text": "I scrape reviews from the google playstore 100k in one go. I look for an app then download all the reviews (star rating, review text, reviewer name, etc.)   \nIn this case I decided that I needed proxies because I didn't want jobs to fail or get blocked part way through.",
          "score": 2,
          "created_utc": "2026-02-18 15:12:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63l8yx",
          "author": "Hour_Analyst_7765",
          "text": "Proxies are my default. My whole framework is set up to use them by default.\n\nThe nice thing is I don't have to worry about rate limiting that much anymore. Now I'm not hammering sites like I'm DDOSing them -- but more like: ooh I've launched 3 jobs in parallel? Oh well, I'm not getting 429 (Too Many Requests) so everything just sails smoothly.\n\nAnd its not like most of my scrapers run jobs that often (most of them only have to complete once a day, some on a hourly basis).\n\nBut still, I do have a few that launches the indexer every 1-2min. And then on top comes new content to grab, plus any images, ajax data, etc. on top. It all adds up and even though I could set really long delays between everything, its still nice to see everything being pulled in swiftly with minimal latency.\n\nAnother advantage: if one IP does get blocked for some duration, I'm not bothered about it at all. All my scripts auto-retry failed jobs. If I'm scraping from 1 IP, 24/7, refreshing the index every 2 minutes.. yeah I'm guessing I will be blocked within a few hours to days. So this is also about redundancy than necessary to function.\n\nFood for thought: if you rely on one proxy provider, and for whatever reason that provider goes down, they are getting blocked by Cloudflare, or they pull the plug for their service, what do you do? At present, I'm DIY'ing a lot of my proxy management, rotation and cloudflare challenges. But inevitably that script will break someday and even though I've redundant proxies, I do not have redundant scraping strategies. So I'm thinking about adding 1 or even 2 tiers of backup strategies in case that fails. Things like scraping APIs that promise a turn-key solution for grabbing HTML from the web, but cost a lot more $$$ per request. But for commercial jobs, it may be worth using if a script falls over on the weekend or holiday due to some (semi-)foreseeable cause.",
          "score": 2,
          "created_utc": "2026-02-18 18:32:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5y92vo",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-17 22:52:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zh1i0",
              "author": "webscraping-ModTeam",
              "text": "üö´ü§ñ No bots",
              "score": 1,
              "created_utc": "2026-02-18 02:48:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o60pahd",
          "author": "Free-Path-5550",
          "text": "Thanks for all the responses. I'm sending very few requests at a time right now so it's not really a scale thing. more the anti-bot side that had me wondering. I was trying to understand the reasoning process before reaching for infrastructure and whether I was on the right track. Sounds like I am so I'll keep plugging away and for sites i cant work through, ill just come back when i know more.",
          "score": 1,
          "created_utc": "2026-02-18 08:17:16",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o61bui5",
          "author": "WhyWontThisWork",
          "text": "What did you use to MITM it?",
          "score": 1,
          "created_utc": "2026-02-18 11:38:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4phlg",
      "title": "Payment processors for a scraping SaaS (high-risk niche)",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r4phlg/payment_processors_for_a_scraping_saas_highrisk/",
      "author": "Accomplished_Mood766",
      "created_utc": "2026-02-14 16:50:20",
      "score": 6,
      "num_comments": 30,
      "upvote_ratio": 0.88,
      "text": "Hi everyone,  \nI‚Äôm running a SaaS that provides scraping services, and I‚Äôm currently struggling with payment processing.\n\nStripe, Paddle, and Lemonsqueezy have all declined us due to the nature of the business. I understand that this niche is often classified as high-risk, but in practice we‚Äôve been operating for 5 months with **zero chargebacks or disputes**. Unfortunately, that doesn‚Äôt seem to matter much to decision-makers at most payment platforms ‚Äî scraping services are automatically flagged as high risk.\n\nI‚Äôd like to ask those of you who are running SaaS products in similar areas (scraping, data extraction, automation, etc.):\n\n* Which payment processors or merchant accounts are you using to accept credit card payments?\n* Are there providers that are more tolerant or experienced with this type of business?\n* Any recommendations or experiences you‚Äôre willing to share would be greatly appreciated.\n\nThanks in advance ‚Äî I‚Äôd really value hearing from others who‚Äôve dealt with this problem.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r4phlg/payment_processors_for_a_scraping_saas_highrisk/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5d5tuf",
          "author": "JohnnyOmmm",
          "text": "Lmao crypto or porno processors",
          "score": 6,
          "created_utc": "2026-02-14 16:54:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d6da7",
              "author": "Accomplished_Mood766",
              "text": "They accept cryptocurrency, but our customers don‚Äôt use crypto. Our users are mostly recruiters, marketers, and regular consumers who are accustomed to traditional credit and debit card payments.",
              "score": 3,
              "created_utc": "2026-02-14 16:57:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5dke6t",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 6,
                  "created_utc": "2026-02-14 18:07:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5dcjg8",
                  "author": "RobSm",
                  "text": "Send invoices and let them pay monthly wiretransfer, etc. You can automate email sending and if not paid - email notifications, etc. Also, paypal? Which has option to use CC directly?",
                  "score": 2,
                  "created_utc": "2026-02-14 17:28:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5e4rl6",
          "author": "convicted_redditor",
          "text": "Try polar.sh",
          "score": 2,
          "created_utc": "2026-02-14 19:51:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5eoyik",
          "author": "Dry_Illustrator977",
          "text": "Why not try crypto?",
          "score": 2,
          "created_utc": "2026-02-14 21:41:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pohf6",
          "author": "amogh-datar",
          "text": "I would suggest integrate multiple payment gateways. That way, if one is terminated suddenly you can make others live and then add another one in the meantime.   \n  \nIdeally you should integrate the high-risk payment gateways. Search for \"high risk payment gateways in \\[country\\]\" on Google and you should find them. They do have high fees but are likely to work for long term for high risk niche like yours. You can contact them to confirm the same.\n\nAlso, if your clients like your service, do offer them a way to pay via crypto or wire transfers to save payment gateway fees.",
          "score": 2,
          "created_utc": "2026-02-16 17:06:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5u9r3j",
          "author": "netmillions",
          "text": "Just reverse-engineer what your competitors are using. Also, a rebrand that doesn't plaster \"LinkedIn\" all over the website would likely be in good order. That alone might keep you under radar with them (and, LinkedIn). I would take all the payment solutions rejecting you as a wake up call.¬†",
          "score": 2,
          "created_utc": "2026-02-17 09:52:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5urcq4",
              "author": "Accomplished_Mood766",
              "text": "Including LinkedIn as a keyword on our website helps with search visibility and organic traffic.",
              "score": 1,
              "created_utc": "2026-02-17 12:21:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6815p9",
          "author": "Comfortable-Visual-5",
          "text": "Hey, \n\nCould you find a workaround for this? I am also launching my saas for scrapping data but unfortunately I'm not getting accepted anywhere.",
          "score": 2,
          "created_utc": "2026-02-19 11:07:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69yyx1",
              "author": "Accomplished_Mood766",
              "text": "At the moment, I‚Äôm considering Whop. They claim to accept everyone and allow payments without connecting Stripe or PayPal. However, they don‚Äôt seem very reliable, and there are many negative reviews about them.",
              "score": 1,
              "created_utc": "2026-02-19 17:43:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o69zh42",
                  "author": "whopmoderator",
                  "text": "Hey there, I work at Whop as the Customer Support Manager - we'd love to clear up any questions/confusion that you may have at all! \n\nIf you want, you can shoot me a DM directly & we can iron out any confusion!",
                  "score": 1,
                  "created_utc": "2026-02-19 17:45:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o69z7wf",
              "author": "Accomplished_Mood766",
              "text": "Please let me know if you find any other options that might work.",
              "score": 1,
              "created_utc": "2026-02-19 17:44:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5gg8cf",
          "author": "Quantum_Rage",
          "text": "I don't what kind of scraping SaaS you're building, but I have seen SaaS apps built on web scraping accept payments via Stripe or Paddle.",
          "score": 1,
          "created_utc": "2026-02-15 04:27:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5imxka",
              "author": "Accomplished_Mood766",
              "text": "I run a SaaS focused on LinkedIn data scraping. Stripe declined us immediately. We worked with Paddle for five months with zero chargebacks, but their risk management team eventually decided to terminate our payment account.",
              "score": 2,
              "created_utc": "2026-02-15 15:09:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kcp1x",
          "author": "awebscrapingguy",
          "text": "I think you are not saying everything, all scraping saas run on Stripe or Paddle without issues",
          "score": 1,
          "created_utc": "2026-02-15 20:13:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kpe1p",
              "author": "Accomplished_Mood766",
              "text": "Stripe explicitly prohibits scraping-related services. Paddle‚Äôs policies were more flexible, and we were able to work with them for five months; however, their risk team eventually decided to stop supporting our account.",
              "score": 2,
              "created_utc": "2026-02-15 21:18:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kjygm",
          "author": "Round_Method_5140",
          "text": "Paypal?",
          "score": 1,
          "created_utc": "2026-02-15 20:50:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9s0j9",
      "title": "Brave search does not scrape linkedin posts like google search?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r9s0j9/brave_search_does_not_scrape_linkedin_posts_like/",
      "author": "bravelogitex",
      "created_utc": "2026-02-20 10:38:01",
      "score": 4,
      "num_comments": 4,
      "upvote_ratio": 0.76,
      "text": "I have this js code to show the latest hiring posts from Linkedin, but nothing shows up:\n\n`const url = new URL(\"https://api.search.brave.com/res/v1/web/search\");`  \n`url.searchParams.append(\"q\", \"hiring site:linkedin.com/posts\");`  \n`url.searchParams.append(\"freshness\", \"pw\");`  \n`url.searchParams.append(\"operators\", \"true\");`\n\nResults returned are 0. If I change the second line to just`hiring site:linkedin.com,` then a few results show but they are only linkedin profiles, not posts.  \n  \nWhat gives? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r9s0j9/brave_search_does_not_scrape_linkedin_posts_like/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o6f1sr6",
          "author": "Quantum_Rage",
          "text": "LinkedIn is a hard target to scrape and Google might have special arrangements to access the data without scraping.",
          "score": 2,
          "created_utc": "2026-02-20 13:22:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i7lmq",
              "author": "cyber_scraper",
              "text": "well, integration with Google search console can immediately send signals about new pages while Brave  doesn't have such option",
              "score": 1,
              "created_utc": "2026-02-20 22:33:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6i9i5f",
              "author": "bravelogitex",
              "text": "Social media becoming walled gardens...",
              "score": 1,
              "created_utc": "2026-02-20 22:43:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r7gsr9",
      "title": "Which tracker/dashboard tools do you guys use to monitor processes?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r7gsr9/which_trackerdashboard_tools_do_you_guys_use_to/",
      "author": "Silver-Tune-2792",
      "created_utc": "2026-02-17 19:54:58",
      "score": 4,
      "num_comments": 6,
      "upvote_ratio": 0.84,
      "text": "Currently, I‚Äôm using status-based updates where a scheduled HTTP request updates the status based on database state.\n\nI‚Äôve heard about tools like Kibana, Grafana, Streamlit, etc., but they seem pretty advanced and time-consuming to set up. \n\nCurious what others are using and what‚Äôs worked well for you.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r7gsr9/which_trackerdashboard_tools_do_you_guys_use_to/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o62luiz",
          "author": "hasdata_com",
          "text": "We use Grafana + Prometheus for our scrapers. Tracks success rates and latency in real-time, plus synthetic tests run throughout the day to catch issues early. Alerts hit Slack when something breaks.\n\nNot the easiest setup but worth it imo.",
          "score": 9,
          "created_utc": "2026-02-18 15:54:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62nx6i",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-18 16:03:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o62pn5c",
                  "author": "webscraping-ModTeam",
                  "text": "ü™ß Please review the sub rules üëâ",
                  "score": 1,
                  "created_utc": "2026-02-18 16:11:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o62nhou",
          "author": "Hour_Analyst_7765",
          "text": "Grafana and a DIY admin panel.\n\nI use a generic event framework that inserts telemetry data on all executed (RPC) functions, with things like call count, execution time, response time, etc. This is how I can monitor for excessive CPU usage or delay. This is plotted in Grafana, which I use for also other projects than just scraping. Its mainly a development tool; kind of like sample tracing but then less granular, however, it runs 24/7.\n\nI then have made my own dashboard to inspect scraping jobs, logs, extracted data, etc. Besides monitoring I have a few manual overrides, such as disabling jobs or resetting the retry counter.\n\nFinally I've some GraphQL endpoints to query much more intricate data that I can't process inside SQL directly. These mainly have to do with network performance and reliability. I may move those over in my own dashboard, but for now I'm plotting them in Grafana.\n\nI've to implement some kind of notification/alert system, for example when failures exceed a certain threshold, or if a scraper detects it can't extract data anymore.",
          "score": 2,
          "created_utc": "2026-02-18 16:01:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xaa4t",
          "author": "dsjflkhs",
          "text": "Tbh just asked ai to create a dashboard with a local SQLite table. Works for me for medium size project, not sure about yours",
          "score": 1,
          "created_utc": "2026-02-17 20:04:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a6fnx",
          "author": "alinarice",
          "text": "once payments, invoices and spreadsheets start mixing, it's chaos. keeping proposal separate is normal, but a single hub for the financial side help a lot. quicken business and finance gets suggested often since it covers invoicing, cash flow, business + personal tracking, and ready to run reports in one place. ",
          "score": 1,
          "created_utc": "2026-02-19 18:18:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5rrds",
      "title": "Let‚Äôs move by step",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r5rrds/lets_move_by_step/",
      "author": "0xMassii",
      "created_utc": "2026-02-15 22:14:31",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 0.71,
      "text": "https://www.reddit.com/r/webscraping/s/0aCA0m6ioo\n\nJust use this post to comment with a website, so send in this format\n\nWebsite and what you want to achieve \n\nAnd then we will make a list so we can proceed by step and start with website with more upvotes \n\nWhat do you think?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r5rrds/lets_move_by_step/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5l6t90",
          "author": "No-Exchange2961",
          "text": "Linkedin!!!",
          "score": 2,
          "created_utc": "2026-02-15 22:50:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l7dh3",
          "author": "Acceptable-Sea-3248",
          "text": "LinkedIn",
          "score": 2,
          "created_utc": "2026-02-15 22:53:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lw6n6",
          "author": "Mysterious_Tip_6793",
          "text": "LinkedIn and Reddit",
          "score": 1,
          "created_utc": "2026-02-16 01:20:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m5c12",
          "author": "lechiffreqc",
          "text": "CRA: Want to maintain auth for allowing automating tax filling \nRevenu Quebec: Same as CRA\nTD: Same, but also automating downloading transactions \nAll other canadian banks for same reason",
          "score": 1,
          "created_utc": "2026-02-16 02:19:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5lbyx",
      "title": "How does strict traffic budgeting affect scraping efficiency?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r5lbyx/how_does_strict_traffic_budgeting_affect_scraping/",
      "author": "Intelligent-Lab6132",
      "created_utc": "2026-02-15 18:02:47",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.81,
      "text": "I‚Äôve been experimenting with a constrained traffic setup for a small scraping project ‚Äî instead of having unlimited proxy rotation, I forced myself to work within a fixed daily traffic budget.\n\nInterestingly, this constraint changed how I approached:\n\n* request pacing\n* session reuse\n* retry logic\n* concurrency tuning\n\nBy optimizing around efficiency per successful request rather than raw volume, I actually saw more stable success rates than I expected.\n\nIt made me wonder:\n\nDo we sometimes over-rotate IPs when smarter request control would perform better?\n\nCurious how others optimize when bandwidth or IP pool size is limited.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r5lbyx/how_does_strict_traffic_budgeting_affect_scraping/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5p58aq",
          "author": "hasdata_com",
          "text": "Nobody rotates IPs for fun. It's usually a last resort when everything else fails",
          "score": 8,
          "created_utc": "2026-02-16 15:37:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5g0xr",
      "title": "Stuck on the one problem during web scraping!",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r5g0xr/stuck_on_the_one_problem_during_web_scraping/",
      "author": "Sufficient-Newt813",
      "created_utc": "2026-02-15 14:31:15",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 0.81,
      "text": "I‚Äôm scraping a site where the source document (Word/PDF-style financial filing) is converted into an .htm file. The HTML structure is inconsistent across filings tables, inline styles, and layout blocks vary from one url to another, so there aren‚Äôt reliable tags or a stable DOM pattern to target.\n\nRight now I‚Äôm using about 12 keyword-based extraction patterns, which gives roughly 90% accuracy, but the approach feels fragile and likely to break as new filings appear.\n\nWhat are more robust strategies for extracting structured data from document-style HTML like this?",
      "is_original_content": false,
      "link_flair_text": "Scaling up üöÄ",
      "permalink": "https://reddit.com/r/webscraping/comments/1r5g0xr/stuck_on_the_one_problem_during_web_scraping/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5ilvfr",
          "author": "tonypaul009",
          "text": "If you‚Äôre getting 90% accuracy with 12 patterns, maybe you should give LLM-based parsing a try. Another method is to render HTML as images and give vision models a try. But will get costly at scale. Or if you have enough labelled examples, trying to finetune something like BERT might work as an entity extractor. The only way to know for sure is to try all these and benchmark them.",
          "score": 1,
          "created_utc": "2026-02-15 15:03:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ixf7r",
          "author": "RandomPantsAppear",
          "text": "I see 3 options\n\n1) LLM processing as a screenshot (expensive but effective)\n\n2) Build your own structure, as you are presently doing. \n\n3) Hybrid Infrastructure - Extract as you are, then send to an LLM to validate what you have extracted. If it detects a failure, then move to screenshot method. \n\n\n3.5) Hybrid Infrastructure Style 2: Instead of using the LLM for validation, have it choose between options. IE send a list of tables, and have it tag them from a preset list of tags that indicate their purpose.",
          "score": 1,
          "created_utc": "2026-02-15 16:01:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}