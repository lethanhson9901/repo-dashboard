{
  "metadata": {
    "last_updated": "2026-01-27 16:59:04",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 10,
    "total_comments": 42,
    "file_size_bytes": 49211
  },
  "items": [
    {
      "id": "1qlewai",
      "title": "I built a CLI that turns websites into real Playwright scrapers",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qlewai/i_built_a_cli_that_turns_websites_into_real/",
      "author": "Acceptable_Grass2812",
      "created_utc": "2026-01-24 05:51:51",
      "score": 16,
      "num_comments": 4,
      "upvote_ratio": 0.74,
      "text": "I built **ScrapeWizard** because using LLMs to write scrapers is slow and expensive ‚Äî you keep generating code, running it, fixing it, and burning API credits.\n\nScrapeWizard does it differently.  \nIt scans the website (DOM, JS, network calls, selectors, pagination) and uses AI **only to generate and fix the scraper code**.  \nThe actual scraping runs locally with Playwright.\n\nSo even if data extraction fails, you still get a full working script with all the site details that you can edit and reuse.\n\nGitHub:  \n[https://github.com/pras-ops/ScrapeWizard](https://github.com/pras-ops/ScrapeWizard)\n\nWould love feedback from people who scrape or automate.",
      "is_original_content": false,
      "link_flair_text": "AI ‚ú®",
      "permalink": "https://reddit.com/r/webscraping/comments/1qlewai/i_built_a_cli_that_turns_websites_into_real/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1mkt89",
          "author": "Habitualcaveman",
          "text": "Web Scraping Copilot for VScode and Scrapy is similar to (re)generate spiders. It guides the LLM to follow a workflow that generates the spiders and creates test fixtures and abstracts the parsing code into something called pageObjects so you get Scrapy spider code you can run anywhere, and just regenerate when the underlying site changes/breaks.",
          "score": 1,
          "created_utc": "2026-01-25 14:58:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uc30g",
              "author": "Acceptable_Grass2812",
              "text": "Web Scraping Copilot for VScode¬†it wroks for static website not js heavy as of now i think",
              "score": 1,
              "created_utc": "2026-01-26 16:21:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ue2ui",
                  "author": "Habitualcaveman",
                  "text": "Yeah you need a browser plugin for the heavy stuff.¬†",
                  "score": 1,
                  "created_utc": "2026-01-26 16:29:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fghkd",
          "author": "_i3urnsy_",
          "text": "Interesting definitely gonna take a look",
          "score": 0,
          "created_utc": "2026-01-24 14:20:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmd4qm",
      "title": "osn-selenium: An open-source Selenium-based framework",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qmd4qm/osnselenium_an_opensource_seleniumbased_framework/",
      "author": "oddshellnick",
      "created_utc": "2026-01-25 07:57:44",
      "score": 15,
      "num_comments": 3,
      "upvote_ratio": 0.83,
      "text": "Standard Selenium WebDriver implementations often face significant limitations in production environments, primarily due to blocking I/O execution patterns which increase overhead when scaling via threading. Furthermore, the lack of native, typed interfaces for Chrome DevTools Protocol (CDP) domains complicates low-level browser control. Additionally, standard automation signatures are easily identified by advanced anti-bot solutions through browser fingerprint analysis.\n\nTo address these issues, I have developed¬†**osn-selenium**, an asynchronous automation framework built on top of Selenium. Specifically architected for Blink-based browsers (Chrome, Edge, Yandex), it utilizes the¬†Trio¬†library to provide structured concurrency. The framework employs a modular Mixin-based architecture that maintains 99% backward compatibility with standard Selenium calls while exposing advanced control interfaces.\n\n**Core Technical Features:**\n\n* **Structured Concurrency (Trio):**¬†Native integration with the Trio event loop via¬†TrioThreadMixin, enabling efficient concurrent management of multiple browser instances.\n* **Typed CDP Executors:**¬†High-level, typed access to all Chrome DevTools Protocol domains. This allows for real-time network request interception and response manipulation directly from Python.\n* **Advanced Fingerprint Spoofing Engine:**¬†Features a built-in registry of over 200 parameters (Canvas, WebGL, AudioContext, etc.). Detection can be enabled in two lines of code. Supports spoofing via static/random values, static/random noise injection, and dynamic modification of value sequences. Additionally, the registry of parameters can be expanded.\n* **Dedicated dev\\_tools Package:**¬†A module designed for background browser event processing. It features specialized loggers for CDP and fingerprinting activity, alongside advanced request interception handlers.\n* **Full Instance Wrappers:**¬†Custom high-level wrappers for all Selenium objects including WebElements, Alerts, ShadowRoots, etc. These are 100% drop-in compatible with vanilla Selenium logic.\n* **Human-Like Interaction Layer:**¬†Implementation of natural mouse movements using Bezier curves with jitter, smooth scrolling algorithms, and human-like typing simulation.\n\nI am currently expanding the framework's capabilities. Short-term goals include automated parameter aggregation for¬†all flags managers, implementing higher-level logic for¬†Network,¬†Page, and¬†Runtime¬†domains in the¬†dev\\_tools¬†package, refining Human-Like movement patterns, and supporting a hybrid driver interface (both mixins and component-attributes). Support for additional Chromium-based browsers is also underway.\n\nThe long-term roadmap includes support for Gecko-based browsers (Firefox) and developing true internal concurrency for single browser instances using Trio memory channels and direct CDP pipe management. I am looking for technical feedback and contributors to help refine the architecture.\n\nIf you are interested in modernizing your Selenium-based infrastructure, I invite you to explore the [repository](https://github.com/oddshellnick/osn-selenium) and contribute to its development.",
      "is_original_content": false,
      "link_flair_text": "Scaling up üöÄ",
      "permalink": "https://reddit.com/r/webscraping/comments/1qmd4qm/osnselenium_an_opensource_seleniumbased_framework/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1mz7t2",
          "author": "cgoldberg",
          "text": "Using `setup.py` for a new package in 2026 is pretty odd.",
          "score": 1,
          "created_utc": "2026-01-25 16:05:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zk413",
          "author": "kts_2001",
          "text": "Good bro I will test it",
          "score": 1,
          "created_utc": "2026-01-27 09:26:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjs0sz",
      "title": "How do you verify what you scrape?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qjs0sz/how_do_you_verify_what_you_scrape/",
      "author": "PrestigiousZombie531",
      "created_utc": "2026-01-22 11:23:38",
      "score": 11,
      "num_comments": 9,
      "upvote_ratio": 0.92,
      "text": "- Let us say you are scraping some news website\n- You wrote the fanciest scraper on the planet and have 10000 news items in your database now\n- How do you verify if it is indeed what exists on the website and there have been no mistakes while downloading?",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qjs0sz/how_do_you_verify_what_you_scrape/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o113i5h",
          "author": "andreew92",
          "text": "Depends on where in the scraping pipeline you think things could go wrong. Are you talking about scraping the incorrect elements on the page or getting blank results?\n\nIf you really wrote a fancy scraper, it should already account and flag any data which is blank/error, or you could check for what you expect the scraped results to look like (eg. a news article should be minimum 500 characters long).\n\nAnother way to validate is to send a random sample to a LLM to verify the data is correct.\n\nIt is always cheaper and easier to check as the data is coming in, rather than after it‚Äôs been collected.",
          "score": 8,
          "created_utc": "2026-01-22 11:40:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11417g",
          "author": "hikingsticks",
          "text": "You can use simple data validation on collection with something like Pydantic, to check the data conforms to whatever specifications you want. That won't guarantee that a block of text is a news article, but using field validators etc you can enforce some basic checks.",
          "score": 5,
          "created_utc": "2026-01-22 11:44:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1i8sq2",
              "author": "Single-Tap-1579",
              "text": "Thanks for the feedback,Pyndatic solved a problem I had for a long time",
              "score": 1,
              "created_utc": "2026-01-24 22:04:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o11k4xd",
          "author": "Hour_Analyst_7765",
          "text": "I capture network traces from Chrome/Firefox (see Developer console), save them as HAR, and then I've unit tests set up for my target sites. I have my framework setup so it can either load resources from the live network, from a cache HAR, or a prerecorded HAR file. Any live network data gets also written into a HAR cache with some rules for how long that data can live there (for example, some \"evergreen\" content never gets invalidated, but I may change the data extraction code at a later point, so I can refresh all my data without hitting the site a million times again).\n\nBasically it means I can also write test cases and rerun them as many times as I'd like. For these prerecorded traces, I've some unit tests in place with the expected data the data extractor should return. To this end, you can also specifically record traces of 404 pages etc. and verify your data extractor recognizes them and does not emit invalid data.\n\nAs for how to verify if the data extraction code is still valid.. yeah thats the harder bit. Some websites can change their layout/HTML structure.. sometimes they may implement a region block so you have to exclude certain countries etc. Typically my code needs to check whether certain elements get found or not, and I often include a few extra for sanity reasons (not actual data that I want to grab). Usually this works quite well.",
          "score": 3,
          "created_utc": "2026-01-22 13:29:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13evdr",
          "author": "Kbot__",
          "text": "Hi,\n\nA few approaches depending on scale:\n\nDuring collection:\n\n- Schema validation (Pydantic mentioned already) - enforce expected types, required fields, min/max lengths\n- Hash each item - helps catch duplicates and track changes on re-scrapes\n- Checkpoint counts - if you expect \\~10k articles and get 8k, something broke\n\nAfter collection:\n\n- Sample verification - pull 50-100 random items, spot check against live site\n- Distribution checks - article lengths, publish dates, categories should follow expected patterns. Outliers = potential issues\n- Null/empty field rates - if 30% of your author fields are blank, your selector probably broke mid-run\n\nFor ongoing scrapes:\n\n- Diff monitoring - track when site structure changes (new class names, missing elements)\n- Sentinel pages - scrape a few known URLs with expected outputs, alert if they drift\n\nThe hardest part isn't catching errors - it's catching \"silent\" failures where the scraper returns plausible-looking garbage. That's where sampling + statistical checks help more than schema validation alone.\n\nHope this helps!",
          "score": 3,
          "created_utc": "2026-01-22 18:43:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11qr60",
          "author": "StoicTexts",
          "text": "Could throw some asserts in. Assert first and last value of a scrape are what you expect or something",
          "score": 2,
          "created_utc": "2026-01-22 14:04:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16kbu3",
          "author": "Ok-Ingenuity-2908",
          "text": "Thoughts on my solution towards scraping on my site, having a rotation of fake content get uploaded to the site and various times of the day/week? How would a scraper differentiate?",
          "score": 1,
          "created_utc": "2026-01-23 04:32:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkygpn",
      "title": "What VPS provider do you use for large scale crawling ?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qkygpn/what_vps_provider_do_you_use_for_large_scale/",
      "author": "damienlusson",
      "created_utc": "2026-01-23 18:18:31",
      "score": 9,
      "num_comments": 15,
      "upvote_ratio": 0.85,
      "text": "Hi,\n\n  \nI'm crawling at a large volume (400 millions pages per month), but hetzner and now netcup blocked my vps because of port scanning.\n\n(they don't allow port scanning fair, but when you show them your script and the use case they don't want to change position and maintain that crawling like Google or Bing does is not possible on their infra, fair they are private company and don't want to take any risk i guess)\n\n  \nWhat provider would you recommend, you use ?\n\n  \nThanks in advance",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qkygpn/what_vps_provider_do_you_use_for_large_scale/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1aa2ws",
          "author": "amemingfullife",
          "text": "Why do you need to port scan for crawling? For websites everything is on 80/443. Sounds dodgy tbh.",
          "score": 9,
          "created_utc": "2026-01-23 18:39:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1agkf7",
              "author": "damienlusson",
              "text": "Hetzner and netcup have blocked my server because they are considering i'm doing netscan and port scanning, both of them are not allowed on their infrastructure.\n\nEven after showing them my scrapy setup and code and arguing that i'm doing what a lot of people like Google or Bing do > crawling they still are saying the above to me\n\nI guess i have the same problem as this guy : [https://www.reddit.com/r/hetzner/comments/18u09s3/hetzner\\_says\\_search\\_engine\\_crawlers\\_like\\_google/](https://www.reddit.com/r/hetzner/comments/18u09s3/hetzner_says_search_engine_crawlers_like_google/)",
              "score": 1,
              "created_utc": "2026-01-23 19:08:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1aiti6",
                  "author": "amemingfullife",
                  "text": "Ah, I see, they *consider* it port scanning even if it isn‚Äôt. Fair enough. \n\nI use Vultr, DigitalOcean and GCP and never had issue crawling around 80m sites.",
                  "score": 3,
                  "created_utc": "2026-01-23 19:19:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1anats",
          "author": "Professional-Fox4161",
          "text": "A few years ago I was able to crawl half a billion pages/day on a cluster of scaleway dedicated servers. Don't know if it's still possible.",
          "score": 1,
          "created_utc": "2026-01-23 19:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1btfp1",
              "author": "unteth",
              "text": "Mind explaining your setup?",
              "score": 1,
              "created_utc": "2026-01-23 23:01:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1c1ukt",
                  "author": "Professional-Fox4161",
                  "text": "If I remember correctly there was something like 12 servers with 6 cores and 32Gb RAM, with 3 x 1TB disks for the crawling DB. 6 servers 4 cores with 32gb for the crawlers. Also 3 servers for the message queue and 1 for the config server.",
                  "score": 3,
                  "created_utc": "2026-01-23 23:46:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1bf0t2",
              "author": "Loud_Bathroom_8023",
              "text": "wtf were you scraping haha",
              "score": 0,
              "created_utc": "2026-01-23 21:50:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1bgkix",
                  "author": "Professional-Fox4161",
                  "text": "The web. Hence \"crawling\", not scraping. As OP said, it's the same thing Google and Bing and many others do and it's totally legit if you respect robots.txt, use a user-agent that clearly state your purpose by providing a link to a web page, and use a list of im addresses that is described somewhere.",
                  "score": 2,
                  "created_utc": "2026-01-23 21:57:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1angi6",
          "author": "Difficult-Cat-4631",
          "text": "Leaseweb",
          "score": 1,
          "created_utc": "2026-01-23 19:41:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1av3ek",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-23 20:16:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bikdv",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-23 22:07:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1lxgvp",
          "author": "ScrapeAlchemist",
          "text": "Hi,\n\nAt 400M pages/month, the real issue is that crawling domains pointing to unreachable IPs gets flagged as \"port scanning\" by automated abuse systems ‚Äî even though it isn't.\n\nOptions that actually work:\n\n1. **Linode** ‚Äî explicitly security-research friendly. Open a ticket explaining your use case and they'll whitelist the activity. Starts at $5/mo.\n\n2. **Smaller dedicated server providers with human support** (IOFlood, etc.) ‚Äî unlike Hetzner's automated systems, you can actually talk to someone and explain your use case. ~$60-130/mo for a dedicated box.\n\n3. **Proxy provider with large IP pool** ‚Äî at your scale you need tens of thousands of rotating IPs anyway. This offloads abuse complaints to infrastructure built for it.\n\nHope this helps.",
          "score": 1,
          "created_utc": "2026-01-25 12:43:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nmkfo",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-25 17:45:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r9321",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-26 03:52:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qod78a",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qod78a/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-01-27 13:01:04",
      "score": 6,
      "num_comments": 4,
      "upvote_ratio": 0.88,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levels‚Äîwhether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) üå±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1qod78a/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o20j8it",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-27 13:45:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20v3bm",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-01-27 14:46:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21d8jp",
          "author": "xRazar",
          "text": "I'm currently in the process of scraping e-sim sites wherever possible I try to find the public APIs for this but it does not seem that effective for most of the sites. Anyone has experience with scraping E-Sim sites (Saily, Nomad as few examples to go off)",
          "score": 1,
          "created_utc": "2026-01-27 16:09:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo77gd",
      "title": "Advice needed: scraping company websites in Python",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qo77gd/advice_needed_scraping_company_websites_in_python/",
      "author": "Working_Taste9458",
      "created_utc": "2026-01-27 07:26:44",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm building a small project that needs to scrape company websites (manufacturers, suppliers, distributors, traders) to collect basic business information. I‚Äôm using Python and want to know what the best approach and tools are today for reliable web scraping. For example, should I start with requests + BeautifulSoup, or go straight to something like Playwright? Also, any general tips or common mistakes to avoid when scraping multiple websites would be really helpful.",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qo77gd/advice_needed_scraping_company_websites_in_python/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1zauu4",
          "author": "Bitter_Caramel305",
          "text": "Playwright is not the choice of any expert it's the choice of dumb beginners.\n\nRequests and bs4 is fine but replace requests with the requests module of curl\\_cffi.  \nThe syntax will be the same, but you'll get TSL fingerprinting of a real browser (Thanks to C) and an optional but powerful request param (impersonate=\"any browser of your choice\").\n\nExample:\n\n    from curl_cffi import requests\n    r = requests.get(url, cookies, headers, impersonate=\"chrome\")\n\n\n\nAlso, always reverse engineer the exposed backend API first and use this as a fallback not primary method.  \nHappy scraping!",
          "score": 9,
          "created_utc": "2026-01-27 08:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zn2lq",
              "author": "scraperouter-com",
              "text": "if curl\\_cffi is blocked you can try scrapling stealthmode but only if you are sure you need the browser (much slower way)",
              "score": 1,
              "created_utc": "2026-01-27 09:53:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o208fu0",
          "author": "Responsible-Fly-990",
          "text": "go with requests + BeautifulSoup if you r a beginner",
          "score": 1,
          "created_utc": "2026-01-27 12:43:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qk5u1r",
      "title": "Reservation Alerts",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qk5u1r/reservation_alerts/",
      "author": "Fearless_Space_2822",
      "created_utc": "2026-01-22 20:30:01",
      "score": 5,
      "num_comments": 22,
      "upvote_ratio": 0.69,
      "text": "Looking to build a scraper that alerts me via discord webhook whenever a reservation opens up for a place that uses [waitwhile.com](http://waitwhile.com) . I don't have much coding experience besides data languages but figured I could code this via AI. Looking for how possible and easy this could be or any tips that you experts have.\n\nThe bot would need to essentially monitor and refresh the site, then as cancellations occur or new times open up, the bot would send some sort of custom webhook to alert me of the time/day available with a link to book. I would probably have it poll every 2-3 minutes and use proxies to avoid IP ban. I was checking around github and other sites to see if something has been made already since this is a very commonly used reservation host. Thanks for all the help in advance and I could provide more information if needed.   \n  \nEDIT: The main error I'm running into is that the bot sends a webhook every time it checks rather than filtering to only when its available, then populating the webhook with info.",
      "is_original_content": false,
      "link_flair_text": "AI ‚ú®",
      "permalink": "https://reddit.com/r/webscraping/comments/1qk5u1r/reservation_alerts/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o17ku9h",
          "author": "Kbot__",
          "text": "Hi,\n\nGood news ‚Äî waitwhile makes this easier than you think. The booking data is embedded directly in the page's HTML as a JSON blob (inside a tag called `__NEXT_DATA__`). No headless browser or JS rendering needed.\n\nSince you're already using n8n, here's what I'd do:\n\n1. **HTTP Request node** ‚Äî simple GET to the waitwhile URL\n2. **HTML Extract node** ‚Äî pull the JSON from the `<script id=\"__NEXT_DATA__\">` tag\n3. **Check the field** `props.ssrLocationData.availableBookingResourceIds` ‚Äî when this array is not empty, slots are open\n4. **IF node** ‚Äî compare to the previous result (store it with a Set node or a file). Only continue if it changed from empty to non-empty\n5. **Discord webhook node** ‚Äî fires only when new slots appear\n\nThat solves your \"webhook on every refresh\" problem ‚Äî you're only alerting on state changes, not on every poll.\n\nI tested it just now with a plain GET request ‚Äî the data comes back without needing proxies or anything special.",
          "score": 4,
          "created_utc": "2026-01-23 09:29:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18sxrn",
              "author": "Super_Refuse_2415",
              "text": "üî•",
              "score": 1,
              "created_utc": "2026-01-23 14:34:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o19ejmb",
              "author": "Fearless_Space_2822",
              "text": "i'll try this",
              "score": 1,
              "created_utc": "2026-01-23 16:16:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o149vc7",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-01-22 21:05:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14enjz",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-22 21:28:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1540ll",
                  "author": "webscraping-ModTeam",
                  "text": "ü™ß Please review the sub rules üëâ",
                  "score": 2,
                  "created_utc": "2026-01-22 23:38:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o157ny3",
          "author": "davak72",
          "text": "When you say ‚Äúthe bot‚Äù sends a webhook every time, what are you referring to?? You mention GitHub, but not that you‚Äôve actually started implementing a bot in any particular language or based on any particular project. It‚Äôs hard to help with 0 context",
          "score": 1,
          "created_utc": "2026-01-22 23:58:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15bbip",
              "author": "Fearless_Space_2822",
              "text": "Sorry for lack of context, as I really don't code. I'm just looking to solve a problem with automation. I want  the bot to send a webhook message into a designated discord channel alerting me of an open reservation on the website, with the link to book said reservation.  \n  \nI mentioned github just as a site that I scoured for possible solutions that have already been made by other people. So far, through json I have a bot that sends messages through a webhook when it refreshes the site, but lacks the actual useful info in the webhook. I have no preferences just want to get this solved.",
              "score": 1,
              "created_utc": "2026-01-23 00:17:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o15bnod",
                  "author": "matty_fu",
                  "text": "If you‚Äôre not comfortable sharing the URL of the specific service you‚Äôre scraping, can you find another one that uses waitwhile? Hopefully the same techniques will apply to both",
                  "score": 1,
                  "created_utc": "2026-01-23 00:19:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o15vuzc",
                  "author": "davak72",
                  "text": "Ohhh. Gotcha. You‚Äôll honestly probably have good success working even with ChatGPT free. What language is your minimal bot in?",
                  "score": 1,
                  "created_utc": "2026-01-23 02:10:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o15r26r",
          "author": "_i3urnsy_",
          "text": "Is your solutions browser or request based? It‚Äôs good you‚Äôre getting the webhook, but sounds like you just are missing whatever the identifier for open or reserved bookings is.\n\nWithout the specific url or a similar one it‚Äôs hard to assist further.",
          "score": 1,
          "created_utc": "2026-01-23 01:43:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o167j3b",
              "author": "Fearless_Space_2822",
              "text": "Im trying to make it work on [https://waitwhile.com/locations/chromehearts/services?registration=booking](https://waitwhile.com/locations/chromehearts/services?registration=booking)\n\nwaitwhile is a popular host for waitlists and reservations.",
              "score": 2,
              "created_utc": "2026-01-23 03:15:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qmbyps",
      "title": "Browser Code: Coding agent for user scripts",
      "subreddit": "webscraping",
      "url": "https://github.com/chebykinn/browser-code",
      "author": "heraldev",
      "created_utc": "2026-01-25 06:51:59",
      "score": 4,
      "num_comments": 5,
      "upvote_ratio": 0.71,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qmbyps/browser_code_coding_agent_for_user_scripts/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1l1q21",
          "author": "RandomPantsAppear",
          "text": "Good damn work. I love the idea of making a virtual fs to represent the webpage",
          "score": 3,
          "created_utc": "2026-01-25 08:11:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l58mh",
          "author": "BodybuilderLost328",
          "text": "Its all fine till the html of all the page exceed the llm context, how are you handling this?\n\nSo like for bigger webpages like amazon this tool wont work right?",
          "score": 3,
          "created_utc": "2026-01-25 08:42:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ncqle",
              "author": "heraldev",
              "text": "It will! The agent in the extension reads the page as a file. This file is formatted and cleaned up - I add spaces and newlines around each html tag, this allows for reading only the parts of it. Then the agent has 3 tools to explore the file - read with offset and limit, grep, and as a last resort it can execute JS to filter elements.",
              "score": 1,
              "created_utc": "2026-01-25 17:03:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wwdei",
          "author": "quarkcarbon",
          "text": "Reducing a semantic rich webpage as a file with some offsets doesn't it lead to inefficient reads and highly expensive ops ? Also it's all fun and games in testing with browser webpage/extension storage. But the min you do it on the real user's browser - their device type/ram + the number of tabs they open and how full is storage is gonna blow out with browser storage errors soon esp since you take the html of pages. Now if you often fallback to user's device FS, then it's another CLI agent with web access.",
          "score": 1,
          "created_utc": "2026-01-26 23:06:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ksdns",
          "author": "heraldev",
          "text": "I‚Äôve been experimenting with embedding an Claude Code-style coding agent directly into the browser.\n\nAt a high level, the agent generates and maintains userscripts and CSS that are re-applied on page load. Rather than just editing DOM via JS in console the agent is treating the page, and the DOM as a **file**.\n\nThe models are often trained in RL sandboxes with full access to the filesystem and bash, so they are really good at using it. So to make the agent behave well, I've simulated this environment.\n\nThe whole state of a page and scripts is implemented as a virtual filesystem hacked on top of `browser.local` storage. URL is mapped to directories, and the agent starts inside this directory. It has the tools to read/edit files, grep around and a fake bash command that is just used for running scripts and executing JS code.\n\nI've tested only with Opus 4.5 so far, and it works pretty reliably.  \nThe state of the file system can be synced to FS, although because Firefox doesn't support Filesystem API, you need to manually import the FS contents first.\n\nThis agent is \\*really\\* useful for extracting things to CSV.",
          "score": 1,
          "created_utc": "2026-01-25 06:52:28",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj1ygy",
      "title": "TLS Help",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qj1ygy/tls_help/",
      "author": "echno1",
      "created_utc": "2026-01-21 16:06:50",
      "score": 3,
      "num_comments": 9,
      "upvote_ratio": 0.81,
      "text": "I‚Äôm using tls-client in Go to mimic real Chrome TLS fingerprints.\n\nEven with:\n\n* Proper client profiles\n* Correct UA + header order\n* HTTP/2 enabled\n\nI‚Äôm still getting detected (real Chrome works on same proxy).\n\nCan anyone help?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qj1ygy/tls_help/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o12aqrh",
          "author": "bluemangodub",
          "text": "Check the headers you are sending and check what chrome is sending.  The answer is always **ALWAYS** the headers.\n\nProbably there is a JS cookie set, which you're not doing as not processing JS",
          "score": 2,
          "created_utc": "2026-01-22 15:43:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0xxil4",
          "author": "army_of_wan",
          "text": "Which vendor is it ? akamai ?",
          "score": 1,
          "created_utc": "2026-01-21 22:35:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0y73ba",
          "author": "Hour_Analyst_7765",
          "text": "Check ja4/akamai hashes",
          "score": 1,
          "created_utc": "2026-01-21 23:25:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11mhqm",
          "author": "Kbot__",
          "text": "Hi,\n\nTLS fingerprinting is a pain. Even with tls-client doing everything \"right,\" there's a ton of signals beyond cipher suites:\n\n* **JA4 fingerprint**¬†(JA3 is basically dead now since browsers randomize TLS extensions) - check yours vs real Chrome at¬†[tls.peet.ws/api/all](http://tls.peet.ws/api/all)\n* **HTTP/2 SETTINGS frame**¬†\\- the order and default values matter more than you'd think\n* **ClientHello extension order**¬†\\- not just having the right extensions, but the sequence\n\nWhat protection are you hitting? Akamai, Cloudflare, something else? That'll help narrow down what's catching you.\n\nPS: Worst comes to worst, Wireshark, intercept the request from a real browser vs your setup, and see what the difference is. I had a site once that I spent weeks on, and Wireshark showed me the order of headers. One more thing, only using Rust i was able to recreate the same order. \n\nGood luck",
          "score": 1,
          "created_utc": "2026-01-22 13:42:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16kcek",
              "author": "matty_fu",
              "text": "Repost with a different JA3 tool - one that works, and isn't covered in popup spam",
              "score": 1,
              "created_utc": "2026-01-23 04:32:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o17melb",
                  "author": "Kbot__",
                  "text": "edited, thanks for bringing that up. If you know a better tool, please let me know.",
                  "score": 1,
                  "created_utc": "2026-01-23 09:44:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1hvj5p",
          "author": "Different_Pick_7951",
          "text": "[https://github.com/enetx/surf](https://github.com/enetx/surf)",
          "score": 1,
          "created_utc": "2026-01-24 21:01:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1se4yt",
          "author": "nez1rat",
          "text": "Your Human score is being fkd by something, most likely Ja4 + bad proxy setup",
          "score": 1,
          "created_utc": "2026-01-26 09:02:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1w8i3j",
          "author": "Overpoweredpixel",
          "text": "what is best tls for  \n\ncaptcha",
          "score": 1,
          "created_utc": "2026-01-26 21:16:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmysbt",
      "title": "I'm starting a web scraping project. Need advices.",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qmysbt/im_starting_a_web_scraping_project_need_advices/",
      "author": "Papenguito",
      "created_utc": "2026-01-25 23:11:50",
      "score": 2,
      "num_comments": 13,
      "upvote_ratio": 0.59,
      "text": "I am going to start a project of web scraping. Is playwright with TS the best option to start i want to scrape some pages o news from my city i need advices to start with this pls ",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qmysbt/im_starting_a_web_scraping_project_need_advices/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1pr5a0",
          "author": "hikingsticks",
          "text": "You need to investigate the pages and see what required to get the data you want. Only use headless browser if you have to, it's much more preferable to not use one if possible.\n\nOpen the network tab and check the requests being made by your browser, see which one(s) have the data you need, and try to replicate them.",
          "score": 6,
          "created_utc": "2026-01-25 23:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ptnwn",
              "author": "Papenguito",
              "text": "i want to get the news from the web pages",
              "score": 0,
              "created_utc": "2026-01-25 23:29:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1q1pwi",
                  "author": "hikingsticks",
                  "text": "Yes... You'd be well served by learning some html basics, and becoming familiar with the network tab. Then watch some John Watson Rooney on YouTube for scraping techniques.\n\nOr just throw AI at it and learn nothing.",
                  "score": 5,
                  "created_utc": "2026-01-26 00:08:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1q0vmy",
                  "author": "Own_Relationship9794",
                  "text": "which website is it?",
                  "score": 1,
                  "created_utc": "2026-01-26 00:04:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ut9su",
          "author": "hasdata_com",
          "text": "Have you looked into Google News RSS? That's usually the easiest starting point if you just need the headlines. For the actual sites, it really comes down to how they load data. If it's simple static HTML, basic request libs work fine. But for anything with JS rendering, you're right, you will need heavier tools like Playwright to handle the dynamic content",
          "score": 4,
          "created_utc": "2026-01-26 17:35:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rxmyb",
          "author": "Key_Investment_6818",
          "text": "basic html parsing with curl\\_cffi should do the job , just make sure you know what elements you want to scrape..",
          "score": 2,
          "created_utc": "2026-01-26 06:41:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1u2hig",
          "author": "bluemangodub",
          "text": "Depends on the site. Really it's trial and error.  Try HTTP requests. If that works, great. If not, do really need a browser? If so, try a browser. Does it work? Great? If not, then finger the anti bot detections and by pass that",
          "score": 2,
          "created_utc": "2026-01-26 15:39:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1u63vt",
              "author": "Papenguito",
              "text": "Thanks mate",
              "score": 1,
              "created_utc": "2026-01-26 15:55:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1q0zqy",
          "author": "Rorschache00714",
          "text": "If if you download Antigravity you can tell the agent to use the browser and do that for you. Have it create a json file with all the scrape data.",
          "score": 1,
          "created_utc": "2026-01-26 00:04:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qat0f",
          "author": "akashpanda29",
          "text": "You can do it from playwright. But playwright is a overkill for most of the website where you can just get html of api data as json directly through a fetch call. \nSo investigating website is the primary step",
          "score": 1,
          "created_utc": "2026-01-26 00:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r11cx",
          "author": "Holiday-Tonight5626",
          "text": "every news site is different. sites know ppl r scraping, so they all have measures to deal with that. some use apis, like npr i think..\nif you want to scrape popular news sites yeah you will have to use pw for a lot of it. wait for the js to render then grab that shit",
          "score": 1,
          "created_utc": "2026-01-26 03:06:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}