{
  "metadata": {
    "last_updated": "2026-03-02 09:15:17",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 12,
    "total_comments": 69,
    "file_size_bytes": 76763
  },
  "items": [
    {
      "id": "1rf0nd0",
      "title": "I built an open-source no code web scraper Chrome extension",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rf0nd0/i_built_an_opensource_no_code_web_scraper_chrome/",
      "author": "Dry-Raspberry-3608",
      "created_utc": "2026-02-26 04:34:32",
      "score": 33,
      "num_comments": 12,
      "upvote_ratio": 0.92,
      "text": "Hey everyone,\n\nI do a fair bit of data collection for my own side projects. Usually, I just write a quick Python script with BeautifulSoup, but sometimes I just want to visit a webpage, click on a few elements, and download a CSV without having to open my terminal or fight with CORS.\n\nI tried a few of the existing visual scraping tools out there, but almost all of them lock you into expensive monthly subscriptions. I really hate the idea of paying a recurring fee just to extract public text, and I don't love my data passing through a random third-party server.\n\nSo I spent the last few weeks building my own alternative. It‚Äôs a completely free, open-source no code web scraper that runs entirely locally in your browser.\n\nHere is how the workflow looks right now:\n\n* You open the extension on the page you want to scrape.\n* You click on the elements you want to grab (it auto-detects repeating patterns like lists, grids, or tables).\n* You name your columns (e.g., \"Price\", \"Product Title\").\n* Hit export, and it generates a clean CSV or JSON file instantly.\n\nBecause it runs locally in your browser, it uses your own IP and session state. This means it doesn't get instantly blocked by standard anti-bot protections the way server-side scrapers do.\n\nSince it's open source, you don't have to worry about sudden paywalls, API caps, or vendor lock-in.\n\nYou can install it directly from the Chrome Web Store here:[https://chromewebstore.google.com/detail/no-code-web-scraper/cogbfdcdnohnoknnogniplimgkdoohea](https://chromewebstore.google.com/detail/no-code-web-scraper/cogbfdcdnohnoknnogniplimgkdoohea)\n\n(The GitHub repo with all the source code is linked on the store page, but let me know if you want me to drop it in the comments).\n\nI'm still actively working on it, so please let me know if you run into bugs. It struggles a bit with deeply nested shadow DOMs right now, but I'm trying to figure out a fix for the next update. Honest feedback or feature ideas are super welcome!",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1rf0nd0/i_built_an_opensource_no_code_web_scraper_chrome/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o7gl5pj",
          "author": "Dry-Raspberry-3608",
          "text": "I am not too proud of it yet so please please tell me if u like it and found bugs i will fix everything by next update. ",
          "score": 2,
          "created_utc": "2026-02-26 04:35:53",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o7gmzbo",
          "author": "Connect-Soil-7277",
          "text": "will definitely give it a try",
          "score": 2,
          "created_utc": "2026-02-26 04:48:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gnhmr",
              "author": "Dry-Raspberry-3608",
              "text": "Aye Thanks",
              "score": 1,
              "created_utc": "2026-02-26 04:52:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7h4zwf",
          "author": "angelarose210",
          "text": "Can you share the repo?",
          "score": 2,
          "created_utc": "2026-02-26 07:10:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gmh75",
          "author": "Accomplished_Ear4947",
          "text": "thanks!",
          "score": 1,
          "created_utc": "2026-02-26 04:45:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gxl8w",
          "author": "XxxHAMZAxxX",
          "text": "Thanks for sharing",
          "score": 1,
          "created_utc": "2026-02-26 06:07:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7i7b1r",
          "author": "Kikoness",
          "text": "Seems very interesting for what I do daily! I'll give it a try. Mind sharing the repo? I couldn't find it on the store page.",
          "score": 1,
          "created_utc": "2026-02-26 12:46:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lqrgo",
          "author": "Anxious_Ad2885",
          "text": "what tech stack do you use for build that extension?",
          "score": 1,
          "created_utc": "2026-02-26 23:09:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mr2qk",
          "author": "the__solo__legend",
          "text": "Thanks will definitely helps mee",
          "score": 1,
          "created_utc": "2026-02-27 02:34:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfwvga",
      "title": "Looking for a Simple Scraper for a Simple Need",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rfwvga/looking_for_a_simple_scraper_for_a_simple_need/",
      "author": "amikigu",
      "created_utc": "2026-02-27 04:29:04",
      "score": 13,
      "num_comments": 26,
      "upvote_ratio": 0.81,
      "text": "Hi all, it seems that most web scraping tools do far more than what I want to do, which is to just scrape the header, main/first image link, tags, and text, of specific articles from various websites, and then put that data in a database of some sort that's usable by Wordpress (or even just into a .csv file at minimum). My goal is to then reformat/summarize said text/data later in a newsletter format. Is there any tool with a relatively simple GUI (or in which the coding isn't outlandishly difficult to use) and with decent tutorials that people would recommend for this? Given that scraping has been a thing for years, and given the clear time and effort that have been spent developing the tools I've already explored, I'm hoping what I want is already out there, and I'm just not finding the right tutorials/links. Thanks in advance for any guidance.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rfwvga/looking_for_a_simple_scraper_for_a_simple_need/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o7nqdkx",
          "author": "Picatrixter",
          "text": "It's funny reading this, because this is exactly what I do for a decent income (mainly Adsense and affiliate): I built a scraper specifically for Wordpress sites (via the standard API). I get the articles from various sources (also following the external links in the body to get more context), rewrite them with AI, resize the featured images, publish them on my own WP sites. Once that's done, phase 2: other scripts start sharing the content on Facebook and other social nets, while another script generates the newsletter from the latest articles, while also adding some affiliate links to it (to books that the readers might be interested in). \n\nMy advice: break this large process into smaller steps, so that it doesn't feel overwhelming, tgen assemble everything into one big pipeline.",
          "score": 6,
          "created_utc": "2026-02-27 06:34:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o1jr6",
              "author": "lp435",
              "text": "I always wondered if this really works. It's unimaginable to me that people read an article and then feel the sudden urge to click on a link an buy a book. But I'm happy that it works for you!",
              "score": 1,
              "created_utc": "2026-02-27 08:11:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7o4yic",
                  "author": "Picatrixter",
                  "text": "It's not just a random book. The books are chosen by a similarity ranking (say 80% of the articles are about recent scientific discoveries - the book will be about something related). For a 100% automated workflow, I am happy with the results.",
                  "score": 1,
                  "created_utc": "2026-02-27 08:44:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7oan8r",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-27 09:38:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7odgef",
                  "author": "webscraping-ModTeam",
                  "text": "üëî Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
                  "score": 1,
                  "created_utc": "2026-02-27 10:05:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ochy8",
              "author": "Naive-Highway-4733",
              "text": "That's sounds awesome. Where do you host your applications may I ask. Is there a way to host it as cheap as possible or do you use your own computer. Is there a way to do this Self-gosted. If you could write an article about it, that would be worth to read",
              "score": 1,
              "created_utc": "2026-02-27 09:56:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7uak34",
                  "author": "Picatrixter",
                  "text": "I host it at home, on a dedicated Ubuntu server (cheap SFF computer). Scripts run on a schedule via cron.",
                  "score": 1,
                  "created_utc": "2026-02-28 06:55:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7t7wmg",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-28 02:16:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7t9qvo",
                  "author": "webscraping-ModTeam",
                  "text": "ü™ß Please review the sub rules üëâ",
                  "score": 1,
                  "created_utc": "2026-02-28 02:27:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7q8ycr",
          "author": "Proof_Resource7669",
          "text": "Honestly, you might be overthinking it. For something that specific, you could probably get a working Python script with Beautiful Soup and Requests in an afternoon. There are tons of straightforward tutorials for exactly that kind of targeted scraping",
          "score": 2,
          "created_utc": "2026-02-27 16:53:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sztdv",
          "author": "alex3321xxx",
          "text": "Just ask Claude or Gemini",
          "score": 2,
          "created_utc": "2026-02-28 01:25:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7odj4b",
          "author": "eurobosch",
          "text": "You can check out [n8n](https://n8n.io), it's a no-code automation tool that you can use for example for scraping, extracting data, reshape it, store it, etc. You can create flows from very simple (2-3 nodes) up to very complex ones (tens or hundreds) with lots of integrations. You can schedule your jobs to run at specific intervals, you can connect it to hundreds of services and tools (GMail, Google Sheets, Postgres databases, etc).   \nI know they had a free plan or you can self-host it with their docker images.  \nCheck out their templates library where you can find already built flows for many scraping scenarios.\n\nSimilar tools are [make.com](http://make.com), zapier.",
          "score": 1,
          "created_utc": "2026-02-27 10:06:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7omwfv",
              "author": "havoc2k10",
              "text": "do you have template for n8n webscraping?",
              "score": 1,
              "created_utc": "2026-02-27 11:29:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7pqybx",
                  "author": "eurobosch",
                  "text": "yeah I have several but they are too complicated for your needs and they are not published on the n8n site, I use them privately for other projects. \n\nyou can search for scraping in the template library [here](https://n8n.io/workflows/?q=scraping) ",
                  "score": 1,
                  "created_utc": "2026-02-27 15:28:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7razlu",
          "author": "justincampbelldesign",
          "text": "Can you provide more details.\n\n* What are a few example sites?\n* Is [this example](https://www.figma.com/proto/cF2LKiZ9DpdxPmmm1cBh1m/Article-extraction?node-id=4-550&t=79IWLzf03CSb3xbs-1&scaling=scale-down&content-scaling=fixed&page-id=0%3A1) of what you are trying to pull correct? (Best viewed on a laptop or desktop computer, use the toolbar in the upper right to zoom in and out.)\n\nYou might want to use the RSS feed. RSS is stable, legal/expected, and doesn‚Äôt break when page HTML changes.  \nMany publishers expose RSS feeds that already include:\n\n* title (header)\n* link\n* publish date\n* excerpt / sometimes full content\n* sometimes featured image (varies)\n\nYou might be able to skip the database  \nRSS feed > WP posts or a custom post type > newsletter formatting later.  \nThis avoids building a separate database and might make this a bit simpler.",
          "score": 1,
          "created_utc": "2026-02-27 19:55:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7t9v3d",
              "author": "amikigu",
              "text": "Thanks for your reply. Some sample articles are these two, with differing degrees of complexity: [https://en.antaranews.com/news/399809/indonesia-allocates-rp335-trillion-for-free-meals-program-in-2026](https://en.antaranews.com/news/399809/indonesia-allocates-rp335-trillion-for-free-meals-program-in-2026)\n\n[https://sahellibertynews.com/2026/02/23/burkina-faso-fighting-terrorism-the-need-to-avoid-the-pitfalls-of-media-misinformation/](https://sahellibertynews.com/2026/02/23/burkina-faso-fighting-terrorism-the-need-to-avoid-the-pitfalls-of-media-misinformation/)\n\nAnd yes, the Figma link shows the bare bones of what I'm trying to do (I like things complicated so I'm hoping to do far more than what's shown, but that link definitely shows the basic gist).\n\nAnd I think pumping an RSS feed into my project is great if I was creating something like an app that feeds you news, or even to creating like a sidebar showing updates from various sources, but since I'm hoping to self-host this content and drive traffic to me, I think I'd still have to add a bit of AI summarization or other \"creative touches\" so that my work isn't viewed as pure copyright infringement. And not all sites have RSS feeds. But since many do, maybe I'll start developing a workflow with RSS and then branch out into scraping and other tools later.",
              "score": 2,
              "created_utc": "2026-02-28 02:28:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7whjg3",
                  "author": "justincampbelldesign",
                  "text": "Got it makes sense thanks for the info. When you say \"you like things complicated\" you mean the process of pulling the data? \nYou also said you're trying to do more than what's shown, does that mean you have additional pieces of data you are pulling or is there more you're trying to do than what's in the original post? \nHave you already tried using replit or similar to quickly build something that can pull this data for you or are you set on finding a pre built option?",
                  "score": 1,
                  "created_utc": "2026-02-28 16:32:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7to40v",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-28 04:00:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7u82xb",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-28 06:33:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rffmmh",
      "title": "Should I focus on bypassing Cloudflare or finding the internal API?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rffmmh/should_i_focus_on_bypassing_cloudflare_or_finding/",
      "author": "Otherwise-Advance466",
      "created_utc": "2026-02-26 16:59:16",
      "score": 8,
      "num_comments": 20,
      "upvote_ratio": 0.83,
      "text": "Hey r/webscraping,\n\nI've been researching web scraping with Cloudflare protection for a while now and I'm at a crossroads. I've done a lot of reading (Stack Overflow threads, GitHub issues, etc.) and I understand the landscape pretty well at this point ‚Äì but I can't decide which approach to actually invest my time in.\n\n# What I've already learned / tried conceptually:\n\n* `undetected_chromedriver` works against basic Cloudflare **but not in headless mode**\n* The workaround for headless on Linux is **Xvfb** (virtual display) with SeleniumBase UC Mode\n* `playwright-stealth`, manually copying cookies/headers, FlareSolverr ‚Äì all **unreliable** against aggressive Cloudflare configs\n* Copying `cf_clearance` cookies into Scrapy requests **doesn't work** because Cloudflare binds them to the original TLS fingerprint (JA3)\n* For serious Cloudflare (Enterprise tier) basically nothing open-source works reliably\n\n# My actual question:\n\nI've heard that many sites using Cloudflare on their frontend actually have **internal APIs** (XHR/Fetch calls) that are either less protected or protected differently (e.g. just an API key).\n\nShould I:\n\n**Option A)** Focus on bypassing Cloudflare using SeleniumBase UC Mode + Xvfb, accepting that it might break at any time and requires a non-headless setup\n\n**Option B)** Dig into the Network tab of the target site, find the internal API calls, and try to replicate those directly with Python requests ‚Äì potentially avoiding Cloudflare entirely\n\n**Option C)** Something else entirely that I'm missing?\n\n# My constraints:\n\n* Running on Linux server (so headless environment)\n* Python preferred\n* Want something reasonably stable, not something that breaks every 2 weeks when Cloudflare updates\n\nWhat would you do in my position? Has anyone had success finding internal APIs on heavily Cloudflare-protected sites? Any tips on what to look for in the Network tab?\n\nThanks in advance\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rffmmh/should_i_focus_on_bypassing_cloudflare_or_finding/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o7ju9ju",
          "author": "Flojomojo0",
          "text": "You should always try to find the internal apis as it simplifies scraping by a lot",
          "score": 10,
          "created_utc": "2026-02-26 17:41:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jw297",
          "author": "Objectdotuser",
          "text": "headless mode will never work, too easy to detect. just commit to running a bunch of machines and browsers",
          "score": 5,
          "created_utc": "2026-02-26 17:49:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lbnu7",
          "author": "Hopeless_Scraping",
          "text": "Use the TLS fingerprints used for the clearance and replicate them in your request",
          "score": 3,
          "created_utc": "2026-02-26 21:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k3plx",
          "author": "scrape-do",
          "text": "*Completely depends on your target domain, but I'm going to assume you have a specific domain in mind that's very aggressively protected by CF.*\n\n  \nSeems like you've tried your luck with the bypass approach, so I would go for the internal API if I were you. They usually have light-protection and you can mimic a legitimate backend call with a few cookies or the right payload.\n\n  \nIf you're building scrapers for multiple domains, go for the internal API first **EVERY TIME** unless it's a basic server-rendered site, you'll build a muscle for it and save huge time on setup and maintenance, not mentioning performance. Compared to front-end changes and CF updates, backend API scrapers rarely breaks.",
          "score": 1,
          "created_utc": "2026-02-26 18:24:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7k3zn6",
              "author": "scrape-do",
              "text": "Although there might be times where the backend is virtually impossible to crack at large-scale, so keep Selenium as an option at all times :)",
              "score": 1,
              "created_utc": "2026-02-26 18:26:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7k3thm",
          "author": "Pauloedsonjk",
          "text": "c + b, a.\nA)I use uc with xvfb in production, we have PHP + python, with proxy, captcha\nand\nC+B) too solved in this way when I need it.",
          "score": 1,
          "created_utc": "2026-02-26 18:25:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kctcp",
          "author": "bluewhalefunk",
          "text": "cloudflare is fine but you good IPs.  Playright patched works fine. IF you get it. tab tab enter (or space) and it will solve\n\n> I've heard that many sites using Cloudflare on their frontend actually have internal APIs (XHR/Fetch calls) that are either less protected or protected differently (e.g. just an API key).\n\nNo one can tell you. That's the thing with scraping. 9/10 no one knows the issues you will fsce until they have done exactly what you want to do. You just have to do it, discover the issues and bang your head against the wall until you solve them.  I've done this for 15+ years, trust me, knowing how you bang your head against the way for hours days weeks until you solve it is the most useful skill you can have.",
          "score": 1,
          "created_utc": "2026-02-26 19:06:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kvlrb",
          "author": "AdministrativeHost15",
          "text": "API calls are protected too. I investigated an error and instead of JSON it was returning the HTML of the captcha page.",
          "score": 1,
          "created_utc": "2026-02-26 20:36:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l4nsp",
              "author": "Otherwise-Advance466",
              "text": "so how do website monitors work then? especially like sneaker retailer monitors, how do they bypass cloudflare ",
              "score": 1,
              "created_utc": "2026-02-26 21:19:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7lfxw6",
          "author": "TabbyTyper",
          "text": "Not sure it can answered here, but what sites are running enterprise-grade cloudflare? Are those tougher than casino sites to avoid detection?",
          "score": 1,
          "created_utc": "2026-02-26 22:13:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mm8kx",
          "author": "cyber_scraper",
          "text": "1. Option B should be always priority where it's feasible.\n2. Compare your costs for all scraping infra to run Option A with some 3rd party services who could provide API to protected websites you need (of course if exist). Maybe it's not worth to build/maintain.\n3. If B and C don't fit - experiment with A",
          "score": 1,
          "created_utc": "2026-02-27 02:06:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7n7cxm",
          "author": "Coding-Doctor-Omar",
          "text": "For browser automation, try strong automation libraries/frameworks such as Scrapling and cloakbrowser. If you want to use scrapy, use scrapy impersonate in order to spoof TLS fingerprints. If you want to use requests, use curl_cffi instead to spoof TLS fingerprints. Obviously, if you can find internal APIs, go for them since that's the best option.",
          "score": 1,
          "created_utc": "2026-02-27 04:13:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nm22n",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-27 05:58:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7odso2",
              "author": "webscraping-ModTeam",
              "text": "üëî Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
              "score": 1,
              "created_utc": "2026-02-27 10:08:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7nzi5k",
          "author": "irrisolto",
          "text": "Apis are behind cloudfare waf too. You need to find out what kind of cloudfare challenge the website has. If it‚Äôs just jsd you can use an opensource solver to get cf_clearence and use it for api requests. If the site it‚Äôs low sec you can get cf_clearence just by making a req to the home page and use it for the api requests. If the page is under UAM you need a solver / browser to get cf cookie",
          "score": 1,
          "created_utc": "2026-02-27 07:53:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7oaixq",
          "author": "Curious_Anteater7293",
          "text": "If there is no API you can use, then bypassing cloudflare is the only option (if you want it free of course. If you dont care about paying for acraping, just use services that do all the work for you)\n\nHonestly, after a week of delwing inside of the antibor systems, making your own bypass is not that hard. However, it requires a lot of skill and time. The best way to bypass the cloudflare is to firstly understand what it does to detect a bot. It scans a lot of params in your browser, using fingerprints and many other things.\n\nWhen I developed my bot system, I made a MITM proxy chain gate that replaced TLS and HTTP/2 fingerprints with valid ones based on the bot's useragent. Then, you need to spoof a ton of fingerprints: webgl, canvas, fonts, useragent, screen resolution, timezone, locale, even Audio.\n\nAlso, you should understand that a real user never can type the whole sentence in a millisecond or click right in the center of the button. Behavioral patterns are important, too\n\nI can only wish you luck xD",
          "score": 1,
          "created_utc": "2026-02-27 09:37:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7q1iko",
          "author": "kingxx773",
          "text": "go with internal api i do same thing too. it will make things faster",
          "score": 1,
          "created_utc": "2026-02-27 16:18:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7u97cv",
          "author": "jagdish1o1",
          "text": "Try to find internal apis first, and if that‚Äôs protected too try making request from cf worker ;) \n\nJust recently i had this issue where the api was returning html but in the network tab it‚Äôs showing JSON response. I‚Äôve created an api on cf worker which simply makes the request to the given url and it surprisingly worked! \n\nMy guess is the api whitelisted the cf network.",
          "score": 1,
          "created_utc": "2026-02-28 06:43:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o83nvus",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-03-01 19:11:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o849jl3",
              "author": "webscraping-ModTeam",
              "text": "üö´ü§ñ No bots",
              "score": 1,
              "created_utc": "2026-03-01 21:01:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rh4kcy",
      "title": "webscraping websites for arbitrage",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rh4kcy/webscraping_websites_for_arbitrage/",
      "author": "misterno123",
      "created_utc": "2026-02-28 14:51:43",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 0.85,
      "text": "Currently I am running a webscraper from home using data center proxies. I scrape only the ASINs in websites where same item has low rank on amazon. It is scraping sites with items for sale in bulk and I buy them on the cheap and sell them on amazon as new. This is just 1 item so to expand , I tried this with electronics and auto parts but most sites asking for physical location to buy in bulk\n\nIt does not have to be on amazon I can sell on ebay also, but I am looking for websites to buy in bulk. Any ideas? or is there a better subreddit to ask this question?\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rh4kcy/webscraping_websites_for_arbitrage/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o81apk7",
          "author": "glowandgo_",
          "text": "not really my lane, but pure scraping arb usually compresses fast. once you automate it, others do too. margins go to zero, esp in electronics.,sites asking for physical location is prob screening resellers, signal about supply control...you‚Äôll get better answers in r/FulfillmentByAmazon or r/Flipping. this sub isn‚Äôt super ops focused..if expanding, think moat not just more sites. pure arb becomes a race.",
          "score": 3,
          "created_utc": "2026-03-01 11:12:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o86t2w3",
          "author": "shadowfax12221",
          "text": "Big digital retailers tend to fight bots like hell to prevent this exact thing from being done. The most effective method I've been able to come up with is to automate your computer's peripherals to grab the segment of your computer screen that contains the data you're after as a screenshot, then digitize it using an OCR model and dump it into a database.",
          "score": 1,
          "created_utc": "2026-03-02 06:36:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rhm1oq",
      "title": "Monthly Self-Promotion - March 2026",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rhm1oq/monthly_selfpromotion_march_2026/",
      "author": "AutoModerator",
      "created_utc": "2026-03-01 03:00:29",
      "score": 7,
      "num_comments": 22,
      "upvote_ratio": 0.83,
      "text": "Hello and howdy, digital miners of¬†r/webscraping!\n\nThe moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!\n\n* Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?\n* Maybe you've got a ground-breaking product in need of some intrepid testers?\n* Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?\n* Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?\n\nWell, this is your time to shine and shout from the digital rooftops - Welcome to your haven!\n\nJust a friendly reminder, we like to keep all our self-promotion in one handy place, so any promotional posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rhm1oq/monthly_selfpromotion_march_2026/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o7zvcsg",
          "author": "HLCYSWAP",
          "text": "I have defeated datadome, cloudflare, akamai and perimeterX at scale. (ticketmaster, meta, x, cia's FOIA, live, dozens of smaller client specific targets)\n\nETL (scraping), databasing, ML training AI (captcha bypass, voice, transformers - LLMs), standard desktop apps like DAWs as seen on my github. my past successes here:\n\n[https://github.com/matthew-fornear](https://github.com/matthew-fornear)\n\nopen to short or long work. you can reach me via reddit, x ([https://www.x.com/fixitorgotojail](https://www.x.com/fixitorgotojail)) or discord",
          "score": 7,
          "created_utc": "2026-03-01 03:48:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o80f9u8",
              "author": "Salty-Mouse7235",
              "text": "Hi,\n\nI have a project. How can i reach out to you? X wont let me message you as I dont have premium",
              "score": 2,
              "created_utc": "2026-03-01 06:17:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o81y3nb",
                  "author": "HLCYSWAP",
                  "text": "I sent you a reddit chat request",
                  "score": 1,
                  "created_utc": "2026-03-01 14:07:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o8053bx",
          "author": "rishiilahoti",
          "text": "Created an open source job scraper for Ashby Hq Jobs.\n\nI was tired of manually checking career pages every day, so I built a full-stack job intelligence platform that scrapes AshbyHQ's public API (used by OpenAI, Notion, Ramp, Cursor, Snowflake, etc.), stores everything in PostgreSQL, and surfaces the best opportunities through a Next.js frontend.\n\nWhat it does:\n\n\\* Scrapes 53+ companies every 12 hours via cron\n\n\\* User can add company via pasting url with slug (jobs.ashbyhq.com/{company})\n\n\\* Detects new, updated, and removed postings using content hashing\n\n\\* Scores every job based on keywords, location, remote preference, and freshness\n\n\\* Lets you filter, search, and mark jobs as applied/ignored (stored locally per browser)\n\nTech: Node.js backend, Neon PostgreSQL, Next.js 16 with Server Components, Tailwind CSS. Hosted for $0 (Vercel + Neon free tier + GitHub Actions for the cron).\n\nWould love suggestions on the project.  \n\nGithub Repo: \\[https://github.com/rishilahoti/ashbyhq-scraper\\](https://github.com/rishilahoti/ashbyhq-scraper)  \n\nLive Website: \\[https://ashbyhq-scraper.vercel.app/\\](https://ashbyhq-scraper.vercel.app/)",
          "score": 2,
          "created_utc": "2026-03-01 04:57:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o834dbf",
              "author": "digital__navigator",
              "text": "cool site",
              "score": 1,
              "created_utc": "2026-03-01 17:40:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o835eea",
                  "author": "rishiilahoti",
                  "text": "Thanks mate",
                  "score": 1,
                  "created_utc": "2026-03-01 17:45:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o80ow1b",
          "author": "Hot-Muscle-7021",
          "text": "We are now offering access to Bet365 data via our platform: [https://pulsescore.net/](https://pulsescore.net/)\n\nOur goal is to provide structured, reliable, and low-latency Bet365 data for developers, analysts, and businesses that need high-quality sports data feeds.\n\nYou can explore what‚Äôs currently available through our **free subscription**, which gives you direct access to data and lets you evaluate the quality, structure, and coverage.\n\nMore datasets, expanded coverage, and additional features are coming soon.\n\nIf you‚Äôre building trading models, analytics tools, betting platforms, or data-driven applications, this can serve as a solid foundation.\n\nVisit: [https://pulsescore.net/](https://pulsescore.net/)  \nFree access available.\n\nIf you have questions or need specific datasets, feel free to reach out.",
          "score": 2,
          "created_utc": "2026-03-01 07:44:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80znvi",
          "author": "CouldBeNapping",
          "text": "Signed my 8th retail client last week for price scraping, PDP checking and competitor promotion reviews.  \nMeans ARR will jump to ¬£110,000. Almost at the point where I'm considering ditching the day job.  \n",
          "score": 2,
          "created_utc": "2026-03-01 09:26:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o813ni8",
              "author": "No-Appointment9068",
              "text": "Congratulations! If you wouldn't mind, can you give me some details on what you offer your clients? I'm looking to move into a similar niche.",
              "score": 1,
              "created_utc": "2026-03-01 10:04:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o816b0n",
                  "author": "CouldBeNapping",
                  "text": "I'm not going to give the secret sauce another person in the UK I'm afraid. It's already a crowded market so I don't want more noise ;)",
                  "score": 1,
                  "created_utc": "2026-03-01 10:30:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o80zu1g",
          "author": "deepwalker_hq",
          "text": "Deepwalker - the ultimate mobile AI agent\n\nhttps://deepwalker.xyz",
          "score": 2,
          "created_utc": "2026-03-01 09:28:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o81gxc5",
          "author": "scorpiock",
          "text": "Geekflare API to scrape any website into Markdown, HTML and JSON.\n\n\nhttps://geekflare.com/api/webscraping/\n\n\n500 free credits to try.¬†",
          "score": 2,
          "created_utc": "2026-03-01 12:07:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80mlm3",
          "author": "digital__navigator",
          "text": "Noob compared to others, web scraped 86,467 courses from 9 universities, you can see course catalog stats like longest and shortest coursenames for them all.\n\n  \nPut everything on a website.\n\n[https://www.degreeviewsite.com/](https://www.degreeviewsite.com/)\n\nIm a uni student trying to do entreprenuership one day.\n\n",
          "score": 1,
          "created_utc": "2026-03-01 07:22:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80p1no",
          "author": "Jonathan_Geiger",
          "text": "Social media API for extracting transcripts, summaries, video data, and more (:\n\n[SocialKit](https://socialkit.dev)",
          "score": 1,
          "created_utc": "2026-03-01 07:45:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o82zbus",
          "author": "Curious_Anteater7293",
          "text": "Built a Go scraper/bot using native Chrome to support DRM parsing. It uses consistent pattern-based fingerprint spoofing to keep 1,000+ instances unique and undetected (tested on Spotify for weeks with zero bans).\n‚ÄãIt was a massive undertaking, but I'm proud of the result. My next goal is to turn this into a standalone browser with a custom scripting language, allowing users to automate tasks while the engine handles the fingerprinting and humanization automatically.",
          "score": 1,
          "created_utc": "2026-03-01 17:16:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o85j4m8",
          "author": "maher_bk",
          "text": "Created a mobile app to summarize (bypass any soft paywalls) and subscribe to multiple (up to 8) pages on the internet to get a daily digest of new content accross all subscription. Basically RSS next gen ;)\nCheck it out here https://www.universalsummarizer.com",
          "score": 1,
          "created_utc": "2026-03-02 01:16:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o85taat",
          "author": "HBM_01",
          "text": "Hi everyone!\n\nWe run Crawlnow, a managed web data extraction service designed for teams that need reliable, production-ready scraping without maintaining complex infrastructure.\n\n\n\nWhat we focus on:\n\n\n\n* ¬†Scraping JavaScript-heavy & anti-bot protected sites\n* ¬†Custom crawlers (no generic ‚Äúone-size-fits-all‚Äù scrapers)\n* ¬†Clean, structured data (CSV / JSON / API delivery)\n* ¬†Scheduled & large-scale crawling\n* ¬†Use cases: market research, price monitoring, SERP data, business directories, reviews, real estate, etc.\n\n\n\n\n\nWe work a lot with research firms, SaaS teams, and analysts who need data they can trust ‚Äî not just scripts that break every week.\n\n\n\nIf you‚Äôre:\n\n\n\n* tired of captchas & blocks\n* spending more time fixing scrapers than using data\n* or need a custom extraction done fast\n\n\n\n\n\nHappy to answer questions or discuss approaches publicly here.\n\nWebsite:¬†[www.crawlnow.com](http://www.crawlnow.com)\n\n",
          "score": 1,
          "created_utc": "2026-03-02 02:19:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o86ha5q",
          "author": "West-System-3822",
          "text": "Hey r/webscraping! I've been building scrapers for Korean web platforms - sharing in case it's useful to anyone working with Korean data.\n\n**12 Korean-specific scrapers on Apify Store** (pay-per-result, no monthly fee):\n\n**Naver ecosystem** (Korea's dominant platform)\n- [Naver Place Reviews](https://apify.com/oxygenated_quagmire/naver-place-reviews) - business reviews from Naver Map\n- [Naver Place Search](https://apify.com/oxygenated_quagmire/naver-place-search) - find businesses on Naver Map\n- [Naver Place Photos](https://apify.com/oxygenated_quagmire/naver-place-photos) - photos from Naver Map businesses\n- [Naver Blog Search](https://apify.com/oxygenated_quagmire/naver-blog-search) - Korea's biggest blogging platform\n- [Naver Blog Reviews](https://apify.com/oxygenated_quagmire/naver-blog-reviews) - user reviews from blog posts\n- [Naver News](https://apify.com/oxygenated_quagmire/naver-news-scraper) - news search + 6 sections\n- [Naver KiN](https://apify.com/oxygenated_quagmire/naver-kin-scraper) - Korea's Q&A platform (like Yahoo Answers)\n- [Naver Webtoon](https://apify.com/oxygenated_quagmire/naver-webtoon-scraper) - webtoon metadata & rankings\n\n**Marketplaces**\n- [Daangn](https://apify.com/oxygenated_quagmire/daangn-market-scraper) - Korea's biggest C2C marketplace\n- [Bunjang](https://apify.com/oxygenated_quagmire/bunjang-market-scraper) - Korean second-hand marketplace\n- [YES24](https://apify.com/oxygenated_quagmire/yes24-book-scraper) - Korea's largest online bookstore\n\n**Entertainment + MCP**\n- [Melon Chart](https://apify.com/oxygenated_quagmire/melon-chart-scraper) - Korea's #1 K-pop streaming chart\n- [Naver Place MCP](https://apify.com/oxygenated_quagmire/naver-place-mcp) - MCP server for AI agents (Streamable HTTP)\n\n**Technical notes:** Most use pure HTTP + JSON parsing (no Playwright). Naver serves data via SSR Apollo state or GraphQL. Happy to answer questions!",
          "score": 1,
          "created_utc": "2026-03-02 05:00:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o8441if",
          "author": "BodybuilderLost328",
          "text": "We built a vibe scraping platform: [rtrvr.ai](http://rtrvr.ai)\n\nJust prompt and we return you datasets. Free to try and you can bring your own LLM API keys!\n\nCan leverage us as an extension for hard to automate sites, and our cloud platform for scale!",
          "score": 0,
          "created_utc": "2026-03-01 20:33:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcxwo5",
      "title": "Scrape transcripts from Spotify",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rcxwo5/scrape_transcripts_from_spotify/",
      "author": "AnglePast1245",
      "created_utc": "2026-02-23 23:36:52",
      "score": 7,
      "num_comments": 9,
      "upvote_ratio": 0.83,
      "text": "Does anyone know a reliable way (via API, browser extension, script, or tool) to scrape or export full episode transcripts from Spotify podcasts?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rcxwo5/scrape_transcripts_from_spotify/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o73tadx",
          "author": "boomersruinall",
          "text": "Following, as I am also looking for answers regarding transcripts",
          "score": 3,
          "created_utc": "2026-02-24 08:51:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7delxm",
          "author": "jagdish1o1",
          "text": "Have you tried charles to intercept the api requests?",
          "score": 2,
          "created_utc": "2026-02-25 18:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7577nl",
          "author": "yyavuz",
          "text": "In the same boat",
          "score": 1,
          "created_utc": "2026-02-24 14:49:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ak2f2",
          "author": "vorty212",
          "text": "Following",
          "score": 1,
          "created_utc": "2026-02-25 08:20:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfdqsb",
      "title": "Web Scraper / Researcher Needed ‚Äì Pre-Opening  Business Leads",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rfdqsb/web_scraper_researcher_needed_preopening_business/",
      "author": "techguyfl17",
      "created_utc": "2026-02-26 15:51:03",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.87,
      "text": "\n\nDescription:\n\nI‚Äôm looking for an experienced web scraper or researcher to help identify brick-and-mortar SMB businesses that are under construction or preparing to open in Florida (starting South Florida/ Florida).\n\nObjective:  \nGenerate weekly leads of businesses BEFORE they launch so I can offer MSP / full-suite technology services.\n\nPrimary Sources:  \n‚Ä¢ County & city permit databases (Tenant Improvement, Buildout, Commercial Remodel, New Construction)  \n‚Ä¢ Business license filings  \n‚Ä¢ Local business journals  \n‚Ä¢ ‚ÄúComing Soon‚Äù storefronts  \n‚Ä¢ Commercial lease announcements\n\nRequired Data:  \n‚Ä¢ Business name  \n‚Ä¢ Address  \n‚Ä¢ Industry/type  \n‚Ä¢ Permit date + status  \n‚Ä¢ Estimated opening date (if available)  \n‚Ä¢ Email/contact (or source link for enrichment)  \n‚Ä¢ Direct source link\n\nDeliverables:  \n‚Ä¢ Weekly Google Sheet or CSV  \n‚Ä¢ No duplicates  \n‚Ä¢ Fresh leads (last 30 days)  \n‚Ä¢ Organized + structured format\n\nTo apply:\n\n1. Describe your experience scraping government portals.\n2. Tell me what tools you use (Python, BeautifulSoup, Scrapy, etc.).\n3. Share a sample output (if available).\n4. Quote hourly rate or per-lead pricing.\n\nThis will  become ongoing weekly work for the right candidate.\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1rfdqsb/web_scraper_researcher_needed_preopening_business/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rheo0m",
      "title": "Created an open source job scraper for Ashby Hq Jobs.",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rheo0m/created_an_open_source_job_scraper_for_ashby_hq/",
      "author": "rishiilahoti",
      "created_utc": "2026-02-28 21:32:32",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "I was tired of manually checking career pages every day, so I built a full-stack job intelligence platform that scrapes AshbyHQ's public API (used by OpenAI, Notion, Ramp, Cursor, Snowflake, etc.), stores everything in PostgreSQL, and surfaces the best opportunities through a Next.js frontend.\n\nWhat it does:\n\n\\* Scrapes 53+ companies every 12 hours via cron\n\n\\* User can add company via pasting url with slug (jobs.ashbyhq.com/{company})\n\n\\* Detects new, updated, and removed postings using content hashing\n\n\\* Scores every job based on keywords, location, remote preference, and freshness\n\n\\* Lets you filter, search, and mark jobs as applied/ignored (stored locally per browser)\n\nTech: Node.js backend, Neon PostgreSQL, Next.js 16 with Server Components, Tailwind CSS. Hosted for $0 (Vercel + Neon free tier + GitHub Actions for the cron).\n\nWould love suggestions on the project.  \n\nGithub Repo: \\[https://github.com/rishilahoti/ashbyhq-scraper\\](https://github.com/rishilahoti/ashbyhq-scraper)  \n\nLive Website: \\[https://ashbyhq-scraper.vercel.app/\\](https://ashbyhq-scraper.vercel.app/)\n\n!\\[img\\](v2y8d00ym7mg1)",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1rheo0m/created_an_open_source_job_scraper_for_ashby_hq/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rdbvik",
      "title": "What's working for you with proxy rotation?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rdbvik/whats_working_for_you_with_proxy_rotation/",
      "author": "Forsaken-Bobcat4065",
      "created_utc": "2026-02-24 09:40:07",
      "score": 3,
      "num_comments": 24,
      "upvote_ratio": 0.81,
      "text": "¬†I‚Äôve been down the scraping rabbit hole lately and honestly‚Ä¶ I‚Äôm spending way too much time dealing with rate limits, CAPTCHAs, random blocks, and instability. \n\nWhat are people using these days to manage proxies and keep things running smoothly? Rotating residential or datacenter proxies, specific libraries, browser automation, or a mix? \n\nI‚Äôm just looking for something that actually works in real-world projects without becoming a full-time maintenance job. Any tools or setups that have made things more stable and hands-off?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rdbvik/whats_working_for_you_with_proxy_rotation/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o75xjf7",
          "author": "Gold_Emphasis1325",
          "text": "The working environment to operate like this is getting crushed. Enough ankle biters creating bots and \"Agents\" caused big players to finally invest the time into closing known gaps that allowed more TOS violations and scraping. It's only going to get more difficult to get away with things. Any long-term plan relying on scraping or having \"robust scraping\" is likely not a plan at all.",
          "score": 7,
          "created_utc": "2026-02-24 16:50:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75520e",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-02-24 14:38:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75axnp",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-02-24 15:07:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ahux6",
          "author": "thomas_estate",
          "text": "Residential proxies are the way to go for anything with serious anti-bot protection. Datacenter IPs get flagged almost immediately on most major sites.\n\nFor rotation, I've had good results with backconnect proxy services that handle the rotation on their end ‚Äî you just hit one endpoint and they cycle IPs automatically. Way less headache than managing your own pool.\n\nBrowser automation (Playwright/Puppeteer) with stealth plugins helps a ton with CAPTCHAs. Some sites still require manual solving services, but between that and residential IPs, most blocks disappear.\n\nWhat scale are you running at? And what types of sites mainly? The approach shifts a lot depending on whether you're hitting a few targets hard vs. scraping broadly across many domains.",
          "score": 2,
          "created_utc": "2026-02-25 07:59:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74orx1",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-24 13:09:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74qoen",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-02-24 13:20:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75m3xf",
          "author": "VonDenBerg",
          "text": "GOOD providers, not trash. ",
          "score": 1,
          "created_utc": "2026-02-24 15:59:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7f5vkf",
              "author": "HardReload",
              "text": "And how would one find those?",
              "score": 1,
              "created_utc": "2026-02-25 23:37:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7h7uag",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 1,
                  "created_utc": "2026-02-26 07:36:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77345g",
          "author": "Alone-Rub-4418",
          "text": "I feel you on the rabbit hole thing. I've had decent luck with a mix of residential proxies and a simple backoff/retry strategy in my scripts. Honestly, the biggest game changer for me was just accepting that some blocks are inevitable and building my scraper to fail gracefully and pick up where it left off",
          "score": 1,
          "created_utc": "2026-02-24 19:57:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78ychh",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-25 01:37:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79dj43",
              "author": "webscraping-ModTeam",
              "text": "üö´ü§ñ No bots",
              "score": 1,
              "created_utc": "2026-02-25 03:02:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o790hj1",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-25 01:49:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79dkep",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-25 03:02:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79ttfj",
          "author": "OwnPrize7838",
          "text": "I only use static clean 0 fraud ISP",
          "score": 1,
          "created_utc": "2026-02-25 04:44:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7f5jdm",
              "author": "HardReload",
              "text": "Can you elaborate?",
              "score": 1,
              "created_utc": "2026-02-25 23:35:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdfm7a",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rdfm7a/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-02-24 13:01:04",
      "score": 3,
      "num_comments": 8,
      "upvote_ratio": 0.67,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levels‚Äîwhether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) üå±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1rdfm7a/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o763nqx",
          "author": "jagdish1o1",
          "text": "Hey! I‚Äôm not sure what category I fall into when it comes to scraping, but I‚Äôve done plenty of scraping projects over the years and have gained solid knowledge of how to scrape various websites.\n\nHere are some tips from my side:\n\n1. Try to avoid using browsers for scraping unless it‚Äôs absolutely necessary. Even if you have to use one, capture the request headers from the browser and try to mimic the request using those headers instead.\n2. Use residential rotating proxies for recurring scraping tasks, especially when you need to scrape a site on a daily basis.\n3. Consider integrating AI into your HTML parsing. This can save you a lot of maintenance work in the long run. Just make sure to enforce structured output.\n4. Write modular code instead of putting everything into one or two scripts. This will save you time on future projects and make maintenance easier.\n5. Use exponential backoff instead of simple retries. Even better, use exponential backoff with jitter. This helps reduce bottlenecks and handle rate limiting more effectively.\n\nIf you already have strong scraping knowledge, consider building APIs for popular websites and selling them on RapidAPI.\n\nThese are the points that come to mind right now. I‚Äôll add more in a reply if I think of anything else.\n\nPeace ‚úåÔ∏è",
          "score": 3,
          "created_utc": "2026-02-24 17:17:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7anwp8",
              "author": "GoingGeek",
              "text": "ai is good but which local small model would u recommend for fast parsing.",
              "score": 1,
              "created_utc": "2026-02-25 08:56:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ar31k",
                  "author": "jagdish1o1",
                  "text": "I use openai or gemini models, haven‚Äôt tried ai models locally. Apis works just fine.",
                  "score": 1,
                  "created_utc": "2026-02-25 09:26:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ew0xq",
                  "author": "Azuriteh",
                  "text": "Pretty much any SLM post 2025, e.g. Qwen3 4b 2507 should work pretty well",
                  "score": 1,
                  "created_utc": "2026-02-25 22:45:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ao4a7",
              "author": "GoingGeek",
              "text": "and do u mind me knocking u in dm\n\n",
              "score": 1,
              "created_utc": "2026-02-25 08:58:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ar6ju",
                  "author": "jagdish1o1",
                  "text": "Sure as long as you‚Äôre not selling something.",
                  "score": 2,
                  "created_utc": "2026-02-25 09:27:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7tu4of",
          "author": "mnlaowai",
          "text": "I‚Äôm looking for someone to help me build something to scrape a relatively simple association directory on a website. Effectively, you need to click on A, and then all of the members with the last name of A come up. Click on each name to get their position and contact info. I want an excel file just listing all of this data. I‚Äôm not a coder but using regular LLMs, I could get A and B done. Would prefer not to spend a couple hours on it though and figure someone here might be able to make something relatively quickly.",
          "score": 1,
          "created_utc": "2026-02-28 04:42:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zvboq",
              "author": "OkAd6989",
              "text": "Which directory is this? Might be able to assist. ",
              "score": 1,
              "created_utc": "2026-03-01 03:48:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rgfbv7",
      "title": "[HELP] How to scrape dynamic webistes with pagination",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1rgfbv7/help_how_to_scrape_dynamic_webistes_with/",
      "author": "nirvana_49",
      "created_utc": "2026-02-27 18:50:49",
      "score": 3,
      "num_comments": 11,
      "upvote_ratio": 0.81,
      "text": "Scraping this URL: \\`https://www.myntra.com/sneakers?rawQuery=sneakers\\`\n\nPagination is working fine ‚Äî the meta text updates (\\`Page 1 of 802 ‚Üí Page 2 of 802\\`) after clicking \\`li.pagination-next\\`, but \\`window.\\_\\_myx.searchData.results.products\\` always returns the same 32 product IDs regardless of which page I'm on.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1rgfbv7/help_how_to_scrape_dynamic_webistes_with/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o7ruj8z",
          "author": "cyber_scraper",
          "text": "If you check network tab in dev tools you would see that there are calls to internal api gateway like:  \n[https://www.myntra.com/gateway/v4/search/sneakers%60?rawQuery=sneakers%60&rows=50&o=99&plaEnabled=true&xdEnabled=false&isFacet=true&p=3](https://www.myntra.com/gateway/v4/search/sneakers%60?rawQuery=sneakers%60&rows=50&o=99&plaEnabled=true&xdEnabled=false&isFacet=true&p=3)   \n  \nSo you just need to handle cookies and change page",
          "score": 5,
          "created_utc": "2026-02-27 21:33:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7x827a",
              "author": "nirvana_49",
              "text": "Thanks",
              "score": 2,
              "created_utc": "2026-02-28 18:44:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7shuwc",
          "author": "abdullah-shaheer",
          "text": "I think you can easily use it's API as it is very prominent in the network requests. Here is an example to get the data:-\n\nimport requests\n\ncookies = {your cookies here from the website}\n\nheaders = {your headers}\n\n\\# use these params to query\n\nparams = {\n\n'rawQuery': 'sneakers\\`',\n\n'rows': '50',\n\n'o': '99',\n\n'plaEnabled': 'true',\n\n'xdEnabled': 'false',\n\n'isFacet': 'true',\n\n'p': '3',\n\n}\n\n\n\nresponse = requests.get('https://www.myntra.com/gateway/v4/search/sneakers%60', params=params, cookies=cookies, headers=headers)\n\nprint(response.text)",
          "score": 2,
          "created_utc": "2026-02-27 23:39:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7rmp9x",
          "author": "BrightProgrammer9590",
          "text": "On each result page, wait for the products to become available. Then parse the list. You may even have to keep track of one of your last product elements to make sure it is gone before assuming new product list is loaded",
          "score": 1,
          "created_utc": "2026-02-27 20:54:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7s0r4e",
          "author": "bootlegDonDraper",
          "text": "I got it working through Playwright.\n\n`window.__myx.searchData.results.products` is set once on page load, and won't update with pagination clicks.\n\nWhen you click next, the browser fires an XHR to \\`/gateway/v4/search/sneakers?rawQuery=sneakers&rows=50&o=49&...\\` which has the next page of products. The frontend updates the DOM from it but doesn't write back to myx, weird choice on Myntra's end for sure.\n\nSo you should intercept that network response instead of reading myx by listening to responses matching \\`/gateway/v4/search/\\` and read .products from the JSON body.",
          "score": 1,
          "created_utc": "2026-02-27 22:04:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7s50lw",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-27 22:27:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7u33gc",
          "author": "jagdish1o1",
          "text": "Alway look for internal apis first, don‚Äôt rely on html parsing.",
          "score": 1,
          "created_utc": "2026-02-28 05:51:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7wwe7i",
          "author": "EntranceSorry7225",
          "text": "Sounds like the pagination might be loading new content via JavaScript but the data object isn't being refreshed. Try checking the network tab in dev tools to see if there's a separate API call being made when you click next sometimes the product data gets fetched from a different endpoint",
          "score": 1,
          "created_utc": "2026-02-28 17:47:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1reg5wg",
      "title": "How to scrape ios/android top downloaded apps for a specific country?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1reg5wg/how_to_scrape_iosandroid_top_downloaded_apps_for/",
      "author": "letopeto",
      "created_utc": "2026-02-25 15:24:02",
      "score": 2,
      "num_comments": 6,
      "upvote_ratio": 0.63,
      "text": "How do you scrape ios/android top downloaded apps (free & paid) for a specific country (Sweden)? beyond 100+ results? The endpoint people use (rss.applemarketingtools.com and itunes) only seem to return 100 results.\n\nI can't figure out what the correct api url to query is and if any auth is required. Any help would be greatly appreciated.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1reg5wg/how_to_scrape_iosandroid_top_downloaded_apps_for/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o7cejqy",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-02-25 15:51:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cj961",
              "author": "letopeto",
              "text": "yes im aware of that website, hence why I made this post. I already made a free trial for the website to get the rankings beyond 30 (they only give 30 w/ a free account, and you get the full list on a paid account/trial). But clearly they are able to scrape beyond 100 but all the endpoints I'm trying is limited to 100 results:\n\ne.g. https://itunes.apple.com/se/rss/topfreeapplications/limit=200/genre=36/json \n\n(only gives 100 despite limit set at 200). Apparently this used to return 200 results but they have limited it to 100.\n\nI think there is a new endpoint url to scrape results from because obviously that website is able to get a list beyond 100 result. How are they doing it?\n\nMy goal isn't just to grab the results today, its a daily scrape to see how rankings have changed over time, and ideally i scrape myself vs having to rely on a 3rd party to do it for me that i have to pay for.",
              "score": 1,
              "created_utc": "2026-02-25 16:13:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7cs5b2",
                  "author": "CouldBeNapping",
                  "text": "I wouldn‚Äôt waste your time or energy trying to hack through Apple‚Äôs security/obscurity for the App Store. \n\nYou‚Äôll likely find that the site I linked to has setup loads of virtual iPhones in Xcode and uses OCR to parse the data. \n\nCould do it yourself but effort and cost vs. impact",
                  "score": 2,
                  "created_utc": "2026-02-25 16:53:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7cyoed",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-25 17:23:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7cqate",
          "author": "Medical-Road-5690",
          "text": "Yeah, the 100 result limit on those public endpoints is a known headache. I've had luck using the official App Store Connect API for iOS data it requires a dev account for authentication but can give you deeper country specific charts. For Android, scraping the Play Store directly with something like undetected requests might be your best bet to get past the first page",
          "score": 1,
          "created_utc": "2026-02-25 16:45:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}