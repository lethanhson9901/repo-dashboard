{
  "metadata": {
    "last_updated": "2026-01-18 02:39:57",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 12,
    "total_comments": 33,
    "file_size_bytes": 53648
  },
  "items": [
    {
      "id": "1qb51ih",
      "title": "I created an open-source toolkit to make your scraper suffer",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qb51ih/i_created_an_opensource_toolkit_to_make_your/",
      "author": "niiotyo",
      "created_utc": "2026-01-12 19:45:04",
      "score": 39,
      "num_comments": 9,
      "upvote_ratio": 0.98,
      "text": "Hey everyone. I am the owner of a small web crawler API.\n\nWhen testing my crawler, I needed a dummy website with many edge cases, different HTTP status codes and tricky scenarios. Something like a toolkit for scraper testing.\n\nI used [**httpstat.us**](http://httpstat.us) before, but it has been down for a while. So I decided to build my own tool.\n\nI created a free, open-source website for this purpose: [**https://crawllab.dev**](https://crawllab.dev)\n\nIt includes:\n\n* All common HTTP status codes\n* Different content types\n* Redirect loops\n* JS rendering\n* PDFs\n* Large responses\n* Empty responses\n* Random content\n* Custom headers\n\nI hope you find it as useful as I do. Feel free to add more weird cases at [https://github.com/webcrawlerapi/crawl-lab](https://github.com/webcrawlerapi/crawl-lab)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qb51ih/i_created_an_opensource_toolkit_to_make_your/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nz8jiwv",
          "author": "99ducks",
          "text": "Time to turn it into a capture the flag challenge kind of like the [Bandit wargame](https://overthewire.org/wargames/bandit/). Users would have to build a scraper, adding a new challenge on each level to get the url to scrape for the next level.",
          "score": 10,
          "created_utc": "2026-01-12 21:32:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzb724f",
              "author": "niiotyo",
              "text": "Will add captcha",
              "score": 2,
              "created_utc": "2026-01-13 06:34:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzc0cap",
          "author": "Independent_Pop_5596",
          "text": "Interesting",
          "score": 2,
          "created_utc": "2026-01-13 11:05:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmecr6",
          "author": "fourhoarsemen",
          "text": "Pretty cool! I'll definitely test this with a new project I've been working on: [wxpath](https://github.com/rodricios/wxpath), a declarative web crawler/scraper that extends XPath semantics.",
          "score": 2,
          "created_utc": "2026-01-14 22:11:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp16tj",
              "author": "niiotyo",
              "text": "Want to add a page with advanced DOM?",
              "score": 1,
              "created_utc": "2026-01-15 08:16:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzukui6",
                  "author": "fourhoarsemen",
                  "text": "By \"advanced DOM\", do you mean dynamically generated pages/content (requiring JS rendering)?",
                  "score": 2,
                  "created_utc": "2026-01-16 02:38:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qd4eig",
      "title": "[Open Source] CLI to inject local cookies for auth",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qd4eig/open_source_cli_to_inject_local_cookies_for_auth/",
      "author": "Bubbly_Gap6378",
      "created_utc": "2026-01-15 00:22:45",
      "score": 13,
      "num_comments": 11,
      "upvote_ratio": 0.88,
      "text": "I've been building scrapers for a while, and the biggest pain point is always the login flow. If I try to automate the login with Selenium or Playwright, I hit 2FA, Captchas, or \"Suspicious Activity\" blocks immediately.\n\nI realized the easiest way around this is to stop trying to automate the login and just reuse the valid session I already have on my local Chrome browser.\n\nI wrote a Python CLI tool (Romek) to handle the extraction.\n\nHow it works under the hood:\n\n1. It locates the local Chrome Cookies SQLite database on your machine.\n2. It decrypts the cookies using the OS-specific master key (DPAPI on Windows, AES on Mac/Linux).\n3. It exports them into a JSON format that Playwright/Selenium can read.\n\nWhy I made it:\n\nI needed to run agents on a headless VPS that could access my accounts on complex sites without triggering the \"New Device\" login flow. By injecting the \"High Trust\" cookies from my main profile, the headless browser looks like my desktop.\n\nThe Tool:\n\nIt's 100% Open Source (MIT) and free.\n\nRepo:[https://github.com/jacobgadek/romek](https://github.com/jacobgadek/romek)\n\nPyPI: `pip install romek`\n\nHopefully, this saves someone else from writing another broken login script.",
      "is_original_content": false,
      "link_flair_text": "Bot detection ðŸ¤–",
      "permalink": "https://reddit.com/r/webscraping/comments/1qd4eig/open_source_cli_to_inject_local_cookies_for_auth/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nznr4wm",
          "author": "matty_fu",
          "text": "What is cloud vault sync and will it be free?",
          "score": 1,
          "created_utc": "2026-01-15 02:35:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nznumii",
              "author": "Bubbly_Gap6378",
              "text": "Hey! Great question.\n\nRomek is designed to be **100% local-first.** The 'vault' is just a standard SQLite file created on your local disk (encrypted with your OS keychain).\n\nThe 'sync' logic I'm building is for moving that file between your own machines (e.g., Laptop -> VPS) via your own infrastructure (SSH/S3), not through a proprietary cloud I own.\n\nThe goal is to make a standalone utility (like `curl` or `jq`) for auth, not a subscription service. It will remain open source and free.",
              "score": 2,
              "created_utc": "2026-01-15 02:56:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzsaopj",
          "author": "Illustrious_Dark9449",
          "text": "Great work, have been looking for a solution to this flow!",
          "score": 1,
          "created_utc": "2026-01-15 19:40:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzthzew",
          "author": "andreaswpv",
          "text": "404 Error on the link",
          "score": 1,
          "created_utc": "2026-01-15 23:05:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzupvff",
          "author": "TobiasMcTelson",
          "text": "404",
          "score": 1,
          "created_utc": "2026-01-16 03:06:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx8rv6",
              "author": "Bubbly_Gap6378",
              "text": "Yeah, sorry about that 404. GitHub flagged the repo last night because the cookie decryption was a bit too 'grey hat' for their compliance filters.\n\nIâ€™m re-architecting the auth stuff to be compliant, but in the meantime, I pivoted to releasing the safety infrastructure first.\n\nI just shipped **Vallignus** (a local Firewall for Agents) to PyPI today. It stops agents from spiraling or hitting bad URLs.\n\nYou can check it out here:[https://pypi.org/project/vallignus/](https://www.google.com/search?q=https://pypi.org/project/vallignus/)\n\n(The cookie/auth features will be added back as a safe module in v0.2.0 next week).",
              "score": 1,
              "created_utc": "2026-01-16 14:14:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzz0mbv",
                  "author": "tocarbajal",
                  "text": "And you have plans to release the main repo soon?",
                  "score": 1,
                  "created_utc": "2026-01-16 19:02:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzoylwo",
          "author": "TorZidan",
          "text": "Inject local cookies? Inject them with what? Eclairs re being filled with creme through a hole in the puffy pastry which you may consider being an injection, but I've never heard of this.",
          "score": 1,
          "created_utc": "2026-01-15 07:52:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcnowm",
      "title": "Async web scraping framework on top of Rust",
      "subreddit": "webscraping",
      "url": "https://github.com/BitingSnakes/silkworm",
      "author": "yehors",
      "created_utc": "2026-01-14 13:47:08",
      "score": 10,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qcnowm/async_web_scraping_framework_on_top_of_rust/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "nzjny11",
          "author": "jwrzyte",
          "text": "this sounds cool thanks for sharing, I'll give it a go when i get a chance",
          "score": 1,
          "created_utc": "2026-01-14 14:43:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzk6p1a",
              "author": "yehors",
              "text": "With disabled GIL, it gives around 230 requests per second. So highly useful when you do a lot of parsing.",
              "score": 1,
              "created_utc": "2026-01-14 16:12:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qem7fs",
      "title": "Help on how to go about scraping faculty directory profiles",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qem7fs/help_on_how_to_go_about_scraping_faculty/",
      "author": "gatherer_benefactor",
      "created_utc": "2026-01-16 17:24:19",
      "score": 9,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nIâ€™m working on a research project that requires building a large-scale dataset of faculty profiles from 200 to 250 business schools worldwide. For each school, I need to collect faculty-level data such as: name, title or role, department, short bio or research interests, sometimes email, CV links, publications. The aim is to systematically scrap faculty directories across many heterogeneous university websites. My current setup is like this: Python, Selenium, BeautifulSoup, MongoDB for storage (timestamped entries to allow longitudinal tracking), one scraper per university (100 already written. I do this with the following workflow: manually inspect the faculty directory, write Selenium logic to collect profile URLs, visit each profile and extract fields with BeautifulSoup and then store the data in mongodb. \n\nThis works, but clearly does not scale well to 200 sites, especially long-term maintenance when sites change structure. What Iâ€™m unsure about and looking for advice on is the architecture for automation. Is â€œone scraper per siteâ€ inevitable at this scale? Any recommendations for organizing scrapers so maintenance doesnâ€™t become a nightmare? What are your toughts or experiences using LLMs to analyze a directory HTML, suggest Selenium actions (pagination, buttons), infer selectors?\n\nBasically my question is what you would do differently if you had to do this again for an academic project with transparency/reproducibility constraints, how would you approach it? Iâ€™m not looking for copy-paste code, more design advice, war stories, or tooling suggestions.\n\nThanks a lot, happy to clarify details if useful!",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qem7fs/help_on_how_to_go_about_scraping_faculty/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o014rjv",
          "author": "Persian_Cat_0702",
          "text": "I'd use Langchain if I wanted to scale",
          "score": 2,
          "created_utc": "2026-01-17 01:32:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcehik",
      "title": "Looking for some help.",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qcehik/looking_for_some_help/",
      "author": "nawakilla",
      "created_utc": "2026-01-14 05:04:50",
      "score": 7,
      "num_comments": 8,
      "upvote_ratio": 0.83,
      "text": "My apologies, i honestly don't know if I'm even in the right place. But to put it as short as possible. \n\nI'm looking to \"clone\" a website? I found a site that has a digital user manual for a kind of rare cnc machine. However I'm paranoid that either the user or the site will take it down/ cease to exist (this has happened multiple times in the past. \n\nWhat I'm looking for: i want to be able to save the web pages locally on my computer. Then be able to open it up and use the site as if i would online. The basic site structure is 1 large image (a picture of the components), with maybe a dozen or so clickable parts. When you click it takes you to a page with a few more detailed pictures of the part with text instructions of basic repair and maintenance. \n\nIs it possible to do? I would like a better/ higher quality way to do this instead of screenshoting one by one. Is this isn't web scrapping, can someone tell me what it might be called so i can start googling?",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qcehik/looking_for_some_help/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nzhn1au",
          "author": "HLCYSWAP",
          "text": "replace [example.com](http://example.com) with your intended target and paste this into cmd/bash/terminal:\n\n    wget --mirror \\\n         --page-requisites \\\n         --adjust-extension \\\n         --convert-links \\\n         --no-parent \\\n         --wait=1 \\\n         --random-wait \\\n         --limit-rate=200k \\\n         --tries=3 \\\n         --user-agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\" \\\n         --reject-regex=\"(login|signin|register|signup)\" \\\n         -e robots=off \\\n         -o wget.log \\\n         https://example.com\n\n\\--mirror -  recursive downloading with infinite depth, downloads the entire site\n\n\\--page-requisites - downloads all the assets needed to display pages properly: images, CSS files, JavaScript files, etc\n\n\\--adjust-extension - adds .html extension to files that don't have one but are HTML\n\n\\--convert-links - rewrites all links in downloaded HTML to point to local files instead of the original URLs, so everything works offline\n\n\\--no-parent - doesn't download anything from parent directories. keeps the download limited to the specific path you specify and below",
          "score": 14,
          "created_utc": "2026-01-14 05:16:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nziydie",
              "author": "Infamous_Land_1220",
              "text": "Goat here",
              "score": 3,
              "created_utc": "2026-01-14 12:12:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzipw9q",
          "author": "99ducks",
          "text": "Have you looked to see if the site is available on archive.org? If it is, you don't have to worry about it disappearing.",
          "score": 4,
          "created_utc": "2026-01-14 11:05:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjhtr5",
              "author": "greg-randall",
              "text": "THIS!\n\nGo to [https://archive.org/](https://archive.org/) and paste in the URL in the Wayback Machine, see if what you want is there, if it is great! \n\nIf it isn't fill the URL in the bottom right box 'Save Page Now' https://web.archive.org/. Click on the dozen clickable parts and save those URLs too!",
              "score": 3,
              "created_utc": "2026-01-14 14:10:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzhs29o",
          "author": "pesta007",
          "text": "I've done a similar project before, and yes you are in the right place. What I did is  scrape all the data of the site (the hard part), recreated their front-end (pretty easy really). Then I developed a backend application to serve the data, and connected it to the front end.\n\nAs you can see, it is a lot of work. But I was willing to do it because the site was simple and I was young and passionate.",
          "score": 1,
          "created_utc": "2026-01-14 05:55:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhw0kj",
              "author": "nawakilla",
              "text": "What do you think would be the best approach? I build a handful of computers so I'm not completely tech illiterate. But I've never once done any kind of coding.",
              "score": 2,
              "created_utc": "2026-01-14 06:27:57",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzhwpyd",
              "author": "pesta007",
              "text": "If you are not a web developer, I don't think my approach would work for you. but there is still tools to capture static images of webpages. \n\nCheck out this post\n\nhttps://www.reddit.com/r/DataHoarder/s/ZAwrQ13Ng5",
              "score": 2,
              "created_utc": "2026-01-14 06:33:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzjrbgj",
          "author": "hasdata_com",
          "text": "Check out HTTrack or smth similar. It's a free, old-school software)",
          "score": 1,
          "created_utc": "2026-01-14 15:00:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qb3fth",
      "title": "chromeheadless vs creepJS",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qb3fth/chromeheadless_vs_creepjs/",
      "author": "bluemangodub",
      "created_utc": "2026-01-12 18:48:22",
      "score": 7,
      "num_comments": 14,
      "upvote_ratio": 0.9,
      "text": "Been trying to get chromeheadless better at anti bot detection evasions.\n\n\nCreepJS: https://abrahamjuliot.github.io/creepjs/  however still shows for \"like headless\" checks:\n\n*    noTaskbar: true\n*    noContentIndex: true\n*    noContactsManager: true\n*    noDownlinkMax: true\n\n\n\nNot much info on this that I can find. The \"headless\" check is 0% but this \"like headless\" is at 31%.\n\n\nSimilar note, trying this site: https://fingerprint-scan.com/ which gives me 50% (edit is today showing 55%)  chance of being a  bot.\n\n\nAnyone know any techniques  / things to look into I can do to improve this?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qb3fth/chromeheadless_vs_creepjs/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nz7wyah",
          "author": "thePsychonautDad",
          "text": "I got a score of 25 on https://fingerprint-scan.com/ from my prod scraper nodes, which are not headless.\n\nCan't get a score below 50 on headless.\n\nThe 25 score uses Chrome + CDP + xdotool on mini-PCs running Ubuntu with a HDMI dummy to emulate the screen.\nI have a fleet of a dozen of those mini-PC, with code distributing the jobs.",
          "score": 3,
          "created_utc": "2026-01-12 19:47:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9llv3",
              "author": "joeyx22lm",
              "text": "Why not xvfb?",
              "score": 1,
              "created_utc": "2026-01-13 00:46:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz9np90",
                  "author": "thePsychonautDad",
                  "text": "We tried it and got detected pretty damn fast. Too many fingerprints that Meta detects instantly.\n\nWith CDP it's running smoothly for months without ever an issue.\nBut it only works with a tool like xdotool, click & keypress emulation gets the accounts blocked within days.",
                  "score": 2,
                  "created_utc": "2026-01-13 00:58:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzd7n7q",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-13 15:30:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzdbssg",
                  "author": "joeyx22lm",
                  "text": "Playwright can target a remote CDP endpoint to control a browser.",
                  "score": 1,
                  "created_utc": "2026-01-13 15:49:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzdf43b",
                  "author": "webscraping-ModTeam",
                  "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
                  "score": 1,
                  "created_utc": "2026-01-13 16:04:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzfsgw7",
              "author": "bluemangodub",
              "text": "*reposting as deleted as mentioned a paid for product (reference now removed)*\n\n> Can't get a score below 50 on headless.\n\nsame. But for this project I need headless.  \n\nDo you not have trouble being on linux? Do you keep linux / debian useragent? \n\n\n> Chrome + CDP\n\nWhy CDP? Why not playwright or puppeteer? Any issues from using CDP? In order to get my score to what it currently is, I had to use a CDP command within puppeteer.  I always had it in the back of mind using CDP was detectable?",
              "score": 1,
              "created_utc": "2026-01-13 22:49:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzlrngo",
                  "author": "thePsychonautDad",
                  "text": "Being on Linux isn't an issue.\nThat's the OS I use on my own computer too, it's a legitimate & valid OS. Just use a common distro rather than one meant for servers. We use Ubuntu 24.04\n\nWhy CDP rather than playwright or others: It uses the same chrome a user would use, with a real profile that has its own history & cookies. No automation fingerprints. Why not headless: So the cursor moves for real, clicks for real, the keyboard events are real keypress/keydown. It triggers all the right tracking on the analytics. The cursor moves like a human would move, it's noisy, it's curving and retracing rather that following a straight line. When it types, it has random timing between keypress,, it makes typo that it corrects with backspace from time to time, ... We make it really hard for system to detect us as a bot. With headless you have to emulate the keyboard & mouse events, you don't trigger any of the cursor tracking, you just advertise yourself as a bot.\n\nIt's slower (which is why we have a lot of those computers sharing the load) but it's nearly undetectable and works reliably long term.\n\nWe also make the profiles have work hours, so they \"sleep\" for 8h every day, adding to the act that this is controlled by a human and not a bot.\n\nWe don't even use VPNs, we don't rotate the IPs. It all run a few household internet plans.",
                  "score": 1,
                  "created_utc": "2026-01-14 20:28:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz7vd3r",
          "author": "Twitty-slapping",
          "text": "I don't know, butI  am interested in the process can you keep updating us",
          "score": 1,
          "created_utc": "2026-01-12 19:39:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q9q7po",
      "title": "can someone tell me if i can scrape chatgpt with 'search' on at scale",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q9q7po/can_someone_tell_me_if_i_can_scrape_chatgpt_with/",
      "author": "Specialist_Force4591",
      "created_utc": "2026-01-11 04:48:34",
      "score": 5,
      "num_comments": 16,
      "upvote_ratio": 0.78,
      "text": "this is for my research purpose, even though I tried to do it for small set of queries (20) , most of the queries do not actually do search and just answer from training data. only one response out of 20 actually did search and gave a good answer.\n\nactual scale - 100k queries (enough for a dataset) for each model , can't use api \n\nonly if someone can hint me what should I do, it would be enough and a great help",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q9q7po/can_someone_tell_me_if_i_can_scrape_chatgpt_with/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nyy377w",
          "author": "Azuriteh",
          "text": "Nope, it won't work, it's not built for that",
          "score": 5,
          "created_utc": "2026-01-11 09:35:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyy4anq",
              "author": "Specialist_Force4591",
              "text": "was able to do it for 20 something queries before hitting a hard auth wall. trying to get a workaround with proxies (i am very new to this)",
              "score": 1,
              "created_utc": "2026-01-11 09:45:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyya1ke",
                  "author": "Specialist_Force4591",
                  "text": "nop, not working. proxies dont work for playwright i guess",
                  "score": 1,
                  "created_utc": "2026-01-11 10:38:56",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyzap4a",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-11 14:58:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzidge",
              "author": "webscraping-ModTeam",
              "text": "ðŸª§ Please review the sub rules ðŸ‘‰",
              "score": 1,
              "created_utc": "2026-01-11 15:37:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz04xhe",
          "author": "Foodforbrain101",
          "text": "Try [HuggingFace's AI Sheets](https://huggingface.co/spaces/aisheets/sheets), they have that exact feature for free, 200 rows at a time.",
          "score": 1,
          "created_utc": "2026-01-11 17:25:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz1lpi1",
              "author": "Specialist_Force4591",
              "text": "cant unerstand what it is? i am looking for response for chatgpt/perplexity/popular models",
              "score": 1,
              "created_utc": "2026-01-11 21:24:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz26f9o",
                  "author": "Foodforbrain101",
                  "text": "So assuming your request is not specifically dependent on the model itself and you care more about having a LLM driven web research agent answer a series of prompts, AI Sheets allows you to:\n\n1. Upload a CSV/Excel/Parquet file containing a table with your data and preferably your prompts that you want to run LLM queries against\n2. Add new columns where you specify a prompt which you can inject content from your current row in curly brackets,\n3. Select a model from a bunch of open source choices for free, many of which have been trained for good agentic capabilities, and\n4. Toggle the web search mode in it.\n\nI believe you can also install the app itself by pulling the github repo and provide your own models and remove the row limit, but that also means setting up the appropriate API keys for the web research functionality and your LLM of choice, which isn't free.\n\nAlso, for ChatGPT, try specifying that the model needs to search the web, use thinking models, and toggle the web tool on for best results.",
                  "score": 1,
                  "created_utc": "2026-01-11 23:03:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz1l7vc",
          "author": "Perfect_Warning_5354",
          "text": "Have you tried the Perplexity API?",
          "score": 1,
          "created_utc": "2026-01-11 21:21:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz1llxy",
              "author": "Specialist_Force4591",
              "text": "reason i am scraping is because i can't use API, only want webUI data which is different",
              "score": 1,
              "created_utc": "2026-01-11 21:23:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz5jjkh",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-12 12:50:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5x9gj",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-12 14:11:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz7nb2c",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-12 19:02:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz8mp6z",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-12 21:47:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qcume9",
      "title": "Built a scraper where crawling/scraping is one XPath expression",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qcume9/built_a_scraper_where_crawlingscraping_is_one/",
      "author": "fourhoarsemen",
      "created_utc": "2026-01-14 18:09:58",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.7,
      "text": "This is wxpath's first public release, and I'd love feedback on the expression syntax, any use cases this might unlock, or anything else. \n\n\n**[wxpath](https://github.com/rodricios/wxpath)** is a declarative web crawler where traversal is expressed directly in XPath. Instead of writing imperative crawl loops, wxpath lets you describe what to follow and what to extract in a single expression (it's async under the hood; results are streamed as theyâ€™re discovered). \n\nBy introducing the `url(...)` operator and the `///` syntax, wxpath's engine can perform deep/recursive web crawling and extraction.\n\nFor example, to build a simple Wikipedia knowledge graph: \n\n\n    import wxpath\n\n    path_expr = \"\"\"\n    url('https://en.wikipedia.org/wiki/Expression_language')\n     ///url(//main//a/@href[starts-with(., '/wiki/') and not(contains(., ':'))])\n     /map{\n        'title': (//span[contains(@class, \"mw-page-title-main\")]/text())[1] ! string(.),\n        'url': string(base-uri(.)),\n        'short_description': //div[contains(@class, 'shortdescription')]/text() ! string(.),\n        'forward_links': //div[@id=\"mw-content-text\"]//a/@href ! string(.)\n     }\n    \"\"\"\n\n    for item in wxpath.wxpath_async_blocking_iter(path_expr, max_depth=1):\n        print(item)\n\n\nOutput:\n\n\n    map{'title': 'Computer language', 'url': 'https://en.wikipedia.org/wiki/Computer_language', 'short_description': 'Formal language for communicating with a computer', 'forward_links': ['/wiki/Formal_language', '/wiki/Communication', ...]}\n    map{'title': 'Advanced Boolean Expression Language', 'url': 'https://en.wikipedia.org/wiki/Advanced_Boolean_Expression_Language', 'short_description': 'Hardware description language and software', 'forward_links': ['/wiki/File:ABEL_HDL_example_SN74162.png', '/wiki/Hardware_description_language', ...]}\n    map{'title': 'Machine-readable medium and data', 'url': 'https://en.wikipedia.org/wiki/Machine_readable', 'short_description': 'Medium capable of storing data in a format readable by a machine', 'forward_links': ['/wiki/File:EAN-13-ISBN-13.svg', '/wiki/ISBN', ...]}\n    ...\n\n---\n\nThe target audience is anyone who: \n\n1. wants to quickly prototype and build web scrapers\n2. familiar with XPath or data selectors\n3. builds datasets (think RAG, data hoarding, etc.)\n4. wants to study link structure of the web (quickly) i.e. web network scientists\n\n---\n\nFor comparison, with Scrapy, you would...\n\n\n    import scrapy\n\n    class QuotesSpider(scrapy.Spider):\n        name = \"quotes\"\n        start_urls = [\n            \"https://quotes.toscrape.com/tag/humor/\",\n        ]\n\n        def parse(self, response):\n            for quote in response.css(\"div.quote\"):\n                yield {\n                    \"author\": quote.xpath(\"span/small/text()\").get(),\n                    \"text\": quote.css(\"span.text::text\").get(),\n                }\n\n            next_page = response.css('li.next a::attr(\"href\")').get()\n            if next_page is not None:\n                yield response.follow(next_page, self.parse)\n\n\nThen from the command line, you would run:\n\n\n    scrapy runspider quotes_spider.py -o quotes.jsonl\n\n\n**wxpath** gives you two options: write directly from a Python script or from the command line.\n\n\n    from wxpath import wxpath_async_blocking_iter \n    from wxpath.hooks import registry, builtin\n\n    path_expr = \"\"\"\n    url('https://quotes.toscrape.com/tag/humor/', follow=//li[@class='next']/a/@href)\n      //div[@class='quote']\n        /map{\n          'author': (./span/small/text())[1],\n          'text': (./span[@class='text']/text())[1]\n          }\n\n\n    registry.register(builtin.JSONLWriter(path='quotes.jsonl'))\n    items = list(wxpath_async_blocking_iter(path_expr, max_depth=3))\n\n\nor from the command line:\n\n\n    wxpath --depth 1 \"\\\n    url('https://quotes.toscrape.com/tag/humor/', follow=//li[@class='next']/a/@href) \\\n      //div[@class='quote'] \\\n        /map{ \\\n          'author': (./span/small/text())[1], \\\n          'text': (./span[@class='text']/text())[1] \\\n          }\" > quotes.jsonl\n\n\n\n---\n\n\n\nGitHub: https://github.com/rodricios/wxpath\n\nPyPI: pip install wxpath",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qcume9/built_a_scraper_where_crawlingscraping_is_one/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qa0se7",
      "title": "Scraping amazon page",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qa0se7/scraping_amazon_page/",
      "author": "Tight-Poet-2442",
      "created_utc": "2026-01-11 14:33:08",
      "score": 3,
      "num_comments": 7,
      "upvote_ratio": 0.67,
      "text": "Iâ€™m fairly new to the web scraping world, and Iâ€™m thinking about doing one of my first projects from scratch. Do you have any solution for scraping Amazon pages?\n\n[https://www.amazon.com/events/wintersale](https://www.amazon.com/events/wintersale)  \n[https://www.amazon.com/deals](https://www.amazon.com/deals)\n\nWith discounts between 70% and 100%.  \nI wonâ€™t deny that I had some help from AI.\n\nIâ€™m using Puppeteer with stealth plugins  \nand data center proxies.\n\nThis Amazon page loads content via AJAX.  \nThe bot scrolls the page, collects deals, clicks buttons if necessary to load more content, and avoids scraping books since my focus is on other promotions with the highest discounts.  \n  \nBut I donâ€™t think itâ€™s very good.  Are there better solutions without Puppeteer?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qa0se7/scraping_amazon_page/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nz0f4f9",
          "author": "abdullah-shaheer",
          "text": "Focus on API reverse engineering and go for the direct requests method. You can use libraries like curl cffi for impersonating browser fingerprints. A lot of ways to do so",
          "score": 4,
          "created_utc": "2026-01-11 18:12:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz04grm",
          "author": "Twitty-slapping",
          "text": "Yes there is a method that is much lighter by using http requests instead. \nBut the challenge is how to make the http requests unrecognizable from what a real browser sends \nYou add some proxies into it and voilÃ  you have a pro scraper",
          "score": 2,
          "created_utc": "2026-01-11 17:22:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz6mi28",
              "author": "Brian1398",
              "text": "Sometimes proxy make it detectable as they are flagged",
              "score": 2,
              "created_utc": "2026-01-12 16:16:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz7w3uo",
                  "author": "Twitty-slapping",
                  "text": "Yes correct, but that's where you can rotate, use sticky or even go ham and use residential  with a sticky connection, and if you are blocked, you rotate and keep doing the same",
                  "score": 1,
                  "created_utc": "2026-01-12 19:43:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz48neh",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-12 06:02:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4wk0x",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-12 09:40:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz5cxza",
          "author": "domharvest",
          "text": "Amazon's ToS explicitly prohibit automated scraping. Pay attention!",
          "score": 1,
          "created_utc": "2026-01-12 12:03:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qddw3x",
      "title": "How do I scrape images from a website with server restrictions?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qddw3x/how_do_i_scrape_images_from_a_website_with_server/",
      "author": "MotivatedMommy",
      "created_utc": "2026-01-15 08:11:50",
      "score": 3,
      "num_comments": 6,
      "upvote_ratio": 0.64,
      "text": "My earlier post got removed when I mentioned a bunch of the steps I've tried because it included names of paid services. I'm going to rephrase and hopefully it will make sense.\n\nThere's a site that I want to scrape an image from. I'm starting with just one image so I don't have to worry about staggering call times. Anyway, when I manually inspect the image element in the browser, and then I click on the image source, I get a \"Referral Denied\" error saying \"you don't have permission to access ____ on this server\". I don't even know how to get the image manually, so I'm not sure how to get it with the scraper.\n\nI've been using a node library that starts with puppet, but I've also been using one that plays wright. Whenever I call \"await fetch()\", I get the error \"network response was not ok\". I've tried changing the user agent, adding extra http headers, and intercepting the request, but I still get the same error. I assume I'm not able to get the image because I'm not calling from that site directly, but since I can see the image on the page, I figure there has to be a way to save it somehow.\n\nI'm new to scraping, so I apologize if this sort of thing was asked before. No matter what I searched for, I couldn't find an answer that worked for me. Any advice is much appreciated ",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qddw3x/how_do_i_scrape_images_from_a_website_with_server/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nzpu8wu",
          "author": "Hour_Analyst_7765",
          "text": "This could possibly be due to multiple reasons.\n\nIt could be as simple as requiring to send a \"Referer\" header in your request to grab the image. Its probably their CDN whitelisted only certain domains that belong to the site. I hope this fixes it for you.\n\nHowever, a more advanced protection I've seen, is a system where the website served images with an one-time token in the URL. Those got consumed while loading the full page in the browser, so even if you send the 'correct' request it would still fail. I verified this by disabling images with uBlock origin, and then manually loading an image. This image loaded once, but after a refresh it failed. This confirmed for me that I could still scrape these, but I would have to load the article page and images from the exact same browser session.\n\nA mild annoyance was that these one-time use tokens meant the URL changes on each request, so you need to find your own labeling system to deduplicate them.",
          "score": 3,
          "created_utc": "2026-01-15 12:32:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqdgnk",
          "author": "domharvest",
          "text": "This is a classic **referrer protection** issue - the server is checking where the request is coming from and blocking direct image access. Here's how to handle it in plain JavaScript\n\nInstead of trying to `fetch()` the image URL directly, use Playwright to screenshot or download it **while you're on the page**:\n\n        // Method 1: Screenshot the image element\n        const imageElement = await page.locator('img[src*=\"whatever\"]');\n        await imageElement.screenshot({ path: 'image.png' });\n        \n        // Method 2: Get image as buffer and save\n        const image = await page.locator('img').first();\n        const buffer = await image.screenshot();\n        await fs.writeFile('image.png', buffer);\n        \n        // Method 3: Use CDP (Chrome DevTools Protocol) to intercept the actual image data\n        await page.route('**/*.{png,jpg,jpeg,gif,webp}', async route => {\n          const response = await route.fetch();\n          const buffer = await response.body();\n          await fs.writeFile('image.png', buffer);\n          await route.continue();\n        });",
          "score": 3,
          "created_utc": "2026-01-15 14:24:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpqmjl",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-15 12:07:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpwf8d",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-15 12:47:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzq6v1j",
          "author": "Coding-Doctor-Omar",
          "text": "Try checking the network tab. Chances are there are requests being made to some API to fetch those images.",
          "score": 1,
          "created_utc": "2026-01-15 13:49:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbqooc",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qbqooc/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-01-13 13:00:51",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 0.81,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levelsâ€”whether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) ðŸŒ±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring ðŸ’°",
      "permalink": "https://reddit.com/r/webscraping/comments/1qbqooc/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nzcoqu8",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-13 13:54:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzcv4cb",
              "author": "webscraping-ModTeam",
              "text": "âš¡ï¸ Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-01-13 14:27:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nze7qd6",
          "author": "psychic_shadow_lugia",
          "text": "Hello all,\n\nI need to get data on about 25000 linkedin profiles. The main thing I want it the education section of each profile (all schools as well as start and end date). But if I can get the full profile info it is even better\n\nI understand scraping linkedin is tricky.\n\nI looked into\n\nscraptable - but it is unable at getting the years\n\nlinkedapi - but the call limit (profile per minute) goes down as you use your credit, which eventually result in about 7 profiles per minute.\n\nI am ok paying for a service as well as coding one myself, and would love some ideas from past experience",
          "score": 1,
          "created_utc": "2026-01-13 18:26:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwmcwy",
              "author": "domharvest",
              "text": "LinkedIN ToS",
              "score": 1,
              "created_utc": "2026-01-16 11:59:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzsylu2",
          "author": "OtherwiseGroup3162",
          "text": "Web Scraping TOS\n\nin a site's terms of service, it says no web scraping. \n\nWould that mean both browser based RPA and Back end HTTP requests that were reverse engineered be included?",
          "score": 1,
          "created_utc": "2026-01-15 21:31:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfm54r",
      "title": "pypecdp - a fully async python driver for chrome using pipes",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qfm54r/pypecdp_a_fully_async_python_driver_for_chrome/",
      "author": "sohaib0717",
      "created_utc": "2026-01-17 19:17:57",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.81,
      "text": "Hey everyone. I built a fully asynchronous chrome driver in Python using POSIX pipes. Instead of websockets, it uses file descriptors to connect to the browser.\n\n* Directly connects and controls the browser, no middleware\n* 100% asynchronous, nothing gets blocked\n* Built completely using built-in Python asyncio \n   * Except one `deprecated` dependency for python-cdp modules\n* Best for running multiple browsers on same machine\n* No risk of zombie chromes if code crashes\n* Easy customization via class inheritance\n* No automation signatures as there is no framework in between\n\nCurrently limited to POSIX based systems only (Linux/Mac).\n\n\n\nBug reports, feature requests and contributions are welcome!\n\n\n\n[https://github.com/sohaib17/pypecdp](https://github.com/sohaib17/pypecdp)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qfm54r/pypecdp_a_fully_async_python_driver_for_chrome/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    }
  ]
}