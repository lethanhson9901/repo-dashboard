{
  "metadata": {
    "last_updated": "2026-01-22 08:58:07",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 12,
    "total_comments": 37,
    "file_size_bytes": 51840
  },
  "items": [
    {
      "id": "1qfm54r",
      "title": "pypecdp - a fully async python driver for chrome using pipes",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qfm54r/pypecdp_a_fully_async_python_driver_for_chrome/",
      "author": "sohaib0717",
      "created_utc": "2026-01-17 19:17:57",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 0.94,
      "text": "Hey everyone. I built a fully asynchronous chrome driver in Python using POSIX pipes. Instead of websockets, it uses file descriptors to connect to the browser using Chrome Dev Protocol.\n\n* Directly connects and controls the browser over CDP, no middleware\n* 100% asynchronous, nothing gets blocked\n* Built completely using built-in Python asyncio \n   * Except one `deprecated` dependency for python-cdp modules\n* Best for running multiple browsers on same machine\n* No risk of zombie chromes if code crashes\n* Easy customization via class inheritance\n* No automation signatures as there is no framework in between\n\nCurrently limited to POSIX based systems only (Linux/Mac).\n\n\n\nBug reports, feature requests and contributions are welcome!\n\n\n\n[https://github.com/sohaib17/pypecdp](https://github.com/sohaib17/pypecdp)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qfm54r/pypecdp_a_fully_async_python_driver_for_chrome/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qem7fs",
      "title": "Help on how to go about scraping faculty directory profiles",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qem7fs/help_on_how_to_go_about_scraping_faculty/",
      "author": "gatherer_benefactor",
      "created_utc": "2026-01-16 17:24:19",
      "score": 12,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nIâ€™m working on a research project that requires building a large-scale dataset of faculty profiles from 200 to 250 business schools worldwide. For each school, I need to collect faculty-level data such as: name, title or role, department, short bio or research interests, sometimes email, CV links, publications. The aim is to systematically scrap faculty directories across many heterogeneous university websites. My current setup is like this: Python, Selenium, BeautifulSoup, MongoDB for storage (timestamped entries to allow longitudinal tracking), one scraper per university (100 already written. I do this with the following workflow: manually inspect the faculty directory, write Selenium logic to collect profile URLs, visit each profile and extract fields with BeautifulSoup and then store the data in mongodb. \n\nThis works, but clearly does not scale well to 200 sites, especially long-term maintenance when sites change structure. What Iâ€™m unsure about and looking for advice on is the architecture for automation. Is â€œone scraper per siteâ€ inevitable at this scale? Any recommendations for organizing scrapers so maintenance doesnâ€™t become a nightmare? What are your toughts or experiences using LLMs to analyze a directory HTML, suggest Selenium actions (pagination, buttons), infer selectors?\n\nBasically my question is what you would do differently if you had to do this again for an academic project with transparency/reproducibility constraints, how would you approach it? Iâ€™m not looking for copy-paste code, more design advice, war stories, or tooling suggestions.\n\nThanks a lot, happy to clarify details if useful!",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qem7fs/help_on_how_to_go_about_scraping_faculty/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o014rjv",
          "author": "Persian_Cat_0702",
          "text": "I'd use Langchain if I wanted to scale",
          "score": 2,
          "created_utc": "2026-01-17 01:32:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ijfyr",
              "author": "gatherer_benefactor",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-19 17:53:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qhcirb",
      "title": "What tool can I use to scrape this website?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qhcirb/what_tool_can_i_use_to_scrape_this_website/",
      "author": "BBQMosquitos",
      "created_utc": "2026-01-19 18:35:42",
      "score": 11,
      "num_comments": 25,
      "upvote_ratio": 1.0,
      "text": "My current resources are not working and put a few browser based scrapers but they don't seem to paginate. \n\n  \nNeed to scrape all 101 pages with company name, email, phone number, website, description, that is currently hiding under the green arrow on the right.\n\n\n\n[https://www.eura-relocation.com/membership/our-members?page=0](https://www.eura-relocation.com/membership/our-members?page=0)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qhcirb/what_tool_can_i_use_to_scrape_this_website/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o0iwjnl",
          "author": "albert_in_vine",
          "text": "You don't need to click the green arrow to access email. It's all available in the page source. Just use the CSS selector below to retrieve the email.\n\n`email = soup.select_one('div[class=\"field field-node--field-email field--name-field-email field--type-email field--label-hidden field__item\"] a').text.strip()`  \n`return email`\n\n    emails = [email.text.strip() for email in soup.\n    select\n    ('div[class=\"field field-node--field-email field--name-field-email field--type-email field--label-hidden field__item\"] a')]\n    return emails\n\nAbove is for one email to get all the emails\n\nRegarding web browsers, I am uncertain about that, but a simple pagination loop will retrieve each detail.\n\n    def pages_lists():\n    Â  Â  pages = [f\"https://www.eura-relocation.com/membership/our-members?page={page}\" for page    in range (1, 102)]\n    return pages",
          "score": 2,
          "created_utc": "2026-01-19 18:51:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ix4vi",
              "author": "BBQMosquitos",
              "text": "Iâ€™m not a programmer so trying to find a tool that will do it.",
              "score": 1,
              "created_utc": "2026-01-19 18:53:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0jqrhw",
                  "author": "unteth",
                  "text": "Here's a CSV containing the scraped data: https://filebin.net/08qzzv8n63l2lw7f. File expires in six days.",
                  "score": 7,
                  "created_utc": "2026-01-19 21:10:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0m00u2",
          "author": "node77",
          "text": "Import scrappy",
          "score": 2,
          "created_utc": "2026-01-20 04:26:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mrpgb",
              "author": "BBQMosquitos",
              "text": "Is there a browser addon or site? \n\nDidn't see much on google. exactly as import scrappy, just scrappy.",
              "score": 1,
              "created_utc": "2026-01-20 08:01:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0jimkw",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-19 20:32:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jps2c",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 2,
              "created_utc": "2026-01-19 21:06:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0kq85r",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 2,
                  "created_utc": "2026-01-20 00:12:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0mw0jb",
                  "author": "webscraping-ModTeam",
                  "text": "ðŸª§ Please review the sub rules ðŸ‘‰",
                  "score": 2,
                  "created_utc": "2026-01-20 08:41:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0mvztm",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-20 08:41:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mfdbt",
          "author": "THenrich",
          "text": "chatgpt can do it. Just prompt it like this:\n\nThere are several green downarrows in this page that show the email address when the green arrow is clicked. Click on each and extract the email address. [https://www.eura-relocation.com/membership/our-members?page=0](https://www.eura-relocation.com/membership/our-members?page=0). Do this for all the pages",
          "score": -1,
          "created_utc": "2026-01-20 06:15:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mtwgb",
              "author": "BBQMosquitos",
              "text": "I think chatgpt would need to be prompted many times and it would create many batches. \n\nWere you able to do so in one go?",
              "score": 1,
              "created_utc": "2026-01-20 08:21:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0ozdhx",
                  "author": "THenrich",
                  "text": "I got the first page and part of the second page. It asked me if I wanted more. I didn't try.   \nMaybe a better prompt would work, telling it how many pages there are. It seems to be a trial and error to find to find the proper prompt.",
                  "score": 1,
                  "created_utc": "2026-01-20 16:46:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qf8fvt",
      "title": "Blocked by Cloudflare despite using curl_cffi",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qf8fvt/blocked_by_cloudflare_despite_using_curl_cffi/",
      "author": "Coding-Doctor-Omar",
      "created_utc": "2026-01-17 08:59:39",
      "score": 7,
      "num_comments": 11,
      "upvote_ratio": 0.74,
      "text": "EDIT: IT FINALLY WORKED! I just had to add the content-type, origin, and referer headers.\n\nPlease help me access this API efficiently.\n\nI am trying to access this API:\n\nhttps://multichain-api.birdeye.so/solana/v3/gems\n\nI am using impersonate and the correct payload for the post request, but I keep getting 403 status code.\n\nThe only way I was able to get the data was use a Python browser automation library, go to the normal web page, and intercept this API's response using a handler (essentially automating the network tab inspection using Python), but this method is very inefficient. Below is my curl_cffi code.\n\n\n```\nfrom curl_cffi import Session\n\n\napi_url = \"https://multichain-api.birdeye.so/solana/v3/gems\"\npayload = {\"limit\":100,\"offset\":0,\"filters\":[],\"shown_time_frame\":\"4h\",\"type\":\"trending\",\"sort_by\":\"price\",\"sort_type\":\"desc\"}\n\nwith Session(impersonate=\"edge\") as session:\n    session.get(\"https://birdeye.so/solana/find-gems\")\n    res = session.post(api_url, data=payload)\n    print(res.status_code)\n```\n\nOutput:\n\n```\n403\n```",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qf8fvt/blocked_by_cloudflare_despite_using_curl_cffi/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o034ekq",
          "author": "expiredUserAddress",
          "text": "I see you've no proxy in use. Use a proxy everytime you're scrapping something",
          "score": 2,
          "created_utc": "2026-01-17 10:55:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o035s55",
              "author": "Coding-Doctor-Omar",
              "text": "I actually have just gotten it to work. The issue was simpler than I thought. I had to provide content-type, origin, and referer values in my headers, in addition to the default headers of impersonate.",
              "score": 3,
              "created_utc": "2026-01-17 11:07:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o03dpxf",
          "author": "abdullah-shaheer",
          "text": "If it's in the json format, then you need to set content header to be in json; I don't remember the exact header, you can search",
          "score": 2,
          "created_utc": "2026-01-17 12:16:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o035mw0",
          "author": "Coding-Doctor-Omar",
          "text": "It turns out I had to add some extra headers. Here is the working code:\n\n```\nfrom curl_cffi import Session\n\n\napi_url = \"https://multichain-api.birdeye.so/solana/v3/gems\"\npayload = {\"limit\":100,\"offset\":0,\"filters\":[],\"shown_time_frame\":\"4h\",\"type\":\"trending\",\"sort_by\":\"price\",\"sort_type\":\"desc\"}\n\nheaders = {\n    \"content-type\": \"application/json\",\n    \"origin\": \"https://birdeye.so\",\n    \"referer\": \"https://birdeye.so/\"\n}\n\nwith Session(impersonate=\"edge\", headers=headers) as session:\n    res = session.post(api_url, json=payload)\n    print(res.status_code)\n```\n\nOutput:\n\n```\n200\n```",
          "score": 2,
          "created_utc": "2026-01-17 11:06:28",
          "is_submitter": true,
          "replies": [
            {
              "id": "o03du94",
              "author": "abdullah-shaheer",
              "text": "Yes this is actually. And if the content is in html form, you will have the need to use the content type header accordingly.",
              "score": 2,
              "created_utc": "2026-01-17 12:17:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0b106q",
          "author": "BeforeICry",
          "text": "Cloudflare typically renders the turnstile captcha even for legit browser requests. That's more like a feature of your target. In these cases, you have to resort to browser + captcha solving.",
          "score": 1,
          "created_utc": "2026-01-18 15:43:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0b5w5l",
              "author": "Coding-Doctor-Omar",
              "text": "I eventually got it to work by providing the content-type, origin, and referer values in the headers, in addition to the default headers provided by impersonate.",
              "score": 2,
              "created_utc": "2026-01-18 16:06:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ckfum",
          "author": "Alternative-842",
          "text": "yo man i had the same issue, Cloudflare just blocks normal requests if u dont send all the headers like content-type origin n referer, even if ur payload is right. i ended up using a headless browser too, way slow tho. u might try adding all the headers exactly like the site does n maybe rotate user agents, that helped me a bit. sometimes curl alone just dont cut it lol",
          "score": 1,
          "created_utc": "2026-01-18 20:05:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0epifb",
              "author": "Coding-Doctor-Omar",
              "text": "It turns out I had to add some extra headers, in addition to the normal impersonate. Here is the working code (luckily still works with curl_cffi alone, without a browser):\n\n```\nfrom curl_cffi import Session\n\n\napi_url = \"https://multichain-api.birdeye.so/solana/v3/gems\"\npayload = {\"limit\":100,\"offset\":0,\"filters\":[],\"shown_time_frame\":\"4h\",\"type\":\"trending\",\"sort_by\":\"price\",\"sort_type\":\"desc\"}\n\nheaders = {\n    \"content-type\": \"application/json\",\n    \"origin\": \"https://birdeye.so\",\n    \"referer\": \"https://birdeye.so/\"\n}\n\nwith Session(impersonate=\"edge\", headers=headers) as session:\n    res = session.post(api_url, json=payload)\n    print(res.status_code)\n```\n\nOutput:\n\n```\n200\n```",
              "score": 1,
              "created_utc": "2026-01-19 02:45:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0npdlg",
          "author": "pablofdezr",
          "text": "Thanks for the tip, so you're bypassing turnstile just using curl\\_cffi and normal headers you intercepted? Nice find, although as someone here said, use proxies or you can get blocked even for fair use",
          "score": 1,
          "created_utc": "2026-01-20 12:49:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfigyu",
      "title": "Open Source Captcha to test scraping methods against",
      "subreddit": "webscraping",
      "url": "https://github.com/WebDecoy/FCaptcha",
      "author": "cport1",
      "created_utc": "2026-01-17 17:00:15",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.78,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Bot detection ðŸ¤–",
      "permalink": "https://reddit.com/r/webscraping/comments/1qfigyu/open_source_captcha_to_test_scraping_methods/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qhynfr",
      "title": "Scaling 100+ Vendor Dashboards Without APIs Is a Nightmare",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qhynfr/scaling_100_vendor_dashboards_without_apis_is_a/",
      "author": "Sufficient-Owl-9737",
      "created_utc": "2026-01-20 11:28:09",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Half these dashboards, AWS console reboots, GCP inventory, Azure billing, have no public APIs. You end up clicking manually or writing fragile Selenium scripts that die on CAPTCHAs, timeouts, or the slightest React tweak. My selectors got wiped twice in one month. Headless Puppeteer handles around ten portals fine. Push it to fifty and localStorage breaks, IP bans hit after a couple of hours, and random modals destroy everything. Playwright lasts longer but scripting human-like flows, dropdown chains and confirm dialogs, feels endless. Has anyone scaled this to a hundred plus portals without losing their mind? Custom UI wrappers pretending to be APIs? Tools that survive vendor UI overhauls and lock-ins?",
      "is_original_content": false,
      "link_flair_text": "Scaling up ðŸš€",
      "permalink": "https://reddit.com/r/webscraping/comments/1qhynfr/scaling_100_vendor_dashboards_without_apis_is_a/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o0ngy5j",
          "author": "Comfortable_Clue5430",
          "text": "For 100+ dashboards, using isolated browser profiles, splitting portals across machines, and replaying interactions with slight randomness helps keep sessions stable and reduces failures.",
          "score": 3,
          "created_utc": "2026-01-20 11:49:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0o9kbs",
          "author": "jinef_john",
          "text": "I think that isn't a scraping(scaling) problem, it's more of a design problem. Maintenance shouldn't be a problem if you make good decisions on architecture and design patterns ( i.e logs, metrics, isolating auth/bootstrap from data retrieval etc). And a lot of times, UI automations should really be a last resort.",
          "score": 3,
          "created_utc": "2026-01-20 14:43:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qeu3ov",
      "title": "FBref Cloudflare Turnstile block. I need to bypass, please help me",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qeu3ov/fbref_cloudflare_turnstile_block_i_need_to_bypass/",
      "author": "Goldrake_Z",
      "created_utc": "2026-01-16 22:20:36",
      "score": 6,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "Hi,\n\nI'm building a Python bot to scrape historical Serie A data from [fbref.com](http://fbref.com) (schedules/fixtures + match stats/lineups). Works perfectly on local PC (Windows) and on my Ubuntu VPS until today â€“ now persistent Cloudflare 403 \"Just a moment...\" onÂ `/en/comps/11/schedule/Serie-A-Stats-Scores-and-Fixtures`.Â **Now even local Windows PC affected**Â (same error).\n\n**Already tried:**\n\n* cloudscraper + cookies/UA rotate + delays â†’ 403/429.\n* Playwright stealth (headless=new, TZ Europe/Rome, mouse sim) + Turnstile iframe/checkbox clicks â†’ title stuck after 60s, no solve (screenshot dumped).\n* FreeProxy IT (e.g. [65.109.176.217:80](http://65.109.176.217:80), rotate 10) â†’ proxies connect but challenge fails.\n* Full Chrome122 headers/Sec-Ch-Ua/it-IT locale â†’ no.\n\n\n\nAlso yesterday sofascore also \"banned\" me and I don't know why. I was using api for lineup and everything was working perfectly, I go to sleep and the next day I find myself banned (403)\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qeu3ov/fbref_cloudflare_turnstile_block_i_need_to_bypass/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o02thfq",
          "author": "Even_Refrigerator233",
          "text": "never use free proxies",
          "score": 4,
          "created_utc": "2026-01-17 09:12:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03hn23",
              "author": "Goldrake_Z",
              "text": "the problem is that it does it on my PC too, I guess it's a new cloudflare security system (on FBref)",
              "score": 1,
              "created_utc": "2026-01-17 12:46:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o053d0f",
          "author": "Afraid-Solid-7239",
          "text": "Pydoll \n\nPersonally using it for a turnstile api",
          "score": 3,
          "created_utc": "2026-01-17 17:46:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0lnk7s",
              "author": "Goldrake_Z",
              "text": "i'll try it. thanks",
              "score": 1,
              "created_utc": "2026-01-20 03:14:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00712j",
          "author": "Goldrake_Z",
          "text": "The full error  \n  \n  \n\\[16/01/26 23:21:21\\] INFO HTTP GET attempt 1/4 -> [https://fbref.com/en/comps/11/schedule/Serie-A-Stats-Scores-and-Fixtures](https://fbref.com/en/comps/11/schedule/Serie-A-Stats-Scores-and-Fixtures)\n\n\\[16/01/26 23:21:21\\] WARNING HTTP GET exception on attempt 1 for https://fbref.com/en/comps/11/schedule/Serie-A-Stats-Scores-and-Fixtures: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/11/schedule/Serie-A-Stats-Scores-and-Fixtures\n\n\\[16/01/26 23:21:21\\] INFO Response Status: 403\n\n\\[16/01/26 23:21:21\\] INFO Response Headers: {'Date': 'Fri, 16 Jan 2026 22:21:21 GMT', 'Content-Type': 'text/html; charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Connection': 'close', 'accept-ch': 'Sec-CH-UA-Bitness, Sec-CH-UA-Arch, Sec-CH-UA-Full-Version, Sec-CH-UA-Mobile, Sec-CH-UA-Model, Sec-CH-UA-Platform-Version, Sec-CH-UA-Full-Version-List, Sec-CH-UA-Platform, Sec-CH-UA, UA-Bitness, UA-Arch, UA-Full-Version, UA-Mobile, UA-Model, UA-Platform-Version, UA-Platform, UA', 'cf-mitigated': 'challenge', 'critical-ch': 'Sec-CH-UA-Bitness, Sec-CH-UA-Arch, Sec-CH-UA-Full-Version, Sec-CH-UA-Mobile, Sec-CH-UA-Model, Sec-CH-UA-Platform-Version, Sec-CH-UA-Full-Version-List, Sec-CH-UA-Platform, Sec-CH-UA, UA-Bitness, UA-Arch, UA-Full-Version, UA-Mobile, UA-Model, UA-Platform-Version, UA-Platform, UA', 'cross-origin-embedder-policy': 'require-corp', 'cross-origin-opener-policy': 'same-origin', 'cross-origin-resource-policy': 'same-origin', 'origin-agent-cluster': '?1', 'permissions-policy': 'accelerometer=(),browsing-topics=(),camera=(),clipboard-read=(),clipboard-write=(),geolocation=(),gyroscope=(),hid=(),interest-cohort=(),magnetometer=(),microphone=(),payment=(),publickey-credentials-get=(),screen-wake-lock=(),serial=(),sync-xhr=(),usb=()', 'referrer-policy': 'same-origin', 'server-timing': 'chlray;desc=\"9bf10160fef75602\", cfOrigin;dur=0,cfEdge;dur=7', 'x-content-type-options': 'nosniff', 'x-frame-options': 'SAMEORIGIN', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Set-Cookie': '\\_\\_cf\\_bm=c1Sa1AyKEq5Izb0qU\\_3R0LY7ZrClNMfXKlwISxno744-1768602081-1.0.1.1-5KzTMVFoBk3zIYyfFJqD\\_QaaclBIldzB.euuFoPPrOiZ5Zke0Yq\\_wJWEau82zxdYFz\\_i1v6v3qAqvYOaF674EYMZBSY.k4diBGmarmEmIXM; path=/; expires=Fri, 16-Jan-26 22:51:21 GMT; [domain=.fbref.com](http://domain=.fbref.com); HttpOnly; Secure; SameSite=None', 'Vary': 'Accept-Encoding', 'Server': 'cloudflare', 'CF-RAY': '9bf10160fef75602-MXP', 'Content-Encoding': 'gzip'}\n\n\\[16/01/26 23:21:21\\] INFO Response Body Snippet: <!DOCTYPE html><html lang=\"en-US\"><head><title>Just a moment...</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"><meta name=\"robots\" content=\"noindex,nofollow\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"><style>\\*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,\"Segoe \n\nUI\",Roboto,\"Helve\n\n\\[16/01/26 23:21:21\\] WARNING âš ï¸ CLOUDFLARE CHALLENGE (Anti-Bot) per https://fbref.com/en/comps/11/schedule/Serie-A-Stats-Scores-and-Fixtures. Attendo 374s fino alle 23:27:35 prim\n\na di riprovare (1/4). \\[Stats: 1 total bans, 1 Cloudflare, 1 consecutive\\]",
          "score": 1,
          "created_utc": "2026-01-16 22:21:45",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o0adkev",
          "author": "Menxii",
          "text": "Did they ban your IP ? Does the website work when you use it normally ?",
          "score": 1,
          "created_utc": "2026-01-18 13:36:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0lnddm",
              "author": "Goldrake_Z",
              "text": "They didn't actually ban me, but they put me in a Cloudflare loop that I can't get out of (even if you click the checkbox). They \"flagged\" me (it's a terminal VPS, so I'm going by gut feeling).",
              "score": 1,
              "created_utc": "2026-01-20 03:13:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0di94r",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-18 22:54:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0fmnn4",
              "author": "webscraping-ModTeam",
              "text": "ðŸª§ Please review the sub rules ðŸ‘‰",
              "score": 1,
              "created_utc": "2026-01-19 06:25:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0fomoh",
          "author": "skyline222",
          "text": "Thatâ€™s cloudflare WAF some captcha solving services can help you gen the cf clearance cookie you need",
          "score": 1,
          "created_utc": "2026-01-19 06:42:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0lnhn6",
              "author": "Goldrake_Z",
              "text": "Even if I'm already \"flagged\" and I get into the cloudflare loop?",
              "score": 1,
              "created_utc": "2026-01-20 03:14:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0on6o4",
                  "author": "skyline222",
                  "text": "yeah but you will need to have user:password proxies, as the cf clearance cookie is bound to the IP.",
                  "score": 2,
                  "created_utc": "2026-01-20 15:50:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qgtv79",
      "title": "Unable to create Reddit app for PRAW, stuck on policy message",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qgtv79/unable_to_create_reddit_app_for_praw_stuck_on/",
      "author": "Glum_Masterpiece_592",
      "created_utc": "2026-01-19 04:04:59",
      "score": 4,
      "num_comments": 3,
      "upvote_ratio": 0.84,
      "text": "Iâ€™m trying to collect Reddit posts on a specific topic for **research purposes** using **PRAW**, but Iâ€™m unable to create a Reddit app to get the client ID/secret.\n\nDuring app creation, Reddit keeps showing a message asking me to read the full policies and wonâ€™t let me proceed (similar to the attached screenshot). Iâ€™ve read the policies but canâ€™t figure out whatâ€™s blocking the app creation.\n\nQuestions:\n\n* Has anyone encountered this issue recently?\n* Is there a specific requirement I might be missing during app setup?\n* If PRAW/app creation isnâ€™t possible, what are **recommended alternatives** for collecting Reddit post data (within Redditâ€™s rules)?\n\nAny pointers would be appreciated. Thanks!\n\nhttps://preview.redd.it/aybix754d8eg1.png?width=1590&format=png&auto=webp&s=8acd9fc278d78c64024673401053063846ebaf9e\n\n",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qgtv79/unable_to_create_reddit_app_for_praw_stuck_on/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o0g8z3f",
          "author": "abdullah-shaheer",
          "text": "I guess from Nov onwards, you can't create apps freely, you have to request to them via an application form for the API access",
          "score": 2,
          "created_utc": "2026-01-19 09:47:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qtqd9",
          "author": "0xMassii",
          "text": "Just scrape Reddit like a real men, you don't need of PRAW",
          "score": 1,
          "created_utc": "2026-01-20 21:50:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhfe65",
      "title": "scraping whatsapp web",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qhfe65/scraping_whatsapp_web/",
      "author": "Sufficient-Bad-532",
      "created_utc": "2026-01-19 20:16:59",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.81,
      "text": "i tried scraping all the phone numbers im realated too, created a quick python script using playwrite, it works like this click chat -> click header -> click search member -> get all members in chat -> go to a different chat.  \nany way after like 5 chats i got banned for 24 hours. \n\nmy question is how do i bypass this ? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qhfe65/scraping_whatsapp_web/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o0p98po",
          "author": "Obvious-Bet-1338",
          "text": "What did you already tried to bypass it ?",
          "score": 1,
          "created_utc": "2026-01-20 17:32:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qh9gkm",
      "title": "Web Scraping API or custom web scraping?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qh9gkm/web_scraping_api_or_custom_web_scraping/",
      "author": "GlebarioS",
      "created_utc": "2026-01-19 16:49:52",
      "score": 3,
      "num_comments": 19,
      "upvote_ratio": 0.67,
      "text": "Hello everyone!\n\nI am new to your community and web scraping in general. I have 6 years of experience in web application development but have never encountered the topic of web scraping. I became interested in this topic when I was planning to implement a pet project for myself to track prices for products that I would like to purchase in the future. That is, the idea was that I would give the application a link to a product from any online store and it, in turn, would constantly extract data from the page and check if the price had changed. I realized that I needed web scraping and I immediately created a simple web scraping on node.js using playwright without a proxy. It coped with simple pages, but if I had already tested serious marketplaces like alibaba, I was immediately blocked. I tried with a proxy but the same thing happened. Then I came across web scraping API (unfortunately I can't remember which service I used) and it worked great! But it is damn expensive. I calculated that if I use web scraping API for my application and the application will scrape each added product every 8 hours for a month, then I will pay $1 per product. That is, if I added 20 products that will be tracked, then I will pay web scraping API +- $20. This is very expensive because I have a couple of dozen different products that I would like to submit (I am a Lego fan, so I have a lot of sets that I want to buy ðŸ˜„)\n\nAs a result, I thought about writing my own web scraping that would be simpler than all other web scraping APIs but at least cheaper. But I have no idea if it will be cheaper at all.\n\nCan someone with experience tell me if it will be cheaper?\n\nMobile/residential or data center proxies?\n\nI have seen many recommendations for web scraping in python, can I still write in node?\n\nIn which direction should I look?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qh9gkm/web_scraping_api_or_custom_web_scraping/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o0ib0wz",
          "author": "bluemangodub",
          "text": "> As a result, I thought about writing my own web scraping that would be simpler than all other web scraping APIs but at least cheaper. But I have no idea if it will be cheaper at all. Can someone with experience tell me if it will be cheaper?\n\nfirstly you need to get your browser solution working, you said you are being blocked.\n\nWill your solution be better?  Cheaper? You are asking \"I am going to build something will be be better and cheaper than something else\". No one knows.\n\nBut you have a LOT of work todo before you can sell this service. Hitting a site once is one thing, running a service hitting 100s / 1000s of services you have captcha issues, anti bot protections etc etc.\n\nGet your system working first before you try to run and sell something that isn't working :-)\n\ngood luck",
          "score": 3,
          "created_utc": "2026-01-19 17:15:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0igc6f",
              "author": "GlebarioS",
              "text": "Thanks for your comment! The thing is that I don't have a goal to create web scraping for sale as a service) I would like to create a minimal but stable version for my personal use. I don't need additional options for caching, LLM, parsing, and so on. The Web Scraping API services that I use do what I want, but there are a bunch of other features that I don't need and I wouldn't want to pay for them. Therefore, I would like to create my own stable but minimal web scraping",
              "score": 1,
              "created_utc": "2026-01-19 17:39:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ias1h",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-01-19 17:14:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0if84k",
              "author": "GlebarioS",
              "text": "Thanks for the advice! Can you suggest which tools should be used for fingerprinting on node.js?",
              "score": 1,
              "created_utc": "2026-01-19 17:34:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0jkq1d",
          "author": "LT823",
          "text": "I had similiar problems like you.\n\nI used Patchright (you can find on github) its line a better browser for scraping instead of using playwright or pupeteer.\n\nThen i conntected every request with a residential Proxy IP (cost me 5$ per month for 50 ips)\n\nAnd also make sure to try without headless - then try headless activated. Mostly it worked for me if you run it without headless.",
          "score": 2,
          "created_utc": "2026-01-19 20:42:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ylrs4",
              "author": "Krokzter",
              "text": "Just to add to this, Patchright doesn't patch headless. You'd need another solution like playwright-stealth or whichever one gets more regular updates",
              "score": 1,
              "created_utc": "2026-01-22 00:44:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ig0ep",
          "author": "nirvanist",
          "text": "Try this one, should cover most of use cases\nhttps://page-replica.com/structured/live-demo",
          "score": 1,
          "created_utc": "2026-01-19 17:38:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0iucha",
          "author": "Traditional-Set-6548",
          "text": "Go get on Coursera and take the Foundation of AI and Machine learning certification from Microsoft. You don't need to do the entire thing or any of it for that matter but they have a section in like the second part of the cert that will give you the basic code and set up for web scraping. You will have all you need to know for the most part to get a basic one going today.",
          "score": 1,
          "created_utc": "2026-01-19 18:41:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kbkyb",
          "author": "Kbot__",
          "text": "The issue you're hitting on Alibaba is proxy quality. Datacenter proxies get blocked immediately - you need \\*\\*residential\\*\\* with a large rotating pool. Mobile/residential are both good, but residential is usually more cost-effective for your use case.\n\n  \nCost-wise: residential proxies run $5-15/GB. For 20 products scraped every 8 hours, you're looking at maybe 30-50MB/day, so roughly \\*\\*$3-5/month total\\*\\*. Pay-as-you-go bandwidth pricing beats per-request, so yes - building your own will be cheaper than $20/month.\n\n  \n\\*\\*Node.js is totally fine\\*\\* - Python gets recommended a lot but Playwright works great in Node. Stick with what you know.\n\n  \nKey things for Alibaba:\n\n\\- Use residential IPs only (not datacenter)\n\n\\- If you're scraping behind login, stick to 1 account = 1 IP (don't mix IPs for the same session)\n\n\\- Block unnecessary requests (images, fonts, tracking) to keep bandwidth down\n\n\\- Add random delays between requests\n\n  \nAlibaba's bot detection is aggressive, so keep your request patterns looking human.",
          "score": 1,
          "created_utc": "2026-01-19 22:54:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0yns9t",
          "author": "Krokzter",
          "text": "For such a low scraping volume, you might not even need a proxy. Try using a stealth solution like nodriver, pydoll, playwright-stealth, etc. and see if that's enough. If you start getting blocked after a few days, then go ahead and try proxies.  \nAs for proxies, datacenter proxies are probably good enough. Even if you get blocked 9 times out of 10 (which it should never actually be this bad), you don't need to optimize for volume at that scale, so you can afford to get blocked as long as it succeeds often enough. You should at least try datacenter proxies first to see if they are good enough, since they are much cheaper.  \nIt always depends on the sites protection, but keep in mind that for low volume, your real IP will always have a better reputation than any proxy on the market.",
          "score": 1,
          "created_utc": "2026-01-22 00:55:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0j5shz",
          "author": "DigIndependent7488",
          "text": "You should be able to use something like riveter and scrape multiple product pages without any issue, shouldn't have to worry about being blocked or proxy management and costs etc. I think setting up your own DIY solution would be doable though but as others have said it won't be entirely free either ahahaha.",
          "score": 0,
          "created_utc": "2026-01-19 19:32:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qi0hiq",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qi0hiq/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-01-20 13:01:02",
      "score": 3,
      "num_comments": 4,
      "upvote_ratio": 0.81,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levelsâ€”whether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) ðŸŒ±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring ðŸ’°",
      "permalink": "https://reddit.com/r/webscraping/comments/1qi0hiq/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o0uidbt",
          "author": "Training_Hand_1685",
          "text": "1 Comment out of 3\n\nHi! Looking for someone who can help me with this. Paid.\n\nWe take \"quizzes\" online that guides our learning. We have to reach to level 6. So we do many of these questions over and over, passing more and more, increasing our level each time. Goal is to level 6. The questions are helpful to show you what you don't know. So, I like to grab the questions, the correct answers, and the explanation - and test myself over and over.\n\nIssues for web scraping (3 to 4 pics).\n\n1. the explanation is always collapsed... you have to click the explanation button for the explanation text to dropdown/appear. Pic 1 in this comment has a closed explanation.\n\n(See other comments. )\n\nhttps://preview.redd.it/i7bvxi2o9peg1.png?width=669&format=png&auto=webp&s=fa5a207dc95ab69c304efcebd83d0c43894ce51a",
          "score": 2,
          "created_utc": "2026-01-21 12:57:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0pfpl9",
          "author": "themoneysystem",
          "text": "Thanks",
          "score": 1,
          "created_utc": "2026-01-20 18:01:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0uihsx",
          "author": "Training_Hand_1685",
          "text": "2 Comment out of 3\n\nPic 2, in this comment, shows the explanation opened.\n\n1. Goal is to constantly use this for multiple chapters across different classes.\n2. Multiple classes use the same quizzing platform. I save the UNIQUE questions+answer+explanation... the questions repeat often so the webscraper would need to check what questions already exist from prior runs. I currently save the questions in google docs (would like to keep it there).\n\nhttps://preview.redd.it/xxzayj42apeg1.png?width=676&format=png&auto=webp&s=bc56c0c8192d0e9e7d0c6b7adb2fa21f022b47e1",
          "score": 1,
          "created_utc": "2026-01-21 12:58:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0uiruj",
          "author": "Training_Hand_1685",
          "text": "https://preview.redd.it/pmgfj5h6apeg1.png?width=1580&format=png&auto=webp&s=6329dd8ca4cc2c450e51441f3f97534d26a698d0\n\n1. 3 comment out of 3 : ------ Pic 3 is an example of how my questions (bolded) are with their answer (below question) and copied explanation from the quizzes. This also shows that you have to scroll within a box within the page.",
          "score": 1,
          "created_utc": "2026-01-21 13:00:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qevknv",
      "title": "Suggestions on dealing with iCloud bans? MITM vs AppStore.",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qevknv/suggestions_on_dealing_with_icloud_bans_mitm_vs/",
      "author": "gotta_cache_em_all",
      "created_utc": "2026-01-16 23:15:15",
      "score": 2,
      "num_comments": 6,
      "upvote_ratio": 0.67,
      "text": "Anyone have any suggestions on this? Itâ€™s a bit annoying trying to watch network requests via MITM for mobile APIs when I keep constantly getting banned by Apple.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qevknv/suggestions_on_dealing_with_icloud_bans_mitm_vs/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o038tc4",
          "author": "You_Cant_Win_This",
          "text": "You (or whatever tool you are using) are doing something wrong. Apple should not be able to detect you intercepting requests or even if they do, that can't be bannable. Now if you are editing them midway..",
          "score": 2,
          "created_utc": "2026-01-17 11:35:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03j6ar",
              "author": "matty_fu",
              "text": "they can detect the request is not genuinely from an Apple device, eg. when the mitm proxy has intercepted & re-transmitted it using its own TLS settings, etc.",
              "score": 2,
              "created_utc": "2026-01-17 12:57:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o01mhaq",
          "author": "matty_fu",
          "text": "Youâ€™re getting banned by Apple for intercepting requests from non-Apple apps?\n\nWhat happens when they ban you, does the iPhone no longer work or you just cant login to your iCloud on the phone?",
          "score": 1,
          "created_utc": "2026-01-17 03:24:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01mr1x",
              "author": "gotta_cache_em_all",
              "text": "So the app in question was Chrono24. The iPhone is fine, I just canâ€™t login to the iCloud account anymore. After getting banned on two different accounts, I gave up. I was curious how people are managing to get around this, because mobile APIs seem to not have as much defense as their web counterparts.",
              "score": 1,
              "created_utc": "2026-01-17 03:26:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o01pv10",
                  "author": "matty_fu",
                  "text": "Does Chrono24 have iCloud integration? Otherwise Iâ€™m not sure how Apple would be able to detect your MITM setup\n\nYou might be able to configure your MITM proxy to intercept only some domains - so you donâ€™t end up intercepting any requests to Apple endpoints, only endpoints owned by the app developer",
                  "score": 3,
                  "created_utc": "2026-01-17 03:46:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mpe2v",
          "author": "Used-Comfortable-726",
          "text": "Give up on MITM for this.",
          "score": 1,
          "created_utc": "2026-01-20 07:40:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}