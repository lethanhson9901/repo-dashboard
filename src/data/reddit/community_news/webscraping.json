{
  "metadata": {
    "last_updated": "2026-01-31 02:49:57",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 11,
    "total_comments": 50,
    "file_size_bytes": 57094
  },
  "items": [
    {
      "id": "1qmd4qm",
      "title": "osn-selenium: An open-source Selenium-based framework",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qmd4qm/osnselenium_an_opensource_seleniumbased_framework/",
      "author": "oddshellnick",
      "created_utc": "2026-01-25 07:57:44",
      "score": 16,
      "num_comments": 3,
      "upvote_ratio": 0.8,
      "text": "Standard Selenium WebDriver implementations often face significant limitations in production environments, primarily due to blocking I/O execution patterns which increase overhead when scaling via threading. Furthermore, the lack of native, typed interfaces for Chrome DevTools Protocol (CDP) domains complicates low-level browser control. Additionally, standard automation signatures are easily identified by advanced anti-bot solutions through browser fingerprint analysis.\n\nTo address these issues, I have developed¬†**osn-selenium**, an asynchronous automation framework built on top of Selenium. Specifically architected for Blink-based browsers (Chrome, Edge, Yandex), it utilizes the¬†Trio¬†library to provide structured concurrency. The framework employs a modular Mixin-based architecture that maintains 99% backward compatibility with standard Selenium calls while exposing advanced control interfaces.\n\n**Core Technical Features:**\n\n* **Structured Concurrency (Trio):**¬†Native integration with the Trio event loop via¬†TrioThreadMixin, enabling efficient concurrent management of multiple browser instances.\n* **Typed CDP Executors:**¬†High-level, typed access to all Chrome DevTools Protocol domains. This allows for real-time network request interception and response manipulation directly from Python.\n* **Advanced Fingerprint Spoofing Engine:**¬†Features a built-in registry of over 200 parameters (Canvas, WebGL, AudioContext, etc.). Detection can be enabled in two lines of code. Supports spoofing via static/random values, static/random noise injection, and dynamic modification of value sequences. Additionally, the registry of parameters can be expanded.\n* **Dedicated dev\\_tools Package:**¬†A module designed for background browser event processing. It features specialized loggers for CDP and fingerprinting activity, alongside advanced request interception handlers.\n* **Full Instance Wrappers:**¬†Custom high-level wrappers for all Selenium objects including WebElements, Alerts, ShadowRoots, etc. These are 100% drop-in compatible with vanilla Selenium logic.\n* **Human-Like Interaction Layer:**¬†Implementation of natural mouse movements using Bezier curves with jitter, smooth scrolling algorithms, and human-like typing simulation.\n\nI am currently expanding the framework's capabilities. Short-term goals include automated parameter aggregation for¬†all flags managers, implementing higher-level logic for¬†Network,¬†Page, and¬†Runtime¬†domains in the¬†dev\\_tools¬†package, refining Human-Like movement patterns, and supporting a hybrid driver interface (both mixins and component-attributes). Support for additional Chromium-based browsers is also underway.\n\nThe long-term roadmap includes support for Gecko-based browsers (Firefox) and developing true internal concurrency for single browser instances using Trio memory channels and direct CDP pipe management. I am looking for technical feedback and contributors to help refine the architecture.\n\nIf you are interested in modernizing your Selenium-based infrastructure, I invite you to explore the [repository](https://github.com/oddshellnick/osn-selenium) and contribute to its development.",
      "is_original_content": false,
      "link_flair_text": "Scaling up üöÄ",
      "permalink": "https://reddit.com/r/webscraping/comments/1qmd4qm/osnselenium_an_opensource_seleniumbased_framework/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1mz7t2",
          "author": "cgoldberg",
          "text": "Using `setup.py` for a new package in 2026 is pretty odd.",
          "score": 1,
          "created_utc": "2026-01-25 16:05:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zk413",
          "author": "kts_2001",
          "text": "Good bro I will test it",
          "score": 1,
          "created_utc": "2026-01-27 09:26:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlewai",
      "title": "I built a CLI that turns websites into real Playwright scrapers",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qlewai/i_built_a_cli_that_turns_websites_into_real/",
      "author": "Acceptable_Grass2812",
      "created_utc": "2026-01-24 05:51:51",
      "score": 14,
      "num_comments": 4,
      "upvote_ratio": 0.71,
      "text": "I built **ScrapeWizard** because using LLMs to write scrapers is slow and expensive ‚Äî you keep generating code, running it, fixing it, and burning API credits.\n\nScrapeWizard does it differently.  \nIt scans the website (DOM, JS, network calls, selectors, pagination) and uses AI **only to generate and fix the scraper code**.  \nThe actual scraping runs locally with Playwright.\n\nSo even if data extraction fails, you still get a full working script with all the site details that you can edit and reuse.\n\nGitHub:  \n[https://github.com/pras-ops/ScrapeWizard](https://github.com/pras-ops/ScrapeWizard)\n\nWould love feedback from people who scrape or automate.",
      "is_original_content": false,
      "link_flair_text": "AI ‚ú®",
      "permalink": "https://reddit.com/r/webscraping/comments/1qlewai/i_built_a_cli_that_turns_websites_into_real/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1mkt89",
          "author": "Habitualcaveman",
          "text": "Web Scraping Copilot for VScode and Scrapy is similar to (re)generate spiders. It guides the LLM to follow a workflow that generates the spiders and creates test fixtures and abstracts the parsing code into something called pageObjects so you get Scrapy spider code you can run anywhere, and just regenerate when the underlying site changes/breaks.",
          "score": 1,
          "created_utc": "2026-01-25 14:58:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uc30g",
              "author": "Acceptable_Grass2812",
              "text": "Web Scraping Copilot for VScode¬†it wroks for static website not js heavy as of now i think",
              "score": 1,
              "created_utc": "2026-01-26 16:21:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ue2ui",
                  "author": "Habitualcaveman",
                  "text": "Yeah you need a browser plugin for the heavy stuff.¬†",
                  "score": 1,
                  "created_utc": "2026-01-26 16:29:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fghkd",
          "author": "_i3urnsy_",
          "text": "Interesting definitely gonna take a look",
          "score": 0,
          "created_utc": "2026-01-24 14:20:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qofvmx",
      "title": "Akamai anti-bot blocking flight search scraping (403/418)",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qofvmx/akamai_antibot_blocking_flight_search_scraping/",
      "author": "Individual-Ship-7587",
      "created_utc": "2026-01-27 14:49:48",
      "score": 9,
      "num_comments": 18,
      "upvote_ratio": 0.77,
      "text": "Hi all,\n\nI‚Äôm attempting to collect public flight search data (routes, dates, mileage pricing) for personal research, at low request rates and without commercial intent.\n\nAirline websites (Azul / LATAM) consistently return 403 and 418 responses, and traffic analysis strongly suggests Akamai Bot Manager / sensor-based protection.\n\n# Environment & attempts so far\n\n* Python and Go\n* Multiple HTTP clients and browser automation frameworks\n* Headless and non-headless browsers\n* Mobile and rotating proxies\n* Header replication (UA, sec-ch-ua, accept, etc.)\n* Session persistence, realistic delays, low RPS\n\nDespite matching headers and basic browser behavior, sessions eventually fail.\n\n# Observed behavior\n\nFrom inspecting network traffic:\n\n* Initial page load sets temporary cookies\n* A follow-up request sends browser fingerprint / behavioral telemetry\n* Only after successful validation are long-lived cookies issued\n* Missing or inconsistent telemetry leads to 403/418 shortly after\n\nThis looks consistent with client-side sensor collection (JS-generated signals rather than static tokens).\n\n# Conceptual question\n\nAt this level of protection, is it generally realistic to:\n\n* Attempt to reproduce sensor payloads manually (outside a real browser), or\n* Does this usually indicate that:\n   * Traditional HTTP-level scraping is no longer viable?\n   * Only full browser execution with real user interaction scales reliably?\n   * Or that the correct approach is to seek alternative data sources (official APIs, licensed feeds, partnerships)?\n\nI‚Äôm not asking for bypass techniques or ToS violations ‚Äî I‚Äôm trying to understand where the practical boundary is for scraping when dealing with modern, behavior-based bot defenses.\n\nAny insight from people who‚Äôve dealt with Akamai or similar systems would be greatly appreciated.\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1qofvmx/akamai_antibot_blocking_flight_search_scraping/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o21hix7",
          "author": "army_of_wan",
          "text": "Have you tried using fire fox automation ? ( puppeteer , camafoux ?)\nHarvest cookies with a browser and then reuse them to scrape the site.",
          "score": 5,
          "created_utc": "2026-01-27 16:27:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24xvrd",
              "author": "letopeto",
              "text": "Isn‚Äôt camoufox widely outdated? Repo hasn‚Äôt been update in a while it‚Äôs still running a very old version of Firefox and last time I tried it it couldn‚Äôt even bypass a simple Cloudflare challenge",
              "score": 2,
              "created_utc": "2026-01-28 02:04:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o265xra",
              "author": "irrisolto",
              "text": "Not gonna work at scale",
              "score": 1,
              "created_utc": "2026-01-28 06:38:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o266nm3",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 1,
                  "created_utc": "2026-01-28 06:43:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21k1xl",
          "author": "Specialist-Egg-3720",
          "text": "once the js challenge is completed in real browser, it sets a cookie, you need to capture that cookie then move on to do your api calls, its very difficult to reproduce a sensor payload without the real browser as it checks for various things such as webgl rendering, viewport, cpu info +gpu info + screen info + other params of your pc to create a unique device specifc signature. Its good option to just solve this challenge in real browser , get the cookie then move on to http request based scraping.",
          "score": 3,
          "created_utc": "2026-01-27 16:38:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o226p1d",
          "author": "ScrapeAlchemist",
          "text": "Hey!\n\nYou've basically hit the wall where HTTP-level scraping stops being practical. Akamai Bot Manager with sensor.js is specifically designed to detect the difference between real browsers and automation.\n\nTo answer your conceptual question directly:\n\n1. **Reproducing sensor payloads manually** - theoretically possible but a nightmare to maintain. The fingerprint includes WebGL rendering, canvas, audio context, and dozens of other signals that change with Akamai updates\n\n2. **Full browser with real interaction** - this works but you need the browser to actually execute the JS and pass the challenges. The cookie harvesting approach others mentioned is the right idea\n\n3. **Alternative data sources** - for flight data specifically, Google Flights pulls from the same GDS systems. Some airlines also have affiliate/partner APIs that are less protected\n\nThe practical boundary: if a site invests in Akamai Bot Manager, they've decided to make scraping hard. You either invest equally in bypassing it (real browsers, residential IPs, proper session handling) or find a different data source.\n\nFor personal research at low volume, a real browser with residential IP and human-like delays might work. But it'll be fragile.\n\nGood luck!",
          "score": 1,
          "created_utc": "2026-01-27 18:16:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22tuoh",
          "author": "Afraid-Solid-7239",
          "text": "reply with the page, and data you want to scrape?",
          "score": 1,
          "created_utc": "2026-01-27 19:56:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27bgei",
              "author": "Individual-Ship-7587",
              "text": "The page is a public flight search results page, for example:\n\n[https://www.latamairlines.com/br/pt/oferta-voos?origin=POA&destination=SCL&outbound=2026-06-17&inbound=2026-03-26&trip=RT&cabin=Economy&redemption=true]()\n\nI‚Äôm only trying to collect the data shown to any user on that page:  \nflight options (route, dates, airline), mileage prices, taxes/fees, cabin class, and basic availability information. No booking, no account access, and no private user data.",
              "score": 2,
              "created_utc": "2026-01-28 12:29:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o265wps",
          "author": "irrisolto",
          "text": "Use a solver to get akamai cookies",
          "score": 1,
          "created_utc": "2026-01-28 06:37:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26nn9p",
              "author": "scraperouter-com",
              "text": "what solver?",
              "score": 1,
              "created_utc": "2026-01-28 09:12:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o279qaa",
                  "author": "irrisolto",
                  "text": "A solver for akamai that gives you cookies",
                  "score": 1,
                  "created_utc": "2026-01-28 12:17:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2b1zbx",
          "author": "nez1rat",
          "text": "The only way to scrape successfully the data is by using a valid TLS fingerprint with valid Akamai cookies in your request.  \n  \nTo test it simply copy paste from your browser the cookies that are being used in the fetch flights request",
          "score": 1,
          "created_utc": "2026-01-28 22:51:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnimgt",
      "title": "Scrape a webpage that uses Akamai",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qnimgt/scrape_a_webpage_that_uses_akamai/",
      "author": "flstckdev",
      "created_utc": "2026-01-26 15:11:09",
      "score": 8,
      "num_comments": 13,
      "upvote_ratio": 0.79,
      "text": "I‚Äôm trying to scrape a webpage that uses Akamai bot protection and need to understand how to properly make HTTP requests that comply with Akamai‚Äôs requirements without using Selenium or Playwright.\n\nDoes anyone have general guidance on how Akamai detects non-browser traffic, what headers/cookies/flows are typically required, or how to structure requests so they behave like a normal browser? Any high-level advice or references would be helpful.\n\nThanks in advance.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qnimgt/scrape_a_webpage_that_uses_akamai/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1u2244",
          "author": "Acrobatic_Idea_3358",
          "text": "Not my write up but this guy has some headers and user agents that worked at some point. Probably  a good starting point, however I have not tested any of these. https://substack.thewebscraping.club/p/scraping-akamai-protected-websites",
          "score": 5,
          "created_utc": "2026-01-26 15:38:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zdfw3",
              "author": "flstckdev",
              "text": "Thanks! Definitely a good starting point.",
              "score": 2,
              "created_utc": "2026-01-27 08:23:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ud3cn",
          "author": "Odd-Lychee1248",
          "text": "I faced akamai bot detection and blocked my requests even I used selenium, but I solved the problem using SeleniumBase undetected mode and I sent the request using ex_script function inside chrome devtools using fetch()",
          "score": 5,
          "created_utc": "2026-01-26 16:25:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zdcgo",
              "author": "flstckdev",
              "text": "That one I did not know of. Thank you, i‚Äôll look into it!",
              "score": 1,
              "created_utc": "2026-01-27 08:22:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o21osch",
          "author": "ScrapeAlchemist",
          "text": "Hey!\n\nAkamai's detection goes way beyond headers - it fingerprints TLS settings (JA3/JA4), HTTP/2 frame ordering, and behavioral signals from their sensor script.\n\nFor pure HTTP without a browser, you'd need to:\n\n- Match TLS fingerprint exactly (curl-impersonate can help here)\n- Replicate HTTP/2 frame ordering and priority settings\n- Handle their cookie validation flow (initial request ‚Üí sensor challenge ‚Üí validated session)\n- Rotate IPs before behavioral patterns get flagged\n\nThe substack article someone linked is a good starting point for headers. But honestly, if the site is running Akamai Bot Manager (not just basic Akamai CDN), pure HTTP scraping becomes a cat-and-mouse game. Their sensor.js collects browser fingerprints that are hard to fake without actual JS execution.\n\nYou might want to check if the site has a mobile app or public API - sometimes those have lighter protections than the web frontend.\n\nGood luck!",
          "score": 3,
          "created_utc": "2026-01-27 16:59:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1u3epk",
          "author": "SurlyJason",
          "text": "I've done a few, and found they vary from stupidly easy (usually HTTP headers) to more complex where I could get some requests to work, but an IP would be blocked after just a few successful requests. \n\nIf you're okay sharing the URL, I have a little utility I made to test sites. I could see if I can tell where on the spectrum it lies.",
          "score": 2,
          "created_utc": "2026-01-26 15:44:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vjxdu",
              "author": "brnbs_dev",
              "text": "How does that utility tool work?",
              "score": 1,
              "created_utc": "2026-01-26 19:28:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21loks",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-27 16:45:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21pzgd",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-27 17:04:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qod78a",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qod78a/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-01-27 13:01:04",
      "score": 8,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levels‚Äîwhether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) üå±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1qod78a/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2b5ims",
          "author": "Working_Map379",
          "text": "I am looking to hire web scraping expert. Please DM me.",
          "score": 2,
          "created_utc": "2026-01-28 23:09:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20j8it",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-27 13:45:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20v3bm",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-01-27 14:46:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21d8jp",
          "author": "xRazar",
          "text": "I'm currently in the process of scraping e-sim sites wherever possible I try to find the public APIs for this but it does not seem that effective for most of the sites. Anyone has experience with scraping E-Sim sites (Saily, Nomad as few examples to go off)",
          "score": 1,
          "created_utc": "2026-01-27 16:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hodl0",
              "author": "error1212",
              "text": "I have",
              "score": 1,
              "created_utc": "2026-01-29 21:58:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21ndjn",
          "author": "EstablishmentOver202",
          "text": "How do you guys deal with cloudflare? Turnstile is killing me",
          "score": 1,
          "created_utc": "2026-01-27 16:53:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24ff3z",
              "author": "Mean_Professional529",
              "text": "Try a scraping API that handles JavaScript rendering and proxy rotation. Some services include built-in CAPTCHA solving for Turnstile. This can help bypass Cloudflare without managing it yourself",
              "score": 1,
              "created_utc": "2026-01-28 00:28:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2e6w04",
              "author": "jonfy98",
              "text": "You could try APIScraper which is efficient but not free,   \nanother idea could be NoDriver for better successrate which i often use for dealing with this kind of problem.",
              "score": 1,
              "created_utc": "2026-01-29 11:37:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2gq3ee",
              "author": "gbertb",
              "text": "not free, but very cost is use spider cloud",
              "score": 1,
              "created_utc": "2026-01-29 19:14:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2p35i9",
              "author": "Open_Passage_7351",
              "text": "[https://github.com/lexiforest/curl\\_cffi](https://github.com/lexiforest/curl_cffi) wrapped with FastAPI endpoint.\n\n\n\nI run my \\`app.py\\` which exposes a single \\`post /api/forward\\` endpoint. I can call that endpoint from any other service passing the target URL. The request is passed through curl\\_cffi, and response returned. I'm using this at a scale of thousands of requests a day with pretty decent success rate (well above 80%).",
              "score": 1,
              "created_utc": "2026-01-30 23:22:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o28bhyx",
          "author": "Either_Height7010",
          "text": "I'm hiring a US-based senior+ reverse engineer. At an incredibly high level, think bypassing anti-bot systems, large-scale web scraping/login automation, and JavaScript-based reverse engineering of web apps. \n\nI'm a third-party recruiter sourcing on behalf of my client. Message me if intrigued!",
          "score": 1,
          "created_utc": "2026-01-28 15:38:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmbyps",
      "title": "Browser Code: Coding agent for user scripts",
      "subreddit": "webscraping",
      "url": "https://github.com/chebykinn/browser-code",
      "author": "heraldev",
      "created_utc": "2026-01-25 06:51:59",
      "score": 8,
      "num_comments": 5,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qmbyps/browser_code_coding_agent_for_user_scripts/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1l1q21",
          "author": "RandomPantsAppear",
          "text": "Good damn work. I love the idea of making a virtual fs to represent the webpage",
          "score": 3,
          "created_utc": "2026-01-25 08:11:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l58mh",
          "author": "BodybuilderLost328",
          "text": "Its all fine till the html of all the page exceed the llm context, how are you handling this?\n\nSo like for bigger webpages like amazon this tool wont work right?",
          "score": 3,
          "created_utc": "2026-01-25 08:42:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ncqle",
              "author": "heraldev",
              "text": "It will! The agent in the extension reads the page as a file. This file is formatted and cleaned up - I add spaces and newlines around each html tag, this allows for reading only the parts of it. Then the agent has 3 tools to explore the file - read with offset and limit, grep, and as a last resort it can execute JS to filter elements.",
              "score": 1,
              "created_utc": "2026-01-25 17:03:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wwdei",
          "author": "quarkcarbon",
          "text": "Reducing a semantic rich webpage as a file with some offsets doesn't it lead to inefficient reads and highly expensive ops ? Also it's all fun and games in testing with browser webpage/extension storage. But the min you do it on the real user's browser - their device type/ram + the number of tabs they open and how full is storage is gonna blow out with browser storage errors soon esp since you take the html of pages. Now if you often fallback to user's device FS, then it's another CLI agent with web access.",
          "score": 1,
          "created_utc": "2026-01-26 23:06:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ksdns",
          "author": "heraldev",
          "text": "I‚Äôve been experimenting with embedding an Claude Code-style coding agent directly into the browser.\n\nAt a high level, the agent generates and maintains userscripts and CSS that are re-applied on page load. Rather than just editing DOM via JS in console the agent is treating the page, and the DOM as a **file**.\n\nThe models are often trained in RL sandboxes with full access to the filesystem and bash, so they are really good at using it. So to make the agent behave well, I've simulated this environment.\n\nThe whole state of a page and scripts is implemented as a virtual filesystem hacked on top of `browser.local` storage. URL is mapped to directories, and the agent starts inside this directory. It has the tools to read/edit files, grep around and a fake bash command that is just used for running scripts and executing JS code.\n\nI've tested only with Opus 4.5 so far, and it works pretty reliably.  \nThe state of the file system can be synced to FS, although because Firefox doesn't support Filesystem API, you need to manually import the FS contents first.\n\nThis agent is \\*really\\* useful for extracting things to CSV.",
          "score": 1,
          "created_utc": "2026-01-25 06:52:28",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpdyt6",
      "title": "Internal Google Maps API endpoints",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qpdyt6/internal_google_maps_api_endpoints/",
      "author": "LouisDeconinck",
      "created_utc": "2026-01-28 15:11:48",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 0.89,
      "text": "I build a scraper that extracts place IDs from the protobuf tiling api. Now I would like to fetch details from each place using this place id (I also have the S2 tile id). Are rhere any good endpoints to do this with?",
      "is_original_content": false,
      "link_flair_text": "Scaling up üöÄ",
      "permalink": "https://reddit.com/r/webscraping/comments/1qpdyt6/internal_google_maps_api_endpoints/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qqxam9",
      "title": "Do I need a residential proxy to mass scrape menus?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qqxam9/do_i_need_a_residential_proxy_to_mass_scrape_menus/",
      "author": "z420a",
      "created_utc": "2026-01-30 05:58:34",
      "score": 7,
      "num_comments": 8,
      "upvote_ratio": 0.9,
      "text": "I have about 30,000 restaurants for which I need to scrape their menus. As far as I know a good chunk of those use services such as uber eats, DoorDash, toasttab, etc to host their menus. \n\nIs it possible to scrape all of that with just my laptop? Or will I get IP banned?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qqxam9/do_i_need_a_residential_proxy_to_mass_scrape_menus/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2k1su0",
          "author": "Puzzleheaded_Row3877",
          "text": "with uber eats you get upto 2000 requests per hour ,I don't think the rest of the sites you mentioned care about rate limiting that much ; and if they do , mix it up by scraping some of the menus from the wayback machine.",
          "score": 8,
          "created_utc": "2026-01-30 06:08:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lxvby",
          "author": "Pooria_P",
          "text": "Don't scrape all with your laptop if you'll be doing it at once. if the websites are not sophisticated, just use the cheapest proxy you can find (typically, static datacenter, but HIGH block rates), or static residentials.  \nUber eats and etc though, I think you have a better chance with rotating residential.",
          "score": 2,
          "created_utc": "2026-01-30 14:35:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2k7x8c",
          "author": "bigtakeoff",
          "text": "naw... guess it depends on where the menus are at",
          "score": 1,
          "created_utc": "2026-01-30 06:56:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kihz0",
          "author": "Bosschopper",
          "text": "Can‚Äôt answer you question but I‚Äôm working on a similar project. Are you doing a menu database? My project is pretty far along the dev pipeline and I‚Äôm hosted on the internet (well, sorta) so if you‚Äôre open to collaborating hit me up.\n\nI‚Äôm far along with UI stuff and features but frankly have no data to feed. DM me about where you‚Äôre at too",
          "score": 1,
          "created_utc": "2026-01-30 08:28:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lncto",
          "author": "HaphazardlyOrganized",
          "text": "Probably not, just don't pound their servers. If you check like once a day at 2am you should be fine",
          "score": 1,
          "created_utc": "2026-01-30 13:41:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mbao3",
          "author": "bluemangodub",
          "text": "Do you? try it. And if you do, you will find out.\n\nThat's it. No one knows, unless they have done it. You can guess. \n\nBut typically, if you are scraping 1 site, and you want 30k pages, if you want to do it slow you might get away with it. Otherwise, proxies allow you to hit it harder.",
          "score": 1,
          "created_utc": "2026-01-30 15:39:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo77gd",
      "title": "Advice needed: scraping company websites in Python",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qo77gd/advice_needed_scraping_company_websites_in_python/",
      "author": "Working_Taste9458",
      "created_utc": "2026-01-27 07:26:44",
      "score": 6,
      "num_comments": 14,
      "upvote_ratio": 0.88,
      "text": "I‚Äôm building a small project that needs to scrape company websites (manufacturers, suppliers, distributors, traders) to collect basic business information. I‚Äôm using Python and want to know what the best approach and tools are today for reliable web scraping. For example, should I start with requests + BeautifulSoup, or go straight to something like Playwright? Also, any general tips or common mistakes to avoid when scraping multiple websites would be really helpful.",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qo77gd/advice_needed_scraping_company_websites_in_python/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1zauu4",
          "author": "Bitter_Caramel305",
          "text": "Playwright is not the choice of any expert it's the choice of dumb beginners.\n\nRequests and bs4 is fine but replace requests with the requests module of curl\\_cffi.  \nThe syntax will be the same, but you'll get TSL fingerprinting of a real browser (Thanks to C) and an optional but powerful request param (impersonate=\"any browser of your choice\").\n\nExample:\n\n    from curl_cffi import requests\n    r = requests.get(url, cookies, headers, impersonate=\"chrome\")\n\n\n\nAlso, always reverse engineer the exposed backend API first and use this as a fallback not primary method.  \nHappy scraping!",
          "score": 11,
          "created_utc": "2026-01-27 08:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zn2lq",
              "author": "scraperouter-com",
              "text": "if curl\\_cffi is blocked you can try scrapling stealthmode but only if you are sure you need the browser (much slower way)",
              "score": 3,
              "created_utc": "2026-01-27 09:53:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o23uv95",
                  "author": "askolein",
                  "text": "But isn‚Äôt most websites not directly rendering html via http requests. I struggle to see any relevant website to scrape without selenium?",
                  "score": 1,
                  "created_utc": "2026-01-27 22:44:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o277s0v",
              "author": "husayd",
              "text": "I feel offended by the first sentence xd. I use both, and sometimes playwright (or selenium) is inevitable, or i am just a dumb beginner.",
              "score": 1,
              "created_utc": "2026-01-28 12:04:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o279aau",
                  "author": "Bitter_Caramel305",
                  "text": "Sorry about that ;) but to be honest, sometimes I reverse engineer the entire website while reverse engineering the API, just so I can avoid the inevitable browser automation.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:14:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o208fu0",
          "author": "Responsible-Fly-990",
          "text": "go with requests + BeautifulSoup if you r a beginner",
          "score": 2,
          "created_utc": "2026-01-27 12:43:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24byq2",
          "author": "Hungry-Working26",
          "text": "For company sites, start with requests and BeautifulSoup. Switch to Playwright only if you see dynamic content. Rotate user agents and add delays between requests to be respectful.\n\nHere's a basic pattern using the requests library:\n\npython\n\nimport requests\n\nfrom bs4 import BeautifulSoup\n\nresponse = requests.get('your\\_url\\_here')\n\nsoup = BeautifulSoup(response.content, 'html.parser')\n\nAlways check the site's robots.txt first",
          "score": 2,
          "created_utc": "2026-01-28 00:11:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2818mg",
          "author": "New-Independence5780",
          "text": "use cheriooCrawlee if it just simple websites that doesnt need js rendering if yes use playwrightCrawlee or puppeterCrawlee",
          "score": 1,
          "created_utc": "2026-01-28 14:51:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28rt0d",
          "author": "wequatimi",
          "text": "So you got ai businessidea=make cash fast.\nMight be entertaining. And educative..",
          "score": 1,
          "created_utc": "2026-01-28 16:49:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2azs7r",
          "author": "nez1rat",
          "text": "Honestly it depends on what are your target sites tho, I can suggest you to use [https://pypi.org/project/curl-cffi/](https://pypi.org/project/curl-cffi/) with BeautifulSoup as you mentioned",
          "score": 1,
          "created_utc": "2026-01-28 22:40:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eubfr",
          "author": "byte_knight_",
          "text": "Definetely start from with requests and bs4 or speed and simplicity, i'd use Playwright only for something JS heavy maybe",
          "score": 1,
          "created_utc": "2026-01-29 14:03:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmysbt",
      "title": "I'm starting a web scraping project. Need advices.",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qmysbt/im_starting_a_web_scraping_project_need_advices/",
      "author": "Papenguito",
      "created_utc": "2026-01-25 23:11:50",
      "score": 6,
      "num_comments": 15,
      "upvote_ratio": 0.75,
      "text": "I am going to start a project of web scraping. Is playwright with TS the best option to start i want to scrape some pages o news from my city i need advices to start with this pls ",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qmysbt/im_starting_a_web_scraping_project_need_advices/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1pr5a0",
          "author": "hikingsticks",
          "text": "You need to investigate the pages and see what required to get the data you want. Only use headless browser if you have to, it's much more preferable to not use one if possible.\n\nOpen the network tab and check the requests being made by your browser, see which one(s) have the data you need, and try to replicate them.",
          "score": 6,
          "created_utc": "2026-01-25 23:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ptnwn",
              "author": "Papenguito",
              "text": "i want to get the news from the web pages",
              "score": 0,
              "created_utc": "2026-01-25 23:29:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1q1pwi",
                  "author": "hikingsticks",
                  "text": "Yes... You'd be well served by learning some html basics, and becoming familiar with the network tab. Then watch some John Watson Rooney on YouTube for scraping techniques.\n\nOr just throw AI at it and learn nothing.",
                  "score": 4,
                  "created_utc": "2026-01-26 00:08:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1q0vmy",
                  "author": "Own_Relationship9794",
                  "text": "which website is it?",
                  "score": 1,
                  "created_utc": "2026-01-26 00:04:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ut9su",
          "author": "hasdata_com",
          "text": "Have you looked into Google News RSS? That's usually the easiest starting point if you just need the headlines. For the actual sites, it really comes down to how they load data. If it's simple static HTML, basic request libs work fine. But for anything with JS rendering, you're right, you will need heavier tools like Playwright to handle the dynamic content",
          "score": 8,
          "created_utc": "2026-01-26 17:35:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rxmyb",
          "author": "Key_Investment_6818",
          "text": "basic html parsing with curl\\_cffi should do the job , just make sure you know what elements you want to scrape..",
          "score": 2,
          "created_utc": "2026-01-26 06:41:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1u2hig",
          "author": "bluemangodub",
          "text": "Depends on the site. Really it's trial and error.  Try HTTP requests. If that works, great. If not, do really need a browser? If so, try a browser. Does it work? Great? If not, then finger the anti bot detections and by pass that",
          "score": 2,
          "created_utc": "2026-01-26 15:39:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1u63vt",
              "author": "Papenguito",
              "text": "Thanks mate",
              "score": 1,
              "created_utc": "2026-01-26 15:55:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o22xdua",
          "author": "elixon",
          "text": "Start by opening the Network tab in your browser and search for the data in the requests there Ctrl+Shift+F in Chrome. Find the request that contains the information you need, right click it, and copy it as curl or fetch. Learn how to make simple, effective HTTP requests directly.\n\nSkip Playwright. It is expensive and unnecessary for 99% of scraping use cases. Only beginners rely on it because they never scale and keep their ambitions low.",
          "score": 2,
          "created_utc": "2026-01-27 20:12:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2au4lc",
          "author": "No-Incident5783",
          "text": "From experience, try not to overcomplicate things. Depending on the complexity of the website it might be better to simply use htpp request or selenium. Also, if you are a beginner, don‚Äôt necessarily use headless browser with selenium or playwright. This way you can see what your code is doing and through which tabs, elements etc it is going through.",
          "score": 2,
          "created_utc": "2026-01-28 22:14:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1q0zqy",
          "author": "Rorschache00714",
          "text": "If if you download Antigravity you can tell the agent to use the browser and do that for you. Have it create a json file with all the scrape data.",
          "score": 1,
          "created_utc": "2026-01-26 00:04:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qat0f",
          "author": "akashpanda29",
          "text": "You can do it from playwright. But playwright is a overkill for most of the website where you can just get html of api data as json directly through a fetch call. \nSo investigating website is the primary step",
          "score": 1,
          "created_utc": "2026-01-26 00:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r11cx",
          "author": "Holiday-Tonight5626",
          "text": "every news site is different. sites know ppl r scraping, so they all have measures to deal with that. some use apis, like npr i think..\nif you want to scrape popular news sites yeah you will have to use pw for a lot of it. wait for the js to render then grab that shit",
          "score": 1,
          "created_utc": "2026-01-26 03:06:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpmlka",
      "title": "Help: BeautifulSoup/Playwright Parsing Logic",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/gallery/1qpmlka",
      "author": "TapProfessional4535",
      "created_utc": "2026-01-28 20:14:09",
      "score": 6,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qpmlka/help_beautifulsoupplaywright_parsing_logic/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o2ck3u1",
          "author": "jlrich10",
          "text": "I would look at scripts. I didnt want to take the time to do it but I would look at this and it see if it has what you need. \\_\\_INITIAL\\_DATA\\_\\_. Give that to Claude or Chatgpt and it will write the parser if it has what you need in it. Using json is usually better if its in there.",
          "score": 2,
          "created_utc": "2026-01-29 03:43:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2n8ciq",
              "author": "TapProfessional4535",
              "text": "I‚Äôve tried ChatGPT and Gemini both. Many hours of back and forth. I‚Äôve got enterprise licenses for both. I‚Äôm a decent promoter for regression modeling and forest modeling so this is making me pull my hair out",
              "score": 1,
              "created_utc": "2026-01-30 18:06:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2fm5s5",
          "author": "Business-Cherry1883",
          "text": "If your end goal is ‚Äú3000 players + rankings‚Äù, I‚Äôd seriously consider¬†**avoiding DOM parsing**¬†for the core dataset and only parsing HTML for the few fields that aren‚Äôt available elsewhere.\n\n* There‚Äôs already a Python package on pip called¬†`twofourseven`¬†that can scrape 247Sports recruiting data and includes a¬†`TransferPortal`¬†class with¬†`getFootballData(year)`¬†that returns a dataframe of everyone who entered the football transfer portal for a given year.‚Äã\n* Once you have that baseline list, use Playwright only for the ‚Äúdetail page‚Äù fields you¬†*must*¬†render (e.g., banners/commitment blocks) and keep the HTML parsing minimal and label-driven (parse ‚ÄúOVR/NATL/ST/Pos‚Äù by the nearby label text, not by assuming order/position).\n\nFor your brittle cases (stars, state rank vs position rank, JUCO variance), don‚Äôt do ‚Äúany number not X must be Y‚Äù. Instead: extract the small text chunk for each section (‚ÄúAs a Transfer‚Äù / ‚ÄúAs a Prospect‚Äù), then regex-match explicit patterns (`OVR`,¬†`NATL`,¬†`ST`,¬†`JUCO`,¬†`KS: 8`, etc.) and treat anything unmatched as ‚Äúunknown‚Äù rather than forcing it into a column.",
          "score": 2,
          "created_utc": "2026-01-29 16:15:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2n9oyk",
              "author": "TapProfessional4535",
              "text": "If I am using GitHub, Databricks, Google Collab, or another LLM, does that change your reco? \n\nI‚Äôm on a machine that is strict with what I can do in native Python app on it. \n\nThis is the first scraping project I‚Äôve ever done FWIW.",
              "score": 1,
              "created_utc": "2026-01-30 18:12:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2a3fbl",
          "author": "TapProfessional4535",
          "text": "The Code Snippet: Here is the full parsing function. Is there a more robust way to handle these dynamic ranking boxes and to separate out Transfer vs. Prospect?\n\ndef parse_profile(html, url, player_id):\n    soup = BeautifulSoup(html, 'lxml')\n    data = {}\n   \n    # Locate the text nodes to find the correct containers\n    transfer_node = soup.find(string=re.compile(\"As a Transfer\"))\n    prospect_node = soup.find(string=re.compile(\"As a Prospect\"))\n\n    # --- 1. Parsing Transfer Section ---\n    if transfer_node:\n        t_container = transfer_node.find_parent('section') or transfer_node.find_parent('div')\n        if t_container:\n            # Stars & Rating\n            stars = t_container.select('.icon-starsolid.yellow')\n            data['Transfer Stars'] = len(stars)\n            rating = t_container.select_one('.rating')\n            if rating: data['Transfer Rating'] = rating.text.strip()\n           \n            # Ranks (Negative Logic)\n            for li in t_container.select('li'):\n                label = li.select_one('h5').get_text(strip=True).upper()\n                val = li.select_one('strong').get_text(strip=True)\n               \n                if 'OVR' in label:\n                    data['Transfer Overall Rank'] = val\n                # If NOT Overall/National/State, assume Position Rank (Fixes WR vs S mismatch)\n                elif label not in ['NATL', 'NATIONAL', 'ST', 'STATE']:\n                    data['Transfer Position Rank'] = val\n\n    # --- 2. Parsing Prospect Section (JUCO Logic) ---\n    if prospect_node:\n        p_container = prospect_node.find_parent('section') or prospect_node.find_parent('div')\n        if p_container:\n            # Check for JUCO header\n            is_juco = \"JUCO\" in p_container.get_text().upper()\n           \n            # Stars (Flag JUCO if empty)\n            stars = p_container.select('.icon-starsolid.yellow')\n            data['Prospect Stars'] = f\"{len(stars)} JUCO\" if is_juco else len(stars)\n\n            # Ranks (Prioritize National, then Position)\n            for li in p_container.select('li'):\n                label = li.select_one('h5').get_text(strip=True).upper()\n                val = li.select_one('strong').get_text(strip=True)\n               \n                if 'NATL' in label or 'NATIONAL' in label:\n                    data['Prospect National Rank'] = f\"{val} JUCO\" if is_juco else val\n                # Filter out State abbreviations (AK, AL, ... TX, etc) to find Position Rank\n                elif label not in ['OVR', 'ST', 'STATE', 'TX', 'FL', 'CA', 'GA']:\n                    data['Prospect Position Rank'] = f\"{val} JUCO\" if is_juco else val\n\n    return data",
          "score": 1,
          "created_utc": "2026-01-28 20:17:07",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2ecmrt",
          "author": "scraperouter-com",
          "text": "Did you check JSON data available in the source code?\n\nhttps://preview.redd.it/x5irlakw5agg1.png?width=442&format=png&auto=webp&s=0b99b76c15763e484568f4b6ac05ddaaa2b80a79\n\nand other similar tags with structured data.",
          "score": 1,
          "created_utc": "2026-01-29 12:19:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2n8nx7",
              "author": "TapProfessional4535",
              "text": "I‚Äôd be lying if I knew what I‚Äôm doing. Savvy in advanced analytics, but this is the first scraping project I‚Äôve ever worked on.",
              "score": 0,
              "created_utc": "2026-01-30 18:07:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}