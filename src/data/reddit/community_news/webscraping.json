{
  "metadata": {
    "last_updated": "2026-01-20 08:59:58",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 13,
    "total_comments": 37,
    "file_size_bytes": 58910
  },
  "items": [
    {
      "id": "1qem7fs",
      "title": "Help on how to go about scraping faculty directory profiles",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qem7fs/help_on_how_to_go_about_scraping_faculty/",
      "author": "gatherer_benefactor",
      "created_utc": "2026-01-16 17:24:19",
      "score": 12,
      "num_comments": 3,
      "upvote_ratio": 0.94,
      "text": "Hi everyone,\n\nI‚Äôm working on a research project that requires building a large-scale dataset of faculty profiles from 200 to 250 business schools worldwide. For each school, I need to collect faculty-level data such as: name, title or role, department, short bio or research interests, sometimes email, CV links, publications. The aim is to systematically scrap faculty directories across many heterogeneous university websites. My current setup is like this: Python, Selenium, BeautifulSoup, MongoDB for storage (timestamped entries to allow longitudinal tracking), one scraper per university (100 already written. I do this with the following workflow: manually inspect the faculty directory, write Selenium logic to collect profile URLs, visit each profile and extract fields with BeautifulSoup and then store the data in mongodb. \n\nThis works, but clearly does not scale well to 200 sites, especially long-term maintenance when sites change structure. What I‚Äôm unsure about and looking for advice on is the architecture for automation. Is ‚Äúone scraper per site‚Äù inevitable at this scale? Any recommendations for organizing scrapers so maintenance doesn‚Äôt become a nightmare? What are your toughts or experiences using LLMs to analyze a directory HTML, suggest Selenium actions (pagination, buttons), infer selectors?\n\nBasically my question is what you would do differently if you had to do this again for an academic project with transparency/reproducibility constraints, how would you approach it? I‚Äôm not looking for copy-paste code, more design advice, war stories, or tooling suggestions.\n\nThanks a lot, happy to clarify details if useful!",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qem7fs/help_on_how_to_go_about_scraping_faculty/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o014rjv",
          "author": "Persian_Cat_0702",
          "text": "I'd use Langchain if I wanted to scale",
          "score": 2,
          "created_utc": "2026-01-17 01:32:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ijfyr",
              "author": "gatherer_benefactor",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-19 17:53:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0jzgqp",
          "author": "orthogonal-ghost",
          "text": "Some thoughts / recommendations on this:  \n   \n1. In general, I‚Äôd recommend *against* using Selenium and would prioritize extracting web data using network requests / API calls when possible. For long-term maintenance, this tends to be more resilient (i.e., APIs are less likely to change than page layouts).¬†\n\n2. We‚Äôre actually building an app (https://app.motie.dev/) to make projects like these much easier to scale and maintain. All you need to do is give it a URL and prompt (e.g., ‚ÄúExtract the name, title or role, department‚Ä¶ for all faculty‚Äù), and it will give you back the data and code to run it locally. We also support scheduled runs, long-term maintenance, etc.¬†:)",
          "score": 1,
          "created_utc": "2026-01-19 21:53:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qd4eig",
      "title": "[Open Source] CLI to inject local cookies for auth",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qd4eig/open_source_cli_to_inject_local_cookies_for_auth/",
      "author": "Bubbly_Gap6378",
      "created_utc": "2026-01-15 00:22:45",
      "score": 11,
      "num_comments": 11,
      "upvote_ratio": 0.8,
      "text": "I've been building scrapers for a while, and the biggest pain point is always the login flow. If I try to automate the login with Selenium or Playwright, I hit 2FA, Captchas, or \"Suspicious Activity\" blocks immediately.\n\nI realized the easiest way around this is to stop trying to automate the login and just reuse the valid session I already have on my local Chrome browser.\n\nI wrote a Python CLI tool (Romek) to handle the extraction.\n\nHow it works under the hood:\n\n1. It locates the local Chrome Cookies SQLite database on your machine.\n2. It decrypts the cookies using the OS-specific master key (DPAPI on Windows, AES on Mac/Linux).\n3. It exports them into a JSON format that Playwright/Selenium can read.\n\nWhy I made it:\n\nI needed to run agents on a headless VPS that could access my accounts on complex sites without triggering the \"New Device\" login flow. By injecting the \"High Trust\" cookies from my main profile, the headless browser looks like my desktop.\n\nThe Tool:\n\nIt's 100% Open Source (MIT) and free.\n\nRepo:[https://github.com/jacobgadek/romek](https://github.com/jacobgadek/romek)\n\nPyPI: `pip install romek`\n\nHopefully, this saves someone else from writing another broken login script.",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1qd4eig/open_source_cli_to_inject_local_cookies_for_auth/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nznr4wm",
          "author": "matty_fu",
          "text": "What is cloud vault sync and will it be free?",
          "score": 1,
          "created_utc": "2026-01-15 02:35:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nznumii",
              "author": "Bubbly_Gap6378",
              "text": "Hey! Great question.\n\nRomek is designed to be **100% local-first.** The 'vault' is just a standard SQLite file created on your local disk (encrypted with your OS keychain).\n\nThe 'sync' logic I'm building is for moving that file between your own machines (e.g., Laptop -> VPS) via your own infrastructure (SSH/S3), not through a proprietary cloud I own.\n\nThe goal is to make a standalone utility (like `curl` or `jq`) for auth, not a subscription service. It will remain open source and free.",
              "score": 2,
              "created_utc": "2026-01-15 02:56:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzsaopj",
          "author": "Illustrious_Dark9449",
          "text": "Great work, have been looking for a solution to this flow!",
          "score": 1,
          "created_utc": "2026-01-15 19:40:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzthzew",
          "author": "andreaswpv",
          "text": "404 Error on the link",
          "score": 1,
          "created_utc": "2026-01-15 23:05:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzupvff",
          "author": "TobiasMcTelson",
          "text": "404",
          "score": 1,
          "created_utc": "2026-01-16 03:06:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx8rv6",
              "author": "Bubbly_Gap6378",
              "text": "Yeah, sorry about that 404. GitHub flagged the repo last night because the cookie decryption was a bit too 'grey hat' for their compliance filters.\n\nI‚Äôm re-architecting the auth stuff to be compliant, but in the meantime, I pivoted to releasing the safety infrastructure first.\n\nI just shipped **Vallignus** (a local Firewall for Agents) to PyPI today. It stops agents from spiraling or hitting bad URLs.\n\nYou can check it out here:[https://pypi.org/project/vallignus/](https://www.google.com/search?q=https://pypi.org/project/vallignus/)\n\n(The cookie/auth features will be added back as a safe module in v0.2.0 next week).",
              "score": 1,
              "created_utc": "2026-01-16 14:14:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzz0mbv",
                  "author": "tocarbajal",
                  "text": "And you have plans to release the main repo soon?",
                  "score": 1,
                  "created_utc": "2026-01-16 19:02:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzoylwo",
          "author": "TorZidan",
          "text": "Inject local cookies? Inject them with what? Eclairs re being filled with creme through a hole in the puffy pastry which you may consider being an injection, but I've never heard of this.",
          "score": 1,
          "created_utc": "2026-01-15 07:52:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcnowm",
      "title": "Async web scraping framework on top of Rust",
      "subreddit": "webscraping",
      "url": "https://github.com/BitingSnakes/silkworm",
      "author": "yehors",
      "created_utc": "2026-01-14 13:47:08",
      "score": 11,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qcnowm/async_web_scraping_framework_on_top_of_rust/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "nzjny11",
          "author": "jwrzyte",
          "text": "this sounds cool thanks for sharing, I'll give it a go when i get a chance",
          "score": 1,
          "created_utc": "2026-01-14 14:43:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzk6p1a",
              "author": "yehors",
              "text": "With disabled GIL, it gives around 230 requests per second. So highly useful when you do a lot of parsing.",
              "score": 1,
              "created_utc": "2026-01-14 16:12:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qfm54r",
      "title": "pypecdp - a fully async python driver for chrome using pipes",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qfm54r/pypecdp_a_fully_async_python_driver_for_chrome/",
      "author": "sohaib0717",
      "created_utc": "2026-01-17 19:17:57",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 0.93,
      "text": "Hey everyone. I built a fully asynchronous chrome driver in Python using POSIX pipes. Instead of websockets, it uses file descriptors to connect to the browser.\n\n* Directly connects and controls the browser, no middleware\n* 100% asynchronous, nothing gets blocked\n* Built completely using built-in Python asyncio \n   * Except one `deprecated` dependency for python-cdp modules\n* Best for running multiple browsers on same machine\n* No risk of zombie chromes if code crashes\n* Easy customization via class inheritance\n* No automation signatures as there is no framework in between\n\nCurrently limited to POSIX based systems only (Linux/Mac).\n\n\n\nBug reports, feature requests and contributions are welcome!\n\n\n\n[https://github.com/sohaib17/pypecdp](https://github.com/sohaib17/pypecdp)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qfm54r/pypecdp_a_fully_async_python_driver_for_chrome/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qcehik",
      "title": "Looking for some help.",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qcehik/looking_for_some_help/",
      "author": "nawakilla",
      "created_utc": "2026-01-14 05:04:50",
      "score": 10,
      "num_comments": 8,
      "upvote_ratio": 0.92,
      "text": "My apologies, i honestly don't know if I'm even in the right place. But to put it as short as possible. \n\nI'm looking to \"clone\" a website? I found a site that has a digital user manual for a kind of rare cnc machine. However I'm paranoid that either the user or the site will take it down/ cease to exist (this has happened multiple times in the past. \n\nWhat I'm looking for: i want to be able to save the web pages locally on my computer. Then be able to open it up and use the site as if i would online. The basic site structure is 1 large image (a picture of the components), with maybe a dozen or so clickable parts. When you click it takes you to a page with a few more detailed pictures of the part with text instructions of basic repair and maintenance. \n\nIs it possible to do? I would like a better/ higher quality way to do this instead of screenshoting one by one. Is this isn't web scrapping, can someone tell me what it might be called so i can start googling?",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qcehik/looking_for_some_help/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nzhn1au",
          "author": "HLCYSWAP",
          "text": "replace [example.com](http://example.com) with your intended target and paste this into cmd/bash/terminal:\n\n    wget --mirror \\\n         --page-requisites \\\n         --adjust-extension \\\n         --convert-links \\\n         --no-parent \\\n         --wait=1 \\\n         --random-wait \\\n         --limit-rate=200k \\\n         --tries=3 \\\n         --user-agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\" \\\n         --reject-regex=\"(login|signin|register|signup)\" \\\n         -e robots=off \\\n         -o wget.log \\\n         https://example.com\n\n\\--mirror -  recursive downloading with infinite depth, downloads the entire site\n\n\\--page-requisites - downloads all the assets needed to display pages properly: images, CSS files, JavaScript files, etc\n\n\\--adjust-extension - adds .html extension to files that don't have one but are HTML\n\n\\--convert-links - rewrites all links in downloaded HTML to point to local files instead of the original URLs, so everything works offline\n\n\\--no-parent - doesn't download anything from parent directories. keeps the download limited to the specific path you specify and below",
          "score": 13,
          "created_utc": "2026-01-14 05:16:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nziydie",
              "author": "Infamous_Land_1220",
              "text": "Goat here",
              "score": 3,
              "created_utc": "2026-01-14 12:12:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzipw9q",
          "author": "99ducks",
          "text": "Have you looked to see if the site is available on archive.org? If it is, you don't have to worry about it disappearing.",
          "score": 5,
          "created_utc": "2026-01-14 11:05:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjhtr5",
              "author": "greg-randall",
              "text": "THIS!\n\nGo to [https://archive.org/](https://archive.org/) and paste in the URL in the Wayback Machine, see if what you want is there, if it is great! \n\nIf it isn't fill the URL in the bottom right box 'Save Page Now' https://web.archive.org/. Click on the dozen clickable parts and save those URLs too!",
              "score": 3,
              "created_utc": "2026-01-14 14:10:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzhs29o",
          "author": "pesta007",
          "text": "I've done a similar project before, and yes you are in the right place. What I did is  scrape all the data of the site (the hard part), recreated their front-end (pretty easy really). Then I developed a backend application to serve the data, and connected it to the front end.\n\nAs you can see, it is a lot of work. But I was willing to do it because the site was simple and I was young and passionate.",
          "score": 1,
          "created_utc": "2026-01-14 05:55:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhw0kj",
              "author": "nawakilla",
              "text": "What do you think would be the best approach? I build a handful of computers so I'm not completely tech illiterate. But I've never once done any kind of coding.",
              "score": 2,
              "created_utc": "2026-01-14 06:27:57",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzhwpyd",
              "author": "pesta007",
              "text": "If you are not a web developer, I don't think my approach would work for you. but there is still tools to capture static images of webpages. \n\nCheck out this post\n\nhttps://www.reddit.com/r/DataHoarder/s/ZAwrQ13Ng5",
              "score": 2,
              "created_utc": "2026-01-14 06:33:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzjrbgj",
          "author": "hasdata_com",
          "text": "Check out HTTrack or smth similar. It's a free, old-school software)",
          "score": 1,
          "created_utc": "2026-01-14 15:00:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qf8fvt",
      "title": "Blocked by Cloudflare despite using curl_cffi",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qf8fvt/blocked_by_cloudflare_despite_using_curl_cffi/",
      "author": "Coding-Doctor-Omar",
      "created_utc": "2026-01-17 08:59:39",
      "score": 9,
      "num_comments": 10,
      "upvote_ratio": 0.77,
      "text": "EDIT: IT FINALLY WORKED! I just had to add the content-type, origin, and referer headers.\n\nPlease help me access this API efficiently.\n\nI am trying to access this API:\n\nhttps://multichain-api.birdeye.so/solana/v3/gems\n\nI am using impersonate and the correct payload for the post request, but I keep getting 403 status code.\n\nThe only way I was able to get the data was use a Python browser automation library, go to the normal web page, and intercept this API's response using a handler (essentially automating the network tab inspection using Python), but this method is very inefficient. Below is my curl_cffi code.\n\n\n```\nfrom curl_cffi import Session\n\n\napi_url = \"https://multichain-api.birdeye.so/solana/v3/gems\"\npayload = {\"limit\":100,\"offset\":0,\"filters\":[],\"shown_time_frame\":\"4h\",\"type\":\"trending\",\"sort_by\":\"price\",\"sort_type\":\"desc\"}\n\nwith Session(impersonate=\"edge\") as session:\n    session.get(\"https://birdeye.so/solana/find-gems\")\n    res = session.post(api_url, data=payload)\n    print(res.status_code)\n```\n\nOutput:\n\n```\n403\n```",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qf8fvt/blocked_by_cloudflare_despite_using_curl_cffi/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o034ekq",
          "author": "expiredUserAddress",
          "text": "I see you've no proxy in use. Use a proxy everytime you're scrapping something",
          "score": 2,
          "created_utc": "2026-01-17 10:55:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o035s55",
              "author": "Coding-Doctor-Omar",
              "text": "I actually have just gotten it to work. The issue was simpler than I thought. I had to provide content-type, origin, and referer values in my headers, in addition to the default headers of impersonate.",
              "score": 3,
              "created_utc": "2026-01-17 11:07:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o035mw0",
          "author": "Coding-Doctor-Omar",
          "text": "It turns out I had to add some extra headers. Here is the working code:\n\n```\nfrom curl_cffi import Session\n\n\napi_url = \"https://multichain-api.birdeye.so/solana/v3/gems\"\npayload = {\"limit\":100,\"offset\":0,\"filters\":[],\"shown_time_frame\":\"4h\",\"type\":\"trending\",\"sort_by\":\"price\",\"sort_type\":\"desc\"}\n\nheaders = {\n    \"content-type\": \"application/json\",\n    \"origin\": \"https://birdeye.so\",\n    \"referer\": \"https://birdeye.so/\"\n}\n\nwith Session(impersonate=\"edge\", headers=headers) as session:\n    res = session.post(api_url, json=payload)\n    print(res.status_code)\n```\n\nOutput:\n\n```\n200\n```",
          "score": 2,
          "created_utc": "2026-01-17 11:06:28",
          "is_submitter": true,
          "replies": [
            {
              "id": "o03du94",
              "author": "abdullah-shaheer",
              "text": "Yes this is actually. And if the content is in html form, you will have the need to use the content type header accordingly.",
              "score": 1,
              "created_utc": "2026-01-17 12:17:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o03dpxf",
          "author": "abdullah-shaheer",
          "text": "If it's in the json format, then you need to set content header to be in json; I don't remember the exact header, you can search",
          "score": 1,
          "created_utc": "2026-01-17 12:16:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0b106q",
          "author": "BeforeICry",
          "text": "Cloudflare typically renders the turnstile captcha even for legit browser requests. That's more like a feature of your target. In these cases, you have to resort to browser + captcha solving.",
          "score": 1,
          "created_utc": "2026-01-18 15:43:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0b5w5l",
              "author": "Coding-Doctor-Omar",
              "text": "I eventually got it to work by providing the content-type, origin, and referer values in the headers, in addition to the default headers provided by impersonate.",
              "score": 1,
              "created_utc": "2026-01-18 16:06:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ckfum",
          "author": "Alternative-842",
          "text": "yo man i had the same issue, Cloudflare just blocks normal requests if u dont send all the headers like content-type origin n referer, even if ur payload is right. i ended up using a headless browser too, way slow tho. u might try adding all the headers exactly like the site does n maybe rotate user agents, that helped me a bit. sometimes curl alone just dont cut it lol",
          "score": 1,
          "created_utc": "2026-01-18 20:05:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0epifb",
              "author": "Coding-Doctor-Omar",
              "text": "It turns out I had to add some extra headers, in addition to the normal impersonate. Here is the working code (luckily still works with curl_cffi alone, without a browser):\n\n```\nfrom curl_cffi import Session\n\n\napi_url = \"https://multichain-api.birdeye.so/solana/v3/gems\"\npayload = {\"limit\":100,\"offset\":0,\"filters\":[],\"shown_time_frame\":\"4h\",\"type\":\"trending\",\"sort_by\":\"price\",\"sort_type\":\"desc\"}\n\nheaders = {\n    \"content-type\": \"application/json\",\n    \"origin\": \"https://birdeye.so\",\n    \"referer\": \"https://birdeye.so/\"\n}\n\nwith Session(impersonate=\"edge\", headers=headers) as session:\n    res = session.post(api_url, json=payload)\n    print(res.status_code)\n```\n\nOutput:\n\n```\n200\n```",
              "score": 1,
              "created_utc": "2026-01-19 02:45:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qhcirb",
      "title": "What tool can I use to scrape this website?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qhcirb/what_tool_can_i_use_to_scrape_this_website/",
      "author": "BBQMosquitos",
      "created_utc": "2026-01-19 18:35:42",
      "score": 8,
      "num_comments": 18,
      "upvote_ratio": 1.0,
      "text": "My current resources are not working and put a few browser based scrapers but they don't seem to paginate. \n\n  \nNeed to scrape all 101 pages with company name, email, phone number, website, description, that is currently hiding under the green arrow on the right.\n\n\n\n[https://www.eura-relocation.com/membership/our-members?page=0](https://www.eura-relocation.com/membership/our-members?page=0)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qhcirb/what_tool_can_i_use_to_scrape_this_website/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o0iwjnl",
          "author": "albert_in_vine",
          "text": "You don't need to click the green arrow to access email. It's all available in the page source. Just use the CSS selector below to retrieve the email.\n\n`email = soup.select_one('div[class=\"field field-node--field-email field--name-field-email field--type-email field--label-hidden field__item\"] a').text.strip()`  \n`return email`\n\n    emails = [email.text.strip() for email in soup.\n    select\n    ('div[class=\"field field-node--field-email field--name-field-email field--type-email field--label-hidden field__item\"] a')]\n    return emails\n\nAbove is for one email to get all the emails\n\nRegarding web browsers, I am uncertain about that, but a simple pagination loop will retrieve each detail.\n\n    def pages_lists():\n    ¬† ¬† pages = [f\"https://www.eura-relocation.com/membership/our-members?page={page}\" for page    in range (1, 102)]\n    return pages",
          "score": 2,
          "created_utc": "2026-01-19 18:51:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ix4vi",
              "author": "BBQMosquitos",
              "text": "I‚Äôm not a programmer so trying to find a tool that will do it.",
              "score": 1,
              "created_utc": "2026-01-19 18:53:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0jqrhw",
                  "author": "unteth",
                  "text": "Here's a CSV containing the scraped data: https://filebin.net/08qzzv8n63l2lw7f. File expires in six days.",
                  "score": 6,
                  "created_utc": "2026-01-19 21:10:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0m00u2",
          "author": "node77",
          "text": "Import scrappy",
          "score": 1,
          "created_utc": "2026-01-20 04:26:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mrpgb",
              "author": "BBQMosquitos",
              "text": "Is there a browser addon or site? \n\nDidn't see much on google. exactly as import scrappy, just scrappy.",
              "score": 1,
              "created_utc": "2026-01-20 08:01:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0jimkw",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-19 20:32:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jps2c",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 2,
              "created_utc": "2026-01-19 21:06:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0kq85r",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 2,
                  "created_utc": "2026-01-20 00:12:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0mw0jb",
                  "author": "webscraping-ModTeam",
                  "text": "ü™ß Please review the sub rules üëâ",
                  "score": 2,
                  "created_utc": "2026-01-20 08:41:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0mvztm",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-20 08:41:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mfdbt",
          "author": "THenrich",
          "text": "chatgpt can do it. Just prompt it like this:\n\nThere are several green downarrows in this page that show the email address when the green arrow is clicked. Click on each and extract the email address. [https://www.eura-relocation.com/membership/our-members?page=0](https://www.eura-relocation.com/membership/our-members?page=0). Do this for all the pages",
          "score": 0,
          "created_utc": "2026-01-20 06:15:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mtwgb",
              "author": "BBQMosquitos",
              "text": "I think chatgpt would need to be prompted many times and it would create many batches. \n\nWere you able to do so in one go?",
              "score": 1,
              "created_utc": "2026-01-20 08:21:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qeu3ov",
      "title": "FBref Cloudflare Turnstile block. I need to bypass, please help me",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qeu3ov/fbref_cloudflare_turnstile_block_i_need_to_bypass/",
      "author": "Goldrake_Z",
      "created_utc": "2026-01-16 22:20:36",
      "score": 6,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "Hi,\n\nI'm building a Python bot to scrape historical Serie A data from [fbref.com](http://fbref.com) (schedules/fixtures + match stats/lineups). Works perfectly on local PC (Windows) and on my Ubuntu VPS until today ‚Äì now persistent Cloudflare 403 \"Just a moment...\" on¬†`/en/comps/11/schedule/Serie-A-Stats-Scores-and-Fixtures`.¬†**Now even local Windows PC affected**¬†(same error).\n\n**Already tried:**\n\n* cloudscraper + cookies/UA rotate + delays ‚Üí 403/429.\n* Playwright stealth (headless=new, TZ Europe/Rome, mouse sim) + Turnstile iframe/checkbox clicks ‚Üí title stuck after 60s, no solve (screenshot dumped).\n* FreeProxy IT (e.g. [65.109.176.217:80](http://65.109.176.217:80), rotate 10) ‚Üí proxies connect but challenge fails.\n* Full Chrome122 headers/Sec-Ch-Ua/it-IT locale ‚Üí no.\n\n\n\nAlso yesterday sofascore also \"banned\" me and I don't know why. I was using api for lineup and everything was working perfectly, I go to sleep and the next day I find myself banned (403)\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qeu3ov/fbref_cloudflare_turnstile_block_i_need_to_bypass/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o02thfq",
          "author": "Even_Refrigerator233",
          "text": "never use free proxies",
          "score": 3,
          "created_utc": "2026-01-17 09:12:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03hn23",
              "author": "Goldrake_Z",
              "text": "the problem is that it does it on my PC too, I guess it's a new cloudflare security system (on FBref)",
              "score": 1,
              "created_utc": "2026-01-17 12:46:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o053d0f",
          "author": "Afraid-Solid-7239",
          "text": "Pydoll \n\nPersonally using it for a turnstile api",
          "score": 3,
          "created_utc": "2026-01-17 17:46:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0lnk7s",
              "author": "Goldrake_Z",
              "text": "i'll try it. thanks",
              "score": 1,
              "created_utc": "2026-01-20 03:14:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00712j",
          "author": "Goldrake_Z",
          "text": "The full error  \n  \n  \n\\[16/01/26 23:21:21\\] INFO HTTP GET attempt 1/4 -> [https://fbref.com/en/comps/11/schedule/Serie-A-Stats-Scores-and-Fixtures](https://fbref.com/en/comps/11/schedule/Serie-A-Stats-Scores-and-Fixtures)\n\n\\[16/01/26 23:21:21\\] WARNING HTTP GET exception on attempt 1 for https://fbref.com/en/comps/11/schedule/Serie-A-Stats-Scores-and-Fixtures: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/11/schedule/Serie-A-Stats-Scores-and-Fixtures\n\n\\[16/01/26 23:21:21\\] INFO Response Status: 403\n\n\\[16/01/26 23:21:21\\] INFO Response Headers: {'Date': 'Fri, 16 Jan 2026 22:21:21 GMT', 'Content-Type': 'text/html; charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Connection': 'close', 'accept-ch': 'Sec-CH-UA-Bitness, Sec-CH-UA-Arch, Sec-CH-UA-Full-Version, Sec-CH-UA-Mobile, Sec-CH-UA-Model, Sec-CH-UA-Platform-Version, Sec-CH-UA-Full-Version-List, Sec-CH-UA-Platform, Sec-CH-UA, UA-Bitness, UA-Arch, UA-Full-Version, UA-Mobile, UA-Model, UA-Platform-Version, UA-Platform, UA', 'cf-mitigated': 'challenge', 'critical-ch': 'Sec-CH-UA-Bitness, Sec-CH-UA-Arch, Sec-CH-UA-Full-Version, Sec-CH-UA-Mobile, Sec-CH-UA-Model, Sec-CH-UA-Platform-Version, Sec-CH-UA-Full-Version-List, Sec-CH-UA-Platform, Sec-CH-UA, UA-Bitness, UA-Arch, UA-Full-Version, UA-Mobile, UA-Model, UA-Platform-Version, UA-Platform, UA', 'cross-origin-embedder-policy': 'require-corp', 'cross-origin-opener-policy': 'same-origin', 'cross-origin-resource-policy': 'same-origin', 'origin-agent-cluster': '?1', 'permissions-policy': 'accelerometer=(),browsing-topics=(),camera=(),clipboard-read=(),clipboard-write=(),geolocation=(),gyroscope=(),hid=(),interest-cohort=(),magnetometer=(),microphone=(),payment=(),publickey-credentials-get=(),screen-wake-lock=(),serial=(),sync-xhr=(),usb=()', 'referrer-policy': 'same-origin', 'server-timing': 'chlray;desc=\"9bf10160fef75602\", cfOrigin;dur=0,cfEdge;dur=7', 'x-content-type-options': 'nosniff', 'x-frame-options': 'SAMEORIGIN', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Set-Cookie': '\\_\\_cf\\_bm=c1Sa1AyKEq5Izb0qU\\_3R0LY7ZrClNMfXKlwISxno744-1768602081-1.0.1.1-5KzTMVFoBk3zIYyfFJqD\\_QaaclBIldzB.euuFoPPrOiZ5Zke0Yq\\_wJWEau82zxdYFz\\_i1v6v3qAqvYOaF674EYMZBSY.k4diBGmarmEmIXM; path=/; expires=Fri, 16-Jan-26 22:51:21 GMT; [domain=.fbref.com](http://domain=.fbref.com); HttpOnly; Secure; SameSite=None', 'Vary': 'Accept-Encoding', 'Server': 'cloudflare', 'CF-RAY': '9bf10160fef75602-MXP', 'Content-Encoding': 'gzip'}\n\n\\[16/01/26 23:21:21\\] INFO Response Body Snippet: <!DOCTYPE html><html lang=\"en-US\"><head><title>Just a moment...</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"><meta name=\"robots\" content=\"noindex,nofollow\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"><style>\\*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,\"Segoe \n\nUI\",Roboto,\"Helve\n\n\\[16/01/26 23:21:21\\] WARNING ‚ö†Ô∏è CLOUDFLARE CHALLENGE (Anti-Bot) per https://fbref.com/en/comps/11/schedule/Serie-A-Stats-Scores-and-Fixtures. Attendo 374s fino alle 23:27:35 prim\n\na di riprovare (1/4). \\[Stats: 1 total bans, 1 Cloudflare, 1 consecutive\\]",
          "score": 1,
          "created_utc": "2026-01-16 22:21:45",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o0adkev",
          "author": "Menxii",
          "text": "Did they ban your IP ? Does the website work when you use it normally ?",
          "score": 1,
          "created_utc": "2026-01-18 13:36:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0lnddm",
              "author": "Goldrake_Z",
              "text": "They didn't actually ban me, but they put me in a Cloudflare loop that I can't get out of (even if you click the checkbox). They \"flagged\" me (it's a terminal VPS, so I'm going by gut feeling).",
              "score": 1,
              "created_utc": "2026-01-20 03:13:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0di94r",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-18 22:54:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0fmnn4",
              "author": "webscraping-ModTeam",
              "text": "ü™ß Please review the sub rules üëâ",
              "score": 1,
              "created_utc": "2026-01-19 06:25:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0fomoh",
          "author": "skyline222",
          "text": "That‚Äôs cloudflare WAF some captcha solving services can help you gen the cf clearance cookie you need",
          "score": 1,
          "created_utc": "2026-01-19 06:42:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0lnhn6",
              "author": "Goldrake_Z",
              "text": "Even if I'm already \"flagged\" and I get into the cloudflare loop?",
              "score": 1,
              "created_utc": "2026-01-20 03:14:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qfigyu",
      "title": "Open Source Captcha to test scraping methods against",
      "subreddit": "webscraping",
      "url": "https://github.com/WebDecoy/FCaptcha",
      "author": "cport1",
      "created_utc": "2026-01-17 17:00:15",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.74,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1qfigyu/open_source_captcha_to_test_scraping_methods/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qddw3x",
      "title": "How do I scrape images from a website with server restrictions?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qddw3x/how_do_i_scrape_images_from_a_website_with_server/",
      "author": "MotivatedMommy",
      "created_utc": "2026-01-15 08:11:50",
      "score": 3,
      "num_comments": 6,
      "upvote_ratio": 0.62,
      "text": "My earlier post got removed when I mentioned a bunch of the steps I've tried because it included names of paid services. I'm going to rephrase and hopefully it will make sense.\n\nThere's a site that I want to scrape an image from. I'm starting with just one image so I don't have to worry about staggering call times. Anyway, when I manually inspect the image element in the browser, and then I click on the image source, I get a \"Referral Denied\" error saying \"you don't have permission to access ____ on this server\". I don't even know how to get the image manually, so I'm not sure how to get it with the scraper.\n\nI've been using a node library that starts with puppet, but I've also been using one that plays wright. Whenever I call \"await fetch()\", I get the error \"network response was not ok\". I've tried changing the user agent, adding extra http headers, and intercepting the request, but I still get the same error. I assume I'm not able to get the image because I'm not calling from that site directly, but since I can see the image on the page, I figure there has to be a way to save it somehow.\n\nI'm new to scraping, so I apologize if this sort of thing was asked before. No matter what I searched for, I couldn't find an answer that worked for me. Any advice is much appreciated ",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qddw3x/how_do_i_scrape_images_from_a_website_with_server/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nzpu8wu",
          "author": "Hour_Analyst_7765",
          "text": "This could possibly be due to multiple reasons.\n\nIt could be as simple as requiring to send a \"Referer\" header in your request to grab the image. Its probably their CDN whitelisted only certain domains that belong to the site. I hope this fixes it for you.\n\nHowever, a more advanced protection I've seen, is a system where the website served images with an one-time token in the URL. Those got consumed while loading the full page in the browser, so even if you send the 'correct' request it would still fail. I verified this by disabling images with uBlock origin, and then manually loading an image. This image loaded once, but after a refresh it failed. This confirmed for me that I could still scrape these, but I would have to load the article page and images from the exact same browser session.\n\nA mild annoyance was that these one-time use tokens meant the URL changes on each request, so you need to find your own labeling system to deduplicate them.",
          "score": 3,
          "created_utc": "2026-01-15 12:32:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqdgnk",
          "author": "domharvest",
          "text": "This is a classic **referrer protection** issue - the server is checking where the request is coming from and blocking direct image access. Here's how to handle it in plain JavaScript\n\nInstead of trying to `fetch()` the image URL directly, use Playwright to screenshot or download it **while you're on the page**:\n\n        // Method 1: Screenshot the image element\n        const imageElement = await page.locator('img[src*=\"whatever\"]');\n        await imageElement.screenshot({ path: 'image.png' });\n        \n        // Method 2: Get image as buffer and save\n        const image = await page.locator('img').first();\n        const buffer = await image.screenshot();\n        await fs.writeFile('image.png', buffer);\n        \n        // Method 3: Use CDP (Chrome DevTools Protocol) to intercept the actual image data\n        await page.route('**/*.{png,jpg,jpeg,gif,webp}', async route => {\n          const response = await route.fetch();\n          const buffer = await response.body();\n          await fs.writeFile('image.png', buffer);\n          await route.continue();\n        });",
          "score": 3,
          "created_utc": "2026-01-15 14:24:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpqmjl",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-15 12:07:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpwf8d",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-15 12:47:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzq6v1j",
          "author": "Coding-Doctor-Omar",
          "text": "Try checking the network tab. Chances are there are requests being made to some API to fetch those images.",
          "score": 1,
          "created_utc": "2026-01-15 13:49:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbqooc",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qbqooc/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-01-13 13:00:51",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 0.81,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levels‚Äîwhether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) üå±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1qbqooc/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nzcoqu8",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-13 13:54:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzcv4cb",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-01-13 14:27:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nze7qd6",
          "author": "psychic_shadow_lugia",
          "text": "Hello all,\n\nI need to get data on about 25000 linkedin profiles. The main thing I want it the education section of each profile (all schools as well as start and end date). But if I can get the full profile info it is even better\n\nI understand scraping linkedin is tricky.\n\nI looked into\n\nscraptable - but it is unable at getting the years\n\nlinkedapi - but the call limit (profile per minute) goes down as you use your credit, which eventually result in about 7 profiles per minute.\n\nI am ok paying for a service as well as coding one myself, and would love some ideas from past experience",
          "score": 1,
          "created_utc": "2026-01-13 18:26:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwmcwy",
              "author": "domharvest",
              "text": "LinkedIN ToS",
              "score": 1,
              "created_utc": "2026-01-16 11:59:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzsylu2",
          "author": "OtherwiseGroup3162",
          "text": "Web Scraping TOS\n\nin a site's terms of service, it says no web scraping. \n\nWould that mean both browser based RPA and Back end HTTP requests that were reverse engineered be included?",
          "score": 1,
          "created_utc": "2026-01-15 21:31:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qevknv",
      "title": "Suggestions on dealing with iCloud bans? MITM vs AppStore.",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qevknv/suggestions_on_dealing_with_icloud_bans_mitm_vs/",
      "author": "gotta_cache_em_all",
      "created_utc": "2026-01-16 23:15:15",
      "score": 3,
      "num_comments": 6,
      "upvote_ratio": 0.67,
      "text": "Anyone have any suggestions on this? It‚Äôs a bit annoying trying to watch network requests via MITM for mobile APIs when I keep constantly getting banned by Apple.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qevknv/suggestions_on_dealing_with_icloud_bans_mitm_vs/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o038tc4",
          "author": "You_Cant_Win_This",
          "text": "You (or whatever tool you are using) are doing something wrong. Apple should not be able to detect you intercepting requests or even if they do, that can't be bannable. Now if you are editing them midway..",
          "score": 2,
          "created_utc": "2026-01-17 11:35:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03j6ar",
              "author": "matty_fu",
              "text": "they can detect the request is not genuinely from an Apple device, eg. when the mitm proxy has intercepted & re-transmitted it using its own TLS settings, etc.",
              "score": 2,
              "created_utc": "2026-01-17 12:57:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o01mhaq",
          "author": "matty_fu",
          "text": "You‚Äôre getting banned by Apple for intercepting requests from non-Apple apps?\n\nWhat happens when they ban you, does the iPhone no longer work or you just cant login to your iCloud on the phone?",
          "score": 1,
          "created_utc": "2026-01-17 03:24:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01mr1x",
              "author": "gotta_cache_em_all",
              "text": "So the app in question was Chrono24. The iPhone is fine, I just can‚Äôt login to the iCloud account anymore. After getting banned on two different accounts, I gave up. I was curious how people are managing to get around this, because mobile APIs seem to not have as much defense as their web counterparts.",
              "score": 1,
              "created_utc": "2026-01-17 03:26:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o01pv10",
                  "author": "matty_fu",
                  "text": "Does Chrono24 have iCloud integration? Otherwise I‚Äôm not sure how Apple would be able to detect your MITM setup\n\nYou might be able to configure your MITM proxy to intercept only some domains - so you don‚Äôt end up intercepting any requests to Apple endpoints, only endpoints owned by the app developer",
                  "score": 3,
                  "created_utc": "2026-01-17 03:46:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mpe2v",
          "author": "Used-Comfortable-726",
          "text": "Give up on MITM for this.",
          "score": 1,
          "created_utc": "2026-01-20 07:40:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcume9",
      "title": "Built a scraper where crawling/scraping is one XPath expression",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qcume9/built_a_scraper_where_crawlingscraping_is_one/",
      "author": "fourhoarsemen",
      "created_utc": "2026-01-14 18:09:58",
      "score": 2,
      "num_comments": 0,
      "upvote_ratio": 0.63,
      "text": "This is wxpath's first public release, and I'd love feedback on the expression syntax, any use cases this might unlock, or anything else. \n\n\n**[wxpath](https://github.com/rodricios/wxpath)** is a declarative web crawler where traversal is expressed directly in XPath. Instead of writing imperative crawl loops, wxpath lets you describe what to follow and what to extract in a single expression (it's async under the hood; results are streamed as they‚Äôre discovered). \n\nBy introducing the `url(...)` operator and the `///` syntax, wxpath's engine can perform deep/recursive web crawling and extraction.\n\nFor example, to build a simple Wikipedia knowledge graph: \n\n\n    import wxpath\n\n    path_expr = \"\"\"\n    url('https://en.wikipedia.org/wiki/Expression_language')\n     ///url(//main//a/@href[starts-with(., '/wiki/') and not(contains(., ':'))])\n     /map{\n        'title': (//span[contains(@class, \"mw-page-title-main\")]/text())[1] ! string(.),\n        'url': string(base-uri(.)),\n        'short_description': //div[contains(@class, 'shortdescription')]/text() ! string(.),\n        'forward_links': //div[@id=\"mw-content-text\"]//a/@href ! string(.)\n     }\n    \"\"\"\n\n    for item in wxpath.wxpath_async_blocking_iter(path_expr, max_depth=1):\n        print(item)\n\n\nOutput:\n\n\n    map{'title': 'Computer language', 'url': 'https://en.wikipedia.org/wiki/Computer_language', 'short_description': 'Formal language for communicating with a computer', 'forward_links': ['/wiki/Formal_language', '/wiki/Communication', ...]}\n    map{'title': 'Advanced Boolean Expression Language', 'url': 'https://en.wikipedia.org/wiki/Advanced_Boolean_Expression_Language', 'short_description': 'Hardware description language and software', 'forward_links': ['/wiki/File:ABEL_HDL_example_SN74162.png', '/wiki/Hardware_description_language', ...]}\n    map{'title': 'Machine-readable medium and data', 'url': 'https://en.wikipedia.org/wiki/Machine_readable', 'short_description': 'Medium capable of storing data in a format readable by a machine', 'forward_links': ['/wiki/File:EAN-13-ISBN-13.svg', '/wiki/ISBN', ...]}\n    ...\n\n---\n\nThe target audience is anyone who: \n\n1. wants to quickly prototype and build web scrapers\n2. familiar with XPath or data selectors\n3. builds datasets (think RAG, data hoarding, etc.)\n4. wants to study link structure of the web (quickly) i.e. web network scientists\n\n---\n\nFor comparison, with Scrapy, you would...\n\n\n    import scrapy\n\n    class QuotesSpider(scrapy.Spider):\n        name = \"quotes\"\n        start_urls = [\n            \"https://quotes.toscrape.com/tag/humor/\",\n        ]\n\n        def parse(self, response):\n            for quote in response.css(\"div.quote\"):\n                yield {\n                    \"author\": quote.xpath(\"span/small/text()\").get(),\n                    \"text\": quote.css(\"span.text::text\").get(),\n                }\n\n            next_page = response.css('li.next a::attr(\"href\")').get()\n            if next_page is not None:\n                yield response.follow(next_page, self.parse)\n\n\nThen from the command line, you would run:\n\n\n    scrapy runspider quotes_spider.py -o quotes.jsonl\n\n\n**wxpath** gives you two options: write directly from a Python script or from the command line.\n\n\n    from wxpath import wxpath_async_blocking_iter \n    from wxpath.hooks import registry, builtin\n\n    path_expr = \"\"\"\n    url('https://quotes.toscrape.com/tag/humor/', follow=//li[@class='next']/a/@href)\n      //div[@class='quote']\n        /map{\n          'author': (./span/small/text())[1],\n          'text': (./span[@class='text']/text())[1]\n          }\n\n\n    registry.register(builtin.JSONLWriter(path='quotes.jsonl'))\n    items = list(wxpath_async_blocking_iter(path_expr, max_depth=3))\n\n\nor from the command line:\n\n\n    wxpath --depth 1 \"\\\n    url('https://quotes.toscrape.com/tag/humor/', follow=//li[@class='next']/a/@href) \\\n      //div[@class='quote'] \\\n        /map{ \\\n          'author': (./span/small/text())[1], \\\n          'text': (./span[@class='text']/text())[1] \\\n          }\" > quotes.jsonl\n\n\n\n---\n\n\n\nGitHub: https://github.com/rodricios/wxpath\n\nPyPI: pip install wxpath",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qcume9/built_a_scraper_where_crawlingscraping_is_one/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    }
  ]
}