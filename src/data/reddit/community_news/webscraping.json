{
  "metadata": {
    "last_updated": "2026-01-10 08:50:01",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 27,
    "total_comments": 149,
    "file_size_bytes": 170769
  },
  "items": [
    {
      "id": "1q76eo1",
      "title": "Built an HTTP client that matches Chrome's JA4/Akamai fingerprint",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q76eo1/built_an_http_client_that_matches_chromes/",
      "author": "sardanioss",
      "created_utc": "2026-01-08 08:24:19",
      "score": 33,
      "num_comments": 37,
      "upvote_ratio": 0.91,
      "text": "Most of the HTTP clients like requests in python gets easily flagged by Cloudflare and such. Specially when it comes to HTTP/3 there are almost no good libraries which has native spoofing like chrome. So I got a little frustated and had built this library in Golang. It mimics chrome from top to bottom in all protocols. This is still definitely not fully ready for production, need a lot of testing and still might have edge cases pending. But please do try this and let me know how it goes for you - [https://github.com/sardanioss/httpcloak](https://github.com/sardanioss/httpcloak) \n\n  \nThanks to cffi bindings, this library is available in Python, Golang, JS and C#\n\nIt mimics Chrome across HTTP/1.1, HTTP/2, and HTTP/3 - matching JA4, Akamai hash, h3\\_hash, and ECH. Even does the TLS extension shuffling that Chrome does per-connection.. Won't help if they're checking JS execution or browser APIs - you'd need a real browser for that. \n\nIf there is any feature missing or something you'd like to get added just lemme know. I'm gonna work on  tcp/ip fingerprinting spoofing too once this lib is stable enough. \n\nIf this is useful for you or you like it then please give it a star, thankyou!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q76eo1/built_an_http_client_that_matches_chromes/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nye4vyp",
          "author": "kiwialec",
          "text": "Nice work, looks great. Open to a pr to introduce ESM module concepts to the nodejs client?",
          "score": 5,
          "created_utc": "2026-01-08 13:13:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nye8dht",
              "author": "sardanioss",
              "text": "Definitely, lemme implement async first and then I'll do this, will be better that way.",
              "score": 3,
              "created_utc": "2026-01-08 13:33:37",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyhnc79",
              "author": "sardanioss",
              "text": "It has now been implemented in the version 1.5.1 please update it and use it. Lemme know if you face any issues, thankyou!",
              "score": 2,
              "created_utc": "2026-01-08 22:49:18",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyql4h7",
              "author": "Bmaxtubby1",
              "text": "Definitely open to it ESM support would be a great addition. Happy to review a PR anytime.",
              "score": 1,
              "created_utc": "2026-01-10 05:22:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nydipkt",
          "author": "PTBKoo",
          "text": "Canvas spoofing, I use rnet which does most of what ur library does but canvas spoofing",
          "score": 2,
          "created_utc": "2026-01-08 10:32:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydkf0q",
              "author": "sardanioss",
              "text": "Canvas fingerprint spoofing requires JS runtime support (basically browser environment) which an HTTP client cannot do at all. Rnet does have websocket support and async but they lack HTTP/3 protocol which is available in mine. I'll be implementing websockets and async too soon.",
              "score": 8,
              "created_utc": "2026-01-08 10:47:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyg3pg8",
                  "author": "tunabr",
                  "text": "Apart from my take at https://github.com/gleicon/browserhttp as a fallback, my only guess for canvas and other fingerprinting would be adapt mozilla js runtime as a middleware to interpret part of the requestâ€¦",
                  "score": 1,
                  "created_utc": "2026-01-08 18:42:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyekuf0",
              "author": "CobbwebBros",
              "text": "rnet does not so canvas spoofing",
              "score": 1,
              "created_utc": "2026-01-08 14:38:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyg090b",
          "author": "wwwhiterabittt",
          "text": "Nice work and thanks for the tls.peet.ws shoutout",
          "score": 2,
          "created_utc": "2026-01-08 18:28:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyg78i7",
              "author": "sardanioss",
              "text": "Thankyou! [tls.peet.ws](http://tls.peet.ws) helped me a lot! Although I wonder why tcp/ip fingerprinting was not available.",
              "score": 2,
              "created_utc": "2026-01-08 18:57:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyhaudy",
                  "author": "wwwhiterabittt",
                  "text": "It broke a while ago, not sure why. Need to get around and fix it, as well as adding HTTP/3 and Quic",
                  "score": 1,
                  "created_utc": "2026-01-08 21:52:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyemawn",
          "author": "renegat0x0",
          "text": "Looks similar to [https://github.com/arman-bd/httpmorph](https://github.com/arman-bd/httpmorph)",
          "score": 1,
          "created_utc": "2026-01-08 14:45:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyenp7c",
              "author": "sardanioss",
              "text": "It doesn't have HTTP/3, usually latest sites and sites served via cds usually use this latest protocol.",
              "score": 1,
              "created_utc": "2026-01-08 14:52:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyemiw8",
          "author": "HexagonWin",
          "text": "is this better than curl-impersonate?",
          "score": 1,
          "created_utc": "2026-01-08 14:47:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyen6ne",
              "author": "sardanioss",
              "text": "It is definitely better and works with latest chrome version/settings.",
              "score": 1,
              "created_utc": "2026-01-08 14:50:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyeq2bx",
          "author": "renegat0x0",
          "text": "My analysis results for python:\n\n \\- sometimes slow. Curlcffi returns around 1.0 second for my raspberry pi. HttpCloak sometimes returns after 6.0s. I am running a crawler in this machine, so take it with a grain of salt\n\n \\- it does not follow redirects. [https://google.com](https://google.com) returns redirect status code\n\n \\- does not support cookies, or 'verify' params, as requests, which makes this API a funny toy, rather than something useful. Can I pass proxies?",
          "score": 1,
          "created_utc": "2026-01-08 15:04:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyeyipb",
              "author": "sardanioss",
              "text": "Not sure how you are testing. First of all I tested with the given link, ran 20 requests and this is the result:  \n  \n**Without Sessions**  \n  \nÂ \\- **httpcloak** \\- 0.771s avg (min 0.350s)  \nÂ \\- **curl\\_cffi** \\- 2.260s avg (min 1.063s)  \nÂ \\- **requests** \\- 3.011s avg (min 1.426s)  \n  \n**With Sessions**  \n  \nÂ \\- **httpcloak** \\- 0.838s avg (min 0.340s)  \nÂ \\- **curl\\_cffi** \\- 0.716s avg (min 0.362s)  \nÂ \\- **requests** \\- 0.693s avg (min 0.357s)  \n  \nSo it is way faster than curl\\_cuffi in direct requests without sessions.\n\n  \nApart from this yes you are right about the redirect, there is a bug over there, thankyou for pointing that out. Lastly the bs about cookies and verify and such is completely wrong, it supports them and works with them without any issue at all. And yes it does supports proxies. Lastly if you read my post carefully - \"This is still definitely not fully ready for production, need a lot of testing and still might have edge cases pending. But please do try this and let me know how it goes for you\" I never said this is ready for production and I want people to use it so they may let me know the issues present in it and fix them properly. I'm really happy and thankful that you pointed out the redirection issue but I guess not everyone knows how to talk properly.",
              "score": 2,
              "created_utc": "2026-01-08 15:43:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyf27wb",
                  "author": "renegat0x0",
                  "text": "python requests .get function should accept \"cookies\", \"verify\" arguments. This funny package .get function does not. Before calling any response bs you should carefully check if you are full of sh$t",
                  "score": -2,
                  "created_utc": "2026-01-08 15:59:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyg3paj",
          "author": "sussinbussin",
          "text": "This is awesome dude, keep up the good work",
          "score": 1,
          "created_utc": "2026-01-08 18:42:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyg7c72",
              "author": "sardanioss",
              "text": "Thankyou very much! Please do use it and lemme know if you'd like to see any feature being added.",
              "score": 1,
              "created_utc": "2026-01-08 18:58:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyg3w8m",
          "author": "tunabr",
          "text": "neat ! following up for the async plans",
          "score": 1,
          "created_utc": "2026-01-08 18:43:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyg7jrx",
              "author": "sardanioss",
              "text": "Definitely! I'm already done with async part, just testing it thoroughly and fixing some minor bugs (which are not minor in reality ðŸ« ðŸ« )",
              "score": 1,
              "created_utc": "2026-01-08 18:59:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyg86uq",
                  "author": "tunabr",
                  "text": "I figured out, thereâ€™s a impedance mismatch there. Lmk if I can be of help testing or elsewhere.",
                  "score": 1,
                  "created_utc": "2026-01-08 19:02:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyhafvx",
          "author": "LordFarquad777_",
          "text": "Oh wow! This is amazing dude, thank you so much for sharing your knowledge with the community!!",
          "score": 1,
          "created_utc": "2026-01-08 21:51:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyk5fkd",
              "author": "sardanioss",
              "text": "Thanks a lot! Please do try out the library for your next project and lemme know how it goes!",
              "score": 1,
              "created_utc": "2026-01-09 07:48:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyj884o",
          "author": "Eastern_Ad_9018",
          "text": "**Does it support carrying custom TLS fingerprints?**",
          "score": 1,
          "created_utc": "2026-01-09 03:48:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyjsygl",
              "author": "Eastern_Ad_9018",
              "text": "I've tried it, and it's really impressive.",
              "score": 1,
              "created_utc": "2026-01-09 06:04:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyk5hyz",
                  "author": "sardanioss",
                  "text": "Thankyou!",
                  "score": 1,
                  "created_utc": "2026-01-09 07:49:06",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyk5h1k",
              "author": "sardanioss",
              "text": "It does support that, it handles all of it internally",
              "score": 1,
              "created_utc": "2026-01-09 07:48:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q6pxwn",
      "title": "Just Started Web Scraping â€” Is This a Good Start?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q6pxwn/just_started_web_scraping_is_this_a_good_start/",
      "author": "franik33",
      "created_utc": "2026-01-07 20:01:30",
      "score": 19,
      "num_comments": 17,
      "upvote_ratio": 0.88,
      "text": "Hi everyone,\n\nI started getting into web scraping about 3â€“4 days ago. I already have some solid experience with Python, and my first scraping project was a public website. I managed to collect around 7,000 records and everything worked as expected.\n\nIâ€™m curious whether this is considered a decent start for someone new to scraping, or if itâ€™s fairly basic stuff.  \nAlso, Iâ€™d like to hear honest opinions: is web scraping still worth investing time in today (for projects, automation, or monetization), or is it becoming a waste of time due to market saturation and restrictions?\n\nAny real-world experiences or insights would be appreciated.\n\nThanks in advance.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q6pxwn/just_started_web_scraping_is_this_a_good_start/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nya2b7i",
          "author": "hikingsticks",
          "text": "Check out John Watson Rooney on YouTube, his whole channel focuses on webscraping techniques and skills.\n\nThe difficulty varies enormously between different websites depending on the level of protection they have. Some will allow you to essentially ddos them without blocking you (don't do this), others will require complex implementations for even a single request.\n\nIt can be good fun, and useful if you have some data you want. Normalisation, aggregation, and retrieval are all part of the pipeline.",
          "score": 20,
          "created_utc": "2026-01-07 21:41:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfjrfq",
          "author": "hasdata_com",
          "text": "Nice work for 3 days in. But yeah, context matters, scraping a static site vs a protected one is night and day. Just be ready for the maintenance, scripts break all the time. Still worth learning imo",
          "score": 10,
          "created_utc": "2026-01-08 17:16:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9g1pq",
          "author": "coolcosmos",
          "text": "Yeah it's a start. Don't worry too much. Try many different sites, learn about apis, different CMSs like WordPress etc...",
          "score": 7,
          "created_utc": "2026-01-07 20:05:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyahq4m",
          "author": "davak72",
          "text": "7,000 records isnâ€™t too much, but it sure beats copying and pasting!!\n\nThe true test of your scraping skills involves error handling over the long term, especially if you have a deployed scraper rather than just a local script. Also, many sites that require a login or that list items for sale are much more protective against scraping, so there are additional skills and techniques required to bypass various levels of protection",
          "score": 3,
          "created_utc": "2026-01-07 22:51:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nypd7l2",
              "author": "JohnnyOmmm",
              "text": "Whatâ€™s a good cpu for 400000 records? Iâ€™m making. A bot for a certain shopping site and it takes me like 13 hours to parse through the listings",
              "score": 1,
              "created_utc": "2026-01-10 01:00:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny9x89h",
          "author": "jellospitr",
          "text": "I need to start doing some scrapingâ€¦ or learn how to do it. Iâ€™d love to merge data from multiple sources into a common library. â€¦  one dayâ€¦.",
          "score": 2,
          "created_utc": "2026-01-07 21:19:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nya84p0",
          "author": "Agreeable-Bug-4901",
          "text": "Iâ€™ve always been interested in doing my own â€œwatch for dealsâ€ tool and could never get the scraping part to work based on like search terms, with like a few static sites to scrape from (e.g. Amazon, Best Buy, etc)",
          "score": 1,
          "created_utc": "2026-01-07 22:07:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyazqvx",
          "author": "datamizer",
          "text": "It's definitely saturated, at least in freelancing terms. Lots of freelancers from Indonesia, India, Central / South America, Eastern Europe have gotten into it in the past 5-10 years and they do it for cheaper than \"first world\" (sorry, not sure on what the best term is here) countries due to cost of living.\n\nFor most clients, cost is their primary concern, then quality, then speed. If they've had quality issues before, quality will be their biggest concern.\n\nThere's still a huge demand for data, and most SaaS products today are data backed. Those are recurring scrapes where data is pushed hourly or daily into databases for further processing. There's a lot of demand for scraping products that have barriers like LinkedIn, TikTok etc. because they are annoying to deal with and are actively hostile to scraping (which is their right).",
          "score": 1,
          "created_utc": "2026-01-08 00:21:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyd0sg4",
              "author": "Coding-Doctor-Omar",
              "text": "It is saturated but not as much as web dev.",
              "score": 1,
              "created_utc": "2026-01-08 07:50:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyd4u3j",
                  "author": "datamizer",
                  "text": "Absolutely agree with that.",
                  "score": 1,
                  "created_utc": "2026-01-08 08:26:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyfa2v8",
          "author": "jagdish1o1",
          "text": "Pursuing web scraping as a sole career is a BIG NO but having web scraping skill in your arsenal is a good thing. Getting public data is so easy that so many extensions and tools are there to do it. \n\nThe real-world web scraping is involve cracking web architecture & systems, what i mean by that is scraping data that is hidden behind walls it's a grey area, unspoken policies and practices. \n\nI have done so many web scraping projects, from basic to advance. In today's AI era, basic scraping not gonna cut it, you need to do learn agentic scraping fancy name for incorporating AI in your web scraping flow. \n\nI'll give you an example, i've done a project where i need to scrape storage units from images which were posted on a website, and the jerry on top was, it was a dynamic website not plain HTML/CSS. Getting the images was the first road-block than i had to extract units from images. Guess what? I trained a custom AI model just for this. \n\nYou need to start thinking like a backend-developer to crack the systems but i will not suggest to purse web scraping as a sole career, include automations and a little web-development as well. \n\nGood luck!",
          "score": 1,
          "created_utc": "2026-01-08 16:34:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfesuw",
              "author": "franik33",
              "text": "Thanks for comment.\nMy main focus is cybersecurity, but Iâ€™m looking for a side skill for some extra income (around 200â€“500$). Iâ€™m considering web scraping, so Iâ€™d like an honest opinion is it still worth learning scraping today for that kind of side income, or is it better to skip it and focus on something else?",
              "score": 1,
              "created_utc": "2026-01-08 16:54:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nypcxae",
          "author": "JohnnyOmmm",
          "text": "Thatâ€™s the most vague info Iâ€™ve ever read on this sub",
          "score": 1,
          "created_utc": "2026-01-10 00:59:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nype9rc",
              "author": "franik33",
              "text": "Ok, skip",
              "score": 1,
              "created_utc": "2026-01-10 01:06:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nylnjt4",
          "author": "domharvest",
          "text": "7k records in your first week is solid. You're past the \"hello world\" phase, which is more than most people who say they want to learn scraping.\n\n**Is it worth it?** Depends what you want:\n\n* **Freelancing:** Still decent demand on Upwork/Fiverr. Rates vary wildly ($15-100+/hour depending on complexity). Anti-bot tech is getting harder, so basic skills aren't enough anymore.\n* **Personal projects:** Absolutely worth it. Data pipelines, price monitoring, research - scraping is a tool, not a career by itself.\n* **Job market:** Rare as a standalone role. More valuable as part of data engineering or automation skills.\n\n**Reality check:** Static HTML scraping is the easy part. The real skills that pay are:\n\n* Handling JavaScript-heavy sites (Playwright/Puppeteer)\n* Bypassing anti-bot systems ethically\n* Building reliable, maintainable pipelines\n* Knowing when scraping isn't the right solution\n\n**Next steps if you're serious:**\n\n* Try a site that requires login or pagination\n* Learn Playwright for dynamic content\n* Build something that runs daily without breaking\n* Contribute to an open-source scraping tool\n\nThe market isn't saturated with *good* scrapers. It's saturated with people who can write a BeautifulSoup script.\n\nWhat kind of site did you scrape? Static or dynamic?",
          "score": 0,
          "created_utc": "2026-01-09 14:29:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q47k94",
      "title": "How much does webscraping cost?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q47k94/how_much_does_webscraping_cost/",
      "author": "TangerineBetter855",
      "created_utc": "2026-01-05 01:08:09",
      "score": 16,
      "num_comments": 23,
      "upvote_ratio": 0.72,
      "text": "is it possible to scrape large sites like youtube or tinder and is scraping apps possible or is it only sites?",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1q47k94/how_much_does_webscraping_cost/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nxqhzb5",
          "author": "zoe_is_my_name",
          "text": "on cost: depends. a small py script running in the background of your already running running pc can be enough for many sites and is practically free. if you want to get around any bot or scraping detection it can get quite expensive using full browsers and proxies.\n\non big sites: yes, of course. the datas gotta come from somwhere after all. it might not be as easy, but certainly possible. its expected that they have better protection, but no protection can ever be perfect.\n\non apps: yes, of course. the datas gotta come from somwhere after all. debugging might be a bit harder, requiring MITMs and admin access on devices instead of a browsers dev console",
          "score": 12,
          "created_utc": "2026-01-05 01:23:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqufrz",
          "author": "RandomPantsAppear",
          "text": "Anything that can be seen by your device can be scraped.  It is literally always possible, 100% of the time. \n\nIt is impossible to know how much it will cost without knowing the target and their protection measures. \n\nEnough time, enough money, and you will have a mechanical hand with hotdogs fingers pressing on screens if that is what is needed.",
          "score": 11,
          "created_utc": "2026-01-05 02:29:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz0lv1",
              "author": "No-Business-7545",
              "text": "this",
              "score": 2,
              "created_utc": "2026-01-06 07:57:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxqw0cf",
          "author": "Ready-Interest-1024",
          "text": "Itâ€™s possible, sites like tinder can be challenging and they will ban you if they find out. But like another comment said, itâ€™s always possible. You just need to figure out how far youâ€™re willing to go",
          "score": 4,
          "created_utc": "2026-01-05 02:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsexod",
          "author": "MapLow2754",
          "text": "for youtube you can use gcloud api with few accounts, afaik 10k request is allowed in free tier.\n\ndepending on your usage you can utilized 10 accounts and get 100k request daily. for tinder i don't know anything about it.",
          "score": 4,
          "created_utc": "2026-01-05 09:12:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsso4a",
          "author": "HockeyMonkeey",
          "text": "Yes, YouTube or Tinder can be scraped. They're also designed to detect automation aggressively. Expect rotating IPs, session handling, and frequent breakage.  \n\nFor platforms like YouTube, official APIs exist and should be your first stop. They're predictable, cheaper, and much easier to justify professionally. Many freelancers build solid work around API-based data before touching scraping at all.\n\nApps are also scrapeable since they talk to backend APIs, but reverse engineering them adds legal and ethical risk.\n\nSo, In terms of Cost: \"Our wallet sets the Limit\"",
          "score": 4,
          "created_utc": "2026-01-05 11:16:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs9y3e",
          "author": "Hour_Analyst_7765",
          "text": "Its always possible, but it depends on how much effort you spend into it, and whether you have the networking facilities.\n\n(Large) sites don't want to be scraped, because their data holds value to themselves or other parties (AI AI AI). So it may result in your IP being banned or rate limited, meaning you need lots of IPs to scrape these sites.\n\nSome sites may get littered with captcha's as a protection means as well, which is an additional hurdle to solve automatically (either via scripts or paid human solvers).\n\nAll of this costs money. Running the servers to run a script to process hundreds of thousands of pages is often not the issue.",
          "score": 2,
          "created_utc": "2026-01-05 08:24:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxswvt5",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-01-05 11:50:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxtu3cc",
              "author": "anupam_cyberlearner",
              "text": "Thats gr8 . Pls tell the framework",
              "score": 1,
              "created_utc": "2026-01-05 15:14:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxtx3tr",
                  "author": "Ok-Depth-6337",
                  "text": "Pure python. \nNo custom libraries",
                  "score": 1,
                  "created_utc": "2026-01-05 15:28:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxtusuh",
              "author": "webscraping-ModTeam",
              "text": "ðŸª§ Please review the sub rules ðŸ‘‰",
              "score": 1,
              "created_utc": "2026-01-05 15:17:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxqwhmi",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-05 02:40:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxrh3m9",
              "author": "webscraping-ModTeam",
              "text": "ðŸ‘” Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
              "score": 1,
              "created_utc": "2026-01-05 04:36:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxsf773",
          "author": "divided_capture_bro",
          "text": "Yes",
          "score": 1,
          "created_utc": "2026-01-05 09:14:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsuu59",
          "author": "NordinCoding",
          "text": "From my experience, if you need headless browsers and proxies in order to evade bot detection it costs way more than its worth but if you can figure out a way to run a script on your own PC without proxies and it works it''ll practically be free",
          "score": 1,
          "created_utc": "2026-01-05 11:34:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxu3o57",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-05 15:59:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxu9h0t",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-05 16:26:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxu7cey",
          "author": "Fragrant_Ad3054",
          "text": "It depends. To scrape a simple web page, you can do it with a microcontroller for $8. There are always more expensive options depending on what you want to do and your goals. It depends on your knowledge, your objectives, and your budget.\n\nBut generally, web scraping is financially accessible to anyone who already has a desktop computer, even if it's a clunker.",
          "score": 1,
          "created_utc": "2026-01-05 16:16:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny30g8d",
          "author": "PursuingMorale",
          "text": "Scraping something like all of YouTube is only doable if you have hundreds of millions of dollars. Half to pay for the scraping and the other half to defend against the law suits.",
          "score": 1,
          "created_utc": "2026-01-06 21:36:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6kn5n",
          "author": "bluemangodub",
          "text": "> is it possible to scrape large sites like youtube or tinder\n\nyes\n\n> and is scraping apps possible or is it only sites?\n\nyes, both.\n\n\nIs it easy. No. Can any one do it. No.\n\n> Cost\n\nIt will cost $(x)\n\n\nhth",
          "score": 1,
          "created_utc": "2026-01-07 11:27:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6u95z",
          "author": "banedlol",
          "text": "47",
          "score": 1,
          "created_utc": "2026-01-07 12:37:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0fqza",
      "title": "Deploying scrapers",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q0fqza/deploying_scrapers/",
      "author": "async-lambda",
      "created_utc": "2025-12-31 15:34:08",
      "score": 14,
      "num_comments": 36,
      "upvote_ratio": 0.89,
      "text": "I know this is, asking a question in very bad faith. I'm a student and I dont have money to spend.\n\nIs there a way I can deploy a headless browser for free? what i mean to ask is, having the convenience to hit an endpoint, and for it to run the scraper and show me results. Its just for personal use. Any services that offer this- or have a generous free tier?\n\nI can learn/am willing to learn stacks, am familiar with most web driver runners selenium/scrapy/playwright/cypress/puppeteer.\n\nThanks for reading \n\nEdit: tasks that I require are very minimal, 2-3 requests per day, with a few button clicks ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q0fqza/deploying_scrapers/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwxgh41",
          "author": "v_maria",
          "text": "Nothing is free. You can host on your own computer for the price of electricity",
          "score": 14,
          "created_utc": "2025-12-31 15:41:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxrjtos",
              "author": "Shekher_05",
              "text": "So that is the only free method ?",
              "score": 1,
              "created_utc": "2026-01-05 04:52:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxsfu1w",
                  "author": "v_maria",
                  "text": "'free' yes",
                  "score": 1,
                  "created_utc": "2026-01-05 09:20:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwxgijg",
          "author": "No-Appointment9068",
          "text": "Pick up an old laptop for free or get a raspberry pi or something?",
          "score": 6,
          "created_utc": "2025-12-31 15:41:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxmr8e",
          "author": "divided_capture_bro",
          "text": "GitHub Actions with public repo.",
          "score": 5,
          "created_utc": "2025-12-31 16:12:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxkdeh",
          "author": "816shows",
          "text": "Don't spin up a full ec2 instance for something you run 2-3 times a day.  Instead build an AWS lambda that you can either trigger on demand via an API gateway call or tie to an Eventbridge schedule. The container that you deploy in the lambda is quite simple, you don't need a lot of sophisticated layers just to have selenium and your script to work.  Hit me up if you want a sample Dockerfile and config details.",
          "score": 5,
          "created_utc": "2025-12-31 16:00:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy126g",
              "author": "Ordinary-Coconut7752",
              "text": "hey, would you mind sending me your docker and config files? Wanna try to deploy my scrapers on Lambda",
              "score": 1,
              "created_utc": "2025-12-31 17:23:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwya8sh",
                  "author": "816shows",
                  "text": "Done!",
                  "score": 1,
                  "created_utc": "2025-12-31 18:08:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwyexmy",
              "author": "73tada",
              "text": "Same here, please! I need an excuse to learn lambda!",
              "score": 1,
              "created_utc": "2025-12-31 18:32:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwypzg5",
                  "author": "816shows",
                  "text": "Sure thing - I added some notes on the configuration and for the uninitiated, you will have to create the new function based on a container image. This means you'll also have to get the Elastic Container Registry setup, and the rest of the details are outlined in this repo:  \n  \n[https://github.com/816shows/public/tree/main/selenium-lambda](https://github.com/816shows/public/tree/main/selenium-lambda)",
                  "score": 2,
                  "created_utc": "2025-12-31 19:28:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwyi4mx",
              "author": "pachinkomachine101",
              "text": "This sounds interesting, could you share it with me too? I'd love to study your setup!",
              "score": 1,
              "created_utc": "2025-12-31 18:48:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxino6",
          "author": "yangshunz",
          "text": "You can run Puppeteer on Vercel functions",
          "score": 2,
          "created_utc": "2025-12-31 15:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxreyz",
          "author": "RandomPantsAppear",
          "text": "AWS has a free tier for EC2, micro instances I believe.Â \n\nFor very low cost you can also use a lambda outside of a vpc I believe (to dodge nat and internet gateway costs). Highly recommend Zappa for flask/django. Donâ€™t forget to protect those endpoints.Â \n\nAnother super low cost setup that I havenâ€™t implemented but could work is an ec2 trigger on S3 uploads. Make the file your url list. Make the upload trigger a script that launches a small fargate instance that shuts down when itâ€™s done. 0.25 vcpu and 256m of ram. That will cost you less than 2 cents per hour.Â ",
          "score": 2,
          "created_utc": "2025-12-31 16:35:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxhyd3",
          "author": "goranculibrk",
          "text": "Amazon should have free tier with some ec2 micro instance. Maybe look into that?",
          "score": 1,
          "created_utc": "2025-12-31 15:48:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxv4v6",
          "author": "Low-Clerk-3419",
          "text": "You can easily deploy something like that in vercel functions, fly.io, railway etc. but you have to keep in mind those free tiers are not meant for scraping; it will be very slow and limited experience.",
          "score": 1,
          "created_utc": "2025-12-31 16:54:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy61v9",
          "author": "Daniel_triathlete",
          "text": "Canâ€™t you handle this task with JDownloader and an old laptop?",
          "score": 1,
          "created_utc": "2025-12-31 17:48:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy70bv",
          "author": "Intelligent_Area_135",
          "text": "Just wrap an endpoint around it and run it on your computer",
          "score": 1,
          "created_utc": "2025-12-31 17:52:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy79vv",
              "author": "Intelligent_Area_135",
              "text": "I use express js as the api around my web scraper, but if you are concerned about your ip getting banned or something, I have deployed to gcp and itâ€™s very cheap but I think there might be better options",
              "score": 1,
              "created_utc": "2025-12-31 17:54:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwybc31",
          "author": "_i3urnsy_",
          "text": "I think GitHub Actions can do this for free if you are cool with the repo being public",
          "score": 1,
          "created_utc": "2025-12-31 18:14:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwycbfh",
          "author": "automationexperts",
          "text": "Try here https://www.pythonanywhere.com/",
          "score": 1,
          "created_utc": "2025-12-31 18:18:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx104rn",
          "author": "hatemjaber",
          "text": "Oracle has a free tier, 4 CPU and 24 GB ram. You can create a couple VMs.",
          "score": 1,
          "created_utc": "2026-01-01 03:40:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6tw33",
          "author": "Rorschache00714",
          "text": "Github offers a student essentials package with a shit ton of free resources. Look into that if you have a school email you can use.",
          "score": 1,
          "created_utc": "2026-01-02 02:59:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8237q",
          "author": "Training-Dinner3340",
          "text": "Depending on what youâ€™re doing, Cloudflare Workers + Browser Rendering may get the job done:\nhttps://developers.cloudflare.com/browser-rendering/\n\nFree tier is okay:\nhttps://developers.cloudflare.com/browser-rendering/pricing/\n\nChatGPT is decent at writing Cloudflare worker scripts. Good luck!",
          "score": 1,
          "created_utc": "2026-01-02 08:37:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9kwa4",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-02 15:24:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9rg0u",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-02 15:55:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxbchys",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-02 20:23:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxckia9",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-03 00:09:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdklgv",
          "author": "Foodforbrain101",
          "text": "I could have sworn that GitHub Actions previously had a \na certain amount of free minutes even for private repos. \n\nRegardless, you can also take a look at using Azure DevOps' equivalent, Azure Pipelines, with 1800 minutes free per month, 60 min per run max for private repos but you have to request it (which is fairly easy and quick).\n\nIf you do go with Azure Pipelines, I suggest using the Microsoft Playwright for Python container image for your pipeline. There's ways to make this setup more metadata-driven too, as you can parameterize Azure Pipelines and use the Azure DevOps API to trigger runs, and you can easily tack on Azure Logic Apps (4000 free actions per month) as a simple orchestrator, use any kind of blob or table storage (even Google Drive) to store and fetch your metadata table containing the schedules and info about which scripts to run. Might be overkill for your needs though, but it's honestly one of the easiest and cheapest ways I've found to run headless browsers.",
          "score": 1,
          "created_utc": "2026-01-03 03:38:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxe6d6k",
          "author": "BeigeBolt",
          "text": "What is most money made by scraping",
          "score": 1,
          "created_utc": "2026-01-03 06:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxnc8d5",
          "author": "Ok_Sir_1814",
          "text": "People saying it costs money when you can use Google colabs and run the script whenever you need to execute the scrapping.",
          "score": 1,
          "created_utc": "2026-01-04 16:30:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxo71py",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-04 18:48:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxol7ag",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-04 19:51:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxrj1lz",
          "author": "convicted_redditor",
          "text": "Yes. Fastapi project",
          "score": 1,
          "created_utc": "2026-01-05 04:48:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxw7epz",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-05 21:47:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxw7rcr",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-05 21:49:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q1y5gh",
      "title": "Is human-like automation actually possible today",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q1y5gh/is_humanlike_automation_actually_possible_today/",
      "author": "learning_linuxsystem",
      "created_utc": "2026-01-02 13:07:07",
      "score": 14,
      "num_comments": 11,
      "upvote_ratio": 0.9,
      "text": "Iâ€™m trying to understand the limits of collecting publicly available information from online platforms (social networks, professional networks, job platforms, etc.), especially for **OSINT, market analysis, or workforce research**.\n\nWhen attempting to collect data directly from platforms, I quickly run into **behavioral detection systems**. This raises a few fundamental questions for me.\n\nAt an intuitive level, it seems possible to:\n\n* add randomness (scrolling, delays, mouse movement),\n* simulate exploration instead of direct actions,\n* or hide client-side activity,\n\nand therefore make an automated actor look human.\n\nBut in practice, this approach seems to break down very quickly.\n\nWhat Iâ€™m trying to understand is **why**, and whether people actually solve this problem differently today.\n\nMy questions are:\n\n1. **Why doesnâ€™t adding randomness make automation behave like a real human?** What parts of human behavior (intent, context, timing, correlation) are hard to reproduce even if actions look human on the surface?\n2. **What do modern platforms analyze beyond basic signals like IP, cookies, or user-agent?** At a conceptual level, what kinds of behavioral patterns make automation detectable?\n3. **Why isnâ€™t hiding or masking client-side actions enough?** Even if visual interactions are hidden, what timing or state-level signals still reveal automation?\n4. **Is this problem mainly technical, or statistical and economic?** Is human-like automation theoretically possible but impractical at scale, or effectively impossible in real-world conditions?\n5. **From an OSINT perspective, how is platform data actually collected today?**\n   * Do people still use automation in any form?\n   * Do they rely more on aggregated or secondary data sources?\n   * Or is the work mostly manual and selective?\n6. **Are these systems truly being â€œbypassed,â€ or are people simply avoiding platforms and using different data paths altogether?**\n\nIâ€™m not looking for instructions on bypassing protections.  \nI want to understand **how behavioral detection works at a high level**, what it can and cannot infer, and what **realistic, sustainable approaches** exist if the goal is insight rather than evasion.\n\n**Note:**  \nSorry in advance â€” I used AI assistance to help write this question. My English isnâ€™t strong enough to clearly express technical ideas, but I genuinely want to understand how these systems work.",
      "is_original_content": false,
      "link_flair_text": "Bot detection ðŸ¤–",
      "permalink": "https://reddit.com/r/webscraping/comments/1q1y5gh/is_humanlike_automation_actually_possible_today/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nxe49t2",
          "author": "RandomPantsAppear",
          "text": "You don't have to deal with mouse movement, because people own tablets and tablets don't have mice, they just jump from destination to destination.\n\nTime is the new currency. Spin up profiles that have browsing history and months behind them, and you'll be doing great. Spin up accounts with these, and you'll be doing even better.\n\nVerifiability is second only to time. Link that identity with emails, with addresses, with names, and you will almost never have problems if you can scale.\n\nAlso it's absurdly easy to detect bot traffic. What's hard is detecting it before the page loads, at scale, within an acceptable delay. Hindsight is 20/20.\n\nIn 10 years, we will all basically be professional schizophrenics, with infinite personalities jealously guarded, curated, protected, and used.",
          "score": 12,
          "created_utc": "2026-01-03 05:52:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfaakv",
              "author": "Soren_Professor",
              "text": "I couldn't have said it better",
              "score": 2,
              "created_utc": "2026-01-03 11:47:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxllqsc",
              "author": "polygraph-net",
              "text": "> it's absurdly easy to detect bot traffic\n\nThis isn't true. I'm a bot detection researcher and many modern bots are extremely difficult to detect.\n\nAre you sure you're detecting stealth bots and not just basic things like puppeteer, playwright, selenium, etc.?",
              "score": 2,
              "created_utc": "2026-01-04 09:37:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxov1vf",
                  "author": "RandomPantsAppear",
                  "text": "I feel like Iâ€™m being misunderstood a bit, I could have worded this better. Your field is incredibly challenging. \n\nThe reason I said bot detection is easy after the fact, is that you then have the time needed to analyze behavioral traits and complex data, and for the relevant data to exist in the first place. \n\nOnce the data exists, itâ€™s easy to spot if you want a quarterly summary of â€œwho was a botâ€. But this is different than blocking bots in real time. \n\nLetâ€™s say I load Facebook, login, and start looking for group names with the goal of finding all relevant groups related to {keyword_list}. \n\nLoading Facebook is normal, logging in is normal, triggering the autocomplete is normal. \n\nWhat is abnormal, is that every time I login, I go immediately to the groups tab and trigger this autocomplete. Or that I spend X time on the groups page, vs Y on the feed. Or one of many non behavioral indicators (mismatch between events firing and device type, etc)\n\nThat takes time to develop, and resources to summarize individually. It takes even more resources to summarize what â€œnormalâ€ is.\n\nThere are not many places that can do this in realtime, for every page load, before sending response headers, or before the bots have already done significant work.",
                  "score": 2,
                  "created_utc": "2026-01-04 20:36:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxls6ga",
                  "author": "learning_linuxsystem",
                  "text": "Out of curiosity, how do you actually detect the more advanced, stealthy bots you mentioned? What gives them away?",
                  "score": 1,
                  "created_utc": "2026-01-04 10:35:09",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxla243",
              "author": "RobSm",
              "text": "> Also it's absurdly easy to detect bot traffic\n\nHow do you know? Maybe half of the 'human traffic' you see in your server are also bots, you just don't detect them? 0_o",
              "score": 1,
              "created_utc": "2026-01-04 07:51:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxfeqg5",
              "author": "learning_linuxsystem",
              "text": "Thanks for the perspective â€” Iâ€™ll take this into account and rethink whatâ€™s realistically possible given the time and identity side of things. Appreciated.",
              "score": 1,
              "created_utc": "2026-01-03 12:22:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxsalj1",
          "author": "Hour_Analyst_7765",
          "text": "My approach: switch proxies often. People will bookmark pages, get links sent through social media, or find articles through search. If you have ran a website, you'll find a lot of traffic eventually just randomly lands on a site and not through its own navigation structure. So I don't really see how much human detection is possible from detecting \"exploration\".\n\nI've my frameworks set up to complete specific jobs in a session based manner. Each session acts like as if it is a new visitor. These sessions can be thrown out as soon as the job finishes, or I may hold cookies for a few days in memory. \n\nHaving said that, I've found some sites blocking requests when walking their indexes from e.g. page 1 to 500 linearly. But even that I'm not sure what is the point.. if a site's search engine is shit or someone only visually remembers having seen an article, then they may as well click that \"Next\" button 100 times.",
          "score": 1,
          "created_utc": "2026-01-05 08:30:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2pu29",
      "title": "[Hiring] Looking for Automation Expert â€“ Paid",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q2pu29/hiring_looking_for_automation_expert_paid/",
      "author": "pageforsource",
      "created_utc": "2026-01-03 08:53:07",
      "score": 11,
      "num_comments": 1,
      "upvote_ratio": 0.79,
      "text": "Hey everyone,\n\nIâ€™m working on a personal web automation project (Node.jsâ€“based) where I need to automate interactions on a few modern websites for data processing / internal tooling purposes.\n\nThe automation involves:\n\nHeadless / real browser automation \n\nHandling anti-bot protections\n\nSolving or bypassing captchas.\n\nRequirements:\nComfortable working with Node.js automation stacks \n\nDm for more details",
      "is_original_content": false,
      "link_flair_text": "Hiring ðŸ’°",
      "permalink": "https://reddit.com/r/webscraping/comments/1q2pu29/hiring_looking_for_automation_expert_paid/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q0u4vg",
      "title": "Monthly Self-Promotion - January 2026",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q0u4vg/monthly_selfpromotion_january_2026/",
      "author": "AutoModerator",
      "created_utc": "2026-01-01 03:00:42",
      "score": 8,
      "num_comments": 32,
      "upvote_ratio": 0.9,
      "text": "Hello and howdy, digital miners ofÂ r/webscraping!\n\nThe moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!\n\n* Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?\n* Maybe you've got a ground-breaking product in need of some intrepid testers?\n* Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?\n* Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?\n\nWell, this is your time to shine and shout from the digital rooftops - Welcome to your haven!\n\nJust a friendly reminder, we like to keep all our self-promotion in one handy place, so any promotional posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q0u4vg/monthly_selfpromotion_january_2026/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nx1h6ai",
          "author": "Bitter_Caramel305",
          "text": "Do people actually come here to read this?",
          "score": 3,
          "created_utc": "2026-01-01 05:52:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1syyp",
              "author": "Jonathan_Geiger",
              "text": "I did :)",
              "score": 2,
              "created_utc": "2026-01-01 07:44:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx21k2a",
                  "author": "Bitter_Caramel305",
                  "text": "And are you also going to buy other people's products/services?",
                  "score": 1,
                  "created_utc": "2026-01-01 09:15:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx1ymcy",
              "author": "indicava",
              "text": "Me too",
              "score": 1,
              "created_utc": "2026-01-01 08:44:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx5iki6",
              "author": "matty_fu",
              "text": "this thread gets 10k+ views each month, and can be helpful for SEO/GEO. also it helps the mod team get less abuse from people wanting to advertise their products - a win for everyone",
              "score": 1,
              "created_utc": "2026-01-01 22:25:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1mea7",
          "author": "Jonathan_Geiger",
          "text": "Iâ€™m building [SocialKit](https://socialkit.dev) itâ€™s a social media scraping API for extracting transcripts, summaries, stats, comments, and more from YouTube, TikTok, Instagram, and Shorts (:\n\nJust reached $700 in MRR!!",
          "score": 3,
          "created_utc": "2026-01-01 06:40:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx642u9",
              "author": "bootlegDonDraper",
              "text": "very cool! will need something like this for my personal project\n\ntwo things maybe as advice:\n\n**1** meta is veeery nosy about scraping, especially for instagram. i would expect to get an email from them as your business grows, and not invest heavily in instagram (publicly at least ;) ) but stay on youtube and tiktok side of things\n\n**2** i dont see any dedicated product for scraping social media ranking for \"tiktok comment scraper\". I would go from [https://www.socialkit.dev/tiktok-comments-api](https://www.socialkit.dev/tiktok-comments-api) this landing page to .../tiktok-comment-scraper-api and optimize for the scraper keyword as it has muuuch higher volume",
              "score": 1,
              "created_utc": "2026-01-02 00:25:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1h7oe",
          "author": "Bitter_Caramel305",
          "text": "# I can scrape that website for you\n\nHi everyone,  \nIâ€™m Vishwas Batra, feel free to call me Vishwas.\n\nBy background and passion, Iâ€™m a full stack developer. Over time, project needs pushed me deeper into web scraping and I ended up genuinely enjoying it.\n\n**A bit of context**\n\nLike most people, I started with browser automation using tools like Playwright and Selenium. Then I moved on to crawlers with Scrapy. Today, my first approach is reverse engineering exposed backend APIs whenever possible.\n\nI have successfully reverse engineered Amazonâ€™s search API, Instagramâ€™s profile API and DuckDuckGoâ€™sÂ `/html`Â endpoint to extract raw JSON data. This approach is far easier to parse than HTML and significantly more resource efficient compared to full browser automation.\n\nThat said, Iâ€™m also realistic. Not every website exposes usable API endpoints. In those cases, I fall back to traditional browser automation or crawler based solutions to meet business requirements.\n\nIf you ever need clean, structured spreadsheets filled with reliable data, Iâ€™m confident I can deliver. I charge nothing upfront and only ask for payment once the work is completed and approved.\n\n**How I approach a project**\n\n* You clarify the data you need such as product name, company name, price, email and the target websites.\n* I audit the sites to identify exposed API endpoints. This usually takes around 30 minutes per typical website.\n* If an API is available, I use it. Otherwise, I choose between browser automation or crawlers depending on the site. I then share the scraping strategy, estimated infrastructure costs and total time required.\n* Once agreed, you provide a BRD or I create one myself, which I usually do as a best practice to stay within clear boundaries.\n* I build the scraper, often within the same day for simple to mid sized projects.\n* I scrape a 100 row sample and share it for review.\n* After approval, you provide credentials for your preferred proxy and infrastructure vendors. I can also recommend suitable vendors and plans if needed.\n* I run the full scrape and stop once the agreed volume is reached, for example 5000 products.\n* I hand over the data in CSV, Google Sheets and XLSX formats along with the scripts.\n\nOnce everything is approved, I request the due payment. For one off projects, we part ways professionally. If you like my work, we continue collaborating on future projects.\n\nA clear win for both sides.\n\nIf this sounds useful, feel free to reach out viaÂ [LinkedIn](https://www.linkedin.com/in/vishwas-batra/)Â or just send me a DM here.",
          "score": 1,
          "created_utc": "2026-01-01 05:52:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1ic2g",
          "author": "ConstIsNull",
          "text": "Building a JSON API for getting jobs listed on company career pages only, no LinkedIn or indeed. Saves you time instead of building your own job scraper. Check it out at jobven.com",
          "score": 1,
          "created_utc": "2026-01-01 06:02:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1tkco",
          "author": "Dear-Cable-5339",
          "text": "**Crawlbase Smart Proxy**Â â€“ scrape any public website without worrying about blocks, CAPTCHAs, or IP bans.  \nWorks great for e-commerce, SERPs, social platforms, and more.\n\nYou send the URL â†’ we handle proxies, rotation, retries, and geo.\n\nðŸ‘‰ Try it free here:Â [https://crawlbase.com/?s=5qGcKLCR](https://crawlbase.com/?s=5qGcKLCR)",
          "score": 1,
          "created_utc": "2026-01-01 07:50:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx20w02",
          "author": "Silentkindfromsauna",
          "text": "[Lindra.ai](www.lindra.ai) for turning any website into an api, still early but already works great for scraping",
          "score": 1,
          "created_utc": "2026-01-01 09:08:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx21jrx",
          "author": "Classic_Exam7405",
          "text": "We built the SOTA AI Web Agent rtrvr.ai for vibe scraping. Just prompt our agent to fill forms, extract data or monitor sites.\n\nScrape strict antibot sites like LinkedIn with our chrome extension.\n\nScale on our cloud/api platform",
          "score": 1,
          "created_utc": "2026-01-01 09:15:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx251r6",
          "author": "Ok-Skirt8939",
          "text": "im building \nhttps://clearproxy.io/\n\nan instant proxy validator.\n(u can check 100 Millions of proxies in less than 1 minute.)\nand.. more features just check it out (free 1M trial Checks)",
          "score": 1,
          "created_utc": "2026-01-01 09:53:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx260us",
          "author": "Long-Movie-2207",
          "text": "It's a real-time IP intel API that detects proxies, VPNs, Tor, datacenter vs residential, even specific providers, and gives a solid risk score (0-100) plus some device fingerprinting stuff.\n\nSuper handy for spotting if your proxies are likely to get flagged. There's a free tier with 5k requests to test it out, no strings attached.\n\nIf anyone's dealing with detection issues, might be worth a quick look: [https://ping0.xyz](https://ping0.xyz)",
          "score": 1,
          "created_utc": "2026-01-01 10:04:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx283xq",
          "author": "renegat0x0",
          "text": "https://github.com/rumca-js/Internet-Places-Database - data\n\n\nhttps://github.com/rumca-js/crawler-buddy - crawler\n\n\nhttps://github.com/rumca-js/webtoolkit - crawling library",
          "score": 1,
          "created_utc": "2026-01-01 10:26:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4ylgo",
          "author": "ziddec",
          "text": "I'm building https://sociavault.com \nA real-time social media scraping API. JSON response, 1 API key, 25+ platforms.",
          "score": 1,
          "created_utc": "2026-01-01 20:41:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5x6vx",
          "author": "scrape-do",
          "text": "***Hey scrapers,***\n\n  \nFirst off, happy new year to everyone :)\n\n\n\nWe're [Scrape.do](http://Scrape.do) ðŸ‘‹  an aggressively agile team behind a robust web scraping API that we've been building (very quietly since we were mainly focused on product) for the last 5 years.\n\n  \n2025 has been THE year for us, as we:\n\n* silently rolled out **next-gen infrastructure changes** that makes us the fastest and the most reliable on **a lot of** target domains, i.e. [async Scrape.do](https://scrape.do/documentation/async-api/)\n* **redesigned** our [dashboard with added playground](https://dashboard.scrape.do/playground) and [reworked documentation](https://scrape.do/documentation/),\n* [launched our new website](https://scrape.do/),\n* published **ready, free, and** [open-source scrapers](https://github.com/scrape-do/scrapedo-scrapers) **for 30 of the toughest domains** (we used SDO to bypass anti-bot, BUT will work with any API or your own setup)\n* an [Amazon Scraper API](https://scrape.do/products/ready-api/amazon-scraper/) as a product, but also a [free Amazon scraping repository](https://github.com/scrape-do/amazon-scraper)(**best one out there**, can easily be used w/o SDO same as above)\n\n  \n*Our support, bypass capabilities, pricing, and speed create an unmatched combination.* â­\n\n\n\n***Yet, we're not satisfied.***\n\n\n\nMore products, features, and most importantly open-source resources are coming your way very soon.\n\n\n\nHappy scraping,\n\nSDO team",
          "score": 1,
          "created_utc": "2026-01-01 23:46:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7taq4",
          "author": "deepwalker_hq",
          "text": "\nHey r/webscraping,\n\nWeâ€™ve been building [Deepwalker](https://deepwalker.xyz) to help overcome the challenges of scraping modern mobile apps. Think automating interactions on TikTok, Instagram, Spotify, and more using real devices.\n\nOur latest post details how we tackled Spotify: [https://deepwalker.xyz/blog/scraping-spotify-is-ez](https://deepwalker.xyz/blog/scraping-spotify-is-ez)\n\nDeepWalker is built for professionals tackling large-scale data extraction and automation.",
          "score": 1,
          "created_utc": "2026-01-02 07:14:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9t67k",
          "author": "Past-Refrigerator803",
          "text": "**Browser4 â€” Agentic browser engine for web scraping & automation**\n\nHi ,\n\nIâ€™m building **Browser4**, an open-source, **agentic browser engine** for serious web scraping and automationâ€”especially where traditional tools start to break down.\n\nBrowser4 runs **inside your own infrastructure** and is designed for teams and individuals who need **control, stability, and scale**.\n\n# What Browser4 offers\n\n* **Agent-native architecture** Built for autonomous browser agents that reason, retry, branch, and adapt.\n* **Deep CDP control** JVM-native, coroutine-safe Chrome DevTools Protocol implementation. Capable of processing **\\~10kâ€“20k highly complex pages per node per day**.\n* **Reliable scraping on dynamic sites** Handles logins, SPAs, infinite scroll, and JS-heavy workflows.\n* **Machine-learning extraction agent** Learns field structures across complex pages **without consuming LLM tokens**.\n* **Multi-strategy data extraction** LLMs, LM agents, selectors, DOM access, JavaScript execution, network interception, and screenshots.\n* **Scheduling & long-running jobs** Designed for production pipelines, not just one-off scripts.\n* **Built-in observability** Explicit state management, retries, and failure handling.\n\nBrowser4 can operate as a **fully autonomous browser agent** that plans and executes end-to-end tasks. At the same time, if youâ€™ve found Selenium or Playwright brittle, RPA tools too heavy, or scraping frameworks too opinionated, Browser4 also works at a **lower abstraction level** and gives you back control.\n\n# Use cases\n\n* AI agents that browse and interact with websites\n* Automation workflows that must survive frequent UI changes\n* Large-scale web scraping systems\n\nThe project is **open source** and under active development. Feedback, issues, and real-world use cases are very welcome.\n\n**GitHub:** [https://github.com/platonai/browser4](https://github.com/platonai/browser4)",
          "score": 1,
          "created_utc": "2026-01-02 16:04:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxe5dr7",
          "author": "Livid-Ad-8185",
          "text": "Solo developer expertise in i**ndustry-leading methods in web scraping, browser automation**. Knowledge in  CDP, TLS, AJ3, AJ4, TCP, socket, BoringSSL, OpenSSL. With experience in building **high-frequency data pipelines** in **Sports Bettings (Fanduel, Draftkings, Pinnacle, Bet365), CFT Markets (PolyMarkets), RealEsate and 200+ (Nike, Adidas).**\n\n**PROJECTS ====================================================**\n\n**BetNet - Odds API** | 100% Markets & Leagues Coverage Odds API on Fanduel, Draftkings and Pinnacle  \n**BW TLS Client** | C++, BoringSSL, HTTP2 |  Simulate Chrome 143 TLS Fingerprints.\n\n**Job Auto-Fill Pro Chrome Extension** | JavaScript, HTML, CSS  \n**SZ - Async CDP Chromium Driver** | CDP, Python, AsyncIO, Multi-threading\n\n**YOUTUBE & DISCORD =============================================**\n\n[https://www.youtube.com/@storm-clouds-development](https://www.youtube.com/@storm-clouds-development)\n\n[https://discord.gg/jfdck6EE](https://discord.gg/jfdck6EE)\n\nhttps://preview.redd.it/al1n4wfntwbg1.png?width=575&format=png&auto=webp&s=07edc4eab90d94000f3e7a1a9678fd522d9d02d0",
          "score": 1,
          "created_utc": "2026-01-03 06:00:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxf2bsc",
          "author": "DataiziNet",
          "text": "Stop building scrapers. Start consuming data.\n\nMost tools here sell you the infrastructure (proxies/browsers) but leave you with the maintenance.[Dataizi](https://dataizi.net/)is a fully managed data extraction service. We don't sell the shovel; we dig the hole.\n\nWhy us?\n\n* 0 Maintenance: We monitor feed health 24/7. When layouts change, we fix the parsers, not you.\n* Complex Targets: We handle JS-heavy sites, hidden APIs, and aggressive anti-bot protections.\n\nPerfect for:\n\n* Price Intelligence: High-frequency SKU tracking for dynamic pricing strategies.\n* Machine Learning/AI: Clean, structured, normalized data ready for model training.\n* SaaS & Agencies: Reliable data feeds without hiring a dedicated scraping engineer.\n\nThe Offer: You provide the URL + Schema. We deliver the API or CSV.\n\n[Get a quote: dataizi.net](https://dataizi.net/)",
          "score": 1,
          "created_utc": "2026-01-03 10:40:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjkene",
          "author": "jobswithgptcom",
          "text": "[https://jobswithgpt.com](https://jobswithgpt.com) \\- job search",
          "score": 1,
          "created_utc": "2026-01-04 01:06:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxl1swy",
          "author": "DinnerStraight9753",
          "text": "Happy new year guys! How was your webscraping experience last year?\n\n# Want some real updates? Here comes the new [**PYPROXY**](http://www.pyproxy.com/?utm-source=zr&utm-keyword=?99) for 2026:\n\n1. **Expanded the unlimited residential IP pool,** key resources of high-demand countries (US, GB, FR, CA...) are now availableï¼\n\n2. [Static ISP proxies](http://www.pyproxy.com/staticisp/?utm-source=nh&utm-keyword=?01) now **support custom IP range selection,** giving you greater control and flexibility in managing your proxy resources.\n\n3.Â [Rotating Residential Proxies](http://www.pyproxy.com/residential/?utm-source=cs&utm-keyword=?99) billing has been updated to a **monthly subscription** model, with unused balances now **eligible for rollover.**\n\nDiscounts are available now for residential proxies, ISP proxies, and SOCKS5 proxies. Don't miss out!",
          "score": 1,
          "created_utc": "2026-01-04 06:39:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlcvb0",
          "author": "Plus-Ambition-5297",
          "text": "If anyone is looking for aÂ [web scraping company](https://go4scrap.in/services/)Â based in India (easier payments/UPI), we at Go4Scrap specialize in directory scraping and anti-bot bypass. We also have a massiveÂ [Technical Wiki](https://go4scrap.in/wiki/)Â for those DIY-ing their setup. Feel free to DM!\"",
          "score": 1,
          "created_utc": "2026-01-04 08:16:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrk2w0",
          "author": "Smart_Confidence2967",
          "text": "Currently building the best Google Maps Scraper (and linkedin). you can try for free here: (still in BETA): [https://onleads.io/google-maps-scraper/](https://onleads.io/google-maps-scraper/)",
          "score": 1,
          "created_utc": "2026-01-05 04:54:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrpu87",
          "author": "HLCYSWAP",
          "text": "back looking for work. I can scrape anything anywhere efficiently. need lambdas for targeted repetitively current work? need all of meta in one dataset? iâ€™m your guy. my previous reddit username was â€˜fixitorgotojailâ€™ (still that on x) \n\nyou can see my successes here: https://www.github.com/matthew-fornear\n\ni have scraped meta, x, the cia, arxiv, as well as a laundry list of sites and databases outside of FAANG. \n\nI also program backends\n\ncost is determined by scope. most of my turn arounds are <4 hours",
          "score": 1,
          "created_utc": "2026-01-05 05:33:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsxpd5",
          "author": "internet-savvyeor",
          "text": "Hey r/webscraping,\n\nIf you havenâ€™t heard of us yet, weâ€™re Ace Proxies, a provider focused on delivering clean, reliable, and consistently stable IPs for scraping, automation, and account management. No inflated claims, no recycled free proxies, just high-quality networks built for performance.\n\nWhat we offer:\n\nâœ… USA Static Residential & Datacenter: Dedicated, stable IPs ideal for long-running scrapes and account operations. Supports both SOCKS5 and HTTP/HTTPS.\n\nâœ… Rotating Residential Proxies: A large global pool from 195+ countries. Usage-based plans (5GB, 10GB, etc.), so you only pay for the bandwidth you need.\n\nWhat makes us different (the no-BS part):\n\nâœ… 24-Hour Free Trial: No credit card, no KYC. Just message us on live chat and weâ€™ll activate it instantly.\n\nâœ… Real Human Support: You get a dedicated support manager, not a ticket queue.\n\nâœ… Open to Collabs & Feedback: We're always improving, partnering, and working directly with users who care about quality.\n\nFor a discount, you can always use **REDDITWebScraping** for 25% off anytime.\n\nExplore our proxy plans here: [https://www.aceproxies.com/buy-proxies](https://www.aceproxies.com/buy-proxies)\n\nIf you want to test a specific subnet or have questions before purchasing, feel free to DM me.\n\nHappy scraping!",
          "score": 1,
          "created_utc": "2026-01-05 11:57:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxt3otl",
          "author": "Final-Worth-2752",
          "text": "Hey everyone, I've made a solution withÂ [https://revrse.ai](https://revrse.ai/)Â to automatically generate an API for any Android app, allowing you to extract data or automate tasks on mobile apps without an emulator",
          "score": 1,
          "created_utc": "2026-01-05 12:41:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxtq3ro",
          "author": "fedir-lebid",
          "text": "Hi there guys\n\nI provide APIs for following websites:  \n\\- Amazon  \n\\- Walmart  \n\\- Ebay  \n\\- Naver SmartStore  \n\\- Shopee  \n\\-Â [JD.com](http://jd.com/)  \n\\- Tmall  \n\\- TikTok Shop\n\nPay per requests basis, handling large scale\n\nIf you are interested find me here:  \nLinkedin:Â [https://www.linkedin.com/in/fedir-lebid/](https://www.linkedin.com/in/fedir-lebid/)  \nEmail:Â [fedir.lebid@webparsers.com](mailto:fedir.lebid@webparsers.com)\n\nOr you can create account on your own using website: (it will grant you 1000 credits for free)  \n[https://webparsers.com/](https://webparsers.com/)",
          "score": 1,
          "created_utc": "2026-01-05 14:53:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6fwez",
          "author": "kingxx773",
          "text": "i am making all type of custom scraper and providing data in any format you want me to provide. if you want to build scraper for your personal project or for your company feel free to drop message in my inbox.",
          "score": 1,
          "created_utc": "2026-01-07 10:47:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1uq5w",
      "title": "Turnstiles, geetest, automation in Rust?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q1uq5w/turnstiles_geetest_automation_in_rust/",
      "author": "Normal-Middle3719",
      "created_utc": "2026-01-02 09:55:40",
      "score": 8,
      "num_comments": 5,
      "upvote_ratio": 0.75,
      "text": "Hey guys,\n\nIâ€™ve been benefiting from the open-source projects here for a while, so I wanted to give back. Iâ€™m a big fan of compiled languages, and I needed a way to handle browser tasks (specifically CAPTCHAs) in Rust without getting flagged.\n\nI forked chromiumoxide and ported the stealth patches from `rebrowser` and `puppeteer-real-browser`. I also built dedicated solvers for Cloudflare and GeeTest.\n\nðŸ§ª The Proof (Detection Results)\n\nIâ€™ve tested this against common scanners and itâ€™s passing:\n\n* **Intoli / WebDriver Advanced:** Passed (WebDriver hidden, Permissions default).\n* **Fingerprint Scanner:** `PHANTOM_UA`, `PHANTOM_PROPERTIES`, and `SELENIUM_DRIVER` all return **OK**.\n* **Canvas/WebGL:** Properly spoofing Google Inc. (NVIDIA) with no broken dimensions.\n* **Stack Traces:** `PHANTOM_OVERFLOW` depth and error names match real Chrome behavior.\n\nðŸ›  The Repos\n\n* [**chaser-oxide**](https://github.com/ccheshirecat/chaser-oxide)â€“ Chromiumoxide fork with stealth/impersonation patches.\n* [**chaser-cf**](https://github.com/ccheshirecat/chaser-cf)â€“ Rust implementation for Cloudflare Turnstile.\n* [**chaser-gt**](https://github.com/ccheshirecat/chaser-gt)â€“ GeeTest solver using deobfuscation (via `rquests`/`curl_cffi`).\n\n**Note:** I shipped these with **C FFI bindings**, so you can use them in **Python, Go, or Node** if you just want the Rust performance/stealth without writing Rust code. I personally prefer this over managing a separate microservice.\n\nðŸ’¬ Curious about your workflows:\n\n1. **Third-party APIs:** For those using paid solvers (Capsolver, etc.), is it for the convenience, or because you don't want to maintain stealth patches yourself?\n2. **Scraping Use Cases:** What are you guys actually building? Iâ€™ll go first: Iâ€™m overengineering automation for crypto casinos because I found some gaps in their flow lol.\n3. **Differentiators:** What actually makes a solver \"good\" in 2026? Is it raw solve speed, or just the success rate on high-entropy challenges?\n\nItâ€™s still early, so feel free to contribute, roast my code, or reach out to collaborate. Happy New Year!",
      "is_original_content": false,
      "link_flair_text": "Bot detection ðŸ¤–",
      "permalink": "https://reddit.com/r/webscraping/comments/1q1uq5w/turnstiles_geetest_automation_in_rust/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nxa1jbi",
          "author": "TinyBeing8001",
          "text": "this is cool and thanks for it\n\nit might help spacing your post content out so itâ€™s more consumable",
          "score": 1,
          "created_utc": "2026-01-02 16:43:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbbgrl",
              "author": "Normal-Middle3719",
              "text": "thanks for the feedback! took your advice and updated the post",
              "score": 1,
              "created_utc": "2026-01-02 20:18:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxlqjis",
          "author": "uzzalhme",
          "text": "Thanks for your awesome work , im also interested to know about the crypto casino gaps .",
          "score": 1,
          "created_utc": "2026-01-04 10:20:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrpxm6",
          "author": "idklolmez",
          "text": "how silly you hardcoded path to patched chromium and yet you haven't provided the patched chromium at all so its effectively useless because chase-oxide will just throw an error due to that file not existing on system",
          "score": 1,
          "created_utc": "2026-01-05 05:34:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny8tn4i",
              "author": "Normal-Middle3719",
              "text": "sorry! i fucked something up last week and it was broken for a few days, fixed and restored it to a normal state now",
              "score": 1,
              "created_utc": "2026-01-07 18:28:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q6wrgy",
      "title": "Matching Products Across 7 Sites",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q6wrgy/matching_products_across_7_sites/",
      "author": "Lazy-Masterpiece8903",
      "created_utc": "2026-01-08 00:25:59",
      "score": 8,
      "num_comments": 14,
      "upvote_ratio": 0.9,
      "text": "I'm building a comparison tool for 7 sites currently just working on each scraper. BUT when it comes to comparing products across 7 sites what's going to be the best way to go with matching the products?\n\nObviously fuzzy matching titles will be the most common solution. But could I use Ai to improve match rate or something? TIA",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q6wrgy/matching_products_across_7_sites/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nyb77n4",
          "author": "mrtain",
          "text": "I've been running [TCGCompare](https://www.tcgcompare.com) which compares over 1000 stores and 5k products for 5 years now.\n\nTried dabbling with all sorts of matching but once you scale up it's just not possible. Even with the most intelligent matcher you're still going to have 10% of products that are just badly listed.\n\nThe only solution that works is human judgement, put all your newly discovered products in a backlog and match them manually. 7 sites shouldn't be too much work unless you're talking 100k new products daily.\n\nGood luck with it, and do let me know how you get on.",
          "score": 5,
          "created_utc": "2026-01-08 00:59:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nybho20",
              "author": "davak72",
              "text": "So, thatâ€™s a really cool site. If I was into TCGs, Iâ€™d definitely start using it!\n\nIâ€™m thinking about making one to compare utility providers in my state",
              "score": 2,
              "created_utc": "2026-01-08 01:54:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nybj0f6",
              "author": "Weary-Interview6167",
              "text": "What's your current matching solution before it hit manual approval?",
              "score": 1,
              "created_utc": "2026-01-08 02:01:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nypfxjx",
                  "author": "mrtain",
                  "text": "A horrible pipeline of token and keyword matching with avg pricing and release date comparison. SKUs and UPC/EAN is the way to go if they are available, but they aren't always available.",
                  "score": 2,
                  "created_utc": "2026-01-10 01:15:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nypd5l1",
              "author": "Srijaa",
              "text": "Hey mrtain, Iâ€™ve been building a fun side project and your knowledge might be super helpful! Are you open to me dming you?",
              "score": 1,
              "created_utc": "2026-01-10 01:00:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nypg01z",
                  "author": "mrtain",
                  "text": "Absolutely, I'll get back to you when I can ðŸ™‚",
                  "score": 1,
                  "created_utc": "2026-01-10 01:16:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nybmetv",
          "author": "KBaggins900",
          "text": "I've done this in the past for a company I worked for. We combined multiple approaches. \n\n1. Use page bread crumbs to organize into products if doing multiple product types\n2. AI to predict the product type \n3. Use specifications data like brand name and model number to cross reference \n4. Human in the loop\n\nCombine these to create automated assignment and have some type of review or override process.",
          "score": 2,
          "created_utc": "2026-01-08 02:19:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyc6o91",
          "author": "Round_Method_5140",
          "text": "What kind of products? Do you have upc or mfg part number, model number?",
          "score": 2,
          "created_utc": "2026-01-08 04:09:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyb2wxz",
          "author": "Weary-Interview6167",
          "text": "Also on the same exact boat. Iâ€™m currently playing with vector search and it seems to work fairly well.",
          "score": 1,
          "created_utc": "2026-01-08 00:37:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybpoji",
          "author": "LeSoviet",
          "text": "depends type of proyect are you generating new inputs ? new ids?\n\nthese ids can be wrong? your db can be smoked?\n\nif someone have a solid algoritm im happy to hear it but imo super depends and its something you need tweak it in the long run",
          "score": 1,
          "created_utc": "2026-01-08 02:36:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycp9px",
          "author": "Kbot__",
          "text": "Hi,\n\nFuzzy matching is great when you are dealing with categories like electronics. But when it comes to clothing, it will almost always fail because \"red shirt\" could be any red shirt. So, if you don't want to do manual work, an LLM is the next best choice.\n\nI would first fuzzy match, then confirm with an LLM.\n\nWhat big companies usually do is download the entire site, all products, turn the data into a table, then run scripts to match by different properties (color, weight, size, material, etc) and then they have a human process and correct the results.",
          "score": 1,
          "created_utc": "2026-01-08 06:14:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyctbc1",
          "author": "andreew92",
          "text": "Iâ€™m working on a project where I scrape and match occupations. And let me tell you the matching is a pain in my butt.\n\nIâ€™m using a process where I spilt the title into different parts. Remove the bullshit words and characters, then run each word against the database to find matches. I then score up the results with points and rank them from most to least with multipler for longer strings. Then the top 10 get sent to a LLM for the final matching judgment, where the model can flag if an entry needs manual human review. \n\nIt ainâ€™t perfect, but for what I am trying to do it seems to work. Itâ€™s worth mentioning that I donâ€™t have many data points to match, I honestly donâ€™t know how well this method would scale with a lot of data.",
          "score": 1,
          "created_utc": "2026-01-08 06:46:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q4ie6b",
      "title": "My 4th pypi lib: I created a stealthy NSE India API scrapper (Python)",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q4ie6b/my_4th_pypi_lib_i_created_a_stealthy_nse_india/",
      "author": "convicted_redditor",
      "created_utc": "2026-01-05 10:37:40",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.91,
      "text": "A few months ago, I shared my library [stealthkit](https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/) and mentioned I was working on a specific stock exchange wrapper that uses it at its core. Well, I finally finished it and published it to PyPI.\n\nItâ€™s called **PNSEA** (Python NSE API). Itâ€™s an open-source library for fetching data from the National Stock Exchange of India without getting hit by the dreaded `403 Forbidden` or rate-limit blocks.\n\n**What My Project Does**\n\n* **Stealth by Default:** Uses my `stealthkit` wrapper (curl\\_cffi) to rotate TLS fingerprints and headers, making requests look like a human browsing on Chrome/Safari. I added more headers specific to NSE website to make it stealtheir.\n* **Deep Data Access:** It doesn't just do stock prices. It pulls **Insider Trading** data, Pledged shares, SAST data, and even Mutual Fund movements.\n* **Analysis Ready:** NSEâ€™s nested JSON is a mess. This lib automatically flattens it into **Pandas DataFrames** so you can jump straight into analysis.\n* **Full FnO Support:** Easy access to Option Chains for NIFTY, BANKNIFTY, and all F&O stocks with built-in filtering.\n\n**Why did I create it?** Iâ€™ve been an FnO trader and dev for years. Most existing NSE wrappers are either outdated, stop working after a week due to blocks, or require you to manually handle cookies and headers every time the NSE website updates its security.\n\nSince all my projects from my Amazon scraper to my finance apps rely on high-quality data, I wanted a \"set it and forget it\" solution for the Indian market. PNSEA is the result of that frustration.\n\n**Pypi:** [https://pypi.org/project/pnsea/](https://pypi.org/project/pnsea/)\n\n**Github:** [https://github.com/theonlyanil/pnsea](https://www.google.com/search?q=https://github.com/theonlyanil/pnsea)\n\n**Target Audience** Algo traders, financial analysts, and developers who are tired of their NSE scrapers breaking every time the site refreshes its bot protection.\n\n**Comparison** Unlike other wrappers that use standard `requests` or `urllib`, this uses browser impersonation natively. It also provides corporate governance data (insider trading) which is usually hidden behind multiple clicks or premium paid APIs.\n\nCheckout its usage on my personal website where I show [insider trading data in a dashboard](https://theonlyanil.com/finance/apps/insider-trading).\n\nItâ€™s open source, so feel free to fork it, add features, or let me know if you find an endpoint thatâ€™s missing!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q4ie6b/my_4th_pypi_lib_i_created_a_stealthy_nse_india/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pzhjmn",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pzhjmn/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2025-12-30 13:01:09",
      "score": 7,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levelsâ€”whether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) ðŸŒ±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring ðŸ’°",
      "permalink": "https://reddit.com/r/webscraping/comments/1pzhjmn/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nx78umh",
          "author": "convicted_redditor",
          "text": "# Created 3 web scraping pypi libs on python. Any way to monetise?\n\n[](https://www.reddit.com/r/webscraping/?f=flair_name%3A%22Scaling%20up%20%F0%9F%9A%80%22)In 2025, I developed 3 pypi libs around webscraping-\n\n1. stealthkit - wrapper over curl\\_cffi with human-like fingerprinting with header rotations and cookie management.\n2. amzpy - built on top of curl\\_cffi (but before stealthkit), scrapes amazon search and product data.\n3. pnsea - built over stealthkit to scrape stock exchange data of India (NSE).\n\nReason to build them was for my personal usage as I developed an amazon related web app last year so I built amzpy. I was building a lot streamlit data based apps (and more) to play with NSE data - like options chain, insider data, etc.\n\nHow can I monetise this skill? Should I build FastAPI and turn into saas?\n\nHow do you guys monetise your web scraping skills?",
          "score": 2,
          "created_utc": "2026-01-02 04:36:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvep5p",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2025-12-31 06:14:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvsb8v",
              "author": "webscraping-ModTeam",
              "text": "âš¡ï¸ Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2025-12-31 08:14:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx082gy",
          "author": "LessBadger4273",
          "text": "Here ya go, if someone can help me with this â€” Libs such as â€œnodriverâ€ seems to be able to completely bypass some antibots like shopee.* ones that also requires js rendering. I guess this is because you are basically using your browser â€œas isâ€, without any automation flag, right? \n\nIf so, why itâ€™s so hard to replicate this at scale using residential proxies? My guess is that once you move this to AWS ec2, for example, those antibots can detect you are in a vm environment and block you, right? Would it be be possible to run this at scale by having an in house farm of old desktops/laptops? Or maybe using some rdp tools? Is it a price constraint that we are not able to bypass these antibots at scale or am I missing something?",
          "score": 1,
          "created_utc": "2026-01-01 00:36:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1cfkv",
              "author": "QuinsZouls",
              "text": "Shopee antibot is heavily dependent of the hardware fingerprint  + ip, I have succeeded experience using a local farm of macbook devices that run google chrome , usually th vm can easily  detected by proof of work and webgl + canvas fingerprint. \nAlso I have succeeded with some cloud VM instances with a GPU",
              "score": 1,
              "created_utc": "2026-01-01 05:12:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx88x0k",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-02 09:42:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8upl6",
              "author": "webscraping-ModTeam",
              "text": "âš¡ï¸ Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-01-02 12:50:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pz1zvp",
      "title": "Scraping reddit?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pz1zvp/scraping_reddit/",
      "author": "AdhesivenessEven7287",
      "created_utc": "2025-12-29 23:39:13",
      "score": 7,
      "num_comments": 4,
      "upvote_ratio": 0.9,
      "text": "Over time I save up pages of articles and comments I think will be interesting. But I've not gotten around to it yet. \n\nHow can I have the links but easily download the page? Baring in mind to view all comments I need to scroll down the page. ",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1pz1zvp/scraping_reddit/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwmyser",
          "author": "g4m3r1",
          "text": "If you just want to scrape the actual content of the reddit post then its quite easy. Just add .json at the end of the URL and reddit will return the content of the post + all comments as beautifully formatted json.\n\nE.g. Try it with your own post here: [https://www.reddit.com/r/webscraping/comments/1pz1zvp/scraping\\_reddit.json](https://www.reddit.com/r/webscraping/comments/1pz1zvp/scraping_reddit.json)",
          "score": 10,
          "created_utc": "2025-12-29 23:50:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwn0m6n",
              "author": "PresidentHoaks",
              "text": "Lol mobile sends me back to this page, will have to check on my lappy",
              "score": 2,
              "created_utc": "2025-12-30 00:00:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwn3ws2",
                  "author": "g4m3r1",
                  "text": "Never tried it on mobile but on desktop this works fine :).",
                  "score": 1,
                  "created_utc": "2025-12-30 00:18:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q5rj8i",
      "title": "Alternative to curl-impersonate",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q5rj8i/alternative_to_curlimpersonate/",
      "author": "Hour_Analyst_7765",
      "created_utc": "2026-01-06 19:00:47",
      "score": 7,
      "num_comments": 8,
      "upvote_ratio": 0.83,
      "text": "I'm writing a C# docker application that rotates proxies automatically and performs the requests for some scrapers I run at home. The program adds lots of instrumentation to optimize reliability. (It stores time-series data on latency, bandwidth, proxy/server-side rejects for each individual proxy+site combination, effectively resulting in each individual site rotating through its own proxy pool)\n\nObviously I need to do some kind of TLS spoofing to support the more tricky websites. I also want to rotate user-agent with a distribution of browser versions *and* OS versions. I've already got some market share databased on caniuse and statcounter..\n\nNow I need a library that can actually execute these browser impersonations. I've been using lexiforest/curl-impersonate, but it falls short on several fronts. I need to customize the user-agent and some other platform-specific headers.. however their recent additions hard-coded profiles into the executable. Even though the documentation outlines I should customize their standard scripts to do this!\n\nUnfortunately, if I run curl with an extra -H 'User agent: ..' it wont replace but send the user-agent header twice.\n\nI've looked at this for a little while, but I'm fearing this change dead ends this project pretty hard.\n\nOf course I could customize it, as the author points everyone to do so.. However scraping is a hobby not my work, so when things need updating, it may not get fixed for days to weeks. I liked using ready-built executables, so I can grab the latest impersonate profiles & market share data on a cronjob..\n\nI've looked at other projects like wreq and rnet, but these are just a Rust crate and/or Python bindings. Not quite what I'm looking for.. although maybe a C# FFI is possible. It does look to be much more comprehensive and actively maintained (more browser profiles, split up by OS etc.)\n\nHowever, before spending a bunch of time on either curl-impersonate or a C#-wreq FFI bridge, is there any other library I missed out on during my Reddit/Google search?",
      "is_original_content": false,
      "link_flair_text": "Bot detection ðŸ¤–",
      "permalink": "https://reddit.com/r/webscraping/comments/1q5rj8i/alternative_to_curlimpersonate/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "ny294ho",
          "author": "irrisolto",
          "text": "https://github.com/bogdanfinn/tls-client-api\n\nNormal C# http library > tls api > final site",
          "score": 6,
          "created_utc": "2026-01-06 19:30:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5wipi",
          "author": "sardanioss",
          "text": "Hi there, check this out, might solve your issue - [https://github.com/sardanioss/httpcloak/](https://github.com/sardanioss/httpcloak/) please do star it if its useful to you, thankyou!",
          "score": 2,
          "created_utc": "2026-01-07 07:49:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyees84",
              "author": "Hour_Analyst_7765",
              "text": "I will have a look thanks!",
              "score": 1,
              "created_utc": "2026-01-08 14:07:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny4i6gt",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-07 02:10:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny4pwid",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-07 02:51:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzyqo0",
      "title": "open-source userscript for google map scraper (it works again)",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pzyqo0/opensource_userscript_for_google_map_scraper_it/",
      "author": "Asleep-Patience-3686",
      "created_utc": "2025-12-31 00:33:36",
      "score": 6,
      "num_comments": 8,
      "upvote_ratio": 0.88,
      "text": "I built this script about six months ago, and it worked well until two months ago when it suddenly stopped functioning. I spent the entire night yesterday and finally resolved the issue.\n\n  \nFunctionality:\n\n1. Automatically scroll to load more results\n2. Retrieve email addresses and Plus Codes\n3. Export in more formats\n4. Support all subdomains of Google Maps sites.\n\nChange logs:\n\n1. The collection button cannot be displayed due to the Google Maps UI redesign.\n2. The POI request data cannot be intercepted.\n3. Added logs to assist with debugging.\n\n[https://greasyfork.org/en/scripts/537223-google-map-scraper](https://greasyfork.org/en/scripts/537223-google-map-scraper)\n\nJust enjoy with free and unlimited leads!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1pzyqo0/opensource_userscript_for_google_map_scraper_it/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwvj8to",
          "author": "Training_Hand_1685",
          "text": "Wow, something like this is exactly what I searched Reddit for. So I can search for non-profits and housing programs in different states (that are listed on google maps) with this? For free? Do I need to be technically savvy (know how to web scrape) or can a newbie/end user, use this?",
          "score": 1,
          "created_utc": "2025-12-31 06:52:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwx6520",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2025-12-31 14:47:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxn57fq",
                  "author": "webscraping-ModTeam",
                  "text": "ðŸª§ Please review the sub rules ðŸ‘‰",
                  "score": 1,
                  "created_utc": "2026-01-04 15:57:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx0yra9",
              "author": "Asleep-Patience-3686",
              "text": "I set up a GitHub repository. It will guide you on how to use it. [https://github.com/webAutomationLover/google-map-scraper](https://github.com/webAutomationLover/google-map-scraper)",
              "score": 1,
              "created_utc": "2026-01-01 03:30:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxitnel",
          "author": "Twitty-slapping",
          "text": "Where are you getting the email address lol ?",
          "score": 1,
          "created_utc": "2026-01-03 22:45:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmg3b8",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-04 13:41:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxn593z",
                  "author": "webscraping-ModTeam",
                  "text": "ðŸª§ Please review the sub rules ðŸ‘‰",
                  "score": 1,
                  "created_utc": "2026-01-04 15:57:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q0kacg",
      "title": "Amazon \"shop other stores\" Beta",
      "subreddit": "webscraping",
      "url": "https://i.redd.it/zlndg2yq4lag1.jpeg",
      "author": "ZanofArc",
      "created_utc": "2025-12-31 18:44:32",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q0kacg/amazon_shop_other_stores_beta/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q6ge5w",
      "title": "Any nodriver/zendrive alternatives?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q6ge5w/any_nodriverzendrive_alternatives/",
      "author": "Edsaur",
      "created_utc": "2026-01-07 14:12:37",
      "score": 6,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "Hello! \n\nI am currently using nodriver/zendrive as my web scraper and also form automation. While it works best most especially for antibot/captcha detection, are there any other alternatives that does what they do? \n\nThank you!",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1q6ge5w/any_nodriverzendrive_alternatives/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "ny7uv9e",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-07 15:53:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny8fi5j",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-07 17:26:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny870bt",
          "author": "orucreiss",
          "text": "[https://github.com/stars/OmerFarukOruc/lists/web-scraping](https://github.com/stars/OmerFarukOruc/lists/web-scraping) check my stars i have starred good repos so far.",
          "score": 1,
          "created_utc": "2026-01-07 16:47:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny89azu",
          "author": "scrape-do",
          "text": "You could look into Botasaurus (https://github.com/omkarcloud/botasaurus) and DrissionPage (https://github.com/g1879/DrissionPage). They **technically** do what NoDriver does, but not really.",
          "score": 1,
          "created_utc": "2026-01-07 16:58:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyb1pwq",
              "author": "Hot-Percentage-2240",
              "text": "What's the difference?",
              "score": 1,
              "created_utc": "2026-01-08 00:31:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nydepob",
                  "author": "scrape-do",
                  "text": "NoDriver is pretty self-explanatory in that it doesnâ€™t use chromedriver or any WebDriver and instead controls browser directly over CDP, removing automation fingerprints. \n\nBotasaurus and DrissionPage do have an automation layer, but rely on stealth techniques to hide or minimize that layer.",
                  "score": 1,
                  "created_utc": "2026-01-08 09:57:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nykx750",
              "author": "Edsaur",
              "text": "Does Drission have an english documentation by anychance?",
              "score": 1,
              "created_utc": "2026-01-09 11:52:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nykyelt",
                  "author": "scrape-do",
                  "text": "[https://github.com/g1879/DrissionPage/tree/master/docs\\_en](https://github.com/g1879/DrissionPage/tree/master/docs_en) these are author approved translations, goes into a lot of details",
                  "score": 1,
                  "created_utc": "2026-01-09 12:01:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyfek1r",
          "author": "Coding-Doctor-Omar",
          "text": "Try [Scrapling](https://github.com/D4Vinci/Scrapling).",
          "score": 1,
          "created_utc": "2026-01-08 16:53:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q4tkp9",
      "title": "How long is a reasonable free maintenance period?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q4tkp9/how_long_is_a_reasonable_free_maintenance_period/",
      "author": "npc-d",
      "created_utc": "2026-01-05 18:24:03",
      "score": 5,
      "num_comments": 9,
      "upvote_ratio": 0.79,
      "text": "Hi everyone, need some advice.\n\nI got an offer for a web scraping project with the following scope:\n\n* Scraping 3 websites daily\n* 2 sites have about 500 URLs each\n* 1 site require logic and form input (about 20 pages total)\n* Custom scraping logic (not a generic scraper tool)\n\nThe project itself is paid as a one-time fee.\n\nThe client is okay with occasional downtime and the data isnâ€™t critical.\n\nThis is my first time taking a freelancing and dev work.\n\nThey asked if I would give them free maintenance / warranty, so my question is:\n\n* How long do you usually include free maintenance after delivery?\n* Do you consider things like site HTML changes, session expiration, or minor breakages as part of that free period?\n* After the free period, do you prefer monthly maintenance, pay-per-fix, or no support unless requested?\n* How much should I charge for a monthly maintenance, or pay-per-fix? Is 5% of one-time fee too small?\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q4tkp9/how_long_is_a_reasonable_free_maintenance_period/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nxvdk0e",
          "author": "RyanRodemoyer2",
          "text": "edit: this is not legal advice and you should consult a professional\n\nYou can protect yourself with a detailed and unambiguous scope of work. Your bulleted list is a starting point but it really should be detailed and in a signed contract with the client. Then your warranty is tied to the original scope and not allowing anyone to weasel or strong arm you into warranty that is actually new scope.\n\nWarranty duration? That's more difficult to answer and probably depends on the size of the original contract and quality of the customer. Warranties are worth charging for - they are NOT free. Perhaps something like 30-60 days after go-live is achieved?\n\nIf you're scraping then THINGS CHANGE that's part of the business. Your scope of work/contract should call out the risk of change to the source data/websites and those are out of your control. I'd say those are NOT warranty because your software didn't break, something else changed.\n\nCharging a monthly retainer is a decent idea and it can include a fixed number of use-it-or-lose-it hours to use at the customers discretion.",
          "score": 6,
          "created_utc": "2026-01-05 19:29:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwtdrj",
          "author": "Afraid-Solid-7239",
          "text": "Personally, what I do for private commissions is I let them know that what they're paying for is simply the version that currently works, and we pre agree on a price for future updates based off of the first price.",
          "score": 3,
          "created_utc": "2026-01-05 23:37:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxv7uzm",
          "author": "Careless-inbar",
          "text": "Don't worry just provide 2 months of maintenance afterwards They are going to keep you permanent to keep it working once they are use to it",
          "score": 2,
          "created_utc": "2026-01-05 19:03:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5qvn0",
          "author": "HockeyMonkeey",
          "text": "Scraping for yourself is forgiving. Scraping for clients is not.\n\nI don't buy your \"The client is okay with occasional downtime and the data isnâ€™t critical.\"\n\nThey donâ€™t care how clever the solution is. they care that data arrives on time and looks consistent week after week.\n\nThe biggest mindset shift is realizing you're selling reliability, not code. Learning that earlier would've saved me a lot of uncomfortable conversations and reactive firefighting.\n\nFree maintenance is how scope creep is born. So 14-30 days should be a *warranty period*, not support. It's there to catch delivery bugs, missed edge cases, or small misunderstandings.   \n  \nNot ongoing site changes.",
          "score": 2,
          "created_utc": "2026-01-07 07:00:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6kfgq",
              "author": "bluemangodub",
              "text": "Ideally yes.  A factory doesn't buy a new machine and then just expect it to work forever. There are machine operators, maintenance engineers, people available to fix it when it breaks. All this is required to be operational. No one expects these people to work for free.\n\nHowever for software.  Your work is usually not valued. Maintenance is expected for free. Updates, new features should be done for free. Your time is worthless, they now own you as a personal developer for ever for free.",
              "score": 1,
              "created_utc": "2026-01-07 11:26:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxtyyk",
          "author": "cgoldberg",
          "text": "I wouldn't include any free maintenance and bill any ongoing maintenance by the hour at whatever your hourly rate is.",
          "score": 1,
          "created_utc": "2026-01-06 02:52:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny3377c",
          "author": "PursuingMorale",
          "text": "0-2 months of free maintenance then a monthly retainer or hourly billing when needed",
          "score": 1,
          "created_utc": "2026-01-06 21:48:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6k4yv",
          "author": "bluemangodub",
          "text": "Scraping /  bot support is hard.  It's a gamble. You complete the job, then tomorrow the site changes, your bot is broken.  Now you just put in 100 hours of work and want to get paid for that. But the client, wanted a solution and they don't have anything.\n\nIt's a risk for both of you. Ideally you factor this potential rework into your price, allowing you to redo the work when it happens and not recharge the customer, or you work for free. It's priced in.\n\nOtherwise you roll the dice. Submit the work, give them a week to submit bugs or sign off the deliverable is done and get paid. And just hope any issues happen after and you recharge for any and all additional work.\n\nAlso need to factor in, will this client give you more work? If so, it may be worth taking the hit.  If they are the type to think \"oh you can just make this little change, it won't take you long, I'd do it myself if I knew how\".  And it's a 10 hour+ change. Run.",
          "score": 1,
          "created_utc": "2026-01-07 11:23:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybkoeg",
          "author": "davak72",
          "text": "This is tricky. Iâ€™ve never delivered scraping software clients, although Iâ€™ve built scrapers to clean up client data from sites with a source of truth during an audit of client data for instance.\n\nWhen billing by the hour, I provided no free maintenance, unless it was something tiny and I didnâ€™t feel like producing an invoice for it.\n\nHowever when billing by the job at a pre-agreed rate, I would pad the estimate with a certain number of hours for a warranty period built into the original quote. The amount would depend on a number of factors such as the size of the project, risks involved, complexity of potential fixes, etc.\n\nIn your case, Iâ€™d consider these things (and many other factors):\n\n1. Are the sites intended to provide freely available information, such as institutions routinely reporting data, or do they have incentives for complicating scraping once they notice frequent similar-looking traffic, such as e-commerce or sites behind a pay wall?\n\n2. How frequently have the sitesâ€™ structure and design changed in the past? Are they fairly stable or frequently updated with new features and UX changes?\n\n3. Are you able to intercept any API call responses directly for JSON or similar data, or are you reliant on HTML and JavaScript that are more challenging to adapt to if they change?\n\nOh, and also, just make sure your contract is very clear that your software is provided for the sites as they stand today, and that any changes required due to changes by the sites themselves will require additional development which will incur additional cost.",
          "score": 1,
          "created_utc": "2026-01-08 02:10:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q12pcv",
      "title": "Scraping in Google Scholar",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q12pcv/scraping_in_google_scholar/",
      "author": "Cuaternion",
      "created_utc": "2026-01-01 12:02:19",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 0.86,
      "text": "Hi, I'm trying to do scraping with some academic profiles in Google Scholar, but maybe the server has restrictions for this activity.\nAny suggestions?\nThanks",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q12pcv/scraping_in_google_scholar/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nx60svn",
          "author": "bootlegDonDraper",
          "text": "hey OP\n\nyou'll hit rate limits everywhere when web scraping, but it's easy to get through\n\n**first solution**, throttle your requests and add random delays between requests.\n\n**second,** instead of scraping it in one go, create a scraper that scrapes a chunk of URLs every hour or so with the rate limiting in first solution\n\n*you don't want to wait?*\n\n**third and most effective,** rotate proxies. if you use a large proxy pool you can run concurrent requests to scrape tens of pages at once without ever being rate limited.\n\nif your proxies are low quality DC proxies, your requests will get blocked. if more than half of your requests aren't blocked, introduce error handling to re-request the same page with another ip if it gets blocked.\n\nvoila",
          "score": 5,
          "created_utc": "2026-01-02 00:07:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q09vdc",
      "title": "Bypassing DataDome",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q09vdc/bypassing_datadome/",
      "author": "Vlad_Beletskiy",
      "created_utc": "2025-12-31 10:25:38",
      "score": 5,
      "num_comments": 11,
      "upvote_ratio": 0.73,
      "text": "Hello, dear community!\n\nIâ€™ve got an issue being detected by DataDome (403 status) while scraping a big resource.\n\n\n\n**What works**\n\nI use Zendriver pointing to my local MacOS Chrome. Navigating to siteâ€™s main page -> waiting for the DataDome endpoint that returns DataDome token -> making subsequent requests via curl\\_cffi (on my local MacOS machine) with that token being sent as a DataDome cookie.  \nIâ€™ve checked that this token lives quite long - is valid for at least several hours, but assume even more (managed to make requests after multiple days).\n\n\n\n**What I want to do that doesnâ€™t work**\n\nI want to deploy it and opted for Docker. Installed Chrome (not Chromium) within the Docker. Tried the same algorithm as above. The outcome is that Iâ€™m able to get token from the DataDome endpoint. But subsequent curl\\_cffi requests fail with 403. Tried curl\\_cffi requests from Docker and locally - both fail, issued token is not valid.\n\nNext thing Iâ€™ve enabled xvfb that resulted in a bit better outcome. Namely, after obtaining the token the next request via curl\\_cffi succeeds, while subsequent ones fail with 403. So, itâ€™s basically single use.\n\nNext Iâ€™ve played with different user agents, set timezone, but the outcome is the same.\n\n\n\nOne more observation - thereâ€™s another request which exposes DataDome token via Set-Cookie response header. If done with Zendriver under Docker, Set-Cookie header for that same endpoint is missing.\n\nSo, my assumption is that my trust score by DataDome is higher than to show me captcha, but lower than to issue a long-living token.\n\n\n\nAnd one more observation - both locally and under Docker requests via curl\\_cffi work with 131st Chrome version being impersonated. Though, 143rd latest Chrome version is used to obtain this token. Any other curl\\_cffi impersonation options just donâ€™t work (result in 403). Why does that happen?\n\nAnd I see that curl\\_cffi supports impersonation of the following OSes only: Win10, MacOS (different versions), iOS. So, in theory it shouldnâ€™t work at all combined with Docker setup?\n\n\n\n**Question** \\- could you please point me in the right direction what to investigate and try next. How do you solve such deployment problems and reliably deploy scraping solutions? And probably you can share advice how to enhance my DataDome bypassing strategy?\n\n\n\nThank you for any input and advices!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q09vdc/bypassing_datadome/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwwa7vq",
          "author": "infaticaIo",
          "text": "DataDome tokens are usually bound to more than just a cookie. They often get tied to the full client â€œshapeâ€ (TLS and HTTP2 fingerprint, IP reputation, timing, browser signals, sometimes even local storage) so a token minted in one environment can be useless in another. That explains why local Mac works but Docker fails, and why you see â€œsingle useâ€ behavior.\n\nWhat to investigate, at a high level:\n\n* Fingerprint consistency: the environment that mints the token needs to match the environment that reuses it. If you mint in a real Chrome and replay with curl, any mismatch in TLS or HTTP2 can invalidate quickly.\n* IP consistency: tokens can be scoped to IP or ASN. Local IP vs Docker egress often differs even on the same machine if you run through different routes.\n* Header and cookie jar completeness: missing Set-Cookie under Docker usually means the JS flow or redirects differ, or a required request wasnâ€™t executed the same way.\n* Version coupling: the fact that only one curl\\_cffi impersonation works suggests the backend is keying on a very specific TLS stack and ordering.\n\nFor deployment, the reliable pattern is usually to keep the whole flow in one place. Either keep requests inside the same browser context that earned the session, or run the replay client with a fingerprint that is as close as possible to that browser and network path. Mixing â€œreal browser to get tokenâ€ with a very different HTTP client is where these systems tend to break.\n\nIf this is for a legitimate use case, the sustainable option is getting approved access or using an official feed. Trying to â€œenhance bypassâ€ is a cat and mouse game and will keep changing.",
          "score": 5,
          "created_utc": "2025-12-31 11:03:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwww0zk",
              "author": "Vlad_Beletskiy",
              "text": "Thank you.\n\nIt seems that IP binding doesn't matter that much for cookie issuing. Because I've tried to obtain DataDome cookie token both with & without proxies (running locally without Docker). And then use via curl\\_cffi for subsequent requests. And it works regardless of the proxy presence while obtaining the token.\n\nInteresting point here - I've obtained cookie using different Chrome version and different OS (MacOS) version compared to that used subsequently during curl\\_cffi impersonation, and that 131rd version still worked. However should have failed. That still seems strange.\n\n  \n\"keep requests inside the same browser context that earned the session\" - yeah, was thinking similarly.",
              "score": 1,
              "created_utc": "2025-12-31 13:49:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwlfqv",
          "author": "Dismal_Pilot_1561",
          "text": "Personally, it works great for me in Docker using a fairly heavy Linux base image, but one that helps boost my Datadome trust score.\n\nJust like you, I first warm up the proxy using a real automated browser combined with a custom captcha solver. Then, I use curl_cffi with the cookies generated by the actual browser, and I save any new cookies if they get updated (which happens quite often).\n\nThe main difference is probably that I'm forced to solve a captcha (not the main page), which significantly increases the Datadome trust score. Also, I make sure to use the correct cookie data and headers to mimic the browser I used as closely as possible.\n\nI use this method for high-frequency scraping. Without pushing it too hard and on a fairly modest machine, I scrape about 15,000 URLs in 4 hours.",
          "score": 2,
          "created_utc": "2025-12-31 12:38:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx15tfk",
          "author": "_mackody",
          "text": "Look at JA3",
          "score": 1,
          "created_utc": "2026-01-01 04:19:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhflci",
          "author": "abdullah-shaheer",
          "text": "It might be then an IP issue, maybe datacenter IP. The second thing is to use pydoll python, go to the website, make 100 or 200 requests using its inbuilt request maker that copies everything from the browser that is already opened, then do some random actions, send 200 to 400 requests again and this way, you can scale up. You can also integrate different residential proxies for doing this in scale. This is far better than copying the token and sending it. The website might be blocking you via some other way other than TLS fingerprinting, if you use pydoll, it will mimic everything from the actual session you're using and requests will never be rejected most probably. I personally have tried this method.",
          "score": 1,
          "created_utc": "2026-01-03 18:43:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxi4t9f",
          "author": "GillesQuenot",
          "text": "Like @Dismal_Pilot_1561, I use my own Datadome captcha solver which works pretty well.\n\nI use an automated browser to scrape Datadome websites. If you have a pool of resid IP, you can even avoid the use to solve captchas.\n\nHave you checked that the version match between Chrome and curl_ffi ?",
          "score": 1,
          "created_utc": "2026-01-03 20:43:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxip56m",
              "author": "Twitty-slapping",
              "text": "You mean you built your own CAPTCHA solver?",
              "score": 1,
              "created_utc": "2026-01-03 22:23:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxsaz6q",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 1,
                  "created_utc": "2026-01-05 08:34:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q7f428",
      "title": "Scaling and Monitoring",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q7f428/scaling_and_monitoring/",
      "author": "OtherwiseGroup3162",
      "created_utc": "2026-01-08 15:43:33",
      "score": 5,
      "num_comments": 16,
      "upvote_ratio": 0.86,
      "text": "I have built a lot of different web scrapers in python that use HTTP requests and they work pretty well...\n\nHowever, we are now looking to scale and orchestrate a lot of them on an ongoing basis.\n\nWhat is the best way to monitor them and if one fails, see where the fail point is easily?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q7f428/scaling_and_monitoring/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nylv44z",
          "author": "hasdata_com",
          "text": "We solve scaling with self-managed RKE2 (waaaay cheaper than managed cloud K8s). Prometheus for metrics, ClickHouse for logs, and synthetic tests running 24/7 to catch broken layouts.",
          "score": 9,
          "created_utc": "2026-01-09 15:06:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf0zka",
          "author": "jinef_john",
          "text": "Once you reach that scale, the right move is to move to docker(containerization) - so you treat the scrapers as containerized jobs, not standalone scripts.\n\nPackage each scraper in Docker and run them under a scheduler/orchestrator (Airflow, Prefect, Argo, etc.).\n\nYou'll get logs,automatic retries, health checks, metrics etc.",
          "score": 8,
          "created_utc": "2026-01-08 15:54:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyf4w9i",
              "author": "Twitty-slapping",
              "text": "simple and yet so effective. But don't you think using a lot of Dockers will require an expensive VPS?   \nWhy not just create a bash script to run multiple scripts on a single machine with pm2 or something similar and you will get the same effect",
              "score": 2,
              "created_utc": "2026-01-08 16:11:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nygwk4j",
                  "author": "No-Business-7545",
                  "text": "why would a lot of dockers require an expensive vps",
                  "score": 1,
                  "created_utc": "2026-01-08 20:50:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyfn08j",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-01-08 17:30:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nygarg0",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-08 19:13:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nygje6w",
                  "author": "webscraping-ModTeam",
                  "text": "ðŸª§ Please review the sub rules ðŸ‘‰",
                  "score": 1,
                  "created_utc": "2026-01-08 19:51:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyjzsag",
          "author": "hiren_p",
          "text": "i think you need 3 systems\n\n1. scraper job tracking, where you can see how many row has been scraped, number of urls success or failed, etc.\n\n2. you need to have track of each url you're requesting  \nlike 2xx response, 4xx response, 5xx response  \nalso if one failed based on response, i suggest retry automatically in case of scraper blocked.\n\n3. where scraper get failed  \nsome scraper failed due to load  \nsome scraper due to website changed or url changed  \nkeep log of this \n\nuse greylog which i used generally for all my project logging",
          "score": 1,
          "created_utc": "2026-01-09 06:59:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nym1nla",
          "author": "Round_Method_5140",
          "text": "What is the volume? Just run them locally on a schedule. Keep in mind a lot of cloud or vps sources may be already banned by the websites you're scraping.",
          "score": 1,
          "created_utc": "2026-01-09 15:36:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyq3hea",
          "author": "joeyx22lm",
          "text": "containers + opentelemetry",
          "score": 1,
          "created_utc": "2026-01-10 03:27:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyipw2m",
          "author": "Past-Refrigerator803",
          "text": "If you need to access a large number of websites and care about performance, cost, and observability, I would recommend **Browser4**: a lightning-fast, coroutine-safe browser with built-in crawler-grade performance and stability, as well as page-level task scheduling, logging, and metrics that capture every system action.\n\nIf your crawling workloads require real web interaction, Browser4 is particularly well suited. If you are maintaining a large and complex set of data extraction rules, Browser4 offers a hybrid data extraction approach that can potentially save you a significant amount of time.\n\nIf you also need to invoke LLM capabilities during data collection, Browser4 is an especially strong fitâ€”for example, using LLMs to analyze pages, dynamically correct extraction rules in real time, and recover from erroneous navigations or unexpected page states.",
          "score": 0,
          "created_utc": "2026-01-09 02:09:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q83adc",
      "title": "Presenting tlshttp, a tls-client wrapper from Go",
      "subreddit": "webscraping",
      "url": "https://github.com/Sekinal/tlshttp",
      "author": "Azuriteh",
      "created_utc": "2026-01-09 08:52:35",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q83adc/presenting_tlshttp_a_tlsclient_wrapper_from_go/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "nyktf2b",
          "author": "Eastern_Ad_9018",
          "text": "It seems there is no streaming request.",
          "score": 1,
          "created_utc": "2026-01-09 11:22:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyq4ezk",
              "author": "Azuriteh",
              "text": "Nope, that's due to the limitation of the underlying tls-client Go library",
              "score": 1,
              "created_utc": "2026-01-10 03:33:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pz8czz",
      "title": "Is it just me or playwright incredibly unstable",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1pz8czz/is_it_just_me_or_playwright_incredibly_unstable/",
      "author": "kilobrew",
      "created_utc": "2025-12-30 04:24:02",
      "score": 4,
      "num_comments": 3,
      "upvote_ratio": 0.76,
      "text": "Iâ€™ve been using playwright in the AWS environment and having nothing but trouble getting it to run without randomly disconnecting, â€œfailed to get worldâ€, or timeouts that really shouldnâ€™t have happened. Hell, Even running AWSâ€™s SAAS bedrock agent_core browser tool has the same issue. \n\nIt seems the only time I can actually use it is if itâ€™s installed on a full blown windows install with a GPU. \n\n\nIs it just me?\n",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1pz8czz/is_it_just_me_or_playwright_incredibly_unstable/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwoupx0",
          "author": "RandomPantsAppear",
          "text": "I ran multiple playwright instances on Fargate instances with 0.25 vcpu and 256m of ram, that were also running redis and celery. \n\nSomething is very wrong if this is the behavior you are getting. \n\n\nHow are you detecting the page load completion success/fail?\n\nHave you checked the process list to make sure processes are successfully exiting?\n\nAre you taking screenshots on the theoretical page load fails? (I am not sure how this works headless, I often run it with xvfb)",
          "score": 1,
          "created_utc": "2025-12-30 06:42:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpm1qz",
          "author": "bluemangodub",
          "text": "Not my experience at all. I have had untold issues with AWS, especially the low price instances. \n\nRun it locally, or on a proper VPS and see if you have the same issues. If not, it's AWS",
          "score": 1,
          "created_utc": "2025-12-30 10:52:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpnb6t",
          "author": "Lafftar",
          "text": "Are you using proxies?",
          "score": 1,
          "created_utc": "2025-12-30 11:04:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q5d3tt",
      "title": "Scraper tests requested.",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q5d3tt/scraper_tests_requested/",
      "author": "YouDaree",
      "created_utc": "2026-01-06 08:27:24",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.67,
      "text": "Does anyone want to test the pre-release of my updated scraper that added Wuxiaworld.\n\nYou can get the zip file here that contains the current build [Release v2.0 Prerelease: Wuxiaworld added Â· martial-god/Benny-Scraper](https://github.com/martial-god/Benny-Scraper/releases/tag/v2.0.0-prerelease)\n\nYou can get info on how to run it on the \\`NewYearResolution\\` branch [martial-god/Benny-Scraper at NewYearResolution](https://github.com/martial-god/Benny-Scraper/tree/NewYearResolution).\n\n\\### For those that don't know how to unpackage.\n\n1. Download the zip file from the prerelease.\n2. Unzip it.\n3. Add the location of that contains \\`Benny-Scraper.exe\\` to your PATH so you can be able to type \\`benny-scraper\\` into your terminal to get results like I did in my 5th recording.\n4. Follow the quick start guide found [https://github.com/martial-god/Benny-Scraper/tree/NewYearResolution#quick-start---download-a-novel-yt-dlp-style](https://github.com/martial-god/Benny-Scraper/tree/NewYearResolution#quick-start---download-a-novel-yt-dlp-style).\n\n\\*\\*Note\\*\\*: As of testing right now, mangakatana and Wuxiaworld work, novelful may work. The others I haven't used to they are up in the air.\n\nFor anyone that does decide to test, thank you in advance and let me know if you have any issues. This is not done as I still need to add a few features including the ability for a logged in user to allow the app to unlock chapters for them automatically.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q5d3tt/scraper_tests_requested/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q5hy2f",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q5hy2f/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-01-06 13:01:06",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 0.81,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levelsâ€”whether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) ðŸŒ±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring ðŸ’°",
      "permalink": "https://reddit.com/r/webscraping/comments/1q5hy2f/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "ny00qvq",
          "author": "bnovo1997",
          "text": "Hey guys! \nWe are currently hiring for a junior data collection role, check it here:  https://www.linkedin.com/jobs/view/4349339647",
          "score": 1,
          "created_utc": "2026-01-06 13:03:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny0gv87",
          "author": "SnooComics1506",
          "text": "Hello, I am looking for a webscrapper to scrape Linkedin",
          "score": 1,
          "created_utc": "2026-01-06 14:34:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny743by",
              "author": "Commercial-Show-2403",
              "text": "I am interesting  \nI am Scraping expert  \nbut Linkedin is not easy",
              "score": 1,
              "created_utc": "2026-01-07 13:37:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyhilu4",
          "author": "hhhhonzik",
          "text": "Anyone with airline award travel scraping experience?",
          "score": 1,
          "created_utc": "2026-01-08 22:27:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyr04fp",
              "author": "TraditionBig6995",
              "text": "interested",
              "score": 1,
              "created_utc": "2026-01-10 07:22:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q2tglx",
      "title": "solving BotDetect Captcha",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q2tglx/solving_botdetect_captcha/",
      "author": "Odd_Ad5698",
      "created_utc": "2026-01-03 12:29:39",
      "score": 2,
      "num_comments": 10,
      "upvote_ratio": 0.6,
      "text": "i'am working on a script that submits a form, that form has a bot detect captcha \\[A-Z0-9\\]\n\n  \ni made the script download the captcha image then i would solve it manually and let the script send the result alongside the form data and other captcha-related hidden fields\n\n  \nthe problem is that the server says the captcha solution doesn't match the image even tho it's correct  \nthat thing happens like 80% of the time even tho it's the same python code\n\n  \nmy goal is to use an ai model that i trained to solve that type of captcha",
      "is_original_content": false,
      "link_flair_text": "Bot detection ðŸ¤–",
      "permalink": "https://reddit.com/r/webscraping/comments/1q2tglx/solving_botdetect_captcha/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nxflgzj",
          "author": "Persian_Cat_0702",
          "text": "Look imto OCR. And also look for local AI models, connect with your code, which will send the captcha to the model, and the model will return an answer. Then use that answer as the captcha bypass.",
          "score": 1,
          "created_utc": "2026-01-03 13:10:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxflmul",
              "author": "Odd_Ad5698",
              "text": "thats what iam already doing, the issue is that the server wont accept the solution (most of the time) even tho the ai got it correctly",
              "score": 1,
              "created_utc": "2026-01-03 13:11:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxfxxcg",
                  "author": "andreew92",
                  "text": "Sounds like they are also relying on some other detection method to determine â€œhuman-nessâ€.\n\nSorry I donâ€™t have a solution - just really interested in your next steps.\n\nI would look into trying to answer the captcha in a more human wayâ€¦ think cursor/scroll movements, the way the answer is entered into the field, etc.\n\nGood luck and keep us posted!",
                  "score": 1,
                  "created_utc": "2026-01-03 14:24:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxgu6ho",
          "author": "Left-Personality3939",
          "text": "Try the cap solver its AI based or For solving image-based CAPTCHAs, theÂ YOLOv8Â is currently considered one of the best due to its high accuracy, efficiency, and ability to be fine-tuned for specific tasks.Â ",
          "score": 1,
          "created_utc": "2026-01-03 17:05:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhcoit",
          "author": "abdullah-shaheer",
          "text": "Avoid solving captchas as much as possible, there would be another way to do the same thing you want to do right now, go for it. If there isn't any and this is the only solution, then go for trained LLMs and OCR python.",
          "score": 1,
          "created_utc": "2026-01-03 18:29:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxheu5j",
              "author": "Odd_Ad5698",
              "text": "unfortunately theres no other way, and the main issue rn is not about ai, its about the solution get marked as \"Wrong\" by the server even tho its correct",
              "score": 1,
              "created_utc": "2026-01-03 18:39:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxhg5x9",
                  "author": "abdullah-shaheer",
                  "text": "Yeah I know, but if you do it manually, then it gets solved or still throws the same issue? If manually it works, then it's the issue that your request is getting detected by their security measure, you need to make it stealthy. What security measures are they using and what are you actually trying to do? There might be some other way or if not, then we can diagnose what is the issue here",
                  "score": 1,
                  "created_utc": "2026-01-03 18:45:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxn221o",
          "author": "nono2chevres",
          "text": "I read that it's easier to solve audio captcha, so if you have a way to ask for audio version (it should be mandatory for accessibility purpose), download the sound file and solve the captcha (with IA I guess)",
          "score": 1,
          "created_utc": "2026-01-04 15:42:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrg7ec",
          "author": "Gaming_ORB",
          "text": "I stead of downloading the image, will screenshotting it work?",
          "score": 1,
          "created_utc": "2026-01-05 04:31:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0o0z1",
      "title": "TLS fingerprint websocket client to bypass cloudflare?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q0o0z1/tls_fingerprint_websocket_client_to_bypass/",
      "author": "vroemboem",
      "created_utc": "2025-12-31 21:34:36",
      "score": 2,
      "num_comments": 4,
      "upvote_ratio": 0.67,
      "text": "What are the best stealth websocket clients (that work with nodejs)?",
      "is_original_content": false,
      "link_flair_text": "Bot detection ðŸ¤–",
      "permalink": "https://reddit.com/r/webscraping/comments/1q0o0z1/tls_fingerprint_websocket_client_to_bypass/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nwzg735",
          "author": "Afraid-Solid-7239",
          "text": "What's the url for the socket you're trying to connect to? I'll play around with it",
          "score": 1,
          "created_utc": "2025-12-31 21:50:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1n7fo",
          "author": "army_of_wan",
          "text": "have you tried building a stealth client using golang's tls\\_library, i bet you could do that using gemini 3 or Claude opus",
          "score": 1,
          "created_utc": "2026-01-01 06:47:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxe4ygl",
          "author": "cq_in_unison",
          "text": "\\`curl\\_cffi\\` can usually get around it, but you mind need to go up to \\`nodriver\\` or scripted playwright. don't forget that it's not always just cloudflare, but a number of prevention tactics.",
          "score": 1,
          "created_utc": "2026-01-03 05:57:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0atdm",
      "title": "Scraping market data CS2/CSGO",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1q0atdm/scraping_market_data_cs2csgo/",
      "author": "Short_Bus_6284",
      "created_utc": "2025-12-31 11:24:18",
      "score": 1,
      "num_comments": 7,
      "upvote_ratio": 0.57,
      "text": "Good evening! Hope this is the right place to ask. I've reached a point where I need metadata and, especially, up to date prices for Counter Strike 2 skins. I understand that there are paid APIs and the Steam API that provide real-time metadata and prices, but to be honest, Iâ€™d prefer to go with free solutions. This brings me to scrapers, since I havenâ€™t been able to find any free APIs that meet my needs. Iâ€™ve dug through GitHub and found some repos, but most of them either donâ€™t work with modern JavaScript heavy sites, or they only scrape limited metadata. The only repo I found that works well isÂ [this one](https://github.com/eovacius/csgodatabase-scraper/), which returns both prices and metadata fairly quickly. However, the project is missing some content, like souvenirs, stickers, cases, etc. It looks like itâ€™s still pretty new, so Iâ€™m sure the content will be updated soon, but I donâ€™t want to wait too long. So, I was hoping some of you might know of any resources or public databases/sites that would let me scrape CS2 skin information. Or, if there are any other free methods to get this info without scraping, that would be super helpful too. Thanks in advance!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1q0atdm/scraping_market_data_cs2csgo/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nx6pmh6",
          "author": "scraping-test",
          "text": "*\"free methods to get this info without scraping\"* \n\ni can think of two: an intern... or divine intervention xd\n\n  \nto be real, the repo you've mentioned looks really well-built. I'm sure you can somewhat easily modify it to extract the additional stuff you want. if you don't know how, clone to cursor and get claude on it",
          "score": 3,
          "created_utc": "2026-01-02 02:34:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8ss17",
              "author": "Short_Bus_6284",
              "text": "Yes, config looks promising but unfortunately im not familiar with Golang. So i guess i would go with claude. Thanks",
              "score": 1,
              "created_utc": "2026-01-02 12:36:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5pmdf",
          "author": "_i3urnsy_",
          "text": "Is your issue with scraping the site or do you need a better data source to scrape info from?",
          "score": 1,
          "created_utc": "2026-01-01 23:04:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8spp9",
              "author": "Short_Bus_6284",
              "text": "Both scraper and target site are fine. Just need more data",
              "score": 2,
              "created_utc": "2026-01-02 12:36:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8k5jl",
          "author": "bluemangodub",
          "text": "you either figure out how to do it yourself, or you pay someone to do.\n\nIF you don't want to wait for the repo you are using, reach out to the dev and offer them money to compete the parts you are needing.",
          "score": 1,
          "created_utc": "2026-01-02 11:25:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8t214",
              "author": "Short_Bus_6284",
              "text": "I'll contant them, either we scale it together or i pay and they do the job. honestly might be cheaper than paid Api or scraper services.",
              "score": 1,
              "created_utc": "2026-01-02 12:38:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdz8vu",
          "author": "Afraid-Solid-7239",
          "text": "What information in specific are you looking to get that the current repo doesn't have?   \nThe files in /json/ of the repo seem fairly extensive? I'm able to find souvenirs and cases in some json files.\n\nThough the dev of that repo in specific, is just scraping csgodatabase.  \nIs the data you want available on csgodatabase.com? I don't mind writing up a scraper for it, for you.",
          "score": 1,
          "created_utc": "2026-01-03 05:14:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}