{
  "metadata": {
    "last_updated": "2026-02-19 09:09:59",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 20,
    "total_comments": 170,
    "file_size_bytes": 143562
  },
  "items": [
    {
      "id": "1r5712p",
      "title": "Scrapling v0.4 is here - Effortless Web Scraping for the Modern Web",
      "subreddit": "webscraping",
      "url": "https://i.redd.it/chi2m8gjjljg1.png",
      "author": "0xReaper",
      "created_utc": "2026-02-15 06:01:04",
      "score": 251,
      "num_comments": 38,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r5712p/scrapling_v04_is_here_effortless_web_scraping_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5h0uzf",
          "author": "Reddit_User_Original",
          "text": "Nice job. Been familiar with your project since v0.3. It's the best of its kind as far as i can tell. I use scrapling when using curl cffi is insufficient, and i need something more powerful. How do you stay on top of the anti bot tech? Have you had to implement changes in response to any new anti bot tech recently? Thanks so much for building this tool.",
          "score": 14,
          "created_utc": "2026-02-15 07:24:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5i40el",
              "author": "0xReaper",
              "text": "Thanks, mate. That means a lot to me. \n\nThe thing is, I have been working in the Web Scraping field for years, and since I made the library, I use it every day. So it's always under heavy testing from me; most of the time, I find issues before users report them because of that.\n\nRegarding security, before switching to Web Scraping, I spent about 8 years in the information security field, including bug hunting. So I was an ethical hacker before all of that. And I spent some time working as backend.",
              "score": 15,
              "created_utc": "2026-02-15 13:19:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hjjyo",
          "author": "Satobarri",
          "text": "Why canâ€™t I decline your cookies on your page?",
          "score": 3,
          "created_utc": "2026-02-15 10:26:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5i30l4",
              "author": "0xReaper",
              "text": "I have fixed it, thanks for pointing that out",
              "score": 9,
              "created_utc": "2026-02-15 13:12:19",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5hwu1b",
              "author": "0xReaper",
              "text": "Oh, I didnâ€™t notice that. Let me have a look at it, I have just switched to zensical with this update so I might have missed something in the configuration.",
              "score": 3,
              "created_utc": "2026-02-15 12:25:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5i31e4",
                  "author": "Satobarri",
                  "text": "Thanks. Not a biggie but makes it suspicious for European visitors.",
                  "score": 3,
                  "created_utc": "2026-02-15 13:12:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5k4fsw",
          "author": "NoN4meBoy",
          "text": "Does it handle datadome ?",
          "score": 3,
          "created_utc": "2026-02-15 19:31:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kfwlh",
          "author": "24props",
          "text": "Iâ€™m currently on my phone and will review this later. I believe that for many people today, due to the widespread use of AI coding, it will be beneficial to create a skill (agentskills.io) to assist users who utilize AI for development or integration. Only because LLMs are never trained on immediate new versions of anything and have knowledge gaps/cutoffs.",
          "score": 3,
          "created_utc": "2026-02-15 20:29:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kk0cb",
              "author": "0xReaper",
              "text": "Yes, I agree, I will work on this soon. I'm just taking a well-deserved rest before working on the next version. There is a lot more to add.",
              "score": 5,
              "created_utc": "2026-02-15 20:51:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nz6jy",
          "author": "Flat_Agent_9174",
          "text": "Wow, it's an amazing tool !",
          "score": 3,
          "created_utc": "2026-02-16 11:20:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jn3on",
          "author": "JerryBond106",
          "text": "Should i use some vpn for this as well, so i don't get ip banned? (I'm new to this, i read proxy is included but don't know the big picture in scraping yet, as it changes rapidly and i wasn't ready to start safely yet)",
          "score": 2,
          "created_utc": "2026-02-15 18:06:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kepjy",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-15 20:23:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5mfpfp",
                  "author": "webscraping-ModTeam",
                  "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
                  "score": 1,
                  "created_utc": "2026-02-16 03:27:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kyzi6",
          "author": "515051505150",
          "text": "One thing Iâ€™ve struggled with is determining the maximum number of requests per minute I can send to a site before getting rate limited or blocked. Is there a feature within scrapling that can help automatically determine the max threshold of scrapes before a siteâ€™s counter-measures kick in?",
          "score": 2,
          "created_utc": "2026-02-15 22:07:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o0e3k",
          "author": "Flat_Agent_9174",
          "text": "Can it bypass Datadome ?",
          "score": 2,
          "created_utc": "2026-02-16 11:31:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o2dcz",
          "author": "imbuilding",
          "text": "Will be trying it out! Thanks",
          "score": 2,
          "created_utc": "2026-02-16 11:47:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5t5bjw",
          "author": "mischiefs",
          "text": "Great project mate! i'm not well versed in scrapping but i'm doing a pet project and got to use it. Got me impressed. Same feeling i got when i installed and tested tailscale, clickhouse or duckdb (more of a data engineer myself lol). it just work!",
          "score": 2,
          "created_utc": "2026-02-17 04:07:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xxi05",
              "author": "0xReaper",
              "text": "Thanks mate! that made my day :D",
              "score": 1,
              "created_utc": "2026-02-17 21:54:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o63g0q2",
          "author": "Careful_Ring2461",
          "text": "Made an Instagram and Tripadvisor scraper using Opus and your scrapling MCP without any issues. You're doing amazing work for newbies like me! ",
          "score": 2,
          "created_utc": "2026-02-18 18:09:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i11sy",
          "author": "Overall-Suit-5531",
          "text": "Interesting! Does it manage JavaScript too?",
          "score": 2,
          "created_utc": "2026-02-15 12:58:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5i31c0",
              "author": "0xReaper",
              "text": "yup",
              "score": 2,
              "created_utc": "2026-02-15 13:12:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hchnq",
          "author": "One-Spend379",
          "text": "Great job ðŸ‘ \nCan it scrap allegro. pl ?",
          "score": 1,
          "created_utc": "2026-02-15 09:17:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jynxn",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-15 19:02:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mfrf7",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-16 03:27:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kdbhy",
          "author": "strasbourg69",
          "text": "Could i use this to scan for emails and phone numbers of for example plumbers, regionally targetted",
          "score": 1,
          "created_utc": "2026-02-15 20:16:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5krjs1",
          "author": "saadcarnot",
          "text": "Can it avoid anti bot stuff like google enterprise v3 captcha?",
          "score": 1,
          "created_utc": "2026-02-15 21:29:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nfd1o",
          "author": "ChallengeEmergency11",
          "text": "How free?",
          "score": 1,
          "created_utc": "2026-02-16 08:15:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5oqdnx",
          "author": "mayodoctur",
          "text": "Does this work for scraping news articles like Al Jazeera, Substack, blogs etc ?",
          "score": 1,
          "created_utc": "2026-02-16 14:21:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ot7op",
          "author": "RageQuitNub",
          "text": "very interesting, does it manage a list of proxy or we have to supply the proxy list?",
          "score": 1,
          "created_utc": "2026-02-16 14:36:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65a9vn",
              "author": "0xReaper",
              "text": "You have to supply it",
              "score": 1,
              "created_utc": "2026-02-18 23:18:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5p82uk",
          "author": "SnooFloofs641",
          "text": "How good is this with anti bot checks and stuff?",
          "score": 1,
          "created_utc": "2026-02-16 15:50:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5t3ae0",
          "author": "Muhammadwaleed",
          "text": "If I want to download videos from a social media site such as facebook such as my saved videos so I can clear my saved list, can it do that?",
          "score": 1,
          "created_utc": "2026-02-17 03:54:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uv0wy",
          "author": "Sensitive_Nobody409",
          "text": "Works with reCaptcha v3 Enterprise?",
          "score": 1,
          "created_utc": "2026-02-17 12:46:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wz6cb",
          "author": "arvcpl",
          "text": "will try it out, thanks",
          "score": 1,
          "created_utc": "2026-02-17 19:12:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o613430",
          "author": "Sparklist",
          "text": "Can I use to scrap photos from a airbnb accomodation page ?",
          "score": 1,
          "created_utc": "2026-02-18 10:25:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pjf4g",
          "author": "mikeb550",
          "text": "how do you deal with companies who forbid scraping their sites?  any of you customers get taken to court?",
          "score": 0,
          "created_utc": "2026-02-16 16:43:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5ptfe",
      "title": "I can scrape anything",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r5ptfe/i_can_scrape_anything/",
      "author": "0xMassii",
      "created_utc": "2026-02-15 20:57:12",
      "score": 68,
      "num_comments": 103,
      "upvote_ratio": 0.71,
      "text": "No selenium, playwright or puppeteer shit, I can scrape anything in full request mode, bypassing every bot protection. It doesn't matter if is Cloudflare, Akamai, PerimeterX etc  \nAfter years in this filed I believe it's time to give something back to the community. I'll start to release open source stuff for the people who want to learn. Let me know which one is the most interesting topic.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r5ptfe/i_can_scrape_anything/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5kqgqc",
          "author": "Raidrew",
          "text": "My body is ready",
          "score": 36,
          "created_utc": "2026-02-15 21:24:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kufee",
              "author": "heelstoo",
              "text": "Can you scrape me, Focker?",
              "score": 22,
              "created_utc": "2026-02-15 21:44:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5l5g7s",
                  "author": "emprezario",
                  "text": "You can scrape anything with nipples. ðŸ¤·â€â™‚ï¸",
                  "score": 12,
                  "created_utc": "2026-02-15 22:42:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kx6se",
          "author": "mizhgun",
          "text": "I can type 1000 characters per minute. It's just total gibberish, though.",
          "score": 17,
          "created_utc": "2026-02-15 21:58:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kytyf",
              "author": "0xMassii",
              "text": "No jokes here, do a check on me",
              "score": -8,
              "created_utc": "2026-02-15 22:07:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l2skh",
          "author": "indicava",
          "text": "Why the hell is this crap being upvoted?",
          "score": 32,
          "created_utc": "2026-02-15 22:28:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l3vi5",
              "author": "0xMassii",
              "text": "The real question is why not?",
              "score": -27,
              "created_utc": "2026-02-15 22:34:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l3e21",
          "author": "beachandbyte",
          "text": "Press X to doubt. \n\nAkami and Cloudflare inspect the TLS stack. It would be possible to spoof that but challenging. But then they pass back js that profiles the browser apis to suspicious requests and the payload in that js is constantly changing. So at least for those requests you would need to either mock the entire surface of the browser Api and return what they would consider reasonable values (at that point just using a browser is likely far easier). Wonâ€™t say itâ€™s 100% not possible but unless your trick is proxying through some other process that is actually getting around all the protection you wonâ€™t be bypassing  cloudflare or Akami purely with requests.",
          "score": 8,
          "created_utc": "2026-02-15 22:31:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l45m1",
              "author": "0xMassii",
              "text": "Bro, bro, bro bot protection are my bitches\nIâ€™m the man who destroy the whole â€¦â€¦â€¦â€¦ website :)",
              "score": -16,
              "created_utc": "2026-02-15 22:35:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l9umx",
                  "author": "You_Cant_Win_This",
                  "text": "ok no we know you are fraud for sure",
                  "score": 5,
                  "created_utc": "2026-02-15 23:07:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kmtz5",
          "author": "EffectiveSeat1505",
          "text": "CF",
          "score": 3,
          "created_utc": "2026-02-15 21:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kymuh",
              "author": "0xMassii",
              "text": "I have a custom solver for CF, soon will be OS, Iâ€™ll start from tomorrow",
              "score": 3,
              "created_utc": "2026-02-15 22:06:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5lqmym",
                  "author": "MurkBRA",
                  "text": "Everyone who tried to make this open source received a DMCA takedown notice.",
                  "score": 3,
                  "created_utc": "2026-02-16 00:46:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5kr2r4",
              "author": "UnlikelyLikably",
              "text": "Second that",
              "score": 1,
              "created_utc": "2026-02-15 21:27:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l632w",
          "author": "who_am_i_to_say_so",
          "text": "Your mom scraped deezenuts with her snaggletooth. Scraping must run in the family.",
          "score": 11,
          "created_utc": "2026-02-15 22:46:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l6f8f",
              "author": "0xMassii",
              "text": "No sense but good for u ðŸ¤£",
              "score": -3,
              "created_utc": "2026-02-15 22:47:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l6jx3",
                  "author": "who_am_i_to_say_so",
                  "text": "I couldnâ€™t resist. Haha",
                  "score": 1,
                  "created_utc": "2026-02-15 22:48:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kllaj",
          "author": "yyavuz",
          "text": "let's gooooooo",
          "score": 8,
          "created_utc": "2026-02-15 20:59:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kp5vy",
          "author": "PTBKoo",
          "text": "Cloudflare turnstile, I have to open a chrome browser for a every single turnstile solve and scraping over 1k daily and my poor server canâ€™t keep up.",
          "score": 3,
          "created_utc": "2026-02-15 21:17:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lnyvm",
          "author": "BossDailyGaming",
          "text": "Talk is cheap, make that gh repo",
          "score": 3,
          "created_utc": "2026-02-16 00:30:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kntfi",
          "author": "askolein",
          "text": "Interested. Def the non selenium request thing. How come? Direct webassembly?",
          "score": 2,
          "created_utc": "2026-02-15 21:10:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kpbqg",
          "author": "No-Exchange2961",
          "text": "Please post the link to the open source!",
          "score": 2,
          "created_utc": "2026-02-15 21:18:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ksxhi",
          "author": "sudbull",
          "text": "LinkedIn",
          "score": 2,
          "created_utc": "2026-02-15 21:36:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kyolu",
              "author": "0xMassii",
              "text": "Ez brother",
              "score": 2,
              "created_utc": "2026-02-15 22:06:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5lb6q1",
          "author": "EntrepreneurSea4283",
          "text": "What's the the hardest thing to scrape",
          "score": 2,
          "created_utc": "2026-02-15 23:14:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lbgki",
              "author": "0xMassii",
              "text": "It depends, for me few POST requests on Akamai gave me hard times",
              "score": 1,
              "created_utc": "2026-02-15 23:16:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5lufaj",
          "author": "crawford5002",
          "text": "Teach me your ways",
          "score": 2,
          "created_utc": "2026-02-16 01:09:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kpi7j",
          "author": "san-vicente",
          "text": "Let me know",
          "score": 1,
          "created_utc": "2026-02-15 21:19:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kq3i9",
          "author": "Any-Dig-3384",
          "text": "DM if legit",
          "score": 1,
          "created_utc": "2026-02-15 21:22:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kri7l",
          "author": "Ladytron2",
          "text": "Facebook events?",
          "score": 1,
          "created_utc": "2026-02-15 21:29:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ks7mg",
          "author": "Srijaa",
          "text": "Iâ€™m in!",
          "score": 1,
          "created_utc": "2026-02-15 21:33:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ku6zv",
          "author": "uneatenbreakfast",
          "text": "The chosen one",
          "score": 1,
          "created_utc": "2026-02-15 21:43:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kup2g",
          "author": "puzz-User",
          "text": "Letâ€™s go!",
          "score": 1,
          "created_utc": "2026-02-15 21:45:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kvsse",
          "author": "living_david_aloca",
          "text": "Well letâ€™s see it",
          "score": 1,
          "created_utc": "2026-02-15 21:51:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kw0gx",
          "author": "Sensitive_Nobody409",
          "text": "Recaptcha v3 pls â¤ï¸ðŸ‘ðŸ¿",
          "score": 1,
          "created_utc": "2026-02-15 21:52:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kyx8n",
              "author": "0xMassii",
              "text": "Ez, but if you want to start from somewhere check solver online",
              "score": 1,
              "created_utc": "2026-02-15 22:07:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kw4m0",
          "author": "Sensitive_Nobody409",
          "text": "Build your own chromium?",
          "score": 1,
          "created_utc": "2026-02-15 21:53:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kx1va",
          "author": "slumdogbi",
          "text": "Amazon with sponsored products",
          "score": 1,
          "created_utc": "2026-02-15 21:57:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kyy8v",
              "author": "0xMassii",
              "text": "Ez",
              "score": 1,
              "created_utc": "2026-02-15 22:07:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ld0o2",
                  "author": "slumdogbi",
                  "text": "ðŸ‘ðŸ‘ðŸ‘ðŸ‘",
                  "score": 1,
                  "created_utc": "2026-02-15 23:25:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5lh0ov",
                  "author": "RealAmerik",
                  "text": "I'm interested in this as well.",
                  "score": 1,
                  "created_utc": "2026-02-15 23:49:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kydj8",
          "author": "Overall-Suit-5531",
          "text": "Do it",
          "score": 1,
          "created_utc": "2026-02-15 22:04:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l0cai",
          "author": "Dorkits",
          "text": "Where",
          "score": 1,
          "created_utc": "2026-02-15 22:15:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l0f36",
              "author": "0xMassii",
              "text": "Everywhere",
              "score": 0,
              "created_utc": "2026-02-15 22:15:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l0kvi",
                  "author": "Dorkits",
                  "text": "The link bro",
                  "score": 1,
                  "created_utc": "2026-02-15 22:16:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l1vnf",
          "author": "sojufles",
          "text": "How not to get rate limited by Cloudflare, with a scraper in a container using 1 IP?",
          "score": 1,
          "created_utc": "2026-02-15 22:23:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l2hlv",
              "author": "0xMassii",
              "text": "Thatâ€™s is related to the website, you need to find vulns on the target or misconfig, otherwise cf will block you",
              "score": 1,
              "created_utc": "2026-02-15 22:26:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l34c6",
                  "author": "sojufles",
                  "text": "Thanks for the reply, it also seems that whenever my scraper in container is rate limited. I can access the site from my local browser. Do you have any idea or experiences why this happens?",
                  "score": 1,
                  "created_utc": "2026-02-15 22:29:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l2jsb",
          "author": "Antop90",
          "text": "The problem isn't emulating the browser, there are already plenty of super effective libraries for that. The real issue is having a pool of very expensive IPv4 addresses to rotate.",
          "score": 1,
          "created_utc": "2026-02-15 22:26:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l2xi4",
              "author": "0xMassii",
              "text": "Yeah, but you can scrape also with cheap resi, but you will be slower obv",
              "score": 1,
              "created_utc": "2026-02-15 22:28:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l3hye",
                  "author": "Antop90",
                  "text": "Sometimes speed is essential, and the only way to achieve it is with a solid proxy pool. no magical alternatives exist",
                  "score": 1,
                  "created_utc": "2026-02-15 22:32:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l2stb",
          "author": "Eudaimonic_me",
          "text": "Google trends",
          "score": 1,
          "created_utc": "2026-02-15 22:28:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l3qzm",
          "author": "saadcarnot",
          "text": "I am working on automating an time critical workflow, however it's protected with Recaptcha v3 Enterprise. I can't afford to wait for solver to get back. I need to avoid it all together. \n\nAny suggestions for me?",
          "score": 1,
          "created_utc": "2026-02-15 22:33:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l4a1w",
              "author": "0xMassii",
              "text": "Look for vulns on the website target",
              "score": 2,
              "created_utc": "2026-02-15 22:36:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l54i8",
                  "author": "saadcarnot",
                  "text": "Couldn't find any, can you list down what to look for? Site is recreation.gov",
                  "score": 1,
                  "created_utc": "2026-02-15 22:40:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5lawia",
          "author": "Inner_Grape_211",
          "text": "can u explain each of ur bypasses? with general tips pls?",
          "score": 1,
          "created_utc": "2026-02-15 23:13:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lbhn3",
              "author": "0xMassii",
              "text": "Iâ€™ll do",
              "score": 1,
              "created_utc": "2026-02-15 23:16:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ls78m",
          "author": "TheKillerScope",
          "text": "Can you scrape Dune or GMGN? PolyMarket? If so, I'd be VERY interested in connecting with you and discussing a possible partnership.",
          "score": 1,
          "created_utc": "2026-02-16 00:55:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lus8f",
          "author": "sunrise_zc",
          "text": "customized browserï¼Ÿ",
          "score": 1,
          "created_utc": "2026-02-16 01:12:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m1ddn",
          "author": "RaiseRuntimeError",
          "text": "Is this r/webscrapingcirclejerk",
          "score": 1,
          "created_utc": "2026-02-16 01:54:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m7qli",
          "author": "Villain_99",
          "text": "LinkedIn please",
          "score": 1,
          "created_utc": "2026-02-16 02:35:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mas5b",
          "author": "G_S_7_wiz",
          "text": "How do you get all amzon reviews..you need cookies for all reviews",
          "score": 1,
          "created_utc": "2026-02-16 02:54:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mb88o",
          "author": "LifeShmucksSoMuch",
          "text": "can you bypass shape?",
          "score": 1,
          "created_utc": "2026-02-16 02:57:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l60ag",
          "author": "DotEnvironmental4718",
          "text": "Can you scrape my butt? Itâ€™s itching",
          "score": 1,
          "created_utc": "2026-02-15 22:45:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l6atr",
              "author": "0xMassii",
              "text": "Crazy statement",
              "score": 2,
              "created_utc": "2026-02-15 22:47:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l9cbf",
          "author": "tradegreek",
          "text": "How do you scrape stuff which requires constantly refreshed cookies (say every 5-10 mins) without using a tool like selenium to obtain the new cookies?",
          "score": 1,
          "created_utc": "2026-02-15 23:04:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l9ki9",
              "author": "0xMassii",
              "text": "Maybe i donâ€™t need cookies either to get the data :)",
              "score": 2,
              "created_utc": "2026-02-15 23:05:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l9n2v",
                  "author": "tradegreek",
                  "text": "I donâ€™t get how is that possible?",
                  "score": 0,
                  "created_utc": "2026-02-15 23:05:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5la1z9",
                  "author": "You_Cant_Win_This",
                  "text": "You do though",
                  "score": 0,
                  "created_utc": "2026-02-15 23:08:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5lg4cf",
          "author": "Afriendlywhiteguy",
          "text": "Teach me your ways",
          "score": 1,
          "created_utc": "2026-02-15 23:44:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lgsti",
          "author": "dca12345",
          "text": "Whatâ€™s wrong with PlayWright?",
          "score": 1,
          "created_utc": "2026-02-15 23:48:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l1e5u",
          "author": "hulleyrob",
          "text": "Docker support?",
          "score": 0,
          "created_utc": "2026-02-15 22:20:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l2be9",
              "author": "0xMassii",
              "text": "We donâ€™t need, but we can",
              "score": 2,
              "created_utc": "2026-02-15 22:25:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l2s5l",
                  "author": "hulleyrob",
                  "text": "Could be very interesting if it will still beat all the tech in a docker container I donâ€™t think there is anything out there that can do that right now.",
                  "score": 0,
                  "created_utc": "2026-02-15 22:28:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l790o",
          "author": "Acceptable-Sea-3248",
          "text": "LinkedIn",
          "score": 0,
          "created_utc": "2026-02-15 22:52:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l7aws",
          "author": "Acceptable-Sea-3248",
          "text": "LinkedIn seems to be the most difficult",
          "score": 0,
          "created_utc": "2026-02-15 22:52:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ldt2m",
          "author": "TillOk5563",
          "text": "The ServiceNow variables fields?",
          "score": 0,
          "created_utc": "2026-02-15 23:30:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5liydm",
          "author": "oizysplutus",
          "text": "Enterprise hcap",
          "score": 0,
          "created_utc": "2026-02-16 00:01:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lk4i8",
          "author": "tuttipazzo",
          "text": "Ok.  You got me interested as well.",
          "score": 0,
          "created_utc": "2026-02-16 00:07:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ll7qd",
          "author": "Sensitive-Finger-404",
          "text": "facebook marketplace",
          "score": 0,
          "created_utc": "2026-02-16 00:14:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lmb1u",
          "author": "bgj556",
          "text": "â€¦ yes please",
          "score": 0,
          "created_utc": "2026-02-16 00:20:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l7598",
          "author": "AdhesivenessEven7287",
          "text": "Reddit",
          "score": -1,
          "created_utc": "2026-02-15 22:51:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7crb3",
      "title": "Reverse engineering the new Datadome VM ðŸ”¥",
      "subreddit": "webscraping",
      "url": "https://github.com/xKiian/datadome-vm",
      "author": "xkiiann",
      "created_utc": "2026-02-17 17:34:44",
      "score": 33,
      "num_comments": 4,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r7crb3/reverse_engineering_the_new_datadome_vm/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5wsih5",
          "author": "ragingpot",
          "text": "I was literally just doing this and this popped up, awesome stuff!!",
          "score": 6,
          "created_utc": "2026-02-17 18:41:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5z47bi",
          "author": "Krokzter",
          "text": "Good work, good luck with the scholarship!",
          "score": 2,
          "created_utc": "2026-02-18 01:41:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wzmpi",
          "author": "arvcpl",
          "text": "that's very handy. thank you!",
          "score": 1,
          "created_utc": "2026-02-17 19:14:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67j4nr",
          "author": "RobSm",
          "text": "Good work",
          "score": 1,
          "created_utc": "2026-02-19 08:15:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4kdoy",
      "title": "scraping vinted - TLS fingerprinting + session rotation",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r4kdoy/scraping_vinted_tls_fingerprinting_session/",
      "author": "DataKazKN",
      "created_utc": "2026-02-14 13:16:31",
      "score": 28,
      "num_comments": 12,
      "upvote_ratio": 0.98,
      "text": "spent the last couple weeks trying to scrape vinted (european secondhand marketplace, ~50M users). figured i'd share what i learned because their anti-bot setup is genuinely impressive and i haven't seen much written about it.\n\n**the stack they're running**\n\nvinted sits behind cloudflare with a pretty aggressive bot management config. but the interesting part isn't cloudflare itself â€” it's what happens after you get past the initial challenge.\n\ntheir API requires a valid session with specific cookies generated through a multi-step auth flow. you can't just grab a token and go. the cookies are tied to your TLS fingerprint, so if your http client doesn't match what a real browser would send, the session gets invalidated silently. no error, no 403 â€” just empty responses or stale data.\n\n**what didn't work**\n\nstarted with plain axios/undici. instant blocks. moved to got-scraping (which uses header-generator under the hood for realistic TLS). got further but still inconsistent â€” about 40% of requests would return empty arrays even with valid-looking sessions.\n\nturned out the issue was TLS fingerprint rotation. if your JA3/JA4 hash changes between requests but your session cookie stays the same, they flag it. so you need to either:\n- keep a consistent TLS fingerprint per session\n- or rotate both session AND fingerprint together\n\n**what actually worked**\n\nended up using a playwright-based cookie factory that spins up a real browser context, completes the oauth flow, captures the authenticated cookies, then passes those to a lightweight http client (got-scraping) that maintains the same TLS fingerprint for the lifetime of that session.\n\nthe key insight was treating sessions as disposable units â€” each one gets ~50-100 requests before you burn it and create a new one. tried pushing to 200+ and the ban rate went from ~2% to ~30%.\n\nalso had to handle their per-country domain routing. vinted merged most of western EU into one catalog but the API endpoints still differ by country (fr, de, es, it, nl, be, pt, etc). each needs its own session pool.\n\n**results**\n\nmanaged to pull ~960 items for a single query (\"nike air max\", france) in one run with full metadata â€” prices, seller stats, item condition, photos, timestamps. the data quality is actually really good once you get past the anti-bot layer.\n\nthe whole session rotation + cookie factory approach runs without proxies for moderate volume. for heavier loads you'd obviously want to add proxy rotation but the TLS fingerprint consistency matters way more than IP rotation in my experience.\n\n**lessons learned**\n\n- TLS fingerprinting is the real gatekeeper now, not just IP bans\n- silent failures (empty responses instead of 403s) are way harder to debug than explicit blocks\n- session lifetime management matters more than raw request volume\n- got-scraping > undici/axios for anything cloudflare-protected\n- playwright for auth, lightweight client for data â€” don't use a full browser for every request\n\ncurious if anyone else has tackled vinted or similar cloudflare-protected marketplaces. the TLS fingerprint binding to sessions was a new one for me.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r4kdoy/scraping_vinted_tls_fingerprinting_session/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5pj7qi",
          "author": "StoneSteel_1",
          "text": "If you were using python, I would have suggested you curl-cffi for TLS Signature spoofing.",
          "score": 4,
          "created_utc": "2026-02-16 16:42:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5q06q7",
              "author": "DataKazKN",
              "text": "yeah curl-cffi is solid for that. i actually started in python but moved to node for the mcp server integration. tls-client (node equivalent) works similarly. the tricky part with vinted isnt just the tls fingerprint though, they also check oauth2 token consistency across requests so you need the full session flow not just spoofed headers.",
              "score": 1,
              "created_utc": "2026-02-16 18:00:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5stjv0",
          "author": "Ok-Depth-6337",
          "text": "Vinted is not too hard\n\nI reversed the app few months ago and i think qorks great. \n\nNo issues. \nTake a look to the app and frida :)",
          "score": 3,
          "created_utc": "2026-02-17 02:52:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5v7u3q",
              "author": "DataKazKN",
              "text": "yeah frida is solid for this kind of thing. i went the browser route mostly because i wanted to keep it simple and not deal with app reversing, but good call on the mobile api. way less fingerprinting headaches there.",
              "score": 1,
              "created_utc": "2026-02-17 14:01:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5pciwo",
          "author": "jagdish1o1",
          "text": "I would've done the same, i once also had vinted related project, although client rejected the proposal. I had the same approach, use the browser for sessions and than use http client to scrape further. \n\nI once used this same approach to mimic an api request where i used the headless browser to clear the auth flow and than grab the requests headers and mimic the requests with http client. I dockerized the whole thing and made few things arguable so that i can run mutliple instances with different logins and scale it. Later i hosted the docker image on AWS and used ECS tasks to scale it. I had so much fun when i was working on this project. \n\n",
          "score": 1,
          "created_utc": "2026-02-16 16:11:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5q06p8",
              "author": "DataKazKN",
              "text": "yeah exactly, browser for the initial session then switch to raw http for speed. the cookie/token extraction step is the key part. once you have valid oauth2 tokens from the browser session you can hammer the api endpoints way faster than any headless approach. vinted specifically checks JA3 fingerprints so the browser bootstrap is basically mandatory unless you spoof TLS signatures.",
              "score": 1,
              "created_utc": "2026-02-16 18:00:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5u7rp8",
          "author": "Aromatic_Tower_1827",
          "text": "Were you able to scrape the description? Or the features?",
          "score": 1,
          "created_utc": "2026-02-17 09:33:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5v7u4z",
              "author": "DataKazKN",
              "text": "yeah descriptions and features come through fine in the api response. the item endpoint returns pretty much everything, title, description, brand, size, condition, photos, price history. just need a valid session token first.",
              "score": 1,
              "created_utc": "2026-02-17 14:01:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o60yr19",
          "author": "NamedBird",
          "text": "Note: It seems that you are actively circumventing security measures, which can be a criminal offense.  \nI hope that you've talked to a lawyer about this. (Some possible consequences are *no* joke.)\n\nRegardless of all that, yes, TLS fingerprinting is now the level we're at.  \nAnd if this dance keeps going, expect content loads to be factored in as well.  \nAt that point it would be more efficient to just let an actual browser load it all.",
          "score": 0,
          "created_utc": "2026-02-18 09:46:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62mggl",
              "author": "DataKazKN",
              "text": "yeah fair point, its always good to check the legal side. the tool itself just reads public listing data though, same stuff you see browsing the site normally. no login bypass or anything like that. but yeah tls fingerprinting is wild now, headless browsers are basically becoming the default approach for anything serious.",
              "score": 1,
              "created_utc": "2026-02-18 15:57:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nzt0y",
          "author": "[deleted]",
          "text": "[removed]",
          "score": -2,
          "created_utc": "2026-02-16 11:26:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ov0to",
              "author": "webscraping-ModTeam",
              "text": "ðŸ‘” Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
              "score": 1,
              "created_utc": "2026-02-16 14:45:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r7c3e9",
      "title": "WebMCP is insane....",
      "subreddit": "webscraping",
      "url": "https://v.redd.it/gzt3djhw73kg1",
      "author": "GeobotPY",
      "created_utc": "2026-02-17 17:11:54",
      "score": 19,
      "num_comments": 5,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "AI âœ¨",
      "permalink": "https://reddit.com/r/webscraping/comments/1r7c3e9/webmcp_is_insane/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o60cp6h",
          "author": "THenrich",
          "text": "WebMCP works only with websites that voluntarily expose functions that expose their data.  \nSince popular websites protect their data they most probably won't expose their data through webMCP to the world. ",
          "score": 3,
          "created_utc": "2026-02-18 06:24:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60le1c",
              "author": "GeobotPY",
              "text": "At the current state yes, but cooked up a open-source community hub where you can upload configs for sites and then when other agents visit that site a chrome extension fetches webMCP's for that particular site. \n\n[webmcp-hub.com](http://webmcp-hub.com)",
              "score": 2,
              "created_utc": "2026-02-18 07:41:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wqdcy",
          "author": "somedude4949",
          "text": "If it becomes a thing I don't think everyone will adapt it because you are exposing internal endpoints to automations this is not gonna work for every website",
          "score": 1,
          "created_utc": "2026-02-17 18:31:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65e98f",
          "author": "Robertusit",
          "text": "You know how to use an AI, local or in cloud, to connect to Google Chrome and help to make a web scraping ?",
          "score": 1,
          "created_utc": "2026-02-18 23:39:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3hy8h",
      "title": "Chrome extension that auto-detects and extracts data from any webpage",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r3hy8h/chrome_extension_that_autodetects_and_extracts/",
      "author": "mjiqbal",
      "created_utc": "2026-02-13 06:33:11",
      "score": 14,
      "num_comments": 5,
      "upvote_ratio": 0.95,
      "text": "we got tired of writing one-off scripts for simple scraping jobs (product listings, search results, directories, tables ) so we built a Chrome extension called Detect and Extract that handles the common cases without writing a single line of code.\n\nHow it works:\n\n1.  Click the extension icon on any page, it automatically detects data structures (tables, lists, grids, card layouts, etc.)\n2.   Preview the data in a grid, rename or delete columns you don't need\n3.   Export as CSV, Excel, JSON, or copy straight to clipboard\n\n The part we are most proud of, multi-page crawling:\n\n*   Point it at the \"Next\" button or page numbers, and it'll auto-crawl through all pages\n*   Handles numbered pagination (1, 2, 3...) and \"Next >\" button patterns\n*   Auto-detects and dismisses modal popups that block the page during crawls (upgrade prompts, cookie banners, etc.)\n*   Built-in deduplication so you don't get repeat rows across pages\n\n  Other stuff:\n\n*  Visual element picker â€” if auto-detection misses something, click any element on the page and it finds all similar items\n*  Presets â€” save your scraping config per site so you don't have to set it up again next time\n*  Minimal permissions â€” only uses activeTab, no background data collection, no account required\n*  Works on most sites â€” e-commerce, directories, forums, search results, dashboards\n\n  What it's NOT:\n\n*   Not a Selenium/Puppeteer replacement â€” this is for visual, interactive scraping\n*   Won't bypass anti-bot measures or CAPTCHAs\n*   Not great for SPAs that require authentication flows or infinite nested navigation\n\n\n\nTrying to make it a solid free tool for people who need quick data without spinning up a whole scraping pipeline.\n\nWould love feedback from this community. What features would make this more useful for your workflows?\n\n[Detect and Extract](https://chromewebstore.google.com/detail/detect-and-extract/kkmibnjkdelljnkoibconbnaenpliefi)\n\nhttps://preview.redd.it/lj6mmc2xg7jg1.png?width=1681&format=png&auto=webp&s=9a0545a48f68145697e2a74d99da783ae07320da\n\n  \nHeads up about the install warning: If you have Chrome's Enhanced Safe Browsing turned on (Chrome Settings > Privacy), you'll see a \"Proceed with caution\" dialog when installing. This is completely normal for all newly published extensions it's not a security issue with the extension itself. Google flags every new extension until it builds up a trust score over time through installs and automated safety reviews. Just click \"Continue to install\" and it works fine. Users with Standard Safe Browsing won't see this at all.  \nReference URL [Regarding that warning](https://support.google.com/chrome/answer/2664769?visit_id=639065601437510841-3127678855&p=cws_enhanced_safe_browsing&rd=1#10745467)  \n",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r3hy8h/chrome_extension_that_autodetects_and_extracts/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o55xo30",
          "author": "Gold_Emphasis1325",
          "text": "I made a few versions of these and settled on one that I sometimes use. I was thinking about trying to productize and ideally make money or at least make connections, but with vibe coding out these days, a whole genre of apps and utilities and projects are now \"useless\" in the customer sense, but gold for the individual -- hyper personalized, \"free (minus your time and tokens)\" and something to showcase....\n\nPrivate-source open roadmap for anyone building their own:  \n\\- cloudflare box interaction  \n\\- old and 2025+ style captchas (and beta unseen human detection bypass)  \n\\- secure interaction with API layer / MCP - LLM - RAG - persistence  \n\\- auth/auth  \n\\- payments system  \n\\- thought through TOS of the plugin/API, privacy (what users will accept) and payments  \n\\- plan for dealing with fragility, constant updates",
          "score": 1,
          "created_utc": "2026-02-13 13:57:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5awbvi",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-14 06:51:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b4cax",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-14 08:06:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5bd2mc",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-14 09:32:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3uw0e",
      "title": "Built two scrapers for european markets what should I learn next?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r3uw0e/built_two_scrapers_for_european_markets_what/",
      "author": "Free-Path-5550",
      "created_utc": "2026-02-13 17:10:14",
      "score": 11,
      "num_comments": 5,
      "upvote_ratio": 0.87,
      "text": "Been working on scrapers for a couple of European marketplaces this past week. My first attempt in this space. I finished two that successfully \"work\"... for now.  Some things I picked up so far:\n\n\\- Raw data isn't enough. Adding computed fields like deal detection, engagement scoring, and price tracking across runs makes the output way more useful than a static dump.\n\n\\- European platforms have aggressive anti-bot compared to US sites from my research. Took real effort to get stable.\n\n\\- You don't need a browser for everything. Keeping it lightweight makes a huge difference.\n\n\\- Biggest lesson learned... was how much I hate DataDome. I was able to slip past it a few times, but usually blocked the next run. I eventually learned to just go around it if possible.\n\nStill early in this spcraping. What should I be learning next? What separates a decent scraper from a great one?  Thanks!",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r3uw0e/built_two_scrapers_for_european_markets_what/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o59r43b",
          "author": "Bitter_Caramel305",
          "text": "Try running your existing scrapers on a few hundred thousand product/items and you'll realize that scrapers without proxies and regular maintains are not stable at all. ",
          "score": 9,
          "created_utc": "2026-02-14 01:48:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64rcxp",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-18 21:46:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o64t934",
              "author": "webscraping-ModTeam",
              "text": "ðŸš«ðŸ¤– No bots",
              "score": 1,
              "created_utc": "2026-02-18 21:54:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r7i5gt",
      "title": "How do i deal with cloudflare turnstile anti-bot using curl cffi?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r7i5gt/how_do_i_deal_with_cloudflare_turnstile_antibot/",
      "author": "letopeto",
      "created_utc": "2026-02-17 20:43:46",
      "score": 11,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "Hey folks I'm trying to do some light scraping against a Cloudflare-protected site and Iâ€™m running into issues. Was wondering if anyone experienced can provide some advice/tips.\n\n**What Iâ€™m doing**\n\nUse a stealth browser (e.g., nodriver) to load the target page and complete whatever Cloudflare presents (no issues with this, i get the cf bypassed and cf clearance cookie).\n\nAfter the browser run, I extract cookies (notably cf_clearance, plus any other set cookies) and then switch to a lightweight HTTP client (curl-cffi) for the actual requests.\n\nThe browser is pinned to a specific UA / UA-latest (e.g., â€œChrome v144â€ UA string).\n\nIn curl-cffi, I attach the cookie jar + headers and use an impersonation profile like impersonate=\"chrome-latest\".\n\n**The issue**\n\nEven with the cookies present, several times the curl-cffi request still gets hit with a Cloudflare challenge again even though the cookies has not expired (could have been just retrieved 5 seconds ago).\n\n**any idea why this is happening? my current hypothesis right now is -**\n\nIs this happening because the clearance/session is bound to signals beyond cookies, like:\n\n* UA + TLS fingerprint mismatch (stealth browser chrome-latest profile might be say â€œChrome 144â€ vs curl-cffi â€œchrome-latestâ€ might be Chrome 143 or something?)\n* Or could it be something else?\n\nQuestions\n\n* How important is it to match the stealth browser version with the curl-cffi version? If this is indeed the underlying issue, whats the best way to \"synchronize\" the chrome version profile in the stealth browser vs the curl-cffi chrome version profile it is using? I don't want to pin it specifically to a version like chrome v144 because then i have to go every single time and update the version manually (and if the version gets too old it likely will trigger an anti-bot challenge as well).\n\n* Is this potential mismatch an issue, or is curl-cffi just not great and triggers the cf challenge often?",
      "is_original_content": false,
      "link_flair_text": "Bot detection ðŸ¤–",
      "permalink": "https://reddit.com/r/webscraping/comments/1r7i5gt/how_do_i_deal_with_cloudflare_turnstile_antibot/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5xje0c",
          "author": "GillesQuenot",
          "text": "You need to stick to the same version on both Chrome and curl_ffi, because TLS fingerprinting reveal the ciphers order that is different from version to version",
          "score": 4,
          "created_utc": "2026-02-17 20:48:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61s4fr",
              "author": "WhyWontThisWork",
              "text": "They are doing that level of inspection? And it never changes the order?",
              "score": 1,
              "created_utc": "2026-02-18 13:26:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o62lbkp",
                  "author": "Hour_Analyst_7765",
                  "text": "The TLS is kind of like the headers for the HTTPS session/tunnel.\n\nYou can also check a browser footprint by checking the HTTP headers, in particular user agent, the Accept and Sec headers. But also the order. What curl-cffi does for you, is spoof the same TLS algorithms/options and HTTP header values, including the orderening.\n\nNow there is 1 caveat: some TLS browsers (Chromeâ€š do some \"greasing\" to randomize TLS order iirc. But HTTP does not. So there are various fingerprints like JA3 and JA4(H) that represents these properties.\n\nWAF look at these properties to determine if a browser is real. If any of these is off, its a giant red flag. TLS and HTTP fingerprints don't change per user or OS, but it can change per browser type and version.\n\nOne problem with curl-cffi though: the spoofing is quite static. And in their infinite wisedom in the latest versions, they're hard coding their headers \\*inside\\* the curl executable.\n\nSec-Fetch-Dest for example, specifies what kind of page the browser is expecting to be requested. For HTML pages this is \"document\", but for CSS, javascript or images this is different. Even iframes get denoted as such. I personally haven't encountered many sites that directly blocks access if you get these wrong (maybe video embeds that get denied if you open that page in a separate tab).. but I won't be surprised if some WAF monitor these headers for bot detection, and we'll see more 403's if you get them wrong.",
                  "score": 1,
                  "created_utc": "2026-02-18 15:51:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o60k9xl",
          "author": "sojufles",
          "text": "Followed!",
          "score": 1,
          "created_utc": "2026-02-18 07:31:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r61a90",
      "title": "Need tought websites to scrape for testing",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r61a90/need_tought_websites_to_scrape_for_testing/",
      "author": "Transformand",
      "created_utc": "2026-02-16 05:39:35",
      "score": 10,
      "num_comments": 25,
      "upvote_ratio": 0.86,
      "text": "I've been developing my own piece of code, that so far has been able to bypass anti-bot security I had a tough time cracking before at scale (such as PerimiterX).\n\nCan you share what sites you think are difficult to access/scrape?\n\nI want to test out my scraper more before open sourcing it",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r61a90/need_tought_websites_to_scrape_for_testing/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5mz52h",
          "author": "ry8",
          "text": "Try BassProShop.",
          "score": 3,
          "created_utc": "2026-02-16 05:50:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mz7p3",
          "author": "Lemon_eats_orange",
          "text": "Jokingly if you can get past the anti bot defenses on piracy anime sites that would be a site to see. Though like seriously you open the chrome developer tools on some of those sites and they will feedback loop you and run your resources dry. \n\nOff the top of my head these sites might be difficult. \nWalmart.com, zoro.com, naver.com and it's subdomains can be incredibly difficult, hermes.com, safeway.com. Definitely shopee.",
          "score": 3,
          "created_utc": "2026-02-16 05:51:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61uds4",
              "author": "FerencS",
              "text": "Yeah, Iâ€™ve heard of some fucking dark magic on anime sites",
              "score": 1,
              "created_utc": "2026-02-18 13:38:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5n45v8",
          "author": "thePsychonautDad",
          "text": "Protonmail email creation steps.",
          "score": 3,
          "created_utc": "2026-02-16 06:33:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xblq9",
              "author": "GillesQuenot",
              "text": "Did you managed to pass the captcha? Looks tough. IA required, maybe a local VLM. Do you have some hints?",
              "score": 2,
              "created_utc": "2026-02-17 20:10:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5r6ztk",
          "author": "ertostik",
          "text": "Let me know if someone pass sahibinden.com protection.",
          "score": 2,
          "created_utc": "2026-02-16 21:25:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5naddf",
          "author": "LT823",
          "text": "Instagram followers of profile",
          "score": 1,
          "created_utc": "2026-02-16 07:29:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nc08b",
          "author": "GuNiKz",
          "text": "reuters, I tried once, but I wasn't successful ",
          "score": 1,
          "created_utc": "2026-02-16 07:44:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ndn62",
          "author": "Little-traveler-1995",
          "text": "Try Hyatt hotel, I guess without using any browser automation tool it is highly impossible to crawl any link.",
          "score": 1,
          "created_utc": "2026-02-16 07:59:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nleq8",
          "author": "trololololol",
          "text": "Google Shopping",
          "score": 1,
          "created_utc": "2026-02-16 09:13:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nm4zv",
          "author": "Unlikely1529",
          "text": "bet365 hehe",
          "score": 1,
          "created_utc": "2026-02-16 09:20:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5no5cn",
          "author": "brateq",
          "text": "Aliexpress, Darty(dot)com",
          "score": 1,
          "created_utc": "2026-02-16 09:39:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5omyc2",
          "author": "vorty212",
          "text": "builtwith",
          "score": 1,
          "created_utc": "2026-02-16 14:02:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5op22g",
          "author": "Krokzter",
          "text": "Idealista is the most aggressive use of Datadome I've seen",
          "score": 1,
          "created_utc": "2026-02-16 14:14:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xbya5",
              "author": "GillesQuenot",
              "text": "Take a look at mobile.de `^^`",
              "score": 1,
              "created_utc": "2026-02-17 20:12:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5uxgdw",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-17 13:01:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vpyg2",
              "author": "webscraping-ModTeam",
              "text": "ðŸ‘” Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
              "score": 1,
              "created_utc": "2026-02-17 15:34:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5w1c83",
          "author": "jagdish1o1",
          "text": "try this one \"extDOTto\" it's a torrent related website with very strong bot protection you can also try with \"hianimeDOTto\" \n\nlet me know once you make it live i'll try it. ",
          "score": 1,
          "created_utc": "2026-02-17 16:31:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ykluc",
          "author": "Legend_Troll_007",
          "text": "LinkedIn",
          "score": 1,
          "created_utc": "2026-02-17 23:55:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65u92l",
          "author": "Gold_Emphasis1325",
          "text": "Indeed, Facebook, [msn.com](http://msn.com)",
          "score": 1,
          "created_utc": "2026-02-19 01:09:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n08hq",
          "author": "nofilmincamera",
          "text": "Sounds kind of insane but I like banks for this. Specifically self service portals that are public. Tended to have recaptcha 3 plus latered protection.",
          "score": 0,
          "created_utc": "2026-02-16 05:59:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n7b3a",
          "author": "scorpiock",
          "text": "Try payment, social media sites",
          "score": 0,
          "created_utc": "2026-02-16 07:01:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r642hh",
      "title": "anyone else tired of ai driven web automation breaking every week?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r642hh/anyone_else_tired_of_ai_driven_web_automation/",
      "author": "Ok_Abrocoma_6369",
      "created_utc": "2026-02-16 08:20:11",
      "score": 9,
      "num_comments": 25,
      "upvote_ratio": 0.77,
      "text": "Seriously, my python scrapers fall apart the moment a site changes a class name or restructures a div.   \nwe mainly monitor competitor pricing, collect public data, and automate internal dashboards but maintaining scripts is killing productivity.   \ni have heard ai can make scrapers more resilient, teaching a system to understand a page and find data on its own.\n\ni am curious what people are actually running in production:   \nwhat does your stack look like?  \ndo you use ai powered web interaction or llms to control browsers?   \nhow do you handle scaling and avoiding blocks in the cloud?\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r642hh/anyone_else_tired_of_ai_driven_web_automation/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5nhyub",
          "author": "CouldBeNapping",
          "text": "We solved it by automating browsers in Windows. 25 VPSâ€™ with a bare bones Windows 10 or 11 install. \nRotating VPNs and residential proxies. \n\nBeen 100% successful for the last 18 months.",
          "score": 3,
          "created_utc": "2026-02-16 08:40:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nuhcs",
              "author": "HarambeTenSei",
              "text": "Not even Linux browsers?",
              "score": 1,
              "created_utc": "2026-02-16 10:38:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5nxerj",
                  "author": "CouldBeNapping",
                  "text": "Most \"real\" people who browse online stores are on Windows.   \nJust adds to the legitimacy of the user profile.",
                  "score": 4,
                  "created_utc": "2026-02-16 11:05:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ngnov",
          "author": "ProgrammerRadiant847",
          "text": "we have been testing a few different frameworks and tbh its still a bit of a patchwork.",
          "score": 2,
          "created_utc": "2026-02-16 08:27:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nwflt",
          "author": "AdhesivenessOld8612",
          "text": "Selector drift is brutal, curious whether people are solving this with smarter patterns in tools like Playwright or actually leaning into AI-driven extraction in production.",
          "score": 2,
          "created_utc": "2026-02-16 10:56:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nhm4x",
          "author": "Azuriteh",
          "text": "I don't recommend AI scrapers at all, the cost scales a lot.\n\nIn any case, if you want to truly do it like that, I personally haven't found any library and actually delivers, you have to build your own agentic harness, but you can take inspiration on already existing harnesses for other tools to make things easier for you.\n\nAnother problem is that even for existing scraping AI libraries, they use easily detected browser emulators, e.g. Playwright, which adds another level of me not wanting to use them.",
          "score": 2,
          "created_utc": "2026-02-16 08:37:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nhvdo",
              "author": "Azuriteh",
              "text": "For scaling it's the same old story: use proxies according to the sophistication of the anti-bot defenses of the site and use camouflaged browsers, keeping track of antibot cookies per browser session.\n\nI usually try to reverse engineer the websites to instead just use TLS fingerprinting mimicry though, as it scales much better, but not always possible sadly (due to IP quality when deploying in cloud most of the time!) ",
              "score": 2,
              "created_utc": "2026-02-16 08:39:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5nirxq",
              "author": "One_Development8489",
              "text": "Playwrath + claude code with chrome plugin (which can auto open chrome and debug as a agent)",
              "score": 1,
              "created_utc": "2026-02-16 08:48:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nhroz",
          "author": "Worth-Culture5131",
          "text": "cost is the killer. we built a prototype with an llm orchestrator, and it was brilliant... until we saw the api bill for 10k pages a day. ",
          "score": 1,
          "created_utc": "2026-02-16 08:38:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nvak6",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-16 10:46:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nycyr",
              "author": "webscraping-ModTeam",
              "text": "âš¡ï¸ Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-02-16 11:13:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5p62jm",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-16 15:41:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qkd9y",
              "author": "webscraping-ModTeam",
              "text": "âš¡ï¸ Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-02-16 19:34:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5pmxjh",
          "author": "Coding-Doctor-Omar",
          "text": "Dont rely on normal html parsing. Look for either internal API requests or json blobs inside script tags in the page source.",
          "score": 1,
          "created_utc": "2026-02-16 16:59:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61pxk2",
          "author": "jagdish1o1",
          "text": "Weâ€™ve also created an internal tool to track prices of our competitors, Iâ€™ve built the tool using cloudflare browser rendering it has a built in AI endpoint which let you describe schema with a prompt. \n\nThe cloudflare browser rendering respect bot protections, if sites that youâ€™re scraping has strong bot protection that this might not work.",
          "score": 1,
          "created_utc": "2026-02-18 13:13:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63v9tb",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-18 19:17:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63zeps",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-18 19:36:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o65u30y",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-19 01:08:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6654y2",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-19 02:12:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ofrca",
          "author": "forklingo",
          "text": "yeah this is basically the tax you pay for scraping anything modern. in my experience ai doesnâ€™t magically fix brittle selectors, it just moves the brittleness up a layer unless youâ€™re really thoughtful about how you structure it. weâ€™ve had better luck combining solid dom heuristics, fallback selectors, and some light semantic matching rather than full llm driven browsing. for scaling and blocks itâ€™s mostly about boring stuff like good proxy rotation, sane request rates, and making traffic look human instead of blasting endpoints. curious if anyone here is actually running llm controlled browsers in prod without the costs getting wild.",
          "score": 0,
          "created_utc": "2026-02-16 13:22:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6uo0k",
      "title": "Reddit Scraping for Academia",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r6uo0k/reddit_scraping_for_academia/",
      "author": "yousufq9",
      "created_utc": "2026-02-17 03:07:44",
      "score": 9,
      "num_comments": 8,
      "upvote_ratio": 0.81,
      "text": "Hey guys! Ive been trying to collect Reddit data for a project Im doing in my course and wanted to get some advice. I applied for the official API access using my institute email but my request was rejected. So I tried alternate methods such as Pushshift but it seems to have been restricted now. Also tried using reddit's JSON endpoints but that only gave me around 1000 of the most recent posts of a sub. Im trying to get all posts in 2024 and 2025 so that method doesnt work for me. Also tried using selenium on old reddit but havent been successful so far.  \n  \nDoes anyone have any suggestions for alternative methods to scrape subreddits or tips on how to get official API access? Any help would be appreciated!",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r6uo0k/reddit_scraping_for_academia/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5t093s",
          "author": "RandomPantsAppear",
          "text": "There is such sad irony here. Aaron Swartz - a cofounder of Reddit - was driven to suicide by criminal charges around illicitly scraping academic data. \n\nAnd now Reddit is rejecting applications from academia to access Reddit, and must be driven to scraping. \n\nFor those of us that believe in the free flow of information, this is unspeakably sad.",
          "score": 15,
          "created_utc": "2026-02-17 03:34:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5t0jtu",
          "author": "swapripper",
          "text": "You donâ€™t need to scrape. Check pushshift sub for dumps on academictorrents. Download what you need & process/filter locally.",
          "score": 6,
          "created_utc": "2026-02-17 03:36:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xaue3",
              "author": "yousufq9",
              "text": "Thanks for the suggestion looking into this!",
              "score": 1,
              "created_utc": "2026-02-17 20:07:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tnleu",
          "author": "jackie-nohashtag",
          "text": "The real academic finding here is that Reddit would rather die than let you read it programmatically. Publish that.",
          "score": 3,
          "created_utc": "2026-02-17 06:25:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vheio",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-17 14:52:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vpd1s",
              "author": "webscraping-ModTeam",
              "text": "ðŸª§ Please review the sub rules ðŸ‘‰",
              "score": 2,
              "created_utc": "2026-02-17 15:32:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5us8z2",
          "author": "Gold_Emphasis1325",
          "text": "Sounds like you're trying to do stuff you're not supposed to. Maybe find another way to apply your craft/learning. There are so many open source datasets... Scraping reddit, bots... these are all increasingly frowned upon as people are experience very real, extreme impacts to their lives as a result of oversold and misused automation, Agents, AI and LLMs",
          "score": 1,
          "created_utc": "2026-02-17 12:27:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7gbwk",
      "title": "how do you decide when something truly requires proxies?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r7gbwk/how_do_you_decide_when_something_truly_requires/",
      "author": "Free-Path-5550",
      "created_utc": "2026-02-17 19:38:16",
      "score": 8,
      "num_comments": 13,
      "upvote_ratio": 0.9,
      "text": "Ok so Im still new and learning this space. I got into it because I was building another app and realized data was the moat. Two weeks later my hyper focus has me deep in this.\n\nSo far Ive built about a dozen tools for different sites at different difficulty levels and theyve worked... mostly. Now I hit a site that seems like it might require a proxy.\n\nBut my real question is not just â€œshould I use a proxyâ€ ..its how do you reason about access patterns and anti-bot defenses before deciding to add infrastructure like proxies?\n\nE.g. Recently I ran into another harder site and most advice online just said use proxies. I didnt want to jump straight to paying for infrastructure so I kept digging. Eventually I found a post suggesting trying the mobile app. I did a MITM looked at the mobile API and that ended up working with a high success rate.\n\nThat made me realize if I had just followed the first advice I saw I wouldnt have learned anything.\n\nSo how do you decide when something truly requires proxies versus when you just havent found the right access pattern yet. Are there signals you look for or is it mostly experience.",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r7gbwk/how_do_you_decide_when_something_truly_requires/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5xalxu",
          "author": "Azuriteh",
          "text": "A small tip: everyone says \"just use proxies\" because they sell proxies.\n\n  \nNow, true scalable web scraping starts from tricks like the one you did, but most of the time if the app is serious enough it won't work at all. More and more apps have anti-bot protected endpoints to the point the MITM gets detected straight away unless you're using an actual Android device connected through ADB to your computer.\n\nMy rule of thumb is, you ONLY use proxies when you need to scale or you need more concurrent requests and you start getting blocked because of that usage pattern. It doesn't make sense to start using proxies if you can't reliably bypass protections first.\n\nAlso don't go straight for \"residential\" or \"mobile\" proxies. Try cheap datacenter proxies first, they can take you very far for quite a lot of things!! But of course they have their limitations too, mainly due to their IP reputation. For example, in your use case, since I'm guessing you found an unprotected endpoint, they might still have some rate limiting in place, so you'll get 429's pretty often, the way to bypass that is to have for example 100 datacenter proxies and making requests in each of those proxies. If you found out that the limit of requests per IP is of about 50 reqs/min, then you can have 50 reqs/min \\* amount of proxies you have.",
          "score": 15,
          "created_utc": "2026-02-17 20:05:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xdb4z",
              "author": "scrapingtryhard",
              "text": "also want to add some sites support ipv6 proxies which are wayyy cheaper its a trick many people dont know, but most cloudflare sites have ipv6 on by default",
              "score": 8,
              "created_utc": "2026-02-17 20:18:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o62uiyj",
          "author": "hasdata_com",
          "text": "Proxies are for when you've tried everything else and still get blocked. Or when volume is high enough that you hit rate limits no matter what. Also if you need specific geo, like SERP data changes by country, so you need proxies in those locations. But try everything else first.",
          "score": 10,
          "created_utc": "2026-02-18 16:33:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5z1yee",
          "author": "Wise_Top4267",
          "text": "You start using proxies when the website starts blocking you. Similarly, if you're going to do a massive scraping, be careful who you scrape and where, because if your ISP doesn't change your IP address, they could add you to blacklists, etc., resulting in a massive block for something as simple as accessing Amazon.",
          "score": 6,
          "created_utc": "2026-02-18 01:29:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60eyr2",
          "author": "CriticalOfSociety",
          "text": "If you have to ask this question then you probably don't need proxies.\n\nProxies are only ever needed when you need concurrency or any type of high scale scraping, since any scraper is much easier to detect when it's sending 10+ requests/s through the same IP. And even this depends on the site's antibot system and it's not always the case.",
          "score": 2,
          "created_utc": "2026-02-18 06:44:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62cqvi",
          "author": "justincampbelldesign",
          "text": "I scrape reviews from the google playstore 100k+ in one go. I look for an app then download all the reviews (star rating, review text, reviewer name, etc.)   \nIn this case I decided that I needed proxies because I didn't want jobs to fail or get blocked part way through.",
          "score": 2,
          "created_utc": "2026-02-18 15:12:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63l8yx",
          "author": "Hour_Analyst_7765",
          "text": "Proxies are my default. My whole framework is set up to use them by default.\n\nThe nice thing is I don't have to worry about rate limiting that much anymore. Now I'm not hammering sites like I'm DDOSing them -- but more like: ooh I've launched 3 jobs in parallel? Oh well, I'm not getting 429 (Too Many Requests) so everything just sails smoothly.\n\nAnd its not like most of my scrapers run jobs that often (most of them only have to complete once a day, some on a hourly basis).\n\nBut still, I do have a few that launches the indexer every 1-2min. And then on top comes new content to grab, plus any images, ajax data, etc. on top. It all adds up and even though I could set really long delays between everything, its still nice to see everything being pulled in swiftly with minimal latency.\n\nAnother advantage: if one IP does get blocked for some duration, I'm not bothered about it at all. All my scripts auto-retry failed jobs. If I'm scraping from 1 IP, 24/7, refreshing the index every 2 minutes.. yeah I'm guessing I will be blocked within a few hours to days. So this is also about redundancy than necessary to function.\n\nFood for thought: if you rely on one proxy provider, and for whatever reason that provider goes down, they are getting blocked by Cloudflare, or they pull the plug for their service, what do you do? At present, I'm DIY'ing a lot of my proxy management, rotation and cloudflare challenges. But inevitably that script will break someday and even though I've redundant proxies, I do not have redundant scraping strategies. So I'm thinking about adding 1 or even 2 tiers of backup strategies in case that fails. Things like scraping APIs that promise a turn-key solution for grabbing HTML from the web, but cost a lot more $$$ per request. But for commercial jobs, it may be worth using if a script falls over on the weekend or holiday due to some (semi-)foreseeable cause.",
          "score": 2,
          "created_utc": "2026-02-18 18:32:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5y92vo",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-17 22:52:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zh1i0",
              "author": "webscraping-ModTeam",
              "text": "ðŸš«ðŸ¤– No bots",
              "score": 1,
              "created_utc": "2026-02-18 02:48:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o60pahd",
          "author": "Free-Path-5550",
          "text": "Thanks for all the responses. I'm sending very few requests at a time right now so it's not really a scale thing. more the anti-bot side that had me wondering. I was trying to understand the reasoning process before reaching for infrastructure and whether I was on the right track. Sounds like I am so I'll keep plugging away and for sites i cant work through, ill just come back when i know more.",
          "score": 1,
          "created_utc": "2026-02-18 08:17:16",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o60z5ny",
          "author": "Pauloedsonjk",
          "text": "Always use a proxy, because you can be blocked and easily identified.\nI analyze dev tools without proxy, and get blocked in any sites for any minutes or hours, then I did a hot spot in my smartphone to analyze the dev tools again.\nIt is possible to use a proxy in your browser too.",
          "score": 1,
          "created_utc": "2026-02-18 09:49:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61bui5",
          "author": "WhyWontThisWork",
          "text": "What did you use to MITM it?",
          "score": 1,
          "created_utc": "2026-02-18 11:38:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r75el6",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r75el6/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-02-17 13:02:04",
      "score": 8,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levelsâ€”whether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) ðŸŒ±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring ðŸ’°",
      "permalink": "https://reddit.com/r/webscraping/comments/1r75el6/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o60rdvm",
          "author": "Hopeless_Scraping",
          "text": "Identity â€œcrisisâ€ - how to monetize my â€œskillsâ€?\n\nhey guys\n\nbasically what the title says. i am currently lost on how to actually monetize my reversing and scraping skills and could use some advice from people actually doing this.\n\na bit about me... i donâ€™t have a degree or any professional dev background. i'm 100% self taught. honestly since LLMs got so good iâ€™ve been able to \"code\" a ton of tools that were way out of my league before. yeah i still have to rely on them for the heavy lifting but it works.\n\nwhenever i see someone post a coding task i try to jump on it if i can handle it but itâ€™s super inconsistent. over a whole year it barely adds up to anything because the tasks just aren't there frequently enough.\n\nthe thing is i have the setup. i have great resources for digital goods, unique proxy providers, the whole nine yards. i'm confident enough that i can answer like 95% of the questions in this sub without even touching an LLM.\n\ni know my way around rest apis and tls stuff (using curl_cffi with my own fingerprints etc). i'm familiar with cdp and camoufox for browser automation and i get how anti-bot mechanisms work. token harvesting, bypassing wafs/captchas, header construction... i can do all of that. i can reverse web apps and desktop apps too.\n\nso what now? i feel like i have the toolkit but no blueprint on how to turn this into a real income stream. \n\nanyone else been in this spot? how did you transition from just \"knowing stuff\" to actually getting paid?",
          "score": 5,
          "created_utc": "2026-02-18 08:37:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o613w8n",
          "author": "Moiz_khurram",
          "text": "Looking for a technical co-founder not a hire, actual equity partner.\n\nBuilt a B2B lead data platform to $35K+ ARR with my brother. We aggregate data from GMB, BuiltWith, Crunchbase, and about ten other sources and deliver it through Slack to agencies. Two people, mostly manual, 450+ sales calls, 370+ community members.\n\nThe product needs to be rebuilt properly. The roadmap involves scraping infrastructure at scale, proxy rotation, anti bot evasion, LinkedIn data extraction, email verification pipelines, and eventually intent-based scraping from job boards. These are the exact problems this community solves every day.\n\nLooking for someone who's done this at real scale and wants co founder equity rather than a freelance rate. 15-30% equity, 4-year vest, 1 year cliff, legal entity already registered.\n\nIf that sounds interesting DM me with something you've built.",
          "score": 2,
          "created_utc": "2026-02-18 10:32:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67mth0",
              "author": "fts_now",
              "text": "Which markets do you serve right now?",
              "score": 1,
              "created_utc": "2026-02-19 08:51:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ykeff",
          "author": "Koyaanisquatsi_",
          "text": "Interesting thread, looking forward to connecting to fellow scrappers",
          "score": 1,
          "created_utc": "2026-02-17 23:54:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65swxd",
              "author": "Bitter_Caramel305",
              "text": "or fellow marketers",
              "score": 1,
              "created_utc": "2026-02-19 01:01:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o64k4c2",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-18 21:12:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o64qlxy",
              "author": "webscraping-ModTeam",
              "text": "âš¡ï¸ Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-02-18 21:42:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r8s88p",
      "title": "I web scraped 72,728 courses from the catalogs of 7 Universities",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r8s88p/i_web_scraped_72728_courses_from_the_catalogs_of/",
      "author": "digital__navigator",
      "created_utc": "2026-02-19 06:50:55",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 0.84,
      "text": "Hey everyone,\n\n  \nI used Python Requests and bs4 and sometimes Selenium to write web scraping scripts to web scrape course catalogs.\n\nHeres a small part of Stanfords.\n\nhttps://preview.redd.it/hngsv9w8dekg1.png?width=1856&format=png&auto=webp&s=20a86e74eeb460ddea53188184c4bdf190c5602d\n\nThen created a system to organize the data, put it in databases, query some statistics, and pipeline it into html files which I present on a website I created, called DegreeView.\n\n  \nI am not selling anything on the site. Its ***currently*** just a project.\n\n  \nThis allowed me to get the number of courses and departments in a universities course catalog, the longest and shortest coursename, and sort all departments by how many courses they have revealing the biggest and smallest departments. \n\nhttps://preview.redd.it/c92z8zaeeekg1.png?width=2926&format=png&auto=webp&s=d67f2dcaff5918d0a05d13890883de45669c96b6\n\n  \nAnd create a page for each department in the course catalog where I do something similar:\n\n* Get the number of courses in the department\n* The shortest and longest course name\n* Other things like what percent are upper-division courses, what percent are lower, and what percent are grad courses\n\nhttps://preview.redd.it/d8gz568feekg1.png?width=2890&format=png&auto=webp&s=ff20bf4324107446c81debdd588deb8accff44ad\n\nFor each university I have to write a custom web scraping script but the general structure of every universities catalog I have scraped is similar, so I haven't had to change too much for any one of them. The hardest was the first one I did, UT Austin, and also the real hardest part was creating the system that handles everything once the data is obtained and allows me to work with differentiating data across different universities. \n\n  \nAlso Stanford was hard to scrape cause I had to use Selenium to get Javascript rendered data.\n\n  \nWeb scraping is definitely the backbone of this project so hopefully some of you guys here find this interesting. \n\nThe only reason I kept this project going and didn't give up is because I always had in my mind that it would be very scalable, and I think it is. I just need to do more web scraping.\n\n  \nCheck out the site at [degreeviewsite.com](https://www.degreeviewsite.com/)",
      "is_original_content": false,
      "link_flair_text": "Scaling up ðŸš€",
      "permalink": "https://reddit.com/r/webscraping/comments/1r8s88p/i_web_scraped_72728_courses_from_the_catalogs_of/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o67i76g",
          "author": "SignificantMath9703",
          "text": "Check on the website's mobile responsiveness but generally the idea is so superb",
          "score": 3,
          "created_utc": "2026-02-19 08:06:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67jh1a",
              "author": "digital__navigator",
              "text": "LMAOO yeah the home page is not responsive.\n\nIts generally not good practice to make excuses but...college is busy lmao.\n\nI realllyyy wanted to stop delaying a multi university launch.\n\n  \nAt least most other pages of the site should have a basic level of responsiveness.\n\nThanks for your reply ðŸ™",
              "score": 2,
              "created_utc": "2026-02-19 08:18:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67bh9c",
          "author": "V01DDev",
          "text": "Amazing, saving this for later, good job",
          "score": 2,
          "created_utc": "2026-02-19 07:05:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67cgjb",
              "author": "digital__navigator",
              "text": "Firstly big thanks means a lot.\n\nWell hey if you wanna support you can sign up for a newsletter I made for the site.\n\n[https://degreeview.beehiiv.com/](https://degreeview.beehiiv.com/)\n\nI'll simply be sending out updates on how progress is going, like adding new universities, working on the backend to add more statistics, or making major stylistic changes.\n\nI could be wrong and over ambitious here but I think I should be able to web scrape at least 50 universities which would be like 300,000+ courses on the site, and a potential user base (but by user theyll probably just visit the site once and never come back) of around 500,000 assuming an average university size of 10,000 students. ",
              "score": 1,
              "created_utc": "2026-02-19 07:13:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4phlg",
      "title": "Payment processors for a scraping SaaS (high-risk niche)",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r4phlg/payment_processors_for_a_scraping_saas_highrisk/",
      "author": "Accomplished_Mood766",
      "created_utc": "2026-02-14 16:50:20",
      "score": 5,
      "num_comments": 26,
      "upvote_ratio": 0.74,
      "text": "Hi everyone,  \nIâ€™m running a SaaS that provides scraping services, and Iâ€™m currently struggling with payment processing.\n\nStripe, Paddle, and Lemonsqueezy have all declined us due to the nature of the business. I understand that this niche is often classified as high-risk, but in practice weâ€™ve been operating for 5 months with **zero chargebacks or disputes**. Unfortunately, that doesnâ€™t seem to matter much to decision-makers at most payment platforms â€” scraping services are automatically flagged as high risk.\n\nIâ€™d like to ask those of you who are running SaaS products in similar areas (scraping, data extraction, automation, etc.):\n\n* Which payment processors or merchant accounts are you using to accept credit card payments?\n* Are there providers that are more tolerant or experienced with this type of business?\n* Any recommendations or experiences youâ€™re willing to share would be greatly appreciated.\n\nThanks in advance â€” Iâ€™d really value hearing from others whoâ€™ve dealt with this problem.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r4phlg/payment_processors_for_a_scraping_saas_highrisk/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5d5tuf",
          "author": "JohnnyOmmm",
          "text": "Lmao crypto or porno processors",
          "score": 6,
          "created_utc": "2026-02-14 16:54:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d6da7",
              "author": "Accomplished_Mood766",
              "text": "They accept cryptocurrency, but our customers donâ€™t use crypto. Our users are mostly recruiters, marketers, and regular consumers who are accustomed to traditional credit and debit card payments.",
              "score": 3,
              "created_utc": "2026-02-14 16:57:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5dke6t",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 6,
                  "created_utc": "2026-02-14 18:07:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5dcjg8",
                  "author": "RobSm",
                  "text": "Send invoices and let them pay monthly wiretransfer, etc. You can automate email sending and if not paid - email notifications, etc. Also, paypal? Which has option to use CC directly?",
                  "score": 2,
                  "created_utc": "2026-02-14 17:28:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5e4rl6",
          "author": "convicted_redditor",
          "text": "Try polar.sh",
          "score": 2,
          "created_utc": "2026-02-14 19:51:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5eoyik",
          "author": "Dry_Illustrator977",
          "text": "Why not try crypto?",
          "score": 2,
          "created_utc": "2026-02-14 21:41:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pohf6",
          "author": "amogh-datar",
          "text": "I would suggest integrate multiple payment gateways. That way, if one is terminated suddenly you can make others live and then add another one in the meantime.   \n  \nIdeally you should integrate the high-risk payment gateways. Search for \"high risk payment gateways in \\[country\\]\" on Google and you should find them. They do have high fees but are likely to work for long term for high risk niche like yours. You can contact them to confirm the same.\n\nAlso, if your clients like your service, do offer them a way to pay via crypto or wire transfers to save payment gateway fees.",
          "score": 2,
          "created_utc": "2026-02-16 17:06:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5u9r3j",
          "author": "netmillions",
          "text": "Just reverse-engineer what your competitors are using. Also, a rebrand that doesn't plaster \"LinkedIn\" all over the website would likely be in good order. That alone might keep you under radar with them (and, LinkedIn). I would take all the payment solutions rejecting you as a wake up call.Â ",
          "score": 2,
          "created_utc": "2026-02-17 09:52:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5urcq4",
              "author": "Accomplished_Mood766",
              "text": "Including LinkedIn as a keyword on our website helps with search visibility and organic traffic.",
              "score": 1,
              "created_utc": "2026-02-17 12:21:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5gg8cf",
          "author": "Quantum_Rage",
          "text": "I don't what kind of scraping SaaS you're building, but I have seen SaaS apps built on web scraping accept payments via Stripe or Paddle.",
          "score": 1,
          "created_utc": "2026-02-15 04:27:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5imxka",
              "author": "Accomplished_Mood766",
              "text": "I run a SaaS focused on LinkedIn data scraping. Stripe declined us immediately. We worked with Paddle for five months with zero chargebacks, but their risk management team eventually decided to terminate our payment account.",
              "score": 2,
              "created_utc": "2026-02-15 15:09:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kcp1x",
          "author": "awebscrapingguy",
          "text": "I think you are not saying everything, all scraping saas run on Stripe or Paddle without issues",
          "score": 1,
          "created_utc": "2026-02-15 20:13:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kpe1p",
              "author": "Accomplished_Mood766",
              "text": "Stripe explicitly prohibits scraping-related services. Paddleâ€™s policies were more flexible, and we were able to work with them for five months; however, their risk team eventually decided to stop supporting our account.",
              "score": 2,
              "created_utc": "2026-02-15 21:18:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kjygm",
          "author": "Round_Method_5140",
          "text": "Paypal?",
          "score": 1,
          "created_utc": "2026-02-15 20:50:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3lfdv",
      "title": "How do you handle scraping directory sites that cap results at ~200?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r3lfdv/how_do_you_handle_scraping_directory_sites_that/",
      "author": "deadlynightshade_x",
      "created_utc": "2026-02-13 10:08:34",
      "score": 4,
      "num_comments": 17,
      "upvote_ratio": 0.65,
      "text": "Hey \n\nI've been trying to pull data from a large online directory/phone book site (Swiss one, but I think the issue is pretty common across similar services like yellow pages, local directories, etc.).The site claims tens of thousands of matching entries (e.g., \\~50k private records for a region), but in practice:\n\n* URL params like &pages=500, &maxnum=500, or whatever don't actually fetch more and it hard-caps visible/returned results around 200.\n\nHas anyone here successfully scraped large volumes from these kinds of directory/phone book sites recently ?\n\n* Do most of them still enforce strict \\~100â€“200 result caps per query/page?\n* What tricks actually work to get around it without getting banned quickly?\n\nJust curious if it's still feasible for bigger datasets or if these sites have mostly locked it down. \n\n\n\nTips, tools, or experiences appreciated !Thanks!",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r3lfdv/how_do_you_handle_scraping_directory_sites_that/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o55w5y8",
          "author": "datamizer",
          "text": "They all set max results per query to normal defaults like 100. Even if you try to request more, the API basically has logic to protect it from serving a very large result set. \n\nAll you do is set a page number or offset, figure out the max result size, then send paginated requests at some interval. That's very standard.\n\nDepends on the site, some just have basic rate limits like no more than 10 requests over 10 seconds from the same IP. Most don't have robust prevention. Most you just pass an origin and a referer and that's good enough.",
          "score": 9,
          "created_utc": "2026-02-13 13:49:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o565cf3",
              "author": "leros",
              "text": "Lots of sites cap the results and don't let you paginate through them all. You need be more creative to scrape everything.Â ",
              "score": 2,
              "created_utc": "2026-02-13 14:38:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56d6i8",
                  "author": "datamizer",
                  "text": "It depends on the specifics of how they implemented it. Some sites there is no mechanism other than having a full sitemap if they offer one. Checking robots.txt to see what they are trying to obscure, using search engines that have their pages indexed etc. It's really just site by site basis.",
                  "score": 1,
                  "created_utc": "2026-02-13 15:17:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5bqpts",
              "author": "deadlynightshade_x",
              "text": "Thank you",
              "score": 1,
              "created_utc": "2026-02-14 11:44:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o595at0",
          "author": "forklingo",
          "text": "a lot of those caps arenâ€™t just ui limits, theyâ€™re intentional backend limits to stop bulk extraction. if the api itself only returns \\~200 per query, tweaking page params usually wonâ€™t help.\n\nwhat tends to work better is slicing the query space instead of trying to increase the page size. for example, split by smaller geographic areas, postal codes, name prefixes, or categories so each query stays under the cap but collectively covers more ground.\n\nalso be careful with â€œgetting aroundâ€ protections. many directory sites have pretty strict terms and rate limits, and they do monitor unusual access patterns. rotating ips or hammering endpoints might work short term but usually gets you blocked fast.\n\nfor larger datasets, sometimes itâ€™s more realistic to look for official data sources, public datasets, or paid access if itâ€™s business critical. scraping big directories at scale has gotten a lot harder over the years.",
          "score": 3,
          "created_utc": "2026-02-13 23:34:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bquvy",
              "author": "deadlynightshade_x",
              "text": "I appreciate your input",
              "score": 1,
              "created_utc": "2026-02-14 11:45:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o57vvg6",
          "author": "Free-Path-5550",
          "text": "Break your one big query into many smaller ones that each return under the cap., then deduplicate after merging since slices will overlap. Well this is how i solved against the site i was trying.",
          "score": 1,
          "created_utc": "2026-02-13 19:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5br7pq",
              "author": "deadlynightshade_x",
              "text": "That what i did and it worked ! Thanks",
              "score": 1,
              "created_utc": "2026-02-14 11:48:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5a85n5",
          "author": "pauldm7",
          "text": "You will likely need to make a lot more queries. Ie instead of searching for widgets, you will search for widgets a, widgets b, widgets aa widgets ab and so on. Cover every combo of letters until the results stop.\n\nIf you can click on the individual results, how is the URL? Numerical IDs?",
          "score": 1,
          "created_utc": "2026-02-14 03:39:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bra4d",
              "author": "deadlynightshade_x",
              "text": "That's exactly what I did and it gave me good results! Thanks",
              "score": 1,
              "created_utc": "2026-02-14 11:49:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bfme2",
          "author": "uncivilized_human",
          "text": "one thing i've found helps - check if there's an internal api the frontend uses. the caps are often enforced at the ui layer but the underlying api might have different limits. open network tab, search, and see what endpoints get hit. mobile apps are worth checking too since their apis tend to be less locked down.",
          "score": 1,
          "created_utc": "2026-02-14 09:57:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5brd9y",
              "author": "deadlynightshade_x",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-02-14 11:49:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5gvbay",
                  "author": "shipping_sideways",
                  "text": "np! good luck with it - the internal apis on sites like that sometimes have cursor-based pagination that lets you get around the arbitrary caps. if you find the actual endpoint the frontend uses, it's often way less restrictive than what they expose to users",
                  "score": 1,
                  "created_utc": "2026-02-15 06:32:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o56bp02",
          "author": "Hour_Analyst_7765",
          "text": "Yes most sites cap request sites.. whether its too big for a client (probably historic reasons), or anti-scraping measure.\n\nHowever, 100 to 200 per call is imo quite good still. If the dataset is 50k, thats 500 pages, and even if you wait a minute per call you'll have that dataset in just over 8 hours. If you refresh daily thats quick enough.\n\nFor me <10 hours is plenty quick. But I often don't have very strict latency requirements. Most of my jobs need to keep up on a \"daily basis\"\n\nThe difficulty is often:\n\n1. Walking pages linearly may get you blocked on WAF nowadays.\n2. Sites that stop serving data after N amount of records, because they don't expect anyone to realistically scan through literal hunrdeds or thousands of records.\n\nIn this case, you'll have to deal with navigation through categories, parametric filters, etc. to reduce the dataset that can be crawled completely.\n\n3. Sites that don't have a way of putting new/modified records in front. I fetch quite a lot of listings and always will look for a \"newest first\" kind of sorting. Fortunately for many sites this is their default. I stop crawling as soon as I see data that I've already crawled.",
          "score": 1,
          "created_utc": "2026-02-13 15:10:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58kdvb",
              "author": "ObjectIndependent827",
              "text": "Yeah this matches what most people run into now itâ€™s less about brute forcing pages and more about breaking the dataset into smaller slices through filters or categories\n\nNewest first sorting is huge too since it lets you maintain coverage without re crawling everything and helps avoid triggering WAF limits over time",
              "score": 2,
              "created_utc": "2026-02-13 21:42:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5br5j3",
              "author": "deadlynightshade_x",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-02-14 11:47:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4mkpv",
      "title": "Data extraction quality / LLM cost",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r4mkpv/data_extraction_quality_llm_cost/",
      "author": "Playgroundmob",
      "created_utc": "2026-02-14 14:53:02",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 0.72,
      "text": "I'm trying to get an idea of how much people/companies would pay per result when scraping websites that require structured data with a reliable JSON schema and high data quality - for example, e-commerce or job listings. especially when dealing with unknown sources.\n\nLLMs like Flash 2.5/3.0, which are actually reliable for consistent results, are not cheap - sometimes we might even reach $0.01â€“$0.03 per extraction.\n\nI'm trying to understand, in real world terms, how much people would pay for solutions that just work.\n\nIf anyone can share real world use cases, that would be awesome.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r4mkpv/data_extraction_quality_llm_cost/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5czh07",
          "author": "RandomPantsAppear",
          "text": "I think the biggest issue you will run into is people doing it cheaper, not using an LLM. You are kind of limiting yourself to one offs, because for any recurring task people will find something more traditional and exponentially cheaper.",
          "score": 1,
          "created_utc": "2026-02-14 16:22:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d1jzs",
              "author": "Playgroundmob",
              "text": "You're right. I'm wondering if there's a use case for massive scrape operation, where we just can't handle building specific scrapers per domain, as we don't know what volume of different websites to investigate and scrape.",
              "score": 1,
              "created_utc": "2026-02-14 16:33:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5d246w",
                  "author": "RandomPantsAppear",
                  "text": "If I were you, Iâ€™d focus on integrating. Thereâ€™s lots of options but I would first build something in that let it work with Google docs, pdf, and images as well as HTML. I would also add several export types. \n\nAfter that I would make a chrome extension that allowed people to either subscribe or top up their balances, and extract data on demand. \n\nI would name the app something that business people would likely search for, not something a coder would search for.",
                  "score": 2,
                  "created_utc": "2026-02-14 16:35:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r3htxr",
      "title": "[Selenium/C#] \"Cannot start the driver service\" in Windows Service",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r3htxr/seleniumc_cannot_start_the_driver_service_in/",
      "author": "AromaticLocksmith662",
      "created_utc": "2026-02-13 06:26:28",
      "score": 3,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nIâ€™ve been banging my head against a wall for a week with a Selenium ChromeDriver issue and could use some fresh eyes.\n\nThe Context:\n\nI have a web scraping tool running as a background Windows Service. It processes license data for different states.\n\nScale: We have about 20 separate Windows Services running in parallel on the same server, each scraping different data sources.\n\nTech Stack: C# .NET, Selenium WebDriver, Chrome (Headless).\n\nVersion: Chrome & Driver are both version 144.0.x.x (Versions are matched).\n\nThe Issue:\n\nEverything was running smoothly until recently. Now, I am getting a WebDriverException claiming it cannot start the driver service on a specific localhost port.\n\nthe exception:\n\nCannot start the driver service on http://localhost:54853/\n\nThe Stack Trace:\n\nat OpenQA.Selenium.DriverService.Start()\n\nat OpenQA.Selenium.Remote.DriverServiceCommandExecutor.Execute(Command commandToExecute)\n\nat OpenQA.Selenium.WebDriver.Execute(String driverCommandToExecute, Dictionary\\`2 parameters)\n\nat OpenQA.Selenium.WebDriver.StartSession(ICapabilities desiredCapabilities)\n\nat OpenQA.Selenium.Chromium.ChromiumDriver..ctor(ChromiumDriverService service, ChromiumOptions options, TimeSpan commandTimeout)\n\nat MyNamespace.LicenseProject.Business.Vermont.VermontLicenseService.ProcessLicense() in ...\\\\VermontLicenseService.cs:line 228\n\ncode:\n\nvar options = new ChromeOptions();\n\noptions.AddArgument(\"--headless\");\n\noptions.AddArgument(\"--no-sandbox\");\n\noptions.AddArgument(\"--disable-dev-shm-usage\");\n\n// I am explicitly setting the driver directory\n\nvar service = ChromeDriverService.CreateDefaultService(driverPath);\n\nservice.HideCommandPromptWindow = true;\n\n// Error implies it fails right here:\n\nusing (var driver = new ChromeDriver(service, options, TimeSpan.FromMinutes(2)))\n\n{\n\n// scraping logic\n\n}\n\nWhat I've Tried/Verified:\n\nVersion Mismatch: Double-checked that the chromedriver.exe version matches the installed Chrome browser version (144.0.x.x).\n\nManual Run: The scraper works fine when I run it as a console app/user mode. It only fails when running as a Windows Service.\n\nCleanup: I suspected \"zombie\" chrome processes were eating up ports, so I added logic to kill orphaned chrome processes, but the issue persists.\n\nHas anyone managed high-volume Selenium instances in a Windows Service environment and seen this port binding error?\n\nAny pointers would be appreciated!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r3htxr/seleniumc_cannot_start_the_driver_service_in/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5b4f4r",
          "author": "jinef_john",
          "text": "I would explicitly enable verbose logging and log that to a file. But here is what I think is happening, with 20 parallel services, they're likely clashing on the default user data directory. You should try and set unique user data directories per service instance.\n\nAlso I suspect the driver might be failing to bind, you could let chromedriver pick its own port, by setting service.Port=0.\n\nIf neither of these help, I'd bet it's a session 0 isolation problem. I believe even in headless, chrome sometimes needs to create a window station or access GDI resources that are restricted in Session 0. So you could try running the service under a specific user account with 'allow service to interact with service'\n\nOr it could even be the headless flags too. \nTry the new headless mode\n```\n--headless=new\n```\nYou could also add other options, disable gpu, extensions,software rasterizer,force color profile to be srgb\n\nBut I would mostly look into the user data directory or session 0 isolation.",
          "score": 1,
          "created_utc": "2026-02-14 08:07:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dpbq2",
          "author": "CuriousCat7871",
          "text": "I know it is windows, but can you run the browser in a docker container instead?",
          "score": 1,
          "created_utc": "2026-02-14 18:32:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n5vqw",
          "author": "THenrich",
          "text": "You should consider switching to using Playwright instead of Selenium. Selenium is quite old and there are advantages to using Playwright.",
          "score": 1,
          "created_utc": "2026-02-16 06:48:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o568ljp",
          "author": "Stunning_Cry_6673",
          "text": "Lol. Selenium was a technology popular 15 years ago . Cant believe someone would use it today from scraping ðŸ˜‚ðŸ˜‚ðŸ˜‚",
          "score": -3,
          "created_utc": "2026-02-13 14:54:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b4tkt",
              "author": "jinef_john",
              "text": "Your favorite tool today, probably uses selenium under the hood ðŸ˜‰",
              "score": 0,
              "created_utc": "2026-02-14 08:11:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5b52xt",
                  "author": "Stunning_Cry_6673",
                  "text": "Lol. In these days when is so simple to get information you come with this garbage affirmation ðŸ¤£ðŸ¤£ðŸ¤£",
                  "score": 0,
                  "created_utc": "2026-02-14 08:14:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r5rrds",
      "title": "Letâ€™s move by step",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r5rrds/lets_move_by_step/",
      "author": "0xMassii",
      "created_utc": "2026-02-15 22:14:31",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 0.71,
      "text": "https://www.reddit.com/r/webscraping/s/0aCA0m6ioo\n\nJust use this post to comment with a website, so send in this format\n\nWebsite and what you want to achieve \n\nAnd then we will make a list so we can proceed by step and start with website with more upvotes \n\nWhat do you think?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r5rrds/lets_move_by_step/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5l6t90",
          "author": "No-Exchange2961",
          "text": "Linkedin!!!",
          "score": 2,
          "created_utc": "2026-02-15 22:50:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l7dh3",
          "author": "Acceptable-Sea-3248",
          "text": "LinkedIn",
          "score": 2,
          "created_utc": "2026-02-15 22:53:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lw6n6",
          "author": "Mysterious_Tip_6793",
          "text": "LinkedIn and Reddit",
          "score": 1,
          "created_utc": "2026-02-16 01:20:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m5c12",
          "author": "lechiffreqc",
          "text": "CRA: Want to maintain auth for allowing automating tax filling \nRevenu Quebec: Same as CRA\nTD: Same, but also automating downloading transactions \nAll other canadian banks for same reason",
          "score": 1,
          "created_utc": "2026-02-16 02:19:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5ca6r",
      "title": "Pull URL with WebScraper",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r5ca6r/pull_url_with_webscraper/",
      "author": "HyperfocusedSoul",
      "created_utc": "2026-02-15 11:22:49",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 0.81,
      "text": "Hi all,\n\n  \nI will openly admit that I am not code-savvy... that said, I need some help.\n\nI am trying to use WebScraper to pull data from multiple pages linked to a starting page. Basically, it is designed to start at a particular website's history (the starting page), and click through/scrape the listed data from every \"new\" page linked in the history. It is doing that fine, but I also want to grab the URL associated with each new entry/page. While it is grabbing the Starting URL, it is not grabbing the individual URLs for each page it is going through. For the life of me, I can not figure out the correct Type and Selector. Does anyone have any tips or advice?\n\n  \nP.s. I would want to add this to an existing Parent Selector that already has all my other selectors/data points as subs...",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r5ca6r/pull_url_with_webscraper/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5hwz3v",
          "author": "Equivalent-Brain-234",
          "text": "Which web scraper are you using? Is it custom built?",
          "score": 1,
          "created_utc": "2026-02-15 12:26:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i48du",
          "author": "akashpanda29",
          "text": "What is the website you are trying to scrape and what's your setup ? \nI can help you",
          "score": 1,
          "created_utc": "2026-02-15 13:20:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}