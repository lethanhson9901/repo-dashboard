{
  "metadata": {
    "last_updated": "2026-02-17 02:59:57",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 19,
    "total_comments": 144,
    "file_size_bytes": 122533
  },
  "items": [
    {
      "id": "1r5712p",
      "title": "Scrapling v0.4 is here - Effortless Web Scraping for the Modern Web",
      "subreddit": "webscraping",
      "url": "https://i.redd.it/chi2m8gjjljg1.png",
      "author": "0xReaper",
      "created_utc": "2026-02-15 06:01:04",
      "score": 230,
      "num_comments": 30,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r5712p/scrapling_v04_is_here_effortless_web_scraping_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5h0uzf",
          "author": "Reddit_User_Original",
          "text": "Nice job. Been familiar with your project since v0.3. It's the best of its kind as far as i can tell. I use scrapling when using curl cffi is insufficient, and i need something more powerful. How do you stay on top of the anti bot tech? Have you had to implement changes in response to any new anti bot tech recently? Thanks so much for building this tool.",
          "score": 15,
          "created_utc": "2026-02-15 07:24:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5i40el",
              "author": "0xReaper",
              "text": "Thanks, mate. That means a lot to me. \n\nThe thing is, I have been working in the Web Scraping field for years, and since I made the library, I use it every day. So it's always under heavy testing from me; most of the time, I find issues before users report them because of that.\n\nRegarding security, before switching to Web Scraping, I spent about 8 years in the information security field, including bug hunting. So I was an ethical hacker before all of that. And I spent some time working as backend.",
              "score": 11,
              "created_utc": "2026-02-15 13:19:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hjjyo",
          "author": "Satobarri",
          "text": "Why canâ€™t I decline your cookies on your page?",
          "score": 3,
          "created_utc": "2026-02-15 10:26:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5i30l4",
              "author": "0xReaper",
              "text": "I have fixed it, thanks for pointing that out",
              "score": 11,
              "created_utc": "2026-02-15 13:12:19",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5hwu1b",
              "author": "0xReaper",
              "text": "Oh, I didnâ€™t notice that. Let me have a look at it, I have just switched to zensical with this update so I might have missed something in the configuration.",
              "score": 3,
              "created_utc": "2026-02-15 12:25:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5i31e4",
                  "author": "Satobarri",
                  "text": "Thanks. Not a biggie but makes it suspicious for European visitors.",
                  "score": 4,
                  "created_utc": "2026-02-15 13:12:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5k4fsw",
          "author": "NoN4meBoy",
          "text": "Does it handle datadome ?",
          "score": 3,
          "created_utc": "2026-02-15 19:31:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kfwlh",
          "author": "24props",
          "text": "Iâ€™m currently on my phone and will review this later. I believe that for many people today, due to the widespread use of AI coding, it will be beneficial to create a skill (agentskills.io) to assist users who utilize AI for development or integration. Only because LLMs are never trained on immediate new versions of anything and have knowledge gaps/cutoffs.",
          "score": 3,
          "created_utc": "2026-02-15 20:29:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kk0cb",
              "author": "0xReaper",
              "text": "Yes, I agree, I will work on this soon. I'm just taking a well-deserved rest before working on the next version. There is a lot more to add.",
              "score": 5,
              "created_utc": "2026-02-15 20:51:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5jn3on",
          "author": "JerryBond106",
          "text": "Should i use some vpn for this as well, so i don't get ip banned? (I'm new to this, i read proxy is included but don't know the big picture in scraping yet, as it changes rapidly and i wasn't ready to start safely yet)",
          "score": 2,
          "created_utc": "2026-02-15 18:06:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kepjy",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-15 20:23:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5mfpfp",
                  "author": "webscraping-ModTeam",
                  "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
                  "score": 1,
                  "created_utc": "2026-02-16 03:27:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kyzi6",
          "author": "515051505150",
          "text": "One thing Iâ€™ve struggled with is determining the maximum number of requests per minute I can send to a site before getting rate limited or blocked. Is there a feature within scrapling that can help automatically determine the max threshold of scrapes before a siteâ€™s counter-measures kick in?",
          "score": 2,
          "created_utc": "2026-02-15 22:07:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nz6jy",
          "author": "Flat_Agent_9174",
          "text": "Wow, it's an amazing tool !",
          "score": 2,
          "created_utc": "2026-02-16 11:20:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o0e3k",
          "author": "Flat_Agent_9174",
          "text": "Can it bypass Datadome ?",
          "score": 2,
          "created_utc": "2026-02-16 11:31:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i11sy",
          "author": "Overall-Suit-5531",
          "text": "Interesting! Does it manage JavaScript too?",
          "score": 2,
          "created_utc": "2026-02-15 12:58:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5i31c0",
              "author": "0xReaper",
              "text": "yup",
              "score": 2,
              "created_utc": "2026-02-15 13:12:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hchnq",
          "author": "One-Spend379",
          "text": "Great job ðŸ‘ \nCan it scrap allegro. pl ?",
          "score": 1,
          "created_utc": "2026-02-15 09:17:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jynxn",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-15 19:02:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mfrf7",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-16 03:27:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kdbhy",
          "author": "strasbourg69",
          "text": "Could i use this to scan for emails and phone numbers of for example plumbers, regionally targetted",
          "score": 1,
          "created_utc": "2026-02-15 20:16:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5krjs1",
          "author": "saadcarnot",
          "text": "Can it avoid anti bot stuff like google enterprise v3 captcha?",
          "score": 1,
          "created_utc": "2026-02-15 21:29:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nfd1o",
          "author": "ChallengeEmergency11",
          "text": "How free?",
          "score": 1,
          "created_utc": "2026-02-16 08:15:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o2dcz",
          "author": "imbuilding",
          "text": "Will be trying it out! Thanks",
          "score": 1,
          "created_utc": "2026-02-16 11:47:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5oqdnx",
          "author": "mayodoctur",
          "text": "Does this work for scraping news articles like Al Jazeera, Substack, blogs etc ?",
          "score": 1,
          "created_utc": "2026-02-16 14:21:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ot7op",
          "author": "RageQuitNub",
          "text": "very interesting, does it manage a list of proxy or we have to supply the proxy list?",
          "score": 1,
          "created_utc": "2026-02-16 14:36:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5p82uk",
          "author": "SnooFloofs641",
          "text": "How good is this with anti bot checks and stuff?",
          "score": 1,
          "created_utc": "2026-02-16 15:50:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pjf4g",
          "author": "mikeb550",
          "text": "how do you deal with companies who forbid scraping their sites?  any of you customers get taken to court?",
          "score": 0,
          "created_utc": "2026-02-16 16:43:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5ptfe",
      "title": "I can scrape anything",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r5ptfe/i_can_scrape_anything/",
      "author": "0xMassii",
      "created_utc": "2026-02-15 20:57:12",
      "score": 68,
      "num_comments": 103,
      "upvote_ratio": 0.71,
      "text": "No selenium, playwright or puppeteer shit, I can scrape anything in full request mode, bypassing every bot protection. It doesn't matter if is Cloudflare, Akamai, PerimeterX etc  \nAfter years in this filed I believe it's time to give something back to the community. I'll start to release open source stuff for the people who want to learn. Let me know which one is the most interesting topic.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r5ptfe/i_can_scrape_anything/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5kqgqc",
          "author": "Raidrew",
          "text": "My body is ready",
          "score": 36,
          "created_utc": "2026-02-15 21:24:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kufee",
              "author": "heelstoo",
              "text": "Can you scrape me, Focker?",
              "score": 22,
              "created_utc": "2026-02-15 21:44:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5l5g7s",
                  "author": "emprezario",
                  "text": "You can scrape anything with nipples. ðŸ¤·â€â™‚ï¸",
                  "score": 12,
                  "created_utc": "2026-02-15 22:42:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kx6se",
          "author": "mizhgun",
          "text": "I can type 1000 characters per minute. It's just total gibberish, though.",
          "score": 17,
          "created_utc": "2026-02-15 21:58:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kytyf",
              "author": "0xMassii",
              "text": "No jokes here, do a check on me",
              "score": -8,
              "created_utc": "2026-02-15 22:07:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l2skh",
          "author": "indicava",
          "text": "Why the hell is this crap being upvoted?",
          "score": 32,
          "created_utc": "2026-02-15 22:28:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l3vi5",
              "author": "0xMassii",
              "text": "The real question is why not?",
              "score": -27,
              "created_utc": "2026-02-15 22:34:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l3e21",
          "author": "beachandbyte",
          "text": "Press X to doubt. \n\nAkami and Cloudflare inspect the TLS stack. It would be possible to spoof that but challenging. But then they pass back js that profiles the browser apis to suspicious requests and the payload in that js is constantly changing. So at least for those requests you would need to either mock the entire surface of the browser Api and return what they would consider reasonable values (at that point just using a browser is likely far easier). Wonâ€™t say itâ€™s 100% not possible but unless your trick is proxying through some other process that is actually getting around all the protection you wonâ€™t be bypassing  cloudflare or Akami purely with requests.",
          "score": 8,
          "created_utc": "2026-02-15 22:31:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l45m1",
              "author": "0xMassii",
              "text": "Bro, bro, bro bot protection are my bitches\nIâ€™m the man who destroy the whole â€¦â€¦â€¦â€¦ website :)",
              "score": -16,
              "created_utc": "2026-02-15 22:35:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l9umx",
                  "author": "You_Cant_Win_This",
                  "text": "ok no we know you are fraud for sure",
                  "score": 5,
                  "created_utc": "2026-02-15 23:07:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kmtz5",
          "author": "EffectiveSeat1505",
          "text": "CF",
          "score": 3,
          "created_utc": "2026-02-15 21:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kymuh",
              "author": "0xMassii",
              "text": "I have a custom solver for CF, soon will be OS, Iâ€™ll start from tomorrow",
              "score": 3,
              "created_utc": "2026-02-15 22:06:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5lqmym",
                  "author": "MurkBRA",
                  "text": "Everyone who tried to make this open source received a DMCA takedown notice.",
                  "score": 3,
                  "created_utc": "2026-02-16 00:46:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5kr2r4",
              "author": "UnlikelyLikably",
              "text": "Second that",
              "score": 1,
              "created_utc": "2026-02-15 21:27:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l632w",
          "author": "who_am_i_to_say_so",
          "text": "Your mom scraped deezenuts with her snaggletooth. Scraping must run in the family.",
          "score": 11,
          "created_utc": "2026-02-15 22:46:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l6f8f",
              "author": "0xMassii",
              "text": "No sense but good for u ðŸ¤£",
              "score": -3,
              "created_utc": "2026-02-15 22:47:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l6jx3",
                  "author": "who_am_i_to_say_so",
                  "text": "I couldnâ€™t resist. Haha",
                  "score": 1,
                  "created_utc": "2026-02-15 22:48:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kllaj",
          "author": "yyavuz",
          "text": "let's gooooooo",
          "score": 8,
          "created_utc": "2026-02-15 20:59:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kp5vy",
          "author": "PTBKoo",
          "text": "Cloudflare turnstile, I have to open a chrome browser for a every single turnstile solve and scraping over 1k daily and my poor server canâ€™t keep up.",
          "score": 3,
          "created_utc": "2026-02-15 21:17:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lnyvm",
          "author": "BossDailyGaming",
          "text": "Talk is cheap, make that gh repo",
          "score": 3,
          "created_utc": "2026-02-16 00:30:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kntfi",
          "author": "askolein",
          "text": "Interested. Def the non selenium request thing. How come? Direct webassembly?",
          "score": 2,
          "created_utc": "2026-02-15 21:10:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kpbqg",
          "author": "No-Exchange2961",
          "text": "Please post the link to the open source!",
          "score": 2,
          "created_utc": "2026-02-15 21:18:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ksxhi",
          "author": "sudbull",
          "text": "LinkedIn",
          "score": 2,
          "created_utc": "2026-02-15 21:36:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kyolu",
              "author": "0xMassii",
              "text": "Ez brother",
              "score": 2,
              "created_utc": "2026-02-15 22:06:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5lb6q1",
          "author": "EntrepreneurSea4283",
          "text": "What's the the hardest thing to scrape",
          "score": 2,
          "created_utc": "2026-02-15 23:14:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lbgki",
              "author": "0xMassii",
              "text": "It depends, for me few POST requests on Akamai gave me hard times",
              "score": 1,
              "created_utc": "2026-02-15 23:16:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5lufaj",
          "author": "crawford5002",
          "text": "Teach me your ways",
          "score": 2,
          "created_utc": "2026-02-16 01:09:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kpi7j",
          "author": "san-vicente",
          "text": "Let me know",
          "score": 1,
          "created_utc": "2026-02-15 21:19:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kq3i9",
          "author": "Any-Dig-3384",
          "text": "DM if legit",
          "score": 1,
          "created_utc": "2026-02-15 21:22:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kri7l",
          "author": "Ladytron2",
          "text": "Facebook events?",
          "score": 1,
          "created_utc": "2026-02-15 21:29:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ks7mg",
          "author": "Srijaa",
          "text": "Iâ€™m in!",
          "score": 1,
          "created_utc": "2026-02-15 21:33:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ku6zv",
          "author": "uneatenbreakfast",
          "text": "The chosen one",
          "score": 1,
          "created_utc": "2026-02-15 21:43:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kup2g",
          "author": "puzz-User",
          "text": "Letâ€™s go!",
          "score": 1,
          "created_utc": "2026-02-15 21:45:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kvsse",
          "author": "living_david_aloca",
          "text": "Well letâ€™s see it",
          "score": 1,
          "created_utc": "2026-02-15 21:51:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kw0gx",
          "author": "Sensitive_Nobody409",
          "text": "Recaptcha v3 pls â¤ï¸ðŸ‘ðŸ¿",
          "score": 1,
          "created_utc": "2026-02-15 21:52:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kyx8n",
              "author": "0xMassii",
              "text": "Ez, but if you want to start from somewhere check solver online",
              "score": 1,
              "created_utc": "2026-02-15 22:07:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kw4m0",
          "author": "Sensitive_Nobody409",
          "text": "Build your own chromium?",
          "score": 1,
          "created_utc": "2026-02-15 21:53:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kx1va",
          "author": "slumdogbi",
          "text": "Amazon with sponsored products",
          "score": 1,
          "created_utc": "2026-02-15 21:57:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kyy8v",
              "author": "0xMassii",
              "text": "Ez",
              "score": 1,
              "created_utc": "2026-02-15 22:07:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ld0o2",
                  "author": "slumdogbi",
                  "text": "ðŸ‘ðŸ‘ðŸ‘ðŸ‘",
                  "score": 1,
                  "created_utc": "2026-02-15 23:25:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5lh0ov",
                  "author": "RealAmerik",
                  "text": "I'm interested in this as well.",
                  "score": 1,
                  "created_utc": "2026-02-15 23:49:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kydj8",
          "author": "Overall-Suit-5531",
          "text": "Do it",
          "score": 1,
          "created_utc": "2026-02-15 22:04:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l0cai",
          "author": "Dorkits",
          "text": "Where",
          "score": 1,
          "created_utc": "2026-02-15 22:15:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l0f36",
              "author": "0xMassii",
              "text": "Everywhere",
              "score": 0,
              "created_utc": "2026-02-15 22:15:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l0kvi",
                  "author": "Dorkits",
                  "text": "The link bro",
                  "score": 1,
                  "created_utc": "2026-02-15 22:16:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l1vnf",
          "author": "sojufles",
          "text": "How not to get rate limited by Cloudflare, with a scraper in a container using 1 IP?",
          "score": 1,
          "created_utc": "2026-02-15 22:23:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l2hlv",
              "author": "0xMassii",
              "text": "Thatâ€™s is related to the website, you need to find vulns on the target or misconfig, otherwise cf will block you",
              "score": 1,
              "created_utc": "2026-02-15 22:26:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l34c6",
                  "author": "sojufles",
                  "text": "Thanks for the reply, it also seems that whenever my scraper in container is rate limited. I can access the site from my local browser. Do you have any idea or experiences why this happens?",
                  "score": 1,
                  "created_utc": "2026-02-15 22:29:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l2jsb",
          "author": "Antop90",
          "text": "The problem isn't emulating the browser, there are already plenty of super effective libraries for that. The real issue is having a pool of very expensive IPv4 addresses to rotate.",
          "score": 1,
          "created_utc": "2026-02-15 22:26:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l2xi4",
              "author": "0xMassii",
              "text": "Yeah, but you can scrape also with cheap resi, but you will be slower obv",
              "score": 1,
              "created_utc": "2026-02-15 22:28:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l3hye",
                  "author": "Antop90",
                  "text": "Sometimes speed is essential, and the only way to achieve it is with a solid proxy pool. no magical alternatives exist",
                  "score": 1,
                  "created_utc": "2026-02-15 22:32:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l2stb",
          "author": "Eudaimonic_me",
          "text": "Google trends",
          "score": 1,
          "created_utc": "2026-02-15 22:28:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l3qzm",
          "author": "saadcarnot",
          "text": "I am working on automating an time critical workflow, however it's protected with Recaptcha v3 Enterprise. I can't afford to wait for solver to get back. I need to avoid it all together. \n\nAny suggestions for me?",
          "score": 1,
          "created_utc": "2026-02-15 22:33:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l4a1w",
              "author": "0xMassii",
              "text": "Look for vulns on the website target",
              "score": 2,
              "created_utc": "2026-02-15 22:36:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l54i8",
                  "author": "saadcarnot",
                  "text": "Couldn't find any, can you list down what to look for? Site is recreation.gov",
                  "score": 1,
                  "created_utc": "2026-02-15 22:40:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5lawia",
          "author": "Inner_Grape_211",
          "text": "can u explain each of ur bypasses? with general tips pls?",
          "score": 1,
          "created_utc": "2026-02-15 23:13:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lbhn3",
              "author": "0xMassii",
              "text": "Iâ€™ll do",
              "score": 1,
              "created_utc": "2026-02-15 23:16:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ls78m",
          "author": "TheKillerScope",
          "text": "Can you scrape Dune or GMGN? PolyMarket? If so, I'd be VERY interested in connecting with you and discussing a possible partnership.",
          "score": 1,
          "created_utc": "2026-02-16 00:55:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lus8f",
          "author": "sunrise_zc",
          "text": "customized browserï¼Ÿ",
          "score": 1,
          "created_utc": "2026-02-16 01:12:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m1ddn",
          "author": "RaiseRuntimeError",
          "text": "Is this r/webscrapingcirclejerk",
          "score": 1,
          "created_utc": "2026-02-16 01:54:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m7qli",
          "author": "Villain_99",
          "text": "LinkedIn please",
          "score": 1,
          "created_utc": "2026-02-16 02:35:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mas5b",
          "author": "G_S_7_wiz",
          "text": "How do you get all amzon reviews..you need cookies for all reviews",
          "score": 1,
          "created_utc": "2026-02-16 02:54:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mb88o",
          "author": "LifeShmucksSoMuch",
          "text": "can you bypass shape?",
          "score": 1,
          "created_utc": "2026-02-16 02:57:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l60ag",
          "author": "DotEnvironmental4718",
          "text": "Can you scrape my butt? Itâ€™s itching",
          "score": 1,
          "created_utc": "2026-02-15 22:45:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l6atr",
              "author": "0xMassii",
              "text": "Crazy statement",
              "score": 2,
              "created_utc": "2026-02-15 22:47:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5l9cbf",
          "author": "tradegreek",
          "text": "How do you scrape stuff which requires constantly refreshed cookies (say every 5-10 mins) without using a tool like selenium to obtain the new cookies?",
          "score": 1,
          "created_utc": "2026-02-15 23:04:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l9ki9",
              "author": "0xMassii",
              "text": "Maybe i donâ€™t need cookies either to get the data :)",
              "score": 2,
              "created_utc": "2026-02-15 23:05:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l9n2v",
                  "author": "tradegreek",
                  "text": "I donâ€™t get how is that possible?",
                  "score": 0,
                  "created_utc": "2026-02-15 23:05:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5la1z9",
                  "author": "You_Cant_Win_This",
                  "text": "You do though",
                  "score": 0,
                  "created_utc": "2026-02-15 23:08:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5lg4cf",
          "author": "Afriendlywhiteguy",
          "text": "Teach me your ways",
          "score": 1,
          "created_utc": "2026-02-15 23:44:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lgsti",
          "author": "dca12345",
          "text": "Whatâ€™s wrong with PlayWright?",
          "score": 1,
          "created_utc": "2026-02-15 23:48:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l1e5u",
          "author": "hulleyrob",
          "text": "Docker support?",
          "score": 0,
          "created_utc": "2026-02-15 22:20:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5l2be9",
              "author": "0xMassii",
              "text": "We donâ€™t need, but we can",
              "score": 2,
              "created_utc": "2026-02-15 22:25:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5l2s5l",
                  "author": "hulleyrob",
                  "text": "Could be very interesting if it will still beat all the tech in a docker container I donâ€™t think there is anything out there that can do that right now.",
                  "score": 0,
                  "created_utc": "2026-02-15 22:28:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5l790o",
          "author": "Acceptable-Sea-3248",
          "text": "LinkedIn",
          "score": 0,
          "created_utc": "2026-02-15 22:52:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l7aws",
          "author": "Acceptable-Sea-3248",
          "text": "LinkedIn seems to be the most difficult",
          "score": 0,
          "created_utc": "2026-02-15 22:52:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ldt2m",
          "author": "TillOk5563",
          "text": "The ServiceNow variables fields?",
          "score": 0,
          "created_utc": "2026-02-15 23:30:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5liydm",
          "author": "oizysplutus",
          "text": "Enterprise hcap",
          "score": 0,
          "created_utc": "2026-02-16 00:01:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lk4i8",
          "author": "tuttipazzo",
          "text": "Ok.  You got me interested as well.",
          "score": 0,
          "created_utc": "2026-02-16 00:07:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ll7qd",
          "author": "Sensitive-Finger-404",
          "text": "facebook marketplace",
          "score": 0,
          "created_utc": "2026-02-16 00:14:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lmb1u",
          "author": "bgj556",
          "text": "â€¦ yes please",
          "score": 0,
          "created_utc": "2026-02-16 00:20:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l7598",
          "author": "AdhesivenessEven7287",
          "text": "Reddit",
          "score": -1,
          "created_utc": "2026-02-15 22:51:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1tmm1",
      "title": "Web scraping sandbox website - scrapingsandbox.com",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r1tmm1/web_scraping_sandbox_website_scrapingsandboxcom/",
      "author": "vickyrathee",
      "created_utc": "2026-02-11 10:14:57",
      "score": 29,
      "num_comments": 21,
      "upvote_ratio": 0.95,
      "text": "Hey guys,\n\nI have published a scraping sandbox website- [scrapingsandbox.com](https://scrapingsandbox.com/) to learn and practice web scraping using Playwright, Puppeteer etc.\n\nFor now, I just kept 500 products in e-commerce styled list, filter, pagination and details page to practicing product website scraping, later I will be adding more pages for different scenarios.\n\nIt's [open source on Github](https://github.com/Agenty/scrapingsandbox), please let me know if have have any suggestion or feedback.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r1tmm1/web_scraping_sandbox_website_scrapingsandboxcom/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4tovor",
          "author": "sakozzy",
          "text": "This is cool man! One suggestion: add a few â€œreal world painâ€ scenarios people actually run into. Stuff like lazy loading/infinite scroll, rate limits, basic anti-bot behavior, and messy HTML. Thatâ€™s where most beginners get stuck",
          "score": 10,
          "created_utc": "2026-02-11 16:37:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xj4rs",
              "author": "vickyrathee",
              "text": "Sure, I will add infinite scroll today and other example in upcoming week. PR welcome if you want to contribute. [https://github.com/Agenty/scrapingsandbox](https://github.com/Agenty/scrapingsandbox) ",
              "score": 4,
              "created_utc": "2026-02-12 04:57:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o55viwg",
              "author": "Gold_Emphasis1325",
              "text": "You can introduce simulated poor network conditions (latency, bandwidth, drops) on the client side.",
              "score": 1,
              "created_utc": "2026-02-13 13:46:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4s0ckj",
          "author": "lp435",
          "text": "Cool idea. How does it work? Have you implemented challenges?",
          "score": 1,
          "created_utc": "2026-02-11 10:39:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s0xu2",
              "author": "vickyrathee",
              "text": "it's hosed on Cloudflare workers to build scraping agents with any language you want. No copy right claim, legal issue. Learn, make videos, blog post etc.",
              "score": 1,
              "created_utc": "2026-02-11 10:44:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5hpe58",
                  "author": "navid_ta",
                  "text": "One of my major challenges was dealing with Cloudflare verification. Using common scraping libraries and headless browsers, is it possible to practice on this service in order to understand how Cloudflare verification behaves, or not?\nWhen I accessed it myself, no verification challenge was triggered.",
                  "score": 1,
                  "created_utc": "2026-02-15 11:21:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tveec",
          "author": "AdministrativeHost15",
          "text": "Consider giving a prize to the user who can scrape the highly protected secret word.",
          "score": 1,
          "created_utc": "2026-02-11 17:07:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xizyf",
              "author": "vickyrathee",
              "text": "Who will pay for the prize? :-)",
              "score": 2,
              "created_utc": "2026-02-12 04:56:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4xugxs",
                  "author": "AdministrativeHost15",
                  "text": "Prize can just be bragging rights that will help you get freelance gigs scraping highly protected sites.",
                  "score": 1,
                  "created_utc": "2026-02-12 06:31:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4u80ee",
          "author": "Gold_Emphasis1325",
          "text": "Legal / TOS issues with provider? Otherwise, it would be nice to have a practice area for Captcha, and the \"click this box\" to prove you're human. Either way, it's all just an arms race between the scrapers and the scrape-ees....",
          "score": 1,
          "created_utc": "2026-02-11 18:06:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xjdx6",
              "author": "vickyrathee",
              "text": "Adding captcha etc on sandbox will be a TOS issue and might risk our account, so will add general examples for learning instead like infinit scroll, form submission, pagination, rate limit etc.",
              "score": 1,
              "created_utc": "2026-02-12 04:59:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o55vdcd",
                  "author": "Gold_Emphasis1325",
                  "text": "Yeah and the TOS are changing constantly. Allowed / gray area today and violation and possibly legal action later...",
                  "score": 1,
                  "created_utc": "2026-02-13 13:45:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4x17cu",
          "author": "Objective-Fun-4533",
          "text": "Cool. Like the UI",
          "score": 1,
          "created_utc": "2026-02-12 02:55:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xjfig",
              "author": "vickyrathee",
              "text": "thanks to Claud Opus 4.6",
              "score": 1,
              "created_utc": "2026-02-12 04:59:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4y8su5",
          "author": "Nel549",
          "text": "Add cloudfare to it or on specific pages",
          "score": 1,
          "created_utc": "2026-02-12 08:46:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yim0a",
          "author": "maxim-kulgin",
          "text": "add products variations? for example like size/color? Thanks man! ",
          "score": 1,
          "created_utc": "2026-02-12 10:23:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5o05f4",
              "author": "vickyrathee",
              "text": "will do",
              "score": 1,
              "created_utc": "2026-02-16 11:29:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4kdoy",
      "title": "scraping vinted - TLS fingerprinting + session rotation",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r4kdoy/scraping_vinted_tls_fingerprinting_session/",
      "author": "DataKazKN",
      "created_utc": "2026-02-14 13:16:31",
      "score": 22,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "spent the last couple weeks trying to scrape vinted (european secondhand marketplace, ~50M users). figured i'd share what i learned because their anti-bot setup is genuinely impressive and i haven't seen much written about it.\n\n**the stack they're running**\n\nvinted sits behind cloudflare with a pretty aggressive bot management config. but the interesting part isn't cloudflare itself â€” it's what happens after you get past the initial challenge.\n\ntheir API requires a valid session with specific cookies generated through a multi-step auth flow. you can't just grab a token and go. the cookies are tied to your TLS fingerprint, so if your http client doesn't match what a real browser would send, the session gets invalidated silently. no error, no 403 â€” just empty responses or stale data.\n\n**what didn't work**\n\nstarted with plain axios/undici. instant blocks. moved to got-scraping (which uses header-generator under the hood for realistic TLS). got further but still inconsistent â€” about 40% of requests would return empty arrays even with valid-looking sessions.\n\nturned out the issue was TLS fingerprint rotation. if your JA3/JA4 hash changes between requests but your session cookie stays the same, they flag it. so you need to either:\n- keep a consistent TLS fingerprint per session\n- or rotate both session AND fingerprint together\n\n**what actually worked**\n\nended up using a playwright-based cookie factory that spins up a real browser context, completes the oauth flow, captures the authenticated cookies, then passes those to a lightweight http client (got-scraping) that maintains the same TLS fingerprint for the lifetime of that session.\n\nthe key insight was treating sessions as disposable units â€” each one gets ~50-100 requests before you burn it and create a new one. tried pushing to 200+ and the ban rate went from ~2% to ~30%.\n\nalso had to handle their per-country domain routing. vinted merged most of western EU into one catalog but the API endpoints still differ by country (fr, de, es, it, nl, be, pt, etc). each needs its own session pool.\n\n**results**\n\nmanaged to pull ~960 items for a single query (\"nike air max\", france) in one run with full metadata â€” prices, seller stats, item condition, photos, timestamps. the data quality is actually really good once you get past the anti-bot layer.\n\nthe whole session rotation + cookie factory approach runs without proxies for moderate volume. for heavier loads you'd obviously want to add proxy rotation but the TLS fingerprint consistency matters way more than IP rotation in my experience.\n\n**lessons learned**\n\n- TLS fingerprinting is the real gatekeeper now, not just IP bans\n- silent failures (empty responses instead of 403s) are way harder to debug than explicit blocks\n- session lifetime management matters more than raw request volume\n- got-scraping > undici/axios for anything cloudflare-protected\n- playwright for auth, lightweight client for data â€” don't use a full browser for every request\n\ncurious if anyone else has tackled vinted or similar cloudflare-protected marketplaces. the TLS fingerprint binding to sessions was a new one for me.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r4kdoy/scraping_vinted_tls_fingerprinting_session/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5pj7qi",
          "author": "StoneSteel_1",
          "text": "If you were using python, I would have suggested you curl-cffi for TLS Signature spoofing.",
          "score": 2,
          "created_utc": "2026-02-16 16:42:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5q06q7",
              "author": "DataKazKN",
              "text": "yeah curl-cffi is solid for that. i actually started in python but moved to node for the mcp server integration. tls-client (node equivalent) works similarly. the tricky part with vinted isnt just the tls fingerprint though, they also check oauth2 token consistency across requests so you need the full session flow not just spoofed headers.",
              "score": 1,
              "created_utc": "2026-02-16 18:00:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5pciwo",
          "author": "jagdish1o1",
          "text": "I would've done the same, i once also had vinted related project, although client rejected the proposal. I had the same approach, use the browser for sessions and than use http client to scrape further. \n\nI once used this same approach to mimic an api request where i used the headless browser to clear the auth flow and than grab the requests headers and mimic the requests with http client. I dockerized the whole thing and made few things arguable so that i can run mutliple instances with different logins and scale it. Later i hosted the docker image on AWS and used ECS tasks to scale it. I had so much fun when i was working on this project. \n\n",
          "score": 1,
          "created_utc": "2026-02-16 16:11:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5q06p8",
              "author": "DataKazKN",
              "text": "yeah exactly, browser for the initial session then switch to raw http for speed. the cookie/token extraction step is the key part. once you have valid oauth2 tokens from the browser session you can hammer the api endpoints way faster than any headless approach. vinted specifically checks JA3 fingerprints so the browser bootstrap is basically mandatory unless you spoof TLS signatures.",
              "score": 1,
              "created_utc": "2026-02-16 18:00:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5stjv0",
          "author": "Ok-Depth-6337",
          "text": "Vinted is not too hard\n\nI reversed the app few months ago and i think qorks great. \n\nNo issues. \nTake a look to the app and frida :)",
          "score": 1,
          "created_utc": "2026-02-17 02:52:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nzt0y",
          "author": "[deleted]",
          "text": "[removed]",
          "score": -2,
          "created_utc": "2026-02-16 11:26:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ov0to",
              "author": "webscraping-ModTeam",
              "text": "ðŸ‘” Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
              "score": 1,
              "created_utc": "2026-02-16 14:45:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3hy8h",
      "title": "Chrome extension that auto-detects and extracts data from any webpage",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r3hy8h/chrome_extension_that_autodetects_and_extracts/",
      "author": "mjiqbal",
      "created_utc": "2026-02-13 06:33:11",
      "score": 14,
      "num_comments": 5,
      "upvote_ratio": 0.9,
      "text": "we got tired of writing one-off scripts for simple scraping jobs (product listings, search results, directories, tables ) so we built a Chrome extension called Detect and Extract that handles the common cases without writing a single line of code.\n\nHow it works:\n\n1.  Click the extension icon on any page, it automatically detects data structures (tables, lists, grids, card layouts, etc.)\n2.   Preview the data in a grid, rename or delete columns you don't need\n3.   Export as CSV, Excel, JSON, or copy straight to clipboard\n\n The part we are most proud of, multi-page crawling:\n\n*   Point it at the \"Next\" button or page numbers, and it'll auto-crawl through all pages\n*   Handles numbered pagination (1, 2, 3...) and \"Next >\" button patterns\n*   Auto-detects and dismisses modal popups that block the page during crawls (upgrade prompts, cookie banners, etc.)\n*   Built-in deduplication so you don't get repeat rows across pages\n\n  Other stuff:\n\n*  Visual element picker â€” if auto-detection misses something, click any element on the page and it finds all similar items\n*  Presets â€” save your scraping config per site so you don't have to set it up again next time\n*  Minimal permissions â€” only uses activeTab, no background data collection, no account required\n*  Works on most sites â€” e-commerce, directories, forums, search results, dashboards\n\n  What it's NOT:\n\n*   Not a Selenium/Puppeteer replacement â€” this is for visual, interactive scraping\n*   Won't bypass anti-bot measures or CAPTCHAs\n*   Not great for SPAs that require authentication flows or infinite nested navigation\n\n\n\nTrying to make it a solid free tool for people who need quick data without spinning up a whole scraping pipeline.\n\nWould love feedback from this community. What features would make this more useful for your workflows?\n\n[Detect and Extract](https://chromewebstore.google.com/detail/detect-and-extract/kkmibnjkdelljnkoibconbnaenpliefi)\n\nhttps://preview.redd.it/lj6mmc2xg7jg1.png?width=1681&format=png&auto=webp&s=9a0545a48f68145697e2a74d99da783ae07320da\n\n  \nHeads up about the install warning: If you have Chrome's Enhanced Safe Browsing turned on (Chrome Settings > Privacy), you'll see a \"Proceed with caution\" dialog when installing. This is completely normal for all newly published extensions it's not a security issue with the extension itself. Google flags every new extension until it builds up a trust score over time through installs and automated safety reviews. Just click \"Continue to install\" and it works fine. Users with Standard Safe Browsing won't see this at all.  \nReference URL [Regarding that warning](https://support.google.com/chrome/answer/2664769?visit_id=639065601437510841-3127678855&p=cws_enhanced_safe_browsing&rd=1#10745467)  \n",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r3hy8h/chrome_extension_that_autodetects_and_extracts/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o55xo30",
          "author": "Gold_Emphasis1325",
          "text": "I made a few versions of these and settled on one that I sometimes use. I was thinking about trying to productize and ideally make money or at least make connections, but with vibe coding out these days, a whole genre of apps and utilities and projects are now \"useless\" in the customer sense, but gold for the individual -- hyper personalized, \"free (minus your time and tokens)\" and something to showcase....\n\nPrivate-source open roadmap for anyone building their own:  \n\\- cloudflare box interaction  \n\\- old and 2025+ style captchas (and beta unseen human detection bypass)  \n\\- secure interaction with API layer / MCP - LLM - RAG - persistence  \n\\- auth/auth  \n\\- payments system  \n\\- thought through TOS of the plugin/API, privacy (what users will accept) and payments  \n\\- plan for dealing with fragility, constant updates",
          "score": 1,
          "created_utc": "2026-02-13 13:57:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5awbvi",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-14 06:51:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b4cax",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-14 08:06:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5bd2mc",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-14 09:32:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3uw0e",
      "title": "Built two scrapers for european markets what should I learn next?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r3uw0e/built_two_scrapers_for_european_markets_what/",
      "author": "Free-Path-5550",
      "created_utc": "2026-02-13 17:10:14",
      "score": 9,
      "num_comments": 3,
      "upvote_ratio": 0.81,
      "text": "Been working on scrapers for a couple of European marketplaces this past week. My first attempt in this space. I finished two that successfully \"work\"... for now.  Some things I picked up so far:\n\n\\- Raw data isn't enough. Adding computed fields like deal detection, engagement scoring, and price tracking across runs makes the output way more useful than a static dump.\n\n\\- European platforms have aggressive anti-bot compared to US sites from my research. Took real effort to get stable.\n\n\\- You don't need a browser for everything. Keeping it lightweight makes a huge difference.\n\n\\- Biggest lesson learned... was how much I hate DataDome. I was able to slip past it a few times, but usually blocked the next run. I eventually learned to just go around it if possible.\n\nStill early in this spcraping. What should I be learning next? What separates a decent scraper from a great one?  Thanks!",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r3uw0e/built_two_scrapers_for_european_markets_what/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o59r43b",
          "author": "Bitter_Caramel305",
          "text": "Try running your existing scrapers on a few hundred thousand product/items and you'll realize that scrapers without proxies and regular maintains are not stable at all. ",
          "score": 8,
          "created_utc": "2026-02-14 01:48:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r103lp",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r103lp/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-02-10 13:01:11",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 0.91,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levelsâ€”whether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) ðŸŒ±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring ðŸ’°",
      "permalink": "https://reddit.com/r/webscraping/comments/1r103lp/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4u7yks",
          "author": "Finding_Away40",
          "text": "webscraping newbie here.  How do I scrape Google maps, or can I scrape google business profiles?  Scraping for a list of residential and commercial paining companies in the US.  Or, is it easiest to use a tool like Outscraper?  thx",
          "score": 1,
          "created_utc": "2026-02-11 18:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vaoah",
          "author": "skinonmychin",
          "text": "How would I scrape the files behind an old website (late aughties) from WebArchive? I recall seeing somewhere a way to download the text files behind it without fragmenting the content.",
          "score": 1,
          "created_utc": "2026-02-11 21:10:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4w4vur",
              "author": "HLCYSWAP",
              "text": "    wget \\\n      --mirror \\                # Mirror the site recursively\n      --convert-links \\         # Convert links to work offline\n      --adjust-extension \\      # Save files with proper extensions (like .html)\n      --page-requisites \\       # Download all resources (CSS, JS, images)\n      --no-parent \\             # Donâ€™t ascend to parent directories\n      -e robots=off \\           # Ignore robots.txt restrictions\n      -P ./local-copy \\         # Save to this local folder\n      \"https://example.com/path/\"",
              "score": 1,
              "created_utc": "2026-02-11 23:43:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o50bzja",
          "author": "Eyoba_19",
          "text": "Currently building a social media (tiktok and instagram mostly) to scrape profiles. The idea would be you can input specific keywords or description of what kind of influencers youâ€™re looking for with specific criteria like min/max follower count, region, engagement rate and so on, and retrieve a set of profiles.\n\nOver time a knowledge graph of profiles will be built and you can thus just fetch from a db instead of scraping.\n\nYou can then ofcourse sell the data back or train AI or whatever, you own your data.\n\nIâ€™m planning it as a Saas, but open to do it as a one-off product(although not sure how to handle constant support for when sites change their API).\n\nLet me know if anyone is interested, would love to talk to some people to kinda hit where this hurts and make sth out of it, DMs are open",
          "score": 1,
          "created_utc": "2026-02-12 16:53:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r642hh",
      "title": "anyone else tired of ai driven web automation breaking every week?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r642hh/anyone_else_tired_of_ai_driven_web_automation/",
      "author": "Ok_Abrocoma_6369",
      "created_utc": "2026-02-16 08:20:11",
      "score": 8,
      "num_comments": 18,
      "upvote_ratio": 0.79,
      "text": "Seriously, my python scrapers fall apart the moment a site changes a class name or restructures a div.   \nwe mainly monitor competitor pricing, collect public data, and automate internal dashboards but maintaining scripts is killing productivity.   \ni have heard ai can make scrapers more resilient, teaching a system to understand a page and find data on its own.\n\ni am curious what people are actually running in production:   \nwhat does your stack look like?  \ndo you use ai powered web interaction or llms to control browsers?   \nhow do you handle scaling and avoiding blocks in the cloud?\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r642hh/anyone_else_tired_of_ai_driven_web_automation/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5nhyub",
          "author": "CouldBeNapping",
          "text": "We solved it by automating browsers in Windows. 25 VPSâ€™ with a bare bones Windows 10 or 11 install. \nRotating VPNs and residential proxies. \n\nBeen 100% successful for the last 18 months.",
          "score": 4,
          "created_utc": "2026-02-16 08:40:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nuhcs",
              "author": "HarambeTenSei",
              "text": "Not even Linux browsers?",
              "score": 1,
              "created_utc": "2026-02-16 10:38:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5nxerj",
                  "author": "CouldBeNapping",
                  "text": "Most \"real\" people who browse online stores are on Windows.   \nJust adds to the legitimacy of the user profile.",
                  "score": 2,
                  "created_utc": "2026-02-16 11:05:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ngnov",
          "author": "ProgrammerRadiant847",
          "text": "we have been testing a few different frameworks and tbh its still a bit of a patchwork.",
          "score": 2,
          "created_utc": "2026-02-16 08:27:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nwflt",
          "author": "AdhesivenessOld8612",
          "text": "Selector drift is brutal, curious whether people are solving this with smarter patterns in tools like Playwright or actually leaning into AI-driven extraction in production.",
          "score": 2,
          "created_utc": "2026-02-16 10:56:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nhm4x",
          "author": "Azuriteh",
          "text": "I don't recommend AI scrapers at all, the cost scales a lot.\n\nIn any case, if you want to truly do it like that, I personally haven't found any library and actually delivers, you have to build your own agentic harness, but you can take inspiration on already existing harnesses for other tools to make things easier for you.\n\nAnother problem is that even for existing scraping AI libraries, they use easily detected browser emulators, e.g. Playwright, which adds another level of me not wanting to use them.",
          "score": 2,
          "created_utc": "2026-02-16 08:37:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nhvdo",
              "author": "Azuriteh",
              "text": "For scaling it's the same old story: use proxies according to the sophistication of the anti-bot defenses of the site and use camouflaged browsers, keeping track of antibot cookies per browser session.\n\nI usually try to reverse engineer the websites to instead just use TLS fingerprinting mimicry though, as it scales much better, but not always possible sadly (due to IP quality when deploying in cloud most of the time!) ",
              "score": 2,
              "created_utc": "2026-02-16 08:39:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5nirxq",
              "author": "One_Development8489",
              "text": "Playwrath + claude code with chrome plugin (which can auto open chrome and debug as a agent)",
              "score": 1,
              "created_utc": "2026-02-16 08:48:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nhroz",
          "author": "Worth-Culture5131",
          "text": "cost is the killer. we built a prototype with an llm orchestrator, and it was brilliant... until we saw the api bill for 10k pages a day. ",
          "score": 1,
          "created_utc": "2026-02-16 08:38:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nvak6",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-16 10:46:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nycyr",
              "author": "webscraping-ModTeam",
              "text": "âš¡ï¸ Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-02-16 11:13:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5p62jm",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-16 15:41:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qkd9y",
              "author": "webscraping-ModTeam",
              "text": "âš¡ï¸ Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-02-16 19:34:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5pmxjh",
          "author": "Coding-Doctor-Omar",
          "text": "Dont rely on normal html parsing. Look for either internal API requests or json blobs inside script tags in the page source.",
          "score": 1,
          "created_utc": "2026-02-16 16:59:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ofrca",
          "author": "forklingo",
          "text": "yeah this is basically the tax you pay for scraping anything modern. in my experience ai doesnâ€™t magically fix brittle selectors, it just moves the brittleness up a layer unless youâ€™re really thoughtful about how you structure it. weâ€™ve had better luck combining solid dom heuristics, fallback selectors, and some light semantic matching rather than full llm driven browsing. for scaling and blocks itâ€™s mostly about boring stuff like good proxy rotation, sane request rates, and making traffic look human instead of blasting endpoints. curious if anyone here is actually running llm controlled browsers in prod without the costs getting wild.",
          "score": 0,
          "created_utc": "2026-02-16 13:22:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r16ix2",
      "title": "Is cookie reuse risky or necessary for modern scraping?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r16ix2/is_cookie_reuse_risky_or_necessary_for_modern/",
      "author": "JosephPRO_",
      "created_utc": "2026-02-10 17:07:40",
      "score": 7,
      "num_comments": 9,
      "upvote_ratio": 0.78,
      "text": "I keep seeing very mixed advice when it comes to cookies in scraping workflows. Some people say reusing cookies is important if you want sessions to look normal and avoid getting blocked, especially on sites with logins or multi-step flows. Others warn that reusing cookies is risky and can actually cause more problems if you carry over bad state or get a session flagged.\n\nFrom what Iâ€™ve seen so far, starting with a fresh session every time sometimes works, but other times it feels like sites expect continuity. Reusing cookies seems to make things more stable in those cases, but Iâ€™m never sure how long is too long or when a session should be thrown away and rebuilt.\n\nIâ€™m trying to figure out what actually works in real-world scraping, not just in theory. Do people here mostly reuse cookies, rotate them often, or avoid them unless absolutely needed? Have cookies been a bigger source of trouble than things like IPs or headers over time?\n\nCurious to hear how others approach this and what youâ€™ve learned from experience.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r16ix2/is_cookie_reuse_risky_or_necessary_for_modern/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4n90y3",
          "author": "v_maria",
          "text": "This honestly depends on the site, thats the fun thing about scraping. There is no real silver bullet you need to learn and understand web",
          "score": 11,
          "created_utc": "2026-02-10 17:10:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n9lh7",
          "author": "army_of_wan",
          "text": "You throw the session away when it expires  and you'll know it expires when you get a 403 instead of a 200. \n\nCatch the exception\n\nInitiate a solving process -> could be cookie farming api or something else\n\nUpdate your session with new cookies\n\nYou may also consider proxy rotation.\n\nGo again.",
          "score": 8,
          "created_utc": "2026-02-10 17:13:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nljif",
          "author": "RandomPantsAppear",
          "text": "Not in the slightest, especially as most sites wonâ€™t block you solely for being an unfamiliar entity. No history + one of (bad ip, bad history, bad headers) will get you blocked though. \n\nThere are multiple ways to handle this\n\n1) Just accept your 50% success rate and abuse celeryâ€™s retry functionality \n\n2) Forge the cookies (integer, timestamp, uuid)\n\n3) Build a history - easier than it sounds. Iâ€™ve actually had scrapers that zip up their chrome profile and upload to s3 when theyâ€™re done, then redownload it when they start again. \n\nâ€”â€”â€”â€”â€”-\n\nThe biggest advantage a scraper has vs history based blocking is that **they can only query data they can access in milliseconds**. It is not acceptable virtually anywhere to add 200ms to every request. \n\nWhat this means is that you only need the basics - a number of requests previously seen, an ideally distant start date, and an ideally recent end date. Thatâ€™s the kind of data most places can quickly cache in volume. \n\nSimilarly (for cookie forging) a remarkable number of places will accept a uuid as legitimate just because it should be a uuid and is a uuid, and cache misses and syncing between multiple nodes is a common issue with real users. \n\nThis can get a little hairy once captchas start rendering and you donâ€™t solve them. But for most solutions, a pretty basic history will go a long way.",
          "score": 3,
          "created_utc": "2026-02-10 18:08:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qoqqp",
          "author": "SharpRule4025",
          "text": "It depends entirely on the site and what their anti-bot is looking for. There's no universal rule.\n\nFor sites with Cloudflare or similar protection, reusing a valid session cookie avoids re-solving the challenge on every request. That's a net positive, fewer challenge pages, faster requests, lower detection risk. The key is rotating them before expiry and not sharing a single cookie across multiple IPs.\n\nFor simpler sites, fresh sessions work fine. The overhead of cookie management isn't worth it if the site doesn't track session state for bot detection.\n\nThe real risk with cookie reuse is carrying a flagged session forward. If one request gets soft-blocked (CAPTCHA, rate limit warning), that cookie is burned. You need logic to detect when a session goes stale and swap it out immediately rather than keep hammering with a poisoned session.",
          "score": 2,
          "created_utc": "2026-02-11 03:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zcirk",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-12 13:59:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zj75h",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-12 14:35:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o567v2g",
          "author": "sohailglt",
          "text": "It depends on the website youâ€™re scraping. Youâ€™ll need to test and try the cookies one by one to see which ones are required.",
          "score": 1,
          "created_utc": "2026-02-13 14:51:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r61a90",
      "title": "Need tought websites to scrape for testing",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r61a90/need_tought_websites_to_scrape_for_testing/",
      "author": "Transformand",
      "created_utc": "2026-02-16 05:39:35",
      "score": 7,
      "num_comments": 15,
      "upvote_ratio": 0.77,
      "text": "I've been developing my own piece of code, that so far has been able to bypass anti-bot security I had a tough time cracking before at scale (such as PerimiterX).\n\nCan you share what sites you think are difficult to access/scrape?\n\nI want to test out my scraper more before open sourcing it",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r61a90/need_tought_websites_to_scrape_for_testing/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5mz52h",
          "author": "ry8",
          "text": "Try BassProShop.",
          "score": 3,
          "created_utc": "2026-02-16 05:50:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mz7p3",
          "author": "Lemon_eats_orange",
          "text": "Jokingly if you can get past the anti bot defenses on piracy anime sites that would be a site to see. Though like seriously you open the chrome developer tools on some of those sites and they will feedback loop you and run your resources dry. \n\nOff the top of my head these sites might be difficult. \nWalmart.com, zoro.com, naver.com and it's subdomains can be incredibly difficult, hermes.com, safeway.com. Definitely shopee.",
          "score": 2,
          "created_utc": "2026-02-16 05:51:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n45v8",
          "author": "thePsychonautDad",
          "text": "Protonmail email creation steps.",
          "score": 3,
          "created_utc": "2026-02-16 06:33:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5naddf",
          "author": "LT823",
          "text": "Instagram followers of profile",
          "score": 1,
          "created_utc": "2026-02-16 07:29:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nc08b",
          "author": "GuNiKz",
          "text": "reuters, I tried once, but I wasn't successful ",
          "score": 1,
          "created_utc": "2026-02-16 07:44:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ndn62",
          "author": "Little-traveler-1995",
          "text": "Try Hyatt hotel, I guess without using any browser automation tool it is highly impossible to crawl any link.",
          "score": 1,
          "created_utc": "2026-02-16 07:59:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nleq8",
          "author": "trololololol",
          "text": "Google Shopping",
          "score": 1,
          "created_utc": "2026-02-16 09:13:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nm4zv",
          "author": "Unlikely1529",
          "text": "bet365 hehe",
          "score": 1,
          "created_utc": "2026-02-16 09:20:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5no5cn",
          "author": "brateq",
          "text": "Aliexpress, Darty(dot)com",
          "score": 1,
          "created_utc": "2026-02-16 09:39:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5omyc2",
          "author": "vorty212",
          "text": "builtwith",
          "score": 1,
          "created_utc": "2026-02-16 14:02:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5op22g",
          "author": "Krokzter",
          "text": "Idealista is the most aggressive use of Datadome I've seen",
          "score": 1,
          "created_utc": "2026-02-16 14:14:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5r6ztk",
          "author": "ertostik",
          "text": "Let me know if someone pass sahibinden.com protection.",
          "score": 1,
          "created_utc": "2026-02-16 21:25:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n7b3a",
          "author": "scorpiock",
          "text": "Try payment, social media sites",
          "score": 0,
          "created_utc": "2026-02-16 07:01:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n08hq",
          "author": "nofilmincamera",
          "text": "Sounds kind of insane but I like banks for this. Specifically self service portals that are public. Tended to have recaptcha 3 plus latered protection.",
          "score": -1,
          "created_utc": "2026-02-16 05:59:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4phlg",
      "title": "Payment processors for a scraping SaaS (high-risk niche)",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r4phlg/payment_processors_for_a_scraping_saas_highrisk/",
      "author": "Accomplished_Mood766",
      "created_utc": "2026-02-14 16:50:20",
      "score": 5,
      "num_comments": 23,
      "upvote_ratio": 0.78,
      "text": "Hi everyone,  \nIâ€™m running a SaaS that provides scraping services, and Iâ€™m currently struggling with payment processing.\n\nStripe, Paddle, and Lemonsqueezy have all declined us due to the nature of the business. I understand that this niche is often classified as high-risk, but in practice weâ€™ve been operating for 5 months with **zero chargebacks or disputes**. Unfortunately, that doesnâ€™t seem to matter much to decision-makers at most payment platforms â€” scraping services are automatically flagged as high risk.\n\nIâ€™d like to ask those of you who are running SaaS products in similar areas (scraping, data extraction, automation, etc.):\n\n* Which payment processors or merchant accounts are you using to accept credit card payments?\n* Are there providers that are more tolerant or experienced with this type of business?\n* Any recommendations or experiences youâ€™re willing to share would be greatly appreciated.\n\nThanks in advance â€” Iâ€™d really value hearing from others whoâ€™ve dealt with this problem.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r4phlg/payment_processors_for_a_scraping_saas_highrisk/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5d5tuf",
          "author": "JohnnyOmmm",
          "text": "Lmao crypto or porno processors",
          "score": 5,
          "created_utc": "2026-02-14 16:54:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d6da7",
              "author": "Accomplished_Mood766",
              "text": "They accept cryptocurrency, but our customers donâ€™t use crypto. Our users are mostly recruiters, marketers, and regular consumers who are accustomed to traditional credit and debit card payments.",
              "score": 3,
              "created_utc": "2026-02-14 16:57:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5dke6t",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 7,
                  "created_utc": "2026-02-14 18:07:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5dcjg8",
                  "author": "RobSm",
                  "text": "Send invoices and let them pay monthly wiretransfer, etc. You can automate email sending and if not paid - email notifications, etc. Also, paypal? Which has option to use CC directly?",
                  "score": 2,
                  "created_utc": "2026-02-14 17:28:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5e4rl6",
          "author": "convicted_redditor",
          "text": "Try polar.sh",
          "score": 2,
          "created_utc": "2026-02-14 19:51:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5eoyik",
          "author": "Dry_Illustrator977",
          "text": "Why not try crypto?",
          "score": 2,
          "created_utc": "2026-02-14 21:41:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pohf6",
          "author": "amogh-datar",
          "text": "I would suggest integrate multiple payment gateways. That way, if one is terminated suddenly you can make others live and then add another one in the meantime.   \n  \nIdeally you should integrate the high-risk payment gateways. Search for \"high risk payment gateways in \\[country\\]\" on Google and you should find them. They do have high fees but are likely to work for long term for high risk niche like yours. You can contact them to confirm the same.\n\nAlso, if your clients like your service, do offer them a way to pay via crypto or wire transfers to save payment gateway fees.",
          "score": 2,
          "created_utc": "2026-02-16 17:06:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gg8cf",
          "author": "Quantum_Rage",
          "text": "I don't what kind of scraping SaaS you're building, but I have seen SaaS apps built on web scraping accept payments via Stripe or Paddle.",
          "score": 1,
          "created_utc": "2026-02-15 04:27:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5imxka",
              "author": "Accomplished_Mood766",
              "text": "I run a SaaS focused on LinkedIn data scraping. Stripe declined us immediately. We worked with Paddle for five months with zero chargebacks, but their risk management team eventually decided to terminate our payment account.",
              "score": 1,
              "created_utc": "2026-02-15 15:09:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kcp1x",
          "author": "awebscrapingguy",
          "text": "I think you are not saying everything, all scraping saas run on Stripe or Paddle without issues",
          "score": 1,
          "created_utc": "2026-02-15 20:13:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kpe1p",
              "author": "Accomplished_Mood766",
              "text": "Stripe explicitly prohibits scraping-related services. Paddleâ€™s policies were more flexible, and we were able to work with them for five months; however, their risk team eventually decided to stop supporting our account.",
              "score": 1,
              "created_utc": "2026-02-15 21:18:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kjygm",
          "author": "Round_Method_5140",
          "text": "Paypal?",
          "score": 1,
          "created_utc": "2026-02-15 20:50:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3lfdv",
      "title": "How do you handle scraping directory sites that cap results at ~200?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r3lfdv/how_do_you_handle_scraping_directory_sites_that/",
      "author": "deadlynightshade_x",
      "created_utc": "2026-02-13 10:08:34",
      "score": 5,
      "num_comments": 17,
      "upvote_ratio": 0.69,
      "text": "Hey \n\nI've been trying to pull data from a large online directory/phone book site (Swiss one, but I think the issue is pretty common across similar services like yellow pages, local directories, etc.).The site claims tens of thousands of matching entries (e.g., \\~50k private records for a region), but in practice:\n\n* URL params like &pages=500, &maxnum=500, or whatever don't actually fetch more and it hard-caps visible/returned results around 200.\n\nHas anyone here successfully scraped large volumes from these kinds of directory/phone book sites recently ?\n\n* Do most of them still enforce strict \\~100â€“200 result caps per query/page?\n* What tricks actually work to get around it without getting banned quickly?\n\nJust curious if it's still feasible for bigger datasets or if these sites have mostly locked it down. \n\n\n\nTips, tools, or experiences appreciated !Thanks!",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r3lfdv/how_do_you_handle_scraping_directory_sites_that/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o55w5y8",
          "author": "datamizer",
          "text": "They all set max results per query to normal defaults like 100. Even if you try to request more, the API basically has logic to protect it from serving a very large result set. \n\nAll you do is set a page number or offset, figure out the max result size, then send paginated requests at some interval. That's very standard.\n\nDepends on the site, some just have basic rate limits like no more than 10 requests over 10 seconds from the same IP. Most don't have robust prevention. Most you just pass an origin and a referer and that's good enough.",
          "score": 7,
          "created_utc": "2026-02-13 13:49:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o565cf3",
              "author": "leros",
              "text": "Lots of sites cap the results and don't let you paginate through them all. You need be more creative to scrape everything.Â ",
              "score": 2,
              "created_utc": "2026-02-13 14:38:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56d6i8",
                  "author": "datamizer",
                  "text": "It depends on the specifics of how they implemented it. Some sites there is no mechanism other than having a full sitemap if they offer one. Checking robots.txt to see what they are trying to obscure, using search engines that have their pages indexed etc. It's really just site by site basis.",
                  "score": 1,
                  "created_utc": "2026-02-13 15:17:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5bqpts",
              "author": "deadlynightshade_x",
              "text": "Thank you",
              "score": 1,
              "created_utc": "2026-02-14 11:44:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o595at0",
          "author": "forklingo",
          "text": "a lot of those caps arenâ€™t just ui limits, theyâ€™re intentional backend limits to stop bulk extraction. if the api itself only returns \\~200 per query, tweaking page params usually wonâ€™t help.\n\nwhat tends to work better is slicing the query space instead of trying to increase the page size. for example, split by smaller geographic areas, postal codes, name prefixes, or categories so each query stays under the cap but collectively covers more ground.\n\nalso be careful with â€œgetting aroundâ€ protections. many directory sites have pretty strict terms and rate limits, and they do monitor unusual access patterns. rotating ips or hammering endpoints might work short term but usually gets you blocked fast.\n\nfor larger datasets, sometimes itâ€™s more realistic to look for official data sources, public datasets, or paid access if itâ€™s business critical. scraping big directories at scale has gotten a lot harder over the years.",
          "score": 3,
          "created_utc": "2026-02-13 23:34:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bquvy",
              "author": "deadlynightshade_x",
              "text": "I appreciate your input",
              "score": 1,
              "created_utc": "2026-02-14 11:45:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o57vvg6",
          "author": "Free-Path-5550",
          "text": "Break your one big query into many smaller ones that each return under the cap., then deduplicate after merging since slices will overlap. Well this is how i solved against the site i was trying.",
          "score": 1,
          "created_utc": "2026-02-13 19:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5br7pq",
              "author": "deadlynightshade_x",
              "text": "That what i did and it worked ! Thanks",
              "score": 1,
              "created_utc": "2026-02-14 11:48:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5a85n5",
          "author": "pauldm7",
          "text": "You will likely need to make a lot more queries. Ie instead of searching for widgets, you will search for widgets a, widgets b, widgets aa widgets ab and so on. Cover every combo of letters until the results stop.\n\nIf you can click on the individual results, how is the URL? Numerical IDs?",
          "score": 1,
          "created_utc": "2026-02-14 03:39:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bra4d",
              "author": "deadlynightshade_x",
              "text": "That's exactly what I did and it gave me good results! Thanks",
              "score": 1,
              "created_utc": "2026-02-14 11:49:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bfme2",
          "author": "uncivilized_human",
          "text": "one thing i've found helps - check if there's an internal api the frontend uses. the caps are often enforced at the ui layer but the underlying api might have different limits. open network tab, search, and see what endpoints get hit. mobile apps are worth checking too since their apis tend to be less locked down.",
          "score": 1,
          "created_utc": "2026-02-14 09:57:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5brd9y",
              "author": "deadlynightshade_x",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-02-14 11:49:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5gvbay",
                  "author": "shipping_sideways",
                  "text": "np! good luck with it - the internal apis on sites like that sometimes have cursor-based pagination that lets you get around the arbitrary caps. if you find the actual endpoint the frontend uses, it's often way less restrictive than what they expose to users",
                  "score": 1,
                  "created_utc": "2026-02-15 06:32:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o56bp02",
          "author": "Hour_Analyst_7765",
          "text": "Yes most sites cap request sites.. whether its too big for a client (probably historic reasons), or anti-scraping measure.\n\nHowever, 100 to 200 per call is imo quite good still. If the dataset is 50k, thats 500 pages, and even if you wait a minute per call you'll have that dataset in just over 8 hours. If you refresh daily thats quick enough.\n\nFor me <10 hours is plenty quick. But I often don't have very strict latency requirements. Most of my jobs need to keep up on a \"daily basis\"\n\nThe difficulty is often:\n\n1. Walking pages linearly may get you blocked on WAF nowadays.\n2. Sites that stop serving data after N amount of records, because they don't expect anyone to realistically scan through literal hunrdeds or thousands of records.\n\nIn this case, you'll have to deal with navigation through categories, parametric filters, etc. to reduce the dataset that can be crawled completely.\n\n3. Sites that don't have a way of putting new/modified records in front. I fetch quite a lot of listings and always will look for a \"newest first\" kind of sorting. Fortunately for many sites this is their default. I stop crawling as soon as I see data that I've already crawled.",
          "score": 1,
          "created_utc": "2026-02-13 15:10:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58kdvb",
              "author": "ObjectIndependent827",
              "text": "Yeah this matches what most people run into now itâ€™s less about brute forcing pages and more about breaking the dataset into smaller slices through filters or categories\n\nNewest first sorting is huge too since it lets you maintain coverage without re crawling everything and helps avoid triggering WAF limits over time",
              "score": 2,
              "created_utc": "2026-02-13 21:42:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5br5j3",
              "author": "deadlynightshade_x",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-02-14 11:47:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r0xvwc",
      "title": "IMDB data scraping",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r0xvwc/imdb_data_scraping/",
      "author": "pavankalyanre",
      "created_utc": "2026-02-10 11:06:49",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "**My old post:**\n\n[ https://www.reddit.com/r/webscraping/comments/o7l9fw/here\\_is\\_how\\_i\\_scraped\\_everything\\_on\\_imdb\\_website/ ](https://www.reddit.com/r/webscraping/comments/o7l9fw/here_is_how_i_scraped_everything_on_imdb_website/?utm_source=chatgpt.com)\n\nHi webscrapers,\n\nA few years ago, I posted about how I scraped around 500k IMDb movies. Iâ€™ve since updated the package to extract data directly from imdb apis instead of using BeautifulSoup or Selenium.\n\nI created scripts to extract IMDb data.\n\nCheck out the package and feel free to fork it and try it out. If you find it useful, a â­ on the repo would be appreciated.\n\nGithub :  [ https://github.com/pavan412kalyan/imdb-movie-scraper/tree/main/ImdbDataExtraction ](https://github.com/pavan412kalyan/imdb-movie-scraper/tree/main/ImdbDataExtraction)\n\nHere is the IMDb movie dataset on Kaggle: updated with 700k + movies now\n\n[ https://www.kaggle.com/datasets/pavan4kalyan/imdb-dataset-of-600k-international-movies/data ](https://www.kaggle.com/datasets/pavan4kalyan/imdb-dataset-of-600k-international-movies/data)\n\nYou can also see the website I built using this data (with some AI):\n\n[ https://realimdb.com/ ](https://realimdb.com/)\n\nScripts included:\n\nImdbDataExtraction/\n\nâ”œâ”€â”€ pages\\_dowloader/           # Movie/TV bulk scraping\n\nâ”œâ”€â”€ search\\_by\\_id/              # Individual lookups\n\nâ”œâ”€â”€ search\\_by\\_string/          # Text-based search\n\nâ”œâ”€â”€ people\\_downloader/         # Celebrity/crew data\n\nâ”œâ”€â”€ videos\\_downloader/         # Video content\n\nâ”œâ”€â”€ images\\_dowloader/          # Image content\n\nâ”œâ”€â”€ review\\_downl",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r0xvwc/imdb_data_scraping/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r1wc9z",
      "title": "scrape data from site that loads data dynamically with javascript???",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r1wc9z/scrape_data_from_site_that_loads_data_dynamically/",
      "author": "Quiet_Dasy",
      "created_utc": "2026-02-11 12:41:37",
      "score": 4,
      "num_comments": 9,
      "upvote_ratio": 0.71,
      "text": "Project Overview: DeckMaster Scraper\n\nLive Site: domain-rec.web.app\n\nTech Stack: Flutter frontend with a Supabase backend.\n\nCurrent Access: Public REST API endpoint (No direct DB credentials).\n\n\nTarget Endpoint:\n https://kxkpdonptbxenljethns.supabase.co/rest/v1/PopularDeckMasters?select=*&limit=50\n\nThe Goal\n\nInstead of just pulling all cards\n, I need to extract the specific card name  ,not card data, contained within each individual page.\n\nThe Challenge\n\nI need a method to iterate through the IDs provided by the main API and scrape the specific card details associated with each entry.\n\nHow to Scrape the Data??\n\nSince the site uses Supabase, i don't actually need to \"scrape\" the HTML. \n",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1r1wc9z/scrape_data_from_site_that_loads_data_dynamically/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o4srk51",
          "author": "BeforeICry",
          "text": "Generally one of the best formats. Static sites require full HTML parsing whereas these you can look for the backend API and likely use a much smaller traffic bandwidth.",
          "score": 2,
          "created_utc": "2026-02-11 13:51:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wbfd8",
          "author": "WouldaCouldaBetta",
          "text": "Cant u just let the page load and scrape the loaded html?",
          "score": 1,
          "created_utc": "2026-02-12 00:21:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yt62h",
          "author": "SharpRule4025",
          "text": "This is actually way easier than you think. Flutter apps with Supabase backends expose the REST API directly, you already found the endpoint. You don't need to scrape the rendered page at all.\n\nJust hit that Supabase endpoint, grab the IDs from the main listing, then iterate through them hitting the detail endpoint for each one. Supabase REST follows PostgREST conventions so you can filter with ?id=eq.{id} to get individual records. No browser, no JS rendering needed.",
          "score": 1,
          "created_utc": "2026-02-12 11:55:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5676nq",
          "author": "sohailglt",
          "text": "Monitor the endpoints, headers, and cookies, then replicate the request exactly as they send it to retrieve the data.",
          "score": 1,
          "created_utc": "2026-02-13 14:47:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4mkpv",
      "title": "Data extraction quality / LLM cost",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r4mkpv/data_extraction_quality_llm_cost/",
      "author": "Playgroundmob",
      "created_utc": "2026-02-14 14:53:02",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 0.72,
      "text": "I'm trying to get an idea of how much people/companies would pay per result when scraping websites that require structured data with a reliable JSON schema and high data quality - for example, e-commerce or job listings. especially when dealing with unknown sources.\n\nLLMs like Flash 2.5/3.0, which are actually reliable for consistent results, are not cheap - sometimes we might even reach $0.01â€“$0.03 per extraction.\n\nI'm trying to understand, in real world terms, how much people would pay for solutions that just work.\n\nIf anyone can share real world use cases, that would be awesome.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r4mkpv/data_extraction_quality_llm_cost/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5czh07",
          "author": "RandomPantsAppear",
          "text": "I think the biggest issue you will run into is people doing it cheaper, not using an LLM. You are kind of limiting yourself to one offs, because for any recurring task people will find something more traditional and exponentially cheaper.",
          "score": 1,
          "created_utc": "2026-02-14 16:22:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d1jzs",
              "author": "Playgroundmob",
              "text": "You're right. I'm wondering if there's a use case for massive scrape operation, where we just can't handle building specific scrapers per domain, as we don't know what volume of different websites to investigate and scrape.",
              "score": 1,
              "created_utc": "2026-02-14 16:33:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5d246w",
                  "author": "RandomPantsAppear",
                  "text": "If I were you, Iâ€™d focus on integrating. Thereâ€™s lots of options but I would first build something in that let it work with Google docs, pdf, and images as well as HTML. I would also add several export types. \n\nAfter that I would make a chrome extension that allowed people to either subscribe or top up their balances, and extract data on demand. \n\nI would name the app something that business people would likely search for, not something a coder would search for.",
                  "score": 2,
                  "created_utc": "2026-02-14 16:35:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r3htxr",
      "title": "[Selenium/C#] \"Cannot start the driver service\" in Windows Service",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r3htxr/seleniumc_cannot_start_the_driver_service_in/",
      "author": "AromaticLocksmith662",
      "created_utc": "2026-02-13 06:26:28",
      "score": 3,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nIâ€™ve been banging my head against a wall for a week with a Selenium ChromeDriver issue and could use some fresh eyes.\n\nThe Context:\n\nI have a web scraping tool running as a background Windows Service. It processes license data for different states.\n\nScale: We have about 20 separate Windows Services running in parallel on the same server, each scraping different data sources.\n\nTech Stack: C# .NET, Selenium WebDriver, Chrome (Headless).\n\nVersion: Chrome & Driver are both version 144.0.x.x (Versions are matched).\n\nThe Issue:\n\nEverything was running smoothly until recently. Now, I am getting a WebDriverException claiming it cannot start the driver service on a specific localhost port.\n\nthe exception:\n\nCannot start the driver service on http://localhost:54853/\n\nThe Stack Trace:\n\nat OpenQA.Selenium.DriverService.Start()\n\nat OpenQA.Selenium.Remote.DriverServiceCommandExecutor.Execute(Command commandToExecute)\n\nat OpenQA.Selenium.WebDriver.Execute(String driverCommandToExecute, Dictionary\\`2 parameters)\n\nat OpenQA.Selenium.WebDriver.StartSession(ICapabilities desiredCapabilities)\n\nat OpenQA.Selenium.Chromium.ChromiumDriver..ctor(ChromiumDriverService service, ChromiumOptions options, TimeSpan commandTimeout)\n\nat MyNamespace.LicenseProject.Business.Vermont.VermontLicenseService.ProcessLicense() in ...\\\\VermontLicenseService.cs:line 228\n\ncode:\n\nvar options = new ChromeOptions();\n\noptions.AddArgument(\"--headless\");\n\noptions.AddArgument(\"--no-sandbox\");\n\noptions.AddArgument(\"--disable-dev-shm-usage\");\n\n// I am explicitly setting the driver directory\n\nvar service = ChromeDriverService.CreateDefaultService(driverPath);\n\nservice.HideCommandPromptWindow = true;\n\n// Error implies it fails right here:\n\nusing (var driver = new ChromeDriver(service, options, TimeSpan.FromMinutes(2)))\n\n{\n\n// scraping logic\n\n}\n\nWhat I've Tried/Verified:\n\nVersion Mismatch: Double-checked that the chromedriver.exe version matches the installed Chrome browser version (144.0.x.x).\n\nManual Run: The scraper works fine when I run it as a console app/user mode. It only fails when running as a Windows Service.\n\nCleanup: I suspected \"zombie\" chrome processes were eating up ports, so I added logic to kill orphaned chrome processes, but the issue persists.\n\nHas anyone managed high-volume Selenium instances in a Windows Service environment and seen this port binding error?\n\nAny pointers would be appreciated!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r3htxr/seleniumc_cannot_start_the_driver_service_in/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5b4f4r",
          "author": "jinef_john",
          "text": "I would explicitly enable verbose logging and log that to a file. But here is what I think is happening, with 20 parallel services, they're likely clashing on the default user data directory. You should try and set unique user data directories per service instance.\n\nAlso I suspect the driver might be failing to bind, you could let chromedriver pick its own port, by setting service.Port=0.\n\nIf neither of these help, I'd bet it's a session 0 isolation problem. I believe even in headless, chrome sometimes needs to create a window station or access GDI resources that are restricted in Session 0. So you could try running the service under a specific user account with 'allow service to interact with service'\n\nOr it could even be the headless flags too. \nTry the new headless mode\n```\n--headless=new\n```\nYou could also add other options, disable gpu, extensions,software rasterizer,force color profile to be srgb\n\nBut I would mostly look into the user data directory or session 0 isolation.",
          "score": 1,
          "created_utc": "2026-02-14 08:07:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dpbq2",
          "author": "CuriousCat7871",
          "text": "I know it is windows, but can you run the browser in a docker container instead?",
          "score": 1,
          "created_utc": "2026-02-14 18:32:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n5vqw",
          "author": "THenrich",
          "text": "You should consider switching to using Playwright instead of Selenium. Selenium is quite old and there are advantages to using Playwright.",
          "score": 1,
          "created_utc": "2026-02-16 06:48:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o568ljp",
          "author": "Stunning_Cry_6673",
          "text": "Lol. Selenium was a technology popular 15 years ago . Cant believe someone would use it today from scraping ðŸ˜‚ðŸ˜‚ðŸ˜‚",
          "score": -3,
          "created_utc": "2026-02-13 14:54:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b4tkt",
              "author": "jinef_john",
              "text": "Your favorite tool today, probably uses selenium under the hood ðŸ˜‰",
              "score": 0,
              "created_utc": "2026-02-14 08:11:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5b52xt",
                  "author": "Stunning_Cry_6673",
                  "text": "Lol. In these days when is so simple to get information you come with this garbage affirmation ðŸ¤£ðŸ¤£ðŸ¤£",
                  "score": 0,
                  "created_utc": "2026-02-14 08:14:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r5rrds",
      "title": "Letâ€™s move by step",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r5rrds/lets_move_by_step/",
      "author": "0xMassii",
      "created_utc": "2026-02-15 22:14:31",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 0.71,
      "text": "https://www.reddit.com/r/webscraping/s/0aCA0m6ioo\n\nJust use this post to comment with a website, so send in this format\n\nWebsite and what you want to achieve \n\nAnd then we will make a list so we can proceed by step and start with website with more upvotes \n\nWhat do you think?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r5rrds/lets_move_by_step/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5l6t90",
          "author": "No-Exchange2961",
          "text": "Linkedin!!!",
          "score": 2,
          "created_utc": "2026-02-15 22:50:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l7dh3",
          "author": "Acceptable-Sea-3248",
          "text": "LinkedIn",
          "score": 2,
          "created_utc": "2026-02-15 22:53:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lw6n6",
          "author": "Mysterious_Tip_6793",
          "text": "LinkedIn and Reddit",
          "score": 1,
          "created_utc": "2026-02-16 01:20:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m5c12",
          "author": "lechiffreqc",
          "text": "CRA: Want to maintain auth for allowing automating tax filling \nRevenu Quebec: Same as CRA\nTD: Same, but also automating downloading transactions \nAll other canadian banks for same reason",
          "score": 1,
          "created_utc": "2026-02-16 02:19:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5ca6r",
      "title": "Pull URL with WebScraper",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r5ca6r/pull_url_with_webscraper/",
      "author": "HyperfocusedSoul",
      "created_utc": "2026-02-15 11:22:49",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 0.81,
      "text": "Hi all,\n\n  \nI will openly admit that I am not code-savvy... that said, I need some help.\n\nI am trying to use WebScraper to pull data from multiple pages linked to a starting page. Basically, it is designed to start at a particular website's history (the starting page), and click through/scrape the listed data from every \"new\" page linked in the history. It is doing that fine, but I also want to grab the URL associated with each new entry/page. While it is grabbing the Starting URL, it is not grabbing the individual URLs for each page it is going through. For the life of me, I can not figure out the correct Type and Selector. Does anyone have any tips or advice?\n\n  \nP.s. I would want to add this to an existing Parent Selector that already has all my other selectors/data points as subs...",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r5ca6r/pull_url_with_webscraper/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5hwz3v",
          "author": "Equivalent-Brain-234",
          "text": "Which web scraper are you using? Is it custom built?",
          "score": 1,
          "created_utc": "2026-02-15 12:26:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i48du",
          "author": "akashpanda29",
          "text": "What is the website you are trying to scrape and what's your setup ? \nI can help you",
          "score": 1,
          "created_utc": "2026-02-15 13:20:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5lbyx",
      "title": "How does strict traffic budgeting affect scraping efficiency?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1r5lbyx/how_does_strict_traffic_budgeting_affect_scraping/",
      "author": "Intelligent-Lab6132",
      "created_utc": "2026-02-15 18:02:47",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Iâ€™ve been experimenting with a constrained traffic setup for a small scraping project â€” instead of having unlimited proxy rotation, I forced myself to work within a fixed daily traffic budget.\n\nInterestingly, this constraint changed how I approached:\n\n* request pacing\n* session reuse\n* retry logic\n* concurrency tuning\n\nBy optimizing around efficiency per successful request rather than raw volume, I actually saw more stable success rates than I expected.\n\nIt made me wonder:\n\nDo we sometimes over-rotate IPs when smarter request control would perform better?\n\nCurious how others optimize when bandwidth or IP pool size is limited.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1r5lbyx/how_does_strict_traffic_budgeting_affect_scraping/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o5p58aq",
          "author": "hasdata_com",
          "text": "Nobody rotates IPs for fun. It's usually a last resort when everything else fails",
          "score": 5,
          "created_utc": "2026-02-16 15:37:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}