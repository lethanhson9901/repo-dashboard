{
  "metadata": {
    "last_updated": "2026-01-19 02:39:57",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 12,
    "total_comments": 28,
    "file_size_bytes": 48809
  },
  "items": [
    {
      "id": "1qb51ih",
      "title": "I created an open-source toolkit to make your scraper suffer",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qb51ih/i_created_an_opensource_toolkit_to_make_your/",
      "author": "niiotyo",
      "created_utc": "2026-01-12 19:45:04",
      "score": 43,
      "num_comments": 9,
      "upvote_ratio": 0.98,
      "text": "Hey everyone. I am the owner of a small web crawler API.\n\nWhen testing my crawler, I needed a dummy website with many edge cases, different HTTP status codes and tricky scenarios. Something like a toolkit for scraper testing.\n\nI used [**httpstat.us**](http://httpstat.us) before, but it has been down for a while. So I decided to build my own tool.\n\nI created a free, open-source website for this purpose: [**https://crawllab.dev**](https://crawllab.dev)\n\nIt includes:\n\n* All common HTTP status codes\n* Different content types\n* Redirect loops\n* JS rendering\n* PDFs\n* Large responses\n* Empty responses\n* Random content\n* Custom headers\n\nI hope you find it as useful as I do. Feel free to add more weird cases at [https://github.com/webcrawlerapi/crawl-lab](https://github.com/webcrawlerapi/crawl-lab)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qb51ih/i_created_an_opensource_toolkit_to_make_your/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nz8jiwv",
          "author": "99ducks",
          "text": "Time to turn it into a capture the flag challenge kind of like the [Bandit wargame](https://overthewire.org/wargames/bandit/). Users would have to build a scraper, adding a new challenge on each level to get the url to scrape for the next level.",
          "score": 10,
          "created_utc": "2026-01-12 21:32:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzb724f",
              "author": "niiotyo",
              "text": "Will add captcha",
              "score": 2,
              "created_utc": "2026-01-13 06:34:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzc0cap",
          "author": "Independent_Pop_5596",
          "text": "Interesting",
          "score": 2,
          "created_utc": "2026-01-13 11:05:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmecr6",
          "author": "fourhoarsemen",
          "text": "Pretty cool! I'll definitely test this with a new project I've been working on: [wxpath](https://github.com/rodricios/wxpath), a declarative web crawler/scraper that extends XPath semantics.",
          "score": 2,
          "created_utc": "2026-01-14 22:11:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp16tj",
              "author": "niiotyo",
              "text": "Want to add a page with advanced DOM?",
              "score": 1,
              "created_utc": "2026-01-15 08:16:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzukui6",
                  "author": "fourhoarsemen",
                  "text": "By \"advanced DOM\", do you mean dynamically generated pages/content (requiring JS rendering)?",
                  "score": 2,
                  "created_utc": "2026-01-16 02:38:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qd4eig",
      "title": "[Open Source] CLI to inject local cookies for auth",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qd4eig/open_source_cli_to_inject_local_cookies_for_auth/",
      "author": "Bubbly_Gap6378",
      "created_utc": "2026-01-15 00:22:45",
      "score": 13,
      "num_comments": 11,
      "upvote_ratio": 0.88,
      "text": "I've been building scrapers for a while, and the biggest pain point is always the login flow. If I try to automate the login with Selenium or Playwright, I hit 2FA, Captchas, or \"Suspicious Activity\" blocks immediately.\n\nI realized the easiest way around this is to stop trying to automate the login and just reuse the valid session I already have on my local Chrome browser.\n\nI wrote a Python CLI tool (Romek) to handle the extraction.\n\nHow it works under the hood:\n\n1. It locates the local Chrome Cookies SQLite database on your machine.\n2. It decrypts the cookies using the OS-specific master key (DPAPI on Windows, AES on Mac/Linux).\n3. It exports them into a JSON format that Playwright/Selenium can read.\n\nWhy I made it:\n\nI needed to run agents on a headless VPS that could access my accounts on complex sites without triggering the \"New Device\" login flow. By injecting the \"High Trust\" cookies from my main profile, the headless browser looks like my desktop.\n\nThe Tool:\n\nIt's 100% Open Source (MIT) and free.\n\nRepo:[https://github.com/jacobgadek/romek](https://github.com/jacobgadek/romek)\n\nPyPI: `pip install romek`\n\nHopefully, this saves someone else from writing another broken login script.",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1qd4eig/open_source_cli_to_inject_local_cookies_for_auth/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nznr4wm",
          "author": "matty_fu",
          "text": "What is cloud vault sync and will it be free?",
          "score": 1,
          "created_utc": "2026-01-15 02:35:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nznumii",
              "author": "Bubbly_Gap6378",
              "text": "Hey! Great question.\n\nRomek is designed to be **100% local-first.** The 'vault' is just a standard SQLite file created on your local disk (encrypted with your OS keychain).\n\nThe 'sync' logic I'm building is for moving that file between your own machines (e.g., Laptop -> VPS) via your own infrastructure (SSH/S3), not through a proprietary cloud I own.\n\nThe goal is to make a standalone utility (like `curl` or `jq`) for auth, not a subscription service. It will remain open source and free.",
              "score": 2,
              "created_utc": "2026-01-15 02:56:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzsaopj",
          "author": "Illustrious_Dark9449",
          "text": "Great work, have been looking for a solution to this flow!",
          "score": 1,
          "created_utc": "2026-01-15 19:40:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzthzew",
          "author": "andreaswpv",
          "text": "404 Error on the link",
          "score": 1,
          "created_utc": "2026-01-15 23:05:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzupvff",
          "author": "TobiasMcTelson",
          "text": "404",
          "score": 1,
          "created_utc": "2026-01-16 03:06:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx8rv6",
              "author": "Bubbly_Gap6378",
              "text": "Yeah, sorry about that 404. GitHub flagged the repo last night because the cookie decryption was a bit too 'grey hat' for their compliance filters.\n\nI‚Äôm re-architecting the auth stuff to be compliant, but in the meantime, I pivoted to releasing the safety infrastructure first.\n\nI just shipped **Vallignus** (a local Firewall for Agents) to PyPI today. It stops agents from spiraling or hitting bad URLs.\n\nYou can check it out here:[https://pypi.org/project/vallignus/](https://www.google.com/search?q=https://pypi.org/project/vallignus/)\n\n(The cookie/auth features will be added back as a safe module in v0.2.0 next week).",
              "score": 1,
              "created_utc": "2026-01-16 14:14:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzz0mbv",
                  "author": "tocarbajal",
                  "text": "And you have plans to release the main repo soon?",
                  "score": 1,
                  "created_utc": "2026-01-16 19:02:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzoylwo",
          "author": "TorZidan",
          "text": "Inject local cookies? Inject them with what? Eclairs re being filled with creme through a hole in the puffy pastry which you may consider being an injection, but I've never heard of this.",
          "score": 1,
          "created_utc": "2026-01-15 07:52:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcehik",
      "title": "Looking for some help.",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qcehik/looking_for_some_help/",
      "author": "nawakilla",
      "created_utc": "2026-01-14 05:04:50",
      "score": 11,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "My apologies, i honestly don't know if I'm even in the right place. But to put it as short as possible. \n\nI'm looking to \"clone\" a website? I found a site that has a digital user manual for a kind of rare cnc machine. However I'm paranoid that either the user or the site will take it down/ cease to exist (this has happened multiple times in the past. \n\nWhat I'm looking for: i want to be able to save the web pages locally on my computer. Then be able to open it up and use the site as if i would online. The basic site structure is 1 large image (a picture of the components), with maybe a dozen or so clickable parts. When you click it takes you to a page with a few more detailed pictures of the part with text instructions of basic repair and maintenance. \n\nIs it possible to do? I would like a better/ higher quality way to do this instead of screenshoting one by one. Is this isn't web scrapping, can someone tell me what it might be called so i can start googling?",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qcehik/looking_for_some_help/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nzhn1au",
          "author": "HLCYSWAP",
          "text": "replace [example.com](http://example.com) with your intended target and paste this into cmd/bash/terminal:\n\n    wget --mirror \\\n         --page-requisites \\\n         --adjust-extension \\\n         --convert-links \\\n         --no-parent \\\n         --wait=1 \\\n         --random-wait \\\n         --limit-rate=200k \\\n         --tries=3 \\\n         --user-agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\" \\\n         --reject-regex=\"(login|signin|register|signup)\" \\\n         -e robots=off \\\n         -o wget.log \\\n         https://example.com\n\n\\--mirror -  recursive downloading with infinite depth, downloads the entire site\n\n\\--page-requisites - downloads all the assets needed to display pages properly: images, CSS files, JavaScript files, etc\n\n\\--adjust-extension - adds .html extension to files that don't have one but are HTML\n\n\\--convert-links - rewrites all links in downloaded HTML to point to local files instead of the original URLs, so everything works offline\n\n\\--no-parent - doesn't download anything from parent directories. keeps the download limited to the specific path you specify and below",
          "score": 13,
          "created_utc": "2026-01-14 05:16:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nziydie",
              "author": "Infamous_Land_1220",
              "text": "Goat here",
              "score": 3,
              "created_utc": "2026-01-14 12:12:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzipw9q",
          "author": "99ducks",
          "text": "Have you looked to see if the site is available on archive.org? If it is, you don't have to worry about it disappearing.",
          "score": 4,
          "created_utc": "2026-01-14 11:05:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjhtr5",
              "author": "greg-randall",
              "text": "THIS!\n\nGo to [https://archive.org/](https://archive.org/) and paste in the URL in the Wayback Machine, see if what you want is there, if it is great! \n\nIf it isn't fill the URL in the bottom right box 'Save Page Now' https://web.archive.org/. Click on the dozen clickable parts and save those URLs too!",
              "score": 3,
              "created_utc": "2026-01-14 14:10:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzhs29o",
          "author": "pesta007",
          "text": "I've done a similar project before, and yes you are in the right place. What I did is  scrape all the data of the site (the hard part), recreated their front-end (pretty easy really). Then I developed a backend application to serve the data, and connected it to the front end.\n\nAs you can see, it is a lot of work. But I was willing to do it because the site was simple and I was young and passionate.",
          "score": 1,
          "created_utc": "2026-01-14 05:55:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhw0kj",
              "author": "nawakilla",
              "text": "What do you think would be the best approach? I build a handful of computers so I'm not completely tech illiterate. But I've never once done any kind of coding.",
              "score": 2,
              "created_utc": "2026-01-14 06:27:57",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzhwpyd",
              "author": "pesta007",
              "text": "If you are not a web developer, I don't think my approach would work for you. but there is still tools to capture static images of webpages. \n\nCheck out this post\n\nhttps://www.reddit.com/r/DataHoarder/s/ZAwrQ13Ng5",
              "score": 2,
              "created_utc": "2026-01-14 06:33:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzjrbgj",
          "author": "hasdata_com",
          "text": "Check out HTTrack or smth similar. It's a free, old-school software)",
          "score": 1,
          "created_utc": "2026-01-14 15:00:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcnowm",
      "title": "Async web scraping framework on top of Rust",
      "subreddit": "webscraping",
      "url": "https://github.com/BitingSnakes/silkworm",
      "author": "yehors",
      "created_utc": "2026-01-14 13:47:08",
      "score": 10,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qcnowm/async_web_scraping_framework_on_top_of_rust/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "nzjny11",
          "author": "jwrzyte",
          "text": "this sounds cool thanks for sharing, I'll give it a go when i get a chance",
          "score": 1,
          "created_utc": "2026-01-14 14:43:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzk6p1a",
              "author": "yehors",
              "text": "With disabled GIL, it gives around 230 requests per second. So highly useful when you do a lot of parsing.",
              "score": 1,
              "created_utc": "2026-01-14 16:12:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qem7fs",
      "title": "Help on how to go about scraping faculty directory profiles",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qem7fs/help_on_how_to_go_about_scraping_faculty/",
      "author": "gatherer_benefactor",
      "created_utc": "2026-01-16 17:24:19",
      "score": 9,
      "num_comments": 1,
      "upvote_ratio": 0.92,
      "text": "Hi everyone,\n\nI‚Äôm working on a research project that requires building a large-scale dataset of faculty profiles from 200 to 250 business schools worldwide. For each school, I need to collect faculty-level data such as: name, title or role, department, short bio or research interests, sometimes email, CV links, publications. The aim is to systematically scrap faculty directories across many heterogeneous university websites. My current setup is like this: Python, Selenium, BeautifulSoup, MongoDB for storage (timestamped entries to allow longitudinal tracking), one scraper per university (100 already written. I do this with the following workflow: manually inspect the faculty directory, write Selenium logic to collect profile URLs, visit each profile and extract fields with BeautifulSoup and then store the data in mongodb. \n\nThis works, but clearly does not scale well to 200 sites, especially long-term maintenance when sites change structure. What I‚Äôm unsure about and looking for advice on is the architecture for automation. Is ‚Äúone scraper per site‚Äù inevitable at this scale? Any recommendations for organizing scrapers so maintenance doesn‚Äôt become a nightmare? What are your toughts or experiences using LLMs to analyze a directory HTML, suggest Selenium actions (pagination, buttons), infer selectors?\n\nBasically my question is what you would do differently if you had to do this again for an academic project with transparency/reproducibility constraints, how would you approach it? I‚Äôm not looking for copy-paste code, more design advice, war stories, or tooling suggestions.\n\nThanks a lot, happy to clarify details if useful!",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qem7fs/help_on_how_to_go_about_scraping_faculty/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o014rjv",
          "author": "Persian_Cat_0702",
          "text": "I'd use Langchain if I wanted to scale",
          "score": 2,
          "created_utc": "2026-01-17 01:32:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qb3fth",
      "title": "chromeheadless vs creepJS",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qb3fth/chromeheadless_vs_creepjs/",
      "author": "bluemangodub",
      "created_utc": "2026-01-12 18:48:22",
      "score": 8,
      "num_comments": 15,
      "upvote_ratio": 0.91,
      "text": "Been trying to get chromeheadless better at anti bot detection evasions.\n\n\nCreepJS: https://abrahamjuliot.github.io/creepjs/  however still shows for \"like headless\" checks:\n\n*    noTaskbar: true\n*    noContentIndex: true\n*    noContactsManager: true\n*    noDownlinkMax: true\n\n\n\nNot much info on this that I can find. The \"headless\" check is 0% but this \"like headless\" is at 31%.\n\n\nSimilar note, trying this site: https://fingerprint-scan.com/ which gives me 50% (edit is today showing 55%)  chance of being a  bot.\n\n\nAnyone know any techniques  / things to look into I can do to improve this?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qb3fth/chromeheadless_vs_creepjs/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nz7wyah",
          "author": "thePsychonautDad",
          "text": "I got a score of 25 on https://fingerprint-scan.com/ from my prod scraper nodes, which are not headless.\n\nCan't get a score below 50 on headless.\n\nThe 25 score uses Chrome + CDP + xdotool on mini-PCs running Ubuntu with a HDMI dummy to emulate the screen.\nI have a fleet of a dozen of those mini-PC, with code distributing the jobs.",
          "score": 5,
          "created_utc": "2026-01-12 19:47:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9llv3",
              "author": "joeyx22lm",
              "text": "Why not xvfb?",
              "score": 1,
              "created_utc": "2026-01-13 00:46:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz9np90",
                  "author": "thePsychonautDad",
                  "text": "We tried it and got detected pretty damn fast. Too many fingerprints that Meta detects instantly.\n\nWith CDP it's running smoothly for months without ever an issue.\nBut it only works with a tool like xdotool, click & keypress emulation gets the accounts blocked within days.",
                  "score": 2,
                  "created_utc": "2026-01-13 00:58:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzd7n7q",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-13 15:30:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzdbssg",
                  "author": "joeyx22lm",
                  "text": "Playwright can target a remote CDP endpoint to control a browser.",
                  "score": 1,
                  "created_utc": "2026-01-13 15:49:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzdf43b",
                  "author": "webscraping-ModTeam",
                  "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
                  "score": 1,
                  "created_utc": "2026-01-13 16:04:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzfsgw7",
              "author": "bluemangodub",
              "text": "*reposting as deleted as mentioned a paid for product (reference now removed)*\n\n> Can't get a score below 50 on headless.\n\nsame. But for this project I need headless.  \n\nDo you not have trouble being on linux? Do you keep linux / debian useragent? \n\n\n> Chrome + CDP\n\nWhy CDP? Why not playwright or puppeteer? Any issues from using CDP? In order to get my score to what it currently is, I had to use a CDP command within puppeteer.  I always had it in the back of mind using CDP was detectable?",
              "score": 1,
              "created_utc": "2026-01-13 22:49:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzlrngo",
                  "author": "thePsychonautDad",
                  "text": "Being on Linux isn't an issue.\nThat's the OS I use on my own computer too, it's a legitimate & valid OS. Just use a common distro rather than one meant for servers. We use Ubuntu 24.04\n\nWhy CDP rather than playwright or others: It uses the same chrome a user would use, with a real profile that has its own history & cookies. No automation fingerprints. Why not headless: So the cursor moves for real, clicks for real, the keyboard events are real keypress/keydown. It triggers all the right tracking on the analytics. The cursor moves like a human would move, it's noisy, it's curving and retracing rather that following a straight line. When it types, it has random timing between keypress,, it makes typo that it corrects with backspace from time to time, ... We make it really hard for system to detect us as a bot. With headless you have to emulate the keyboard & mouse events, you don't trigger any of the cursor tracking, you just advertise yourself as a bot.\n\nIt's slower (which is why we have a lot of those computers sharing the load) but it's nearly undetectable and works reliably long term.\n\nWe also make the profiles have work hours, so they \"sleep\" for 8h every day, adding to the act that this is controlled by a human and not a bot.\n\nWe don't even use VPNs, we don't rotate the IPs. It all run a few household internet plans.",
                  "score": 1,
                  "created_utc": "2026-01-14 20:28:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o09qjqj",
              "author": "svearige",
              "text": "This is very close to what I‚Äôm doing. Not that advanced though, but almost.",
              "score": 1,
              "created_utc": "2026-01-18 10:32:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz7vd3r",
          "author": "Twitty-slapping",
          "text": "I don't know, butI  am interested in the process can you keep updating us",
          "score": 1,
          "created_utc": "2026-01-12 19:39:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfm54r",
      "title": "pypecdp - a fully async python driver for chrome using pipes",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qfm54r/pypecdp_a_fully_async_python_driver_for_chrome/",
      "author": "sohaib0717",
      "created_utc": "2026-01-17 19:17:57",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.85,
      "text": "Hey everyone. I built a fully asynchronous chrome driver in Python using POSIX pipes. Instead of websockets, it uses file descriptors to connect to the browser.\n\n* Directly connects and controls the browser, no middleware\n* 100% asynchronous, nothing gets blocked\n* Built completely using built-in Python asyncio \n   * Except one `deprecated` dependency for python-cdp modules\n* Best for running multiple browsers on same machine\n* No risk of zombie chromes if code crashes\n* Easy customization via class inheritance\n* No automation signatures as there is no framework in between\n\nCurrently limited to POSIX based systems only (Linux/Mac).\n\n\n\nBug reports, feature requests and contributions are welcome!\n\n\n\n[https://github.com/sohaib17/pypecdp](https://github.com/sohaib17/pypecdp)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qfm54r/pypecdp_a_fully_async_python_driver_for_chrome/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qf8fvt",
      "title": "Blocked by Cloudflare despite using curl_cffi",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qf8fvt/blocked_by_cloudflare_despite_using_curl_cffi/",
      "author": "Coding-Doctor-Omar",
      "created_utc": "2026-01-17 08:59:39",
      "score": 6,
      "num_comments": 9,
      "upvote_ratio": 0.69,
      "text": "EDIT: IT FINALLY WORKED! I just had to add the content-type, origin, and referer headers.\n\nPlease help me access this API efficiently.\n\nI am trying to access this API:\n\nhttps://multichain-api.birdeye.so/solana/v3/gems\n\nI am using impersonate and the correct payload for the post request, but I keep getting 403 status code.\n\nThe only way I was able to get the data was use a Python browser automation library, go to the normal web page, and intercept this API's response using a handler (essentially automating the network tab inspection using Python), but this method is very inefficient. Below is my curl_cffi code.\n\n\n```\nfrom curl_cffi import Session\n\n\napi_url = \"https://multichain-api.birdeye.so/solana/v3/gems\"\npayload = {\"limit\":100,\"offset\":0,\"filters\":[],\"shown_time_frame\":\"4h\",\"type\":\"trending\",\"sort_by\":\"price\",\"sort_type\":\"desc\"}\n\nwith Session(impersonate=\"edge\") as session:\n    session.get(\"https://birdeye.so/solana/find-gems\")\n    res = session.post(api_url, data=payload)\n    print(res.status_code)\n```\n\nOutput:\n\n```\n403\n```",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qf8fvt/blocked_by_cloudflare_despite_using_curl_cffi/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o034ekq",
          "author": "expiredUserAddress",
          "text": "I see you've no proxy in use. Use a proxy everytime you're scrapping something",
          "score": 2,
          "created_utc": "2026-01-17 10:55:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o035s55",
              "author": "Coding-Doctor-Omar",
              "text": "I actually have just gotten it to work. The issue was simpler than I thought. I had to provide content-type, origin, and referer values in my headers, in addition to the default headers of impersonate.",
              "score": 3,
              "created_utc": "2026-01-17 11:07:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o035mw0",
          "author": "Coding-Doctor-Omar",
          "text": "It turns out I had to add some extra headers. Here is the working code:\n\n```\nfrom curl_cffi import Session\n\n\napi_url = \"https://multichain-api.birdeye.so/solana/v3/gems\"\npayload = {\"limit\":100,\"offset\":0,\"filters\":[],\"shown_time_frame\":\"4h\",\"type\":\"trending\",\"sort_by\":\"price\",\"sort_type\":\"desc\"}\n\nheaders = {\n    \"content-type\": \"application/json\",\n    \"origin\": \"https://birdeye.so\",\n    \"referer\": \"https://birdeye.so/\"\n}\n\nwith Session(impersonate=\"edge\", headers=headers) as session:\n    res = session.post(api_url, json=payload)\n    print(res.status_code)\n```\n\nOutput:\n\n```\n200\n```",
          "score": 2,
          "created_utc": "2026-01-17 11:06:28",
          "is_submitter": true,
          "replies": [
            {
              "id": "o03du94",
              "author": "abdullah-shaheer",
              "text": "Yes this is actually. And if the content is in html form, you will have the need to use the content type header accordingly.",
              "score": 1,
              "created_utc": "2026-01-17 12:17:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o03dpxf",
          "author": "abdullah-shaheer",
          "text": "If it's in the json format, then you need to set content header to be in json; I don't remember the exact header, you can search",
          "score": 1,
          "created_utc": "2026-01-17 12:16:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0b106q",
          "author": "BeforeICry",
          "text": "Cloudflare typically renders the turnstile captcha even for legit browser requests. That's more like a feature of your target. In these cases, you have to resort to browser + captcha solving.",
          "score": 1,
          "created_utc": "2026-01-18 15:43:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0b5w5l",
              "author": "Coding-Doctor-Omar",
              "text": "I eventually got it to work by providing the content-type, origin, and referer values in the headers, in addition to the default headers provided by impersonate.",
              "score": 1,
              "created_utc": "2026-01-18 16:06:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ckfum",
          "author": "Alternative-842",
          "text": "yo man i had the same issue, Cloudflare just blocks normal requests if u dont send all the headers like content-type origin n referer, even if ur payload is right. i ended up using a headless browser too, way slow tho. u might try adding all the headers exactly like the site does n maybe rotate user agents, that helped me a bit. sometimes curl alone just dont cut it lol",
          "score": 1,
          "created_utc": "2026-01-18 20:05:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcume9",
      "title": "Built a scraper where crawling/scraping is one XPath expression",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qcume9/built_a_scraper_where_crawlingscraping_is_one/",
      "author": "fourhoarsemen",
      "created_utc": "2026-01-14 18:09:58",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.7,
      "text": "This is wxpath's first public release, and I'd love feedback on the expression syntax, any use cases this might unlock, or anything else. \n\n\n**[wxpath](https://github.com/rodricios/wxpath)** is a declarative web crawler where traversal is expressed directly in XPath. Instead of writing imperative crawl loops, wxpath lets you describe what to follow and what to extract in a single expression (it's async under the hood; results are streamed as they‚Äôre discovered). \n\nBy introducing the `url(...)` operator and the `///` syntax, wxpath's engine can perform deep/recursive web crawling and extraction.\n\nFor example, to build a simple Wikipedia knowledge graph: \n\n\n    import wxpath\n\n    path_expr = \"\"\"\n    url('https://en.wikipedia.org/wiki/Expression_language')\n     ///url(//main//a/@href[starts-with(., '/wiki/') and not(contains(., ':'))])\n     /map{\n        'title': (//span[contains(@class, \"mw-page-title-main\")]/text())[1] ! string(.),\n        'url': string(base-uri(.)),\n        'short_description': //div[contains(@class, 'shortdescription')]/text() ! string(.),\n        'forward_links': //div[@id=\"mw-content-text\"]//a/@href ! string(.)\n     }\n    \"\"\"\n\n    for item in wxpath.wxpath_async_blocking_iter(path_expr, max_depth=1):\n        print(item)\n\n\nOutput:\n\n\n    map{'title': 'Computer language', 'url': 'https://en.wikipedia.org/wiki/Computer_language', 'short_description': 'Formal language for communicating with a computer', 'forward_links': ['/wiki/Formal_language', '/wiki/Communication', ...]}\n    map{'title': 'Advanced Boolean Expression Language', 'url': 'https://en.wikipedia.org/wiki/Advanced_Boolean_Expression_Language', 'short_description': 'Hardware description language and software', 'forward_links': ['/wiki/File:ABEL_HDL_example_SN74162.png', '/wiki/Hardware_description_language', ...]}\n    map{'title': 'Machine-readable medium and data', 'url': 'https://en.wikipedia.org/wiki/Machine_readable', 'short_description': 'Medium capable of storing data in a format readable by a machine', 'forward_links': ['/wiki/File:EAN-13-ISBN-13.svg', '/wiki/ISBN', ...]}\n    ...\n\n---\n\nThe target audience is anyone who: \n\n1. wants to quickly prototype and build web scrapers\n2. familiar with XPath or data selectors\n3. builds datasets (think RAG, data hoarding, etc.)\n4. wants to study link structure of the web (quickly) i.e. web network scientists\n\n---\n\nFor comparison, with Scrapy, you would...\n\n\n    import scrapy\n\n    class QuotesSpider(scrapy.Spider):\n        name = \"quotes\"\n        start_urls = [\n            \"https://quotes.toscrape.com/tag/humor/\",\n        ]\n\n        def parse(self, response):\n            for quote in response.css(\"div.quote\"):\n                yield {\n                    \"author\": quote.xpath(\"span/small/text()\").get(),\n                    \"text\": quote.css(\"span.text::text\").get(),\n                }\n\n            next_page = response.css('li.next a::attr(\"href\")').get()\n            if next_page is not None:\n                yield response.follow(next_page, self.parse)\n\n\nThen from the command line, you would run:\n\n\n    scrapy runspider quotes_spider.py -o quotes.jsonl\n\n\n**wxpath** gives you two options: write directly from a Python script or from the command line.\n\n\n    from wxpath import wxpath_async_blocking_iter \n    from wxpath.hooks import registry, builtin\n\n    path_expr = \"\"\"\n    url('https://quotes.toscrape.com/tag/humor/', follow=//li[@class='next']/a/@href)\n      //div[@class='quote']\n        /map{\n          'author': (./span/small/text())[1],\n          'text': (./span[@class='text']/text())[1]\n          }\n\n\n    registry.register(builtin.JSONLWriter(path='quotes.jsonl'))\n    items = list(wxpath_async_blocking_iter(path_expr, max_depth=3))\n\n\nor from the command line:\n\n\n    wxpath --depth 1 \"\\\n    url('https://quotes.toscrape.com/tag/humor/', follow=//li[@class='next']/a/@href) \\\n      //div[@class='quote'] \\\n        /map{ \\\n          'author': (./span/small/text())[1], \\\n          'text': (./span[@class='text']/text())[1] \\\n          }\" > quotes.jsonl\n\n\n\n---\n\n\n\nGitHub: https://github.com/rodricios/wxpath\n\nPyPI: pip install wxpath",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qcume9/built_a_scraper_where_crawlingscraping_is_one/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qfigyu",
      "title": "Open Source Captcha to test scraping methods against",
      "subreddit": "webscraping",
      "url": "https://github.com/WebDecoy/FCaptcha",
      "author": "cport1",
      "created_utc": "2026-01-17 17:00:15",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.71,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1qfigyu/open_source_captcha_to_test_scraping_methods/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qddw3x",
      "title": "How do I scrape images from a website with server restrictions?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qddw3x/how_do_i_scrape_images_from_a_website_with_server/",
      "author": "MotivatedMommy",
      "created_utc": "2026-01-15 08:11:50",
      "score": 3,
      "num_comments": 6,
      "upvote_ratio": 0.64,
      "text": "My earlier post got removed when I mentioned a bunch of the steps I've tried because it included names of paid services. I'm going to rephrase and hopefully it will make sense.\n\nThere's a site that I want to scrape an image from. I'm starting with just one image so I don't have to worry about staggering call times. Anyway, when I manually inspect the image element in the browser, and then I click on the image source, I get a \"Referral Denied\" error saying \"you don't have permission to access ____ on this server\". I don't even know how to get the image manually, so I'm not sure how to get it with the scraper.\n\nI've been using a node library that starts with puppet, but I've also been using one that plays wright. Whenever I call \"await fetch()\", I get the error \"network response was not ok\". I've tried changing the user agent, adding extra http headers, and intercepting the request, but I still get the same error. I assume I'm not able to get the image because I'm not calling from that site directly, but since I can see the image on the page, I figure there has to be a way to save it somehow.\n\nI'm new to scraping, so I apologize if this sort of thing was asked before. No matter what I searched for, I couldn't find an answer that worked for me. Any advice is much appreciated ",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qddw3x/how_do_i_scrape_images_from_a_website_with_server/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nzpu8wu",
          "author": "Hour_Analyst_7765",
          "text": "This could possibly be due to multiple reasons.\n\nIt could be as simple as requiring to send a \"Referer\" header in your request to grab the image. Its probably their CDN whitelisted only certain domains that belong to the site. I hope this fixes it for you.\n\nHowever, a more advanced protection I've seen, is a system where the website served images with an one-time token in the URL. Those got consumed while loading the full page in the browser, so even if you send the 'correct' request it would still fail. I verified this by disabling images with uBlock origin, and then manually loading an image. This image loaded once, but after a refresh it failed. This confirmed for me that I could still scrape these, but I would have to load the article page and images from the exact same browser session.\n\nA mild annoyance was that these one-time use tokens meant the URL changes on each request, so you need to find your own labeling system to deduplicate them.",
          "score": 3,
          "created_utc": "2026-01-15 12:32:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqdgnk",
          "author": "domharvest",
          "text": "This is a classic **referrer protection** issue - the server is checking where the request is coming from and blocking direct image access. Here's how to handle it in plain JavaScript\n\nInstead of trying to `fetch()` the image URL directly, use Playwright to screenshot or download it **while you're on the page**:\n\n        // Method 1: Screenshot the image element\n        const imageElement = await page.locator('img[src*=\"whatever\"]');\n        await imageElement.screenshot({ path: 'image.png' });\n        \n        // Method 2: Get image as buffer and save\n        const image = await page.locator('img').first();\n        const buffer = await image.screenshot();\n        await fs.writeFile('image.png', buffer);\n        \n        // Method 3: Use CDP (Chrome DevTools Protocol) to intercept the actual image data\n        await page.route('**/*.{png,jpg,jpeg,gif,webp}', async route => {\n          const response = await route.fetch();\n          const buffer = await response.body();\n          await fs.writeFile('image.png', buffer);\n          await route.continue();\n        });",
          "score": 3,
          "created_utc": "2026-01-15 14:24:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpqmjl",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-15 12:07:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpwf8d",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-15 12:47:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzq6v1j",
          "author": "Coding-Doctor-Omar",
          "text": "Try checking the network tab. Chances are there are requests being made to some API to fetch those images.",
          "score": 1,
          "created_utc": "2026-01-15 13:49:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbqooc",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qbqooc/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-01-13 13:00:51",
      "score": 3,
      "num_comments": 5,
      "upvote_ratio": 0.81,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levels‚Äîwhether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) üå±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1qbqooc/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "nzcoqu8",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-13 13:54:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzcv4cb",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-01-13 14:27:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nze7qd6",
          "author": "psychic_shadow_lugia",
          "text": "Hello all,\n\nI need to get data on about 25000 linkedin profiles. The main thing I want it the education section of each profile (all schools as well as start and end date). But if I can get the full profile info it is even better\n\nI understand scraping linkedin is tricky.\n\nI looked into\n\nscraptable - but it is unable at getting the years\n\nlinkedapi - but the call limit (profile per minute) goes down as you use your credit, which eventually result in about 7 profiles per minute.\n\nI am ok paying for a service as well as coding one myself, and would love some ideas from past experience",
          "score": 1,
          "created_utc": "2026-01-13 18:26:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwmcwy",
              "author": "domharvest",
              "text": "LinkedIN ToS",
              "score": 1,
              "created_utc": "2026-01-16 11:59:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzsylu2",
          "author": "OtherwiseGroup3162",
          "text": "Web Scraping TOS\n\nin a site's terms of service, it says no web scraping. \n\nWould that mean both browser based RPA and Back end HTTP requests that were reverse engineered be included?",
          "score": 1,
          "created_utc": "2026-01-15 21:31:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}