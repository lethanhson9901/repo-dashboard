{
  "metadata": {
    "last_updated": "2026-02-02 09:09:57",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 13,
    "total_comments": 59,
    "file_size_bytes": 64052
  },
  "items": [
    {
      "id": "1qs66k0",
      "title": "Couldn't find proxy directory with filters so built one",
      "subreddit": "webscraping",
      "url": "https://i.redd.it/xo702jo5kpgg1.png",
      "author": "Consistent-Feed-7323",
      "created_utc": "2026-01-31 16:04:15",
      "score": 26,
      "num_comments": 28,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qs66k0/couldnt_find_proxy_directory_with_filters_so/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2uiaup",
          "author": "Important-Seat-1882",
          "text": "Do you have here information on whether proxies can be used behind login and auth? ",
          "score": 2,
          "created_utc": "2026-01-31 20:06:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2v3efn",
              "author": "Consistent-Feed-7323",
              "text": "If you mean whenever proxies auth is by IP or login;password - yeah, there's a filter for that.",
              "score": 3,
              "created_utc": "2026-01-31 21:50:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2xewcb",
                  "author": "Lemon_eats_orange",
                  "text": "Important Seat may be referring to if the IP's themselves can be used for a login use case where the data isn't public. Some providers won't allow logging into some accounts with their proxies. Could be wrong though.",
                  "score": 1,
                  "created_utc": "2026-02-01 06:17:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2wjdbx",
          "author": "scrapingtryhard",
          "text": "EPICCCCC, the footer is still 2025 though u should fix it",
          "score": 2,
          "created_utc": "2026-02-01 02:41:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wjz7n",
              "author": "Consistent-Feed-7323",
              "text": "Dear god, that's probably the last thing I would ever think of lmao. Thank you!",
              "score": 1,
              "created_utc": "2026-02-01 02:45:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ywrbm",
                  "author": "scrapingtryhard",
                  "text": "add proxyon to it lol",
                  "score": 2,
                  "created_utc": "2026-02-01 13:50:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xf4q8",
          "author": "Lemon_eats_orange",
          "text": "Overall this is pretty good! No marketing just a list of proxies and what they can be used for. And you can check their trust pilots.",
          "score": 2,
          "created_utc": "2026-02-01 06:19:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xjubp",
          "author": "SilentlySufferingZ",
          "text": "Which require KYC? Accept Bitcoin?",
          "score": 2,
          "created_utc": "2026-02-01 06:59:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ywymi",
              "author": "scrapingtryhard",
              "text": "proxyon",
              "score": 1,
              "created_utc": "2026-02-01 13:51:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zfmfm",
          "author": "zrsca",
          "text": "This is so cool! Maybe add a sort by $/GB option. Really good site üôè",
          "score": 2,
          "created_utc": "2026-02-01 15:31:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zspeq",
              "author": "Consistent-Feed-7323",
              "text": "Thank you, I will try to add something like that in the future, biggest problem is to define price, since I'm putting minimal available for purchase price and services are often providing wholesale deals",
              "score": 1,
              "created_utc": "2026-02-01 16:32:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31asxq",
                  "author": "zrsca",
                  "text": "Yeah I figured that‚Äòd make it difficult. You could add a Filter option for how many GB the user wants to buy and use the according pricing they provide. But I think that‚Äòd be a pain in the a** to implement let alone maintain.",
                  "score": 1,
                  "created_utc": "2026-02-01 20:42:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o31o11p",
          "author": "illusivejosiah",
          "text": "Really well built. I run Illusory (mobile proxy provider) and I actually found this useful. Didn't realize we're the only service with dedicated IPs, IPv6, UDP, and unlimited bandwidth all-in-one. Appreciate the insight",
          "score": 2,
          "created_utc": "2026-02-01 21:47:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32bmgj",
              "author": "Consistent-Feed-7323",
              "text": "Thank you! The idea actually came when I was helping with marker research for my friend's proxy service and there was no place with such information.",
              "score": 1,
              "created_utc": "2026-02-01 23:50:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o32rzdg",
                  "author": "illusivejosiah",
                  "text": "Makes sense. Thanks for adding us!",
                  "score": 1,
                  "created_utc": "2026-02-02 01:21:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33cdcn",
          "author": "FranBattan",
          "text": "This is gold man, really appreciate it. Thanks",
          "score": 2,
          "created_utc": "2026-02-02 03:18:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t6bbc",
          "author": "akashpanda29",
          "text": "This is nice one. Kudos dude",
          "score": 1,
          "created_utc": "2026-01-31 16:17:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2t7gdm",
              "author": "Consistent-Feed-7323",
              "text": "Thank you very much, right now doing a round of \"marketing\", than will grab some more providers and start testing them",
              "score": 3,
              "created_utc": "2026-01-31 16:23:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2t9jt4",
          "author": "Fragrant_Ad3054",
          "text": "I really like the font used.",
          "score": 1,
          "created_utc": "2026-01-31 16:33:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2tbzmo",
              "author": "Consistent-Feed-7323",
              "text": "Thank you, it's monospace, I'm used to use it in my projects although it's not the best practice and is less readable than default.",
              "score": 1,
              "created_utc": "2026-01-31 16:44:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tg2n6",
          "author": "Alarmed_Scar_925",
          "text": "Thank you!!",
          "score": 1,
          "created_utc": "2026-01-31 17:04:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2v630c",
          "author": "Sergiowild",
          "text": "Good idea! Did you put your ref links on there?",
          "score": 1,
          "created_utc": "2026-01-31 22:03:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vq8af",
              "author": "Consistent-Feed-7323",
              "text": "Thank you. Not yet, but will do in some future. Ref links are requiring registration in most services and it's such a headache.",
              "score": 1,
              "created_utc": "2026-01-31 23:51:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2xxdz5",
          "author": "Mean_Yam5488",
          "text": "Nice work on the directory. Been looking for something like this when comparing providers.\n\nOne thing that would be super helpful - a filter for whether they allow account logins. Some providers block you from logging into social media or other accounts with their IPs, which kills a lot of use cases.",
          "score": 1,
          "created_utc": "2026-02-01 09:03:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y739u",
              "author": "Consistent-Feed-7323",
              "text": "Thank you! I do have similar to this info in restrictions, but many providers are either not disclosing it or referring as \"and other restrictions\" which can't be verified until testing. It's definitely something worth a separate filter, just can't make it right now.",
              "score": 1,
              "created_utc": "2026-02-01 10:32:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qqxam9",
      "title": "Do I need a residential proxy to mass scrape menus?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qqxam9/do_i_need_a_residential_proxy_to_mass_scrape_menus/",
      "author": "z420a",
      "created_utc": "2026-01-30 05:58:34",
      "score": 15,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "I have about 30,000 restaurants for which I need to scrape their menus. As far as I know a good chunk of those use services such as uber eats, DoorDash, toasttab, etc to host their menus. \n\nIs it possible to scrape all of that with just my laptop? Or will I get IP banned?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qqxam9/do_i_need_a_residential_proxy_to_mass_scrape_menus/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2k1su0",
          "author": "Puzzleheaded_Row3877",
          "text": "with uber eats you get upto 2000 requests per hour ,I don't think the rest of the sites you mentioned care about rate limiting that much ; and if they do , mix it up by scraping some of the menus from the wayback machine.",
          "score": 9,
          "created_utc": "2026-01-30 06:08:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lxvby",
          "author": "Pooria_P",
          "text": "Don't scrape all with your laptop if you'll be doing it at once. if the websites are not sophisticated, just use the cheapest proxy you can find (typically, static datacenter, but HIGH block rates), or static residentials.  \nUber eats and etc though, I think you have a better chance with rotating residential.",
          "score": 3,
          "created_utc": "2026-01-30 14:35:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kihz0",
          "author": "Bosschopper",
          "text": "Can‚Äôt answer you question but I‚Äôm working on a similar project. Are you doing a menu database? My project is pretty far along the dev pipeline and I‚Äôm hosted on the internet (well, sorta) so if you‚Äôre open to collaborating hit me up.\n\nI‚Äôm far along with UI stuff and features but frankly have no data to feed. DM me about where you‚Äôre at too",
          "score": 2,
          "created_utc": "2026-01-30 08:28:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mbao3",
          "author": "bluemangodub",
          "text": "Do you? try it. And if you do, you will find out.\n\nThat's it. No one knows, unless they have done it. You can guess. \n\nBut typically, if you are scraping 1 site, and you want 30k pages, if you want to do it slow you might get away with it. Otherwise, proxies allow you to hit it harder.",
          "score": 2,
          "created_utc": "2026-01-30 15:39:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2k7x8c",
          "author": "bigtakeoff",
          "text": "naw... guess it depends on where the menus are at",
          "score": 1,
          "created_utc": "2026-01-30 06:56:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lncto",
          "author": "HaphazardlyOrganized",
          "text": "Probably not, just don't pound their servers. If you check like once a day at 2am you should be fine",
          "score": 1,
          "created_utc": "2026-01-30 13:41:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs08kg",
      "title": "I upgraded my YouTube data tool ‚Äî (much faster + simpler API)",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qs08kg/i_upgraded_my_youtube_data_tool_much_faster/",
      "author": "nagmee",
      "created_utc": "2026-01-31 11:43:24",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 0.81,
      "text": "A few months ago I shared my Python tool for fetching YouTube data. After feedback, I refactored everything and added some features with 2.0 version.\n\nHere's the new features:\n\n* Get structured comments alongside with transcript and metadata.\n* `ytfetcher`¬†is now fully synchronous, simplifying usage and architecture.\n* Pre-Filter videos based on metadata such as¬†`view_count`,¬†`duration`¬†and¬†`title`.\n* Fetch data with playlist id or search query to similar to Youtube Search Bar.\n* Simpler CLI usage.\n\nI also solved a very critical bug with this version which is metadata and transcripts are might not be aligned properly.\n\nI still have a lot of futures to add. So if you guys have any suggestions I'd love to hear.\n\nHere's the full changelog if you want to check;¬†\n\n[https://github.com/kaya70875/ytfetcher/releases/tag/v2.0](https://github.com/kaya70875/ytfetcher/releases/tag/v2.0)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qs08kg/i_upgraded_my_youtube_data_tool_much_faster/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qrdn9r",
      "title": "Need help",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qrdn9r/need_help/",
      "author": "imvdave",
      "created_utc": "2026-01-30 18:27:57",
      "score": 9,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "I have a list of 2M+ online stores for which I want to detect the technology.\n\n  \nI have the script, but I often face 429 errors due to many websites belonging to Shopify.\n\n  \nIs there any way to speed this up?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qrdn9r/need_help/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2nkqci",
          "author": "scraperouter-com",
          "text": "use rotating proxies",
          "score": 2,
          "created_utc": "2026-01-30 19:00:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nsf8c",
          "author": "Puzzleheaded_Row3877",
          "text": "rotate the IP's. Also organize your list so that you are not hitting shopify 50 times in a row.",
          "score": 2,
          "created_utc": "2026-01-30 19:35:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nrwgu",
          "author": "greg-randall",
          "text": "Can you do a DNS lookup on your domains and build a list of Shopify owned IPs?",
          "score": 1,
          "created_utc": "2026-01-30 19:32:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nzkav",
          "author": "NZRedditUser",
          "text": "well if you get a 429 (if you dont wanna solve the proxy issue) just check where the redirect goes if you do domain/admin if it goes -> x myshopify com then you know its shopify and can make assessments via that?",
          "score": 1,
          "created_utc": "2026-01-30 20:08:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oczsh",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-30 21:12:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2odeps",
              "author": "webscraping-ModTeam",
              "text": "üëî Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) or try your request on Fiverr or Upwork. For anything else, please contact the mod team.",
              "score": 1,
              "created_utc": "2026-01-30 21:14:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qofvmx",
      "title": "Akamai anti-bot blocking flight search scraping (403/418)",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qofvmx/akamai_antibot_blocking_flight_search_scraping/",
      "author": "Individual-Ship-7587",
      "created_utc": "2026-01-27 14:49:48",
      "score": 8,
      "num_comments": 22,
      "upvote_ratio": 0.73,
      "text": "Hi all,\n\nI‚Äôm attempting to collect public flight search data (routes, dates, mileage pricing) for personal research, at low request rates and without commercial intent.\n\nAirline websites (Azul / LATAM) consistently return 403 and 418 responses, and traffic analysis strongly suggests Akamai Bot Manager / sensor-based protection.\n\n# Environment & attempts so far\n\n* Python and Go\n* Multiple HTTP clients and browser automation frameworks\n* Headless and non-headless browsers\n* Mobile and rotating proxies\n* Header replication (UA, sec-ch-ua, accept, etc.)\n* Session persistence, realistic delays, low RPS\n\nDespite matching headers and basic browser behavior, sessions eventually fail.\n\n# Observed behavior\n\nFrom inspecting network traffic:\n\n* Initial page load sets temporary cookies\n* A follow-up request sends browser fingerprint / behavioral telemetry\n* Only after successful validation are long-lived cookies issued\n* Missing or inconsistent telemetry leads to 403/418 shortly after\n\nThis looks consistent with client-side sensor collection (JS-generated signals rather than static tokens).\n\n# Conceptual question\n\nAt this level of protection, is it generally realistic to:\n\n* Attempt to reproduce sensor payloads manually (outside a real browser), or\n* Does this usually indicate that:\n   * Traditional HTTP-level scraping is no longer viable?\n   * Only full browser execution with real user interaction scales reliably?\n   * Or that the correct approach is to seek alternative data sources (official APIs, licensed feeds, partnerships)?\n\nI‚Äôm not asking for bypass techniques or ToS violations ‚Äî I‚Äôm trying to understand where the practical boundary is for scraping when dealing with modern, behavior-based bot defenses.\n\nAny insight from people who‚Äôve dealt with Akamai or similar systems would be greatly appreciated.\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Bot detection ü§ñ",
      "permalink": "https://reddit.com/r/webscraping/comments/1qofvmx/akamai_antibot_blocking_flight_search_scraping/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o21hix7",
          "author": "army_of_wan",
          "text": "Have you tried using fire fox automation ? ( puppeteer , camafoux ?)\nHarvest cookies with a browser and then reuse them to scrape the site.",
          "score": 5,
          "created_utc": "2026-01-27 16:27:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24xvrd",
              "author": "letopeto",
              "text": "Isn‚Äôt camoufox widely outdated? Repo hasn‚Äôt been update in a while it‚Äôs still running a very old version of Firefox and last time I tried it it couldn‚Äôt even bypass a simple Cloudflare challenge",
              "score": 2,
              "created_utc": "2026-01-28 02:04:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2tzfuj",
                  "author": "Coding-Doctor-Omar",
                  "text": "Yes, it is outdated now, but its owner recently announced that it's back to development and should return back to its performance this year.",
                  "score": 1,
                  "created_utc": "2026-01-31 18:36:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o265xra",
              "author": "irrisolto",
              "text": "Not gonna work at scale",
              "score": 1,
              "created_utc": "2026-01-28 06:38:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o266nm3",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 1,
                  "created_utc": "2026-01-28 06:43:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21k1xl",
          "author": "Specialist-Egg-3720",
          "text": "once the js challenge is completed in real browser, it sets a cookie, you need to capture that cookie then move on to do your api calls, its very difficult to reproduce a sensor payload without the real browser as it checks for various things such as webgl rendering, viewport, cpu info +gpu info + screen info + other params of your pc to create a unique device specifc signature. Its good option to just solve this challenge in real browser , get the cookie then move on to http request based scraping.",
          "score": 3,
          "created_utc": "2026-01-27 16:38:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o226p1d",
          "author": "ScrapeAlchemist",
          "text": "Hey!\n\nYou've basically hit the wall where HTTP-level scraping stops being practical. Akamai Bot Manager with sensor.js is specifically designed to detect the difference between real browsers and automation.\n\nTo answer your conceptual question directly:\n\n1. **Reproducing sensor payloads manually** - theoretically possible but a nightmare to maintain. The fingerprint includes WebGL rendering, canvas, audio context, and dozens of other signals that change with Akamai updates\n\n2. **Full browser with real interaction** - this works but you need the browser to actually execute the JS and pass the challenges. The cookie harvesting approach others mentioned is the right idea\n\n3. **Alternative data sources** - for flight data specifically, Google Flights pulls from the same GDS systems. Some airlines also have affiliate/partner APIs that are less protected\n\nThe practical boundary: if a site invests in Akamai Bot Manager, they've decided to make scraping hard. You either invest equally in bypassing it (real browsers, residential IPs, proper session handling) or find a different data source.\n\nFor personal research at low volume, a real browser with residential IP and human-like delays might work. But it'll be fragile.\n\nGood luck!",
          "score": 2,
          "created_utc": "2026-01-27 18:16:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22tuoh",
          "author": "Afraid-Solid-7239",
          "text": "reply with the page, and data you want to scrape?",
          "score": 1,
          "created_utc": "2026-01-27 19:56:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27bgei",
              "author": "Individual-Ship-7587",
              "text": "The page is a public flight search results page, for example:\n\n[https://www.latamairlines.com/br/pt/oferta-voos?origin=POA&destination=SCL&outbound=2026-06-17&inbound=2026-03-26&trip=RT&cabin=Economy&redemption=true]()\n\nI‚Äôm only trying to collect the data shown to any user on that page:  \nflight options (route, dates, airline), mileage prices, taxes/fees, cabin class, and basic availability information. No booking, no account access, and no private user data.",
              "score": 2,
              "created_utc": "2026-01-28 12:29:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o265wps",
          "author": "irrisolto",
          "text": "Use a solver to get akamai cookies",
          "score": 1,
          "created_utc": "2026-01-28 06:37:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26nn9p",
              "author": "scraperouter-com",
              "text": "what solver?",
              "score": 1,
              "created_utc": "2026-01-28 09:12:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o279qaa",
                  "author": "irrisolto",
                  "text": "A solver for akamai that gives you cookies",
                  "score": 1,
                  "created_utc": "2026-01-28 12:17:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2b1zbx",
          "author": "nez1rat",
          "text": "The only way to scrape successfully the data is by using a valid TLS fingerprint with valid Akamai cookies in your request.  \n  \nTo test it simply copy paste from your browser the cookies that are being used in the fetch flights request",
          "score": 1,
          "created_utc": "2026-01-28 22:51:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u009e",
          "author": "Coding-Doctor-Omar",
          "text": "Have u tried using curl_cffi with impersonate=\"edge\" or \"firefox\" and with session cookies? I bet it will bypass 90% of anti-bot protections. The only things it will fail to bypass are interactive challenges.",
          "score": 1,
          "created_utc": "2026-01-31 18:39:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zr8z9",
              "author": "Financial-Image6455",
              "text": "That worked, thank you very much.",
              "score": 1,
              "created_utc": "2026-02-01 16:26:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qnimgt",
      "title": "Scrape a webpage that uses Akamai",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qnimgt/scrape_a_webpage_that_uses_akamai/",
      "author": "flstckdev",
      "created_utc": "2026-01-26 15:11:09",
      "score": 6,
      "num_comments": 13,
      "upvote_ratio": 0.7,
      "text": "I‚Äôm trying to scrape a webpage that uses Akamai bot protection and need to understand how to properly make HTTP requests that comply with Akamai‚Äôs requirements without using Selenium or Playwright.\n\nDoes anyone have general guidance on how Akamai detects non-browser traffic, what headers/cookies/flows are typically required, or how to structure requests so they behave like a normal browser? Any high-level advice or references would be helpful.\n\nThanks in advance.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qnimgt/scrape_a_webpage_that_uses_akamai/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1u2244",
          "author": "Acrobatic_Idea_3358",
          "text": "Not my write up but this guy has some headers and user agents that worked at some point. Probably  a good starting point, however I have not tested any of these. https://substack.thewebscraping.club/p/scraping-akamai-protected-websites",
          "score": 6,
          "created_utc": "2026-01-26 15:38:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zdfw3",
              "author": "flstckdev",
              "text": "Thanks! Definitely a good starting point.",
              "score": 2,
              "created_utc": "2026-01-27 08:23:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ud3cn",
          "author": "Odd-Lychee1248",
          "text": "I faced akamai bot detection and blocked my requests even I used selenium, but I solved the problem using SeleniumBase undetected mode and I sent the request using ex_script function inside chrome devtools using fetch()",
          "score": 4,
          "created_utc": "2026-01-26 16:25:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zdcgo",
              "author": "flstckdev",
              "text": "That one I did not know of. Thank you, i‚Äôll look into it!",
              "score": 1,
              "created_utc": "2026-01-27 08:22:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o21osch",
          "author": "ScrapeAlchemist",
          "text": "Hey!\n\nAkamai's detection goes way beyond headers - it fingerprints TLS settings (JA3/JA4), HTTP/2 frame ordering, and behavioral signals from their sensor script.\n\nFor pure HTTP without a browser, you'd need to:\n\n- Match TLS fingerprint exactly (curl-impersonate can help here)\n- Replicate HTTP/2 frame ordering and priority settings\n- Handle their cookie validation flow (initial request ‚Üí sensor challenge ‚Üí validated session)\n- Rotate IPs before behavioral patterns get flagged\n\nThe substack article someone linked is a good starting point for headers. But honestly, if the site is running Akamai Bot Manager (not just basic Akamai CDN), pure HTTP scraping becomes a cat-and-mouse game. Their sensor.js collects browser fingerprints that are hard to fake without actual JS execution.\n\nYou might want to check if the site has a mobile app or public API - sometimes those have lighter protections than the web frontend.\n\nGood luck!",
          "score": 3,
          "created_utc": "2026-01-27 16:59:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1u3epk",
          "author": "SurlyJason",
          "text": "I've done a few, and found they vary from stupidly easy (usually HTTP headers) to more complex where I could get some requests to work, but an IP would be blocked after just a few successful requests. \n\nIf you're okay sharing the URL, I have a little utility I made to test sites. I could see if I can tell where on the spectrum it lies.",
          "score": 2,
          "created_utc": "2026-01-26 15:44:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vjxdu",
              "author": "brnbs_dev",
              "text": "How does that utility tool work?",
              "score": 1,
              "created_utc": "2026-01-26 19:28:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21loks",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-27 16:45:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21pzgd",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-27 17:04:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qod78a",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qod78a/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-01-27 13:01:04",
      "score": 6,
      "num_comments": 12,
      "upvote_ratio": 0.81,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levels‚Äîwhether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) üå±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1qod78a/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2b5ims",
          "author": "Working_Map379",
          "text": "I am looking to hire web scraping expert. Please DM me.",
          "score": 3,
          "created_utc": "2026-01-28 23:09:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20j8it",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-27 13:45:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20v3bm",
              "author": "webscraping-ModTeam",
              "text": "‚ö°Ô∏è Please continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1) to promote products and services",
              "score": 1,
              "created_utc": "2026-01-27 14:46:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21d8jp",
          "author": "xRazar",
          "text": "I'm currently in the process of scraping e-sim sites wherever possible I try to find the public APIs for this but it does not seem that effective for most of the sites. Anyone has experience with scraping E-Sim sites (Saily, Nomad as few examples to go off)",
          "score": 1,
          "created_utc": "2026-01-27 16:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hodl0",
              "author": "error1212",
              "text": "I have",
              "score": 1,
              "created_utc": "2026-01-29 21:58:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21ndjn",
          "author": "EstablishmentOver202",
          "text": "How do you guys deal with cloudflare? Turnstile is killing me",
          "score": 1,
          "created_utc": "2026-01-27 16:53:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24ff3z",
              "author": "Mean_Professional529",
              "text": "Try a scraping API that handles JavaScript rendering and proxy rotation. Some services include built-in CAPTCHA solving for Turnstile. This can help bypass Cloudflare without managing it yourself",
              "score": 1,
              "created_utc": "2026-01-28 00:28:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2e6w04",
              "author": "jonfy98",
              "text": "You could try APIScraper which is efficient but not free,   \nanother idea could be NoDriver for better successrate which i often use for dealing with this kind of problem.",
              "score": 1,
              "created_utc": "2026-01-29 11:37:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2gq3ee",
              "author": "gbertb",
              "text": "not free, but very cost is use spider cloud",
              "score": 1,
              "created_utc": "2026-01-29 19:14:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2p35i9",
              "author": "Open_Passage_7351",
              "text": "[https://github.com/lexiforest/curl\\_cffi](https://github.com/lexiforest/curl_cffi) wrapped with FastAPI endpoint.\n\n\n\nI run my \\`app.py\\` which exposes a single \\`post /api/forward\\` endpoint. I can call that endpoint from any other service passing the target URL. The request is passed through curl\\_cffi, and response returned. I'm using this at a scale of thousands of requests a day with pretty decent success rate (well above 80%).",
              "score": 1,
              "created_utc": "2026-01-30 23:22:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o28bhyx",
          "author": "Either_Height7010",
          "text": "I'm hiring a US-based senior+ reverse engineer. At an incredibly high level, think bypassing anti-bot systems, large-scale web scraping/login automation, and JavaScript-based reverse engineering of web apps. \n\nI'm a third-party recruiter sourcing on behalf of my client. Message me if intrigued!",
          "score": 1,
          "created_utc": "2026-01-28 15:38:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo77gd",
      "title": "Advice needed: scraping company websites in Python",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qo77gd/advice_needed_scraping_company_websites_in_python/",
      "author": "Working_Taste9458",
      "created_utc": "2026-01-27 07:26:44",
      "score": 5,
      "num_comments": 15,
      "upvote_ratio": 0.78,
      "text": "I‚Äôm building a small project that needs to scrape company websites (manufacturers, suppliers, distributors, traders) to collect basic business information. I‚Äôm using Python and want to know what the best approach and tools are today for reliable web scraping. For example, should I start with requests + BeautifulSoup, or go straight to something like Playwright? Also, any general tips or common mistakes to avoid when scraping multiple websites would be really helpful.",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qo77gd/advice_needed_scraping_company_websites_in_python/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1zauu4",
          "author": "Bitter_Caramel305",
          "text": "Playwright is not the choice of any expert it's the choice of dumb beginners.\n\nRequests and bs4 is fine but replace requests with the requests module of curl\\_cffi.  \nThe syntax will be the same, but you'll get TSL fingerprinting of a real browser (Thanks to C) and an optional but powerful request param (impersonate=\"any browser of your choice\").\n\nExample:\n\n    from curl_cffi import requests\n    r = requests.get(url, cookies, headers, impersonate=\"chrome\")\n\n\n\nAlso, always reverse engineer the exposed backend API first and use this as a fallback not primary method.  \nHappy scraping!",
          "score": 12,
          "created_utc": "2026-01-27 08:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zn2lq",
              "author": "scraperouter-com",
              "text": "if curl\\_cffi is blocked you can try scrapling stealthmode but only if you are sure you need the browser (much slower way)",
              "score": 3,
              "created_utc": "2026-01-27 09:53:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o23uv95",
                  "author": "askolein",
                  "text": "But isn‚Äôt most websites not directly rendering html via http requests. I struggle to see any relevant website to scrape without selenium?",
                  "score": 1,
                  "created_utc": "2026-01-27 22:44:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o277s0v",
              "author": "husayd",
              "text": "I feel offended by the first sentence xd. I use both, and sometimes playwright (or selenium) is inevitable, or i am just a dumb beginner.",
              "score": 1,
              "created_utc": "2026-01-28 12:04:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o279aau",
                  "author": "Bitter_Caramel305",
                  "text": "Sorry about that ;) but to be honest, sometimes I reverse engineer the entire website while reverse engineering the API, just so I can avoid the inevitable browser automation.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:14:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o208fu0",
          "author": "Responsible-Fly-990",
          "text": "go with requests + BeautifulSoup if you r a beginner",
          "score": 2,
          "created_utc": "2026-01-27 12:43:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24byq2",
          "author": "Hungry-Working26",
          "text": "For company sites, start with requests and BeautifulSoup. Switch to Playwright only if you see dynamic content. Rotate user agents and add delays between requests to be respectful.\n\nHere's a basic pattern using the requests library:\n\npython\n\nimport requests\n\nfrom bs4 import BeautifulSoup\n\nresponse = requests.get('your\\_url\\_here')\n\nsoup = BeautifulSoup(response.content, 'html.parser')\n\nAlways check the site's robots.txt first",
          "score": 2,
          "created_utc": "2026-01-28 00:11:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2818mg",
          "author": "New-Independence5780",
          "text": "use cheriooCrawlee if it just simple websites that doesnt need js rendering if yes use playwrightCrawlee or puppeterCrawlee",
          "score": 1,
          "created_utc": "2026-01-28 14:51:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28rt0d",
          "author": "wequatimi",
          "text": "So you got ai businessidea=make cash fast.\nMight be entertaining. And educative..",
          "score": 1,
          "created_utc": "2026-01-28 16:49:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2azs7r",
          "author": "nez1rat",
          "text": "Honestly it depends on what are your target sites tho, I can suggest you to use [https://pypi.org/project/curl-cffi/](https://pypi.org/project/curl-cffi/) with BeautifulSoup as you mentioned",
          "score": 1,
          "created_utc": "2026-01-28 22:40:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eubfr",
          "author": "byte_knight_",
          "text": "Definetely start from with requests and bs4 or speed and simplicity, i'd use Playwright only for something JS heavy maybe",
          "score": 1,
          "created_utc": "2026-01-29 14:03:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpmlka",
      "title": "Help: BeautifulSoup/Playwright Parsing Logic",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/gallery/1qpmlka",
      "author": "TapProfessional4535",
      "created_utc": "2026-01-28 20:14:09",
      "score": 5,
      "num_comments": 8,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qpmlka/help_beautifulsoupplaywright_parsing_logic/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o2fm5s5",
          "author": "Business-Cherry1883",
          "text": "If your end goal is ‚Äú3000 players + rankings‚Äù, I‚Äôd seriously consider¬†**avoiding DOM parsing**¬†for the core dataset and only parsing HTML for the few fields that aren‚Äôt available elsewhere.\n\n* There‚Äôs already a Python package on pip called¬†`twofourseven`¬†that can scrape 247Sports recruiting data and includes a¬†`TransferPortal`¬†class with¬†`getFootballData(year)`¬†that returns a dataframe of everyone who entered the football transfer portal for a given year.‚Äã\n* Once you have that baseline list, use Playwright only for the ‚Äúdetail page‚Äù fields you¬†*must*¬†render (e.g., banners/commitment blocks) and keep the HTML parsing minimal and label-driven (parse ‚ÄúOVR/NATL/ST/Pos‚Äù by the nearby label text, not by assuming order/position).\n\nFor your brittle cases (stars, state rank vs position rank, JUCO variance), don‚Äôt do ‚Äúany number not X must be Y‚Äù. Instead: extract the small text chunk for each section (‚ÄúAs a Transfer‚Äù / ‚ÄúAs a Prospect‚Äù), then regex-match explicit patterns (`OVR`,¬†`NATL`,¬†`ST`,¬†`JUCO`,¬†`KS: 8`, etc.) and treat anything unmatched as ‚Äúunknown‚Äù rather than forcing it into a column.",
          "score": 3,
          "created_utc": "2026-01-29 16:15:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2n9oyk",
              "author": "TapProfessional4535",
              "text": "If I am using GitHub, Databricks, Google Collab, or another LLM, does that change your reco? \n\nI‚Äôm on a machine that is strict with what I can do in native Python app on it. \n\nThis is the first scraping project I‚Äôve ever done FWIW.",
              "score": 1,
              "created_utc": "2026-01-30 18:12:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ck3u1",
          "author": "jlrich10",
          "text": "I would look at scripts. I didnt want to take the time to do it but I would look at this and it see if it has what you need. \\_\\_INITIAL\\_DATA\\_\\_. Give that to Claude or Chatgpt and it will write the parser if it has what you need in it. Using json is usually better if its in there.",
          "score": 2,
          "created_utc": "2026-01-29 03:43:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2n8ciq",
              "author": "TapProfessional4535",
              "text": "I‚Äôve tried ChatGPT and Gemini both. Many hours of back and forth. I‚Äôve got enterprise licenses for both. I‚Äôm a decent promoter for regression modeling and forest modeling so this is making me pull my hair out",
              "score": 1,
              "created_utc": "2026-01-30 18:06:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ecmrt",
          "author": "scraperouter-com",
          "text": "Did you check JSON data available in the source code?\n\nhttps://preview.redd.it/x5irlakw5agg1.png?width=442&format=png&auto=webp&s=0b99b76c15763e484568f4b6ac05ddaaa2b80a79\n\nand other similar tags with structured data.",
          "score": 2,
          "created_utc": "2026-01-29 12:19:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2n8nx7",
              "author": "TapProfessional4535",
              "text": "I‚Äôd be lying if I knew what I‚Äôm doing. Savvy in advanced analytics, but this is the first scraping project I‚Äôve ever worked on.",
              "score": 0,
              "created_utc": "2026-01-30 18:07:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o34b0zm",
                  "author": "kev_11_1",
                  "text": "yeah try looking at this script tag in the html. Just click inspect on the page and then search for sthe cript tag that has json data and tell Gemini or GPT to take it from there.",
                  "score": 1,
                  "created_utc": "2026-02-02 07:34:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2a3fbl",
          "author": "TapProfessional4535",
          "text": "The Code Snippet: Here is the full parsing function. Is there a more robust way to handle these dynamic ranking boxes and to separate out Transfer vs. Prospect?\n\ndef parse_profile(html, url, player_id):\n    soup = BeautifulSoup(html, 'lxml')\n    data = {}\n   \n    # Locate the text nodes to find the correct containers\n    transfer_node = soup.find(string=re.compile(\"As a Transfer\"))\n    prospect_node = soup.find(string=re.compile(\"As a Prospect\"))\n\n    # --- 1. Parsing Transfer Section ---\n    if transfer_node:\n        t_container = transfer_node.find_parent('section') or transfer_node.find_parent('div')\n        if t_container:\n            # Stars & Rating\n            stars = t_container.select('.icon-starsolid.yellow')\n            data['Transfer Stars'] = len(stars)\n            rating = t_container.select_one('.rating')\n            if rating: data['Transfer Rating'] = rating.text.strip()\n           \n            # Ranks (Negative Logic)\n            for li in t_container.select('li'):\n                label = li.select_one('h5').get_text(strip=True).upper()\n                val = li.select_one('strong').get_text(strip=True)\n               \n                if 'OVR' in label:\n                    data['Transfer Overall Rank'] = val\n                # If NOT Overall/National/State, assume Position Rank (Fixes WR vs S mismatch)\n                elif label not in ['NATL', 'NATIONAL', 'ST', 'STATE']:\n                    data['Transfer Position Rank'] = val\n\n    # --- 2. Parsing Prospect Section (JUCO Logic) ---\n    if prospect_node:\n        p_container = prospect_node.find_parent('section') or prospect_node.find_parent('div')\n        if p_container:\n            # Check for JUCO header\n            is_juco = \"JUCO\" in p_container.get_text().upper()\n           \n            # Stars (Flag JUCO if empty)\n            stars = p_container.select('.icon-starsolid.yellow')\n            data['Prospect Stars'] = f\"{len(stars)} JUCO\" if is_juco else len(stars)\n\n            # Ranks (Prioritize National, then Position)\n            for li in p_container.select('li'):\n                label = li.select_one('h5').get_text(strip=True).upper()\n                val = li.select_one('strong').get_text(strip=True)\n               \n                if 'NATL' in label or 'NATIONAL' in label:\n                    data['Prospect National Rank'] = f\"{val} JUCO\" if is_juco else val\n                # Filter out State abbreviations (AK, AL, ... TX, etc) to find Position Rank\n                elif label not in ['OVR', 'ST', 'STATE', 'TX', 'FL', 'CA', 'GA']:\n                    data['Prospect Position Rank'] = f\"{val} JUCO\" if is_juco else val\n\n    return data",
          "score": 1,
          "created_utc": "2026-01-28 20:17:07",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpdyt6",
      "title": "Internal Google Maps API endpoints",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qpdyt6/internal_google_maps_api_endpoints/",
      "author": "LouisDeconinck",
      "created_utc": "2026-01-28 15:11:48",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.76,
      "text": "I build a scraper that extracts place IDs from the protobuf tiling api. Now I would like to fetch details from each place using this place id (I also have the S2 tile id). Are rhere any good endpoints to do this with?",
      "is_original_content": false,
      "link_flair_text": "Scaling up üöÄ",
      "permalink": "https://reddit.com/r/webscraping/comments/1qpdyt6/internal_google_maps_api_endpoints/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qsmo66",
      "title": "Monthly Self-Promotion - February 2026",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qsmo66/monthly_selfpromotion_february_2026/",
      "author": "AutoModerator",
      "created_utc": "2026-02-01 03:00:42",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 0.81,
      "text": "Hello and howdy, digital miners of¬†r/webscraping!\n\nThe moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!\n\n* Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?\n* Maybe you've got a ground-breaking product in need of some intrepid testers?\n* Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?\n* Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?\n\nWell, this is your time to shine and shout from the digital rooftops - Welcome to your haven!\n\nJust a friendly reminder, we like to keep all our self-promotion in one handy place, so any promotional posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qsmo66/monthly_selfpromotion_february_2026/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2x5kjy",
          "author": "HLCYSWAP",
          "text": "ETL (scraping), databasing, ML training AI (captcha bypass, voice, transformers - LLMs), standard desktop apps like DAWs as seen on my github. my past successes here:\n\n[https://github.com/matthew-fornear](https://github.com/matthew-fornear)\n\nopen to short or long work. you can reach me via reddit, x (https://www.x.com/fixitorgotojail) or discord",
          "score": 5,
          "created_utc": "2026-02-01 05:05:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31w4uw",
          "author": "malvads",
          "text": "Hi! After some time, researching into complex crawlers for web-to-llm data, I created [https://github.com/malvads/mojo](https://github.com/malvads/mojo). Mojo is an extremely fast C++ web scraper with multi-depth capabilities ready to ingest data into RAG-like systems, it scans entire websites and converts them to Markdown format. It also downloads artifacts like PDFs/others . It‚Äôs incredibly fast, so it can run on AWS Lambdas or any Cloud Provider, and it supports rendering (via Chrome CDP, if --render flag). Additionally, it has an internal reverse proxy with proxy rotation to facilitate scraping (when CDP, to not relaunch chrome instances, this is not for normal HTTP reqs). Mojo can scrape full websites in seconds while using very low CPU and RAM. Precompiled binaries are also available.",
          "score": 2,
          "created_utc": "2026-02-01 22:27:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3079iu",
          "author": "LessBadger4273",
          "text": "Giving out 100M+ Shopee and Aliexpress product page dataset for free for educational purposes. Reach out to Octaprice.com to learn how to get it",
          "score": 1,
          "created_utc": "2026-02-01 17:39:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrw98m",
      "title": "Data Scraping - What to use?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qrw98m/data_scraping_what_to_use/",
      "author": "Fabulous_Variety_256",
      "created_utc": "2026-01-31 07:47:26",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "My tech stack - NextJS 16, Typescript, Prisma 7, Postgres, Zod 4, RHF, Tailwindcss, ShadCN, Better-Auth, Resend, Vercel\n\nI'm working on a project to add to my cv. It shows data for gaming - matches, teams, games, leagues etc and also I provide predictions.\n\nMy goal is to get into my first job as a junior full stack web developer.\n\nI‚Äôm not done yet, I have at least 2 months to work on this project.\n\n\n\nThe thing is - I have another thing to do.\n\nI need to scrape data from another site. I want to get all the matches, the teams etc.\n\nWhen I enter a match there, it will not load everything. It will start loading the match details one by one when I'm scrolling.\n\n\n\nHow should I do it:\n\n\n\nIn the same project I'm building?\n\n\n\nIn a different project?\n\n\n\nIf 2, maybe I should show that I can handle another technologies besides next?:\n\n\n\nShould I do it with NextJS also\n\n\n\nShould I do it with NodeJS+Express?\n\n\n\nAnything else?\n\n\n\n\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qrw98m/data_scraping_what_to_use/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2rmxc0",
          "author": "hikingsticks",
          "text": "It sounds like you need to set up a headless browser based web scraper to get the data you need, then process it and stick it in a database.\n\nWhere are you deploying it? If you're using a VPS, consider one docker container running the database, one running the API to serve up the data, and one that gets started up periodically to run the scraper and insert the data into the database.\n\nPlaywright is a common choice for headless scraping, it can be done with javascript.",
          "score": 6,
          "created_utc": "2026-01-31 10:14:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sau1y",
          "author": "ketopraktanjungduren",
          "text": "If you're on NodeJS then use Playwright.¬†\n\n\nI'm on Python, and I use requests other tha Playwright. Maybe you also need a library that's like requests in python\n\n\nThese two are more than enough to start with",
          "score": 1,
          "created_utc": "2026-01-31 13:28:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tuqra",
          "author": "RandomPantsAppear",
          "text": "You‚Äôre going to find a lot more support for scraping related activities in Python, not JavaScript. \n\nPython is the language of choice for data processing and analysis, so it‚Äôs also the language of choice for acquisition.",
          "score": 1,
          "created_utc": "2026-01-31 18:14:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrdd1y",
      "title": "Asking for advice and tips.",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qrdd1y/asking_for_advice_and_tips/",
      "author": "LowDiscount6694",
      "created_utc": "2026-01-30 18:18:10",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 0.67,
      "text": "Context: former software engineer and data analyst.\n\nGood morning to all of my master,\n\nI would like to seek an advice how to make become a better web scraper. I am using python selenium web scraping, pandas for data manipulation and third party vendor. I am not comfortable to my scraping skills I used to create a scraping in first quarter of last year. And currently I've been able to apply to a company. Since they hiring for web scraping engineer. I am confident that I will passed the exercises. Since I got the asking data. Now, what do I need to make my scraping become undetectable? I used the residential proxies provided Also the captcha bypass. I just wanted to learn how to apply the fingerprinting and etc. because I wanted to got hired so I can pay house bills. :( anything advice that you want to share.\n\nThank you for listening to me.",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qrdd1y/asking_for_advice_and_tips/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o2nesmn",
          "author": "Active_Winner",
          "text": "Use playwright stealth, seleniumbase and drissionpage",
          "score": 1,
          "created_utc": "2026-01-30 18:34:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p1dg4",
          "author": "johnster929",
          "text": "Zendriver kept me under the radar, no need for proxy IP service",
          "score": 1,
          "created_utc": "2026-01-30 23:12:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}