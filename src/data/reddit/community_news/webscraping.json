{
  "metadata": {
    "last_updated": "2026-02-08 03:39:58",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 15,
    "total_comments": 58,
    "file_size_bytes": 71026
  },
  "items": [
    {
      "id": "1qttig8",
      "title": "How are you using AI to help build scrapers?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qttig8/how_are_you_using_ai_to_help_build_scrapers/",
      "author": "lieutenant_lowercase",
      "created_utc": "2026-02-02 12:18:06",
      "score": 19,
      "num_comments": 24,
      "upvote_ratio": 0.82,
      "text": "I use Claude Code for a lot of my programming but doesn't seem particularily useful when I'm writing web scrapers. I still have to load up the site, go to dev tools, inspect all the requests, find the private API's, figure out headers / cookies, check if its protected by Cloudflare / Akamai etc.. Perhaps once I have that I can dump all my learnings into claude code with some scaffolding at get it to write the scraper, but its still quite painful to do. My major time sink is understanding the structure of the site/app and its protections rather than writing the actual code. \n\nI'm not talking about using AI to parse websites, thats the easy bit tbh. I'm talking about the actual code generation. Do people give their LLM's access to the browser and let it figure it out? Anything else you guys are doing?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qttig8/how_are_you_using_ai_to_help_build_scrapers/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o363xue",
          "author": "somedude4949",
          "text": "Pass har file with requests I need to use give custom prompt on how to built and voila after few minutes everything working and integrate it depends on my use case",
          "score": 6,
          "created_utc": "2026-02-02 15:30:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hy7di",
          "author": "Forsaken_Lie_8606",
          "text": "honestly the biggest win for me was using ai to generate the initial selector logic and then manually tweaking it. saves tons of time on boilerplate",
          "score": 3,
          "created_utc": "2026-02-04 08:36:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rvm5j",
              "author": "Krokzter",
              "text": "What do you use for that?",
              "score": 1,
              "created_utc": "2026-02-05 19:49:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35d5s1",
          "author": "sjmanzur",
          "text": "I started using antigravity with playwright and it‚Äôs a game changer really",
          "score": 5,
          "created_utc": "2026-02-02 13:05:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35jpsl",
          "author": "balletpaths",
          "text": "I point it to a URL, give a sample code format and let it rip! Then I adjust and make minor modifications.",
          "score": 2,
          "created_utc": "2026-02-02 13:44:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38sqni",
          "author": "orthogonal-ghost",
          "text": "I've thought about this problem a lot. The main challenge as you've noted is given the coding agent the proper context (HTML, network requests, javascript, etc.).  \n  \nTo address this, we built a specialized agent to programmatically \"inspect\" a web site for that context and to generate a Python script to scrape it. With that comes its own share of challenges (e.g., naively passing in all the HTML on a given web page can very quickly eat up an LLM's context), but we've found that it's been quite successful in building scrapers once it has the right things to look at.",
          "score": 2,
          "created_utc": "2026-02-02 23:04:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3t8a9h",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-05 23:53:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3tokbg",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 1,
                  "created_utc": "2026-02-06 01:27:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o39j6il",
          "author": "calimovetips",
          "text": "i mostly use ai after i‚Äôve mapped the network calls. it‚Äôs good for turning notes into clean request code, retries, backoff, and a sane pipeline. the hard part is still modeling sessions and state, plus deciding what‚Äôs stable. are you scraping mostly xhr/json endpoints or full browser flows?",
          "score": 2,
          "created_utc": "2026-02-03 01:30:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3b2w3c",
              "author": "lieutenant_lowercase",
              "text": "XHR endpoints where available but fall back to full browser if i need",
              "score": 2,
              "created_utc": "2026-02-03 07:58:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3cphek",
          "author": "Hundreds-Of-Beavers",
          "text": "Built a Playwright agent to help with this - we gave it access to both a live browser session and a Typescript environment, so it can inspect the DOM, then write & execute Playwright code to test out the implementation against the browser.  And gave it tools for data extraction/screenshots/etc.\n\nBasically, our approach is we let the LLM do the majority of the work (and give it the tools to do so), but can then go in and troubleshoot the scraper as necessary",
          "score": 2,
          "created_utc": "2026-02-03 15:09:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3m79m3",
          "author": "Imaginary_Gate_698",
          "text": " AI helps more after you‚Äôve already done the hard part. We use it to scaffold Scrapy spiders, normalize responses, write retry and parsing glue, that kind of stuff. The discovery phase you described, mapping requests, session behavior, which calls actually matter, is still very manual and very site specific.\n\nGiving an LLM a browser sounds nice, but in practice it‚Äôs slow and it misses why things break under volume. It won‚Äôt notice session churn patterns, subtle header dependencies, or why a flow works once and then degrades. Where it‚Äôs been useful is once you‚Äôve identified the right endpoints, you can dump a clean HAR or request samples in and let it generate a first pass, then you tune from there. The real time sink is still understanding how the app behaves over time, not writing the code.",
          "score": 2,
          "created_utc": "2026-02-04 22:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35e014",
          "author": "No-Appointment9068",
          "text": "I sometimes download the page source, set up a test for the output I want and then let AI have a crack at getting the selectors correct, they often produce quite brittle selectors but it's very easy to then fix with the same process.",
          "score": 2,
          "created_utc": "2026-02-02 13:10:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35reg9",
              "author": "jwrzyte",
              "text": "this - almost all my parsing code is generated by copilot, then i can test against it within pytest & scrapy. and make any changes as needed",
              "score": 1,
              "created_utc": "2026-02-02 14:26:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36cmpi",
                  "author": "No-Appointment9068",
                  "text": "A little tip! Chrome dev tools let's you copy selectors if you right click the element within the inspect view, for one off scripts I just use those. I Don't bother getting any fancier with AI.",
                  "score": 3,
                  "created_utc": "2026-02-02 16:11:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36mtp6",
          "author": "Tharnwell",
          "text": "Following. I'm currently building a web platform almost entirely with AI. Development and content creation are fully automated.\n\nThe only part AI still struggles with in my workflow is sourcing images from the web. I‚Äôm aware of copyright concerns, but in my specific use case this isn‚Äôt a major issue.\n\nWhile AI can generate images, they don‚Äôt work well for my needs.",
          "score": 1,
          "created_utc": "2026-02-02 16:57:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o374yiq",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-02 18:20:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3aco0k",
              "author": "webscraping-ModTeam",
              "text": "üí∞ Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-02-03 04:25:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37jkzh",
          "author": "builderbycuriosity",
          "text": "Give your Claude code access to MCP servers like Playwright, which can automate the browser. It may not be perfect, but it will do the job.",
          "score": 1,
          "created_utc": "2026-02-02 19:27:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39eeeu",
          "author": "xRazar",
          "text": "I had a lot of success using Agent-Browser with Skills to integrate it into the models into OpenCode. The agents scan through the site trying to find public APIs if that fails it goes back to classic scraping.",
          "score": 1,
          "created_utc": "2026-02-03 01:03:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35jwhf",
          "author": "AdministrativeHost15",
          "text": "Have the AI generate the scraping script. Don't code review it or try to fix it. Just have a test that determines if it returns any useful data or not. If it doesn't have the AI regenerate it.",
          "score": 0,
          "created_utc": "2026-02-02 13:45:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxb7cr",
      "title": "Bypass Cloudflare Security Challenge Page in Headless Mode",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qxb7cr/bypass_cloudflare_security_challenge_page_in/",
      "author": "jagdish1o1",
      "created_utc": "2026-02-06 07:28:54",
      "score": 17,
      "num_comments": 27,
      "upvote_ratio": 0.9,
      "text": "I‚Äôm working on a web scraping project where i need to scrape 20+ websites and schedule them. \n\nI‚Äôm using a vps for all this. Most of the sites are bypassed using seleniumbase + playwright but I‚Äôm stuck with one website which is working normal but not in headless mode. \n\nI‚Äôve tried residential proxies and cdp browsers but nothing seems to be working. \n\nI have a plan B to create an RDP and schedule this single scraper there but i want to avoid that.\n\nI‚Äôm stuck, do you guys have any suggest? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qxb7cr/bypass_cloudflare_security_challenge_page_in/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o3vby4y",
          "author": "HLCYSWAP",
          "text": "create a headed browser with persistent context in playwright solve the challenge and save then load the session headless or setup dev tools debug port and connect via a headless script \n\nor reuse existing chrome instance via selenium \n\nor use selenium-wire to capture headed browser traffic and replay it in headless",
          "score": 13,
          "created_utc": "2026-02-06 08:36:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vf7jv",
              "author": "jagdish1o1",
              "text": "Thanks for the suggestion, i've tried this approach but it also didn't worked. ",
              "score": 2,
              "created_utc": "2026-02-06 09:08:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3y45rp",
                  "author": "HLCYSWAP",
                  "text": "you shouldnt be using browsers to scrape anyways, pull the cookies and send the request directly with a TLS impersonation",
                  "score": 5,
                  "created_utc": "2026-02-06 18:41:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3w2g66",
          "author": "Curiouser666",
          "text": "Which is the specific site giving you issues?",
          "score": 2,
          "created_utc": "2026-02-06 12:27:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w9y9z",
              "author": "jagdish1o1",
              "text": "It‚Äôs a gov website, we need to scrape publicly available data from there.",
              "score": 1,
              "created_utc": "2026-02-06 13:14:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3xdnpr",
                  "author": "hajuherne",
                  "text": "Is it dynamic or static html?",
                  "score": 1,
                  "created_utc": "2026-02-06 16:35:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3w9b6c",
          "author": "hackbyown",
          "text": "Try with real chrome browser, controlled via playwright with channel set to chrome",
          "score": 2,
          "created_utc": "2026-02-06 13:10:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w9pu5",
              "author": "hackbyown",
              "text": "Or try with websockets connect chrome over cdp, it should work or you can try to add xvnc and in headed mode with xvfb , try to forward server ip to local to get back a interactive session window ehere you can see what's happening in headful mode is captcha getting solved automatically or not \n\nAnd you can share that website for reference here so that people in this community can take a look",
              "score": 2,
              "created_utc": "2026-02-06 13:13:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3wabrv",
                  "author": "jagdish1o1",
                  "text": "I‚Äôve already tried that seleniumbase does this but it‚Äôs getting detected in headless mode. And i cannot run the scraper in headed mode inside a vps. RDP is the solution but i want to avoid that coz it‚Äôs an overall kill for a small task.",
                  "score": 1,
                  "created_utc": "2026-02-06 13:16:54",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3xvagn",
          "author": "jagdish1o1",
          "text": "I‚Äôve managed to solve the issue with seleniumbase + playwright + headed mode using xvfb-run on ubuntu vps. \n\nThank you all for the suggests and comment.",
          "score": 2,
          "created_utc": "2026-02-06 17:59:07",
          "is_submitter": true,
          "replies": [
            {
              "id": "o42f9e0",
              "author": "scorpiock",
              "text": "If looking for a managed solution, you may try Geekflare Web Scraping API. ",
              "score": 1,
              "created_utc": "2026-02-07 11:58:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wr2eq",
          "author": "akashpanda29",
          "text": "Can you provide me the website you want to scrape . I can check that for you",
          "score": 1,
          "created_utc": "2026-02-06 14:46:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3wtulq",
          "author": "eimattz",
          "text": "Switch to puppeter with stealth plugins. I did this 2 days ago, from playwright.",
          "score": 1,
          "created_utc": "2026-02-06 15:00:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xn88u",
          "author": "gami2009",
          "text": "use seleniumbase library, it open browser in undetected mode, i work 100% cloude flare human captcha 100%, i alredy tested it¬†",
          "score": 1,
          "created_utc": "2026-02-06 17:20:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42egu7",
          "author": "Coding-Doctor-Omar",
          "text": "Try Scrapling's (Async)StealthySession.",
          "score": 1,
          "created_utc": "2026-02-07 11:51:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qx7i8d",
      "title": "Find all mentions of a URL on Reddit",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qx7i8d/find_all_mentions_of_a_url_on_reddit/",
      "author": "Unmoovable",
      "created_utc": "2026-02-06 04:10:01",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 0.77,
      "text": "Turns out, a lot of the old reddit json endpoints still work. If you use this endpoint: https://www(dot)reddit.com/domain/github.com/.json (not linked cause don't want this to get auto-modded), it will return X number of posts that link to that domain, and can use it for any domain.\n\nBeen trying to figure out how to integrate this into an app I've been working on, and thought I'd share. Interesting use cases?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qx7i8d/find_all_mentions_of_a_url_on_reddit/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o3ym78n",
          "author": "divided_capture_bro",
          "text": "Shhh",
          "score": 2,
          "created_utc": "2026-02-06 20:08:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v7jrg",
          "author": "Coding-Doctor-Omar",
          "text": "Thanks a lot man!!!",
          "score": 2,
          "created_utc": "2026-02-06 07:55:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o432cor",
          "author": "Status_Profile4078",
          "text": "\\#script for lead generation, you can change githubdotcom to any other link and change the keywords  \n  \n  \nimport requests\n\nimport time\n\n\n\n\\# Target the JSON feed\n\nurl = \"https://www.reddit.com/domain/github.com/.json\"\n\n\\# Reddit requires a custom User-Agent to prevent 429 errors\n\nheaders = {'User-Agent': 'MyAutomationScript/0.1'}\n\n\n\ndef check\\_for\\_leads():\n\nresponse = requests.get(url, headers=headers)\n\ndata = response.json()\n\nposts = data\\['data'\\]\\['children'\\]\n\n\n\nfor post in posts:\n\ntitle = post\\['data'\\]\\['title'\\]\n\npermalink = post\\['data'\\]\\['permalink'\\]\n\n\n\n\\# Look for keywords indicating a \"requirement\" or \"problem\"\n\nkeywords = \\[\"help\", \"fix\", \"issue\", \"automate\", \"request\", \"hi\"\\]\n\nif any(word in title.lower() for word in keywords):\n\nprint(f\"Potential Lead Found: {title}\")\n\nprint(f\"Link: https://reddit.com{permalink}\\\\n\")\n\n\n\nif \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":\n\nwhile True:\n\ncheck\\_for\\_leads()\n\ntime.sleep(600) # Check every 10 minutes",
          "score": 1,
          "created_utc": "2026-02-07 14:32:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o435i6z",
              "author": "Coding-Doctor-Omar",
              "text": "Why are you exposing yourself in the user agent? Why not use a realistic user agent instead?",
              "score": 1,
              "created_utc": "2026-02-07 14:49:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o44m2fi",
                  "author": "Status_Profile4078",
                  "text": "Becausee it works in this case",
                  "score": 1,
                  "created_utc": "2026-02-07 19:10:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qurakm",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qurakm/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-02-03 13:00:35",
      "score": 9,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levels‚Äîwhether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) üå±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring üí∞",
      "permalink": "https://reddit.com/r/webscraping/comments/1qurakm/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o3iswjr",
          "author": "Ok-Tower-7808",
          "text": "Best Way to Scrape LinkedIn for Thesis Research?\n\n  \nHi everyone!\n\nI‚Äôm currently working on my thesis, where I‚Äôm researching how a specific master‚Äôs degree is valued on the labour market.\n\nTo do this, I‚Äôm looking to gather data from LinkedIn‚Äîspecifically employment status, job titles, and ideally get their tags and the ‚Äúabout‚Äù section used by people who hold this degree.\n\nDoes anyone know of any good tools or methods to scrape this kind of data from LinkedIn? Any advice or experience would be greatly appreciated!\n\nThanks in advance üòä\n\n¬†",
          "score": 2,
          "created_utc": "2026-02-04 12:54:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3k16rm",
              "author": "Fragrant_Ad3054",
              "text": "Hello,\n\nThere are online services available that often allow you, using a Chrome extension for example, to save and export all sorts of data, including the data you're looking for.\n\nThese services are usually paid, and you pay based on the amount of information you export using the extension.\n\nHow much data would you need approximately (in thousands)? This question is to guide you based on your answer, not to sell a product I created or a product for which I receive a commission (I thought I'd mention it, haha).\n\nEdit: I advise against creating your own program to do this because, having done it a few years ago, it takes a lot of time and money. It's best to use existing free or paid tools; this will save you considerable time and money, unless the program is specifically designed for your thesis.",
              "score": 0,
              "created_utc": "2026-02-04 16:39:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3rtw1d",
          "author": "nez1rat",
          "text": "Personally grab cookies from browser and u can go full py requests!",
          "score": 1,
          "created_utc": "2026-02-05 19:41:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43sk4b",
          "author": "Critical_Ad_9397",
          "text": "In pay-per-view campaigns, Whop claims to count only real TikTok video views.\n\nHow do they do this? I looked through the TikTok APIs, but I couldn‚Äôt find any data that differentiates real views from suspicious ones.\n\nI am looking for technical answer, not workaround, since I am not Whop creator, I actually wonder how it works.",
          "score": 1,
          "created_utc": "2026-02-07 16:45:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyn0p8",
      "title": "Open sourced my business' data extraction framework",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qyn0p8/open_sourced_my_business_data_extraction_framework/",
      "author": "Apprehensive-File169",
      "created_utc": "2026-02-07 19:31:07",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Through years of webscraping, a huge issue I faced is discrepancy between data types and extraction types and varying website formats.\n\n\n\nA website that has an API, some html docs, json within the html, multiple potential formats and versions etc. all need code flows to extract the same data. And then, how do you have resiliency and consistency in data extraction when the value is usually in place A with an xpath, sometimes place B with a json, and as a last resort regex search for place C?\n\n\n\nMy framework, chadselect, pulls html json and raw text into one class that allows selection across all 4 extraction frameworks (xpath, css, regex, jmespath) to build consistent data collection.\n\n    cs = ChadSelect()\n    cs.add_html('<>some html</>')\n\n    result = cs.select_first([\n        (0, \"css:#exact-id\"),\n        (0, \"xpath://span[@class='alt']/text()\"),\n        (0, r\"regex:fallback:\\s*(.+)\"),\n    ])\n    \n\n  \nOne more addition, common xpath functions like normalize space, trim, substring, replace are built into all selectors - not only limited to xpath anymore. Callable with simple '>>' piping:\n\n    result = cs.select(0, \"css:.vin >> substring-after('VIN: ') >> substring(0, 3) >> lowercase()\")\n    \n\n  \nFuthermore, it's already preconfigured with what I've found to be the fastest engines for each type of querying (lxml, selectolax, re, and jmespath). So hopefully it will be a boost to consistency, dev convenience, and execution time.\n\n\n\nI'm trying to get into open sourcing some projects and frameworks I've built. It would mean the world to me if this was useful to anyone. Please leave issues or comments for any bugs or feature requests. \n\n\n\nThank you for your time\n\n  \n\n\n[https://github.com/markjacksoncerberus/chadselect](https://github.com/markjacksoncerberus/chadselect)\n\n[https://pypi.org/project/chadselect/](https://pypi.org/project/chadselect/)\n\n[https://crates.io/crates/chadselect](https://crates.io/crates/chadselect)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qyn0p8/open_sourced_my_business_data_extraction_framework/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qt7pju",
      "title": "litecrawl - minimal async crawler for targeted, incremental scraping",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qt7pju/litecrawl_minimal_async_crawler_for_targeted/",
      "author": "Little_Ant_3459",
      "created_utc": "2026-02-01 19:14:58",
      "score": 7,
      "num_comments": 4,
      "upvote_ratio": 0.9,
      "text": "I kept hitting the same pattern at work: \"we need to index this specific section of this website, with these rules, on this schedule.\" Each case was slightly different - different URL patterns, different update frequencies, different extraction logic.\n\nScrapy felt like overkill. I didn't need a framework with spiders and pipelines and middleware. I needed a tool I could call with parameters and forget about.\n\nSo I built litecrawl: one async function that manages its own state in SQLite.\n\nThe idea is you spin up a separate instance per use case. Each gets its own DB file, its own cron job, its own config. No orchestration, no shared state, no central scheduler. Just isolated, idempotent processes that pick up where they left off.\n\n    from litecrawl import litecrawl\n    \n    litecrawl(\n        sqlite_path=\"council.db\",\n        start_urls=[\"https://example.com/minutes\"],\n        include_patterns=[r\"https://example\\.com/minutes/\\d+\"],\n        n_concurrent=5,\n        fresh_factor=0.5\n    )\n\nIt handles the boring-but-important stuff:\n\n* Adaptive scheduling - backs off for static pages, speeds up for frequently changing content\n* Crash recovery - claims pages with row-level locking, releases stalled jobs automatically\n* Content hashing - only flags pages as \"fresh\" when something actually changed\n* SSRF protection - validates all resolved IPs, not just the first one\n* robots.txt - cached per domain with async fetching\n* Downloads - catches PDFs/ZIPs that trigger downloads instead of navigation\n\nDesigned to run via cron wrapped in `timeout`. If it crashes or gets killed, the next run continues where it left off.\n\n`pip install litecrawl`\n\nGitHub: [https://github.com/jakobmwang/litecrawl](https://github.com/jakobmwang/litecrawl)\n\nBuilt this for various data projects. Would love feedback - especially if you spot edge cases I haven't considered.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qt7pju/litecrawl_minimal_async_crawler_for_targeted/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o34s1j6",
          "author": "WindScripter",
          "text": "this looks awesome. gonna give it a try and let you know how it works, i generally have a similar use case as you",
          "score": 1,
          "created_utc": "2026-02-02 10:17:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36c9no",
              "author": "Little_Ant_3459",
              "text": "Cool! Looking forward to hear from you.",
              "score": 1,
              "created_utc": "2026-02-02 16:09:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o34y20u",
          "author": "renegat0x0",
          "text": " \\- I am not an expert, but it looks as if it used \"functional\" programming approach, but where everything is in a one file. I don't like projects where everything is in one file\n\n \\- was it vibe coded? I do not really see any relevant commit history",
          "score": 1,
          "created_utc": "2026-02-02 11:12:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36d4c9",
              "author": "Little_Ant_3459",
              "text": "A valid criticism. I am beginning to get use cases where a few link-related add-ons would be handy. When I get around to add that functionality, I should split it into more than one file for readability.\n\nAnd while I use LLM's intensively in most of my work, this one I actually hand-held more than usual, since they didn't understand the paradigm/approach.",
              "score": 2,
              "created_utc": "2026-02-02 16:13:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1quhl21",
      "title": "Tadpole - A modular and extensible DSL built for web scraping",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1quhl21/tadpole_a_modular_and_extensible_dsl_built_for/",
      "author": "tadpolehq",
      "created_utc": "2026-02-03 03:55:04",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.76,
      "text": "Hello!\n\nI wanted to share my recent project: Tadpole. It is a custom DSL built on top of [KDL](https://kdl.dev/) specifically for web scraping and browser automation.\n\nCheck out the documentation: https://tadpolehq.com/\nGithub Repo: https://github.com/tadpolehq/tadpole\n\n## Why?\nIt is designed to be modular and allows local and remote imports from git repositories. It also allows you to compose and slot complex `actions` and `evaluators`. There's tons of built-in functionality already to build on top of!\n\n### Example\n```kdl\nimport \"modules/redfin/mod.kdl\" repo=\"github.com/tadpolehq/community\"\n\nmain {\n  new_page {\n    redfin.search text=\"=text\"\n    wait_until\n    redfin.extract_from_card extract_to=\"addresses\" {\n      address {\n        redfin.extract_address_from_card\n      }\n    }\n  }\n}\n```\n\nand to run it:\n```bash\ntadpole run redfin.kdl --input '{\"text\": \"Seattle, WA\"}' --auto --output output.json\n```\n\nand the output:\n```json\n{\n  \"addresses\": [\n      {\n        \"address\": \"2011 E James St, Seattle, WA 98122\"\n      },\n      {\n        \"address\": \"8020 17th Ave NW, Seattle, WA 98117\"\n      },\n      {\n        \"address\": \"4015 SW Donovan St, Seattle, WA 98136\"\n      },\n      {\n        \"address\": \"116 13th Ave, Seattle, WA 98122\"\n      }\n      ...\n    ]\n}\n```\n\nIt is incredibly powerful to be able to now easily share and reuse scraper code the community creates! There's finally a way to standardize this logic.\n\n## Why not AI?\nAI is not doing a great job in this area, it's also incredibly inefficient and having noticeable environmental impact. People actually like to code.\n\n## Why not just Puppeteer?\nTadpole doesn't just call `Input.dispatchMouseEvent`, commands like `click` and `hover` are actually composed of several actions that use a bezier curve, and ease out functions to try to simulate human behavior. You get the ability to easily abstract away everything into the DSL.  The decentralized package manager also lets you share your code without the additional overhead and complexity that comes with npm or pip.\n\n**Note:** Tadpole is not built on Puppeteer, it implements CDP method calls and manages its own websocket.\n\nThe package was just released! Had a great time dealing with changesets not replacing the `workspace:` prefix. There will be bugs, but I will be actively releasing new features. Hope you guys enjoy this project!\n\nAlso, I created a repository: https://github.com/tadpolehq/community\nfor people to share their scraper code if they want to!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1quhl21/tadpole_a_modular_and_extensible_dsl_built_for/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o407qmi",
          "author": "Forsaken_Lie_8606",
          "text": "great question validation is everything i learned from my failures",
          "score": 1,
          "created_utc": "2026-02-07 01:18:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtv2ob",
      "title": "How to scrape Instagram followers/followings in chronological order?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qtv2ob/how_to_scrape_instagram_followersfollowings_in/",
      "author": "Any_Independent375",
      "created_utc": "2026-02-02 13:30:16",
      "score": 3,
      "num_comments": 13,
      "upvote_ratio": 0.72,
      "text": "Hi everyone,\n\nI‚Äôm trying to understand how some websites are able to show Instagram followers or followings in **chronological order** for **public accounts**.\n\nI already looked into this:\n\n* When opening the followers/following popup on Instagram, the list is **not** shown in chronological order.\n* The web request [https://www.instagram.com/api/v1/friendships/{USER\\_ID}/following/?count=12](https://www.instagram.com/api/v1/friendships/{USER_ID}/following/?count=12) returns users in **exactly the same order** as shown in the popup, which again is **not chronological**.\n* The response does **not** include any obvious timestamp like followed\\_at, nor an incrementing ID that would allow sorting by time.\n\nI‚Äôm interested in **how this is technically possible at all**.\n\nAny insights from people who have looked into this would be really appreciated. \n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qtv2ob/how_to_scrape_instagram_followersfollowings_in/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o3e1btn",
          "author": "Forsaken_Lie_8606",
          "text": "so i was trying to do something similar a while back and i stumbled upon this thing where if you use the instagram graph api, you can get the followers in a specific order, but its not exactly chronological, its more like a ranked order based on teh users interaction with the account, but if you combine that with the users profile info, like when they joined instagram, you can kinda sort it out, ngl its a lot of work and not very accurate, but i did manage to get a list of around 80% of the followers in the correct order, tbh i dont know if its worth the effort tho, lol",
          "score": 2,
          "created_utc": "2026-02-03 18:50:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3x13zv",
              "author": "Galaxy_garret44",
              "text": "How do you do this?",
              "score": 1,
              "created_utc": "2026-02-06 15:36:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o36fmsn",
          "author": "HLCYSWAP",
          "text": "on the assumption you‚Äôve truly exhausted all web api: they are getting different data via the mobile API, stuffing the headers of the request so they get more data than intended, or simply lying ü§•\n\nedit: could also be scraping daily and adding in new accounts via falsified timestamps but correct date",
          "score": 1,
          "created_utc": "2026-02-02 16:24:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38tryl",
              "author": "Any_Independent375",
              "text": "I don't think they are lying or scraping random accounts every day. I created an account, followed 6 profiles and the websites were able to list all followings in chronological order. There must be a way to achieve this.\n\nCould you clarify what you mean by mobile API and how to access it?",
              "score": 1,
              "created_utc": "2026-02-02 23:10:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o38z77j",
                  "author": "HLCYSWAP",
                  "text": "are they able to do this for any account at any moment? if so, they have access to APIs (likely mobile) that are exposing more data. is it for specific accounts only? if so, might just be a notification scraper.\n\n\n\nthe mobile app hits different endpoints than web. youre going to need to run the mobile traffic through a network request sniffer ala burp suite, http toolkit, etc. but these apps use certificate pinning meaning if you use the burp CA cert to sniff traffic itll deny you access to any traffic on the app so youll need a rooted/jailbroken mobile or an emulator with the ability to unpin certs ala frida/objection/manual APK patching with as old a build of instragram as you can get.",
                  "score": 1,
                  "created_utc": "2026-02-02 23:39:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36iopj",
          "author": "Hour_Analyst_7765",
          "text": "It could be they are checking over and over for months to years.\n\nWithout ground truth its also hard to check if any site is 100% accurate too.",
          "score": 1,
          "created_utc": "2026-02-02 16:38:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38teuw",
              "author": "Any_Independent375",
              "text": "I created a fresh Instagram account, followed 6 profiles and then checked the followers/followings with 3 different websites. They were able to list the followers/followings in chronological order. \n\nI also tried a bunch of other profiles and it was accurate.\n\nThere must be a way to achieve this.",
              "score": 2,
              "created_utc": "2026-02-02 23:08:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3bjvy7",
                  "author": "Hour_Analyst_7765",
                  "text": "Well, like I said.. create a difference list from the previously known data and then add these events with a timestamp of the scrape event (if the site doesn't report it). The accuracy of the order then depends on how often they can scrape the list.\n\nYou could try follow some accounts, but do it in different orders from multiple of your own accounts.\n\nI presume it takes a while to refresh the data, so if you follow with different order it should either confirm whether its invisible or not to this site. If it still has perfect correlation then that suggests they are getting the data from somewhere else.\n\nI apply a similar technique for scraping articles from sites. I will look for a post & modify dates in the <meta> tags, search engine JSON or a custom parser from the HTML. If there is nothing found, then I will default the date tag to the time of the page load, which has limited value but at least its timestamped. I also need these dates for orderings and the like.",
                  "score": 1,
                  "created_utc": "2026-02-03 10:42:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o377pbv",
          "author": "Affectionate_Cold209",
          "text": "Doesn't it needs proxy?\n\n(completely a question not the answer your are intereted in)",
          "score": 1,
          "created_utc": "2026-02-02 18:33:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3cd3nn",
          "author": "janalpadi",
          "text": "I‚Äôm currently expanding the scraper to include bios, follower counts, and following counts for each profile. Also, is it still possible to sort by chronological order on the Instagram mobile app? I haven‚Äôt used the app in ages since I‚Äôve been focusing on the web version for scraping. Let me know if you manage to figure that out",
          "score": 1,
          "created_utc": "2026-02-03 14:04:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v5m8g",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-06 07:37:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3v88qw",
              "author": "webscraping-ModTeam",
              "text": "ü™ß Please review the sub rules üëâ",
              "score": 1,
              "created_utc": "2026-02-06 08:01:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qtcyok",
      "title": "Non sucking, easy tool to convert websites to LLM ready data, Mojo",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qtcyok/non_sucking_easy_tool_to_convert_websites_to_llm/",
      "author": "malvads",
      "created_utc": "2026-02-01 22:29:29",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.81,
      "text": "Hey all! After running into¬†*only paid tools or overly complicated setups*¬†for turning web pages into structured data for LLMs, I built¬†**Mojo,**¬†a¬†**simple, free, open-source tool**¬†that does exactly that. It‚Äôs designed to be easy to use and integrate into real workflows.\n\nIf you‚Äôve ever needed to prepare site content for an AI workflow without shelling out for paid services or wrestling with complex scrapers, this might help. Would love feedback, issues, contributions, use cases, etc. <3\n\n[https://github.com/malvads/mojo](https://github.com/malvads/mojo)¬†(and it's MIT licensed)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qtcyok/non_sucking_easy_tool_to_convert_websites_to_llm/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qx6ser",
      "title": "I built a CLI Manga Archiver in Python",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/gallery/1qx6ser",
      "author": "Sea-Jelly-2360",
      "created_utc": "2026-02-06 03:35:05",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.72,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qx6ser/i_built_a_cli_manga_archiver_in_python/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1quoftu",
      "title": "HTML parser to query on computed CSS rather than class selectors",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1quoftu/html_parser_to_query_on_computed_css_rather_than/",
      "author": "Hour_Analyst_7765",
      "created_utc": "2026-02-03 10:26:54",
      "score": 3,
      "num_comments": 8,
      "upvote_ratio": 0.81,
      "text": "Some websites try to obfuscate HTML DOM by changing CSS class names to random gibberish, but also move CSS modifiers all around.\n\nFor example, I have 1 site that prints some data with <b> to create bold text, but with a page load they generate several nested divs which each get a random CSS class, some of them containing bullshit modifications, and then set the bold font that way. And F5, you're right, the DOM changed again.\n\nSo basically, I need a HTML DOM parser that folds all these CSS classes together and makes CSS properties accessible. Much alike the \"Computed\" tab in the element inspector of a browser. If I can then write a tree selector query for these properties, then I think I'm golden.\n\nI'm using C# by the way. I've looked at AngleSharp with its CSS extension, but it actually crashes on this HTML DOM when trying to \"Render\" the website. It may perhaps be fixable but I'm interested in hearing other suggestions, because I'm certainly not the only one with this issue.\n\nI'm open to libraries from other languages, although, I haven't tried using them so far for this site.\n\nI'm not that interested in AI or Playwright/headless browser solutions, because of overhead costs.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1quoftu/html_parser_to_query_on_computed_css_rather_than/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o3emqnp",
          "author": "Resident-Piano-1663",
          "text": "Puppeteer works great for me I use the eval$$ on IG to get usernames and send dms and that website has the worst classes and nested class names they randomly change and I created a scraper that sends dms and 3nmonths later I haven't had to change any code it still works",
          "score": 3,
          "created_utc": "2026-02-03 20:30:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bwybo",
          "author": "matty_fu",
          "text": "URL?",
          "score": 1,
          "created_utc": "2026-02-03 12:27:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3f0m64",
              "author": "Hour_Analyst_7765",
              "text": "Sites like [carousell.com](http://carousell.com)",
              "score": 1,
              "created_utc": "2026-02-03 21:35:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3glixt",
                  "author": "matty_fu",
                  "text": "are there any more details you can offer? eg. which pieces of data are you trying to lift off the page, and maybe given an example of the before and after HTML",
                  "score": 1,
                  "created_utc": "2026-02-04 02:38:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3fbde1",
          "author": "worldtest2k",
          "text": "Sometimes when they change the class names the tree hierarchy stays the same. So if you know the value you want is in the 5th nested div then just count down to it to locate it",
          "score": 1,
          "created_utc": "2026-02-03 22:26:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3m0rpr",
          "author": "prehensilemullet",
          "text": "This isn‚Äôt necessarily for obfuscation purposes in all cases where you see gibberish class names. ¬†CSS-in-JS libraries generate class names, not always in a deterministic manner.",
          "score": 1,
          "created_utc": "2026-02-04 22:12:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qw22h0",
      "title": "Wikipedia and hCaptcha",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qw22h0/wikipedia_and_hcaptcha/",
      "author": "Fear_The_Creeper",
      "created_utc": "2026-02-04 21:46:41",
      "score": 3,
      "num_comments": 14,
      "upvote_ratio": 0.67,
      "text": "Hi! Wikipedia has been experimenting with using hCaptca as a replacement for their decades-old homebrewed system. They are looking at the text-based version, not the version that uses cookies.\n\nPlease note that there are currently two factions at Wikipedia. The Wikimedia foundation wants anyone scraping the site to pay them, while pretty much everybody on the Wikipedia side want to make everything super easy to scrape and reuse any way you want. I am in that second group.\n\nWhat both factions agree on is that we don't want to make it easy for spammers to actually edit Wikipedia. I think you scrapers would agree -- you want to scrape that sweet Wikipedia content, not a bunch of spam. \n\nSo I thought that I would ask the experts. How easy is it to bypass hCapcha? What are your general opinions about it? \n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qw22h0/wikipedia_and_hcaptcha/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o3rmcd7",
          "author": "Fear_The_Creeper",
          "text": "For all of the answers regarding defeating Captchas, much appreciated but not relevant to the Wikipedia case. Wikipedia's editors **want** you to be able to scrape everything on Wikipedia, Wikipedia's Captcha is only to stop spammers, not webscrapers.\n\nAlso, for Wikipedia, putting barriers in front of the most dimwitted spammers gets rid of 99% of the spam. Stopping the idiot who just discovered Infowars or Scientology or who sells cryptocurrency and wants to promote it on thousands of Wikipedia pages is the main goal, not stopping people who read r/webscrapping",
          "score": 2,
          "created_utc": "2026-02-05 19:05:59",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o3mo7qz",
          "author": "sussinbussin",
          "text": "Right now hCaptcha is the better solution because most of the big commercial solver solutions dropped support for it. Whoever is determined to spam will spam no matter what you throw at them tho",
          "score": 1,
          "created_utc": "2026-02-05 00:18:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ndgok",
          "author": "abdullah-shaheer",
          "text": "hcaptcha is hard to bypass if it's security level is set to the highest. Besides this, try to implement Akamai Bot manager as well. These can make Wikipedia only tough to scrape, but it can be scraped because almost everything (~99%) can be scraped using proper techniques and ways.",
          "score": 1,
          "created_utc": "2026-02-05 02:41:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oitlx",
              "author": "error1212",
              "text": "Akamai is one of the worst (easiest to bypass) bot management solutions on the market.",
              "score": 1,
              "created_utc": "2026-02-05 07:39:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ojm9q",
                  "author": "abdullah-shaheer",
                  "text": "Yes, you're right but I myself experienced having a tough time against Akamai bot manager in the beginning. Later on, got the solution of using it's special cookie and various other methods to bypass and trick it üôÉ. But still normal requests using requests, curl cffi, stealth requests, TLS requests won't work here at all which most beginners try against Akamai and give up getting no results. Am I right or have I missed something?",
                  "score": 1,
                  "created_utc": "2026-02-05 07:47:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3p0o28",
          "author": "Brian1398",
          "text": "If you want protection, just add shape security. HCaptcha is good, but only if it's the enterprise version.",
          "score": 1,
          "created_utc": "2026-02-05 10:30:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3urfwk",
          "author": "AnUuglyMan",
          "text": "Check this out, I actually built this to solve a huge pain point we had:\n\nhttps://github.com/RoloBits/isHumanCadence\n\nI work at an ATS and the amount of spam from auto-appliers is getting ridiculous. This pkg helps identify the real humans vs. the bots.\n\nHappy to help if you have any questions about it.",
          "score": 1,
          "created_utc": "2026-02-06 05:37:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ux475",
              "author": "matty_fu",
              "text": "where does the scoring run - client or server side? if server side, are you streaming full resolution of events or sampling?",
              "score": 1,
              "created_utc": "2026-02-06 06:22:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3wg3t0",
                  "author": "AnUuglyMan",
                  "text": "It's all client-side, so it's super fast",
                  "score": 1,
                  "created_utc": "2026-02-06 13:48:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qyx0zj",
      "title": "Holy Grail: Open Source Autonomous AI Agent With Custom WebScraper",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qyx0zj/holy_grail_open_source_autonomous_ai_agent_with/",
      "author": "AppropriateLeather63",
      "created_utc": "2026-02-08 02:35:45",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[https://github.com/dakotalock/holygrailopensource](https://github.com/dakotalock/holygrailopensource)\n\nReadme is included.\n\nWhat it does: This is my passion project. It is an end to end development pipeline that can run autonomously. It also has stateful memory, an in app IDE, live internet access, an in app internet browser, a pseudo self improvement loop, and more.\n\nThis is completely open source and free to use.\n\nIf you use this, please credit the original project. I‚Äôm open sourcing it to try to get attention and hopefully a job in the software development industry.\n\nTarget audience: Software developers\n\nComparison: It‚Äôs like replit if replit has stateful memory, an in app IDE, an in app internet browser, and improved the more you used it. It‚Äôs like replit but way better lol\n\nCodex can pilot this autonomously for hours at a time (see readme), and has. The core LLM I used is Gemini because it‚Äôs free, but this can be changed to GPT very easily with very minimal alterations to the code (simply change the model used and the api call function). Llama could also be plugged in.",
      "is_original_content": false,
      "link_flair_text": "AI ‚ú®",
      "permalink": "https://reddit.com/r/webscraping/comments/1qyx0zj/holy_grail_open_source_autonomous_ai_agent_with/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qxlgyi",
      "title": "Unable to scrape the job listings: Error 4xx Europe",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qxlgyi/unable_to_scrape_the_job_listings_error_4xx_europe/",
      "author": "MavFir",
      "created_utc": "2026-02-06 15:58:47",
      "score": 2,
      "num_comments": 17,
      "upvote_ratio": 0.6,
      "text": "Hi All,\n\nI created a Python script that scrapes job listings based on job title and location and saves the results into a CSV file. However, it consistently results in 4xx errors when I try to run it.\n\nCould you clarify if scraping job listings from such websites is legally restricted in Europe, particularly in Germany?\n\nBest regards,",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qxlgyi/unable_to_scrape_the_job_listings_error_4xx_europe/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o3x6b6o",
          "author": "Puzzleheaded_Row3877",
          "text": "No site is truly 'restricted' you're just doing something wrong.",
          "score": 4,
          "created_utc": "2026-02-06 16:01:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ysapa",
          "author": "coconut_cow",
          "text": "For your second question - all publicly available content is allowed to be scraped.",
          "score": 4,
          "created_utc": "2026-02-06 20:39:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xjcva",
          "author": "pesta007",
          "text": "They know you are trying to scrape them and they don't like it. That's why they are blocking your requests.",
          "score": 3,
          "created_utc": "2026-02-06 17:02:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3x9osf",
          "author": "Easy_Philosopher_210",
          "text": "Use pupeteer, most cases these scrapers fail because job postings are in heavy js iframes for ATS sites like workday, pupeteer or playwright if optimised properly can handle that.",
          "score": 2,
          "created_utc": "2026-02-06 16:17:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xa7q0",
          "author": "Consistent-Young6851",
          "text": "Did you set up a valid User-agent in the request ? Which library are you using ?",
          "score": 2,
          "created_utc": "2026-02-06 16:19:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zbtjq",
              "author": "Consistent-Young6851",
              "text": "You probably have to include headers with a User-agent in your request. But I doubt BeautifulSoup is still a choice in 2026, except if you're scraping some pretty old stuff.\nBetter give a try with either Selenium or Playwright.",
              "score": 2,
              "created_utc": "2026-02-06 22:16:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3xbi5y",
              "author": "MavFir",
              "text": "requests, BeautifulSoup",
              "score": 1,
              "created_utc": "2026-02-06 16:25:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3yw8w6",
                  "author": "Dry_Illustrator977",
                  "text": "ü§£bro is using the beginner stuff, it‚Äôs time to level up",
                  "score": 1,
                  "created_utc": "2026-02-06 20:59:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3x724p",
          "author": "ponteencuatro",
          "text": "4xx what? 403? 401? 429?",
          "score": 1,
          "created_utc": "2026-02-06 16:04:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xbgbb",
              "author": "MavFir",
              "text": "400, 403, 401",
              "score": 1,
              "created_utc": "2026-02-06 16:25:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3xfvix",
                  "author": "ponteencuatro",
                  "text": "in that case check that the user agent is set properly, check on the browser if the website sets any cookies and add them to the requests or any authorization header, there is tons of reason that can happen",
                  "score": 3,
                  "created_utc": "2026-02-06 16:45:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3yymvz",
          "author": "unteth",
          "text": "What‚Äôs the site?",
          "score": 1,
          "created_utc": "2026-02-06 21:11:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o408p5q",
              "author": "MavFir",
              "text": "Indeed Germany for example",
              "score": 1,
              "created_utc": "2026-02-07 01:24:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3x71og",
          "author": "Far-Good-3143",
          "text": "Status code starting from 4 usually means you are being rate limited or blocked by the server. This means the site is telling you to stop sending automated requests.\n\nWhether or not it is legal to scrape the site depends on their terms of use. You should read it on that particular site itself.",
          "score": 1,
          "created_utc": "2026-02-06 16:04:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xnf0i",
          "author": "Curiouser666",
          "text": "Are you complying with the Terms & Conditions of the site?\n\nDoes it forbid scraping in the T&Cs?",
          "score": 1,
          "created_utc": "2026-02-06 17:21:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu33u6",
      "title": "Scraping booking.com images",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qu33u6/scraping_bookingcom_images/",
      "author": "exe188",
      "created_utc": "2026-02-02 18:24:45",
      "score": 0,
      "num_comments": 9,
      "upvote_ratio": 0.5,
      "text": "Hi everyone,\n\nI‚Äôm working on a holiday lead generation platform with about 80 accommodation pages.\nFor each one, I‚Äôd like to show ~10 real images (rooms, facilities, etc.) from public Booking.com hotel pages.\n\nExample:\nhttps://www.booking.com/hotel/nl/center-parcs-het-meerdal.nl.html\n\nDoing this manually would take ages üòÖ, so before I go down the wrong path, I‚Äôd love some general guidance. Couldnt find anything regarding scraping the images when I searched for it. Seems to be more complex then just scraping the html",
      "is_original_content": false,
      "link_flair_text": "Getting started üå±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qu33u6/scraping_bookingcom_images/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o3bcun5",
          "author": "SatisfactionClear654",
          "text": "XHR/Fetch",
          "score": 1,
          "created_utc": "2026-02-03 09:35:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gdrio",
          "author": "overquantityi",
          "text": "Pagination + network interception",
          "score": 1,
          "created_utc": "2026-02-04 01:54:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37epif",
          "author": "greg-randall",
          "text": "The images are linked in the HTML.\n\nTell your favorite LLM to \n\n>Write python that opens the url [https://www.booking.com/hotel/nl/center-parcs-het-meerdal.nl.html](https://www.booking.com/hotel/nl/center-parcs-het-meerdal.nl.html) using playwright. After the page is fully loaded download all the img tag images from the page ie <img class=\"f6c12c77eb c0e44985a8 c09abd8a52 ca3dad4476\" src=\"https://cf.bstatic.com/xdata/images/hotel/max1024x768/100389149.jpg?k=0206f1ee2426349c1a98390bcb10dc81e651878a0193a816064df8389a94ee1c\\&amp;o=\" alt=\"a group of people in boats on a lake at Center Parcs Meerdal Limburg-Brabant in America\" loading=\"lazy\">  \nCreate a CSV with the image filename and the alt text.",
          "score": -1,
          "created_utc": "2026-02-02 19:04:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o389sud",
              "author": "sussinbussin",
              "text": "There‚Äôs actually a way to do this without spinning up a full browser. Normally, if all the data is already present in the page source, a good http client is enough. Booking tho throws a curveball by using an aws waf reverse proxy that serves a js challenge before you get the actual html, which makes it seem like a browser is mandatory, but in practice though, you can just grab the aws waf token that was issued for you and reuse it for as long as it's alive to scrape with plain requests\n\nhttps://preview.redd.it/0tqm28z7c5hg1.png?width=1018&format=png&auto=webp&s=8ef79cab35742330e3b58cc23ea333ec9ba9c9af\n\n    from curl_cffi import requests\n    from bs4 import BeautifulSoup\n    \n    session = requests.Session()\n    \n    session.cookies.set(\n    ¬† ¬† name=\"aws-waf-token\",\n    ¬† ¬† value=\"paste your waf token here\",\n    ¬† ¬† domain=\".booking.com\",\n    ¬† ¬† path=\"/\",\n    )\n    \n    resp = session.get(\n    ¬† ¬† \"https://www.booking.com/hotel/nl/center-parcs-het-meerdal.nl.html\",\n    ¬† ¬† impersonate=\"chrome136\",\n    )\n    \n    soup = BeautifulSoup(resp.text, \"html.parser\")\n    \n    urls = []\n    for s in soup.find_all(\"script\"):\n    ¬† ¬† parts = (s.string or \"\").split(\"highres_url:\")[1:]\n    ¬† ¬† if parts:\n    ¬† ¬† ¬† ¬† for p in parts:\n    ¬† ¬† ¬† ¬† ¬† ¬† print(p.split(\"'\")[1])\n    ¬† ¬† ¬† ¬† break\n    \n    for url in urls[:10]:\n    ¬† ¬† print(url)",
              "score": 5,
              "created_utc": "2026-02-02 21:30:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3e6s3x",
                  "author": "greg-randall",
                  "text": "Sure, but this person is clearly not super technical, or they'd already know how to do this. Plus it's only 80 pages so not a big deal to have a browser running in the background for a couple of minutes?",
                  "score": 1,
                  "created_utc": "2026-02-03 19:15:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}