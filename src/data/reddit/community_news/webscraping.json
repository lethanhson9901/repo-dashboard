{
  "metadata": {
    "last_updated": "2026-01-24 16:49:57",
    "time_filter": "week",
    "subreddit": "webscraping",
    "total_items": 13,
    "total_comments": 42,
    "file_size_bytes": 55847
  },
  "items": [
    {
      "id": "1qfm54r",
      "title": "pypecdp - a fully async python driver for chrome using pipes",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qfm54r/pypecdp_a_fully_async_python_driver_for_chrome/",
      "author": "sohaib0717",
      "created_utc": "2026-01-17 19:17:57",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hey everyone. I built a fully asynchronous chrome driver in Python using POSIX pipes. Instead of websockets, it uses file descriptors to connect to the browser using Chrome Dev Protocol.\n\n* Directly connects and controls the browser over CDP, no middleware\n* 100% asynchronous, nothing gets blocked\n* Built completely using built-in Python asyncio \n   * Except one `deprecated` dependency for python-cdp modules\n* Best for running multiple browsers on same machine\n* No risk of zombie chromes if code crashes\n* Easy customization via class inheritance\n* No automation signatures as there is no framework in between\n\nCurrently limited to POSIX based systems only (Linux/Mac).\n\n\n\nBug reports, feature requests and contributions are welcome!\n\n\n\n[https://github.com/sohaib17/pypecdp](https://github.com/sohaib17/pypecdp)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qfm54r/pypecdp_a_fully_async_python_driver_for_chrome/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qhcirb",
      "title": "What tool can I use to scrape this website?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qhcirb/what_tool_can_i_use_to_scrape_this_website/",
      "author": "BBQMosquitos",
      "created_utc": "2026-01-19 18:35:42",
      "score": 10,
      "num_comments": 26,
      "upvote_ratio": 0.92,
      "text": "My current resources are not working and put a few browser based scrapers but they don't seem to paginate. \n\n  \nNeed to scrape all 101 pages with company name, email, phone number, website, description, that is currently hiding under the green arrow on the right.\n\n\n\n[https://www.eura-relocation.com/membership/our-members?page=0](https://www.eura-relocation.com/membership/our-members?page=0)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qhcirb/what_tool_can_i_use_to_scrape_this_website/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o0iwjnl",
          "author": "albert_in_vine",
          "text": "You don't need to click the green arrow to access email. It's all available in the page source. Just use the CSS selector below to retrieve the email.\n\n`email = soup.select_one('div[class=\"field field-node--field-email field--name-field-email field--type-email field--label-hidden field__item\"] a').text.strip()`  \n`return email`\n\n    emails = [email.text.strip() for email in soup.\n    select\n    ('div[class=\"field field-node--field-email field--name-field-email field--type-email field--label-hidden field__item\"] a')]\n    return emails\n\nAbove is for one email to get all the emails\n\nRegarding web browsers, I am uncertain about that, but a simple pagination loop will retrieve each detail.\n\n    def pages_lists():\n    Â  Â  pages = [f\"https://www.eura-relocation.com/membership/our-members?page={page}\" for page    in range (1, 102)]\n    return pages",
          "score": 2,
          "created_utc": "2026-01-19 18:51:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ix4vi",
              "author": "BBQMosquitos",
              "text": "Iâ€™m not a programmer so trying to find a tool that will do it.",
              "score": 1,
              "created_utc": "2026-01-19 18:53:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0jqrhw",
                  "author": "unteth",
                  "text": "Here's a CSV containing the scraped data: https://filebin.net/08qzzv8n63l2lw7f. File expires in six days.",
                  "score": 6,
                  "created_utc": "2026-01-19 21:10:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0m00u2",
          "author": "node77",
          "text": "Import scrappy",
          "score": 2,
          "created_utc": "2026-01-20 04:26:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mrpgb",
              "author": "BBQMosquitos",
              "text": "Is there a browser addon or site? \n\nDidn't see much on google. exactly as import scrappy, just scrappy.",
              "score": 1,
              "created_utc": "2026-01-20 08:01:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1dg0xe",
          "author": "Practical-Talk213",
          "text": "incase of next time and you need to get info from a dynamic site use playwright, beautifulSoup might work for static sites but JS sites not so much",
          "score": 1,
          "created_utc": "2026-01-24 04:41:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jimkw",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-19 20:32:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jps2c",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 2,
              "created_utc": "2026-01-19 21:06:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0kq85r",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 2,
                  "created_utc": "2026-01-20 00:12:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0mw0jb",
                  "author": "webscraping-ModTeam",
                  "text": "ðŸª§ Please review the sub rules ðŸ‘‰",
                  "score": 2,
                  "created_utc": "2026-01-20 08:41:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0mvztm",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-20 08:41:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mfdbt",
          "author": "THenrich",
          "text": "chatgpt can do it. Just prompt it like this:\n\nThere are several green downarrows in this page that show the email address when the green arrow is clicked. Click on each and extract the email address. [https://www.eura-relocation.com/membership/our-members?page=0](https://www.eura-relocation.com/membership/our-members?page=0). Do this for all the pages",
          "score": -1,
          "created_utc": "2026-01-20 06:15:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mtwgb",
              "author": "BBQMosquitos",
              "text": "I think chatgpt would need to be prompted many times and it would create many batches. \n\nWere you able to do so in one go?",
              "score": 1,
              "created_utc": "2026-01-20 08:21:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0ozdhx",
                  "author": "THenrich",
                  "text": "I got the first page and part of the second page. It asked me if I wanted more. I didn't try.   \nMaybe a better prompt would work, telling it how many pages there are. It seems to be a trial and error to find to find the proper prompt.",
                  "score": 1,
                  "created_utc": "2026-01-20 16:46:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qjs0sz",
      "title": "How do you verify what you scrape?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qjs0sz/how_do_you_verify_what_you_scrape/",
      "author": "PrestigiousZombie531",
      "created_utc": "2026-01-22 11:23:38",
      "score": 8,
      "num_comments": 8,
      "upvote_ratio": 0.8,
      "text": "- Let us say you are scraping some news website\n- You wrote the fanciest scraper on the planet and have 10000 news items in your database now\n- How do you verify if it is indeed what exists on the website and there have been no mistakes while downloading?",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qjs0sz/how_do_you_verify_what_you_scrape/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o113i5h",
          "author": "andreew92",
          "text": "Depends on where in the scraping pipeline you think things could go wrong. Are you talking about scraping the incorrect elements on the page or getting blank results?\n\nIf you really wrote a fancy scraper, it should already account and flag any data which is blank/error, or you could check for what you expect the scraped results to look like (eg. a news article should be minimum 500 characters long).\n\nAnother way to validate is to send a random sample to a LLM to verify the data is correct.\n\nIt is always cheaper and easier to check as the data is coming in, rather than after itâ€™s been collected.",
          "score": 9,
          "created_utc": "2026-01-22 11:40:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11417g",
          "author": "hikingsticks",
          "text": "You can use simple data validation on collection with something like Pydantic, to check the data conforms to whatever specifications you want. That won't guarantee that a block of text is a news article, but using field validators etc you can enforce some basic checks.",
          "score": 4,
          "created_utc": "2026-01-22 11:44:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11k4xd",
          "author": "Hour_Analyst_7765",
          "text": "I capture network traces from Chrome/Firefox (see Developer console), save them as HAR, and then I've unit tests set up for my target sites. I have my framework setup so it can either load resources from the live network, from a cache HAR, or a prerecorded HAR file. Any live network data gets also written into a HAR cache with some rules for how long that data can live there (for example, some \"evergreen\" content never gets invalidated, but I may change the data extraction code at a later point, so I can refresh all my data without hitting the site a million times again).\n\nBasically it means I can also write test cases and rerun them as many times as I'd like. For these prerecorded traces, I've some unit tests in place with the expected data the data extractor should return. To this end, you can also specifically record traces of 404 pages etc. and verify your data extractor recognizes them and does not emit invalid data.\n\nAs for how to verify if the data extraction code is still valid.. yeah thats the harder bit. Some websites can change their layout/HTML structure.. sometimes they may implement a region block so you have to exclude certain countries etc. Typically my code needs to check whether certain elements get found or not, and I often include a few extra for sanity reasons (not actual data that I want to grab). Usually this works quite well.",
          "score": 4,
          "created_utc": "2026-01-22 13:29:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11qr60",
          "author": "StoicTexts",
          "text": "Could throw some asserts in. Assert first and last value of a scrape are what you expect or something",
          "score": 2,
          "created_utc": "2026-01-22 14:04:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13evdr",
          "author": "Kbot__",
          "text": "Hi,\n\nA few approaches depending on scale:\n\nDuring collection:\n\n- Schema validation (Pydantic mentioned already) - enforce expected types, required fields, min/max lengths\n- Hash each item - helps catch duplicates and track changes on re-scrapes\n- Checkpoint counts - if you expect \\~10k articles and get 8k, something broke\n\nAfter collection:\n\n- Sample verification - pull 50-100 random items, spot check against live site\n- Distribution checks - article lengths, publish dates, categories should follow expected patterns. Outliers = potential issues\n- Null/empty field rates - if 30% of your author fields are blank, your selector probably broke mid-run\n\nFor ongoing scrapes:\n\n- Diff monitoring - track when site structure changes (new class names, missing elements)\n- Sentinel pages - scrape a few known URLs with expected outputs, alert if they drift\n\nThe hardest part isn't catching errors - it's catching \"silent\" failures where the scraper returns plausible-looking garbage. That's where sampling + statistical checks help more than schema validation alone.\n\nHope this helps!",
          "score": 2,
          "created_utc": "2026-01-22 18:43:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16kbu3",
          "author": "Ok-Ingenuity-2908",
          "text": "Thoughts on my solution towards scraping on my site, having a rotation of fake content get uploaded to the site and various times of the day/week? How would a scraper differentiate?",
          "score": 1,
          "created_utc": "2026-01-23 04:32:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlewai",
      "title": "I built a CLI that turns websites into real Playwright scrapers",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qlewai/i_built_a_cli_that_turns_websites_into_real/",
      "author": "Acceptable_Grass2812",
      "created_utc": "2026-01-24 05:51:51",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 0.69,
      "text": "I built **ScrapeWizard** because using LLMs to write scrapers is slow and expensive â€” you keep generating code, running it, fixing it, and burning API credits.\n\nScrapeWizard does it differently.  \nIt scans the website (DOM, JS, network calls, selectors, pagination) and uses AI **only to generate and fix the scraper code**.  \nThe actual scraping runs locally with Playwright.\n\nSo even if data extraction fails, you still get a full working script with all the site details that you can edit and reuse.\n\nGitHub:  \n[https://github.com/pras-ops/ScrapeWizard](https://github.com/pras-ops/ScrapeWizard)\n\nWould love feedback from people who scrape or automate.",
      "is_original_content": false,
      "link_flair_text": "AI âœ¨",
      "permalink": "https://reddit.com/r/webscraping/comments/1qlewai/i_built_a_cli_that_turns_websites_into_real/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1fghkd",
          "author": "_i3urnsy_",
          "text": "Interesting definitely gonna take a look",
          "score": 1,
          "created_utc": "2026-01-24 14:20:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qh9gkm",
      "title": "Web Scraping API or custom web scraping?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qh9gkm/web_scraping_api_or_custom_web_scraping/",
      "author": "GlebarioS",
      "created_utc": "2026-01-19 16:49:52",
      "score": 7,
      "num_comments": 19,
      "upvote_ratio": 0.89,
      "text": "Hello everyone!\n\nI am new to your community and web scraping in general. I have 6 years of experience in web application development but have never encountered the topic of web scraping. I became interested in this topic when I was planning to implement a pet project for myself to track prices for products that I would like to purchase in the future. That is, the idea was that I would give the application a link to a product from any online store and it, in turn, would constantly extract data from the page and check if the price had changed. I realized that I needed web scraping and I immediately created a simple web scraping on node.js using playwright without a proxy. It coped with simple pages, but if I had already tested serious marketplaces like alibaba, I was immediately blocked. I tried with a proxy but the same thing happened. Then I came across web scraping API (unfortunately I can't remember which service I used) and it worked great! But it is damn expensive. I calculated that if I use web scraping API for my application and the application will scrape each added product every 8 hours for a month, then I will pay $1 per product. That is, if I added 20 products that will be tracked, then I will pay web scraping API +- $20. This is very expensive because I have a couple of dozen different products that I would like to submit (I am a Lego fan, so I have a lot of sets that I want to buy ðŸ˜„)\n\nAs a result, I thought about writing my own web scraping that would be simpler than all other web scraping APIs but at least cheaper. But I have no idea if it will be cheaper at all.\n\nCan someone with experience tell me if it will be cheaper?\n\nMobile/residential or data center proxies?\n\nI have seen many recommendations for web scraping in python, can I still write in node?\n\nIn which direction should I look?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qh9gkm/web_scraping_api_or_custom_web_scraping/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o0ib0wz",
          "author": "bluemangodub",
          "text": "> As a result, I thought about writing my own web scraping that would be simpler than all other web scraping APIs but at least cheaper. But I have no idea if it will be cheaper at all. Can someone with experience tell me if it will be cheaper?\n\nfirstly you need to get your browser solution working, you said you are being blocked.\n\nWill your solution be better?  Cheaper? You are asking \"I am going to build something will be be better and cheaper than something else\". No one knows.\n\nBut you have a LOT of work todo before you can sell this service. Hitting a site once is one thing, running a service hitting 100s / 1000s of services you have captcha issues, anti bot protections etc etc.\n\nGet your system working first before you try to run and sell something that isn't working :-)\n\ngood luck",
          "score": 3,
          "created_utc": "2026-01-19 17:15:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0igc6f",
              "author": "GlebarioS",
              "text": "Thanks for your comment! The thing is that I don't have a goal to create web scraping for sale as a service) I would like to create a minimal but stable version for my personal use. I don't need additional options for caching, LLM, parsing, and so on. The Web Scraping API services that I use do what I want, but there are a bunch of other features that I don't need and I wouldn't want to pay for them. Therefore, I would like to create my own stable but minimal web scraping",
              "score": 1,
              "created_utc": "2026-01-19 17:39:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ias1h",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-01-19 17:14:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0if84k",
              "author": "GlebarioS",
              "text": "Thanks for the advice! Can you suggest which tools should be used for fingerprinting on node.js?",
              "score": 1,
              "created_utc": "2026-01-19 17:34:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0jkq1d",
          "author": "LT823",
          "text": "I had similiar problems like you.\n\nI used Patchright (you can find on github) its line a better browser for scraping instead of using playwright or pupeteer.\n\nThen i conntected every request with a residential Proxy IP (cost me 5$ per month for 50 ips)\n\nAnd also make sure to try without headless - then try headless activated. Mostly it worked for me if you run it without headless.",
          "score": 2,
          "created_utc": "2026-01-19 20:42:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ylrs4",
              "author": "Krokzter",
              "text": "Just to add to this, Patchright doesn't patch headless. You'd need another solution like playwright-stealth or whichever one gets more regular updates",
              "score": 1,
              "created_utc": "2026-01-22 00:44:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ig0ep",
          "author": "nirvanist",
          "text": "Try this one, should cover most of use cases\nhttps://page-replica.com/structured/live-demo",
          "score": 1,
          "created_utc": "2026-01-19 17:38:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0iucha",
          "author": "Traditional-Set-6548",
          "text": "Go get on Coursera and take the Foundation of AI and Machine learning certification from Microsoft. You don't need to do the entire thing or any of it for that matter but they have a section in like the second part of the cert that will give you the basic code and set up for web scraping. You will have all you need to know for the most part to get a basic one going today.",
          "score": 1,
          "created_utc": "2026-01-19 18:41:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kbkyb",
          "author": "Kbot__",
          "text": "The issue you're hitting on Alibaba is proxy quality. Datacenter proxies get blocked immediately - you need \\*\\*residential\\*\\* with a large rotating pool. Mobile/residential are both good, but residential is usually more cost-effective for your use case.\n\n  \nCost-wise: residential proxies run $5-15/GB. For 20 products scraped every 8 hours, you're looking at maybe 30-50MB/day, so roughly \\*\\*$3-5/month total\\*\\*. Pay-as-you-go bandwidth pricing beats per-request, so yes - building your own will be cheaper than $20/month.\n\n  \n\\*\\*Node.js is totally fine\\*\\* - Python gets recommended a lot but Playwright works great in Node. Stick with what you know.\n\n  \nKey things for Alibaba:\n\n\\- Use residential IPs only (not datacenter)\n\n\\- If you're scraping behind login, stick to 1 account = 1 IP (don't mix IPs for the same session)\n\n\\- Block unnecessary requests (images, fonts, tracking) to keep bandwidth down\n\n\\- Add random delays between requests\n\n  \nAlibaba's bot detection is aggressive, so keep your request patterns looking human.",
          "score": 1,
          "created_utc": "2026-01-19 22:54:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0yns9t",
          "author": "Krokzter",
          "text": "For such a low scraping volume, you might not even need a proxy. Try using a stealth solution like nodriver, pydoll, playwright-stealth, etc. and see if that's enough. If you start getting blocked after a few days, then go ahead and try proxies.  \nAs for proxies, datacenter proxies are probably good enough. Even if you get blocked 9 times out of 10 (which it should never actually be this bad), you don't need to optimize for volume at that scale, so you can afford to get blocked as long as it succeeds often enough. You should at least try datacenter proxies first to see if they are good enough, since they are much cheaper.  \nIt always depends on the sites protection, but keep in mind that for low volume, your real IP will always have a better reputation than any proxy on the market.",
          "score": 1,
          "created_utc": "2026-01-22 00:55:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0j5shz",
          "author": "DigIndependent7488",
          "text": "You should be able to use something like riveter and scrape multiple product pages without any issue, shouldn't have to worry about being blocked or proxy management and costs etc. I think setting up your own DIY solution would be doable though but as others have said it won't be entirely free either ahahaha.",
          "score": 0,
          "created_utc": "2026-01-19 19:32:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfigyu",
      "title": "Open Source Captcha to test scraping methods against",
      "subreddit": "webscraping",
      "url": "https://github.com/WebDecoy/FCaptcha",
      "author": "cport1",
      "created_utc": "2026-01-17 17:00:15",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.76,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Bot detection ðŸ¤–",
      "permalink": "https://reddit.com/r/webscraping/comments/1qfigyu/open_source_captcha_to_test_scraping_methods/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qkygpn",
      "title": "What VPS provider do you use for large scale crawling ?",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qkygpn/what_vps_provider_do_you_use_for_large_scale/",
      "author": "damienlusson",
      "created_utc": "2026-01-23 18:18:31",
      "score": 5,
      "num_comments": 12,
      "upvote_ratio": 0.74,
      "text": "Hi,\n\n  \nI'm crawling at a large volume (400 millions pages per month), but hetzner and now netcup blocked my vps because of port scanning.\n\n(they don't allow port scanning fair, but when you show them your script and the use case they don't want to change position and maintain that crawling like Google or Bing does is not possible on their infra, fair they are private company and don't want to take any risk i guess)\n\n  \nWhat provider would you recommend, you use ?\n\n  \nThanks in advance",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qkygpn/what_vps_provider_do_you_use_for_large_scale/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o1aa2ws",
          "author": "amemingfullife",
          "text": "Why do you need to port scan for crawling? For websites everything is on 80/443. Sounds dodgy tbh.",
          "score": 7,
          "created_utc": "2026-01-23 18:39:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1agkf7",
              "author": "damienlusson",
              "text": "Hetzner and netcup have blocked my server because they are considering i'm doing netscan and port scanning, both of them are not allowed on their infrastructure.\n\nEven after showing them my scrapy setup and code and arguing that i'm doing what a lot of people like Google or Bing do > crawling they still are saying the above to me\n\nI guess i have the same problem as this guy : [https://www.reddit.com/r/hetzner/comments/18u09s3/hetzner\\_says\\_search\\_engine\\_crawlers\\_like\\_google/](https://www.reddit.com/r/hetzner/comments/18u09s3/hetzner_says_search_engine_crawlers_like_google/)",
              "score": 1,
              "created_utc": "2026-01-23 19:08:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1aiti6",
                  "author": "amemingfullife",
                  "text": "Ah, I see, they *consider* it port scanning even if it isnâ€™t. Fair enough. \n\nI use Vultr, DigitalOcean and GCP and never had issue crawling around 80m sites.",
                  "score": 2,
                  "created_utc": "2026-01-23 19:19:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1anats",
          "author": "Professional-Fox4161",
          "text": "A few years ago I was able to crawl half a billion pages/day on a cluster of scaleway dedicated servers. Don't know if it's still possible.",
          "score": 1,
          "created_utc": "2026-01-23 19:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1btfp1",
              "author": "unteth",
              "text": "Mind explaining your setup?",
              "score": 1,
              "created_utc": "2026-01-23 23:01:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1c1ukt",
                  "author": "Professional-Fox4161",
                  "text": "If I remember correctly there was something like 12 servers with 6 cores and 32Gb RAM, with 3 x 1TB disks for the crawling DB. 6 servers 4 cores with 32gb for the crawlers. Also 3 servers for the message queue and 1 for the config server.",
                  "score": 1,
                  "created_utc": "2026-01-23 23:46:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1bf0t2",
              "author": "Loud_Bathroom_8023",
              "text": "wtf were you scraping haha",
              "score": 0,
              "created_utc": "2026-01-23 21:50:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1bgkix",
                  "author": "Professional-Fox4161",
                  "text": "The web. Hence \"crawling\", not scraping. As OP said, it's the same thing Google and Bing and many others do and it's totally legit if you respect robots.txt, use a user-agent that clearly state your purpose by providing a link to a web page, and use a list of im addresses that is described somewhere.",
                  "score": 1,
                  "created_utc": "2026-01-23 21:57:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1angi6",
          "author": "Difficult-Cat-4631",
          "text": "Leaseweb",
          "score": 1,
          "created_utc": "2026-01-23 19:41:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1av3ek",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-23 20:16:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bikdv",
              "author": "webscraping-ModTeam",
              "text": "ðŸ’° Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide](https://www.reddit.com/r/webscraping/wiki/index/). You may also wish to re-submit your post to the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1).",
              "score": 1,
              "created_utc": "2026-01-23 22:07:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qk5u1r",
      "title": "Reservation Alerts",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qk5u1r/reservation_alerts/",
      "author": "Fearless_Space_2822",
      "created_utc": "2026-01-22 20:30:01",
      "score": 4,
      "num_comments": 22,
      "upvote_ratio": 0.67,
      "text": "Looking to build a scraper that alerts me via discord webhook whenever a reservation opens up for a place that uses [waitwhile.com](http://waitwhile.com) . I don't have much coding experience besides data languages but figured I could code this via AI. Looking for how possible and easy this could be or any tips that you experts have.\n\nThe bot would need to essentially monitor and refresh the site, then as cancellations occur or new times open up, the bot would send some sort of custom webhook to alert me of the time/day available with a link to book. I would probably have it poll every 2-3 minutes and use proxies to avoid IP ban. I was checking around github and other sites to see if something has been made already since this is a very commonly used reservation host. Thanks for all the help in advance and I could provide more information if needed.   \n  \nEDIT: The main error I'm running into is that the bot sends a webhook every time it checks rather than filtering to only when its available, then populating the webhook with info.",
      "is_original_content": false,
      "link_flair_text": "AI âœ¨",
      "permalink": "https://reddit.com/r/webscraping/comments/1qk5u1r/reservation_alerts/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o17ku9h",
          "author": "Kbot__",
          "text": "Hi,\n\nGood news â€” waitwhile makes this easier than you think. The booking data is embedded directly in the page's HTML as a JSON blob (inside a tag called `__NEXT_DATA__`). No headless browser or JS rendering needed.\n\nSince you're already using n8n, here's what I'd do:\n\n1. **HTTP Request node** â€” simple GET to the waitwhile URL\n2. **HTML Extract node** â€” pull the JSON from the `<script id=\"__NEXT_DATA__\">` tag\n3. **Check the field** `props.ssrLocationData.availableBookingResourceIds` â€” when this array is not empty, slots are open\n4. **IF node** â€” compare to the previous result (store it with a Set node or a file). Only continue if it changed from empty to non-empty\n5. **Discord webhook node** â€” fires only when new slots appear\n\nThat solves your \"webhook on every refresh\" problem â€” you're only alerting on state changes, not on every poll.\n\nI tested it just now with a plain GET request â€” the data comes back without needing proxies or anything special.",
          "score": 5,
          "created_utc": "2026-01-23 09:29:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18sxrn",
              "author": "Super_Refuse_2415",
              "text": "ðŸ”¥",
              "score": 1,
              "created_utc": "2026-01-23 14:34:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o19ejmb",
              "author": "Fearless_Space_2822",
              "text": "i'll try this",
              "score": 1,
              "created_utc": "2026-01-23 16:16:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o149vc7",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-01-22 21:05:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14enjz",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-22 21:28:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1540ll",
                  "author": "webscraping-ModTeam",
                  "text": "ðŸª§ Please review the sub rules ðŸ‘‰",
                  "score": 2,
                  "created_utc": "2026-01-22 23:38:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o157ny3",
          "author": "davak72",
          "text": "When you say â€œthe botâ€ sends a webhook every time, what are you referring to?? You mention GitHub, but not that youâ€™ve actually started implementing a bot in any particular language or based on any particular project. Itâ€™s hard to help with 0 context",
          "score": 1,
          "created_utc": "2026-01-22 23:58:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15bbip",
              "author": "Fearless_Space_2822",
              "text": "Sorry for lack of context, as I really don't code. I'm just looking to solve a problem with automation. I want  the bot to send a webhook message into a designated discord channel alerting me of an open reservation on the website, with the link to book said reservation.  \n  \nI mentioned github just as a site that I scoured for possible solutions that have already been made by other people. So far, through json I have a bot that sends messages through a webhook when it refreshes the site, but lacks the actual useful info in the webhook. I have no preferences just want to get this solved.",
              "score": 1,
              "created_utc": "2026-01-23 00:17:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o15bnod",
                  "author": "matty_fu",
                  "text": "If youâ€™re not comfortable sharing the URL of the specific service youâ€™re scraping, can you find another one that uses waitwhile? Hopefully the same techniques will apply to both",
                  "score": 1,
                  "created_utc": "2026-01-23 00:19:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o15vuzc",
                  "author": "davak72",
                  "text": "Ohhh. Gotcha. Youâ€™ll honestly probably have good success working even with ChatGPT free. What language is your minimal bot in?",
                  "score": 1,
                  "created_utc": "2026-01-23 02:10:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o15r26r",
          "author": "_i3urnsy_",
          "text": "Is your solutions browser or request based? Itâ€™s good youâ€™re getting the webhook, but sounds like you just are missing whatever the identifier for open or reserved bookings is.\n\nWithout the specific url or a similar one itâ€™s hard to assist further.",
          "score": 1,
          "created_utc": "2026-01-23 01:43:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o167j3b",
              "author": "Fearless_Space_2822",
              "text": "Im trying to make it work on [https://waitwhile.com/locations/chromehearts/services?registration=booking](https://waitwhile.com/locations/chromehearts/services?registration=booking)\n\nwaitwhile is a popular host for waitlists and reservations.",
              "score": 2,
              "created_utc": "2026-01-23 03:15:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qgtv79",
      "title": "Unable to create Reddit app for PRAW, stuck on policy message",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qgtv79/unable_to_create_reddit_app_for_praw_stuck_on/",
      "author": "Glum_Masterpiece_592",
      "created_utc": "2026-01-19 04:04:59",
      "score": 4,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Iâ€™m trying to collect Reddit posts on a specific topic for **research purposes** using **PRAW**, but Iâ€™m unable to create a Reddit app to get the client ID/secret.\n\nDuring app creation, Reddit keeps showing a message asking me to read the full policies and wonâ€™t let me proceed (similar to the attached screenshot). Iâ€™ve read the policies but canâ€™t figure out whatâ€™s blocking the app creation.\n\nQuestions:\n\n* Has anyone encountered this issue recently?\n* Is there a specific requirement I might be missing during app setup?\n* If PRAW/app creation isnâ€™t possible, what are **recommended alternatives** for collecting Reddit post data (within Redditâ€™s rules)?\n\nAny pointers would be appreciated. Thanks!\n\nhttps://preview.redd.it/aybix754d8eg1.png?width=1590&format=png&auto=webp&s=8acd9fc278d78c64024673401053063846ebaf9e\n\n",
      "is_original_content": false,
      "link_flair_text": "Getting started ðŸŒ±",
      "permalink": "https://reddit.com/r/webscraping/comments/1qgtv79/unable_to_create_reddit_app_for_praw_stuck_on/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o0g8z3f",
          "author": "abdullah-shaheer",
          "text": "I guess from Nov onwards, you can't create apps freely, you have to request to them via an application form for the API access",
          "score": 2,
          "created_utc": "2026-01-19 09:47:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0qtqd9",
          "author": "0xMassii",
          "text": "Just scrape Reddit like a real men, you don't need of PRAW",
          "score": 1,
          "created_utc": "2026-01-20 21:50:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qi0hiq",
      "title": "Weekly Webscrapers - Hiring, FAQs, etc",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qi0hiq/weekly_webscrapers_hiring_faqs_etc/",
      "author": "AutoModerator",
      "created_utc": "2026-01-20 13:01:02",
      "score": 4,
      "num_comments": 6,
      "upvote_ratio": 0.76,
      "text": "**Welcome to the weekly discussion thread!**\n\nThis is a space for web scrapers of all skill levelsâ€”whether you're a seasoned expert or just starting out. Here, you can discuss all things scraping, including:\n\n* Hiring and job opportunities\n* Industry news, trends, and insights\n* Frequently asked questions, like \"How do I scrape LinkedIn?\"\n* Marketing and monetization tips\n\nIf you're new to web scraping, make sure to check out the [Beginners Guide](https://webscraping.fyi) ðŸŒ±\n\nCommercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the [monthly thread](https://reddit.com/r/webscraping/about/sticky?num=1)",
      "is_original_content": false,
      "link_flair_text": "Hiring ðŸ’°",
      "permalink": "https://reddit.com/r/webscraping/comments/1qi0hiq/weekly_webscrapers_hiring_faqs_etc/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o0uidbt",
          "author": "Training_Hand_1685",
          "text": "1 Comment out of 3\n\nHi! Looking for someone who can help me with this. Paid.\n\nWe take \"quizzes\" online that guides our learning. We have to reach to level 6. So we do many of these questions over and over, passing more and more, increasing our level each time. Goal is to level 6. The questions are helpful to show you what you don't know. So, I like to grab the questions, the correct answers, and the explanation - and test myself over and over.\n\nIssues for web scraping (3 to 4 pics).\n\n1. the explanation is always collapsed... you have to click the explanation button for the explanation text to dropdown/appear. Pic 1 in this comment has a closed explanation.\n\n(See other comments. )\n\nhttps://preview.redd.it/i7bvxi2o9peg1.png?width=669&format=png&auto=webp&s=fa5a207dc95ab69c304efcebd83d0c43894ce51a",
          "score": 2,
          "created_utc": "2026-01-21 12:57:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0x4bvm",
              "author": "fourhoarsemen",
              "text": "Hey, I dm'd you. I write custom crawlers and scrapers, and can help you with this. Let me know!",
              "score": 1,
              "created_utc": "2026-01-21 20:20:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0pfpl9",
          "author": "themoneysystem",
          "text": "Thanks",
          "score": 1,
          "created_utc": "2026-01-20 18:01:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0uihsx",
          "author": "Training_Hand_1685",
          "text": "2 Comment out of 3\n\nPic 2, in this comment, shows the explanation opened.\n\n1. Goal is to constantly use this for multiple chapters across different classes.\n2. Multiple classes use the same quizzing platform. I save the UNIQUE questions+answer+explanation... the questions repeat often so the webscraper would need to check what questions already exist from prior runs. I currently save the questions in google docs (would like to keep it there).\n\nhttps://preview.redd.it/xxzayj42apeg1.png?width=676&format=png&auto=webp&s=bc56c0c8192d0e9e7d0c6b7adb2fa21f022b47e1",
          "score": 1,
          "created_utc": "2026-01-21 12:58:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0uiruj",
          "author": "Training_Hand_1685",
          "text": "https://preview.redd.it/pmgfj5h6apeg1.png?width=1580&format=png&auto=webp&s=6329dd8ca4cc2c450e51441f3f97534d26a698d0\n\n1. 3 comment out of 3 : ------ Pic 3 is an example of how my questions (bolded) are with their answer (below question) and copied explanation from the quizzes. This also shows that you have to scroll within a box within the page.",
          "score": 1,
          "created_utc": "2026-01-21 13:00:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhfe65",
      "title": "scraping whatsapp web",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qhfe65/scraping_whatsapp_web/",
      "author": "Sufficient-Bad-532",
      "created_utc": "2026-01-19 20:16:59",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 0.81,
      "text": "i tried scraping all the phone numbers im realated too, created a quick python script using playwrite, it works like this click chat -> click header -> click search member -> get all members in chat -> go to a different chat.  \nany way after like 5 chats i got banned for 24 hours. \n\nmy question is how do i bypass this ? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qhfe65/scraping_whatsapp_web/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o0p98po",
          "author": "Obvious-Bet-1338",
          "text": "What did you already tried to bypass it ?",
          "score": 1,
          "created_utc": "2026-01-20 17:32:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18kmef",
          "author": "_i3urnsy_",
          "text": "Consider proxies. Adding random delays between clicks. Maybe doing it in small batches over an extended period of time.",
          "score": 1,
          "created_utc": "2026-01-23 13:51:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o18l2rl",
              "author": "Sufficient-Bad-532",
              "text": "I domt think proxies will work because its connected to my whatsapp acount therfore doesnt matther where i do it in the world its still connected to my account, maybe like scraping 1 chat a day will work but it still will take me about a year to complete all the chats",
              "score": 1,
              "created_utc": "2026-01-23 13:54:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qhynfr",
      "title": "Scaling 100+ Vendor Dashboards Without APIs Is a Nightmare",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qhynfr/scaling_100_vendor_dashboards_without_apis_is_a/",
      "author": "Sufficient-Owl-9737",
      "created_utc": "2026-01-20 11:28:09",
      "score": 3,
      "num_comments": 4,
      "upvote_ratio": 0.72,
      "text": "Half these dashboards, AWS console reboots, GCP inventory, Azure billing, have no public APIs. You end up clicking manually or writing fragile Selenium scripts that die on CAPTCHAs, timeouts, or the slightest React tweak. My selectors got wiped twice in one month. Headless Puppeteer handles around ten portals fine. Push it to fifty and localStorage breaks, IP bans hit after a couple of hours, and random modals destroy everything. Playwright lasts longer but scripting human-like flows, dropdown chains and confirm dialogs, feels endless. Has anyone scaled this to a hundred plus portals without losing their mind? Custom UI wrappers pretending to be APIs? Tools that survive vendor UI overhauls and lock-ins?",
      "is_original_content": false,
      "link_flair_text": "Scaling up ðŸš€",
      "permalink": "https://reddit.com/r/webscraping/comments/1qhynfr/scaling_100_vendor_dashboards_without_apis_is_a/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o0ngy5j",
          "author": "Comfortable_Clue5430",
          "text": "For 100+ dashboards, using Anchor Browser with isolated browser profiles, splitting portals across machines, and replaying interactions with slight randomness helps keep sessions stable and reduces failures and maintain reliability at scale.",
          "score": 3,
          "created_utc": "2026-01-20 11:49:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0o9kbs",
          "author": "jinef_john",
          "text": "I think that isn't a scraping(scaling) problem, it's more of a design problem. Maintenance shouldn't be a problem if you make good decisions on architecture and design patterns ( i.e logs, metrics, isolating auth/bootstrap from data retrieval etc). And a lot of times, UI automations should really be a last resort.",
          "score": 3,
          "created_utc": "2026-01-20 14:43:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj1ygy",
      "title": "TLS Help",
      "subreddit": "webscraping",
      "url": "https://www.reddit.com/r/webscraping/comments/1qj1ygy/tls_help/",
      "author": "echno1",
      "created_utc": "2026-01-21 16:06:50",
      "score": 3,
      "num_comments": 6,
      "upvote_ratio": 0.81,
      "text": "Iâ€™m using tls-client in Go to mimic real Chrome TLS fingerprints.\n\nEven with:\n\n* Proper client profiles\n* Correct UA + header order\n* HTTP/2 enabled\n\nIâ€™m still getting detected (real Chrome works on same proxy).\n\nCan anyone help?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/webscraping/comments/1qj1ygy/tls_help/",
      "domain": "self.webscraping",
      "is_self": true,
      "comments": [
        {
          "id": "o12aqrh",
          "author": "bluemangodub",
          "text": "Check the headers you are sending and check what chrome is sending.  The answer is always **ALWAYS** the headers.\n\nProbably there is a JS cookie set, which you're not doing as not processing JS",
          "score": 2,
          "created_utc": "2026-01-22 15:43:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0xxil4",
          "author": "army_of_wan",
          "text": "Which vendor is it ? akamai ?",
          "score": 1,
          "created_utc": "2026-01-21 22:35:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0y73ba",
          "author": "Hour_Analyst_7765",
          "text": "Check ja4/akamai hashes",
          "score": 1,
          "created_utc": "2026-01-21 23:25:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11mhqm",
          "author": "Kbot__",
          "text": "Hi,\n\nTLS fingerprinting is a pain. Even with tls-client doing everything \"right,\" there's a ton of signals beyond cipher suites:\n\n* **JA4 fingerprint**Â (JA3 is basically dead now since browsers randomize TLS extensions) - check yours vs real Chrome atÂ [tls.peet.ws/api/all](http://tls.peet.ws/api/all)\n* **HTTP/2 SETTINGS frame**Â \\- the order and default values matter more than you'd think\n* **ClientHello extension order**Â \\- not just having the right extensions, but the sequence\n\nWhat protection are you hitting? Akamai, Cloudflare, something else? That'll help narrow down what's catching you.\n\nPS: Worst comes to worst, Wireshark, intercept the request from a real browser vs your setup, and see what the difference is. I had a site once that I spent weeks on, and Wireshark showed me the order of headers. One more thing, only using Rust i was able to recreate the same order. \n\nGood luck",
          "score": 1,
          "created_utc": "2026-01-22 13:42:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16kcek",
              "author": "matty_fu",
              "text": "Repost with a different JA3 tool - one that works, and isn't covered in popup spam",
              "score": 1,
              "created_utc": "2026-01-23 04:32:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o17melb",
                  "author": "Kbot__",
                  "text": "edited, thanks for bringing that up. If you know a better tool, please let me know.",
                  "score": 1,
                  "created_utc": "2026-01-23 09:44:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}